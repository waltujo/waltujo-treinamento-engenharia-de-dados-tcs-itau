@ARTICLE{6914469,
author={Otero, Carlos E. and Peter, Adrian},
journal={IEEE Intelligent Systems},
title={Research Directions for Engineering Big Data Analytics Software},
year={2015},
volume={30},
number={1},
pages={13-19},
abstract={Many software startups and research and development efforts are actively trying to harness the power of big data and create software with the potential to improve almost every aspect of human life. As these efforts continue to increase, full consideration needs to be given to the engineering aspects of big data software. Since these systems exist to make predictions on complex and continuous massive datasets, they pose unique problems during specification, design, and verification of software that needs to be delivered on time and within budget. But, given the nature of big data software, can this be done? Does big data software engineering really work? This article explores the details of big data software, discusses the main problems encountered when engineering big data software, and proposes avenues for future research.},
keywords={Software reliability;Big data;Intelligent systems;Mathematical model;Data models;Software engineering;big data;software engineering;reliability;design;testing;quality;intelligent systems},
doi={10.1109/MIS.2014.76},
ISSN={1941-1294},
month={Jan},}
@INPROCEEDINGS{8621977,
author={Hou, Shufang and Gong, Zhicheng and Nie, Qiangqiang and Xiao, Quanwu and Tan, Yudong},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={A hybrid predictive model for high-frequency and multi-periodic data in call center of online travel agency},
year={2018},
volume={},
number={},
pages={3128-3134},
abstract={Ctrip.com is a leading online travel company in China, providing travel services including accommodation and flight reservations. With high quality customer service in mind, Ctrip.com has made substantial investments in establishing its own in-house call center. In order to optimize customer satisfaction while controlling operational costs, call center management must determine staffing level at least a week in advance. Our paper addresses this issue by proposing a methodology that predicts the upcoming week's call volume in half-hour increments. The proposed method is a hybrid of a time series TBATS model (Trigonometric seasonal formulation, Box-Cox transform, ARIMA errors, Trend, and Seasonal components), a model of processing time series, and boosting tree theory. For high prediction accuracy, we also considered the effects of related events such as flight delays and ticket sales, and illustrated how to incorporate them as exogenous variables in the tree model part. With this approach, call center management can arrange staffing availability efficiently based on call volume predictions.},
keywords={Predictive models;Time series analysis;Delays;Meteorology;Data models;Business intelligence;Market research;call volume prediction;high frequency data;multi-period data;boosting tree},
doi={10.1109/BigData.2018.8621977},
ISSN={},
month={Dec},}
@ARTICLE{7919253,
author={Perera, Lokukaluge Prasad and Mo, Brage},
journal={IEEE Transactions on Vehicular Technology},
title={Machine Intelligence Based Data Handling Framework for Ship Energy Efficiency},
year={2017},
volume={66},
number={10},
pages={8659-8666},
abstract={Appropriate navigation strategies should be developed to overcome the current shipping industrial challenges under emission-control-based energy efficiency measures. Effective navigation strategies should be based on accurate ship performance and navigation information; therefore, various onboard data handling systems are installed on ships to collect large-scale datasets. Ship performance and navigation data that are collected to develop such navigation strategies can be an integrated part of the ship energy efficiency management plan (SEEMP). Hence, the SEEMP with various navigation strategies can play an important part of e-navigation under modern integrated bridge systems. This study proposes a machine-intelligence-based data handling framework for ship performance and navigation data to improve the quality of the respective navigation strategies. The prosed framework is divided into two main sections of pre and post processing. The data pre-processing is an onboard application that consists of sensor faults detection, data classification, and data compression steps. The data post processing is a shore-based application (i.e., in data centers) and that consists of data expansion, integrity verification, and data regression steps. Finally, a ship performance and navigation dataset of a selected vessel is analyzed through the proposed framework and successful results are presented in this study.},
keywords={Navigation;Marine vehicles;Data handling;Energy efficiency;Data analysis;Data visualization;Safety;Big data;data handling;emission control;energy efficiency;machine intelligence;shipping industry},
doi={10.1109/TVT.2017.2701501},
ISSN={1939-9359},
month={Oct},}
@INPROCEEDINGS{7906022,
author={Hosseini, Mohammad-Parsa and Soltanian-Zadeh, Hamid and Elisevich, Kost and Pompili, Dario},
booktitle={2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP)},
title={Cloud-based deep learning of big EEG data for epileptic seizure prediction},
year={2016},
volume={},
number={},
pages={1151-1155},
abstract={Developing a Brain-Computer Interface (BCI) for seizure prediction can help epileptic patients have a better quality of life. However, there are many difficulties and challenges in developing such a system as a real-life support for patients. Because of the nonstationary nature of EEG signals, normal and seizure patterns vary across different patients. Thus, finding a group of manually extracted features for the prediction task is not practical. Moreover, when using implanted electrodes for brain recording massive amounts of data are produced. This big data calls for the need for safe storage and high computational resources for real-time processing. To address these challenges, a cloud-based BCI system for the analysis of this big EEG data is presented. First, a dimensionality-reduction technique is developed to increase classification accuracy as well as to decrease the communication bandwidth and computation time. Second, following a deep-learning approach, a stacked autoencoder is trained in two steps for unsupervised feature extraction and classification. Third, a cloud-computing solution is proposed for real-time analysis of big EEG data. The results on a benchmark clinical dataset illustrate the superiority of the proposed patient-specific BCI as an alternative method and its expected usefulness in real-life support of epilepsy patients.},
keywords={Cloud computing;Electroencephalography;Feature extraction;Big Data;Epilepsy;Real-time systems;Electrodes;Big Data;Brain-Computer Interface;Cloud Computing;Deep Learning;EEG;Epilepsy;Seizure Prediction},
doi={10.1109/GlobalSIP.2016.7906022},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9006489,
author={Park, Hyunseop and Ko, Hyunwoong and Lee, Yung-Tsun T. and Cho, Hyunbo and Witherell, Paul},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={A Framework for Identifying and Prioritizing Data Analytics opportunities in Additive Manufacturing},
year={2019},
volume={},
number={},
pages={3458-3467},
abstract={Many industries, including manufacturing, are adopting data analytics (DA) in making decisions to improve quality, cost, and on-time delivery. In recent years, more research and development efforts have applied DA to additive manufacturing (AM) decision-making problems such as part design and process planning. Though there are many AM decision-making problems, not all benefit greatly from DA. This may be due to insufficient AM data, unreliable data quality, or the fact that DA is not cost effective when it is applied to some AM problems. This paper proposes a framework to investigate DA opportunities in a manufacturing operation, specifically AM. The proposed framework identifies and prioritizes AM potential opportunities where DA can make impact. The proposed framework is presented in a five-tier architecture, including value, decision-making, data analytics, data, and data source tiers. A case study is developed to illustrate how the proposed framework identifies DA opportunities in AM.},
keywords={Decision making;Solid modeling;Mechanical variables measurement;Electric variables measurement;Shape measurement;Data analysis;Analytical models;Data analytics;opportunity identification and prioritization;architecture;additive manufacturing},
doi={10.1109/BigData47090.2019.9006489},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9103804,
author={Xing, Zhang and Zhu, Li and Lijun, Zhang},
booktitle={2020 International Conference on Computer Engineering and Application (ICCEA)},
title={A Study on the Application of the Technology of Big Data and Artificial Intelligence to Audit},
year={2020},
volume={},
number={},
pages={797-800},
abstract={Artificial intelligence has great advantages in improving the audit efficiency, reducing the audit risk and changing the work mode, which can help find value and create value for audit, and greatly improve the quality of audit. But it will also bring some new technical and systemic risks, which have profound influence on the subject and object. The theory and practice of audit must be carefully analyzed, in particular, by using data mining to make audit judgments, using association statistics to detect trails, using cluster analysis to determine the focus of audit content, using intelligent visualization to speed up the progress of audit, using distributed processing system to improve the effectiveness of audit.},
keywords={Big Data;Artificial intelligence;Data mining;Data models;Data analysis;Analytical models;Industries;Artificial intelligence;audit;Efficiency;Rationality},
doi={10.1109/ICCEA50009.2020.00174},
ISSN={},
month={March},}
@INPROCEEDINGS{9406925,
author={Sun, Jingyu and Wei, Dong and Shagor, Md Masum Billa},
booktitle={2020 Eighth International Conference on Advanced Cloud and Big Data (CBD)},
title={Hard Example Mining based Adversarial Autoencoder Recommendation Algorithm},
year={2020},
volume={},
number={},
pages={103-106},
abstract={Commonly used datasets in recommendation research suffer from unbalanced data distribution, sparsity, and different user rating preferences. All these problems affect the quality of recommendation. Thus, this paper proposed a recommendation model by combining hard example mining with adversarial autoencoder. Considering the difference in users' preference, Mean Model based triplet loss algorithm was introduced to classify the dataset into positive and negative samples and thus improve the quality of the training data. Using classified samples, the rating prediction model was trained from both reconstruction and adversarial aspects. Adam optimization algorithm was used to calculate different update gradients for different parameters. Experimental results show that the recommendation model improves the recommendation accuracy significantly, and several performance indicators are better than baseline models.},
keywords={Training data;Predictive models;Big Data;Prediction algorithms;Data models;Classification algorithms;Data mining;recommendation system;hard example mining;Mean Model;adversarial autoencoder},
doi={10.1109/CBD51900.2020.00027},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7004412,
author={Shin, Jennifer},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Investigating the accuracy of the openFDA API using the FDA Adverse Event Reporting System (FAERS)},
year={2014},
volume={},
number={},
pages={48-53},
abstract={The US Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) is a database that contains information on adverse event and medication error reports submitted to the FDA. Each quarter the FDA releases the data to the public, but accessing the data requires researchers to download, import, and consolidate data for every quarter starting from 2004. In an effort to provide easier access to this, the FDA launched the openFDA initiative in June 2014, which gives the public API access to information about adverse events reports. Although the API enables easier access to the FAERS data, the quality of the API design and the features of the data set will determine the reliability of the information retrieved. Thus, errors in the API can result in inaccurate and unreliable data analysis. Furthermore, the number of adverse events reports retrieved by the API for a particular drug can differ from the FAERS data files due to the openFDA harmonization process and the existence of multiple entries and variations for any given drug name in the FAERS data files. Since there are no universal rule that can be used to identify errors or potential issues, we propose evaluating the openFDA API by searching for a particular drug (brand name), Yaz, and the generic name, Drospirenone Ethinyl Estradiol, and comparing the results against the FAERS data files. Our results show that in the case of Yaz, the openFDA API and the drug harmonization process is inaccurate and inconsistent.},
keywords={Drugs;US Government agencies;Databases;Accuracy;IEEE Potentials;Big data;openFDA;FDA;adverse events;FDA Adverse Events Reporting System;FAERS},
doi={10.1109/BigData.2014.7004412},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9468763,
author={Liu, Zhong-Jie and Tseng, Shih-Pang},
booktitle={2020 8th International Conference on Orange Technology (ICOT)},
title={An Efficient Data Collection and Analysis System for Teaching Evolution},
year={2020},
volume={},
number={},
pages={1-3},
abstract={How to effectively and efficiently evaluate learning quality in the higher education institutions has become one very important issue. The teaching quality assessment (TQA) is one of the measures to improve the teachers' learning activities planning and organization. In the past, the assessment from the students' feedback is not enough to more general and comprehensive evaluate the teaching quality. For supporting the administrators and non-administrators peer review, we proposed a total teaching quality assessment system. The TQA system has implemented and used in CCIT, China.},
keywords={Fuzzy logic;Education;Organizations;Data collection;Quality assessment;Planning;Social implications of technology;Constructivism;Engineering Education;Problem-based Learning},
doi={10.1109/ICOT51877.2020.9468763},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8079850,
author={Wen, Huaiyu and Shah, Fadia and Li, Jianping and Memon, Raheel Ahmed and Shah, Faiza},
booktitle={2016 13th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)},
title={X-Ray image reliability using biorthogonal wavelet compression for medical big data},
year={2016},
volume={},
number={},
pages={256-259},
abstract={Medical Big Data (MBD) is all concerned with the medical related bio-informatics data. The importance of medical investigations for disease identification and complications highly depend upon the quality data. This scientific era is the age of digital imaging and information processing. For every individual the medical record is collection of important data. Due to the smart telecommunication technology initiatives, MBD is very crucial research area. This data is in the form of images and video files, which are generated after technical examinations like CT scan, MRI, X-ray, ECG, and many others.},
keywords={Image coding;X-ray imaging;Big Data;Reliability;Wavelet analysis;Medical diagnostic imaging;Medical Big Data;Biorthogonal Wavelet;X-Ray;Compression},
doi={10.1109/ICCWAMTIP.2016.8079850},
ISSN={},
month={Dec},}
@ARTICLE{8392685,
author={Yacchirema, Diana C. and Sarabia-JáCome, David and Palau, Carlos E. and Esteve, Manuel},
journal={IEEE Access},
title={A Smart System for Sleep Monitoring by Integrating IoT With Big Data Analytics},
year={2018},
volume={6},
number={},
pages={35988-36001},
abstract={Obtrusive sleep apnea (OSA) is one of the most important sleep disorders because it has a direct adverse impact on the quality of life. Intellectual deterioration, decreased psychomotor performance, behavior, and personality disorders are some of the consequences of OSA. Therefore, a real-time monitoring of this disorder is a critical need in healthcare solutions. There are several systems for OSA detection. Nevertheless, despite their promising results, these systems not guiding their treatment. For these reasons, this research presents an innovative system for both to detect and support of treatment of OSA of elderly people by monitoring multiple factors such as sleep environment, sleep status, physical activities, and physiological parameters as well as the use of open data available in smart cities. Our system architecture performs two types of processing. On the one hand, a pre-processing based on rules that enables the sending of real-time notifications to responsible for the care of elderly, in the event of an emergency situation. This pre-processing is essentially based on a fog computing approach implemented in a smart device operating at the edge of the network that additionally offers advanced interoperability services: technical, syntactic, and semantic. On the other hand, a batch data processing that enables a descriptive analysis that statistically details the behavior of the data and a predictive analysis for the development of services, such as predicting the least polluted place to perform outdoor activities. This processing uses big data tools on cloud computing. The performed experiments show a 93.3% of effectivity in the air quality index prediction to guide the OSA treatment. The system's performance has been evaluated in terms of latency. The achieved results clearly demonstrate that the pre-processing of data at the edge of the network improves the efficiency of the system.},
keywords={Sleep apnea;Monitoring;Biomedical monitoring;Real-time systems;Computer architecture;Big Data;Sensors;Internet-of-Things;big data;interoperability;sleep monitoring;health monitoring;open data;fog computing;cloud computing},
doi={10.1109/ACCESS.2018.2849822},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{6916940,
author={Alhamazani, Khalid and Ranjan, Rajiv and Jayaraman, Prem Prakash and Mitra, Karan and Wang, Meisong and Huang, Zhiqiang George and Wang, Lizhe and Rabhi, Fethi},
booktitle={2014 IEEE 15th International Conference on Mobile Data Management},
title={Real-Time QoS Monitoring for Cloud-Based Big Data Analytics Applications in Mobile Environments},
year={2014},
volume={1},
number={},
pages={337-340},
abstract={The service delivery model of cloud computing acts as a key enabler for big data analytics applications enhancing productivity, efficiency and reducing costs. The ever increasing flood of data generated from smart phones and sensors such as RFID readers, traffic cams etc require innovative provisioning and QoS monitoring approaches to continuously support big data analytics. To provide essential information for effective and efficient bid data analytics application QoS monitoring, in this paper we propose and develop CLAMS-Cross-Layer Multi-Cloud Application Monitoring-as-a-Service Framework. The proposed framework: (a) performs multi-cloud monitoring, and (b) addresses the issue of cross-layer monitoring of applications. We implement and demonstrate CLAMS functions on real-world multi-cloud platforms such as Amazon and Azure.},
keywords={Monitoring;Quality of service;Big data;Cloud computing;Conferences;Real-time systems;Computational modeling;multi-clouds;cross-layer monitoring;QoS;cloud computing},
doi={10.1109/MDM.2014.74},
ISSN={2375-0324},
month={July},}
@INPROCEEDINGS{8394424,
author={Liu, Yi-Yuan and Pan, Telung and Cheng, Bor-Wen},
booktitle={2018 IEEE International Conference on Applied System Invention (ICASI)},
title={Volume of surgery and medical quality: A big data analysis of hip hemiarthroplasty},
year={2018},
volume={},
number={},
pages={943-945},
abstract={Background and purpose: Hemiarthroplasty of the hip is one of most common surgery for the patient with the displaced moral neck fracture. Methods: This study analyzed the big data sample of national database of all hospital admissions underwent hip hemiarthroplasty from the date of January 2004 to December 2006. Results: Lots of insights were found from big data relational analysis techniques, and the result shows that the less provider volume of the surgeon or more Charlson comorbidity index (CCI) scores let to higher admission mortality and complication rate. Conclusions: The evidence of this study shows that the surgeon volume affects the medical quality and utilization and should apply on the reference of reimbursements and to evaluate the performance of surgeon for reward.},
keywords={Hospitals;Surgery;Hip;Joints;Medical diagnostic imaging;Correlation;medical quality;volume of surgery;hip hemiarthroplasty},
doi={10.1109/ICASI.2018.8394424},
ISSN={},
month={April},}
@ARTICLE{8293821,
author={Li, Xiaoyong and Yuan, Jie and Ma, Huadong and Yao, Wenbin},
journal={IEEE Transactions on Information Forensics and Security},
title={Fast and Parallel Trust Computing Scheme Based on Big Data Analysis for Collaboration Cloud Service},
year={2018},
volume={13},
number={8},
pages={1917-1931},
abstract={Providing high trustworthy service is the most fundamental task for any cloud computing platform. Users are willing to deliver their computing tasks and the most sensitive data to cloud data centers, which is based on the trust relationship established between users and cloud service providers. However, with the development of collaboration cloud computing, how to provider fast response for a large number of users' service requests becomes a challenging problem. In order to quickly provide highly trustworthy services, the service platform must efficiently and quickly reply tens of millions of service requests, and automatically match-make tens of thousands of service resources. In this context, lightweight and fast (high-speed, low-overhead) trust computing schemes become the fundamental demand for implementing a trustworthy and collaborative cloud service. In this paper, we propose an innovative and parallel trust computing scheme based on big data analysis for the trustworthy cloud service environment. First, a distributed and modular perceiving architecture for large-scale virtual machines' service behavior is proposed relying on distributed monitoring agents. Then, an adaptive, lightweight, and parallel trust computing scheme is proposed for big monitored data. To the best of our knowledge, this paper is the first to use a blocked and parallel computing mechanism, the speed of trust calculation is greatly accelerated, which makes this trust computing scheme very suitable for a large-scale cloud computing environment. Performance analysis and experimental results verify feasibility and effectiveness of the proposed scheme.},
keywords={Cloud computing;Collaboration;Monitoring;Security;Big Data;Quality of service;Computer architecture;Cloud computing;service behavior monitoring;trust computing;big data analysis},
doi={10.1109/TIFS.2018.2806925},
ISSN={1556-6021},
month={Aug},}
@ARTICLE{7572985,
author={Li, Yaliang and Liu, Chaochun and Du, Nan and Fan, Wei and Li, Qi and Gao, Jing and Zhang, Chenwei and Wu, Hao},
journal={IEEE Transactions on Big Data},
title={Extracting Medical Knowledge from Crowdsourced Question Answering Website},
year={2020},
volume={6},
number={2},
pages={309-321},
abstract={The medical crowdsourced question answering (Q&A) websites are booming in recent years, and an increasingly large amount of patients and doctors are involved. The valuable information from these medical crowdsourced Q&A websites can benefit patients, doctors and the society. One key to unleash the power of these Q&A websites is to extract medical knowledge from the noisy question-answer pairs and filter out unrelated or even incorrect information. Facing the daunting scale of information generated on medical Q&A websites everyday, it is unrealistic to fulfill this task via supervised method due to the expensive annotation cost. In this paper, we propose a Medical Knowledge Extraction (MKE) system that can automatically provide high-quality knowledge triples extracted from the noisy question-answer pairs, and at the same time, estimate expertise for the doctors who give answers on these Q&A websites. The MKE system is built upon a truth discovery framework, where we jointly estimate trustworthiness of answers and doctor expertise from the data without any supervision. We further tackle three unique challenges in the medical knowledge extraction task, namely representation of noisy input, multiple linked truths, and the long-tail phenomenon in the data. The MKE system is applied to real-world datasets crawled from xywy.com, one of the most popular medical crowdsourced Q&A websites. Both quantitative evaluation and case studies demonstrate that the proposed MKE system can successfully provide useful medical knowledge and accurate doctor expertise. We further demonstrate a real-world application, Ask A Doctor, which can automatically give patients suggestions to their questions.},
keywords={Medical services;Medical diagnostic imaging;Noise measurement;Big data;Data mining;Knowledge discovery;Crowdsourced question answering;medical knowledge extraction;truth discovery},
doi={10.1109/TBDATA.2016.2612236},
ISSN={2332-7790},
month={June},}
@INPROCEEDINGS{8258223,
author={Müller, Daniel and Te, Yiea-Funk and Jain, Pratiksha},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Improving data quality through high precision gender categorization},
year={2017},
volume={},
number={},
pages={2628-2636},
abstract={First name to gender mappings have been widely recognized as a critical tool to complete, study and validate data records in a range of different areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 6 million people, provided by a car insurance. We then study how naming conventions have changed over time and how they differ by nationality. Second, we build a probabilistic first name to gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in a two label and three label setting and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies' outcomes and find that our mapping produces high precision results. We validate that the additional information of nationality and year of birth improve the recall scores of name to gender mappings. Therefore, it constitutes an efficient process to improve data quality of organizations' records, whenever the attribute gender is missing or unreliable.},
keywords={Organizations;Patents;Databases;Systematics;Pragmatics;data quality improvement;hot deck imputation;record completion;gender name mapping;patenting},
doi={10.1109/BigData.2017.8258223},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6974862,
author={Sänger, Johannes and Richthammer, Christian and Hassan, Sabri and Pernul, Günther},
booktitle={2014 25th International Workshop on Database and Expert Systems Applications},
title={Trust and Big Data: A Roadmap for Research},
year={2014},
volume={},
number={},
pages={278-282},
abstract={We are currently living in the age of Big Data coming along with the challenge to grasp the golden opportunities at hand. This mixed blessing also dominates the relation between Big Data and trust. On the one side, large amounts of trust-related data can be utilized to establish innovative data-driven approaches for reputation-based trust management. On the other side, this is intrinsically tied to the trust we can put in the origins and quality of the underlying data. In this paper, we address both sides of trust and Big Data by structuring the problem domain and presenting current research directions and inter-dependencies. Based on this, we define focal issues which serve as future research directions for the track to our vision of Next Generation Online Trust within the FORSEC project.},
keywords={Big data;Security;Cloud computing;Context;Computer science;Data mining;Conferences;Big Data;trust;reputation},
doi={10.1109/DEXA.2014.63},
ISSN={2378-3915},
month={Sep.},}
@INPROCEEDINGS{8328387,
author={Aseeri, Ahmad O. and Zhuang, Yu and Alkatheiri, Mohammed Saeed},
booktitle={2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)},
title={A Memory Capacity-Aware Algorithm for Fast Clustering of Disk-Resident Big Datasets},
year={2017},
volume={},
number={},
pages={194-201},
abstract={Clustering is one of the most commonly used data mining techniques. The K-means and its variants are popular clustering methods. The simplistic Lloyd K-means algorithm, with randomly chosen initial cluster centers, suffers from poor clustering quality and high iteration numbers, especially unsuitable for clustering large datasets. Successful methods that choose a good set of initial cluster centers include the algorithm of Bradley and Fayyad using sampled data subsets, and the bisecting K-means algorithm of Steinbach, Karypis, and Kumar. Recently, it was discovered that iterations in the two-means algorithm used in bisecting K-means to bisect a subset can be limited to small numbers while still maintaining the final clustering quality for the bisecting K-means algorithm. In this paper, for datasets larger than memory capacity, we develop an iteration limiting strategy for bisecting K-means which adaptively determines the number of iterations for each call of the two-means bisecting subroutine based on the memory capacity of a computer and the size of the data subset to be partitioned. The strategy has been incorporated into the bisecting K-means algorithm, applied to the large challenge-response datasets of Physical Unclonable Functions that the authors are investigating, with comparison with the sampled-subsets algorithm of Bradley and Fayyad. Testing results show high computing efficiency for the bisecting K-means algorithm incorporated with the iteration limiting strategy while exhibiting almost identical clustering quality.},
keywords={Clustering algorithms;Partitioning algorithms;Standards;Limiting;Heuristic algorithms;Linear programming;Big Data;Clustering;bisecting K-means;limited K-mean iterations;Big data},
doi={10.1109/DASC-PICom-DataCom-CyberSciTec.2017.44},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6544914,
author={Dong, Xin Luna and Srivastava, Divesh},
booktitle={2013 IEEE 29th International Conference on Data Engineering (ICDE)},
title={Big data integration},
year={2013},
volume={},
number={},
pages={1245-1248},
abstract={The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.},
keywords={Data integration;Couplings;Big data;Seminars;Databases;Educational institutions;Joining processes},
doi={10.1109/ICDE.2013.6544914},
ISSN={1063-6382},
month={April},}
@INPROCEEDINGS{6903700,
author={Mohamed, Nader and Al-Jaroodi, Jameela},
booktitle={2014 International Conference on High Performance Computing & Simulation (HPCS)},
title={Real-time big data analytics: Applications and challenges},
year={2014},
volume={},
number={},
pages={305-310},
abstract={Timely analytics over big data is a key factor for success in many business and service domains. Some examples of these domains include finance, transportation, energy, security, military, and emergency response. Several big data applications in these domains rely on fast and timely analytics based on available data to make quality decisions. This paper surveys real-time big data analytics applications and their technical challenges.},
keywords={Big data;Real-time systems;Vehicles;Roads;Decision making;Real-Time processing;Big data;Data analytics},
doi={10.1109/HPCSim.2014.6903700},
ISSN={},
month={July},}
@INPROCEEDINGS{7363860,
author={Diaz-Aviles, Ernesto and Pinelli, Fabio and Lynch, Karol and Nabi, Zubair and Gkoufas, Yiannis and Bouillet, Eric and Calabrese, Francesco and Coughlan, Eoin and Holland, Peter and Salzwedel, Jason},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Towards real-time customer experience prediction for telecommunication operators},
year={2015},
volume={},
number={},
pages={1063-1072},
abstract={Telecommunications operators (telcos) traditional sources of income, voice and SMS, are shrinking due to customers using over-the-top (OTT) applications such as WhatsApp or Viber. In this challenging environment it is critical for telcos to maintain or grow their market share, by providing users with as good an experience as possible on their network. But the task of extracting customer insights from the vast amounts of data collected by telcos is growing in complexity and scale everey day. How can we measure and predict the quality of a user's experience on a telco network in real-time? That is the problem that we address in this paper. We present an approach to capture, in (near) real-time, the mobile customer experience in order to assess which conditions lead the user to place a call to a telco's customer care center. To this end, we follow a supervised learning approach for prediction and train our Restricted Random Forest model using, as a proxy for bad experience, the observed customer transactions in the telco data feed before the user places a call to a customer care center. We evaluate our approach using a rich dataset provided by a major African telecommunication's company and a novel big data architecture for both the training and scoring of predictive models. Our empirical study shows our solution to be effective at predicting user experience by inferring if a customer will place a call based on his current context. These promising results open new possibilities for improved customer service, which will help telcos to reduce churn rates and improve customer experience, both factors that directly impact their revenue growth.},
keywords={Mobile communication;Real-time systems;Feeds;Predictive models;Context;Big data;Telecom operators;Customer Care;Big Data;Predictive Analytics},
doi={10.1109/BigData.2015.7363860},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7588930,
author={Rao, Divya and Ng, Wee Keong},
booktitle={2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)},
title={Information Pricing: A Utility Based Pricing Mechanism},
year={2016},
volume={},
number={},
pages={754-760},
abstract={Big data is the biggest asset today, for organizations to amass and leverage to improve on services and profits. Knowing this, data brokers have excelled in the task of collecting information about internet users from a variety of sources, both online and offline, and then sell this information to interested parties. This robs the users of the monetizing potential of their information. In this paper, we have put forth our idea for users to monetize their information and at the same time, preserve their privacy by obfuscating their information. Once this is done, the utility of the information is obtained by comparing the cumulative distribution functions (CDF) of the data before and after obfuscation. This would allow the buyers in the information market to obtain a fair estimate of the quality of information that they would be purchasing from the users. In our opinion, this is a fair and balanced method that caters to the needs to all the stakeholders involved in the information market scenario.},
keywords={Data privacy;Privacy;Pricing;Organizations;Internet;Big data;big data;information pricing;information demand;information value},
doi={10.1109/DASC-PICom-DataCom-CyberSciTec.2016.132},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8969830,
author={Dahlan, Iqbal Ahmad and Hamami, Faqih and Supangkat, Suhono Harso and Hidayat, Fadhil},
booktitle={2019 International Conference on ICT for Smart Society (ICISS)},
title={Big Data Implementation of Smart Rapid Transit using CCTV Surveillance},
year={2019},
volume={7},
number={},
pages={1-5},
abstract={This paper presents the implementation on smart system for rapid transit using CCTV surveillance. Researchers proposed deep learning algorithms to detect objects with Convolutional Neural Network (CNN) and monitoring passengers' behavior like flow analytics, avoiding dangerous areas, and preventing intruder visitor[1][2]. Research of this paper also implemented in Railway Station in Bandung with multiple CCTV sources. The system aims to make station better and able to improve quality of service in many scope areas (safe, secure and convenient)[3]. Objective of this research is to develop smart surveillance with CCTV in smart station. The system consists of deep learning algorithm and big data technologies such as Hadoop, Apache Kafka and Apache Spark.},
keywords={Railways;Deep Learning;Big Data;Convolutional Neural Network},
doi={10.1109/ICISS48059.2019.8969830},
ISSN={2640-0545},
month={Nov},}
@INPROCEEDINGS{8323652,
author={Sudsee, Bhuridech and Kaewkasi, Chanwit Kaewkasi},
booktitle={2018 20th International Conference on Advanced Communication Technology (ICACT)},
title={A productivity improvement of distributed software testing using checkpoint},
year={2018},
volume={},
number={},
pages={78-84},
abstract={The advancement of storage technologies and the fast-growing number of generated data have made the world moved into the Big Data era. In this past, we had many data mining tools but they are inadequate to process Data-Intensive Scalable Computing workloads. The Apache Spark framework is a popular tool designed for Big Data processing. It leverages in-memory processing techniques that make Spark up to 100 times faster than Hadoop. Testing this kind of Big Data program is time consuming. Unfortunately, developers lack a proper testing framework, which cloud help assure quality of their data-intensive processing programs, while saving development time. We propose Distributed Test Checkpointing (DTC) for Apache Spark, DTC applies unit testing to the Big Data software development life cycle and reduce time spent for each testing loop with checkpoint. From the experimental results, we found that in the subsequence rounds of unit testing, DTC dramatically speed the testing time up to 450-500% faster. In case of storage, DTC can cut unnecessary data off and make the storage 19.7 times saver than the original checkpoint of Spark.},
keywords={Sparks;Big Data;Checkpointing;Software testing;Tools;Debugging;Distributed Checkpointing;Apache Spark;Big Data Testing;Software Testing},
doi={10.23919/ICACT.2018.8323652},
ISSN={},
month={Feb},}
@INPROCEEDINGS{9322544,
author={Al Jawarneh, Isam Mashhour and Bellavista, Paolo and Corradi, Antonio and Foschini, Luca and Montanari, Rebecca},
booktitle={GLOBECOM 2020 - 2020 IEEE Global Communications Conference},
title={Locality-Preserving Spatial Partitioning for Geo Big Data Analytics in Main Memory Frameworks},
year={2020},
volume={},
number={},
pages={1-6},
abstract={The easily reachable IoT edge devices have caused the accumulation of vast amounts of geo-referenced data traces that can help in performing deep insightful analytics. Geospatial data in real geometries are normally clumped into batches and has strong autocorrelation properties which can be exploited in discovering interesting insights. Current plain Cloud computing frameworks are not attuned to the shape of data. Most importantly, data splitting is an important precursor in data parallelization mechanisms. Current systems mostly focus on general data workloads, thus are giving attention mostly to load balancing while splitting the data to Cloud computing resources. However, many benefits can be reaped by being attuned to the spatial characteristics while distributing the data, thus striking a plausible balance between load balancing and spatial data locality preservation normally leads to achieving better time-based QoS goals, which then leads to an optimized provisioning of Cloud computing resources. In this paper, we have designed a spatial batch processing engine that comprises a custom spatial data locality aware partitioning method for disseminating spatial data loads in Cloud computing clusters. We have also extended a state-of-art benchmark density-based clustering method that is known as DBSCAN-MR and implemented a standard compliant prototype on top of a best-in-breed de facto Cloud-based main memory processing framework, Apache Spark. Our results show that our partitioning method with the associated spatial query optimizers can achieve gains that significantly outperform baselines.},
keywords={Spatial databases;Cloud computing;Distributed databases;Quality of service;Geometry;Sparks;Partitioning algorithms;spatial join;Spark;DBSCAN;data partitioning;smart city},
doi={10.1109/GLOBECOM42002.2020.9322544},
ISSN={2576-6813},
month={Dec},}
@INPROCEEDINGS{9298131,
author={Liu, Jingang and Xia, Chunhe and Yan, Haihua and Sun, Jie},
booktitle={2020 11th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)},
title={A Feasible Chinese Text Data Preprocessing Strategy},
year={2020},
volume={},
number={},
pages={0234-0239},
abstract={With the rapid rise of artificial intelligence technologies such as machine learning and the rapid development of the big data industry, more and more attention is paid to the use of data itself, especially the Chinese text data, which is more complex in expression and richer in the information. It is a necessary step to process the raw Chinese text data before it is used for specific application tasks. However, the current strategies for processing data are generally to deal with data in different fields and specific application tasks. In this paper, to further improve the quality of Chinese data processing and give play to the application value of Chinese data, we propose a general and feasible Chinese text preprocessing strategy, named the multi-level data preprocessing strategy (MLDPS). This strategy uses four effective links to process raw Chinese text data systematically. We believe that the proposed MLDPS has relatively strong practical significance, and provides a better idea for preprocessing Chinese text data.},
keywords={Cleaning;Data preprocessing;Task analysis;Security;Data privacy;Labeling;Data mining;big data;preprocessing;data cleaning;strategy},
doi={10.1109/UEMCON51285.2020.9298131},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7870557,
author={Mavroeidakos, Theodoros and Tsolis, Nikolaos and Vergados, Dimitrios D.},
booktitle={2016 3rd Smart Cloud Networks & Systems (SCNS)},
title={Centralized management of medical big data in Intensive Care Unit: A security analysis},
year={2016},
volume={},
number={},
pages={1-5},
abstract={The digital evolution of computing environments has affected the collection and management of medical data. Since there is a constantly growing number of remote sensors and medical systems, which augment the volume of medical data, a new challenge about the management of these big data is created. As the legacy systems in effect do not have the ability to manage this rate of data efficiently, it is of great importance to address this challenge following a different approach. In this particular unit of hospitals, ICU, the usage of complex medical systems in order to provide adequate quality of healthcare services and the velocity of the decision making process, which is performed by intensivists, are critical. To this end, an approach to this problem constitutes the Centralized Management (CM) of medical data in Intensive Care Unit's(ICU) environment. The autoscalling computing environment, which will be orchestrated in the context of operations of the CM will ease tasks, such as the aggregation of medical data by different sources, the performance of analytic functions and it will enhance the intensivists' cognition. Moreover, this type of management leads to security and privacy challenges that should be addressed in order to assemble a highly trusted environment both in technical and functional level. As a result, prior to the adaptation of the CM in ICU, specific security, as well as privacy guidelines, should be determined.},
keywords={Medical services;Big data;Biomedical monitoring;Security;Blood;Medical diagnostic imaging;Monitoring;Centralized management;medical big data;intensive care unit;security challenges;security guidelines},
doi={10.1109/SCNS.2016.7870557},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9146074,
author={Gudivada, Akhil and Tabrizi, Nasseh},
booktitle={2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)},
title={Developing Concept Enriched Models for Processing Big Data Within the Medical Domain},
year={2019},
volume={},
number={},
pages={222-229},
abstract={As more and more domains are incorporating cognitive computing tools to develop models to process and understand data in a cohesive, yet effective manner, the medical domain is also seeking advancements aided by artificial intelligence. While the amount of research available to any individual increases regularly, the ability to keep up with new information becomes a challenge due to the sheer quantity of information. The use of artificial intelligence to help process large amounts of information can overcome those barriers. However, progress in this field is hindered by several challenges including: incomplete medical data sets, the confidential nature of data as it holds private information of individuals, the complexity and nuances of natural language (within medicine), and even the unwillingness of health-care providers to adopt newer techniques. Though the data may be specialized, the models and techniques designed and discussed in this paper can help provide a framework, or starting point for those interested in effectively developing, maintaining, and using these models to help improve the quality of health-care. The purpose of this paper is to serve as resource which can be used to start developing similar models and put them to use in everyday practice in the medical domain.},
keywords={Cognitive systems;Cancer;Data models;Tools;Medical diagnostic imaging;Computational modeling;artificial intelligence;Cognitive computing;big data;medical data;information retrieval},
doi={10.1109/ICCICC46617.2019.9146074},
ISSN={},
month={July},}
@INPROCEEDINGS{9378466,
author={Huang, Chau-Lin Charly and Munasinghe, Thilanka},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Exploring Various Applicable Techniques to Detect Smoke on the Satellite Images},
year={2020},
volume={},
number={},
pages={5703-5705},
abstract={Every year, the wildfires ravage broad areas of natural forest and nearby regions, causing substantial financial and life losses and deteriorating the air quality. More air hazards are emitted to the atmosphere reaching as high as the stratosphere propagating through the air currents. With the aggravation of climate change, wildfires of either human or natural cause could become more ferocious and devastating. A feasible solution is to detect the wildfire and respond early before the fire spread becomes irreversible. Satellite imagery serves as a cost-effective means to update near-real-time holistic landscape views of land and sea over extended periods. Such an advantage makes early fire detection and warning even in remote areas possible. The rendered images provided by the satellites’ various instruments incorporate various channels to provide real and artificial colors to reveal landscape details imperceptible to the naked eyes. This imagery dataset discussed in this paper derives from NASA’s Aqua and Terra satellites and make available at the NASA-IMPACT data share repository [1]. The dataset totals 704 images of cropped frames and their labeled images taken during both satellites’ extensive flyby observations. The images also contain spatial-temporal information serving as relevant metadata for analysis. This paper provides a survey of the recent advances in neural network-based object detection techniques followed by machine learning and deep learning-based methods to detect and localize smoke. A comprehensive elaboration of the datasets follows the method overview.},
keywords={Satellites;Fires;Terrestrial atmosphere;Object detection;Machine learning;Big Data;Metadata;Climate change;smoke detection;wildfire;scene classification;object localization;CNN},
doi={10.1109/BigData50022.2020.9378466},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7979870,
author={Tan, Yi and Ou, Weihua and Long, Fei and Wang, Pengpeng and Xue, Yunhao},
booktitle={2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
title={Multi-view Clustering via Co-regularized Nonnegative Matrix Factorization with Correlation Constraint},
year={2016},
volume={},
number={},
pages={1-6},
abstract={With the increasing availability of multi-view nonnegative data in practical applications, multi-view learning based on nonnegative matrix factorization (NMF) has attracted more and more attentions. However, previous works are either difficult to generate meaningful clustering results in terms of views with heterogeneous quality, or sensitive to noise. To address these problems, we propose a co-regularized nonnegative matrix factorization method with correlation constraint (CO-NMFCC) for multi-view clustering, which jointly exploits both consistent and complementary information across multiple views. Different from previous works, we aim at integrating information from multiple views efficiently and making it more robust to the presence of noisy views. More specifically, correlation constraint is imposed on the low-dimensional space to learn a common representation shared by multiple views. Meanwhile, we exploit the complementary information of multiple views through the coregularization to accommodate the imbalance of the quality of views. In addition, experiments on two real datasets demonstrate that CO-NMFCC is an effective and promising algorithm for practical applications.},
keywords={Correlation;Feature extraction;Linear programming;Image color analysis;Big Data;Shape;Kernel;multi-view clustering;nonnegative matrix factorization;correlation constraint;co-regularization},
doi={10.1109/CCBD.2016.012},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9185292,
author={Lee, Janghwan and Xiong, Wei and Jang, Wonhyouk},
booktitle={2020 31st Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC)},
title={Trace Data Analytics with Knowledge Distillation : DM: Big Data Management and Mining},
year={2020},
volume={},
number={},
pages={1-8},
abstract={In this paper, we propose the “trace data analytics” for classifying fault conditions from multivariate time series sensor signals using well-known deep CNN models. In our approach, multiple sensor signals are converted into two dimensional representations using the proposed conversion methods to optimize the classification performance. Many studies on the prediction of manufacturing results using sensor signals have been conducted in the field of fault detection and classification for display and semiconductor manufacturing processes. It is challenging to apply machine learning to real-life manufacturing problems due to practical limitations, class imbalance and data insufficiency, which also make it difficult to produce a generalized model. To overcome these challenges, we propose using omni-supervised learning but with a new approach to knowledge distillation that ensembles predictions from multiple instantiations of a CNN model of synthetically generated data samples from a deep generative model. Our experiment results show that the fault classification accuracy improves substantially by applying trace data analytics to manufacturing data from display fabrication lines. The results also show that the quality of trained CNN models using the proposed knowledge distillation is maintained steadily and stably.},
keywords={Data models;Predictive models;Manufacturing;Task analysis;Machine learning;Training;Data analysis;Fault classification;Semi-supervised learning;Variational autoencoder;display manufacturing;knowledge distillation},
doi={10.1109/ASMC49169.2020.9185292},
ISSN={2376-6697},
month={Aug},}
@ARTICLE{8535091,
author={Zhou, Zhenyun and Yu, Houjian and Xu, Chen and Chang, Zheng and Mumtaz, Shahid and Rodriguez, Jonathan},
journal={IEEE Communications Magazine},
title={BEGIN: Big Data Enabled Energy-Efficient Vehicular Edge Computing},
year={2018},
volume={56},
number={12},
pages={82-89},
abstract={Vehicular edge computing is essential to support future emerging multimedia-rich and delay-sensitive applications in vehicular networks. However, the massive deployment of edge computing infrastructures induces new problems including energy consumption and carbon pollution. This motivates us to develop BEGIN (Big data enabled EnerGy-efficient vehIcular edge computiNg), a programmable, scalable, and flexible framework for integrating big data analytics with vehicular edge computing. In this article, we first present a comprehensive literature review. Then the overall design principle of BEGIN is described with an emphasis on computing domain and data domain convergence. In the next section, we classify big data in BEGIN into four categories and then describe their features and potential values. Four typical application scenarios in BEGIN including node deployment, resource adaptation and workload allocation, energy management, and proactive caching and pushing, are provided to illustrate how to achieve energy-efficient vehicular edge computing by using big data. A case study is presented to demonstrate the feasibility of BEGIN and the superiority of big data in energy efficiency improvement. Finally, we conclude this work and outline future research open issues.},
keywords={Big Data;Edge computing;Energy efficiency;Cloud computing;Resource management;Energy consumption;Quality of service},
doi={10.1109/MCOM.2018.1700910},
ISSN={1558-1896},
month={December},}
@INPROCEEDINGS{7590400,
author={Zhu, Ming and Liu, Xiao-Yang and Qiu, Meikang and Shen, Ruimin and Shu, Wei and Wu, Min-You},
booktitle={2016 IEEE/ACM 24th International Symposium on Quality of Service (IWQoS)},
title={Traffic big data based path planning strategy in public vehicle systems},
year={2016},
volume={},
number={},
pages={1-2},
abstract={Public vehicle (PV) systems will be efficient traffic-management platforms in future smart cities, where PVs provide ridesharing trips with balanced QoS (quality of service). PV systems differ from traditional ridesharing due to that the paths and scheduling tasks are calculated by a server according to passengers' requests, and all PVs corporate with each other to achieve higher transportation efficiency. Path planning is the primary problem. The current path planning strategies become inefficient especially for traffic big data in cities of large population and urban area. To ensure real-time scheduling, we propose one efficient path planning strategy with balanced QoS (e.g., waiting time, detour) by restricting search area for each PV, so that a large number of computation is saved. Simulation results based on the Shanghai (China) urban road network show that, the computation can be reduced by 34% compared with the exhaustive search method since many requests violating QoS are excluded.},
keywords={Quality of service;Vehicles;Urban areas;Big data;Path planning;Servers;Scheduling},
doi={10.1109/IWQoS.2016.7590400},
ISSN={},
month={June},}
@INPROCEEDINGS{7877179,
author={Halgamuge, Saman K.},
booktitle={2016 7th International Conference on Intelligent Systems, Modelling and Simulation (ISMS)},
title={Deep Near Unsupervised Learning for Data Analysis in Metabolomics, Drug-Drug Interaction Discovery and Human Gait Recognition},
year={2016},
volume={},
number={},
pages={5-6},
abstract={We have been working on the application of Machining Learning in Metabolomics, Drug-Drug Interaction Discovery and Human Gait Recognition [1-5], profiling large data sets. Extraction of vital information about 1) plant metabolomics that can improve the environment and food quality, 2) in vitro neuronal network behavior patterns for various drugs that can be used to characterize drugs for brain deceases and 3) human gait patterns that can reveal various diseases were among these applications. We have demonstrated with considerable success in using unsupervised clustering techniques to analyze genetic and metabolomic data. This includes analysis of drought resistance in wheat [4] and microbial metagenomes [5]. We introduced two methods: Near Unsupervised Learning (NUL) and Sub-sample Error Graphs (SEGs) [5] to analyze large amount of data. Self Organizing Map (SOM), which is one of the widely used Unsupervised Neural Networks, has been used as a data-mining tool due to its ability to map high dimensional data into a two dimensional feature map, which is expected to be topology preserving allowing users to visually identify clusters by their topological relationships in terms of their proximity on the map. The Growing Self Organizing Maps (GSOM) further allows the map size to be determined by the algorithm, which relies upon a user set parameter called Spread Factor (SF) [6]. The wide availability of GPUs for affordable prices allows faster comparison of various SOMs with different maps sizes taking away some of the advantages GSOM claimed. Further development of GSOM into a Dynamic SOM Tree exploits the possibility of varying SF to obtain multiple GSOMs from a small number of compact clusters to a large number of sparse clusters [7]. NUL methods can be applied on GSOM using a small number of labels that should be available for every class. This is however not realistic in some applications where the number of classes cannot be explicitly known. We propose a new method call Deep Near Unsupervised Learning (D-NUL), where Dynamic SOM tree is used instead of GSOM and the number of classes are not assumed to be known. The implementation of Dynamic SOM tree methods with varying SF on GPUs will make the computation with D-NUL possible for many problems in big data analytics.},
keywords={Metabolomics;Unsupervised learning;Biomedical engineering;Big Data;Bioinformatics;Intelligent systems;Biological system modeling},
doi={10.1109/ISMS.2016.77},
ISSN={2166-0670},
month={Jan},}
@INPROCEEDINGS{6691753,
author={Nambiar, Raghunath and Bhardwaj, Ruchie and Sethi, Adhiraaj and Vargheese, Rajesh},
booktitle={2013 IEEE International Conference on Big Data},
title={A look at challenges and opportunities of Big Data analytics in healthcare},
year={2013},
volume={},
number={},
pages={17-22},
abstract={Big Data analytics can revolutionize the healthcare industry. It can improve operational efficiencies, help predict and plan responses to disease epidemics, improve the quality of monitoring of clinical trials, and optimize healthcare spending at all levels from patients to hospital systems to governments. This paper provides an overview of Big Data, applicability of it in healthcare, some of the work in progress and a future outlook on how Big Data analytics can improve overall quality in healthcare systems.},
keywords={Information management;Data handling;Data storage systems;Industries;Companies;Diseases;Big Data;Healthcare},
doi={10.1109/BigData.2013.6691753},
ISSN={},
month={Oct},}
@ARTICLE{7979537,
author={Huang, Chao and Wang, Dong and Tao, Jun and Mann, Brian},
journal={IEEE Transactions on Big Data},
title={On Physical-Social-Aware Localness Inference by Exploring Big Data from Location-Based Services},
year={2020},
volume={6},
number={4},
pages={679-690},
abstract={A user's localness (i.e., whether a user is a local resident in a city or not) and a venue's local attractiveness (i.e., the likelihood of a venue to attract local people) are important information for many location-based applications related with Cyber-Physical Systems (CPS), such as participatory sensing, urban planning, traffic control and localized travel recommendations. Previous effort has been devoted to geo-locating users in a city using supervised learning approaches, which depend on the availability of high quality training datasets. However, it is difficult to obtain such training datasets in the real-world CPS applications due to the issue of privacy. In this work, we develop an unsupervised approach, called a Physical-Social-Aware Inference (PSAI) scheme, to jointly infer a user's localness and a venue's local attractiveness by exploring both the physical and social information embedded in the location-based social networks (LBSN). We further implement a parallel PSAI framework on the platform of a Graphic Processing Unit (GPU) to enhance its ability to process large-scale data. Our extensive experiments on the real-world LBSN datasets demonstrate the effectiveness and efficiency of the PSAI scheme compared to the state-of-the-art baselines.},
keywords={Urban areas;Cyber-physical systems;Graphics processing units;Big Data;Twitter;Sensors;Location awareness;Big Data;cyber-physical systems;physical-social constraints;localness inference;location-based social networks},
doi={10.1109/TBDATA.2017.2726551},
ISSN={2332-7790},
month={Dec},}
@ARTICLE{8788596,
author={Wu, Di and Wang, Hao and Mohammed, Hadi and Seidu, Razak},
journal={IEEE Transactions on Sustainable Computing},
title={Quality Risk Analysis for Sustainable Smart Water Supply Using Data Perception},
year={2020},
volume={5},
number={3},
pages={377-388},
abstract={Constructing Sustainable Smart Water Supply systems are facing serious challenges all around the world with the fast expansion of modern cities. Water quality is influencing our life ubiquitously and prioritizing all the urban management. Traditional urban water quality control mostly focused on routine tests of quality indicators, which include physical, chemical, and biological groups. However, the inevitable delay for biological indicators has increased the health risk and leads to accidents such as massive infections in many big cities. In this paper, we first analyze the problem, technical challenges, and research questions. Then, we provide a possible solution by building a risk analysis framework for the urban water supply system. It takes indicator data we collected from industrial processes to perceive water quality changes, and further for risk detection. In order to provide explainable results, we propose an Adaptive Frequency Analysis (Adp-FA) method to resolve the data using indicators' frequency domain information for their inner relationships and individual prediction. We also investigate the scalability properties of this method from indicator, geography, and time domains. For the application, we select industrial quality data sets collected from a Norwegian project in four different urban water supply systems, as Oslo, Bergen, Strømmen, and Ålesund. We employ the proposed method to test spectrogram, prediction accuracy, and time consumption, comparing with classical Artificial Neural Network and Random Forest methods. The results show our method better perform in most of the aspects. It is feasible to support industrial water quality risk early warnings and further decision support.},
keywords={Water quality;Water resources;Lakes;Water pollution;Biology;Urban areas;Sensors;Sustainable water supply;water quality control;data perception;risk evaluation;frequency analysis;scalability},
doi={10.1109/TSUSC.2019.2929953},
ISSN={2377-3782},
month={July},}
@INPROCEEDINGS{9002940,
author={Harvey, Julie and Kumar, Sathish},
booktitle={2019 IEEE Symposium Series on Computational Intelligence (SSCI)},
title={Data Science for K-12 Education},
year={2019},
volume={},
number={},
pages={2482-2488},
abstract={Data science is a field that can be used in a variety of settings. Education is one of the fields that is expanding its use of data science to improve the quality of education. The United States denotes primary and secondary school as grades kindergarten (K) through 12th grade. This is representative of education prior to college/university level. Data science in K-12 education is evaluated and important to the field of education because educators, administrators, and stakeholders are always looking for ways to close the achievement gap and increase performance of all students. Student performance evaluation using data science is crucial to closing this gap. Data mining is used in the evaluation and analysis of student performance, educational programs and educational instruction. It is also used to create prediction models for future student success. A K-12 education dataset will be used to evaluate student performance. This paper will explore and display student performance based on a variety of factors and data. Data science in K-12 education and its impact on student performance and educator use of this data is discussed. We have also performed review of existing work in the data analytics for K-12 education along with their limitations.},
keywords={Education;Big Data;Data mining;Data science;Predictive models;Companies;education;big data;north carolina;school performance grade;curriculum;data mining;data driven instruction},
doi={10.1109/SSCI44817.2019.9002940},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7158508,
author={Gorton, Ian and Klein, John and Nurgaliev, Albert},
booktitle={2015 12th Working IEEE/IFIP Conference on Software Architecture},
title={Architecture Knowledge for Evaluating Scalable Databases},
year={2015},
volume={},
number={},
pages={95-104},
abstract={Designing massively scalable, highly available big data systems is an immense challenge for software architects. Big data applications require distributed systems design principles to create scalable solutions, and the selection and adoption of open source and commercial technologies that can provide the required quality attributes. In big data systems, the data management layer presents unique engineering problems, arising from the proliferation of new data models and distributed technologies for building scalable, available data stores. Architects must consequently compare candidate database technology features and select platforms that can satisfy application quality and cost requirements. In practice, the inevitable absence of up-to-date, reliable technology evaluation sources makes this comparison exercise a highly exploratory, unstructured task. To address these problems, we have created a detailed feature taxonomy that enables rigorous comparison and evaluation of distributed database platforms. The taxonomy captures the major architectural characteristics of distributed databases, including data model and query capabilities. In this paper we present the major elements of the feature taxonomy, and demonstrate its utility by populating the taxonomy for nine different database technologies. We also briefly describe QuABaseBD, a knowledge base that we have built to support the population and querying of database features by software architects. QuABaseBD links the taxonomy to general quality attribute scenarios and design tactics for big data systems. This creates a unique, dynamic knowledge resource for architects building big data systems.},
keywords={Distributed databases;Taxonomy;Big data;Scalability;Computer architecture;Data models;scalable software systems;big data;software architecture knowledge base;feature taxonomy},
doi={10.1109/WICSA.2015.26},
ISSN={},
month={May},}
@ARTICLE{8469815,
author={Mi, Jun and Wang, Kun and Li, Peng and Guo, Song and Sun, Yanfei},
journal={IEEE Communications Magazine},
title={Software-Defined Green 5G System for Big Data},
year={2018},
volume={56},
number={11},
pages={116-123},
abstract={The 5G system has been recognized as the most promising technology to provide high-quality network services. As a huge number of networking and computing equipments that generate big data are integrated into the 5G system, energy efficiency becomes the major challenge in building a green 5G system. In this article, we propose a software-defined green 5G system for big data, which consists of three planes: the control plane, the data plane and the energy plane. The data plane contains networking and computing equipments, which can be powered by both traditional grid and renewable energy sources in the energy plane. The control plane monitors the system status and configures the corresponding equipments to achieve energy efficiency and quality-of-service. Furthermore, to reduce the overhead of this software- defined architecture, we investigate a FRS to eliminate redundant system monitoring information. To integrate features in software-defined architecture, we propose an AIFS to mine latent rules among features. Simulation results indicate that our proposals achieve higher efficiency in the green 5G system.},
keywords={Monitoring;5G mobile communication;Green products;Big Data;Renewable energy sources;Quality of service;Computer architecture},
doi={10.1109/MCOM.2017.1700048},
ISSN={1558-1896},
month={November},}
@INPROCEEDINGS{7592729,
author={Sotsenko, Alisa and Jansen, Marc and Milrad, Marcelo and Rana, Juwel},
booktitle={2016 IEEE 4th International Conference on Future Internet of Things and Cloud Workshops (FiCloudW)},
title={Using a Rich Context Model for Real-Time Big Data Analytics in Twitter},
year={2016},
volume={},
number={},
pages={228-233},
abstract={In this paper we present an approach for contextual big data analytics in social networks, particularly in Twitter. The combination of a Rich Context Model (RCM) with machine learning is used in order to improve the quality of the data mining techniques. We propose the algorithm and architecture of our approach for real-time contextual analysis of tweets. The proposed approach can be used to enrich and empower the predictive analytics or to provide relevant context-aware recommendations.},
keywords={Context;Twitter;Big data;Real-time systems;Sparks;Context modeling;Measurement;rich context model;big data;context analytics;twitter;k-means clustering},
doi={10.1109/W-FiCloud.2016.55},
ISSN={},
month={Aug},}
@ARTICLE{7534839,
author={Hossain, M. Shamim and Moniruzzaman, Mohd and Muhammad, Ghulam and Ghoneim, Ahmed and Alamri, Atif},
journal={IEEE Transactions on Services Computing},
title={Big Data-Driven Service Composition Using Parallel Clustered Particle Swarm Optimization in Mobile Environment},
year={2016},
volume={9},
number={5},
pages={806-817},
abstract={The proliferation of mobile computing and smartphone technologies has resulted in an increasing number and range of services from myriad service providers. These mobile service providers support numerous emerging services with differing quality metrics but similar functionality. Facilitating an automated service workflow requires fast selection and composition of services from the services pool. The mobile environment is ambient and dynamic in nature, requiring more efficient techniques to deliver the required service composition promptly to users. Selecting the optimum required services in a minimal time from the numerous sets of dynamic services is a challenge. This work addresses the challenge as an optimization problem. An algorithm is developed by combining particle swarm optimization and k-means clustering. It runs in parallel using MapReduce in the Hadoop platform. By using parallel processing, the optimum service composition is obtained in significantly less time than alternative algorithms. This is essential for handling large amounts of heterogeneous data and services from various sources in the mobile environment. The suitability of this proposed approach for big data-driven service composition is validated through modeling and simulation.},
keywords={Mobile communication;Quality of service;Big data;Particle swarm optimization;Optimization;Clustering algorithms;Organizations;Big Data;clustered particle swarm optimization;service composition;mobile environment;big data management},
doi={10.1109/TSC.2016.2598335},
ISSN={1939-1374},
month={Sep.},}
@INPROCEEDINGS{7996546,
author={Gu, Liqiu and Wang, Kun and Liu, Xiulong and Guo, Song and Liu, Bo},
booktitle={2017 IEEE International Conference on Communications (ICC)},
title={A reliable task assignment strategy for spatial crowdsourcing in big data environment},
year={2017},
volume={},
number={},
pages={1-6},
abstract={With the ubiquitous deployment of the mobile devices with increasingly better communication and computation capabilities, an emerging model called spatial crowdsourcing is proposed to solve the problem of unstructured big data by publishing location-based tasks to participating workers. However, massive spatial data generated by spatial crowdsourcing entails a critical challenge that the system has to guarantee quality control of crowdsourcing. This paper first studies a practical problem of task assignment, namely reliability aware spatial crowdsourcing (RA-SC), which takes the constrained tasks and numerous dynamic workers into consideration. Specifically, the worker confidence is introduced to reflect the completion reliability of the assigned task. Our RA-SC problem is to perform task assignments such that the reliability under budget constraints is maximized. Then, we reveal the typical property of the proposed problem, and design an effective strategy to achieve a high reliability of the task assignment. Besides the theoretical analysis, extensive experimental results also demonstrate that the proposed strategy is stable and effective for spatial crowdsourcing.},
keywords={Crowdsourcing;Big Data;Reliability theory;Sensors;Computational modeling;Measurement;Big data;crowdsourcing;task assignment},
doi={10.1109/ICC.2017.7996546},
ISSN={1938-1883},
month={May},}
@ARTICLE{8316780,
author={Wang, Tian and Bhuiyan, Md Zakirul Alam and Wang, Guojun and Rahman, Md. Arafatur and Wu, Jie and Cao, Jiannong},
journal={IEEE Communications Magazine},
title={Big Data Reduction for a Smart City’s Critical Infrastructural Health Monitoring},
year={2018},
volume={56},
number={3},
pages={128-133},
abstract={Critical infrastructure monitoring is one of the most important applications of a smart city. The objective is to monitor the integrity of the structures (e.g., buildings, bridges) and detect and pinpoint the locations of possible events (e.g., damages, cracks). Regarding today's complex structures, collecting data using wireless sensor data over extensive vertical lengths creates enormous challenges. With a direct BS deployment, a big amount of data will accumulate to be relayed to the BS. As a result, traditional models and schemes developed for health monitoring are largely challenged by low-cost, quality-guaranteed, and real-time event monitoring. In this article, we propose BigReduce, a cloud based health monitoring application with an IoT framework that could cover most of the key infrastructures of a smart city under an umbrella and provide event monitoring. To reduce the burden of big data processing at the BS and enhance the quality of event detection, we integrate real-time data processing and intelligent decision making capabilities with BigReduce. Particularly, we provide two innovative schemes for health event monitoring so that an IoT sensor can use them locally; one is a big data reduction scheme, and the other is a decision making scheme. We believe that BigReduce will result in a remarkable performance in terms of data reduction, energy cost reduction, and the quality of monitoring.},
keywords={Monitoring;Big Data;Cloud computing;Computational modeling;Mathematical model;Smart cities},
doi={10.1109/MCOM.2018.1700303},
ISSN={1558-1896},
month={March},}
@INPROCEEDINGS{9006452,
author={Henning, Sören and Hasselbring, Wilhelm},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Scalable and Reliable Multi-Dimensional Aggregation of Sensor Data Streams},
year={2019},
volume={},
number={},
pages={3512-3517},
abstract={Ever-increasing amounts of data and requirements to process them in real time lead to more and more analytics platforms and software systems being designed according to the concept of stream processing. A common area of application is the processing of continuous data streams from sensors, for example, IoT devices or performance monitoring tools. In addition to analyzing pure sensor data, analyses of data for groups of sensors often need to be performed as well. Therefore, data streams of the individual sensors have to be continuously aggregated to a data stream for a group. Motivated by a real-world application scenario, we propose that such a stream aggregation approach has to allow for aggregating sensors in hierarchical groups, support multiple such hierarchies in parallel, provide reconfiguration at runtime, and preserve the scalability and reliability qualities induced by applying stream processing techniques. We propose a stream processing architecture fulfilling these requirements, which can be integrated into existing big data architectures. We present a pilot implementation of such an extended architecture and show how it is used in industry. Furthermore, in experimental evaluations we show that our solution scales linearly with the amount of sensors and provides adequate reliability in the case of faults.},
keywords={Computer architecture;Topology;Reliability;Buildings;Power demand;Scalability;Production},
doi={10.1109/BigData47090.2019.9006452},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9434509,
author={Rui, Xiang and Yuru, Xu},
booktitle={2020 International Conference on Big Data and Social Sciences (ICBDSS)},
title={Strategy Deviance and Internal Control Quality : Based on big data samples empirical evidence of listed companies},
year={2020},
volume={},
number={},
pages={54-58},
abstract={This paper selects Chinese listed companies from 2008 to 2017 as data samples, which is collected by professional database. Through statistical analysis, it transforms qualitative questions into quantitative questions. This empirically tests the relationship between strategy deviance and internal control quality. It is found that there is a significant negative correlation between strategy deviance and internal control quality. In the further analysis, the relationship between strategy deviance and internal control quality is different in different life cycles of enterprises. This research uses big data samples to enriches the study on corporate strategy and internal control quality, and provides a certain warning function for the capital market to supervise the internal control of enterprises with great strategy deviance.},
keywords={Correlation;Statistical analysis;Databases;Social sciences;Process control;Companies;Transforms;empirical research;big data;corporate strategy;corporate governance},
doi={10.1109/ICBDSS51270.2020.00020},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8711436,
author={Qin, Baoling and Lin, Xiaowei and Li, Sina and Luo, Qiao and Zheng, Fenglin and Cai, JieJian and Luo, Yunshi},
booktitle={2019 IEEE 2nd International Conference on Information and Computer Technologies (ICICT)},
title={Design and Application of Fog Computing Model Based on Big Data},
year={2019},
volume={},
number={},
pages={93-97},
abstract={Fog computing based on big data is a hot topic in the research of computing technology at home and abroad. With the wide application and popularity of IoT (Internet of Things), the big data generated by edge devices is exploding, and cloud computing models are becoming increasingly inadequate to meet the needs of big data processing and communication, which is mainly manifested as follows. Slow data processing, insufficient storage space, prolonged communication and many other issues. Fog computing, of which the advantage is distributed computing, namely the "de-centralized" mode calculation, is the suitable solution to solve these problems. In the IoT system, fog computing model based on big data is constructed to distribute the big data computing, storage and communication in the system to the edge device. The purpose is to make the system structure simpler, more modular and intelligent, duce network congestion, exploit advantages of edge devices and improve high quality intelligence of IoT applications, and moreover, to reduce the deployment of IoT hardware and operating costs. Taking the cloud robotics as an example, it is proposed to embed the fog computing technology in the cloud robotics system, which greatly improves the computing function of the cloud robotics system. In short, it provides theoretical support and scientific experimental basis for the informationization and intelligence of all walks of life, and its research has certain value and significance.},
keywords={Cloud computing;Edge computing;Big Data;Computational modeling;Data models;Internet of Things;Sensors;big data;fog computing;cloud computing;internet of things;fog models},
doi={10.1109/INFOCT.2019.8711436},
ISSN={},
month={March},}
@ARTICLE{7912315,
author={Chen, Min and Hao, Yixue and Hwang, Kai and Wang, Lu and Wang, Lin},
journal={IEEE Access},
title={Disease Prediction by Machine Learning Over Big Data From Healthcare Communities},
year={2017},
volume={5},
number={},
pages={8869-8879},
abstract={With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013-2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.},
keywords={Diseases;Hospitals;Prediction algorithms;Machine learning algorithms;Big Data;Data models;Big data analytics;machine learning;healthcare},
doi={10.1109/ACCESS.2017.2694446},
ISSN={2169-3536},
month={},}
@ARTICLE{9299499,
author={Liu, Jie and Cao, Yijia and Li, Yong and Guo, Yixiu and Deng, Wei},
journal={CSEE Journal of Power and Energy Systems},
title={A big data cleaning method based on improved CLOF and Random Forest for distribution network},
year={2020},
volume={},
number={},
pages={1-10},
abstract={In order to improve the data quality, the big data cleaning method of distribution network was studied in this paper. First, the Local Outlier Factor (LOF) algorithm based on DBSCAN clustering was used to detect outliers. However, due to the difficulty in determining the LOF threshold, a method of dynamically calculating the threshold based on the transformer districts and time was proposed. Besides, the LOF algorithm combines the statistical distribution method to reduce the "misjudgment rate". Aiming at the diversity and complexity of data missing forms in power big data, this paper improved the Random Forest imputation algorithm, which can be applied to various forms of missing data, especially the blocked missing data and even some horizontal or vertical data completely missing. The data in this paper were from real data of 44 transformer districts of a certain 10kV line in distribution network. Experimental results showed that outlier detection was accurate and suitable for any shape and multidimensional power big data. The improved Random Forest imputation algorithm was suitable for all missing forms, with higher imputation accuracy and better model stability. By comparing the network loss prediction between the data using this data cleaning method and the data removing outliers and missing values, it was found that the accuracy of network loss prediction had been improved by nearly 4 percentage points using the data cleaning method mentioned in this paper. Additionally, as the proportion of bad data increased, the difference between the prediction accuracy of cleaned data and that of uncleaned data was greater.},
keywords={Cleaning;Distribution networks;Big Data;Prediction algorithms;Clustering algorithms;Data models;Anomaly detection;Data cleaning;Outliers detection;missing data imputation;LOF;DBSCAN;Random Forest},
doi={10.17775/CSEEJPES.2020.04080},
ISSN={2096-0042},
month={},}
@ARTICLE{8374410,
author={Jang, Busik and Park, Sangdon and Lee, Joohyung and Hahn, Sang-Geun},
journal={IEEE Access},
title={Three Hierarchical Levels of Big-Data Market Model Over Multiple Data Sources for Internet of Things},
year={2018},
volume={6},
number={},
pages={31269-31280},
abstract={This paper proposes three hierarchical levels of a competitive big-data market model. We consider that a service provider gathers data from multiple data sources and provides valuable information from refined data as a service to its customers. Under our approach, a service provider determines optimal data procurement from multiple data sources within its budget constraint. The multiple data sources follow the service provider's action by independently submitting bidding prices to the service provider. Further, customers decide whether to subscribe or not based on the subscription fee, their willingness-to-pay, and the quality of the refined data. We study the economic benefits of such a market model by analyzing the hierarchical decision making procedures as a Stackelberg game. We show the existence and the uniqueness of the Nash equilibrium (NE), and the NE solution is given as a closed form. Finally, we reveal that the obtained unique equilibrium solution maximizes the payoff of all market participants.},
keywords={Data models;Games;Analytical models;Internet of Things;Data mining;Nash equilibrium;Numerical models;Data market;Internet of Things;stackelberg game;industrial informatics},
doi={10.1109/ACCESS.2018.2845105},
ISSN={2169-3536},
month={},}
@ARTICLE{8703176,
author={Ruan, Junhu and Jiang, Hua and Li, Xiaoyu and Shi, Yan and Chan, Felix T. S. and Rao, Weizhen},
journal={IEEE Transactions on Industrial Informatics},
title={A Granular GA-SVM Predictor for Big Data in Agricultural Cyber-Physical Systems},
year={2019},
volume={15},
number={12},
pages={6510-6521},
abstract={The connection of physical agriculture with corresponding cyber systems is helpful to achieve precision agriculture. Real-time data from agriculture sensors can provide decision supports to improve the yields and quality of agricultural products, but also bring about challenges one of which is how to mine useful information from these vast amounts of data at acceptable computation costs. To deal with the dimension disaster problem faced by most conventional mining algorithms, in this paper we combine granulation techniques and genetic algorithm (GA) with a support vector machine (SVM) to propose a granular GA-SVM. In the integrated predictor, three granulation methods, that is, Min-Median-Max granulation, Quartile-Median granulation, and fuzzy granulation, are introduced to break down big data in agricultural cyber-physical systems into small-scale granules, and GA is used to find the optimal values of SVM penalty parameter and kernel parameter from the reduced granules. Internet of Things (IoT) data from Luochuan Apple Experimental Demonstration Station in Shaanxi Province, China, verified that the proposed granular GA-SVM predictor is effective to make big data prediction with reduced computation time and equivalent accuracy. Moreover, the predicted environment information could provide guidance for growers achieving precise management of apple planting.},
keywords={Support vector machines;Cyber-physical systems;Genetic algorithms;Soil;Big Data;Agriculture;Humidity;Agricultural cyber-physical systems;big data;granulation computing;prediction;support vector machine (SVM)},
doi={10.1109/TII.2019.2914158},
ISSN={1941-0050},
month={Dec},}
@INPROCEEDINGS{8622041,
author={Dash, Saroj Kumar and Safro, Ilya and Srinivasamurthy, Ravisutha Sakrepatna},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Spatio-temporal prediction of crimes using network analytic approach},
year={2018},
volume={},
number={},
pages={1912-1917},
abstract={It is quite evident that majority of the population lives in urban area today than in any time of the human history. This trend seems to increase in coming years. A study [5] says that nearly 80.7% of total population in USA stays in urban area. By 2030 nearly 60% of the population in the world will live in or move to cities. With the increase in urban population, it is important to keep an eye on criminal activities. By doing so, governments can enforce intelligent policing systems and hence many government agencies and local authorities have made the crime data publicly available. In this paper, we analyze Chicago city crime data fused with other social information sources using network analytic techniques to predict criminal activity for the next year. We observe that as we add more layers of data which represent different aspects of the society, the quality of prediction is improved. Our prediction models not just predict total number of crimes for the whole Chicago city, rather they predict number of crimes for all types of crimes and for different regions in City of Chicago.},
keywords={Urban areas;Law enforcement;Predictive models;Biological system modeling;Sociology;Statistics;Libraries;Data Fusion;Network Analysis;Crime Data Analysis;Data Analysis for Government},
doi={10.1109/BigData.2018.8622041},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9145071,
author={Byabazaire, John and O'Hare, Gregory and Delaney, Declan},
booktitle={2020 IEEE International Conference on Communications Workshops (ICC Workshops)},
title={Data Quality and Trust : A Perception from Shared Data in IoT},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Internet of Things devices and data sources areseeing increased use in various application areas. The pro-liferation of cheaper sensor hardware has allowed for widerscale data collection deployments. With increased numbers ofdeployed sensors and the use of heterogeneous sensor typesthere is increased scope for collecting erroneous, inaccurate orinconsistent data. This in turn may lead to inaccurate modelsbuilt from this data. It is important to evaluate this data asit is collected to determine its validity. This paper presents ananalysis of data quality as it is represented in Internet of Things(IoT) systems and some of the limitations of this representation. The paper discusses the use of trust as a heuristic to drive dataquality measurements. Trust is a well-established metric that hasbeen used to determine the validity of a piece or source of datain crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework forrepresenting data quality effectively within the big data modeland why a trust backed framework is important especially inheterogeneously sourced IoT data streams.},
keywords={Data integrity;Data models;Big Data;Biological system modeling;Ecosystems;Standards},
doi={10.1109/ICCWorkshops49005.2020.9145071},
ISSN={2474-9133},
month={June},}
@INPROCEEDINGS{8451786,
author={Liling, Zhao and Zelin, Zhang and Quansen, Sun},
booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},
title={Deep Learning Based Super Resolution Using Significant and General Regions},
year={2018},
volume={},
number={},
pages={2516-2520},
abstract={Today, Big Data brings benefits to many areas of scientific research. However, processing these large amounts of data often requires extensive computing time and a large storage space. Global feature analysis is considered to be universal in traditional super resolution methods, but it is not applicable to Big Data. There remains a viewpoint that it is not necessary to address all data equally and impartially. Focusing on useful information can make the massive data analysis possible and more effective. In this paper, we consider the significant regions, and thus, we propose a new super resolution approach that uses significant and general information. Under the framework of a convolutional neural network, the training process is performed on the significant parts of the training data set, and the reconstruction process considers significant parts separately; then, a super resolution image will be obtained according to each different demand. This concept is easy to understand, but it can be achieved only via the Big Data approach with many similar images on the Internet and the effective deep learning algorithm. Experiments show that our new approach can reduce the testing time and obtain a high-quality reconstructed image.},
keywords={Training;Image resolution;Image reconstruction;Feature extraction;Roads;Machine learning;Big Data;deep learning;super resolution;significant regions},
doi={10.1109/ICIP.2018.8451786},
ISSN={2381-8549},
month={Oct},}
@INPROCEEDINGS{8519983,
author={Su, Chuan-Hsiang and Lin, Shang-Chih and Chang, Chieh-Ming and Huang, Yennun},
booktitle={2018 International Conference on System Science and Engineering (ICSSE)},
title={A Fuzzy Risk Assessment Strategy Based on Big Data for Multinational Financial Markets},
year={2018},
volume={},
number={},
pages={1-6},
abstract={This research aims to use data science methods to mine valuable information in big data and use fuzzy theory to construct a risk assessment strategy that is applicable to multinational financial markets. First of all, in order to ensure that the data of multinational financial markets are better connected, low-quality data has been cleaned up and simplified, including missing value and too much time gap. Then, we analyze the daily signal fluctuations based on statistical methods to find the causality and investment risks of multinational financial markets. Finally, the fuzzy inference system consists of multiple inputs and multiple outputs. The inputs are “US stocks”, “Kur”, “Ske”, “CF” and “SD”, respectively, and the outputs are “ups”, “downs”, “uncertain”, “may-be-ups”, and “may-be-downs”. From the experimental results, it is known that the misjudgment ratio is used as a prerequisite for performance evaluation, and reliable results are obtained for both tradable ratio (Low/Medium-risk area: 1.8, 16 %) and accuracy (Low/Medium-risk area: 66.7, 70.7 %). In summary, the performance of the proposed method has been verified. The risk assessment of multinational financial markets has become a possibility. In future research work, we will continue to explore the possibility of machine learning and optimization algorithms to improve performance and share this result on an open platform.},
keywords={Risk management;Big Data;Statistical analysis;Stock markets;Filtering;Data mining},
doi={10.1109/ICSSE.2018.8519983},
ISSN={2325-0925},
month={June},}
@INPROCEEDINGS{7796171,
author={Qiu, Longfei and Gai, Keke and Qiu, Meikang},
booktitle={2016 IEEE International Conference on Smart Cloud (SmartCloud)},
title={Optimal Big Data Sharing Approach for Tele-Health in Cloud Computing},
year={2016},
volume={},
number={},
pages={184-189},
abstract={The rapid development of tele-health systems have received driving engagements from various emerging techniques, such as big data and cloud computing. Sharing data among multiple tele-health systems is an adaptive approach for improving service quality via the network-based technologies. However, current implementations of data sharing in cloud computing is still facing the restrictions caused by the networking capacities and virtual machine switches. In this paper, we focus on the problem of data sharing obstacles in cloud computing and propose an approach that uses dynamic programming to produce optimal solutions to data sharing mechanisms. The proposed approach is called Optimal Telehealth Data Sharing Model (OTDSM), which considers transmission probabilities, maximizing network capacities, and timing constraints. Our experimental results have proved the flexibility and adoptability of the proposed method.},
keywords={Timing;Capacity planning;Cloud computing;Distributed databases;Heuristic algorithms;Big data;Medical services;Tele-health;big data;cloud computing;data sharing},
doi={10.1109/SmartCloud.2016.21},
ISSN={},
month={Nov},}
@ARTICLE{7397883,
author={Brun, Olivier and Wang, Lan and Gelenbe, Erol},
journal={IEEE Journal on Selected Areas in Communications},
title={Big Data for Autonomic Intercontinental Overlays},
year={2016},
volume={34},
number={3},
pages={575-583},
abstract={This paper uses big data and machine learning for the real-time management of Internet scale quality-of-service (QoS) route optimisation with an overlay network. Based on the collection of data sampled every 2 min over a large number of source-destinations pairs, we show that intercontinental Internet protocol (IP) paths are far from optimal with respect to QoS metrics such as end-to-end round-trip delay. We, therefore, develop a machine learning-based scheme that exploits large scale data collected from communicating node pairs in a multihop overlay network that uses IP between the overlay nodes, and selects paths that provide substantially better QoS than IP. Inspired from cognitive packet network protocol, it uses random neural networks with reinforcement learning based on the massive data that is collected, to select intermediate overlay hops. The routing scheme is illustrated on a 20-node intercontinental overlay network that collects some 2 × 106 measurements per week, and makes scalable distributed routing decisions. Experimental results show that this approach improves QoS significantly and efficiently.},
keywords={Routing;Internet;IP networks;Monitoring;Overlay networks;Quality of service;Routing protocols;The Internet;Big Data;Network QoS;Smart Overlays;Random Neural Network;Cognitive Packet Network;The Internet;big data;network quality of service (QoS);smart overlays;random neural network;cognitive packet network},
doi={10.1109/JSAC.2016.2525518},
ISSN={1558-0008},
month={March},}
@INPROCEEDINGS{9006241,
author={Wen, Andrew and Wang, Yanshan and Kaggal, Vinod C. and Liu, Sijia and Liu, Hongfang and Fan, Jungwei},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Enhancing Clinical Information Retrieval through Context-Aware Queries and Indices},
year={2019},
volume={},
number={},
pages={2800-2807},
abstract={The big data revolution has created a hefty demand for searching large-scale electronic health records (EHRs) to support clinical practice, research, and administration. Despite the volume of data involved, fast and accurate identification of clinical narratives pertinent to a clinical case being seen by any given provider is crucial for decision-making at the point of care. In the general domain, this capability is accomplished through a combination of the inverted index data structure, horizontal scaling, and information retrieval (IR) scoring algorithms. These technologies are also being used in the clinical domain, but have met limited success, particularly as clinical cases become more complex. One barrier affecting clinical performance is that contextual information, such as negation, temporality, and the subject of clinical mentions, impact clinical relevance but is not considered in general IR methodologies. In this study, we implemented a solution by identifying and incorporating the aforementioned semantic contexts as part of IR indexing/scoring with Elasticsearch. Experiments were conducted in comparison to baseline approaches with respect to: 1) evaluation of the impact on the quality (relevance) of the returned results, and 2) evaluation of the impact on execution time and storage requirements. The results showed a 5.1-23.1% improvement in retrieval quality, along with achieving 35% faster query execution time. Cost-wise, the solution required 1.5-2 times larger space and about 3 times increase in indexing time. The higher relevance demonstrated the merit of incorporating contextual information into clinical IR, and the near-constant increase in time and space suggested promising scalability.},
keywords={Indexes;Diabetes;Big Data;Manganese;Semantics;Clinical diagnosis;Payloads;Electronic Health Records;EHR;Information Retrieval;Clinical Information Retrieval;Elasticsearch},
doi={10.1109/BigData47090.2019.9006241},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8079864,
author={Li, Fadia Shah Jianping and Dagadu Caleb, Joshua and Zhou, Wang and Shah, Yasir and Shah, Faiza},
booktitle={2016 13th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)},
title={Wavelets for medical big data access improvement strategy over wireless network},
year={2016},
volume={},
number={},
pages={316-319},
abstract={Medical Big Data (MBD) is the most critical form of information. MBD over a colossal Wireless Network (WN) is a sophisticated area of concern. Every day WN is becoming more dense and complicated. The MBD can be saved to wireless network after applying suitable Wavelet Compression. The quality of MBD is the most important factor; which means efficient and reliable transmission. Wavelet Compression ensures the best image compression schemes such that the quality of data is not affected. MBD is in the form of medical reports, x-rays, CT scan image, audio video files. MBD over WN needs limited bandwidth utilization. Software Define Network (SDN) divides the network infrastructure into two planes. The Control Plane manages data flow and the Data Plane deals with the data. In-network coordinating cache via SDN support is a better way of network efficiency improvement. The cache size is very limited and can entertain only a limited number of requests. Our proposed system for MBD Cache (MBDC) is efficient to improvement the network performance using Wavelets. It is capable to work in Heterogeneous Network (Het Net) and Software Define Network (SDN) Architecture. By considering any shortest path like K-Means algorithm, the neighboring-content cache awareness reduces the latency time to fulfill the request from destination.},
keywords={Biomedical imaging;Computer architecture;Image coding;Big Data;Bandwidth;Wireless networks;Software;Coordinating Cache;Medical Big Data;SDN;Wavelet Compression},
doi={10.1109/ICCWAMTIP.2016.8079864},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9284541,
author={Martins, Wictor Souza and Tardiole Kuehne, Bruno and Sobrinho, Rafael Ferreira and Preti, Fábio},
booktitle={2020 IEEE International Conference on Services Computing (SCC)},
title={A Reference Method for Performance Evaluation in Big Data Architectures},
year={2020},
volume={},
number={},
pages={1-8},
abstract={This paper presents a reference method for performance evaluation in Big Data architectures, called by Improvement Method for Big Data Architectures (IMBDA) aiming to increase the performance, and consequently raising the quality of service provided. The method will contribute to small businesses and startups that have limited financial re-sources (impossible to invest in market solutions). The proposed approach considers the relationship of the processes in a data processing flow to find possible bottlenecks and optimization points. To this end, IMBDA collects system logs to compose functional metrics (e.g., processing time) and non-functional metrics (e.g., CPU and memory utilization, and other cloud computing infrastructure resources). The system stores these metrics in an external data analysis tool that investigates the correlation of performance between processes. The reference method applies to the architecture of a Big Data application, which provides solutions in fleet logistics. With the use of IMBDA, it was possible to identify performance bottlenecks, allowing the reconfiguration of the architecture to increase service quality at the lowest possible cost.},
keywords={Measurement;Performance evaluation;Correlation;Data integrity;Computer architecture;Tools;Business;Big Data;cross-layer architecture;performance analysis and aids;pipeline processors;quality of services;Service Level Agreement},
doi={10.1109/SCC49832.2020.00044},
ISSN={2474-2473},
month={Nov},}
@INPROCEEDINGS{9204954,
author={Kok, Chee Hoo and Azlan, Mohd Azrul Mohd and Ong, Soon Ee},
booktitle={2020 10th IEEE International Conference on Control System, Computing and Engineering (ICCSCE)},
title={IoT based Low Cost Distributed Air Quality Monitoring System for Big Data Collection},
year={2020},
volume={},
number={},
pages={7-10},
abstract={The resolution and accuracy of the data is a major challenge during data collection for air quality related research. In Malaysia, each monitoring station is located very far from each other causing the resolution of data to be considerably low. In this paper, we proposed a framework of Distributed Air Quality Monitoring System (DAQMS) which utilizes the full benefits of Internet of Things (IoT) for a better area coverage for data collection. We implemented the mobile hardware device by interfacing ESP32 microcontroller with multiple sensors at a much lower cost compared to the existing immobile air quality monitoring stations. The data are continuously being streamed to our cloud server and is ready to be queried for research purpose. Users can also view the latest air-quality indicators via the web-based dashboard application. Finally, we discussed on the future planning to further improve the data accuracy and user's experience. Our proposed framework does not only help in resolving the issue of data granularity but also helps in raising the awareness of air pollution among the general public.},
keywords={Air quality;Monitoring;Temperature sensors;Databases;Temperature measurement;Cloud computing;Air Pollution;Environmental Monitoring;Distributed System;Internet-of-Things;Cloud Computing},
doi={10.1109/ICCSCE50387.2020.9204954},
ISSN={},
month={Aug},}
@ARTICLE{8110696,
author={Zoha, Ahmed and Saeed, Arsalan and Farooq, Hasan and Rizwan, Ali and Imran, Ali and Imran, Muhammad Ali},
journal={IEEE Transactions on Mobile Computing},
title={Leveraging Intelligence from Network CDR Data for Interference Aware Energy Consumption Minimization},
year={2018},
volume={17},
number={7},
pages={1569-1582},
abstract={Cell densification is being perceived as the panacea for the imminent capacity crunch. However, high aggregated energy consumption and increased inter-cell interference (ICI) caused by densification, remain the two long-standing problems. We propose a novel network orchestration solution for simultaneously minimizing energy consumption and ICI in ultra-dense 5G networks. The proposed solution builds on a big data analysis of over 10 million CDRs from a real network that shows there exists strong spatio-temporal predictability in real network traffic patterns. Leveraging this, we develop a novel scheme to pro-actively schedule radio resources and small cell sleep cycles yielding substantial energy savings and reduced ICI, without compromising the users QoS. This scheme is derived by formulating a joint Energy Consumption and ICI minimization problem and solving it through a combination of linear binary integer programming, and progressive analysis based heuristic algorithm. Evaluations using: 1) a HetNet deployment designed for Milan city where big data analytics are used on real CDRs data from the Telecom Italia network to model traffic patterns, 2) NS-3 based Monte-Carlo simulations with synthetic Poisson traffic show that, compared to full frequency reuse and always on approach, in best case, the proposed scheme can reduce energy consumption in HetNets to 1/8th while providing same or better QoS.},
keywords={Energy consumption;Interference;Mobile computing;Traffic control;Telecommunication traffic;Big Data;Mobile communication;5G;heterogeneous networks;small cells;energy efficiency;inter-cell interference;resource allocation;binary integer linear programming;CDRs;big data analytics},
doi={10.1109/TMC.2017.2773609},
ISSN={1558-0660},
month={July},}
@INPROCEEDINGS{7979912,
author={Mohamed, Ehab and Hong, Zheng},
booktitle={2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
title={Hadoop-MapReduce Job Scheduling Algorithms Survey},
year={2016},
volume={},
number={},
pages={237-242},
abstract={The big data computing era is coming to be a fact in all daily life. As data-intensive become a reality in many of scientific branches, finding an efficient strategy for massive data computing systems has become a multi-objective improvement. Processing these huge data on the distributed hardware clusters as Clouds needs a powerful computation model like Hadoop-MapReduce. In this paper, we studied various schedulers developed in Hadoop in Cloud Environments, features and issues. Most existing studies considered the improvement in the performance from the single point of view (scheduling, locality of data, the correctness of the data, etc) but very few literature involved multi-objectives improvements (quality requirements, scheduling entities, and dynamic environment adaptation), especially in heterogeneous parallel and distributed systems. Hadoop and MapReduce are two important aspects in big data for handling structured and unstructured data. The Creation of an algorithm for node selection is essential to improve and optimize the performance of the MapReduce. This paper introduces a survey of the previous work done in the Hadoop-MapReduce scheduling and gives some suggestion for the improvement of it.},
keywords={Scheduling algorithms;Big Data;Distributed databases;Scheduling;File systems;Cloud computing;Big data;Job Scheduling;Hadoop;MapReduce},
doi={10.1109/CCBD.2016.054},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6816763,
author={Golab, Lukasz and Johnson, Theodore},
booktitle={2014 IEEE 30th International Conference on Data Engineering},
title={Data stream warehousing},
year={2014},
volume={},
number={},
pages={1290-1293},
abstract={Data stream warehousing is a data management technology designed to simultaneously handle big-data and fast-data. Conceptually, a data stream warehouse can be thought of as a data warehouse system that is updated in nearly-real time rather than during downtimes, or as a data stream management system that stores a very long history. In this tutorial, we 1) motivate the need for data stream warehouse systems using real-life examples drawn from our experiences in network and data center monitoring, 2) describe several possible system architectures for data stream warehousing, 3) discuss various issues in query languages, performance optimizations and data stream quality, and 4) conclude with a discussion of open problems.},
keywords={Warehousing;Data warehouses;Optimization;Educational institutions;Monitoring;Real-time systems;Data mining},
doi={10.1109/ICDE.2014.6816763},
ISSN={2375-026X},
month={March},}
@INPROCEEDINGS{8588272,
author={Cadersaib, Bibi Zarine and Ben Sta, Hatem and Gobin Rahimbux, Baby Ashwin},
booktitle={2018 Sixth International Conference on Enterprise Systems (ES)},
title={Making an Interoperability Approach between ERP and Big Data Context},
year={2018},
volume={},
number={},
pages={146-153},
abstract={Enterprise Resource Planning (ERP) systems are deeply rooted as part of organisations' IT systems. These systems have attracted different types of research including project management, technological implications as well as educational research. With the new data era, it is key that studies complementing ERP and Big Data integration are well established. During the initial stage of literature analysis, it was found that interoperability was an important aspect in ERP and Big Data integration. However, there were limited studies linking ERP, Big Data and Interoperability. A thorough background analysis was therefore performed to understand state-of-the-art linking these three domains and the results are presented in this paper. First, the link between ERP, Big Data and Interoperability is discussed. Second, the main findings from the articles linking ERP, Big Data and Interoperability are highlighted. Finally, the research areas linking these three domains are derived.},
keywords={Interoperability;Big Data;Semantics;Tools;Quality assessment;ERP, Big Data, Interoperability, Enterprise Architecture},
doi={10.1109/ES.2018.00030},
ISSN={2572-6609},
month={Oct},}
@INPROCEEDINGS{9607145,
author={Gao, Xiang},
booktitle={2020 2nd International Conference on Applied Machine Learning (ICAML)},
title={Analysis on The Construction Path of “Curriculum Thought And Politics” in Higher Vocational Chinese Teaching Under The Background of Big Data},
year={2020},
volume={},
number={},
pages={208-211},
abstract={Through the proper integration of ideological and political courses in Chinese teaching, the educational synergy under the ideological and political construction should be developed to train higher vocational students to form a correct outlook on life, values and world view. Exist in the construction process of the concept of “ideology is not clear, the main function does not reach the designated position” and “teaching, technology, education and other issues, so you need to actively carry out effective teaching ideology construction in higher vocational colleges, and large background data, establish a scientific training system, through the mechanism innovation, professional and scientific ideological education; Innovative teaching methods, emphasizing the modern technology teaching means of “teaching by words and deeds”; Build a scientific evaluation system and incentive mechanism, build a high standard and high quality ideological and political course teacher team, strengthen the cooperation between ideological and political course teachers and professional course teachers, ideological and political education into professional course education, permeate into students' thinking, learning and life. This is undoubtedly an effective way to achieve the goal of ideological and political construction of Chinese teaching courses in higher vocational colleges.},
keywords={Training;Technological innovation;Education;Production;Machine learning;Big Data;Standards;Higher vocational Chinese teaching;curriculum thought and politics;construction path},
doi={10.1109/ICAML51583.2020.00050},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8573653,
author={Kumar, K.Arun and Karthikeyan, N. and Karthick, S.},
booktitle={2018 International Conference on Soft-computing and Network Security (ICSNS)},
title={Statistical Analysis of Big Data to Improvise Health Care},
year={2018},
volume={},
number={},
pages={1-5},
abstract={In the world today the information technology had played a great impact in the form of rapid digitization. The challenge towards this digitization lies in collection, analysis, storage of such huge datasets. Moreover, the analysis of such datasets may infer some new information such as diagnosis, therapy and prevention at an early stage. This paper proposes to use statistical tests to quantitatively analyses the datasets of patients to reveal some new unknown patterns. May not to an increasedlife span it may provide the precaution as the repeated analysis of various data samples may mark the beginning of identification of new panorama for some life threatening malignant disease. This paper focuses on various sampling tests to prove the effectivenessof the therapy and the varied outcomes across the subject of diversified malignancy of the malady. Further this helps easy recognition of entangled health care environment. These statistical methods can be implemented in any of the programming languages like R, Octave, MatLab. This paper summarizes the utilization of available statistical methods to improve the quality of treatment and the life of entire ailing population.},
keywords={Medical services;Big Data;Statistical analysis;Sociology;Medical diagnostic imaging;Tools},
doi={10.1109/ICSNS.2018.8573653},
ISSN={},
month={Feb},}
@INPROCEEDINGS{9378292,
author={Nam, Yihyun and Cha, Sangwhan},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Correlation Analysis between Median Income level of District and Quality of Medical Service in Seoul, Korea},
year={2020},
volume={},
number={},
pages={5789-5791},
abstract={The importance of medical facilities to citizens is increasing due to frequently occurring pandemics and increased interest in primary health. Although these facilities must be spread widely throughout places to be easily accessed to everyone, it is typically concentrated in central, urban areas. The quality of medical service can be evaluated by the ratio of hospitals of different sizes--clinic, hospital, and territory hospital--since large hospitals offer a higher quality medical care. This research aims to analyze the correlation between each district's median income level in Seoul and medical service quality using a quantitative analysis method based on correlation coefficient analysis. The correlation analysis showed that territory hospitals have a strong correlation with the district's income level, while general hospitals and clinics have a higher correlation with the number of residents and workers.},
keywords={Correlation;Hospitals;Pandemics;Statistical analysis;Urban areas;Insurance;Big Data},
doi={10.1109/BigData50022.2020.9378292},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7529552,
author={Hanrong Lu and Xin Chen and Xuhui Lan and Feng Zheng},
booktitle={2016 IEEE International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Duplicate data detection using GNN},
year={2016},
volume={},
number={},
pages={167-170},
abstract={In some applications like data warehousing, data mining or information integration data must be cleaned as a preprocessing step to ensure the quality of data and the performance of applications. An essential work in data cleaning is duplicate record detection. Existing detection methods apply to different data models and record types. Certain difficulties with those studies are still to be overcome. This paper proposes a genetic neural network based approach to duplicate record detection. The topology and weight vector of a neural network are firstly optimized by a genetic algorithm for the given data set before it is used to perform the detection. The method can enhance the detection accuracy and alleviate the many problems with previous works.},
keywords={Cleaning;Genetics;record detection;data cleaning;neural network;genetic algorithm;GNN},
doi={10.1109/ICCCBDA.2016.7529552},
ISSN={},
month={July},}
@INPROCEEDINGS{7498306,
author={Cao, Wei and Wu, Zhengwei and Wang, Dong and Li, Jian and Wu, Haishan},
booktitle={2016 IEEE 32nd International Conference on Data Engineering (ICDE)},
title={Automatic user identification method across heterogeneous mobility data sources},
year={2016},
volume={},
number={},
pages={978-989},
abstract={With the ubiquity of location based services and applications, large volume of mobility data has been generated routinely, usually from heterogeneous data sources, such as different GPS-embedded devices, mobile apps or location based service providers. In this paper, we investigate efficient ways of identifying users across such heterogeneous data sources. We present a MapReduce-based framework called Automatic User Identification (AUI) which is easy to deploy and can scale to very large data set. Our framework is based on a novel similarity measure called the signal based similarity (SIG) which measures the similarity of users' trajectories gathered from different data sources, typically with very different sampling rates and noise patterns. We conduct extensive experimental evaluations, which show that our framework outperforms the existing methods significantly. Our study on one hand provides an effective approach for the mobility data integration problem on large scale data sets, i.e., combining the mobility data sets from different sources in order to enhance the data quality. On the other hand, our study provides an in-depth investigation for the widely studied human mobility uniqueness problem under heterogeneous data sources.},
keywords={Trajectory;Urban areas;Buildings;Noise measurement;Mobile communication;Navigation;Education},
doi={10.1109/ICDE.2016.7498306},
ISSN={},
month={May},}
@INPROCEEDINGS{7508135,
author={Kaur, Arvinder and Vig, Vidhi},
booktitle={2016 6th International Conference - Cloud System and Big Data Engineering (Confluence)},
title={Challenges in data extraction from Open Source software repositories},
year={2016},
volume={},
number={},
pages={316-319},
abstract={Open source softwares (OSS) are a boon to research and development. Their free access and availability of thousands of projects pushes research into new dimensions. But are these repositories actually helpful in quality research? Heterogeneous and incomplete data, lack of integration between repositories, performance and usability issues and lack of documentation are few factors that affect the quality of research in Open Source. In this paper, we lay down the difficulties experienced while working on extraction and analysis of data from open source repositories for quality research.},
keywords={Decision support systems;Big data;Conferences;Open Source Software;repositories;Software engineering;mining},
doi={10.1109/CONFLUENCE.2016.7508135},
ISSN={},
month={Jan},}
@INPROCEEDINGS{7381890,
author={Song, Wei and Wu, Dong and Wong, Raymond and Fong, Simon and Cho, Kyungeun},
booktitle={2015 Tenth International Conference on Digital Information Management (ICDIM)},
title={A real-time interactive data mining and visualization system using parallel computing},
year={2015},
volume={},
number={},
pages={10-13},
abstract={As color and geometry representations are easily recognized by human's brain, data visualization technologies provide data mining results with natural and intuitive operation interfaces. Currently, data mining users always plot 2D and 3D diagrams using the traditional data visualization tools. However, it is difficult to visualize big data in real time using these tools. To enhance the visualization quality, this paper proposes a real-time data mining and visualization system for big data. We apply a graphic processing unit (GPU) programming technology to enhance the data mining algorithms and visualization methods in parallel. For unified development environment supports, we utilize a TCP/IP to deliver datasets between different data mining platforms and databases. In the visualization client, this system provides two human computer interaction methods, including a graphics user interface (GUI) and a natural user interface (NUI). To verify the efficacy of our proposed system, we integrated it with an NUI engine, the WinSocket API and the DirectX.},
keywords={Data visualization;Data mining;Graphics processing units;Programming;Big data;Graphical user interfaces;Real-time systems;Data mining;Information visualization;Parallel computing;Natural user interface},
doi={10.1109/ICDIM.2015.7381890},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9378449,
author={Zhao, Kai and Di, Sheng and Lian, Xin and Li, Sihuan and Tao, Dingwen and Bessac, Julie and Chen, Zizhong and Cappello, Franck},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={SDRBench: Scientific Data Reduction Benchmark for Lossy Compressors},
year={2020},
volume={},
number={},
pages={2716-2724},
abstract={Efficient error-controlled lossy compressors are becoming critical to the success of today's large-scale scientific applications because of the ever-increasing volume of data produced by the applications. In the past decade, many lossless and lossy compressors have been developed with distinct design principles for different scientific datasets in largely diverse scientific domains. In order to support researchers and users assessing and comparing compressors in a fair and convenient way, we establish a standard compression assessment benchmark - Scientific Data Reduction Benchmark (SDRBench)1. SDRBench contains a vast variety of real-world scientific datasets across different domains, summarizes several critical compression quality evaluation metrics, and integrates many state-of-the-art lossy and lossless compressors. We demonstrate evaluation results using SDRBench and summarize six valuable takeaways that are helpful to the in-depth understanding of lossy compressors.},
keywords={Measurement;Conferences;Benchmark testing;Big Data;Compressors;Standards},
doi={10.1109/BigData50022.2020.9378449},
ISSN={},
month={Dec},}
@INPROCEEDINGS{4400967,
author={Klein, Anja and Do, Hong-Hai and Hackenbroich, Gregor and Karnstedt, Marcel and Lehner, Wolfgang},
booktitle={2007 IEEE 23rd International Conference on Data Engineering Workshop},
title={Representing Data Quality for Streaming and Static Data},
year={2007},
volume={},
number={},
pages={3-10},
abstract={In smart item environments, multitude of sensors are applied to capture data about product conditions and usage to guide business decisions as well as production automation processes. A big issue in this application area is posed by the restricted quality of sensor data due to limited sensor precision as well as sensor failures and malfunctions. Decisions derived on incorrect or misleading sensor data are likely to be faulty. The issue of how to efficiently provide applications with information about data quality (DQ) is still an open research problem. In this paper, we present a flexible model for the efficient transfer and management of data quality for streaming as well as static data. We propose a data stream metamodel to allow for the propagation of data quality from the sensors up to the respective business application without a significant overhead of data. Furthermore, we present the extension of the traditional RDBMS metamodel to permit the persistent storage of data quality information in a relational database. Finally, we demonstrate a data quality metadata mapping to close the gap between the streaming environment and the target database. Our solution maintains a flexible number of DQ dimensions and supports applications directly consuming streaming data or processing data filed in a persistent database.},
keywords={Relational databases;Intelligent sensors;Quality management;Automation;Sensor systems;Production;Temperature sensors;Application software;Computer science;Costs},
doi={10.1109/ICDEW.2007.4400967},
ISSN={},
month={April},}
@INPROCEEDINGS{6683250,
author={Wong, Peter K. C. and Kalam, Akhtar and Barr, Robert},
booktitle={22nd International Conference and Exhibition on Electricity Distribution (CIRED 2013)},
title={A “big data” challenge — Turning smart meter voltage quality data into actionable information},
year={2013},
volume={},
number={},
pages={1-4},
abstract={This paper provides analysis of over and under voltage events captured by smart meters. Using data analysis techniques, the relationship of voltage events is analyzed with respect to ambient temperature, times of the day, days of the week, duration, magnitude and embedded generation status. Clusters of voltage violation sites are grouped to upstream supply distribution substations and zone substations. The analysis forms the basis of proactive voltage improvement that can be undertaken by the supply utility.},
keywords={},
doi={10.1049/cp.2013.0647},
ISSN={},
month={June},}
@INPROCEEDINGS{7202945,
author={Zhou, Hucheng and Lou, Jian-Guang and Zhang, Hongyu and Lin, Haibo and Lin, Haoxiang and Qin, Tingting},
booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
title={An Empirical Study on Quality Issues of Production Big Data Platform},
year={2015},
volume={2},
number={},
pages={17-26},
abstract={Big Data computing platform has evolved to be a multi-tenant service. The service quality matters because system failure or performance slowdown could adversely affect business and user experience. There is few study in literature on service quality issues of production Big Data computing platform. In this paper, we present an empirical study on the service quality issues of Microsoft ProductA, which is a company-wide multi-tenant Big Data computing platform, serving thousands of customers from hundreds of teams. ProductA has a well-defined incident management process, which helps customers report and mitigate service quality issues on 24/7 basis. This paper explores the common symptom, causes and mitigation of service quality issues in Big Data computing. We conduct an empirical study on 210 real service quality issues in ProductA. Our major findings include (1) 21.0% of escalations are caused by hardware faults; (2) 36.2% are caused by system side defects; (3) 37.2% are due to customer side faults. We also studied the general diagnosis process and the commonly adopted mitigation solutions. Our findings can help improve current development and maintenance practice of Big Data computing platform, and motivate tool support.},
keywords={Big data;Hardware;Electronic mail;Iron;Software engineering;Business;Programming},
doi={10.1109/ICSE.2015.130},
ISSN={1558-1225},
month={May},}
@INPROCEEDINGS{7207313,
author={Zhu, Hong and Bayley, Ian and Younas, M. and Lightfoot, David and Yousef, Basel and Liu, Dongmei},
booktitle={2015 IEEE International Congress on Big Data},
title={Big SaaS: The Next Step Beyond Big Data},
year={2015},
volume={},
number={},
pages={775-784},
abstract={Software-as-a-Service (SaaS) is a model of cloud computing in which software functions are delivered to the users as services. The past few years have witnessed its global flourishing. In the foreseeable future, SaaS applications will integrate with the Internet of Things, Mobile Computing, Big Data, Wireless Sensor Networks,and many other computing and communication technologies to deliver customizable intelligent services to a vast population.This will give rise to an era of what we call Big SaaS systems of unprecedented complexity and scale. They will have huge numbers of tenants/users interrelated in complex ways. The code will be complex too and require Big Data but provide great value to the customer. With these benefits come great societal risks, however,and there are other drawbacks and challenges. For example, it is difficult to ensure the quality of data and metadata obtained from crowdsourcing and to maintain the integrity of conceptual model. Big SaaS applications will also need to evolve continuously. This paper will discuss how to address these challenges at all stages of the software lifecycle.},
keywords={Software as a service;Computer architecture;Checkpointing;Fault tolerance;Fault tolerant systems;Ontologies},
doi={10.1109/BigDataCongress.2015.131},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{8706733,
author={Usachev, V. A. and Voronova, L. I. and Voronov, V. I. and Zharov, I. A. and Strelnikov, V. G.},
booktitle={2019 Systems of Signals Generating and Processing in the Field of on Board Communications},
title={Neural Network Using to Analyze the Results of Environmental Monitoring of Water},
year={2019},
volume={},
number={},
pages={1-6},
abstract={The acuteness of the environmental tracking problem is constantly growing. Currently, environmental issues are analyzed using big data. Many open data sources (Kaggle, Open Data Portal of the Russian Federation, etc.) contain a variety of environmental information. Based on the data and using the tools for analyzing big data and machine learning, a system has been developed that simulates the state of water quality in the Moscow waters. On the basis of the indicators obtained, the neural network was trained, which classifies the state of the reservoir into good and deviant.},
keywords={Reservoirs;Intelligent sensors;Temperature measurement;Data warehouses;Conductivity;big data;neural network;machine learning;water quality monitoring;sensors},
doi={10.1109/SOSG.2019.8706733},
ISSN={},
month={March},}
@ARTICLE{8023834,
author={Zhao, Guoshuai and Liu, Tianlei and Qian, Xueming and Hou, Tao and Wang, Huan and Hou, Xingsong and Li, Zhetao},
journal={IEEE Transactions on Services Computing},
title={Location Recommendation for Enterprises by Multi-Source Urban Big Data Analysis},
year={2020},
volume={13},
number={6},
pages={1115-1127},
abstract={Effective location recommendation is an important problem in both research and industry. Much research has focused on personalized recommendation for users. However, there are more uses such as site selection for firms and factories. In this study, we try to solve site selection problem by recommending some locations satisfying special requirements. There are many factors affecting it, including functions of architecture, building cost, pollution discharge etc. We focus on the specific site selection of meteorological observation stations in this paper with leveraging the factors of functions of architecture and building cost from multi-source urban big data. We consider not only recommending the locations that can provide more accurate prediction and cover more areas, but also minimizing the cost of building new stations. We design an extensible two-stage framework for the station placing including prediction model and recommendation model. It is very convenient for executives to add more real-life factors into our approach. We have some empirical findings and evaluate the proposed approach using the real meteorological data of Shaanxi province, China. Experiment results show the better performance of our approach than existing commonly used methods.},
keywords={Predictive models;Computational modeling;Atmospheric modeling;Air quality;Computer architecture;Recommender systems;Big Data;Big data;location recommendation;recommender system;site selection;urban computing},
doi={10.1109/TSC.2017.2747538},
ISSN={1939-1374},
month={Nov},}
@ARTICLE{7480394,
author={Moyne, James and Samantaray, Jamini and Armacost, Michael},
journal={IEEE Transactions on Semiconductor Manufacturing},
title={Big Data Capabilities Applied to Semiconductor Manufacturing Advanced Process Control},
year={2016},
volume={29},
number={4},
pages={283-291},
abstract={As requirements on data volumes, rates, quality, merging, and analytics increase exponentially in the digital universe, semiconductor manufacturers are faced with a need for new approaches to data management and use across the Fab. These are often termed “big data” challenges. In our industry big data solutions will be key to scaling advanced process control (APC) solutions to finer levels of control and diagnostics. However the main impact will be to better enable more effective predictive technologies such as predictive maintenance (PdM), virtual metrology and yield prediction, all of which utilize data from traditional APC capabilities that include fault detection and classification and run-to-run control. PdM represents one area where big data solutions are generating significant benefits across a variety of process types. Moving to big data solutions involves addressing the aforementioned requirements either with enhancements of existing systems or moving to more big data friendly platforms. Big data friendly platforms applied to APC systems provide quantifiable cost-of-ownership and speed improvements, thereby better enabling high quality prediction solutions. Initially, big data solutions will largely be delegated to off-line and on-time critical tasks; over the longer term these big data solutions will increasingly be leveraged for time critical and real-time capabilities.},
keywords={Big data;Real-time systems;Process control;Predictive control;Database systems;Advanced process control;big data;database management systems;hadoop;prediction methods;predictive control;predictive maintenance;process control;virtual metrology;yield prediction},
doi={10.1109/TSM.2016.2574130},
ISSN={1558-2345},
month={Nov},}
@ARTICLE{7063262,
author={Alahakoon, Damminda and Yu, Xinghuo},
journal={IEEE Transactions on Industrial Informatics},
title={Smart Electricity Meter Data Intelligence for Future Energy Systems: A Survey},
year={2016},
volume={12},
number={1},
pages={425-436},
abstract={Smart meters have been deployed in many countries across the world since early 2000s. The smart meter as a key element for the smart grid is expected to provide economic, social, and environmental benefits for multiple stakeholders. There has been much debate over the real values of smart meters. One of the key factors that will determine the success of smart meters is smart meter data analytics, which deals with data acquisition, transmission, processing, and interpretation that bring benefits to all stakeholders. This paper presents a comprehensive survey of smart electricity meters and their utilization focusing on key aspects of the metering process, different stakeholder interests, and the technologies used to satisfy stakeholder interests. Furthermore, the paper highlights challenges as well as opportunities arising due to the advent of big data and the increasing popularity of cloud environments.},
keywords={Smart meters;Electricity;Smart grids;Real-time systems;Informatics;Power quality;Data analysis;Smart meters;Smart Grids;Data analytics;Cloud computing;Artificial Intelligence;Machine learning;Big Data;Automated Meter Infrastructure;Privacy;Internet of Things;Artificial intelligence;automated meter infrastructure;big data;cloud computing;data analytics;Internet of Things (IoT);machine learning;privacy;smart grids (SGs);smart meters},
doi={10.1109/TII.2015.2414355},
ISSN={1941-0050},
month={Feb},}
@ARTICLE{8125557,
author={Alwasel, Khaled and Li, Yinhao and Jayaraman, Prem Prakash and Garg, Saurabh and Calheiros, Rodrigo N. and Ranjan, Rajiv},
journal={IEEE Cloud Computing},
title={Programming SDN-Native Big Data Applications: Research Gap Analysis},
year={2017},
volume={4},
number={5},
pages={62-71},
abstract={Software-Defined Networking has involved as a preferred abstraction for sharing network resources within a cloud datacenter in response to simultaneous data retrieval and computation demands from around the world. However, several research challenges need to be investigated before SDN powered-cloud datacenters are able to efficiently process big data as defined by its “4V” characteristics. Big data enabled-systems have to be able to respond to concurrent requests and allocate computing (e.g., virtual machine instances), storage (e.g., disk space) and networking (e.g., bandwidth) resources efficiently and effectively.},
keywords={Cloud computing;Containers;Big Data applications;Quality of service;Yarn;Resource management;Software Defined Big Data Cloud;cross-layer QoS management},
doi={10.1109/MCC.2017.4250934},
ISSN={2325-6095},
month={Sep.},}
@INPROCEEDINGS{7510724,
author={Zhao, Yeru and Huang, Zhiwu and Liu, Weirong and Peng, Jun and Zhang, Qianqian},
booktitle={2016 IEEE International Conference on Communications (ICC)},
title={A combinatorial double auction based resource allocation mechanism with multiple rounds for geo-distributed data centers},
year={2016},
volume={},
number={},
pages={1-6},
abstract={With the explosion of application of big data, it becomes inefficient and infeasible to process big data stream by using conventional data service infrastructure and management system. Cloud computing platform having multiple geo-distributed data centers is expected to be the most efficient platform to process the big data stream. In this paper, a multi-round combinational double auction based mechanism is proposed to allocate the resources of geo-distributed data centers to multiple users with large data stream processing tasks. This mechanism combines the advantages of combinatorial auction and double auction. In the proposed mechanism, the different types of VMs can be integrated into a bundle to be bid. The auction is double and conducted from both users and data centers. Different from existed double auctions, the QoS level is taken into consideration. In addition, the multiple rounds mode is adopted, so the failed users and data centers have the chance to adjust bids and asks to participate next auction round, increasing the ratio of successful transactions. Simulation results validate the effectiveness of the proposed mechanism.},
keywords={Cloud computing;Quality of service;Resource management;Big data;Computational modeling;Distributed databases;Pricing;Big data;combinatorial double auction;cloud computing;geo-distributed data centers;resource allocation},
doi={10.1109/ICC.2016.7510724},
ISSN={1938-1883},
month={May},}
@ARTICLE{8808169,
author={Garcia, Antonio J. and Toril, Matias and Oliver, Pablo and Luna-Ramirez, Salvador and Garcia, Rafael},
journal={IEEE Communications Magazine},
title={Big Data Analytics for Automated QoE Management in Mobile Networks},
year={2019},
volume={57},
number={8},
pages={91-97},
abstract={Over the last years, there has been a significant increase in the number of services in mobile networks. This trend has forced operators to change their network management processes to ensure adequate user QoE, instead of adequate QoS. As a result, customer experience management is now a critical task for mobile network operators, who demand tools for QoE monitoring on an individual user basis. With the latest advances in information technologies, the newest TMA solutions can leverage the huge amount of information available from network elements and interfaces in mobile networks. However, data processing algorithms in these tools are still to be defined. In this work, we review the shortcomings and challenges in the use of TMA applications in mobile networks, and how these can be empowered by big data analytics. For this purpose, a methodology to validate a generic big-data-driven TMA framework with user terminal agents in a real cellular network is outlined. A use case is presented to show the potential and limitations of these applications for monitoring end-user QoE in a live LTE network.},
keywords={Quality of experience;Monitoring;Measurement;Big Data;Network interfaces;IP networks},
doi={10.1109/MCOM.2019.1800374},
ISSN={1558-1896},
month={August},}
@ARTICLE{9040257,
author={Alkurd, Rawan and Abualhaol, Ibrahim and Yanikomeroglu, Halim},
journal={IEEE Communications Magazine},
title={Big-Data-Driven and AI-Based Framework to Enable Personalization in Wireless Networks},
year={2020},
volume={58},
number={3},
pages={18-24},
abstract={Current communication networks use design methodologies that prevent the realization of maximum network efficiency. In the first place, while users' perception of satisfactory service diverges widely, current networks are designed to be a "universal fit," where they are generally over-engineered to deliver services appealing to all types of users. Also, current networks lack user-level data cognitive intelligence that would enable fast personalized network decisions and actions through automation. Thus, in this article, we propose the utilization of AI, big data analytics, and real-time non-intrusive user feedback in order to enable the personalization of wireless networks. Based on each user's actual QoS requirements and context, a multi-objective formulation enables the network to micro-manage and optimize the provided QoS and user satisfaction levels simultaneously. Moreover, in order to enable user feedback tracking and measurement, we propose a user satisfaction model based on the zone of tolerance concept. Furthermore, we propose a big-data-driven and AI-based personalization framework to integrate personalization into wireless networks. Finally, we implement a personalized network prototype to demonstrate the proposed personalization concept and its potential benefits through a case study. The case study shows how personalization can be realized to enable the efficient optimization of network resources such that certain requirement levels of user satisfaction and revenue in the form of saved resources are achieved.},
keywords={Quality of service;Wireless networks;Real-time systems;Artificial intelligence;Big Data},
doi={10.1109/MCOM.001.1900533},
ISSN={1558-1896},
month={March},}
@ARTICLE{7523405,
author={Wang, Jianmin and Song, Shaoxu and Zhu, Xiaochen and Lin, Xuemin and Sun, Jiaguang},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Efficient Recovery of Missing Events},
year={2016},
volume={28},
number={11},
pages={2943-2957},
abstract={For various entering and transmission issues raised by human or system, missing events often occur in event data, which record execution logs of business processes. Without recovering the missing events, applications such as provenance analysis or complex event processing built upon event data are not reliable. Following the minimum change discipline in improving data quality, it is also rational to find a recovery that minimally differs from the original data. Existing recovery approaches fall short of efficiency owing to enumerating and searching over all of the possible sequences of events. In this paper, we study the efficient techniques for recovering missing events. According to our theoretical results, the recovery problem appears to be NP-hard. Nevertheless, advanced indexing, pruning techniques are developed to further improve the recovery efficiency. The experimental results demonstrate that our minimum recovery approach achieves high accuracy, and significantly outperforms the state-of-the-art technique for up to five orders of magnitudes improvement in time performance.},
keywords={Business;Petri nets;Engineering drawings;Indexes;Routing;Sun;Data mining;Data repairing;event data processing;petri net},
doi={10.1109/TKDE.2016.2594785},
ISSN={1558-2191},
month={Nov},}
@INPROCEEDINGS{7841598,
author={Vakilinia, Shahin and Zhang, Xinyao and Qiu, Dongyu},
booktitle={2016 IEEE Global Communications Conference (GLOBECOM)},
title={Analysis and Optimization of Big-Data Stream Processing},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Big data processing is rapidly growing in recent years due to the immediate demanding of many applications. This growth compels industries to leverage scheduling in order to optimally allocate the resources to the big data streams which requires data-driven big data analysis. Moreover, optimal scheduling of big data stream process should guarantee the QoS requirements of computing tasks. Execution deadlines of tasks within the streams is specified as one of the most significant QoS factors. In this paper, we study the scheduling and execution of big data stream processes. First, a queueing theory approach to the modeling of the streams as a collection of sequential and parallel tasks is proposed. It is assumed that heterogeneous threads are required to handle various big data tasks such as processing, storing and searching which may have quite general service time distributions. Then, with the proposed model, an optimization problem is defined to minimize the total number of resources required to serve the big data streams while guaranteeing the QoS requirements of their tasks. An algorithm is also proposed to mitigate the complexity order of the optimization problem. The objective of this research is to minimize the stream processing resources in terms of threads with constraints over the task waiting time of the application tasks. We apply the proposed scheduling algorithm to Apache Storm, a distributed real-time computation platform, to optimize the cloud resource requirements. The experiment results validate our analysis.},
keywords={Instruction sets;Big data;Delays;Cloud computing;Analytical models;Optimization;Queueing analysis},
doi={10.1109/GLOCOM.2016.7841598},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7825175,
author={Bu, Aixian and Wang, Liguo},
booktitle={2016 International Conference on Smart City and Systems Engineering (ICSCSE)},
title={Research on the Rule of Acupuncture and Moxibustion for Treatment of Obesity Based on Data Mining},
year={2016},
volume={},
number={},
pages={602-605},
abstract={At present, big data has been applied to all aspects of life, for large data research and data mining information can be greatly convenient for people's production and life. With the continuous development of domestic medical treatment, people use acupuncture technique of obesity has become possible, the in-depth study of data mining provides convenient for medical workers to provide the better service for the people. This will not only improve the quality of people's lives, but also effectively improve the people's physical condition. Data mining is a new technology which is currently in the field of computer, this paper uses the basic principle of data mining will be applied to the study of the treatment of obesity, lay a solid theoretical foundation and strong support for the successful treatment of obesity patients. Different from the previous research methods, the method based on data mining not only improves the accuracy of the treatment of disease, but also effectively shorten the time of treatment, reduce the economic consumption. This paper used the good classification performance of K-means algorithm and parallel genetic algorithm (GA) with high robustness of the comprehensive, based on a comprehensive understanding of the acupuncture treatment of obesity, building the basic framework of intelligent system. In the subsequent learning process combined with the actual situation to verify the practicality of the model, confirmed the feasibility of the model, to provide the basis and reference for the follow-up medical treatment program.},
keywords={Obesity;Clustering algorithms;Data mining;Algorithm design and analysis;Genetic algorithms;Diseases;Classification algorithms;acupuncture technique;data mining;regularity of obesity;K-means algorithm;parallel genetic algorithm},
doi={10.1109/ICSCSE.2016.0162},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9365869,
author={Nolack Fote, Fabrice and Mahmoudi, Saïd and Roukh, Amine and Ahmed Mahmoudi, Sidi},
booktitle={2020 5th International Conference on Cloud Computing and Artificial Intelligence: Technologies and Applications (CloudTech)},
title={Big Data Storage and Analysis for Smart Farming},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Smart Farming has always been referred to as agriculture, but nowadays, that is no longer the case. Today, Smart farming is made up of Precision Agriculture (PA) and Precision Livestock Farming (PLF). Big Data technologies and algorithms can be relevant for managing and monitoring data related to any farm. Precision livestock farming concerns genetics, animal welfare, animal nutrition, reproduction, species protection and animal health. This paper presents a general overview of Big Data tools that can be applied in a smart farming application. New Technologies are offering many tools used to facilitate the management of data collection, risk minimization, climate change anticipation, secure storage and analysis, etc. The main purpose of Big Data tools is to increase productions in order to offer higher quantities while ensuring higher quality products. However, they remain some issues that need to be accomplished.},
keywords={Animals;Climate change;Big Data;Reliability;Expert systems;Monitoring;Digital agriculture;Precision engineering;Smart Farming;Big Data;Precision Agriculture;Precision Livestock Farming},
doi={10.1109/CloudTech49835.2020.9365869},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9298089,
author={Liu, Qian and Hauswirth, Manfred},
booktitle={2020 11th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)},
title={A Provenance Meta Learning Framework for Missing Data Handling Methods Selection},
year={2020},
volume={},
number={},
pages={0349-0358},
abstract={Missing data is a big problem in many real-world data sets and applications, which can lead to wrong or misleading results of analyses and lower quality and confidence in the results. A large number of missing data handling methods have been proposed in the research community but there exists no universally single best method which can handle all the missing data problems. To select the right method for a specific missing data handling problem, it usually depends on multiple inter-twined factors. To alleviate this methods selection problem, in this paper, we propose a Provenance Meta Learning Framework to simplify this process. We conducted an extensive literature review over 118 missing data handling method survey papers from 2000 to 2019. With this review, we analyse 9 influential factors and 12 selection criteria for missing data handling methods and further perform a detailed analysis of 6 popular missing data handling methods (4 machine learning methods, i.e., KNN Imputation (KNNI), Weighted KNN Imputation (WKNNI), K Means Imputation (KMI), and Fuzzy KMI (FKMI), and 2 ad-hoc methods, i.e., Median/Mode Imputation (MMI) and Group/Class MMI (CMMI)). We focus on missing data handling methods selection for 3 different classification techniques, i.e., C4.5, KNN, and RIPPER. In our evaluations, we adopt 25 real world data sets from KEEL and UCI data sets repositories. Our Provenance Meta Learning Framework suggests that using KNNI to handle missing values when missing data mechanism is Missing Complete At Random (MCAR), missing data pattern is uni-attribute missing data pattern, or monotone missing data pattern, missing data rate is within [1%,5%], number of class labels is 2, sample size is no more than 10'000, since it can keep classification performance better and have higher imputation accuracy and imputation exhaustiveness than all the other 5 missing data handling methods when subsequent classification methods are KNN or RIPPER.},
keywords={Data handling;Measurement;Data mining;Bibliographies;Training;Machine learning;Time measurement;Provenance Meta Learning;Meta Rule Induction;Automated Missing Data Handling Methods Selection;Missing Data Handling/Treatment},
doi={10.1109/UEMCON51285.2020.9298089},
ISSN={},
month={Oct},}
@ARTICLE{7978034,
author={Xu, Xiaolong and Liu, Xinxin and Liu, Xiaoxiao and Sun, Yanfei},
journal={Journal of Systems Engineering and Electronics},
title={Truth finder algorithm based on entity attributes for data conflict solution},
year={2017},
volume={28},
number={3},
pages={617-626},
abstract={The Internet now is a large-scale platform with big data. Finding truth from a huge dataset has attracted extensive attention, which can maintain the quality of data collected by users and provide users with accurate and efficient data. However, current truth finder algorithms are unsatisfying, because of their low accuracy and complication. This paper proposes a truth finder algorithm based on entity attributes (TFAEA). Based on the iterative computation of source reliability and fact accuracy, TFAEA considers the interactive degree among facts and the degree of dependence among sources, to simplify the typical truth finder algorithms. In order to improve the accuracy of them, TFAEA combines the one-way text similarity and the factual conflict to calculate the mutual support degree among facts. Furthermore, TFAEA utilizes the symmetric saturation of data sources to calculate the degree of dependence among sources. The experimental results show that TFAEA is not only more stable, but also more accurate than the typical truth finder algorithms.},
keywords={Algorithm design and analysis;Reliability;Internet;Telecommunications;Big Data;Data models;truth finder;data reliability;entity attribute;data conflict},
doi={10.21629/JSEE.2017.03.21},
ISSN={1004-4132},
month={June},}
@INPROCEEDINGS{7184912,
author={Munger, T. and Desa, Subhas and Wong, Chris},
booktitle={2015 IEEE First International Conference on Big Data Computing Service and Applications},
title={The Use of Domain Knowledge Models for Effective Data Mining of Unstructured Customer Service Data in Engineering Applications},
year={2015},
volume={},
number={},
pages={427-438},
abstract={Despite the fact that enterprises are routinely collecting massive amounts of data from customers, only a relatively small body of knowledge engineering (KE) work has addressed methods and application of KE to the design, development, and maintenance of engineering systems and products. A major challenge when applying KE to such applications is that the data is often unstructured and in the form of text exchanges between the customer and the enterprise. While the importance of modelling domain knowledge in order to produce meaningful results from mining unstructured data has been recognized, most approaches are based primarily on the linguistic structure of the text and keyword taxonomies. These approaches share the common issue that the knowledge extraction results are often not properly structured for solving the engineering problem of interest and, therefore, require manual post-processing before they can be applied. Our hypothesis is that the a priori modelling of the engineering problem of interest is crucial for both (1) efficient (rapid) collection, representation, and structuring of domain knowledge, and (2) the proper integration of domain knowledge with analytical KE methods in order facilitate the extraction of useful knowledge. In order to validate our hypothesis, we apply this approach to the important real-world engineering problem of monitoring the occurrence of product failure modes, and thereby product quality, using customer support cases. In order to translate the free-form text provided by the customer into engineering failure modes we use two methods from engineering design, the Function Analysis System Technique (FAST) and Failure Modes and Effects Analysis (FMEA), to provide the necessary domain knowledge model. This model then drives the collection, representation, and structuring of the failure modes for the product of interest. These failure modes are used as the class labels when applying data mining classification techniques (e.g., Support Vector Machine) to the support case data. The labelled support case data then can be aggregated by failure mode in order to compute a number of failure mode metrics that can be used to monitor product quality. We have demonstrated our approach to monitor the quality of a network security product at a large computer networking company using a data set of 100,000 customer support cases.},
keywords={Knowledge engineering;Monitoring;Data mining;Measurement;Product design;Quality assessment;Computational modeling;domain knowledge modelling;data mining;knowledge engineering},
doi={10.1109/BigDataService.2015.46},
ISSN={},
month={March},}
@INPROCEEDINGS{8034975,
author={Siriweera, T.H. Akila S. and Paik, Incheon and Kumara, Banage T.G.S.},
booktitle={2017 IEEE International Conference on Services Computing (SCC)},
title={QoS and Customizable Transaction-Aware Selection for Big Data Analytics on Automatic Service Composition},
year={2017},
volume={},
number={},
pages={116-123},
abstract={As services for Big Data Analysis (BDA) become prevalent, analysis services with intelligence and autonomy using automatic service composition (ASC) show bright prospects in the BDA market. Selection is one of the most important phases of successful ASC process. Moreover, it became competitive with the rise of demand for the services and criticalness of the BDA process. It is a challenge to accomplish a successful uninterruptable composition while serving diverse custom selection requirements. In the case of failure, it results in complete loss of time and resources. Traditional approaches are not applicable to handle failures during long running transactions. Instead, compensation suggests to being an error recovery. Therefore, analytics transactions scheduled as a composition of a set of compensable transactions. However, compensable services are a higher price and consume more time. Moreover, consumers equipped with diverse requirements. It is necessary to guarantee the critical stages of workflow using compensable services rather than whole workflow. Therefore, we proposed customizable Transaction and QoS-aware service selection approach under five user custom settings based on genetic algorithm (GA) to address above concerns. QoS-awareness facilitated by multi-objective QoS criteria and GA is used for multivariate optimization. We conducted a thorough evaluation, and it shows proposed method effectively and efficiently reach the global optimal of the overall selection criteria.},
keywords={Quality of service;Web services;Automation;Genetic algorithms;Big Data;Planning;Optimization;Automatic Service Composition;Web service Selection;Big Data Analytics;Transactional Web Services;Genetic Algorithm},
doi={10.1109/SCC.2017.22},
ISSN={2474-2473},
month={June},}
@INPROCEEDINGS{8433715,
author={Globa, Larysa and Kurdecha, Vasyl and Ishchenko, Ivan and Zakharchuk, Andrii and Kunieva, Nataliia},
booktitle={2018 IEEE International Black Sea Conference on Communications and Networking (BlackSeaCom)},
title={The Intellectual IoT-System for Monitoring the Base Station Quality of Service},
year={2018},
volume={},
number={},
pages={1-5},
abstract={The operator needs to process large volumes of data in the Internet of Things systems. There is not enough simple and statistical data analysis methods when operating with a large volume of data. The paper deals with the approach to big data analyses that use of fuzzy logic and fuzzy knowledge base for big data processing. The mathematical methods that allow to obtain efficiently the required solution in conditions of high performance requirements are considered. The discussed mathematical methods were tested for the base station quality of service determination. The proposed approach is efficient for big data processing.},
keywords={Base stations;Quality of service;Fuzzy logic;Telecommunications;Computer architecture;Monitoring;Big Data;Internet of Things;fuzzy logic;fuzzy knowledge base;data processing;base station quality of service},
doi={10.1109/BlackSeaCom.2018.8433715},
ISSN={},
month={June},}
@INPROCEEDINGS{8621994,
author={Liang, Junjie and Hu, Jinlong and Dong, Shoubin and Honavar, Vasant},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Top-N-Rank: A Scalable List-wise Ranking Method for Recommender Systems},
year={2018},
volume={},
number={},
pages={1052-1058},
abstract={We propose Top-N-Rank, a novel family of list-wise Learning-to-Rank models for reliably recommending the N top-ranked items. The proposed models optimize a variant of the widely used cumulative discounted gain (DCG) objective function which differs from DCG in two important aspects: (i) It limits the evaluation of DCG only on the top N items in the ranked lists, thereby eliminating the impact of low-ranked items on the learned ranking function; and (ii) it incorporates weights that allow the model to leverage multiple types of implicit feedback with differing levels of reliability or trustworthiness. Because the resulting objective function is non-smooth and hence challenging to optimize, we consider two smooth approximations of the objective function, using the traditional sigmoid function and the rectified linear unit (ReLU). We propose a family of learning-to-rank algorithms (Top-N-Rank) that work with any smooth objective function. Then, a more efficient variant, Top-N-Rank.ReLU, is introduced, which effectively exploits the properties of ReLU function to reduce the computational complexity of Top-N-Rank from quadratic to linear in the average number of items rated by users. The results of our experiments using two widely used benchmarks, namely, the MovieLens data set and the Amazon Video Games data set demonstrate that: (i) The "top-N truncation" of the objective function substantially improves the ranking quality of the top N recommendations; (ii) using the ReLU for smoothing the objective function yields significant improvement in both ranking quality as well as runtime as compared to using the sigmoid; and (iii) Top-N-Rank.ReLU substantially outperforms the well-performing list-wise ranking methods in terms of ranking quality.},
keywords={Linear programming;Reliability;Smoothing methods;Training;Computational complexity;Big Data;Computational modeling;list-wise ranking;top n recommendation;learning to rank;latent factor model;rectifier function},
doi={10.1109/BigData.2018.8621994},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8511976,
author={Qaim, Waleed Bin and Ozkasap, Oznur},
booktitle={2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)},
title={DRAW: Data Replication for Enhanced Data Availability in IoT-based Sensor Systems},
year={2018},
volume={},
number={},
pages={770-775},
abstract={Internet of Things (IoT) technology is gaining increasing popularity with the ubiquity of the Internet. It has the potential to connect real-world physical objects to the Internet to make them readily accessible to users by deploying Wireless Sensor Networks (WSNs). However, WSNs face various challenges due to the nature of deployment and limited resources of sensor nodes. WSNs may also suffer from node failures as well as local memory shortages which result in significant amount of data loss. Data replication is a promising technique to preserve valuable sensed data in the network. In this paper, we propose DRAW, a fully distributed hop-by-hop data replication technique for IoT-based wireless sensor systems. DRAW ensures maximum data availability under high node failures to preserve data. Our extensive simulation results show that compared to a state-of-the- art technique, DRAW improves data availability and average replicas created in the network with a maximum gain of about 15% and 18%, respectively. Furthermore, DRAW provides a better replica spread which determines the quality of data dissemination in the network.},
keywords={Wireless sensor networks;Distributed databases;Internet of Things;Fault tolerance;Monitoring;Data dissemination;Internet-of-things-(IoT);Wireless-Sensor-Networks;Data-Replication;Data-Availability;Fault-tolerance},
doi={10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00133},
ISSN={},
month={Aug},}
@ARTICLE{9234761,
author={Epelde, Gorka and Beristain, Andoni and Alvarez, Roberto and Arrúe, Mónica and Ezkerra, Iker and Belar, Oihana and Bilbao, Roberto and Nikolic, Gorana and Shi, Xi and De Moor, Bart and Mulvenna, Maurice},
journal={IEEE Instrumentation & Measurement Magazine},
title={Quality of data measurements in the big data era: Lessons learned from MIDAS project},
year={2020},
volume={23},
number={7},
pages={18-24},
abstract={In recent years, digitalization of traditional manual processes with a tendency towards a sensorized world and person-generated information streams has led to a massive availability and exponential generation of heterogeneous data in most areas of life. This has been facilitated by the cost reduction and capability improvements of Information and Communications Technology (ICT) for storage, processing and transmission.},
keywords={Data integrity;Information and communications technology;Data storage systems;Data visualization;Decision making;Measurement uncertainty},
doi={10.1109/MIM.2020.9234761},
ISSN={1941-0123},
month={Oct},}
@ARTICLE{8306455,
author={Rathore, Punit and Ghafoori, Zahra and Bezdek, James C. and Palaniswami, Marimuthu and Leckie, Christopher},
journal={IEEE Transactions on Cybernetics},
title={Approximating Dunn’s Cluster Validity Indices for Partitions of Big Data},
year={2019},
volume={49},
number={5},
pages={1629-1641},
abstract={Dunn's internal cluster validity index is used to assess partition quality and subsequently identify a “best” crisp partition of n objects. Computing Dunn's index (DI) for partitions of n p-dimensional feature vector data has quadratic time complexity O(pn2), so its computation is impractical for very large values of n. This note presents six methods for approximating DI. Four methods are based on Maximin sampling, which identifies a skeleton of the full partition that contains some boundary points in each cluster. Two additional methods are presented that estimate boundary points associated with unsupervised training of one class support vector machines. Numerical examples compare approximations to DI based on all six methods. Four experiments on seven real and synthetic data sets support our assertion that computing approximations to DI with an incremental, neighborhood-based Maximin skeleton is both tractable and reliably accurate.},
keywords={Indexes;Big Data;Clustering algorithms;Measurement;Skeleton;Cybernetics;Support vector machines;Approximate Dunn’s indices;big data;boundary point estimation;data skeleton;Dunn’s index (DI);internal cluster validity;Maximin sampling},
doi={10.1109/TCYB.2018.2806886},
ISSN={2168-2275},
month={May},}
@INPROCEEDINGS{8360353,
author={Sarabia-Jacome, David and Belsa, Andreu and Palau, Carlos E. and Esteve, Manuel},
booktitle={2018 IEEE International Conference on Cloud Engineering (IC2E)},
title={Exploiting IoT Data and Smart City Services for Chronic Obstructive Pulmonary Diseases Risk Factors Monitoring},
year={2018},
volume={},
number={},
pages={351-356},
abstract={Chronic Obstructive Pulmonary Disease (COPD) is a wide concept to describe a group of diseases which affect a normal respiratory function and cause a considerable impact on patients' life quality and healthcare costs. There are few IoT systems in the present literature that are focused on monitoring and management of COPD patients, but they are not focused on the vast amount of data that IoT generates in a large scale deployment, and the integration of Smart City services. For these reasons, this paper presents an innovative system based on Cloud Computing and Big Data technologies to integrate Smart City services into a large scale scenario. To do so, this system proposes a Big Data architecture based on Apache Spark libraries to provide data integration, storage, descriptive and predictive analysis. Also, the system provides a web interface application where users can visualize the data analysis results. They show that the data enrichment function performed on the Big Data architecture provides more information about the environment to improve decisionmaking. This way, the system helps COPD patients to get more involved into decision making process and promote an active and healthy life by recommending the least polluted area to performance their daily activities.},
keywords={Monitoring;Smart cities;Big Data;Cloud computing;Computer architecture;Sensors;Diseases;component;Internet of Things;Big Data;Chronic Obstructive Pulmonary Disease;Smart City},
doi={10.1109/IC2E.2018.00060},
ISSN={},
month={April},}
@INPROCEEDINGS{6944539,
author={Park, Sungmee and Jayaraman, Sundaresan},
booktitle={2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
title={A transdisciplinary approach to wearables, big data and quality of life},
year={2014},
volume={},
number={},
pages={4155-4158},
abstract={Today, the term “wearable” goes beyond the traditional definition of clothing; it refers to an accessory that enables personalized mobile information processing. We define the concept of wearables, present their attributes and discuss their role at the core of an ecosystem for harnessing big data. We discuss the concept of a meta-wearable and propose a transdisciplinary approach to transform the field and enhance the quality of life for everyone.},
keywords={Biomedical monitoring;Big data;Clothing;Monitoring;Medical services;Fabrics},
doi={10.1109/EMBC.2014.6944539},
ISSN={1558-4615},
month={Aug},}