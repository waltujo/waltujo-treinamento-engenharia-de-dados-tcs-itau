@INPROCEEDINGS{7765504,
author={Zheng, Yu},
booktitle={2016 IEEE 24th International Requirements Engineering Conference (RE)},
title={Urban Computing: Tackling Urban Challenges Using Big Data},
year={2016},
volume={},
number={},
pages={3-3},
abstract={Urban computing is a process of acquisition, integration, and analysis of big and heterogeneous data generated by a diversity of sources in cities to tackle urban challenges, e.g. air pollution, energy consumption and traffic congestion. Urban computing connects unobtrusive and ubiquitous sensing technologies, advanced data management and analytics models, and novel visualization methods, to create win-win-win solutions that improve urban environment, human life quality, and city operation systems. Urban computing is an inter-disciplinary field where computer science meets urban planning, transportation, economy, the environment, sociology, and energy, etc., in the context of urban spaces. In this talk, I will overview the framework of urban computing, discussing its key challenges and methodologies from computer science's perspective. This talk will also present a diversity of urban computing applications, ranging from big data-driven environmental protection to transportation, from urban planning to urban economy. The research has been not only published at prestigious conferences but also deployed in the real world. More details can be found on the homepage of urban computing1.},
keywords={Computer science;Conferences;Big data;Urban planning;Business;Requirements engineering},
doi={10.1109/RE.2016.14},
ISSN={2332-6441},
month={Sep.},}
@INPROCEEDINGS{9095877,
author={Li, Song and Ning, Sun and Yezhou, Yao and Jingjing, Tian and Wenxue, Zhang and Liang, Chi},
booktitle={2019 2nd International Conference on Safety Produce Informatization (IICSPI)},
title={Application of Data Mining Technology in the Recall of Defective Automobile Products in China ——A Typical Case of the Construction of Digital China},
year={2019},
volume={},
number={},
pages={541-545},
abstract={According to multisource quality safety data of defective automobile products, key quality safety factors of defective automobile products are extracted, a defect information indicator system for automobile products is systematically constructed and a correlated graph is established between quality safety factors. Based on the optimization and correlation of the quality safety factor indicator system, Big Data technology is used to design a data structure for multisource quality safety information cluster, develop a data platform for the defect information analysis of automobile products and achieve information clustering and correlation analysis based on multisource quality safety data, providing technical support for the recall management of defective automobile products.},
keywords={Automobiles;Safety;Big Data;Government;Personnel;Data mining;automobile recall;data mining;information cluster},
doi={10.1109/IICSPI48186.2019.9095877},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9196235,
author={Wang, Ya and Yang, Yanmei},
booktitle={2020 International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)},
title={Research on Rural Development Innovation and Development Based on Big Data},
year={2020},
volume={},
number={},
pages={83-86},
abstract={Big data provides brand-new means and tools for comprehensive rural revitalization. It is a powerful support for the comprehensive rural revitalization and an important way to achieve the integrated development of the digital economy and rural revitalization and improve the quality and level of rural revitalization. At present, some places in China have carried out fruitful practical explorations on the promotion of rural revitalization by big data, which has vigorously promoted rural revitalization in depth. However, in the course of practice, there are also imperfect supporting measures, weak infrastructure, lack of applied talents, and data information. There are outstanding problems, such as institutional obstacles, in sharing and integration, and further innovations are needed in platform construction, system improvement, and personnel training to fully play the role of big data in rural revitalization.},
keywords={Big Data;Acceleration;Industries;Q measurement;Conferences;Artificial intelligence;Internet of Things;big data;rural revitalization;innovation},
doi={10.1109/ICBAIE49996.2020.00024},
ISSN={},
month={June},}
@INPROCEEDINGS{9150284,
author={Tan, Mian and Wang, Lin and Feng, Fujian and Xia, Dawen},
booktitle={2020 International Conference on Big Data and Informatization Education (ICBDIE)},
title={An Error Feedback Method to Enhance Teaching Ability of Young Teachers for Classroom Teaching Process},
year={2020},
volume={},
number={},
pages={306-309},
abstract={Teaching ability is a hot issue of teachers' team construction, and plays an important role in establishing teachers' teaching quality evaluation system, changing classroom teaching models and establishing teaching supervision and operation mechanisms. According to the main problems in teaching management and classroom teaching process, such as the difficulty in analyzing teaching data and quantitative evaluation, this paper proposes an error feedback method of teaching information based on classroom teaching process data, which is generated by teachers, students, teaching administrators and teaching supervisors. This method can generate teaching error information by comparing teaching process data and teaching evaluation data, and it is helpful for young teachers to improve teaching methods. Besides, the teaching information feedback method has important reference significance for the concrete implementation of the teaching quality monitoring system in the teaching closed loop.},
keywords={Statistical analysis;Analytical models;Data models;Training;Data science;Correlation;Teaching ability;Error feedback;Teaching evaluation information;teaching process information},
doi={10.1109/ICBDIE50010.2020.00077},
ISSN={},
month={April},}
@INPROCEEDINGS{8901246,
author={Fangqin, Zhang and Yan, Bai},
booktitle={2019 International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Research on Quality Evaluation Method of Digital Teaching Resources Design Capability Based on Cloud Computing},
year={2019},
volume={},
number={},
pages={293-298},
abstract={To improve the intelligent evaluation ability of digital teaching resource design ability and optimize the quality evaluation model, a cloud computing-based digital teaching resource design capability quality intelligent evaluation method was proposed. Digital data collection and statistical analysis methods were used for digitization. Teaching resource design capability quality statistical sample sequence sampling, using digital teaching resource design capability quantitative evaluation method in cloud computing environment, constructing big data distribution model of digital teaching resource design ability quality statistical sample sequence, combined with quantitative regression analysis method for big data characteristics Extraction and information regression analysis, constructing the feature extraction model of digital teaching resource design ability quality statistical analysis, taking the distribution status of teaching resources as the evaluation object, combined with quantitative recursive analysis method to carry out adaptive evaluation of digital teaching resource design ability quality statistical sample sequence. Adopting bus design and sensing quantitative tracking and recognition technology to carry out the system construction of digital teaching resource design capability quality, using local bus control method to carry out digital teaching resource design ability quality intelligence Estimated load instructions, to achieve design evaluation system. The test results show that the design of digital learning resources designed to assess the ability of intelligent quality assessment system has good performance, good intelligence.},
keywords={Cloud computing;Computational modeling;Education;Big Data;Feature extraction;Data models;Regression analysis;quality evaluation;digital teaching resources;cloud computing},
doi={10.1109/ICSGEA.2019.00074},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8789220,
author={Wang, Fei and Yin, Xiaohua and Pian, Ruiqi and Bai, Mingshan and Pan, Dedong and Hao, Chengbin},
booktitle={2017 International Conference on Computer Technology, Electronics and Communication (ICCTEC)},
title={Research on Error Detection Technology of Electric Power Dispatch Based on Big Data},
year={2017},
volume={},
number={},
pages={392-395},
abstract={With the gradual increase of power grid access, the risk of wrong operation of power grid dispatching is gradually increasing. It is very important to study the power dispatching error prevention technology to avoid the unstable operation of the power system and the illegal operation of the illegal network. Therefore, this paper proposes the use of big data technology and the overall scheduling work to improve the quality of power dispatching.},
keywords={Dispatching;Conferences;Processor scheduling;Data communication;Protocols;Information services;Big Data;BigData;Power dispatching;Anti error technology;smart},
doi={10.1109/ICCTEC.2017.00091},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8824850,
author={Nisha, R S and Radha, R},
booktitle={2019 3rd International Conference on Computing and Communications Technologies (ICCCT)},
title={A Systematic analysis of Data-intensive MOOCs and their key Challenges},
year={2019},
volume={},
number={},
pages={245-252},
abstract={Big Data blends modern technologies with numerous data management techniques to handle a wide variety of concerns that occur when operating with data of huge volume, variety and velocity. Big data deals with complex semi-structured and unstructured data from several sources and formats which include Social Media content in free form, data from E-commerce sites, Weather forecasting statistics, Clinical Diagnosis, Share Market Transactions and Smart Computing Environments. In the same way, big data offers substantial prospects in the discipline of Education, E-Learning and Learning Analytics. Application of big data analytics in E-Learning helps to assess the quality of Teaching, Development of Curriculum, predict learning outcomes, Career Development and Readiness, Attrition Risks and Feedback Analysis. The Massive Open Online Courses (MOOCs) have produced a major influence on E-Learning with the availability of Live and pre-recorded Lectures, Easy-to-learn Tutorials, Novel Assessment Methodologies, Quick feedback and results. In this paper, we present the various Technologies that formulate the MOOCs and address the learning paradigms and key challenges.},
keywords={Discussion forums;Education;Solid modeling;Data visualization;Visual analytics;Tools;Big Data;Big Data Analytics;Learning Analytics;MOOCs;Discussion forums;Grade prediction},
doi={10.1109/ICCCT2.2019.8824850},
ISSN={},
month={Feb},}
@INPROCEEDINGS{8258453,
author={Ye, Ailun and Chinthalapati, V. L. Raju and Serguieva, Antoaneta and Tsang, Edward},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Developing sustainable trading strategies using directional changes with high frequency data},
year={2017},
volume={},
number={},
pages={4265-4271},
abstract={Market prices are traditionally recorded in fixed time intervals. Directional Change is an alternative approach to summarize price movements in financial markets that is consistent with across all time scales. Unlike time series, directional change summarizes the big data in finance by focusing on the intrinsic time of the data. This captures deeper intrinsic data qualities and thus trading strategies based on directional change are more sustainable and less disruptive. In this paper, we propose four trading strategies using the concept of directional change and explore the combination with technical analysis. The trading strategies are tested using EUR/USD and GBP/USD high frequency FX market data. Empirical results show good performance of our trading strategies based on thresholds, and that combining with technical analysis brings further improvement.},
keywords={Market research;Data models;Currencies;Time series analysis;Analytical models;Mathematical model;FX trading;directional changes;sustainable trading strategies},
doi={10.1109/BigData.2017.8258453},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8814515,
author={Zhao, Yali and Calheiros, Rodrigo N. and Vasilakos, Athanasios V. and Bailey, James and Sinnott, Richard O.},
booktitle={2019 IEEE 12th International Conference on Cloud Computing (CLOUD)},
title={SLA-Aware and Deadline Constrained Profit Optimization for Cloud Resource Management in Big Data Analytics-as-a-Service Platforms},
year={2019},
volume={},
number={},
pages={146-155},
abstract={Discovering optimal data analytics solutions to extract value from data for better and faster decision making is essential for many application domains, especially in the big data era. Big data analytics typically requires a tremendous amount of computational resources to process large data volumes that can be very expensive and time consuming. Our research focuses on providing optimization solutions for Analytics-as-a-Service (AaaS) platforms that automatically and elastically provision cloud resources to execute queries guaranteeing Service Level Agreements (SLAs) across a range of Quality of Service (QoS) requirements. We propose admission control and resource scheduling algorithms for AaaS platforms to maximize profits while providing time-minimized query execution plans to meet user demands and expectations. To enable timely responses as required for many domains, the algorithms utilize data splitting-based query admission and resource scheduling offering parallel processing on the split datasets. Extensive experiments are conducted to evaluate the algorithm performance compared to state-of-the-art optimization algorithms. Experimental results show that our algorithms perform significantly better from a range of perspectives, including increasing query admission rates and creating higher profits, whilst supporting efficient resource configurations that are able to support big data processing demands under tight deadlines.},
keywords={Profit Optimization, Service Level Agreement, Admission Control, Resource Scheduling, Analytics-as-a-Service, Data Splitting, Big Data, Cloud Computing.},
doi={10.1109/CLOUD.2019.00034},
ISSN={2159-6190},
month={July},}
@INPROCEEDINGS{8735465,
author={Salamai, Abdullah and Hussain, Omar and Saberi, Morteza},
booktitle={2019 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS)},
title={Decision Support System for Risk Assessment Using Fuzzy Inference in Supply Chain Big Data},
year={2019},
volume={},
number={},
pages={248-253},
abstract={Currently, organisations find it difficult to design a Decision Support System (DSS) that can predict various operational risks, such as financial and quality issues, with operational risks responsible for significant economic losses and damage to an organisation's reputation in the market. This paper proposes a new DSS for risk assessment, called the Fuzzy Inference DSS (FIDSS) mechanism, which uses fuzzy inference methods based on an organisation's big data collection. It includes the Emerging Association Patterns (EAP) technique that identifies the important features of each risk event. Then, the Mamdani fuzzy inference technique and several membership functions are evaluated using the firm's data sources. The FIDSS mechanism can enhance an organisation's decision-making processes by quantifying the severity of a risk as low, medium or high. When it automatically predicts a medium or high level, it assists organisations in taking further actions that reduce this severity level.},
keywords={Fuzzy logic;Supply chains;Risk management;Big Data;Fuzzy sets;Decision support systems;Analytical models;risk Identification;risk assessment;supply chain management;emerging association patterns;mamdani fuzzy inference;big data},
doi={10.1109/HPBDIS.2019.8735465},
ISSN={},
month={May},}
@INPROCEEDINGS{9201656,
author={Zong, Ping and Jiang, Junyan and Qin, Jun},
booktitle={2020 15th International Conference on Computer Science & Education (ICCSE)},
title={Study of High-Dimensional Data Analysis based on Clustering Algorithm},
year={2020},
volume={},
number={},
pages={638-641},
abstract={With the rapid development of big data, the scale, dimensions, diversity and sparsity of high-dimensional data restrict the effectiveness of traditional clustering algorithms. This paper mainly focuses on high-dimensional data clustering. Starting from the traditional K-means clustering algorithm and subspace clustering algorithm based on self-representation model, an improved algorithm is designed and implemented based on the existing clustering algorithm in this paper. The improved algorithm has better clustering quality by combining the "distance optimization method" and the "density method" to determine the initial clustering center. The feasibility and effectiveness of improved algorithm are verified through simulation experiments.},
keywords={Clustering algorithms;Partitioning algorithms;Optimization methods;Telecommunications;Euclidean distance;Big Data;Clustering algorithms;High-dimensional data;K-means algorithm;Distance optimization;Density},
doi={10.1109/ICCSE49874.2020.9201656},
ISSN={2473-9464},
month={Aug},}
@INPROCEEDINGS{9179610,
author={Chouhan, Ashish and Prabhune, Ajinkya and Prabhuraj, Paneesh and Chaudhari, Hitesh},
booktitle={2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService)},
title={DWreck: A Data Wrecker Framework for Generating Unclean Datasets},
year={2020},
volume={},
number={},
pages={78-87},
abstract={In this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. In a typical data-analysis pipeline, data cleaning is the most cost-intensive, laborious, and time-consuming step. Unclean dataset or partially cleaned dataset can lead to incorrect training of machine learning models and result in wrong conclusions. Generally, data-scientists examine null, missing, or duplicate values, and the dataset is cleaned by removing the entire record or imputing the values. However, deleting the records, or imputing the values cannot be termed as comprehensive cleaning, as these cleaning techniques may result in a reduction in the population of data, and increased error in estimation due to biased values. For systematically cleaning an unclean dataset, it is necessary to comply with the data quality dimensions such as completeness, validity, consistency, accuracy, and conformity. The errors described as violations of expectations for completeness, accuracy, timeliness, consistency and other dimensions of data quality often impede the successful completion of information processing streams and consequently degrade the dependent business processes. Therefore, educating a data-scientist for comprehensively cleaning a raw-dataset acquired for analysis is an incremental learning process. Moreover, for extensive training on cleaning a dataset on different quality dimensions, it is necessary to provide a variety of datasets that are unclean on various data quality dimensions. Hence, in this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. The DWreck framework is designed on the principles of microservices architecture pattern. For allowing function-specific extensibility, the DWreck comprises four groups of microservices: (a) Dataset Profiling, (b) Data type Processing, (c) Counterproductive Dimensions, and (d) Miscellaneous. The orchestrator coordinates the different microservices in a complex workflow that is further split into three sub-workflows to generate an unclean (wrecked) dataset as an output. Finally, we evaluate the DWreck framework on twenty seed-datasets to generate corresponding wrecked datasets.},
keywords={Generators;Data integrity;XML;Cleaning;Tools;Databases;Pipelines;data generators;data quality dimensions;data cleaning;microservice architectures;data management},
doi={10.1109/BigDataService49289.2020.00020},
ISSN={},
month={Aug},}
@ARTICLE{8304398,
author={Xu, Chen and Zhou, Zhenyu},
journal={IEEE Wireless Communications},
title={Vehicular Content Delivery: A Big Data Perspective},
year={2018},
volume={25},
number={1},
pages={90-97},
abstract={The appearance of the Internet of Vehicles enables comfort driving experiences and content- rich multimedia services for in-vehicle users. The vehicular network provides specific scenario- centric content delivery services involving data of vehicle status, user behaviors, and environmental features. In this article, we focus on vehicular content delivery from a big data perspective. After a comprehensive review of state-of-the-art works, we elaborate the potential value of big data in vehicular information and content services by introducing several typical application scenarios. According to the data characteristics, we classify the vehicular data into three categories, that is, location-centric, user-centric, and vehicle-centric, and then illustrate an implementation of big data collection and analysis. A real-world big data application in social-based vehicular networks is presented, and simulation results show that the big-data-enabled content delivery strategy can obtain a performance gain of user satisfaction with the delivered contents compared to the case without consideration of social big data. Finally, we conclude the article with several future research topics.},
keywords={Big Data;Content management;Accidents;Wireless communication;Multimedia communication;Intelligent vehicles;Quality of experience},
doi={10.1109/MWC.2018.1700224},
ISSN={1558-0687},
month={February},}
@INPROCEEDINGS{8581037,
author={Angeli, Alessia and Piccolomini, Elena Loli and Marfia, Gustavo},
booktitle={2018 IEEE 29th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)},
title={Learning about Fashion exploiting the Big Multimedia Data},
year={2018},
volume={},
number={},
pages={48-51},
abstract={We here propose to collect and analyze large amounts of multimedia data from different public and private sources, in the form of text, image, video, to predict relevant information about specific quantities related to fashion brands, such as their sales volumes and/or trends. To this aim, we deem deep learning techniques as the suitable instrument capable of managing extremely large amounts of multimedia data. While a few works exist in literature on learning applications in the fashion area, where text is used to perform sentiment analysis operations, limited research has also considered images and videos in this context. In this paper, starting with an overview of the state of the art of the applications of artificial intelligence for fashion, we set the stage for a holistic approach for the deep learning based analysis of multimedia data related to fashion.},
keywords={Social network services;Streaming media;Clothing;Big Data;Ecosystems;Market research},
doi={10.1109/PIMRC.2018.8581037},
ISSN={2166-9589},
month={Sep.},}
@INPROCEEDINGS{9378036,
author={Zhou, Wang and Klein, Levente J. and Lu, Siyuan},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={PAIRS AutoGeo: an Automated Machine Learning Framework for Massive Geospatial Data},
year={2020},
volume={},
number={},
pages={1755-1763},
abstract={An automated machine learning framework for geospatial data named PAIRS AutoGeo is introduced on IBM PAIRS Geoscope big data and analytics platform. The frame-work simplifies the development of industrial machine learning solutions leveraging geospatial data to the extent that the user inputs are minimized to merely a text file containing labeled GPS coordinates. PAIRS AutoGeo automatically gathers required data at the location coordinates, assembles the training data, performs quality check, and trains multiple machine learning models for subsequent deployment. The framework is validated using a realistic industrial use case of tree species classification. Open-source tree species data are used as the input to train a random forest classifier and a modified ResNet model for 10-way tree species classification based on aerial imagery, which leads to an accuracy of 59.8% and 81.4%, respectively. This use case exemplifies how PAIRS AutoGeo enables users to leverage machine learning without extensive geospatial expertise.},
keywords={Training data;Machine learning;Vegetation;Big Data;Data models;Geospatial analysis;Random forests;automated machine learning;geospatial;remote sensing;image classification;PAIRS},
doi={10.1109/BigData50022.2020.9378036},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8959927,
author={Kuo, Chun-Hao and Lee, Shih-Hsiung and Yang, Chu-Sing},
booktitle={2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI)},
title={Case Study: Parameters Optimization Process for Flat Panel Display Manufacture},
year={2019},
volume={},
number={},
pages={1-4},
abstract={Taiwan's flat panel display manufacturing is famous internationally. Its outstanding manufacturing capability can produce high quality panels in the shortest time. In the production process of flat panel displays, the process yield is the best indicator reflecting the good and bad of manufacturers. The yield is direct. The decision is to the company's profit and reputation. Add to this, it is also the trust and acceptance of the company's customers. When the products on the production line are abnormal in quality, the process and equipment engineers must immediately discuss the crux of the problem and improve it. For decomposition of analysis and process management to find out the method, it needs to carry out the problem in linear verification of confirmation whether the problem is a single problem or not. Furthermore, it will produce customer complaints, dissatisfaction with the company, causing the company's losses. Therefore, using Knowledge Discovery from Data (KDD) to establish a process of big data analysis is core technology in our work. We proposed the improvement process that how to quickly find the variation factor and couple with machine learning. Hence the research is based on data pre-processing, modeling. The experiment shows that we find out the right parameters and optimze them. Finally, the yield improvement was increasing to 66%.},
keywords={Data mining;Big Data;Machine learning;Production;Data models;Computer architecture;Companies;big data analysis;machine learning;industry 4.0;yield improvement},
doi={10.1109/TAAI48200.2019.8959927},
ISSN={2376-6824},
month={Nov},}
@ARTICLE{7900340,
author={Luo, Xin and Zhou, MengChu and Li, Shuai and Xia, YunNi and You, Zhu-Hong and Zhu, QingSheng and Leung, Hareton},
journal={IEEE Transactions on Cybernetics},
title={Incorporation of Efficient Second-Order Solvers Into Latent Factor Models for Accurate Prediction of Missing QoS Data},
year={2018},
volume={48},
number={4},
pages={1216-1228},
abstract={Generating highly accurate predictions for missing quality-of-service (QoS) data is an important issue. Latent factor (LF)-based QoS-predictors have proven to be effective in dealing with it. However, they are based on first-order solvers that cannot well address their target problem that is inherently bilinear and nonconvex, thereby leaving a significant opportunity for accuracy improvement. This paper proposes to incorporate an efficient second-order solver into them to raise their accuracy. To do so, we adopt the principle of Hessian-free optimization and successfully avoid the direct manipulation of a Hessian matrix, by employing the efficiently obtainable product between its Gauss-Newton approximation and an arbitrary vector. Thus, the second-order information is innovatively integrated into them. Experimental results on two industrial QoS datasets indicate that compared with the state-of-the-art predictors, the newly proposed one achieves significantly higher prediction accuracy at the expense of affordable computational burden. Hence, it is especially suitable for industrial applications requiring high prediction accuracy of unknown QoS data.},
keywords={Quality of service;Predictive models;Optimization;Computational modeling;Mathematical model;Data models;Web services;Big data;latent factor model;missing data prediction;quality-of-service (QoS);second-order solver;service computing sparse matrices;Web service},
doi={10.1109/TCYB.2017.2685521},
ISSN={2168-2275},
month={April},}
@INPROCEEDINGS{8070824,
author={Jain, Himanshi and Jain, Raksha},
booktitle={2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC)},
title={Big data in weather forecasting: Applications and challenges},
year={2017},
volume={},
number={},
pages={138-142},
abstract={Increasing evidence of climate change worldwide is becoming the reason to understand a lot more about the weather — everything from what's going to happen tomorrow to what's coming next year. To forecast weather we need to analyze a large set of data therefore use of big data in weather forecasting will provide numerous advantages such as saving lives, improving the quality of life, reducing risks, enhancing profitability and humanity. Some examples of these domains include Forecasting Solar Power for Utility Operations, large-scale crop production forecasts for global food security, in precision agriculture for future farming and space weather forecasting. In order to know how these applications could impact normal operations this paper defines various weather forecasting applications and technical challenges.},
keywords={Computational modeling;Big Data;Wind forecasting;Predictive models;Numerical models;Climate change;Big data;Weather forecasting;Analyze data},
doi={10.1109/ICBDACI.2017.8070824},
ISSN={},
month={March},}
@INPROCEEDINGS{8514459,
author={Mejri, Safa and Touati, Haifa and Kamoun, Farouk},
booktitle={2018 International Conference on High Performance Computing & Simulation (HPCS)},
title={Are NDN Congestion Control Solutions Compatible with Big Data Traffic?},
year={2018},
volume={},
number={},
pages={978-984},
abstract={Big Data refers to analyzing the massive volume of data by combining different applications in order to save time, efficiency and quality when interpreting data. Controlling the data transfer along the network is a fundamental question in Big Data. In this context, the transport model of the Named Data Networking (NDN) architecture introduces several new features, especially name-based retrieval policy and smaller data transfer time thanks to the interest aggregation and in-network caching. These distinguishing features make the NDN a suitable communication model for Big Data transfer. But, since in NDN content can be retrieved from multiple caches and through multiple paths, the traditional host-to-host congestion control schemes become inconsistent. Hence there is a need for an efficient congestion control scheme that takes into account the tremendous volume of data generated by Big Data processing and NDN characteristics. In this paper, we give a detailed understanding of NDN benefits over traditional TCP/IP stack for Big Data transfer, then we focus on efficient control of Big Data transfer over NDN. We give a comprehensive overview of recent Named Data congestion control solutions and evaluate and discuss their relevance for Big Data applications.},
keywords={Big Data;Distributed databases;Computer architecture;Data models;TCPIP;Internet;Data transfer},
doi={10.1109/HPCS.2018.00154},
ISSN={},
month={July},}
@INPROCEEDINGS{9276830,
author={Zhang, Huaxin and Liu, Yu and Wang, Zituo and Li, Tiansong and Cao, Keyin},
booktitle={2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)},
title={Research on Film Data Preprocessing and Visualization},
year={2020},
volume={1},
number={},
pages={946-952},
abstract={Data is the core of information, and good data quality is a prerequisite for many data analysis. Data cleaning is to increase the fault tolerance rate by correcting the error value of detected data. This paper aims to solve the problem of data set processing and visualization in the recommendation algorithm, so as to better apply in the field of recommendation algorithm. The recommendation algorithm and data sets Movielens and IMDB are analyzed theoretically. First, data set A was processed from data reading and movie score calculation; Again, the IMDB is processed in four steps to make it more suitable for the recommendation algorithm field; Finally, the plot function is used to visualize the key information. experiment shows: The data set sorted out by the above methods can effectively improve the quality and availability of data and provide relevant basis for better application in the algorithm.},
keywords={Motion pictures;Data visualization;Visualization;Prediction algorithms;Market research;Arrays;Electronic commerce;recommended algorithm;Dataset;data processing;data visualization},
doi={10.1109/ICIBA50161.2020.9276830},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7740644,
author={Crespino, Anna Maria and Corallo, Angelo and Lazoi, Mariangela and Barbagallo, Donato and Appice, Annalisa and Malerba, Donato},
booktitle={2016 IEEE 2nd International Forum on Research and Technologies for Society and Industry Leveraging a better tomorrow (RTSI)},
title={Anomaly detection in aerospace product manufacturing: Initial remarks},
year={2016},
volume={},
number={},
pages={1-4},
abstract={Manufacturing companies need to acquire, analyze and share large amounts of information and data to sustain competitive advantage in complex environments. In the context of complex manufacturing, an increasing number of devices, sensors and people are connected to internal networks dramatically changing the ability to generate, communicate, share and access data. Therefore, the data volume has become so large that it cannot be processed using conventional methods. Many companies have dramatically boosted profits and have met consumer demands more proactively, by utilizing automated data collection to feed information into a big data analytics program. In the aerospace manufacturing sector, there is a growing need to consider Big Data solutions to add value to their business services and to optimize their internal production processes. Manufacturing data are an important source of knowledge that can be recorded from different data sources such as sensors and enterprise. The majority of this data are stream processed i.e., they are produced by analytics performed on “in-motion” data. A real-time predictive analysis can help detecting manufacturing anomalies thus improving the production processes and the quality of product. This paper aims to shortly describe the initial findings of an action research study performed in the aerospace industry pilot of the TOREADOR European project.},
keywords={Manufacturing;Production;Real-time systems;Sensors;Aerospace industry;Big data;Europe;Big Data Analytics;Anomaly Detection;Streaming Data;Aerospace Manufacturing},
doi={10.1109/RTSI.2016.7740644},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8884820,
author={Rafferty, Joseph and Medina-Quero, Javier and Quinn, Susan and Saunders, Catherine and Ekerete, Idongesit and Nugent, Chris and Synnott, Jonathan and Garcia-Constantino, Matias},
booktitle={2019 IEEE International Conference on Big Data, Cloud Computing, Data Science & Engineering (BCD)},
title={Thermal Vision Based Fall Detection via Logical and Data driven Processes},
year={2019},
volume={},
number={},
pages={35-40},
abstract={Inadvertent falls can cause serious, and potentially fatal injuries, to at risk individuals. One such community of at-risk individuals is the elderly population where age related complications, such as osteoporosis and dementia, can further increase the incidence and negative impact of such falls. Notably, falls within that community has been identified as the leading cause of injury related preventable death, hospitalization and reduction to quality of life. In such cases, rapid detection of, and reaction, to fall events has shown to be critical to reduce the negative effects of falls within this community. Currently, a range of fall detection solutions exist, however, they have several deficiencies related to the core approach that has been adopted. This study has developed an ensemble of thermal vision-based, big data facilitated, solutions which aim to address some of these deficiencies. An evaluation of these logical and data-driven processes has occurred with the promising results presented within this manuscript. Finally, opportunities future work and real-world evaluation have occurred and are underway.},
keywords={Thermal sensors;Senior citizens;Big Data;Injuries;Microsoft Windows;Privacy;Fall Detection;Ambient Assistive Living;Big Data;Thermal Vision;Computer Vision;Convolutional Neural Network;Healthcare;Internet of Things;Assisted Ageing;Pervasive Computing},
doi={10.1109/BCD.2019.8884820},
ISSN={},
month={May},}
@INPROCEEDINGS{9434507,
author={Yue-ming, Zheng and Cheng, Zhang},
booktitle={2020 International Conference on Big Data and Social Sciences (ICBDSS)},
title={The Impact of Cross-border E-commerce on China’s Agricultural Products Export : —An Empirical Study Based on Big Data Processing},
year={2020},
volume={},
number={},
pages={106-110},
abstract={In recent years, China's agricultural product export trade has achieved rapid growth with the help of cross-border e-commerce platforms, which has played a positive role in China's agricultural and economic development. Based on the construction of China's cross-border e-commerce development index through the Entropy method, this paper uses 2008-2019 agricultural product import and export big data to establish a model, and then analyzes the relationship between cross-border e-commerce and China's agricultural product export trade. The results show that cross-border e-commerce has indeed promoted the export trade of agricultural products. However, the agricultural product logistics system is not sound enough, and the quality of agricultural products needs to be improved, which restricts the development of China's agricultural export. Based on the analysis, this article believes that the problem can be solved in terms of improving logistics capabilities, improving quality, and increasing talent training.},
keywords={Training;Analytical models;Social sciences;Big Data;Agricultural products;Entropy;Data models;Cross-border e-commerce;Agricultural products;Exports;Big Data},
doi={10.1109/ICBDSS51270.2020.00032},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6698559,
author={Alahakoon, Damminda and Yu, Xinghuo},
booktitle={2013 IEEE International Workshop on Inteligent Energy Systems (IWIES)},
title={Advanced analytics for harnessing the power of smart meter big data},
year={2013},
volume={},
number={},
pages={40-45},
abstract={Smart meters or advanced metering infrastructure (AMI) are being deployed in many countries around the world. Smart meters are the basic building block of the smart grid and governments have invested vast amounts in smart meter deployment targeting wide economic, social and environmental benefits. The key functionality of the smart meter is the capture and transfer of data relating to the consumption (electricity, gas) and events such as power quality and meter status. Such capability has also resulted in the generation of an unprecedented data volume, speed of collection and complexity, which has resulted in the so called big data challenge. To realize the hidden value and power in such data, it is important to use the appropriate tools and technology which are currently being called advanced analytics. In this paper we define a smart metering landscape and discuss different technologies available for harnessing the smart meter captured data. Main limitations and challenges with existing techniques with big data are also highlighted and several future directions in smart metering are presented.},
keywords={Electricity;Information management;Data handling;Data storage systems;Real-time systems;Smart grids;Advanced Metering Infrastructure (AMI);Smart Meters;Data Mining;Analytics;Big Data;Stream Analytics},
doi={10.1109/IWIES.2013.6698559},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7545217,
author={Su, Chuan-Jun and Chen, Yin-An},
booktitle={2016 International Symposium on Computer, Consumer and Control (IS3C)},
title={Social Media Analytics Based Product Improvement Framework},
year={2016},
volume={},
number={},
pages={393-396},
abstract={In the traditional world, marketing studies managed to employ various techniques to explore customers' consensual experiences toward products with limited information available. The uses of surveys, focus groups or regular individual interviews are some of the frequently used methods by marketers. We are now entering in the era of Big Data. The explosion and profusion of the unprecedented scale of heterogeneous data available in this new era allow us to acquire further insights and knowledge about the market for improving the quality of products. In this paper, we present a Social Media-based Product Improvement Framework (SM-PIF) which is capable of deriving recommendations for product improvement and subsequently increase the product's market competitiveness. The recommendation generated by the SM-PIF is expected to be more accurate and less biased than traditional methods due to its "Big Data" nature.},
keywords={Twitter;Media;Data mining;Batteries;Big data;Feature extraction;Finite impulse response filters;Big Data;Social Media;Social Media-based Product Improvement Framework (SM-PIF)},
doi={10.1109/IS3C.2016.107},
ISSN={},
month={July},}
@INPROCEEDINGS{8258061,
author={Sobolevsky, Stanislav and Massaro, Emanuele and Bojic, Iva and Arias, Juan Murillo and Ratti, Carlo},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Predicting regional economic indices using big data of individual bank card transactions},
year={2017},
volume={},
number={},
pages={1313-1318},
abstract={For centuries quality of life was a subject of studies across different disciplines. However, only with the emergence of a digital era, it became possible to investigate this topic on a larger scale. Over time it became clear that quality of life not only depends on one, but on three relatively different parameters: social, economic and well-being measures. In this study we focus only on the first two, since the last one is often very subjective and consequently hard to measure. Using a complete set of bank card transactions recorded by Banco Bilbao Vizcaya Argentaria (BBVA) during 2011 in Spain, we first create a feature space by defining various meaningful characteristics of a particular area performance through activity of its businesses, residents and visitors. We then evaluate those quantities by considering available official statistics for Spanish provinces (e.g., housing prices, unemployment rate, life expectancy) and investigate whether they can be predicted based on our feature space. For the purpose of prediction, our study proposes a supervised machine learning approach. Our finding is that there is a clear correlation between individual spending behavior and official socioeconomic indexes denoting quality of life. Moreover, we believe that this modus operandi is useful to understand, predict and analyze the impact of human activity on the wellness of our society on scales for which there is no consistent official statistics available (e.g., cities and towns, districts or smaller neighborhoods).},
keywords={Business;Economic indicators;Urban areas;Predictive models;Education;Microeconomics},
doi={10.1109/BigData.2017.8258061},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8702037,
author={Sudsee, Bhuridech and Kaewkasi, Chanwit},
booktitle={2019 21st International Conference on Advanced Communication Technology (ICACT)},
title={An Improvement of a Checkpoint-based Distributed Testing Technique on a Big Data Environment},
year={2019},
volume={},
number={},
pages={1081-1090},
abstract={The advancement of storage technologies and the fast-growing number of generated data have made the world moved into the Big Data era. In this past, we had many data mining tools but they are inadequate to process Data-Intensive Scalable Computing workloads. The Apache Spark framework is a popular tool designed for Big Data processing. It leverages in-memory processing techniques that make Spark up to 100 times faster than Hadoop. Testing this kind of Big Data program is time consuming. Unfortunately, developers lack a proper testing framework, which cloud help assure quality of their data-intensive processing programs while saving development time and storage usages.We propose Distributed Test Checkpointing (DTC) for Apache Spark. DTC applies unit testing to the Big Data software development life cycle and reduce time spent for each testing loop with checkpoint. By using checkpoint technique, DTC keeps quality of Big Data processing software while keeps an inexpensive testing cost by overriding original Spark mechanism so that developers no pain to learn how to use DTC. Moreover, DTC has no addition abstraction layers. Developers can upgrade to a new version of Spark seamlessly. From the experimental results, we found that in the subsequence rounds of unit testing, DTC dramatically speed the testing time up to 450-500% faster. In case of storage, DTC can cut unnecessary data off and make the storage 19.7 times saver than the original checkpoint of Spark. DTC can be used either in case of JVM termination or testing with random values.},
keywords={Sparks;Testing;Big Data;Checkpointing;Tools;Cluster computing;Software;Distributed Checkpointing;Apache Spark;Big Data Testing;Software Testing},
doi={10.23919/ICACT.2019.8702037},
ISSN={1738-9445},
month={Feb},}
@INPROCEEDINGS{8422106,
author={Meng, Qianyu and Wang, Kun and Liu, Bo and Miyazaki, Toshiaki and He, Xiaoming},
booktitle={2018 IEEE International Conference on Communications (ICC)},
title={QoE-Based Big Data Analysis with Deep Learning in Pervasive Edge Environment},
year={2018},
volume={},
number={},
pages={1-6},
abstract={In the age of big data, the services in pervasive edge environment are expected to offer end-users better Quality of Experience (QoE) than that in a normal edge environment. Nevertheless, various types of edge devices with storage, delivery, and sensing are coming into our environment and produce the high-dimensional big data accompanied by a volume of pervasive big data increasingly with a lot of redundancy. Therefore, the satisfaction of QoE becomes the primary challenge in high dimensional big data on the basis of pervasive edge environment. In this paper, we first propose a QoE model to evaluate the quality of service in pervasive edge environment. The value of QoE does not only include the accurate data, but also the transmission rate. Then, on the basis of the accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on Deep Learning, which is suitable for pervasive edge environment with high-dimensional big data analysis. Simulation results reveal that our proposals could achieve high QoE performance.},
keywords={Big Data;Quality of experience;Training;Tensile stress;Machine learning;Data models;Quality of service},
doi={10.1109/ICC.2018.8422106},
ISSN={1938-1883},
month={May},}
@INPROCEEDINGS{8389462,
author={Bhardwaj, Ashu and Singh, Williamjeet},
booktitle={2017 International Conference on Intelligent Sustainable Systems (ICISS)},
title={Systematic review of big data analytics in governance},
year={2017},
volume={},
number={},
pages={501-506},
abstract={With advent of technology, data is increasing abruptly day by day. Traditional database systems are not capable of processing and handling such a voluminous data. Big data analytics has the capability of processing, handling and analyzing the large datasets or stream of data. Big data analytics play important role in fields such as healthcare, agriculture, smart grid and policy making. Big data analytics with governance improves planning and decision making phases for government projects. It helps to improve the quality of government services. This paper focuses on the different application areas of governance in which big data analytics play a role and tools used to handle big data management problem. The existing work is classified into different categories and is presented using visualizations. This paper also deals with challenges related to governance field.},
keywords={Big Data;Tools;Databases;Java;Conferences;Government;Decision making;Big data analytics;Governance;Tools;Challenges;Big data framework},
doi={10.1109/ISS1.2017.8389462},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7373957,
author={Severiens, Thomas},
booktitle={2015 IIAI 4th International Congress on Advanced Applied Informatics},
title={Quality Measurement beyond Bibliometry},
year={2015},
volume={},
number={},
pages={483-486},
abstract={Research Administration has always been an important part of research organization, as it has the authority to boost research activities in selected fields. One of the challenges is, to identify those directions of research and those spontaneously popping up findings as early as possible, which will become of high importance for economy and research in the nearer or more distant future. Without good and well tested technical aids, this task may be impossible to meet. For many years, bibliometry was the method of first choice, to identify persons and groups of high impact (=quality?) research. The problem with using bibliometry is, that publishing is a slow process, resulting in measuring the quality and impact of those activities years back. It also results in support for those groups and individuals, who already had been supported for a long time, moving those researchers out of focus, who may come up with new, innovative findings. This paper gives an overview of research activities for measuring research quality beyond the bibliometric paradigm, but making use of big data, research data and computer based learning in Germany. It also gives some short but generally understandable introduction into the used technologies to enable the audience to discuss on the shown methodologies and techniques.},
keywords={Training data;Training;Big data;Europe;Bibliometrics;Voltage measurement;Frequency measurement;research selection criteria;bibliometry;computer based learning;big data},
doi={10.1109/IIAI-AAI.2015.219},
ISSN={},
month={July},}
@INPROCEEDINGS{8621870,
author={Baralis, Elena and Garza, Paolo and Pastor, Eliana},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={A Density-based Preprocessing Technique to Scale Out Clustering},
year={2018},
volume={},
number={},
pages={2078-2087},
abstract={Clustering big data is a challenging task, because the majority of high-quality clustering algorithms do not scale well with respect to the data set cardinality. To tackle the scalability problem, we propose a general-purpose density-based preprocessing technique, called SCOUT, implemented in the Spark framework. It allows compacting the original data by means of a set of representative points, while still preserving the original data distribution and density information. This small set of representative points may become the input to almost any clustering algorithm. Thus, also complex, high-quality in-memory algorithms can be applied. A thorough experimental evaluation shows that the proposed approach is efficient and at the same time effective.},
keywords={Clustering algorithms;Big Data;Scalability;Sparks;Cluster computing;Task analysis;Data preprocessing;Preprocessing;Clustering;Spark-based clustering},
doi={10.1109/BigData.2018.8621870},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9382573,
author={Hou, Hong and Meng, Hui},
booktitle={2020 Management Science Informatization and Economic Innovation Development Conference (MSIEID)},
title={Evaluation of Material Suppliers Based on BP Neural Network under The Background of Big Data},
year={2020},
volume={},
number={},
pages={12-16},
abstract={Nowdays for construction enterprises, there are more material suppliers to choose under the background of big data application in construction industry. Therefore how to evaluate and select the material suppliers correctly and wisely become a particularly important work for manager to control the cost and quality of a project. Firstly, the material supplier evaluation index system is constructed, then the method of obtaining relevant data is put forward. Based on the above, the material supplier evaluation model is constructed by using the BP neural network algorithm. Finally, the applicability and reliability of this model are demonstrated through an example, which provides reference for the evaluation of material suppliers in the practice of building materials procurement.},
keywords={Technological innovation;Systematics;Neural networks;Project management;Data models;Standards;Qualifications;BP neural network;big data platform;material supplier},
doi={10.1109/MSIEID52046.2020.00010},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9418813,
author={enze, Ju and yingge, Du},
booktitle={2020 International Conference on Information Science and Education (ICISE-IE)},
title={Research on the Quality of the Master Training in the Context of Big Data———Based on the Economic Model of Public Trust},
year={2020},
volume={},
number={},
pages={131-134},
abstract={With the development of my country's information technology, it can be found that my country's master's degree is increasing in a certain proportion every year. Affected by the novel coronavirus epidemic, policies will be introduced in 2020 to increase the enrollment expansion of masters by 20%. Big data has found that the proportion of enrollment expansion has increased significantly compared with previous years. The large increase in the proportion of enrollment has raised concerns about whether the gold content of master's degree will decline in this era of big data. Based on the SPSS of statistical analysis by network computers, this paper constructs an economic model to explore whether the current quality of master's training can solve the public's concerns, and thus introduce educational policies that are in line with the development of information technology.},
keywords={Training;Economics;Analytical models;Information science;Gold;Epidemics;Statistical analysis;building model;education policy;cultivation quality;SPSS of statistical analysis;information technology},
doi={10.1109/ICISE51755.2020.00034},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8669595,
author={Jiang, Ying and Zhang, Na and Fang, Ying},
booktitle={2019 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={The Analysis and Design of Ship Monitoring System Based on Hybrid Replication Technology},
year={2019},
volume={},
number={},
pages={456-459},
abstract={As the core of informatization, data has a huge significance to the development of information-based enterprises. Data replication technology is an important approach to solve the problem of enterprise data sharing based on distributed database system. It plays a crucial role in promoting business integration of enterprises and institutions, improving data quality, enhancing data sharing and improving the application level of back-end big data analysis [1]. It is necessary to do research for making a good data management of the distributed database application system, synchronizing the data to the data, preventing data conflicting and being able to synchronize or asynchronous replication. Combining with the database design model of the ship monitoring and control system, this paper mainly described how to complete the construction of distributed database system using Oracle, based on advanced replication technology named as the combination of multi-agent replication and materialized views hybrid replication technology.},
keywords={Distributed databases;Database systems;Synchronization;Business;Monitoring;Marine vehicles;Distributed database;advanced replication;materialized view;data conflict},
doi={10.1109/ICITBS.2019.00118},
ISSN={},
month={Jan},}
@ARTICLE{8424509,
author={Wen, Chenglu and Sun, Xiaotian and Hou, Shiwei and Tan, Jinbin and Dai, Yudi and Wang, Cheng and Li, Jonathan},
journal={IEEE Geoscience and Remote Sensing Letters},
title={Line Structure-Based Indoor and Outdoor Integration Using Backpacked and TLS Point Cloud Data},
year={2018},
volume={15},
number={11},
pages={1790-1794},
abstract={This letter presents a line structure-based method for integration of centimeter-level indoor backpacked scanning point clouds and millimeter-level outdoor terrestrial laser scanning point clouds. Using 3-D lines for registration, instead of matching points directly, can improve the robustness of the method and adapt to multisource point cloud data of different qualities. Considering the limited overlapping between indoor and outdoor scenes, line structures are extracted from overlapped wall areas that may be included in interior and exterior data. Here, a patch-based method labels a point cloud into wall, ceiling, floor categories, as well as assigning the candidate overlapping walls. Then, lines structures are extracted from the wall plane point cloud. Potential door and window line structures are detected and refined for point cloud registration. Last, an iterative closest point-based method is used to fine tune the registration results. Our results show that the proposed method effectively integrates a promising map of indoor and outdoor scenes.},
keywords={Three-dimensional displays;Data mining;Lasers;Labeling;Data models;Semantics;Solid modeling;Indoor outdoor integration;line feature;point clouds;semantic labeling},
doi={10.1109/LGRS.2018.2856514},
ISSN={1558-0571},
month={Nov},}
@ARTICLE{7447706,
author={Kanoun, Karim and Tekin, Cem and Atienza, David and Schaar, Mihaela van der},
journal={IEEE Transactions on Computers},
title={Big-Data Streaming Applications Scheduling Based on Staged Multi-Armed Bandits},
year={2016},
volume={65},
number={12},
pages={3591-3605},
abstract={Several techniques have been recently proposed to adapt Big-Data streaming applications to existing many core platforms. Among these techniques, online reinforcement learning methods have been proposed that learn how to adapt at run-time the throughput and resources allocated to the various streaming tasks depending on dynamically changing data stream characteristics and the desired applications performance (e.g., accuracy). However, most of state-of-the-art techniques consider only one single stream input in its application model input and assume that the system knows the amount of resources to allocate to each task to achieve a desired performance. To address these limitations, in this paper we propose a new systematic and efficient methodology and associated algorithms for online learning and energy-efficient scheduling of Big-Data streaming applications with multiple streams on many core systems with resource constraints. We formalize the problem of multi-stream scheduling as a staged decision problem in which the performance obtained for various resource allocations is unknown. The proposed scheduling methodology uses a novel class of online adaptive learning techniques which we refer to as staged multi-armed bandits (S-MAB). Our scheduler is able to learn online which processing method to assign to each stream and how to allocate its resources over time in order to maximize the performance on the fly, at run-time, without having access to any offline information. The proposed scheduler, applied on a face detection streaming application and without using any offline information, is able to achieve similar performance compared to an optimal semi-online solution that has full knowledge of the input stream where the differences in throughput, observed quality, resource usage and energy efficiency are less than 1, 0.3, 0.2 and 4 percent respectively.},
keywords={Machine learning;Throughput;Quality of service;Resource management;Complexity theory;Adaptation models;Data mining;Big data;Scheduling;machine learning;many-core platforms;data mining;big-data;multiple streams processing;concept drift},
doi={10.1109/TC.2016.2550454},
ISSN={1557-9956},
month={Dec},}
@INPROCEEDINGS{6916274,
author={Balasingam, B. and Sankavaram, M. S. and Choi, K. and Ayala, D. F. M. and Sidoti, D. and Pattipati, K. and Willett, P. and Lintz, C. and Commeau, G. and Dorigo, F. and Fahrny, J.},
booktitle={17th International Conference on Information Fusion (FUSION)},
title={Online anomaly detection in big data},
year={2014},
volume={},
number={},
pages={1-8},
abstract={In this paper, the problem of online anomaly detection in multi-attributed, asynchronous data from a large number of individual devices is considered. It has become increasingly common for many services, such as video-on-demand (VOD), to have connected customers where hundreds of millions of subscribers access a cluster of content servers for online services. It is important to monitor these transactions online, in order to ensure acceptable quality of experience to the customers as well as for detecting any abnormal or undesirable activities. Our proposed anomaly detection strategy works in two phases: First we perform intermittent anomaly detection in space, using data from the entire set of devices for a short duration in time. This phase employs principal component analysis (PCA) for data reduction and captures models of normal and abnormal features. Then, these identified models are used to monitor each subscriber's devices online in order to quickly detect any abnormalities. The proposed approach is demonstrated on Comcast's Xfinity video streaming data.},
keywords={Principal component analysis;Data models;Hidden Markov models;Big data;IP networks;Computational modeling;Monitoring;Big data analytics;anomaly detection;principal component analysis (PCA);anomaly detection;Page test;video on demand (VOD)},
doi={},
ISSN={},
month={July},}
@INPROCEEDINGS{8444581,
author={Zhang, Mingming and Wo, Tianyu and Xie, Tao},
booktitle={2018 IEEE International Conference on Pervasive Computing and Communications (PerCom)},
title={A Platform Solution of Data-Quality Improvement for Internet-of-Vehicle Services},
year={2018},
volume={},
number={},
pages={1-7},
abstract={Interconnection and intelligence have become the latest trends of the new generation of vehicle and transportation technologies. Applications built upon platforms of cloud-centered vehicle networking, i.e., Internet-of-Vehicles (IoVs), have been increasingly developed and deployed to provide data-centric services (e.g., driving assistance). Because these services are often safety critical, assuring service dependability has become an important requirement. In this paper, we propose DQI, a platform-level solution of Data-Quality Improvement designed to assure service dependability for Internet-of-Vehicle services. As an example, DQI is deployed in CarStream, an industrial system of big data processing designed for chauffeured car services. Via CarStream, over 30,000 vehicles are organized in a virtual vehicle network by sharing vehicle-status data in a near real-time manner. Such data often have low-quality issues and compromise the dependability of data-centric services. DQI includes techniques of data-quality improvement, including detecting outliers, extracting frequent patterns, and interpolating sequences. DQI enhances the dependability of data-centric services in IoVs by addressing the common data-quality requirements at the platform level. Upper-level services can benefit from DQI for data-quality improvement and reduce the complexity of service logic. We evaluate DQI by using a three-year dataset of vehicles and real applications deployed in CarStream. The result shows that compared with existing approaches, DQI can effectively restore missing data and correct anomalies with more than 30.0% improvement in precision. By studying multiple real applications, we also show that this data-quality improvement can indeed enhance the dependability of IoV services.},
keywords={Interpolation;Task analysis;Inspection;Cloud computing;Roads;Data integrity;Conferences;Dependability;Interpolation;Sequence Matching;Data Quality;Big Data;Internet-of-Vehicles},
doi={10.1109/PERCOM.2018.8444581},
ISSN={2474-249X},
month={March},}
@INPROCEEDINGS{6691760,
author={Zolfaghar, Kiyana and Meadem, Naren and Teredesai, Ankur and Roy, Senjuti Basu and Chin, Si-Chi and Muckian, Brian},
booktitle={2013 IEEE International Conference on Big Data},
title={Big data solutions for predicting risk-of-readmission for congestive heart failure patients},
year={2013},
volume={},
number={},
pages={64-71},
abstract={Developing holistic predictive modeling solutions for risk prediction is extremely challenging in healthcare informatics. Risk prediction involves integration of clinical factors with socio-demographic factors, health conditions, disease parameters, hospital care quality parameters, and a variety of variables specific to each health care provider making the task increasingly complex. Unsurprisingly, many of such factors need to be extracted independently from different sources, and integrated back to improve the quality of predictive modeling. Such sources are typically voluminous, diverse, and vary significantly over the time. Therefore, distributed and parallel computing tools collectively termed big data have to be developed. In this work, we study big data driven solutions to predict the 30-day risk of readmission for congestive heart failure (CHF) incidents. First, we extract useful factors from National Inpatient Dataset (NIS) and augment it with our patient dataset from Multicare Health System (MHS). Then, we develop scalable data mining models to predict risk of readmission using the integrated dataset. We demonstrate the effectiveness and efficiency of the open-source predictive modeling framework we used, describe the results from various modeling algorithms we tested, and compare the performance against baseline non-distributed, non-parallel, non-integrated small data results previously published to demonstrate comparable accuracy over millions of records.},
keywords={Heart;Diseases;Information management;Data handling;Data storage systems;Predictive models;Healthcare;Knowledge-Discovery;Risk Prediction},
doi={10.1109/BigData.2013.6691760},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9334900,
author={Hashemipour, Seyed Naser and Aghaei, Jamshid and Kavousi-fard, Abdullah and Niknam, Taher and Salimi, Ladan and del Granado, Pedro Crespo and Shafie-khah, Miadreza and Wang, Fei and Catalão, João P. S.},
booktitle={2020 IEEE Industry Applications Society Annual Meeting},
title={Big Data Compression in Smart Grids via Optimal Singular Value Decomposition},
year={2020},
volume={},
number={},
pages={1-8},
abstract={The smart grid is a fully automatic delivery grid for electricity power with a two-way reliable flow of electricity and information among different equipment on the grid. With the rapid development of smart grids, smart meters and sensors are used to monitor the system and provide a wide reporting which produce a huge amount of data in various part of the grid. To logical manage this trouble, the presented paper proposes a new lossy data compression approach for big data compression. In the proposed method, at the first step, the optimal singular value decomposition (OSVD) is applied to a matrix that achieves the optimal number of singular values to the sending process and the other ones will be neglected. This goal is done due to the quality of retrieved data and the rate of compression ratio. In the presented scheme, to implementation of the optimization framework, various intelligent optimization methods are used to determine the number of optimal values in the elimination stage. The efficiency and capabilities of the proposed method are examined using the experimental dataset of several residential microgrid consumers and market dataset. Simulation results show the high performance and efficiency of the proposed model in smart grids with big data.},
keywords={Simulation;Data compression;Big Data;Smart meters;Sensor systems;Smart grids;Singular value decomposition;Big data;Data compression;Smart Grid;Optimization;Singular value decomposition},
doi={10.1109/IAS44978.2020.9334900},
ISSN={2576-702X},
month={Oct},}
@INPROCEEDINGS{7783434,
author={Bagheri, Azam and Bollen, Math},
booktitle={2016 17th International Conference on Harmonics and Quality of Power (ICHQP)},
title={Additional information from voltage dips},
year={2016},
volume={},
number={},
pages={328-332},
abstract={This paper presents some methods to extract additional information from voltage dip recordings, beyond residual voltage and duration. Additionally it discusses some issues related to the massive amount of data obtained from modern measurements that, is referred to as Big Data. The paper proposes some Deep Learning based algorithms as good candidates to extract complex features from big data as a step towards additional information. The applications of the information include predicting individual equipment performance, fault type and location, protection operation, and overall load behavior. Individual equipment and overall load include production as well as consumption.},
keywords={Feature extraction;Circuit faults;Voltage fluctuations;Data mining;Big data;Trajectory;Machine learning;Power quality;Voltage dips;Big Data;Deep learning;distributed generation;fault-ride-through},
doi={10.1109/ICHQP.2016.7783434},
ISSN={2164-0610},
month={Oct},}
@ARTICLE{8372637,
author={Dong, Yongquan and Dragut, Eduard C. and Meng, Weiyi},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Normalization of Duplicate Records from Multiple Sources},
year={2019},
volume={31},
number={4},
pages={769-782},
abstract={Data consolidation is a challenging issue in data integration. The usefulness of data increases when it is linked and fused with other data from numerous (Web) sources. The promise of Big Data hinges upon addressing several big data integration challenges, such as record linkage at scale, real-time data fusion, and integrating Deep Web. Although much work has been conducted on these problems, there is limited work on creating a uniform, standard record from a group of records corresponding to the same real-world entity. We refer to this task as record normalization. Such a record representation, coined normalized record, is important for both front-end and back-end applications. In this paper, we formalize the record normalization problem, present in-depth analysis of normalization granularity levels (e.g., record, field, and value-component) and of normalization forms (e.g., typical versus complete). We propose a comprehensive framework for computing the normalized record. The proposed framework includes a suit of record normalization methods, from naive ones, which use only the information gathered from records themselves, to complex strategies, which globally mine a group of duplicate records before selecting a value for an attribute of a normalized record. We conducted extensive empirical studies with all the proposed methods. We indicate the weaknesses and strengths of each of them and recommend the ones to be used in practice.},
keywords={Data integration;Standards;Task analysis;Databases;Google;Data mining;Terminology;Record normalization;data quality;data fusion;web data integration;deep web},
doi={10.1109/TKDE.2018.2844176},
ISSN={1558-2191},
month={April},}
@INPROCEEDINGS{8560288,
author={Ma, Shenglan and Wang, Hao and Xu, Botong and Xiao, Hong and Xie, Fangkai and Dai, Hong-Ning and Tao, Ran and Yi, Ruihua and Wang, Tongsen},
booktitle={2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={Banking Comprehensive Risk Management System Based on Big Data Architecture of Hybrid Processing Engines and Databases},
year={2018},
volume={},
number={},
pages={1844-1851},
abstract={Banks are shifting from a simple credit risk management model to the comprehensive risk management model. Banking risks come from many channels and systems. Big data technology provides an innovative and effective solution for data management, and thus is suitable to be applied in the risk management scenarios that require high-quality data and complex data analysis. This paper firstly proposes big data architecture of hybrid processing engines and databases. This architecture uses Hadoop ecosystem with ETL and Spark processing engines, and using massive parallel processing databases (MPP), transactional databases, and HDFS. Then a banking comprehensive risk management system prototype based on the proposed big data architecture is implemented. Comparisons and evaluations clearly demonstrate that the proposed system has better performance.},
keywords={comprehensive risk management;big data;hybrid architecture},
doi={10.1109/SmartWorld.2018.00310},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8901326,
author={Zeyong, Wang and Yutian, Hong and Zhongzheng, Tong},
booktitle={2019 International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Risk Assessment Model and Experimental Analysis of Electric Power Production Based on Big Data},
year={2019},
volume={},
number={},
pages={88-91},
abstract={This paper studies the characteristics of big data of power, and aims at the data quality problems faced by power system. It puts forward an assessment method of power system data quality. Based on the characteristics of large power data, a series of indicators influencing the data are analyzed and hierarchically divided to determine the measurement standard of power production data during the process of risk management, namely, the risk index system. Then, the risk assessment model of power data is established by referring to the assessment model in other fields or the rules of deduction and induction in data mining. It can be used to evaluate the quality of power system data, and find a framework and solution suitable for large data quality assessment. Finally, the model is implemented on Hadoop platform, which proves that it takes into account the completeness of the index system, the objectivity of the assessment method and the rapidity of the calculation method.},
keywords={risk assessment;electric power;fuzzy comprehensive evaluation;Hadoop;index},
doi={10.1109/ICSGEA.2019.00028},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7558018,
author={Siriweera, T.H. Akila S. and Paik, Incheon and Zhang, Jia and Kumara, Banage T.G.S.},
booktitle={2016 IEEE International Conference on Web Services (ICWS)},
title={Big Data Analytic Service Discovery Using Social Service Network with Domain Ontology and Workflow Awareness},
year={2016},
volume={},
number={},
pages={324-331},
abstract={In the era of Big Data, data analysis gives strong competition power to enterprises. As services for Big Data Analysis (BDA) become prevalent, analysis services with intelligence and autonomy using automatic service composition show very bright prospects in the BDA market. Service composition consists of four stages: workflow generation, discovery, selection, and execution. In this paper, we propose a novel service discovery approach that considers two key concerns in the discovery domain towards better quality as well as effective service composition. BDA services are fine grained according to the domain and functional behaviors. The services need a domain context-aware and precision-guided discovery approach. Therefore, we propose domain ontology-based service discovery. It is mainly focused on the BDA domain for precise service discovery considering all behavioral signatures between queries and services. As for the second concern, components in composed services depend greatly on each other in situations such as workflow for data analysis. We show that linking services together considering sociability or user preference gives better discovery performance. We propose a Linked Social Service Network (LSSN) with multiple feature attribute-based service discovery for BDA. Our approach combines two advantages, the precision and sociability of Web services. The experimental results show that both of these methods perform well based on their perspectives, better than previous approaches.},
keywords={Ontologies;Big data;Data models;Analytical models;Web services;Semantics;High-temperature superconductors;Big data analytics;Web service discovery;service composition;Linked Social Service Networks;domain ontology},
doi={10.1109/ICWS.2016.49},
ISSN={},
month={June},}
@INPROCEEDINGS{8622583,
author={Li, Fucun and Wu, Jianqing and Dong, Fang and Lin, Jiayin and Sun, Geng and Chen, Huaming and Shen, Jun},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Ensemble Machine Learning Systems for the Estimation of Steel Quality Control},
year={2018},
volume={},
number={},
pages={2245-2252},
abstract={Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out.},
keywords={Steel;Production;Quality control;Machine learning;Manufacturing;Steel industry;Companies;ensemble learning;steel quality control;intelligent manufacturing;data mining},
doi={10.1109/BigData.2018.8622583},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7338514,
author={Blyumin, Semen L. and Borovkova, Galina S. and Serova, Kseniya V. and Sysoev, Anton S.},
booktitle={2015 9th International Conference on Application of Information and Communication Technologies (AICT)},
title={Analysis of finite fluctuations for solving big data management problems},
year={2015},
volume={},
number={},
pages={48-51},
abstract={The paper is related to the solution of problems of regulated city intersections and assessment of quality of teaching staff activity. The proposed method of solving these problems - the method of Analysis of finite fluctuations - showed good results represented in the examples below.},
keywords={Education;Additives;Indexes;Chlorine;Computational modeling;Atmospheric measurements;Particle measurements;Analysis of finite fluctuations;regulated intersection;teaching staff activity;big data management},
doi={10.1109/ICAICT.2015.7338514},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9006281,
author={Romsaiyud, Walisa and Schnoor, Henning and Hasselbring, Wilhelm},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Improving k-Nearest Neighbor Pattern Recognition Models for Privacy-Preserving Data Analysis},
year={2019},
volume={},
number={},
pages={5804-5813},
abstract={Supervised learning classification models use labeled data to train models on a discrete form for generating predictions. A major challenge addressed in this paper is training a machine learning model to the recognition of a pattern data perspective of the original datasets and privacy-preserving datasets to improve predictive models. The model training process, the training datasets, and validation datasets are mixed with data and privacy-preserving data cause overfitting from high variance in the machine learning algorithm. This paper addresses a k-Nearest Neighbor algorithm to build models, apply an automated hyperparameter tuning method to determine the optimal parameters based on the characteristics before the training process of a large volume datasets. Evaluating the model to achieve goals based on a high score of accuracy results on quality prediction and performance models. The experiments from our real datasets and the UCI machine learning repository show the best method for all of the training data and conduct difference experiments for improving accuracy, feasibility, correctness and reliability of the scheme.},
keywords={Data models;Predictive models;Training;Machine learning;Tuning;Prediction algorithms;Machine learning algorithms;supervised learning classification models;privacy-preserving data;k-Nearest Neighbor (k-NN);automated hyperparameter tuning},
doi={10.1109/BigData47090.2019.9006281},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8920823,
author={Budiarti, Rizqi Putri Nourma and Sukaridhoto, Sritrusta and Hariadi, Mochamad and Purnomo, Mauridhi Hery},
booktitle={2019 International Conference on Computer Science, Information Technology, and Electrical Engineering (ICOMITEE)},
title={Big Data Technologies using SVM (Case Study: Surface Water Classification on Regional Water Utility Company in Surabaya)},
year={2019},
volume={},
number={},
pages={94-101},
abstract={How important to the role of water for the survival of living beings, not only for human but also the other living beings need water as one of the elements that support the continuity of life in every living creature. To maintain the necessity of water resources such as river, recently the need for monitoring systems that able to take the parameter of water quality using sensors important. In the previous paper, we built the Internet of Things to get the data using a passive sensor and an active sensor. As additionally, we built Big-Data system equipped with machine learning algorithm that can perform water quality classification with the Support Vector Machine method. This system monitoring every activity in the Karang Pilang area and applying classification. The result of this system that the big data system can perform the classification of river water quality in interactive and accurate. The result discusses that we were able to classify by using Support Vector Machine with accuracy level 0.9138 by using Linear kernel and 0.8372 by using RBF kernel. From the ROC result, we achieved AUC value until 0.93. It's mean we achieved an excellent result.},
keywords={Support vector machines;Big Data;Sensors;Water pollution;Machine learning;Kernel;Surface water classification;Water quality;IOT;Big Data;Support Vector Machine},
doi={10.1109/ICOMITEE.2019.8920823},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8622378,
author={Kaplunovich, Alex and Yesha, Yelena},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Consolidating billions of Taxi rides with AWS EMR and Spark in the Cloud : Tuning, Analytics and Best Practices},
year={2018},
volume={},
number={},
pages={4501-4507},
abstract={Saving nature using Big Data Analytics is a very noble goal. Using New York taxi rides data, we decided to learn how many rides could be consolidated. It was a journey we would like to share. First, we had to choose the platform for calculation between Amazon Athena, Serverless Microservices, SQL or NoSql databases, Hadoop and Spark. Then, we had to find an optimal solution for the platform using assorted tuning and optimization techniques. Although the problem seems to be straight forward, it turned out that the solution is quite challenging because of the input size, data quality, calculation complexities and numerous EMR/Spark tuning options. We have been using New York taxi data from 2009 to 2017 to quantify the rides that can be joined together. The taxi rides were consolidated based on pickup location, pickup time and drop-off location. We have been calculating the percentage of taxi rides that can be joined. The benchmark originally set was rides within five minutes with a pickup and drop-off locations within half a kilometer. Then we started experimenting with different times and locations. We have been using parquet format, parallel Scala collections, compression, filtering, new column introduction, tuning parameters, I/O overhead tuning, bucketing, timeouts and partitioning. Over 1.2 billion rides were processed using Amazon EMR with Spark. We have been optimizing calculation time and processing price. Spark has hundreds of parameters, EMR has over fifty instances to choose from. It was challenging to process our data within reasonable time. We were able to find the optimal Spark queries (plans), tested different types of joins and compared their performances. Also, we were able to compare I/O and in-memory operations during partitioning and large files manipulation (the input file sizes were hundreds of Gigabytes). The results were amazing - we could consolidate around thirty five percent of total rides, saving tons of gas and improving environment and traffic in New York City.},
keywords={Sparks;Public transportation;Servers;Tuning;Big Data;Structured Query Language;Tools;Analytics;Spark;EMR;Cloud;BigData;Best Practices;Parquet;AWS;Optimization;Tuning},
doi={10.1109/BigData.2018.8622378},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7764382,
author={Harleen and Garg, Naveen},
booktitle={2016 International Conference on Research Advances in Integrated Navigation Systems (RAINS)},
title={Analysis of Hadoop performance and unstructured data using Zeppelin},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Big data has been designed as next 20 years for innovation, competition and productivity. It helps to top opportunities and address the top challenges. Every person has 320 * times data in their library and this data can't be examined by traditional data processing application devices, within the acceptable time. The difficulties incorporate the zones of capture, storage, search, sharing, exchange, examination, and representation of this data. Management of this data which ensures that the data from varied sources is processed error free and good quality to perform analysis. Hadoop is designed to handle the extremely high volumes of data in any structure. This paper explores the Hadoop cluster on Amazon Elastic Cloud, perform the benchmark of data load time with traditional data processing application and Hadoop. Secondly we analyze the unstructured data in Zeppelin with Spark.},
keywords={Big data;Sparks;Navigation;Rain;Real-time systems;Distributed databases;Amazon Web Services;Hadoop Distributed File System;Elastic Map Reduce;Elastic Compute Cloud},
doi={10.1109/RAINS.2016.7764382},
ISSN={},
month={May},}
@ARTICLE{8824131,
author={Zhang, Caifeng and Ma, Rui and Sun, Shiwei and Li, Yujie and Wang, Yichuan and Yan, Zhijun},
journal={IEEE Access},
title={Optimizing the Electronic Health Records Through Big Data Analytics: A Knowledge-Based View},
year={2019},
volume={7},
number={},
pages={136223-136231},
abstract={Many hospitals are suffering from ineffective use of big data analytics with electronic health records (EHRs) to generate high quality insights for their clinical practices. Organizational learning has been a key role in improving the use of big data analytics with EHRs. Drawing on the knowledge-based view and big data lifecycle, we investigate how the three modes of knowledge can achieve meaningful use of big data analytics with EHRs. To test the associations in the proposed research model, we surveyed 580 nurses of a large hospital in China in 2019. Structural equation modelling was used to examine relationships between knowledge mode of EHRs and meaningful use of EHRs. The results reveal that know-what about EHRs utilization, know-how EHRs storage and utilization, and know-why storage and utilization can improve nurses' meaningful use of big data analytics with EHRs. This study contributes to the existing digital health and big data literature by exploring the proper adaptation of analytical tools to EHRs from the different knowledge mode in order to shape meaningful use of big data analytics with EHRs.},
keywords={Big Data;Hospitals;Organizations;Electronic medical records;Biomedical imaging;Big data analytics;electronic health records and impacts;knowledge-based view},
doi={10.1109/ACCESS.2019.2939158},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9038949,
author={Sisyukov, Artem N. and Bondarev, Vlad K. and Yulmetova, Olga S.},
booktitle={2020 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)},
title={ERP Data Analysis and Visualization in High-Performance Computing Environment},
year={2020},
volume={},
number={},
pages={509-512},
abstract={In the era of the fourth industrial revolution the enterprise resource planning system (ERP) becomes a foundation for interconnection between logistics systems, production facilities, smart machines, IoT-enabled devices and other enterprise data sources. The paper proposes an approach to extend the ERP integrated analytical tools capabilities by processing ERP data in a multi-tenant GPU-enabled high-performance computing (HPC) environment. Corporate analytic features in conjunction with GPU in-memory processing of big structured and unstructured data increase the performance and analysis effectiveness for enterprise machine learning (ML) tasks. The approach proposes sharing the data in GPU memory using open analytic platform along with existed ERP analytical capabilities on example of SAP S/4Hana. Considered solution accelerates data scientists work with ERP data sets and could be used for faster quality AI model creation and easier data interaction in unspecific for ERP visualization way like immersive learning with virtual or augmented reality (VR/AR).},
keywords={Performance evaluation;Soft sensors;Data visualization;Graphics processing units;Machine learning;Production facilities;Libraries;ERP;GPU;SAP;SAP HANA;data analysis;artificial intelligence;visualization},
doi={10.1109/EIConRus49466.2020.9038949},
ISSN={2376-6565},
month={Jan},}
@INPROCEEDINGS{7152552,
author={Zeng, Xuezhi and Ranjan, Rajiv and Strazdins, Peter and Garg, Saurabh Kumar and Wang, Lizhe},
booktitle={2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
title={Cross-Layer SLA Management for Cloud-hosted Big Data Analytics Applications},
year={2015},
volume={},
number={},
pages={765-768},
abstract={As we come to terms with various big data challenges, one vital issue remains largely untouched. That is service level agreement (SLA) management to deliver strong Quality of Service (QoS) guarantees for big data analytics applications (BDAA) sharing the same underlying infrastructure, for example, a public cloud platform. Although SLA and QoS are not new concepts as they originated much before the cloud computing and big data era, its importance is amplified and complexity is aggravated by the emergence of time-sensitive BDAAs such as social network-based stock recommendation and environmental monitoring. These applications require strong QoS guarantees and dependability from the underlying cloud computing platform to accommodate real-time responses while handling ever-increasing complexities and uncertainties. Hence, the over-reaching goal of this PhD research is to develop novel simulation, modelling and benchmarking tools and techniques that can aid researchers and practitioners in studying the impact of uncertainties (contention, failures, anomalies, etc.) on the final SLA and QoS of a cloud-hosted BDAA.},
keywords={Cloud computing;Grid computing;Big data;Computers;Conferences;Real-time systems;Best practices;Cloud Computing;Big Data;Service Level Agreement},
doi={10.1109/CCGrid.2015.175},
ISSN={},
month={May},}
@INPROCEEDINGS{8648644,
author={D'souza, Kevin Joy and Ansari, Zahid},
booktitle={2018 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM)},
title={Big Data Science in Building Medical Data Classifier Using Naïve Bayes Model},
year={2018},
volume={},
number={},
pages={76-80},
abstract={currently, maintenance of clinical databases has become a crucial task in the medical field. The patient data consisting of various features and diagnostics related to disease should be entered with the utmost care to provide quality services. As the data stored in medical databases may contain missing values and redundant data, mining of the medical data becomes cumbersome. As it can affect the results of mining, it is essential to have good data preparation and data reduction before applying data mining algorithms. Prediction of disease becomes quick and easier if data is precise and consistent and free from noise. One of the key specialty of Naive Bayes classifiers is that they are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Evaluation of closed-form expression can be achieved by Maximum-likelihood training. Which requires linear time, rather than by expensive iterative approximation as used for many other types of classifiers. This research uses data science approach to diognize the medical data. In this article, a study has been conducted by using naïve Bayes classifier to classify the medical data. The suitability of the classifier and the accuracy of the classifier are measured using different performance criteria. This study is useful for researchers and developers in understanding and using a classification technique in medical diagnosis.},
keywords={Medical diagnostic imaging;Data mining;Training;Data models;Bayes methods;Diseases;Feature extraction;Classifier, ROC, R-Tool, TP, FP, Bayes},
doi={10.1109/CCEM.2018.00020},
ISSN={},
month={Nov},}
@ARTICLE{7530891,
author={Dai, Wenyun and Qiu, Longfei and Wu, Ana and Qiu, Meikang},
journal={IEEE Transactions on Big Data},
title={Cloud Infrastructure Resource Allocation for Big Data Applications},
year={2018},
volume={4},
number={3},
pages={313-324},
abstract={Increasing popular big data applications bring about invaluable information, but along with challenges to industrial community and academia. Cloud computing with unlimited resources seems to be the way out. However, this panacea cannot play its role if we do not arrange fine allocation for cloud infrastructure resources. In this paper, we present a multi-objective optimization algorithm to trade off the performance, availability, and cost of Big Data application running on Cloud. After analyzing and modeling the interlaced relations among these objectives, we design and implement our approach on experimental environment. Finally, three sets of experiments show that our approach can run about 20 percent faster than traditional optimization approaches, and can achieve about 15 percent higher performance than other heuristic algorithms, while saving 4 to 20 percent cost.},
keywords={Cloud computing;Resource management;Big data;Algorithm design and analysis;Optimization;Heuristic algorithms;Quality of service;Cloud infrastructure;big data;resource allocation;multi-objective optimization},
doi={10.1109/TBDATA.2016.2597149},
ISSN={2332-7790},
month={Sep.},}
@INPROCEEDINGS{8257992,
author={Shang, Chao and Palmer, Aaron and Sun, Jiangwen and Chen, Ko-Shin and Lu, Jin and Bi, Jinbo},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={VIGAN: Missing view imputation with generative adversarial networks},
year={2017},
volume={},
number={},
pages={766-775},
abstract={In an era when big data are becoming the norm, there is less concern with the quantity but more with the quality and completeness of the data. In many disciplines, data are collected from heterogeneous sources, resulting in multi-view or multi-modal datasets. The missing data problem has been challenging to address in multi-view data analysis. Especially, when certain samples miss an entire view of data, it creates the missing view problem. Classic multiple imputations or matrix completion methods are hardly effective here when no information can be based on in the specific view to impute data for such samples. The commonly-used simple method of removing samples with a missing view can dramatically reduce sample size, thus diminishing the statistical power of a subsequent analysis. In this paper, we propose a novel approach for view imputation via generative adversarial networks (GANs), which we name by VIGAN. This approach first treats each view as a separate domain and identifies domain-to-domain mappings via a GAN using randomly-sampled data from each view, and then employs a multi-modal denoising autoencoder (DAE) to reconstruct the missing view from the GAN outputs based on paired data across the views. Then, by optimizing the GAN and DAE jointly, our model enables the knowledge integration for domain mappings and view correspondences to effectively recover the missing view. Empirical results on benchmark datasets validate the VIGAN approach by comparing against the state of the art. The evaluation of VIGAN in a genetic study of substance use disorders further proves the effectiveness and usability of this approach in life science.},
keywords={Gallium nitride;Data models;Generators;Noise reduction;Image reconstruction;Training;Matrix decomposition;missing data;missing view;generative adversarial networks;autoencoder;domain mapping;cycle-consistent},
doi={10.1109/BigData.2017.8257992},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7425974,
author={Sung-min Kim and Young-guk Ha},
booktitle={2016 International Conference on Big Data and Smart Computing (BigComp)},
title={Automated discovery of small business domain knowledge using web crawling and data mining},
year={2016},
volume={},
number={},
pages={481-484},
abstract={It has become an era where everything is on the web with ever more chances of data utilization on the web. Still, there are obstacles to make the use of the web efficiently. With too much information, Internet users have often come across information that are not relevant for their use. On top of that, until recently, most of web content have not contained semantic information, posing difficulties for mechanical analysis. The Semantic Web emerged as a way to tackle those poor qualities of the web. Adopting formal languages such as RDF or OWL, the semantic web has made the Internet become more highly available for computer-based analysis. In this study, what we aimed at is building a small business knowledge base to provide useful information for small business owners for their marketing strategies or dynamic QA systems for their restaurant recommendation services. The knowledge base was built according to the concept of the Semantic Web. To build the knowledge base, first, it is needed to conduct web crawling from different web sources including social media. However, the crawled data typically come in informal and do not have any semantic information. So we devised text mining techniques to catch useful information from them and generate formal knowledge for the knowledge base.},
keywords={Business;Knowledge based systems;OWL;Media;Knowledge discovery;semantic web;knowledge base;knowledge discovery;small business;social data analysis;web crawling;data mining},
doi={10.1109/BIGCOMP.2016.7425974},
ISSN={2375-9356},
month={Jan},}
@INPROCEEDINGS{6544913,
author={Condie, Tyson and Mineiro, Paul and Polyzotis, Neoklis and Weimer, Markus},
booktitle={2013 IEEE 29th International Conference on Data Engineering (ICDE)},
title={Machine learning on Big Data},
year={2013},
volume={},
number={},
pages={1242-1244},
abstract={Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.},
keywords={Databases;Seminars;Machine learning algorithms;Tutorials;Communities;Computational modeling;Big data},
doi={10.1109/ICDE.2013.6544913},
ISSN={1063-6382},
month={April},}
@ARTICLE{7872424,
author={Gu, Ke and Tao, Dacheng and Qiao, Jun-Fei and Lin, Weisi},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Learning a No-Reference Quality Assessment Model of Enhanced Images With Big Data},
year={2018},
volume={29},
number={4},
pages={1301-1313},
abstract={In this paper, we investigate into the problem of image quality assessment (IQA) and enhancement via machine learning. This issue has long attracted a wide range of attention in computational intelligence and image processing communities, since, for many practical applications, e.g., object detection and recognition, raw images are usually needed to be appropriately enhanced to raise the visual quality (e.g., visibility and contrast). In fact, proper enhancement can noticeably improve the quality of input images, even better than originally captured images, which are generally thought to be of the best quality. In this paper, we present two most important contributions. The first contribution is to develop a new no-reference (NR) IQA model. Given an image, our quality measure first extracts 17 features through analysis of contrast, sharpness, brightness and more, and then yields a measure of visual quality using a regression module, which is learned with big-data training samples that are much bigger than the size of relevant image data sets. The results of experiments on nine data sets validate the superiority and efficiency of our blind metric compared with typical state-of-the-art full-reference, reduced-reference and NA IQA methods. The second contribution is that a robust image enhancement framework is established based on quality optimization. For an input image, by the guidance of the proposed NR-IQA measure, we conduct histogram modification to successively rectify image brightness and contrast to a proper level. Thorough tests demonstrate that our framework can well enhance natural images, low-contrast images, low-light images, and dehazed images. The source code will be released at https://sites.google.com/site/guke198701/publications.},
keywords={Measurement;Feature extraction;Image quality;Training;Entropy;Visualization;Image enhancement;Big data learning;enhancement;image quality assessment (IQA);no-reference (NR)/blind},
doi={10.1109/TNNLS.2017.2649101},
ISSN={2162-2388},
month={April},}
@INPROCEEDINGS{8029847,
author={Hu, Guoqiang and Zhang, Xin and Duan, Ning and Gao, Peng},
booktitle={2017 IEEE International Conference on Web Services (ICWS)},
title={Towards Reliable Online Services Analyzing Mobile Sensor Big Data},
year={2017},
volume={},
number={},
pages={849-852},
abstract={Sensors are pervasively deployed on mobile devices with the development of Internet of Things technology. Value-added services are innovated and developed by analyzing data streams from massive number of mobile sensors in online mode. Due to dynamic working condition of mobile sensors and the high data rate, back end analytic services confront incoming streams with large rate fluctuation and out-of-order time series. This puts forward special challenges in service implementation for commercial applications, where good reliability/scalability performance is a must. In this paper, a data ingestion and scheduling framework is proposed to enable large-scale tempo-spatial streams analysis in a reliable and cost-effective way. A case study on a real world application adopting this framework is introduced and its pilot result is presented.},
keywords={Mobile communication;Air quality;Fluctuations;Queueing analysis;Protocols;Rectifiers;component;mobile sensor;online analytic services;data streams;Internet of Things},
doi={10.1109/ICWS.2017.104},
ISSN={},
month={June},}
@INPROCEEDINGS{9260346,
author={Xue, Bi},
booktitle={2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={An Optimization Scheme of Network Marketing Based on Big Data},
year={2020},
volume={},
number={},
pages={466-469},
abstract={In this paper, from the perspective of consumers, the personalized information service mode of e-commerce is systematically studied through the research on the influencing factors of users' purchase of personalized recommended goods under the background of big data. Based on the analysis of the influencing factors of personalized online marketing recommendation service, the influencing factor model of personalized recommendation service is established, and a targeted questionnaire survey is formed. Then, we use the recommendation technology based on collaborative filtering to compare the marketing effect. Finally, combining the theory of big data and precision marketing, it is comprehensively applied to the marketing plan of e-commerce enterprises. By optimizing the precision marketing strategy of a large-scale shopping website, it can solve the actual problems in current online marketing. It is hoped that the strategy can provide a reference for other enterprises in the same type of e-commerce field during the process of precision marketing.},
keywords={Quality function deployment;Voltage control;Smart grids;Germanium;Conferences;Automation;big data;network marketing;advertising;recommendation},
doi={10.1109/ICSGEA51094.2020.00107},
ISSN={},
month={June},}
@INPROCEEDINGS{9243708,
author={Petrova-Antonova, Dessislava and Baychev, Stefan and Pavlova, Irena and Pavlov, Georgi},
booktitle={2020 5th International Conference on Smart and Sustainable Technologies (SpliTech)},
title={Air Quality Visual Analytics with Kibana},
year={2020},
volume={},
number={},
pages={1-6},
abstract={The recent studies report that the short-term fluctuations of air pollution levels are directly related to the hospital admissions of patients with pneumonia and bronchitis. In addition, the long-term exposure to air pollution causes significant health problems, including cardiovascular disease, lung cancer and respiratory disease such as emphysema. At the same time, a huge amount of air quality data is collected by public air quality monitoring systems in different parts of the world. Official reporting from government is one of reliable sources of data. The European Environment Agency for Europe's Air Quality and Clean Air Asia databases, the Global Burden of Disease epidemiological study, and peer-reviewed journal articles are other sources. Multidimensional visualization of such big amounts of data, including temporal granularities and spatial distribution, is a challenging question. In order to address this challenge, the paper proposes a software solution for visual analytics of air quality data using the potential of Big Data technologies. Its architecture, implementation with Elasticsearch and Kibana, and actual results from data visualization are presented. The findings of the paper show that the proposed solution provides more intuitive perception and valuable insight through multi-perspective air pollution graphs.},
keywords={Visual analytics;Atmospheric modeling;Data visualization;Europe;Big Data;Air pollution;Data models;air pollution;Elasticsearch;Kibana;spatial- temporal data visualization;visual analytics},
doi={10.23919/SpliTech49282.2020.9243708},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9458218,
author={Wanganga, George and Qu, Yanzhen},
booktitle={2020 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={An Auto Optimized Payment Service Requests Scheduling Algorithm via Data Analytics through Machine Learning},
year={2020},
volume={},
number={},
pages={1498-1502},
abstract={Traditional customer payment service scheduling approaches cannot cope with the modern demand for timely, high-quality service due to the disruption of big data within small and medium-sized payment solution providers (SaMS-PSP). While many customers have access to modern technologies to lodge their service requests easily and fast, SaMS-PSPs do not have equally automated big data-driven capabilities to handle the growing demands of these service requests. To effectively improve SaMS-PSP’s customer payment service requests processing speeds, personnel optimization, throughput, and low latency scheduling, we have developed a new customer payment service request scheduling algorithm via matching request priority with the best personnel to handle the request based on data analytics through machine learning. Our experiments and testing have confirmed the merits of this new algorithm. We are also in the process of applying this new algorithm in real-world payment operations.},
keywords={Machine learning algorithms;Data analysis;Scheduling algorithms;Machine learning;Throughput;Scheduling;Data models;Big data;big data analytics;priority scheduling;machine learning;data analytics;performance optimization},
doi={10.1109/CSCI51800.2020.00277},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7529531,
author={Van-Dai Ta and Chuan-Ming Liu and Nkabinde, Goodwill Wandile},
booktitle={2016 IEEE International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Big data stream computing in healthcare real-time analytics},
year={2016},
volume={},
number={},
pages={37-42},
abstract={The healthcare industry is changing at a dramatic rate. There are multiple processes going on within the health sector. These processes not only impact the care of individuals but also help medical practitioners and the delivery of care and services. The industry can take advantage of big data analytics to ensure that all the multiple processes within the industry are running smoothly. Big data analytics is not just an opportunity but a necessity. Recently, big data stream computing has been studied in order to improve the quality of healthcare services and reduce costs by capability support prediction, thus making decisions in real-time. This paper proposes a generic architecture for big data healthcare analytic by using open sources, including Hadoop, Apache Storm, Kafka and NoSQL Cassandra. The combination of high throughput publish-subscribe messaging for streams, distributed real-time computing, and distributed storage system can effectively analyze a huge amount of healthcare data coming with a rapid rate.},
keywords={Medical services;Monitoring;Biomedical monitoring;Genomics;Bioinformatics;Magnetic resonance imaging;Manuals;big data;stream computing;real-time;healthcare analytics;storm;Kafka;NoSQL Cassandra},
doi={10.1109/ICCCBDA.2016.7529531},
ISSN={},
month={July},}
@INPROCEEDINGS{7119147,
author={Parthasarathy, Aditi and Chaturvedi, Abha and Kokane, Shashwati and Warty, Chirag and Nema, Shikha},
booktitle={2015 IEEE Aerospace Conference},
title={Transmission of big data over MANETs},
year={2015},
volume={},
number={},
pages={1-6},
abstract={Big data recently has gained tremendous importance in the way information is being disseminated. Transaction based data, unstructured data streaming to and fro from social media, increasing amounts of sensor and machine-to-machine data and many such examples rely on big data in conjunction with cloud computing. It is desirable to create wireless networks on-the-fly as per the demand or a given situation. In such a scenario reliable transmission of big data over mobile Ad-Hoc networks plays a key role. Limitations like low bandwidth, congestion and loss of packets pose a challenge for such systems. Hence an effective routing mechanism plays an important role. The proposed protocol is Multi-path QoS Routing (MPQR) protocol. Existing protocols try to establish a single path for communication. The proposed paper focuses on distributing tickets in the network. Also it can be divided into sub-tickets to get an optimum multi-path. The principal advantage is its high performance in the case of bandwidth limited environments when compared to existing protocols.},
keywords={Bandwidth;Routing protocols;Quality of service;Ad hoc networks;Routing;Mobile computing;Big Data;Mobile ad hoc network (MANET);Multi-path QoS Routing (MPQR) protocol;Quality-of-service (QoS)},
doi={10.1109/AERO.2015.7119147},
ISSN={1095-323X},
month={March},}
@INPROCEEDINGS{8572185,
author={Deshpande, Priya and Rasin, Alexander and Brown, Eli and Furst, Jacob and Raicu, Daniela S. and Montner, Steven M. and Armato, Samuel G.},
booktitle={2018 IEEE Life Sciences Conference (LSC)},
title={Big Data Integration Case Study for Radiology Data Sources},
year={2018},
volume={},
number={},
pages={195-198},
abstract={Today's digitized world urgently needs Big Data integration and analysis. Healthcare records are responsible for generating petabytes of data in a single day. Such data is heterogeneous in nature, captured in different files and formats, and varies from hospital to hospital. By integrating data from different sources and extracting meaningful information for the medical community, we can improve the overall quality of patient care. Our research targets the problem of integration for health records. To start, we already developed the Integrated Radiology Image search (IRIS) engine, which could represent a data integration framework for the healthcare domain. IRIS provided support for multiple public data sources and incorporated medical ontologies which would help radiologists and improve search interpretation by considering the meaning of the search query terms. In this paper, we describe a case study of data integration for radiology data sources. While the need for data integration is self-evident, we learned that rather than being a single step, data integration is an iterative process that requires continuous integration of metadata and additional supporting data sources. Our results show that an each step of data integration further improved IRIS engine results.},
keywords={Radiology;Education;Medical services;Ontologies;Medical diagnostic imaging;Data integration},
doi={10.1109/LSC.2018.8572185},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7916864,
author={Gowri, R. and Rathipriya, R.},
booktitle={2016 Online International Conference on Green Engineering and Technologies (IC-GET)},
title={Quality based clustering using MapReduce framework},
year={2016},
volume={},
number={},
pages={1-5},
abstract={The problem of data deluge is prevailing everywhere. Analyzing voluminous and variety of data is a great challenge to the researchers. The MapReduce framework is adapted to many computational methodologies to overcome these issues. Clustering is one of the most commonly used data mining techniques in various pattern analysis applications. This paper is mainly focuses on quality based data clustering using MapReduce framework for fast processing. In order to satisfy the many pattern analysis research applications.},
keywords={Indexes;Mathematical model;Data mining;Correlation;Parallel processing;Big Data;Couplings;Map Reduce;Clustering;Davies Bouldin Index;Average Correlation Value;Cluster quality;Parallel Computing;Distributed Computing;Big Data},
doi={10.1109/GET.2016.7916864},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9060352,
author={Xiao, Yunlong and Gu, Yang and Wang, Jiwei and Wu, Tong},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={A Collaborative Multi-modality Selection Method Based on Data Utility Assessment},
year={2019},
volume={},
number={},
pages={454-459},
abstract={Multimodal fusion is more and more widely used in the field of machine learning, but it faces a prominent problem in practical application: data utility is not stable. The data of different modalities may be missing and noisy randomly, which will interfere the machine learning model of multi-modal fusion. Most of the existing multi-modal fusion methods neglect data utility problems or only adopt simple data denoising methods to improve data utility. To solve the problem of unstable data utility, we propose a data selection method based on the evaluation of data utility. By training a special machine learning model, the optimal modal combination is predicted according to the quality evaluation of multi-modal data samples to accomplish the dynamic selection of data modalities. The experimental results show that the proposed method can effectively improve the accuracy of multi-modal recognition under low data utility.},
keywords={Data integrity;Machine learning;Data models;Training;Task analysis;Mathematical model;Gesture recognition;data selection;multimodal;data utility;data quality assessment},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00120},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9148843,
author={Xia, Qiufen and Bai, Luyao and Xu, Zichuan and Liang, Weifa and Rana, Omer and Wu, Guowei},
booktitle={ICC 2020 - 2020 IEEE International Conference on Communications (ICC)},
title={Learning-based Online Query Evaluation for Big Data Analytics in Mobile Edge Clouds},
year={2020},
volume={},
number={},
pages={1-7},
abstract={The rise of big data brings extraordinary benefits and opportunities to businesses and governments. Enterprise users can analyze their consumers' data and infer the business value obtained, such as purchasing goods correlations, customer preferences, and hidden patterns. Meanwhile, with the emerge of big data processing frameworks, such as Hadoop and Tensor-flow, more and more mobile users are embracing big data analytics by issuing queries to analyze their data. In this paper, we investigate the problem of Quality-of-Service (QoS) aware query evaluation for big data analytics in a mobile edge cloud to maximize the system throughput while minimizing the query evaluation time of each admitted query, by exploring the materialization of intermediate query results. We consider dynamic big-data query evaluations where user queries arrive one by one without the knowledge of future arrivals, and the system needs to respond to each query by accepting or rejecting the query immediately. We propose an online algorithm for query admissions within a finite time horizon, the proposed algorithm can intelligently determine whether some immediate results during a query evaluation need to be materialized for later use of other queries, by making use of the Reinforcement Learning (RL) method with predictions. We finally investigate the performance of the proposed algorithm by simulations, and results show that the performance of the proposed algorithm is promising, by achieving a higher system throughput while reducing the average evaluation cost per query by from 20% to 52% compared to the comparison benchmarks.},
keywords={Big Data;Query processing;Quality of service;Delays;Heuristic algorithms;Throughput;Prediction algorithms},
doi={10.1109/ICC40277.2020.9148843},
ISSN={1938-1883},
month={June},}
@INPROCEEDINGS{9439788,
author={Bohlouli, Mahdi and Schrage, Martin},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Scalable Multi-Criteria Decision-Making: A MapReduce deployed Big Data Approach for Skill Analytics},
year={2020},
volume={},
number={},
pages={1-9},
abstract={The main question in today's rapidly changing world is how fast and what sort of corresponding knowledge should an agent be adopted to?! This can be defined as knowledge mapping problem for decision based on large scale datasets with veracity and accuracy as key criteria, especially in safety-critical systems. The following paper proposes a hybrid ans scalable approach for Multi-Criteria Decision Making (MCDM) problems that is deployed in MapReduce. The main sector specific problem that is solved is to recommend training resources that efficiently improves skill gaps of job seekers. The main innovations of this work are: (1) the use of large scale semi-real skill analytics and training resources dataset (Dataset Perspective), (2) a hybrid MCDM approach that resolves skill gaps by matching required skills to the training resources (Decision Support Perspective). This can be applied to any other sector with the context of matching problems. (3) the use of MapReduce as scalable processing approach to deliver lower processing latency and higher quality for large scale datasets (Big Data and Scalability Perspective). The experimental results showed 89% accuracy in the clustering and matching results. The recommendation results have been tested and verified with the industrial partner.},
keywords={Training;Technological innovation;Scalability;Conferences;Decision making;Big Data;MapReduce;Sclable Clustering;AHP;TOPSIS;Skill Analytics},
doi={10.1109/BigData50022.2020.9439788},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8258006,
author={Baeza-Yates, Ricardo and Liaghat, Zeinab},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Quality-efficiency trade-offs in machine learning for text processing},
year={2017},
volume={},
number={},
pages={897-904},
abstract={As the amount of available digital documents keeps growing rapidly, extracting useful information from them has become a major challenge. Data mining, natural language processing, and machine learning are powerful techniques that can be used together to deal with this problem. Depending on the task at hand, there are many different approaches that can be used. The methods available are continuously improved, but not all of them have been tested and compared in a set of coherent problems using supervised machine learning algorithms. For example, what happens to the quality of the methods if we increase the training data size from, say, 100 MB to over 1 GB? Moreover, are quality gains worth it when the rate of data processing diminishes? Can we trade quality for time efficiency and recover the quality loss by just being able to process more data? We attempt to answer these questions in a general way for text processing tasks, considering the trade-offs involving training data size, learning time, and quality obtained. For this, we propose a performance trade-off framework and apply it to three important tasks: Named Entity Recognition, Sentiment Analysis and Document Classification. These problems were also chosen because they have different levels of object granularity: words, paragraphs, and documents. For each problem, we selected several supervised machine learning algorithms and we evaluated the trade-offs of them on large publicly available data sets (news, reviews, patents). To explore these trade-offs, we use different data subsets of increasing size ranging from 50 MB to several GB. For the last two tasks, we also consider similar algorithms with two different data sets and two evaluation techniques, to study their impact on the resulting trade-offs. We find that the results do not change significantly and that most of the time the best algorithms are the ones with fastest processing time. However, we also show that the results for small data (say less than 100 MB) are different from the results for big data and in those cases the best algorithm is much harder to determine.},
keywords={Algorithm design and analysis;Machine learning algorithms;Training data;Training;Text processing;Patents;Supervised machine learning algorithms;text processing;algorithmic trade-offs;learning trade-offs},
doi={10.1109/BigData.2017.8258006},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9107804,
author={Ju, Xingang and Lian, Feiyu and Zhang, Yuan},
booktitle={2019 6th International Conference on Information Science and Control Engineering (ICISCE)},
title={Data Cleaning Optimization for Grain Big Data Processing using Task Merging},
year={2019},
volume={},
number={},
pages={225-233},
abstract={Data quality has exerted important influence over the application of grain big data, so data cleaning is a necessary and important work. In MapReduce frame, we can use parallel technique to execute data cleaning in high scalability mode, but due to the lack of effective design there are amounts of computing redundancy in the process of data cleaning, which results in lower performance. In this research, we found some tasks often are carried out multiple times on same input files, or require same operation results in the process of data cleaning. For this problem, we proposed a new optimization technique that is based on task merge. By merging simple or redundancy computations on same input files, the number of the loop computation in MapReduce can be reduced greatly. The experiment shows, by this means, the overall system runtime is significantly reduced, which proves that the process of data cleaning is optimized. In this paper, we optimized several modules of data cleaning such as entity identification, inconsistent data restoration, and missing value filling. Experimental results show that the proposed method in this paper can increase efficiency for grain big data cleaning.},
keywords={grain big data;data cleaning;task merging;hadoop;mapReduce},
doi={10.1109/ICISCE48695.2019.00053},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7508130,
author={Kaur, Preet Chandan and Ghorpade, Tushar and Mane, Vanita},
booktitle={2016 6th International Conference - Cloud System and Big Data Engineering (Confluence)},
title={Analysis of data security by using anonymization techniques},
year={2016},
volume={},
number={},
pages={287-293},
abstract={Modern technology generates such a huge amount of public and private datasets that its security becomes an inevitable task. Initially the priority was provided for data security for the data of organization's and firm's, but nowadays it is necessary to provide security for personal data as well. So to achieve the data security, it is mandatory to preserve the privacy of personal information for that we use anonymization technique such as generalization, bucketization, multi set-based generalization, one-attribute-per-column slicing, slicing and suppression with slicing are applied to avoid retrieval of data from database. Thus, privacy preservation means to protect the data value and it is used for data mining in order to get the valid and accurate results. These are discussed and successfully analyzed with different parameters such as revealed co-relation quality (linkage property), loss of information, type of data, security (privacy preserved) and membership disclosure in this paper. The analysis shows that suppression with slicing is an innovative technique that preserves the privacy of identity of an individual in a database better than previously mentioned techniques.},
keywords={Data privacy;Privacy;Diseases;Data security;Databases;Anonymization;Data Security;Identity Disclosure;Privacy Preservation;Published Data},
doi={10.1109/CONFLUENCE.2016.7508130},
ISSN={},
month={Jan},}
@INPROCEEDINGS{9221699,
author={Yao, Wei},
booktitle={2020 International Workshop on Electronic Communication and Artificial Intelligence (IWECAI)},
title={Path Analysis of Using Big Data to Innovate Archives Management Model and Improve Service Ability},
year={2020},
volume={},
number={},
pages={91-93},
abstract={Archives management is an important part of the administrative institutions. Under the new situation, archives management departments at all levels must make full use of various high-quality resources and advanced science and technology, so as to improve the overall quality and service of archives management in China. The article mainly analyzes the current status under the background of big data, and analyzes the innovative path of model with the relevant principles, hoping to help archives management in public institutions.},
keywords={Analytical models;Conferences;Big Data;Data models;Information technology;Artificial intelligence;archives;innovation;development;principle;path},
doi={10.1109/IWECAI50956.2020.00025},
ISSN={},
month={June},}
@INPROCEEDINGS{9260363,
author={Shuang, Huang},
booktitle={2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Research on E-commerce Credit Information Evaluation Based on Social Big Data},
year={2020},
volume={},
number={},
pages={510-514},
abstract={In this paper, the complexity of business data in social business environment is analyzed. Firstly, the POS rule matching method is used to extract feature points from user evaluation text. Then, we use extended matching method to select effective POS rules from a large number of training comment texts, use effective POS rules to extract feature viewpoint pairs from test comment texts, and provide pruning method to delete invalid feature viewpoint pairs. A product recommendation model based on feature view pairs in online reviews is proposed. In the feature view pair extraction, the feature view pairs are extracted by dynamic window according to the matching relationship of multiple sentence structures and feature views. Combined with the viewpoint of product feature tree aggregation feature, the product feature score is given. Finally, it is found that the improved comprehensive model has better topic recognition effect than standard LDA model. The improved model can help e-commerce enterprises to recommend hot topics on micro-blog and help users to make purchase decisions.},
keywords={Quality function deployment;Zirconium;Radio frequency;Smart grids;Germanium;Conferences;Automation;social business;micro-blog;big data;POS;feature extraction},
doi={10.1109/ICSGEA51094.2020.00116},
ISSN={},
month={June},}
@INPROCEEDINGS{8126088,
author={Salavati, Hassan and Gandomani, Taghi Javdani and Sadeghi, Rasool},
booktitle={2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},
title={A robust software architecture based on distributed systems in big data healthcare},
year={2017},
volume={},
number={},
pages={1701-1705},
abstract={Healthcare information is growing significantly and using big data solutions like NoSQL databases for huge volume of data and data processing distribution are urgency. Moreover, various standards in electronic health record (EHR) employe an online transaction processing (OLTP) database that interacts with family health team to integrate all patient's clinical data from birth to death. In contrast, OLTP database management systems are not suitable for large scale distribution environment and NoSQL databases is not compatible to transactional and interactional processes. The main aim of this paper is to describe a robust architecture, based on distributed system and compatible with EHR application requirements in order to reduce OLTP limitations. The experimental results indicate up to 100% logical data to improve system performance on parallel processing and 76% data availability on EHR distribution at worst case. Moreover, some important quality features like maintainability, testability, scalability environment compatibility, load distribution are supported in proposed software architecture.},
keywords={Distributed databases;Medical services;Big Data;Computer architecture;Routing;NoSQL databases;big data healthcare;distributed system;software architecture;design pattern;EHR},
doi={10.1109/ICACCI.2017.8126088},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8411081,
author={Al-Jaroodi, Jameela and Mohamed, Nader},
booktitle={2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)},
title={Service-Oriented Architecture for Big Data Analytics in Smart Cities},
year={2018},
volume={},
number={},
pages={633-640},
abstract={A smart city has recently become an aspiration for many cities around the world. These cities are looking to apply the smart city concept to improve sustainability, quality of life for residents, and economic development. The smart city concept depends on employing a wide range of advanced technologies to improve the performance of various services and activities such as transportation, energy, healthcare, and education, while at the same time improve the city's resources utilization and initiate new business opportunities. One of the promising technologies to support such efforts is the big data technology. Effective and intelligent use of big data accumulated over time in various sectors can offer many advantages to enhance decision making in smart cities. In this paper we identify the different types of decision making processes involved in smart cities. Then we propose a service-oriented architecture to support big data analytics for decision making in smart cities. This architecture allows for integrating different technologies such as fog and cloud computing to support different types of analytics and decision-making operations needed to effectively utilize available big data. It provides different functions and capabilities to use big data and provide smart capabilities as services that the architecture supports. As a result, different big data applications will be able to access and use these services for varying proposes within the smart city.},
keywords={Smart cities;Big Data;Cloud computing;Decision making;Wireless sensor networks;Service-oriented architecture;Smart City;Big Data;Service-Oriented Architecture;Cloud Computing;Fog Computing;Middleware},
doi={10.1109/CCGRID.2018.00052},
ISSN={},
month={May},}
@INPROCEEDINGS{8622353,
author={Vogt, Marco and Stiemer, Alexander and Schuldt, Heiko},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Polypheny-DB: Towards a Distributed and Self-Adaptive Polystore},
year={2018},
volume={},
number={},
pages={3364-3373},
abstract={Cloud providers are more and more confronted with very diverse and heterogeneous requirements their customers impose on the management of data. First, these requirements stem from service-level agreements that specify a desired degree of availability and a guaranteed latency. As a consequence, Cloud providers replicate data across data centers or availability zones and/or partition data and place it close to the location of their customers. Second, the workload at each Cloud data center or availability zone is diverse and may significantly change over time - e. g., an OLTP workload during regular business hours and OLAP analyzes over night. For this, polystore and multistore databases have recently been introduced as they are intrinsically able to cope with such mixed and varying workloads. While the problem of heterogeneous requirements on data management in the Cloud is either addressed at global level by replicating and partitioning data across data centers or at local level by providing polystore systems in a Cloud data center, there is no integrated solution that leverages the benefits of both approaches. In this paper, we present the Polypheny-DB vision of a distributed polystore system that seamlessly combines replication and partitioning with local polystores and that is able to dynamically adapt all parts of the system when the workload changes. We present the basic building blocks for both parts of the system and we discuss open challenges towards the implementation of the Polypheny-DB vision.},
keywords={Data centers;Distributed databases;Database systems;Business;Protocols;Big Data;Polystore database;data replication;data partitioning},
doi={10.1109/BigData.2018.8622353},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7432708,
author={Zhang, Lu and Chen, Yanxia and Zhu, Jie and Pan, Mingyu and Sun, Zhou and Wang, Weixian},
booktitle={2015 5th International Conference on Electric Utility Deregulation and Restructuring and Power Technologies (DRPT)},
title={Data quality analysis and improved strategy research on operations management system for electric vehicles},
year={2015},
volume={},
number={},
pages={2715-2720},
abstract={It is very important for Operations Management System (OMS) and big data analysis application to improve the data quality of Electric Vehicle (EV) charging service. This paper focuses on the charging transaction record data from the Beijing EV charging OMS, and analyzes error types and distributed locations of the abnormal data. Based on the mathematical logic among various kinds of operation data, the data selection rules and processing method are proposed, and the system improved scheme is given. Through the design and application of data selection and processing module, the abnormal data can be timely detected and corrected. It is also beneficial for the system operation and maintenance to improve the acquisition data quality. The comparative analysis results verify the feasibility and effectivity of the proposed scheme. This research is a necessary guarantee for the big data technology application.},
keywords={Big data;Distributed databases;Decision support systems;Power industry;Electric vehicles;Systems operation;Maintenance engineering;electric vehicle;operations management system;big data;data quality;data selection and processing},
doi={10.1109/DRPT.2015.7432708},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8047109,
author={Feng, Jianbo},
booktitle={2016 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={Atmosphere Quality Monitoring and Analysis Based on Intelligent Algorithm},
year={2016},
volume={},
number={},
pages={77-80},
abstract={Data fusion theory is adopted in this article to discuss the ANN's implementation on data process and corresponding improved schemes, and the integration advantage of GA and neural network is also explained. Then we propose a GA based neural network model to realize data fusion with self learning habits and self organizing of BP network. The improved scheme overcomes the deficiency of BP algorithm in slow convergence speed, and getting local minimum. The example of environment quality evaluation is used to improve the prediction accuracy. The experiments also show the evaluation result of our method is more accurate than that of expert evaluation method, which provides a novel research idea.},
keywords={Transportation;Big Data;Smart cities;atmosphere quality;GA;BP network;intelligent algorithm},
doi={10.1109/ICITBS.2016.128},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8284099,
author={Zaffar, Maryam and Hashmani, Manzoor Ahmed and Savita, K. S.},
booktitle={2017 IEEE Conference on Big Data and Analytics (ICBDA)},
title={Performance analysis of feature selection algorithm for educational data mining},
year={2017},
volume={},
number={},
pages={7-12},
abstract={Student's academic performance is the main focus of all educational institutions. Educational Data Mining (EDM) is an emerging research area help the educational institutions to improve the performance of their students. Feature Selection (FS) algorithms remove irrelevant data from the educational dataset and hence increases the performance of classifiers used in EDM techniques. This paper present an analysis of the performance of feature selection algorithms on student data set. The obtained results of the different FS algorithms and classifiers will also help the new researchers in finding the best combinations of FS algorithms and classifiers. Selecting relevant features for student prediction model is very sensitive issue for educational stakeholders, as they have to take decisions on the basis of results of prediction models. Furthermore our paper is an attempt of playing a positive role in the improvement of education quality, as well as guides new researchers in making academic intervention.},
keywords={Classification algorithms;Feature extraction;Data mining;Prediction algorithms;Niobium;Radio frequency;Algorithm design and analysis;Educational data mining;Feature Selection Algorithm;Classifiers},
doi={10.1109/ICBDAA.2017.8284099},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9150233,
author={Li-sha, YAO},
booktitle={2020 International Conference on Big Data and Informatization Education (ICBDIE)},
title={The Application of mixed online and offline teaching model based on MOOC+SPOC in Data Structure Course},
year={2020},
volume={},
number={},
pages={289-292},
abstract={In view of the current teaching problems of data structure courses and the characteristics of our university's specialty cultivation, a mixed online and offline teaching mode based on MOOC+SPOC is discussed and implemented. It relies on the national high-quality MOOC of data structure and constructs the SPOC course of data structure that meets the needs of the university with the help of the “super star” platform. The feasibility and effectiveness of the new teaching model are verified through the evaluation of teachers and students and the achievement of course objectives, which provides experience for promoting teaching reform.},
keywords={Education;Task analysis;Binary trees;Online services;Big Data;Data models;SPOC;online and offline;mixed teaching},
doi={10.1109/ICBDIE50010.2020.00073},
ISSN={},
month={April},}
@INPROCEEDINGS{8625275,
author={Canbek, Gürol and Sagiroglu, Seref and Taskaya Temizel, Tugba},
booktitle={2018 International Congress on Big Data, Deep Learning and Fighting Cyber Terrorism (IBIGDELFT)},
title={New Techniques in Profiling Big Datasets for Machine Learning with a Concise Review of Android Mobile Malware Datasets},
year={2018},
volume={},
number={},
pages={117-121},
abstract={As the volume, variety, velocity aspects of big data are increasing, the other aspects such as veracity, value, variability, and venue could not be interpreted easily by data owners or researchers. The aspects are also unclear if the data is to be used in machine learning studies such as classification or clustering. This study proposes four techniques with fourteen criteria to systematically profile the datasets collected from different resources to distinguish from one another and see their strong and weak aspects. The proposed approach is demonstrated in five Android mobile malware datasets in the literature and in security industry namely Android Malware Genome Project, Drebin, Android Malware Dataset, Android Botnet, and Virus Total 2018. The results have shown that the proposed profiling methods reveal remarkable insight about the datasets comparatively and directs researchers to achieve big but more visible, qualitative, and internalized datasets.},
keywords={Malware;Big Data;Machine learning;Mobile applications;Genomics;Bioinformatics;Aerospace electronics;data profiling;data quality;big data;malware detection;mobile malware;machine learning;classification;Android;feature engineering},
doi={10.1109/IBIGDELFT.2018.8625275},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7536334,
author={Lawson, Victor J. and Ramaswamy, Lakshmish},
booktitle={2016 International Conference on Distributed Computing in Sensor Systems (DCOSS)},
title={TAU-FIVE: A Multi-tiered Architecture for Data Quality and Energy-Sustainability in Sensor Networks},
year={2016},
volume={},
number={},
pages={169-176},
abstract={Current research on wireless sensor networks "WSNs" in the Internet of Things "IoT" has focused on performance, scalability and energy efficiency. Innovations in these areas have many challenges due to the increasing volume of smart device data streams in the internet of Everything "IoE". Data feeds from future IoE systems such as the internet of vehicles, smart homes and smart-cities will need real time consolidation. This merger of technologies will require innovative big data algorithms and architectures that authenticate the data streams. A primary concern is in dynamically quantifying the data quality "DQ" of the streams while constructing real-time metrics to assess the energy efficiency "EE" of these IoE devices. In order to define the relationship between sensor stream DQ and EE, we propose our multi-tiered cloud-service architecture TAU-FIVE. The technical contributions of our framework includes data quality and energy efficiency models based on 7 DQ attributes and multiple reprogrammable smart sensors that dynamically modify and regulate the DQ and EE of a WSN. Our research maintains that WSN's can balance sustainability with quality of service by creating real-time metrics that merge energy usage with data stream integrity. This equilibrium will impact energy awareness in the IoT as the multitude of batch device data streams are integrated with the variety of social and professional networks and evolve into the IoE.},
keywords={Measurement;Feeds;Computer architecture;Wireless sensor networks;Energy efficiency;Clouds;Data models;Data quality;cloud computing;energy model;applications to sensing;green networks},
doi={10.1109/DCOSS.2016.42},
ISSN={2325-2944},
month={May},}
@INPROCEEDINGS{8780271,
author={Yu, Wenjin and Dillon, Tharam and Mostafa, Fahed and Rahayu, Wenny and Liu, Yuehua},
booktitle={2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS)},
title={Implementation of Industrial Cyber Physical System: Challenges and Solutions},
year={2019},
volume={},
number={},
pages={173-178},
abstract={The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards.},
keywords={Big Data;Manufacturing;Industries;Sensors;Data integrity;Real-time systems;Cloud computing;Cyber-Physical System;Internet of Things;Industry 4.0;cloud computing;big data ecosystem;data quality},
doi={10.1109/ICPHYS.2019.8780271},
ISSN={},
month={May},}
@INPROCEEDINGS{8609555,
author={Liu, Bangqi and Li, Xin and Wang, Yafei and Wang, Haoran and Xu, Feiyang},
booktitle={2018 International Joint Conference on Information, Media and Engineering (ICIME)},
title={The System Framework of Data Mining and Learning Analysis for Smart Classroom},
year={2018},
volume={},
number={},
pages={331-336},
abstract={The research topics in educational data mining area at home and abroad are transforming from the concept, principle level to modeling analysis and application level, and at the same time transforming from statistical monitoring of educational quality, educational policy and other macro-level applications to the further development of school teaching, student learning and other micro applications. On the conceptual discussion of educational data mining and learning analytics, this paper proposes a data model and a system architecture combined with the teaching practice, elaborates "Four modeling and three analysis" methodology in the study of education data, constructs The "whole chessboard" analysis of data mining in the smart classroom, in which 13 specific research questions are derived. Moreover, it puts forward four kinds of application modes of data mining in the smart classroom and finally explores the application of interaction between teacher and students of smart classroom based on real data.},
keywords={Education;Data mining;Analytical models;Data models;Indexes;Predictive models;Big Data;Smart Classroom, Big Data in Education, Data Mining, Learning Analysis Analytics, Teacher Student Interaction Index},
doi={10.1109/ICIME.2018.00077},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8882761,
author={Ullah, Faheem and Ali Babar, M.},
booktitle={2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS)},
title={QuickAdapt: Scalable Adaptation for Big Data Cyber Security Analytics},
year={2019},
volume={},
number={},
pages={81-86},
abstract={Big Data Cyber Security Analytics (BDCA) leverages big data technologies for collecting, storing, and analyzing a large volume of security events data to detect cyber-attacks. Accuracy and response time, being the most important quality concerns for BDCA, are impacted by changes in security events data. Whilst it is promising to adapt a BDCA system's architecture to the changes in security events data for optimizing accuracy and response time, it is important to consider large search space of architectural configurations. Searching a large space of configurations for potential adaptation incurs an overwhelming adaptation time, which may cancel the benefits of adaptation. We present an adaptation approach, QuickAdapt, to enable quick adaptation of a BDCA system. QuickAdapt uses descriptive statistics (e.g., mean and variance) of security events data and fuzzy rules to (re) compose a system with a set of components to ensure optimal accuracy and response time. We have evaluated QuickAdapt for a distributed BDCA system using four datasets. Our evaluation shows that on average QuickAdapt reduces adaptation time by 105× with a competitive adaptation accuracy of 70% as compared to an existing solution.},
keywords={Big Data;Quality of service;Time factors;Feature extraction;Computer crime;Computer architecture;big data, cyber security, adaptation, accuracy},
doi={10.1109/ICECCS.2019.00016},
ISSN={},
month={Nov},}
@ARTICLE{8291124,
author={Enayet, Asma and Razzaque, Md. Abdur and Hassan, Mohammad Mehedi and Alamri, Atif and Fortino, Giancarlo},
journal={IEEE Communications Magazine},
title={A Mobility-Aware Optimal Resource Allocation Architecture for Big Data Task Execution on Mobile Cloud in Smart Cities},
year={2018},
volume={56},
number={2},
pages={110-117},
abstract={In recent years, the smart city concept, which involves multiple disciplines, for example, smart healthcare, smart transportation, and smart community, has become popular because of its ability to improve urban citizens' quality of life. However, most services in these areas of smart cities have become data-driven, thus generating big data that require seamless real-time access, sharing, storing, processing, and analysis anywhere at any time for intelligent decision making to improve living standards. In this scenario, MCC can play a vital role by allowing a mobile device to access and offload big-data-related tasks to powerful cloudlet servers attached to many wireless APs, thus ensuring that the QoS demands of end users are met. However, the connectivity of mobile devices with a given AP is not continuous, but rather sporadic with varying signal strengths. Furthermore, the heterogeneity of the cloudlet resources and the big data application requests place additional challenges in making optimal code execution decision. To cope with this problem, this article proposes a mobility- aware optimal resource allocation architecture, namely Mobi-Het, for remote big data task execution in MCC that offers higher efficiency in timeliness and reliability. The system architecture and key components of the proposed resource allocation service are presented and evaluated. The results of experiments and simulations have demonstrated the effectiveness and efficiency of the proposed Mobi-Het architecture for mobile big data applications.},
keywords={Cloud computing;Mobile handsets;Smart cities;Big Data applications;Resource management;Mobile communication},
doi={10.1109/MCOM.2018.1700293},
ISSN={1558-1896},
month={Feb},}
@INPROCEEDINGS{8258504,
author={Moon, Aekyeung and Kim, Jaeyoung and Zhang, Jialing and Liu, Hang and Woo Son, Seung},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Understanding the impact of lossy compressions on IoT smart farm analytics},
year={2017},
volume={},
number={},
pages={4602-4611},
abstract={As the volume of data collected by various IoT stations increases, Big Data management and analytics becomes a huge challenge for IoT applications. Although Big Data can potentially benefit from data compression techniques, the chances are that compression will reduce a negligible amount of data such that it would not worth the effort. The insight of this paper is that only lossy compression can unleash the power of compression to IoT because, compared with its counterpart (lossless one), it can significantly reduce the data volume by taking advantages of spatiotemporal patterns. However, lossy compression faces the challenge of compressing too much data thus losing the data fidelity, which might affect the quality of analytics outcomes. To understand the impact of lossy compression on IoT data management and analytics, we evaluate several classification algorithms on agricultural sensor data reconstructed based on energy concentration. Specifically, we applied three transformation based lossy compression mechanisms to five real-world sensor data from IoT weather stations. Our experimental results indicate that there is a distinctive relationship between energy concentration on the transformed coefficients and compression ratio as well as the amount of error introduced. While we observe a general trend where the higher energy concentration the lower compression and error rates, we also observe that the impact on classification accuracy varies among data sets and algorithms we evaluated.},
keywords={Meteorology;Discrete cosine transforms;Temperature measurement;Agriculture;Discrete wavelet transforms;Temperature sensors;Smart farm;lossy compression;IoT;signal processing;data fidelity},
doi={10.1109/BigData.2017.8258504},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7995984,
author={Pillmann, Johannes and Wietfeld, Christian and Zarcula, Adrian and Raugust, Thomas and Alonso, Daniel Calvo},
booktitle={2017 IEEE Intelligent Vehicles Symposium (IV)},
title={Novel common vehicle information model (CVIM) for future automotive vehicle big data marketplaces},
year={2017},
volume={},
number={},
pages={1910-1915},
abstract={Even though connectivity services have been introduced in many of the most recent car models, access to vehicle data is currently limited due to its proprietary nature. The European project AutoMat has therefore developed an open Marketplace providing a single point of access for brand-independent vehicle data. Thereby, vehicle sensor data can be leveraged for the design and implementation of entirely new services even beyond traffic-related applications (such as hyper-local traffic forecasts). This paper presents the architecture for a Vehicle Big Data Marketplace as enabler of cross-sectorial and innovative vehicle data services. Therefore, the novel Common Vehicle Information Model (CVIM) is defined as an open and harmonized data model, allowing the aggregation of brand-independent and generic data sets. Within this work the realization of a prototype CVIM and Marketplace implementation is presented. The two use-cases of local weather prediction and road quality measurements are introduced to show the applicability of the AutoMat concept and prototype to non-automotive applications.},
keywords={Big Data;Automobiles;Data models;Cloud computing;Engines;Histograms;Standards},
doi={10.1109/IVS.2017.7995984},
ISSN={},
month={June},}
@INPROCEEDINGS{9302543,
author={Zada, Muhammad Sadiq Hassan and Yuan, Bo and Anjum, Ashiq and Azad, Muhammad Ajmal and Khan, Wajahat Ali and Reiff-Marganiec, Stephan},
booktitle={2020 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)},
title={Large-scale Data Integration Using Graph Probabilistic Dependencies (GPDs)},
year={2020},
volume={},
number={},
pages={27-36},
abstract={The diversity and proliferation of Knowledge bases have made data integration one of the key challenges in the data science domain. The imperfect representations of entities, particularly in graphs, add additional challenges in data integration. Graph dependencies (GDs) were investigated in existing studies for the integration and maintenance of data quality on graphs. However, the majority of graphs contain plenty of duplicates with high diversity. Consequently, the existence of dependencies over these graphs becomes highly uncertain. In this paper, we proposed graph probabilistic dependencies (GPDs) to address the issue of uncertainty over these large-scale graphs with a novel class of dependencies for graphs. GPDs can provide a probabilistic explanation for dealing with uncertainty while discovering dependencies over graphs. Furthermore, a case study is provided to verify the correctness of the data integration process based on GPDs. Preliminary results demonstrated the effectiveness of GPDs in terms of reducing redundancies and inconsistencies over the benchmark datasets.},
keywords={Probabilistic logic;Uncertainty;Data integration;Data integrity;Redundancy;Scalability;Erbium;data integration;information retrieval;NoSQL databases;graph probabilistic dependencies;data science},
doi={10.1109/BDCAT50828.2020.00028},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9260298,
author={Zirui, Gu},
booktitle={2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={An Evaluation Approach of Financial Performance of University Based on Big Data},
year={2020},
volume={},
number={},
pages={462-465},
abstract={In the big data age, the development and application of cloud accounting technology has an important impact on the financial performance management of enterprises and universities. This paper designs a set of evaluation subsystem based on the financial index data of a university. The system consists of attribute reduction and evaluation. Firstly, the attribute reduction method of rough set is used to eliminate the redundancy of indexes and reduce the calculation amount of later evaluation. Then, in the multi-attribute comprehensive evaluation method, the analytic hierarchy process and entropy method are analyzed to solve the problem of dimensional inconsistency in the index data, and the specific steps and workflow are described in detail. In the case analysis, the main modules of the system are tested. Finally, the tests show that our scheme can acquire a simplified index system, and the results meet the expected requirements.},
keywords={Quality function deployment;Zirconium;Smart grids;Hafnium;Germanium;Automation;evaluation;financial performance;rough set;big data;AHP},
doi={10.1109/ICSGEA51094.2020.00106},
ISSN={},
month={June},}
@INPROCEEDINGS{9064159,
author={Yang, Jie and Cao, Yong and Huang, Biao-sheng and Zhao, You-jie},
booktitle={2019 IEEE 5th International Conference on Computer and Communications (ICCC)},
title={A Ditributed Algorithm for Quality Assessment of Biological Sequencing Based on MapReduce},
year={2019},
volume={},
number={},
pages={188-192},
abstract={DNA sequencing technology has played an important role on life sciences, especially Illumina’s sequencer. It was used for more and more biological genomic and transcriptomic projects. Faced with the huge amount of biological sequencing data, it is a problem how to assess its quality quickly. In this paper, we developed a distributed algorithm based on MapReduce, which can assess the quality of biological sequencing in parallel. In order to validate the algorithm, different data sizes (1G - 20G) were used to test by different computing nodes (1 - 20) in Hadoop platform. The results show that the parallel efficiency improves continuously following with the increase of data size and computing nodes. And the algorithm has better parallel efficiency when data size and computing nodes greater than 5Gb and 10 processors. This work effectively saves the time of quality assessment of biological sequencing.},
keywords={Sequential analysis;Genomics;Bioinformatics;Quality assessment;DNA;Distributed algorithms;Distributed Computing;MapReduce;Biological Sequencing;Quality Assessment},
doi={10.1109/ICCC47050.2019.9064159},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7337020,
author={Asri, Hiba and Mousannif, Hajar and Al Moatassime, Hassan and Noel, Thomas},
booktitle={2015 International Conference on Cloud Technologies and Applications (CloudTech)},
title={Big data in healthcare: Challenges and opportunities},
year={2015},
volume={},
number={},
pages={1-7},
abstract={Mobile phones, sensors, patients, hospitals, researchers, providers and organizations are nowadays, generating huge amounts of healthcare data. The real challenge in healthcare systems is how to find, collect, analyze and manage information to make people's lives healthier and easier, by contributing not only to understand new diseases and therapies but also to predict outcomes at earlier stages and make real-time decisions. In this paper, we explain the potential benefits of big data to healthcare and explore how it improves treatment and empowers patients, providers and researchers. We also describe the ability of reality mining in collecting large amounts of data to understand people's habits, detect and predict outcomes, and illustrate the benefits of big data analytics through five effective new pathways that could be adopted to promote patients' health, enhance medicine, reduce cost and improve healthcare value and quality. We cover some big data solutions in healthcare and we shed light on implementations, such as Electronic Healthcare Record (HER) and Electronic Healthcare Predictive Analytics (e-HPA) in US hospitals. Furthermore, we complete the picture by highlighting some challenges that big data analytics faces in healthcare.},
keywords={Big data;Sensors;Data mining;Diseases;Mobile communication;Temperature measurement;big data;reality mining;healthcare;analytics},
doi={10.1109/CloudTech.2015.7337020},
ISSN={},
month={June},}
@INPROCEEDINGS{7751639,
author={Zhang, Tao and Cheng, Xinzhou and Yuan, Mingqiang and Xu, Lexi and Cheng, Chen and Chao, Kun},
booktitle={2016 16th International Symposium on Communications and Information Technologies (ISCIT)},
title={Mining target users for mobile advertising based on telecom big data},
year={2016},
volume={},
number={},
pages={296-301},
abstract={The mobile advertising industry in China has developed rapidly in recent years. Many companies and brands tend to employ mobile advertising in order to reach the target customers accurately. However, the conversion rates associated with the advertising campaigns are usually quite low due to the low quality of the datasets and impropriate predictive model. In this paper, we propose a novel mobile advertising system architecture based on telecom big data analytics. The defined multi-dimensional user portrait is introduced for user label oriented ad display strategy or as the basic database for the further user classification algorithm. We also adopt the widely used logistic regression algorithm in this paper to improve the target accuracy. The result of use case, which is calculated from the real-time collected cellular network data, also shows the superior performance of the proposed mobile advertising system.},
keywords={Decision support systems;Advertising;Handheld computers;Mobile communication;Telecommunications;Big data;Data models;mobile advertising;telecom big data;user portrait;user classification;targeting},
doi={10.1109/ISCIT.2016.7751639},
ISSN={},
month={Sep.},}
@ARTICLE{8746175,
author={Saberi, Morteza and Hussain, Omar Khadeer and Chang, Elizabeth},
journal={IEEE Access},
title={Quality Management of Workers in an In-House Crowdsourcing-Based Framework for Deduplication of Organizations’ Databases},
year={2019},
volume={7},
number={},
pages={90715-90730},
abstract={While organizations in the current era of big data are generating massive volumes of data, they also need to ensure that its quality is maintained for it to be useful in decision-making purposes. The problem of dirty data plagues every organization. One aspect of dirty data is the presence of duplicate data records that negatively impact the organization's operations in many ways. Many existing approaches attempt to address this problem by using traditional data cleansing methods. In this paper, we address this problem by using an in-house crowdsourcing-based framework, namely, DedupCrowd. One of the main obstacles of crowdsourcing-based approaches is to monitor the performance of the crowd, by which the integrity of the whole process is maintained. In this paper, a statistical quality control-based technique is proposed to regulate the performance of the crowd. We apply our proposed framework in the context of a contact center, where the Customer Service Representatives are used as the crowd to assist in the process of deduplicating detection. By using comprehensive working examples, we show how the different modules of the DedupCrowd work not only to monitor the performance of the crowd but also to assist in duplicate detection.},
keywords={Crowdsourcing;Databases;Task analysis;Object recognition;Monitoring;Error analysis;Big Data;Quality management;quality control;data quality;duplicate detection;in-house crowdsourcing},
doi={10.1109/ACCESS.2019.2924979},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9347261,
author={Xing, Yahong and Meng, Changhong and Li, Chunhui and Xi, Guoping and Jia, Xinping and Bai, Yang and Pang, Wei and Shen, Zeyuan and Zhang, Zhiwen},
booktitle={2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2)},
title={Lean Operation and Maintenance Evaluation Technology of Power Grid Equipment Based on Improved Big Data Cleaning Method},
year={2020},
volume={},
number={},
pages={2749-2752},
abstract={With the extensive application of information acquisition and transmission technology in power system, the extraction of equipment basic data becomes feasible. From the perspective of lean operation and maintenance, this paper systematically combs the scale structure, equipment quality dimension, equipment efficiency dimension and operation and maintenance cost dimension of distribution network equipment assets, and uses correlation analysis and principal component analysis to deeply mine the index data to study the power grid Data relationship of equipment related information. A data stream cleaning method based on sliding window is proposed to detect abnormal data sequence and clean data. The clustering factors are calculated by principal component analysis method, and 55 kinds of data are reduced. Finally, a set of 16 principal component data processing flow of distribution network equipment status is constructed. Finally, the feasibility and effectiveness of the evaluation index system and evaluation method are verified by comprehensive evaluation of the equipment status of a certain distribution network.},
keywords={Distribution networks;Maintenance engineering;Data processing;Power grids;Cleaning;Indexes;Principal component analysis;equipment;data cleaning;principal component analysis method},
doi={10.1109/EI250167.2020.9347261},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8781575,
author={Wang, Ko-Chih and Xu, Jiayi and Woodring, Jonathan and Shen, Han-Wei},
booktitle={2019 IEEE Pacific Visualization Symposium (PacificVis)},
title={Statistical Super Resolution for Data Analysis and Visualization of Large Scale Cosmological Simulations},
year={2019},
volume={},
number={},
pages={303-312},
abstract={Cosmologists build simulations for the evolution of the universe using different initial parameters. By exploring the datasets from different simulation runs, cosmologists can understand the evolution of our universe and approach its initial conditions. A cosmological simulation nowadays can generate datasets on the order of petabytes. Moving datasets from the supercomputers to post data analysis machines is infeasible. We propose a novel approach called statistical super-resolution to tackle the big data problem for cosmological data analysis and visualization. It uses datasets from a few simulation runs to create a prior knowledge, which captures the relation between low-and high-resolution data. We apply in situ statistical down-sampling to datasets generated from simulation runs to minimize the requirements of I/O bandwidth and storage. High-resolution datasets are reconstructed from the statistical down-sampled data by using the prior knowledge for scientists to perform advanced data analysis and render high-quality visualizations.},
keywords={Data models;Data analysis;Data visualization;Analytical models;Computational modeling;Image resolution;Bandwidth;ensemble data;cosmological data;in situ analysis},
doi={10.1109/PacificVis.2019.00043},
ISSN={2165-8773},
month={April},}
@INPROCEEDINGS{7583046,
author={Shi, Peichang and Li, Yiying and Ding, Bo and Jiang, Longquan and Liu, Hui and Zhang, Jie},
booktitle={2016 World Automation Congress (WAC)},
title={A data-driven mechanism for large-scale data distribution},
year={2016},
volume={},
number={},
pages={1-6},
abstract={As The integration of Physical space and cyberspace, the large-scale data distributing to diversification terminal which is geographical distribution of mass has become a huge challenge. When the data size can't be processed by the technology for traditional scope, how to deal with the user quality of service and efficient use of system resources has become an important issue of concern, with the resources becoming limited. This paper presents a data-driven mechanism for large-scale data distribution which is consists of four core part of the data production, data collection and pre-processing, data analysis engine, data consumption, aims to excavate the valuable information to improve the efficiency of resource use and accurate fault location for the Large-scale data distribution system. At the same time, this paper studies the resource scheduling optimization with analyzing data driven for the system behavior and Fault location with analyzing data-driven environment, which proves the effectiveness for the operation of the Large-scale data distribution system optimization by the data-driven working.},
keywords={Servers;Monitoring;Distributed databases;Big data;Real-time systems;Business;Data-driven;Large-scale Data Distribution;Monitoring and Analysis;Resource Scheduling;Fault Diagnosis},
doi={10.1109/WAC.2016.7583046},
ISSN={},
month={July},}