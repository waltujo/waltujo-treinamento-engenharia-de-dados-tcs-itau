@INPROCEEDINGS{8332632,
author={Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun},
booktitle={2017 14th Web Information Systems and Applications Conference (WISA)},
title={A Big Data Framework for Electric Power Data Quality Assessment},
year={2017},
volume={},
number={},
pages={289-292},
abstract={Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.},
keywords={Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework},
doi={10.1109/WISA.2017.29},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8029366,
author={Taleb, Ikbal and Serhani, Mohamed Adel},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={Big Data Pre-Processing: Closing the Data Quality Enforcement Loop},
year={2017},
volume={},
number={},
pages={498-501},
abstract={In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.},
keywords={Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing},
doi={10.1109/BigDataCongress.2017.73},
ISSN={},
month={June},}
@INPROCEEDINGS{9245455,
author={Loetpipatwanich, Sakda and Vichitthamaros, Preecha},
booktitle={2020 1st International Conference on Big Data Analytics and Practices (IBDAP)},
title={Sakdas: A Python Package for Data Profiling and Data Quality Auditing},
year={2020},
volume={},
number={},
pages={1-4},
abstract={Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.},
keywords={Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline},
doi={10.1109/IBDAP50342.2020.9245455},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8078796,
author={HongJu, Xiao and Fei, Wang and FenMei, Wang and XiuZhen, Wang},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
title={Some key problems of data management in army data engineering based on big data},
year={2017},
volume={},
number={},
pages={149-152},
abstract={This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.},
keywords={Data engineering;Data analysis;Data integration;Big Data;Distributed databases;Data models;Uncertainty;army data engineering;data management;data integration;data analysis;representation of data analysis results;data quality},
doi={10.1109/ICBDA.2017.8078796},
ISSN={},
month={March},}
@INPROCEEDINGS{9148352,
author={Peng, Zhibin and Chen, Yuefeng and Zhang, Zehong and Qiu, Queling and Han, Xiaoqiang},
booktitle={2020 International Conference on Computer Information and Big Data Applications (CIBDA)},
title={Implementation of Water Quality Management Platform for Aquaculture Based on Big Data},
year={2020},
volume={},
number={},
pages={70-74},
abstract={In order to ensure the quality and quantity of aquaculture, aquaculture farmers need to grasp the water quality in time. However, most farmers have to collect water quality data manually at present, and cannot store and reuse that information rapidly. This paper aims to use SpringBoot framework and JPA framework to build a big data platform of acquisition automation and visualization, which realizes the data analysis and display of heterogeneous water quality and breeding information. The platform can make the water quality prediction and real-time warning. Meanwhile, it realizes the management of robots, users and breeding experts. The application of this platform will bring better social benefits to aquaculture farmers.},
keywords={Data visualization;Aquaculture;Data mining;Big Data;Neural networks;Data models;Predictive models;aquaculture;big data;water quality warning;data visualization},
doi={10.1109/CIBDA50819.2020.00024},
ISSN={},
month={April},}
@INPROCEEDINGS{9005614,
author={Patel, Jayesh},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={An Effective and Scalable Data Modeling for Enterprise Big Data Platform},
year={2019},
volume={},
number={},
pages={2691-2697},
abstract={The enormous growth of the internet, enterprise applications, social media, and IoT devices in the current time caused a huge spike in enterprise data growth. Big data platform provided scalable storage to manage enterprise data growth and served easier data access to decision-makers, stakeholders and business users. It is a well-known challenge to classify, organize and store all this data and process it to provide business insights. Due to nature, variety, velocity, volume and value of data make it difficult to effectively process big data. Enterprises face challenges to apply complex business rules, to generate insights and to support data-driven decisions in a timely fashion. As big data lake integrates streams of data from a bunch of business units, stakeholders usually analyze enterprise-wide data from various data models. Data models are a vital component of Big data platform. Users may do complex processing, run queries and perform big table joins to generate required metrics depending on the available data models. It is usually a time consuming and resource-intensive process to find the value from data. It is a no-brainer that big data platform in the enterprise needs high-quality data modeling methods to reach an optimal mix of cost, performance, and quality. This paper addresses these challenges by proposing an effective and scalable way to organize and store data in Big Data Lake. It presents some of the basic principles and methodology to build scalable data models in a distributed environment. It also describes how it overcomes common challenges and presents findings.},
keywords={Data models;Big Data;Business;Analytical models;Computational modeling;Lakes;Solid modeling;Big Data;Big Data Lake;Scalable Data Modeling;Hadoop;Spark;Business Intelligence;Big Data Analytics},
doi={10.1109/BigData47090.2019.9005614},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8605945,
author={Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle={2018 International Conference on Innovations in Information Technology (IIT)},
title={Big Data Quality Assessment Model for Unstructured Data},
year={2018},
volume={},
number={},
pages={69-74},
abstract={Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.},
keywords={Big Data;Data integrity;Data mining;Feature extraction;Data models;Measurement;Quality assessment;Big Data;Data Quality;Unstructured Data;Quality of Unstructured Big Data},
doi={10.1109/INNOVATIONS.2018.8605945},
ISSN={2325-5498},
month={Nov},}
@INPROCEEDINGS{9006294,
author={Arruda, Darlan and Madhavji, Nazim H.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications},
year={2019},
volume={},
number={},
pages={5977-5979},
abstract={The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.},
keywords={Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool},
doi={10.1109/BigData47090.2019.9006294},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8622388,
author={Norman, Ryan and Bolin, Jason and Powell, Edward T. and Amin, Sanket and Nacker, John},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Using Big Data Analytics to Create a Predictive Model for Joint Strike Fighter},
year={2018},
volume={},
number={},
pages={3590-3596},
abstract={The amount of information needed to acquire knowledge on today's acquisition systems is growing exponentially due to more complex, higher resolution, software-intensive acquisition systems that need to operate in System-of-Systems (SoS), Family-of-Systems (FoS), Joint, and Coalition environments. Unfortunately, the tools and methods necessary to rapidly collect, aggregate, and analyze this information have not evolved as a whole in conjunction with this increased system complexity and, therefore, has made analysis and evaluation increasingly deficient and ineffective. The Test Resource Management Center's (TRMC's) vision is to build a DoD test and evaluation (T&E) knowledge management (KM) and analysis capability that leverages commercial big data analysis and cloud computing technologies to improve evaluation quality and reduce decision-making time. An evaluation revolution, starting with the Joint Strike Fighter (JSF) program, is underway to ensure the T&E community can support the demands of next-generation weapon systems.The true product of T&E is knowledge ascertained through the collection of information about a system or item under test. However, the T&E community's ability to provide this knowledge is hampered by more complex systems, more complex environments, and the need to be more agile in support of strategic initiatives, such as agile acquisition and the 3rd Offset Strategy. This increased complexity and need for speed cause delayed analysis and problems that go undetected during T&E. The primary reason for these shortfalls is antiquated tools and processes that make data hard to locate, aggregate, and convert into knowledge. In short, DoD has not evolved its evaluation infrastructure as its weapon systems have evolved.Conversely, commercial entities, such as medical observation and diagnosis, electric power distribution, retail, and industrial manufacturing, have embraced agility in their methodologies while modernizing analytics capabilities to keep up with the massive influx of data. Raw physical sensors could provide data, higher-quality image or video cameras, radio frequency identification (RFID) devices, faster data collectors, more detailed point-of-sale information or digitized records, and ultimately is providing more data to analysts in size and complexity than ever before. As more data has become available, an interrelated phenomenon is the desire of analysts to ask more detailed questions about their consumers and their business infrastructure. To drive the process of implementing big data analytics, businesses have begun establishing analytics centers which either take pre-defined business cases and apply methods to address them or implement existing knowledge within the data architecture to create a higher level of awareness to business groups or the company at-large. To meet these demands, data storage and computation architectures have become more sophisticated, dozens of technologies were developed for large-scale processing (such as Apache Hadoop or GreenPlum), and streaming architectures which allow data to be processed and actioned on in real-time as it is collected have become commonplace. The net result of these commercial best practices is a solid foundation for the DoD to transform how it uses data to achieve faster, better, and smarter decisions throughout the acquisition lifecycle.},
keywords={Big Data;Knowledge management;Tools;US Department of Defense;Cloud computing;Computer architecture;Data analysis;Big Data;Data Analytics;Knowledge Management;Data Management;Virtualization;Cloud Computing;Predictive Maintainance;Department of Defense;Test and Evaluation},
doi={10.1109/BigData.2018.8622388},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9403767,
author={Peng, Fang and Wang, Honggang and Zhuang, Li and Wang, Minnan and Yang, Chengyue},
booktitle={2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)},
title={Methods of enterprise electronic file content information mining under big data environment},
year={2020},
volume={},
number={},
pages={5-8},
abstract={As the product of the digital age, big data technology and computer information technology can greatly improve the efficiency and quality of file management and promote the development of enterprises. Based on this, this paper first analyzes the current status of enterprise archives management; Secondly, this paper discusses the countermeasures of information mining of electronic documents of innovative enterprises in the digital age. Text information mining is beneficial to improve the efficiency of text information search and utilization, aiming at the existing problems of traditional methods, the text information mining method is proposed.},
keywords={Scalability;Big Data;Information age;Search problems;Data mining;Electronic countermeasures;Software engineering;Big data environment;Enterprise electronic documents;Data mining},
doi={10.1109/ICBASE51474.2020.00008},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8432043,
author={Pan, Xing and Zhang, Manli and Chen, Xi},
booktitle={2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
title={A Method of Quality Improvement Based on Big Quality Warranty Data Analysis},
year={2018},
volume={},
number={},
pages={643-644},
abstract={Quality warranty data includes big data of product use and customer services, which is foundation of product quality and reliability improvement. This paper presents a method of quality warranty data analysis, which is based on the big data analysis technology. By means of the method of association rules mining, it distinguishes the association rules of failure modes while feeding back the information to the process of product design, production, and usage. To achieve product fault location and fault disposal, the key factors such as fault type and fault cause are analyzed. Meanwhile, this paper adopted the principles of PDCA circulation to propose a procedure of product quality improvement. The quality improvement procedure based on quality warranty data analysis provides a comprehensive and systematic quality improvement for different stages and different types of products. Finally, a case study of household appliances in China is given to illustrate the method.},
keywords={Warranties;Data mining;Product design;Quality assessment;Databases;Big Data;Reliability engineering;quality warranty data;big data analysis;association rules;quality improvement;PDCA},
doi={10.1109/QRS-C.2018.00115},
ISSN={},
month={July},}
@ARTICLE{8935096,
author={Li, Mingda and Wang, Hongzhi and Li, Jianzhong},
journal={Big Data Mining and Analytics},
title={Mining conditional functional dependency rules on big data},
year={2020},
volume={3},
number={1},
pages={68-84},
abstract={Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.},
keywords={Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality},
doi={10.26599/BDMA.2019.9020019},
ISSN={2096-0654},
month={March},}
@INPROCEEDINGS{8386521,
author={Hongxun, Tian and Honggang, Wang and Kun, Zhou and Mingtai, Shi and Haosong, Li and Zhongping, Xu and Taifeng, Kang and Jin, Li and Yaqi, Cai},
booktitle={2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory},
year={2018},
volume={},
number={},
pages={248-252},
abstract={Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.},
keywords={Data integrity;Power quality;Monitoring;Power measurement;Redundancy;Big Data;Business;power qualiy;data quality;big data;data provenance;data assessment},
doi={10.1109/ICCCBDA.2018.8386521},
ISSN={},
month={April},}
@INPROCEEDINGS{9378148,
author={O’Shea, Enda and Khan, Rafflesia and Breathnach, Ciara and Margaria, Tiziana},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD},
year={2020},
volume={},
number={},
pages={1914-1923},
abstract={The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).},
keywords={Java;Quality assurance;Semantics;Big Data;Tools;Syntactics;Cleaning;Data Collection;Data Analytics;Model-Driven Development;Historical Data;Data Parsing;Data Cleaning;Ethics;Data Assurance;Data Quality},
doi={10.1109/BigData50022.2020.9378148},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8078781,
author={Ning, Xiuli and Xu, Yingcheng and Gao, Xiaohong and Li, Ying},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
title={Missing data of quality inspection imputation algorithm base on stacked denoising auto-encoder},
year={2017},
volume={},
number={},
pages={84-88},
abstract={Analyzing and processing big data of quality inspection is the key factor in ensuring product quality and People's property security. Big data of quality inspection collected by social network and E-commerce is missing in most cases. And the incompleteness of data brings huge challenge for analyzing and processing. Therefore, the algorithm of data filling based on stacked denoising auto-encoder is proposed in this text. As the experiment shows that the algorithm proposed in this text is effective in dealing with big data of quality inspection.},
keywords={Filling;Algorithm design and analysis;Noise reduction;Training;Clustering algorithms;Inspection;Big Data;Big data of quality inspection;Stacked denoising auto-encoder;Filling algorithm},
doi={10.1109/ICBDA.2017.8078781},
ISSN={},
month={March},}
@INPROCEEDINGS{7364093,
author={Beecks, Christian and Uysal, Merih Seran and Seidl, Thomas},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Gradient-based signatures for big multimedia data},
year={2015},
volume={},
number={},
pages={2834-2835},
abstract={With the continuous increase of heterogeneous multimedia data, the question of how to access big multimedia data efficiently has become of crucial importance. In order to provide fast access to complex multimedia data, we propose to approximate content-based features of multimedia objects by means of generative models. The proposed gradient-based signatures epitomize a high quality content-based approximation of multimedia objects and facilitate efficient indexing and query processing at large scale.},
keywords={Multimedia communication;Multimedia databases;Adaptation models;Data models;Indexing;Big data;Query processing;Big Multimedia Data;Content-based Information Access;Gradient-based Signatures;Feature Signatures},
doi={10.1109/BigData.2015.7364093},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8862267,
author={Juneja, Ashish and Das, Nripendra Narayan},
booktitle={2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)},
title={Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application},
year={2019},
volume={},
number={},
pages={559-563},
abstract={Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.},
keywords={Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing},
doi={10.1109/COMITCon.2019.8862267},
ISSN={},
month={Feb},}
@INPROCEEDINGS{8725668,
author={Jin, Li and Haosong, Li and Zhongping, Xu and Ting, Wang and Shuai, Wang and Yutong, Wei and Dongliang, Hu and Chunting, Kang and Jia, Wu and Dan, Su},
booktitle={2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Research on Wide-area Distributed Power Quality Data Fusion Technology of Power Grid},
year={2019},
volume={},
number={},
pages={185-188},
abstract={With the advancement of the "big operation" system construction, the online monitoring system for power quality has been integrated, and various power quality data have been incorporated into relevant organizations for unified management. Power quality management has a larger range of data, more types, and higher frequency. It needs to realize the unified storage management and efficient access of massive heterogeneous power quality data for the characteristics of data applications and the collection and aggregation of these effective data. This paper proposes a new type of grid wide-area distributed power quality data integration architecture, which is designed for multi-source, heterogeneous, distributed data integration technology and wide-area distributed data storage technology to solve the big data source problem and realize the sharing of power quality data information of the whole network.},
keywords={Power quality;Data integration;Distributed databases;Monitoring;Power grids;Computer architecture;Data models;Power Quality;Wide Area Distribution;Data Integration},
doi={10.1109/ICCCBDA.2019.8725668},
ISSN={},
month={April},}
@INPROCEEDINGS{7364064,
author={Becker, David and King, Trish Dunn and McMullen, Bill},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Big data, big data quality problem},
year={2015},
volume={},
number={},
pages={2644-2653},
abstract={A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the "truth about Big Data" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.},
keywords={Big data;Biomedical imaging;Personnel;Complexity theory;Bioinformatics;Process control;Instruments;Big Data;Data Quality;Returns to Scale},
doi={10.1109/BigData.2015.7364064},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9260067,
author={Wong, Ka Yee. and Wong, Raymond K.},
booktitle={2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)},
title={Big Data Quality Prediction on Banking Applications: Extended Abstract},
year={2020},
volume={},
number={},
pages={791-792},
abstract={Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.},
keywords={Big Data;Data models;Banking;Neural networks;Industries;Data integrity;Computational modeling;Big Data;Data Noise;Data Quality;Prediction and Machine Learning},
doi={10.1109/DSAA49011.2020.00119},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7364065,
author={Rao, Dhana and Gudivada, Venkat N and Raghavan, Vijay V.},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Data quality issues in big data},
year={2015},
volume={},
number={},
pages={2654-2660},
abstract={Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.},
keywords={Databases;Big data;Measurement;Context;Cleaning;Biology;Computers;Data quality;big data;biological data;information quality},
doi={10.1109/BigData.2015.7364065},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7816918,
author={Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik},
booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)},
title={Big Data Quality: A Quality Dimensions Evaluation},
year={2016},
volume={},
number={},
pages={759-765},
abstract={Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.},
keywords={Big data;Measurement;Feature extraction;Quality assessment;Social network services;Companies;Context;Big Data;data quality dimensions;data quality evaluation;Big data sampling},
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122},
ISSN={},
month={July},}
@INPROCEEDINGS{8104203,
author={Gonzalez-Alonso, P. and Vilar, R. and Lupiañez-Villanueva, F.},
booktitle={2017 IEEE 30th International Symposium on Computer-Based Medical Systems (CBMS)},
title={Meeting Technology and Methodology into Health Big Data Analytics Scenarios},
year={2017},
volume={},
number={},
pages={284-285},
abstract={Health organizations are collecting more data from a wider array of sources at greater speed every day. The analysis of this vast amount of data creates new opportunities to deliver modern personalized health and social care services. Big Data Analytics and underlying technologies have the potential to process and analyze these data to extract meaningful insights for improving quality of care, efficiency and sustainability of health and social care systems. Health organizations face therefore a new scenario where analytical tools must accommodate both traditional business intelligence and Big Data approaches, resulting in important technological and methodological challenges to be tackled. In this paper, we present a methodological approach to address the introduction of Big Data Analytics technologies into an integrated care provider.},
keywords={Big Data;Organizations;Medical services;Tools;Medical diagnostic imaging;Design methodology;Healthcare;health analytics;big data analytics;big data analytical frameworks;translational medicine},
doi={10.1109/CBMS.2017.71},
ISSN={2372-9198},
month={June},}
@INPROCEEDINGS{8465410,
author={Ogudo, Kingsley A. and Nestor, Dahj Muwawa Jean},
booktitle={2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)},
title={Modeling of an Efficient Low Cost, Tree Based Data Service Quality Management for Mobile Operators Using in-Memory Big Data Processing and Business Intelligence use Cases},
year={2018},
volume={},
number={},
pages={1-8},
abstract={Network Operators are shifting their business interest towards Data services in a geometric progression manner, as Data services is becoming the major source of Telco revenue. The wide use of Data platforms; such as WhatsApp, Skype, Hangout and other Over the Top (OTT) voice applications over the traditional voice services is a clear indication that Network Operators need to adjust their business model and needs. And couple with the adoption of Smartphones usage which grows continuously year by year, this means more subscribers to manage, large amount of transactions generated, more network resources to be added and evidently more human technical expertise required to ensure good service quality. That has led to high investment on Robust Service Quality Management (SQM) and Customer Experience Management (CEM) to stay competitive in the market. The high investment is justified by the integration of Big Data Solutions, Machine Learning capabilities and good visualization of insight data. However, the Return on Investment (ROI) of the expensive systems are not as conspicuous as the provided functionalities and business rules. Therefore, in this paper an efficient model for low cost SQM system is presented, exploring the advantages of In-Memory Big Data processing and low cost business Intelligence tools to showcase how a good Service Quality Management can be implemented with no big investment.},
keywords={Quality of service;Big Data;Business intelligence;Structured Query Language;Tools;Sparks;Service Quality Management;In-Memory Big Data;Business Intelligence;Service Quality Index;Over The Top Application (OTT);Data Traffic and ROI},
doi={10.1109/ICABCD.2018.8465410},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8397554,
author={Zhang, Pengcheng and Xiong, Fang and Gao, Jerry and Wang, Jimin},
booktitle={2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={Data quality in big data processing: Issues, solutions and open problems},
year={2017},
volume={},
number={},
pages={1-7},
abstract={With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.},
keywords={Big Data;Data integrity;Data analysis;Social network services;Data preprocessing;Internet;Big Data;Big data processing;Data Quality;Recommendation system;Prediction system},
doi={10.1109/UIC-ATC.2017.8397554},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8258406,
author={Wu, Donghui},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={A big data analytics framework for forecasting rare customer complaints: A use case of predicting MA members' complaints to CMS},
year={2017},
volume={},
number={},
pages={3965-3967},
abstract={Centers for Medicare & Medicaid Services (CMS) publishes Medicare Part C Star Ratings each year to measure the quality of care of Medicare Advantage (MA) contracts. One of the key measures is Complaints about the Health Plan, which is captured in Complaints Tracking Module (CTM). Complaints resulted in CTM are rare events: for MA contracts with 2-5 star ratings, number of complaints for every 1,000 members range from .10 to 1.84 over last 5 years. Reducing number of complaints is extremely important to MA plans as they impact CMS reimbursements to MA plans. Forecasting and reducing complaints is an extremely technically challenging task, and involves ethics considerations in patients' rights and privacy. In this research, we constructed a big data analytics framework for forecasting rare customer complaints. First, we built a big data ingestion pipelines on a Hadoop platform: a) Ingest MA plan's customer complaints data from CTM from past 3 years. b) Ingest health plan's call center data for MA members from past 3 years, including both structured data and unstructured text script for the calls. c) Ingest MA members' medical claims, including members' demographics and enrollment history. d) Ingest MA members' pharmacy claims. e) Integrate and unified data from above sources, and enrich the data with additional engineered features into a big wide table, one row per member for analysis and modeling. Second, we designed a unique decision tree based Large Ensemble with Over-Sampling (LEOS) algorithm, which mimics random forest but with extreme oversampling of target class to increase bias, and leverages the parallel computing of Hadoop clusters by generating thousands of fixed size training data sets, and for each such dataset training a decision trees with similar fixed tree structure, and ensemble them. Third, we validated our framework and LEOS learning algorithm with real data, and also discussed ethics issues we encountered in handling data and applying findings from research.},
keywords={Decision trees;Training;Training data;Big Data;Prediction algorithms;Ethics;Contracts;CMS Star Ratings;customer complaints;big data analytics;ensemble methods;ethics;call center data;medical claims},
doi={10.1109/BigData.2017.8258406},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9005530,
author={Muniswamaiah, Manoj and Agerwala, Tilak and Tappert, Charles C.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Federated Query processing for Big Data in Data Science},
year={2019},
volume={},
number={},
pages={6145-6147},
abstract={As the number of databases continues to grow data scientists need to use data from different sources to run machine learning algorithms for analysis. Data science results depend upon the quality of data been extracted. The objective of this research paper is to implement a federated query processing framework which extracts data from different data sources and stores the result datasets in a common in-memory data format. This helps data scientists to perform their analysis and execute machine learning algorithms using different data engines without having to convert the data into their native data format and improve the performance.},
keywords={Databases;Machine learning algorithms;Engines;Machine learning;Task analysis;Protocols;Big Data;Machine learning;big data;federated query;database;query optimizer;in-memory data;data science},
doi={10.1109/BigData47090.2019.9005530},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7584971,
author={Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal and Nujum, Alramzana},
booktitle={2016 IEEE International Congress on Big Data (BigData Congress)},
title={An Hybrid Approach to Quality Evaluation across Big Data Value Chain},
year={2016},
volume={},
number={},
pages={418-425},
abstract={While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.},
keywords={Big data;Metadata;Measurement;Quality assessment;Quality of service;Unified modeling language;Big Data;Quality assessment;Metadata;Quality metrics;quality Metadata;Quality of process;Hybrid quality assessment},
doi={10.1109/BigDataCongress.2016.65},
ISSN={},
month={June},}
@INPROCEEDINGS{9006358,
author={Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Provenance–aware workflow for data quality management and improvement for large continuous scientific data streams},
year={2019},
volume={},
number={},
pages={3260-3266},
abstract={Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.},
keywords={Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science},
doi={10.1109/BigData47090.2019.9006358},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7364060,
author={Nobles, Alicia L. and Vilankar, Ketki and Wu, Hao and Barnes, Laura E.},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Evaluation of data quality of multisite electronic health record data for secondary analysis},
year={2015},
volume={},
number={},
pages={2612-2620},
abstract={Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.},
keywords={Documentation;Data warehouses;Medical services;Big data;Databases;Cleaning;Medical diagnostic imaging;electronic health records;data quality;big data;multiple data vendors;metrics},
doi={10.1109/BigData.2015.7364060},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9422032,
author={Mao, Yifan and Huang, Shasha and Cui, Shuo and Wang, HaiFeng and Zhang, Junyan and Ding, Wenhao},
booktitle={2020 2nd International Conference on Information Technology and Computer Application (ITCA)},
title={Multi dimensional data distribution monitoring based on OLAP},
year={2020},
volume={},
number={},
pages={298-302},
abstract={With the rapid development of the Internet, society is gradually entering the information age, and various data in enterprises have become the most important strategic core resources of all enterprises. The operation and decision-making of enterprises all require a large amount of data analysis. Nowadays, many companies do not pay enough attention to the monitoring of data asset distribution. In addition, various internal systems such as financial management and ERP systems are relatively independent. Each system has its own data organization standard, which makes it difficult to conduct a unified management of data. This also directly leads to the one-sided and subjective problem of enterprise managers' distribution of data assets. With the construction of the data center of each enterprise, the data of each system is aggregated to the center through data integration technology. Therefore, all enterprises need to build a multi-dimensional data distribution monitoring model around data links to comprehensively monitor the status of various data distributions across the company's entire network, and improve data service capabilities and sharing capabilities as well as the company's operational capabilities. This article uses OLAP technology to construct a multi-dimensional data distribution monitoring model for the data link in the process of power enterprise data integration. This article first selects the dimensions and metrics that need to be monitored in the multidimensional data, and then constructs the conceptual model, logical model and physical model of the multidimensional data using on line analytical processing technology. Finally, an example analysis of OLAP system architecture based on B/S structure is realized. The overall data distribution of the enterprise can be grasped by analyzing the various dimensions of the data link, such as System type, location distribution, and time.},
keywords={Measurement;Analytical models;Standards organizations;Data integration;Systems architecture;Financial management;Information age;data link;OLAP;multidimensional data model},
doi={10.1109/ITCA52113.2020.00070},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8669638,
author={Li, Lianzhi},
booktitle={2019 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={Evaluation Model of Education Service Quality Satisfaction in Colleges and Universities Dependent on Classification Attribute Big Data Feature Selection Algorithm},
year={2019},
volume={},
number={},
pages={645-649},
abstract={In view of the insufficiency in the education service quality in colleges and universities, a kind of evaluation model of the education service quality satisfaction in the colleges and universities that is dependent on the classification attribute big data feature selection algorithm is put forward in this paper based on the existing work. On the basis of detailed description of the model components, further study on the evaluation method of the proposed model for the education service quality satisfaction in the colleges and universities is carried out. Under the guidance of the evaluation model of the education service quality satisfaction in the colleges and universities, the method for the construction of the evaluation model of the education service quality satisfaction in the colleges and universities is studied with the orientation to the education service resources in the colleges and universities under the open big data environment. In addition, experimental verification is carried out on the basis of the evaluation data in the 360 Encyclopedia on the education service quality satisfaction in the colleges and universities. The experimental results show that the model and method put forward in this paper can effectively evaluate the quality of the education service in the colleges and universities.},
keywords={Education;Correlation;Data models;Encyclopedias;Big Data;Mutual information;Compounds;Classification Attribute Big Data Feature Selection Algorithm;Education Service Quality in Colleges and Universities;Education Service in Colleges and Universities;Satisfaction Evaluation},
doi={10.1109/ICITBS.2019.00160},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8258267,
author={Catarci, Tiziana and Scannapieco, Monica and Console, Marco and Demetrescu, Camil},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={My (fair) big data},
year={2017},
volume={},
number={},
pages={2974-2979},
abstract={Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.},
keywords={Big Data;Pipelines;Ontologies;Metadata;Google;Quality-driven policies;Big Data pipeline;ontology-based quality checking;Big Data confidentiality},
doi={10.1109/BigData.2017.8258267},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8457745,
author={Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle={2018 IEEE International Congress on Big Data (BigData Congress)},
title={Big Data Quality: A Survey},
year={2018},
volume={},
number={},
pages={166-173},
abstract={With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.},
keywords={Big Data;Data integrity;Quality management;Data visualization;Databases;Sensors;Social network services;Big Data, Data Quality, Quality Management framework, Quality of Big Data},
doi={10.1109/BigDataCongress.2018.00029},
ISSN={},
month={July},}
@INPROCEEDINGS{8787092,
author={Guan, Zhibin and Ji, Tongkai and Qian, Xu and Ma, Yan and Hong, Xuehai},
booktitle={2017 5th Intl Conf on Applied Computing and Information Technology/4th Intl Conf on Computational Science/Intelligence and Applied Informatics/2nd Intl Conf on Big Data, Cloud Computing, Data Science (ACIT-CSII-BCD)},
title={A Survey on Big Data Pre-processing},
year={2017},
volume={},
number={},
pages={241-247},
abstract={In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.},
keywords={Big Data;Data integration;Data analysis;Dimensionality reduction;Feature extraction;Data integrity;big data processing;data pre-processing;data cleansing;dimension reduction;data quality},
doi={10.1109/ACIT-CSII-BCD.2017.49},
ISSN={},
month={July},}
@INPROCEEDINGS{8622590,
author={Rueda, Diego F. and Vergara, Dahyr and Reniz, David},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Big Data Streaming Analytics for QoE Monitoring in Mobile Networks: A Practical Approach},
year={2018},
volume={},
number={},
pages={1992-1997},
abstract={Traditionally, Mobile Network Operators (MNOs) use a set of Key Performance Indicators (KPIs) to measure the quality offered to their customers. However, these KPIs do not reflect the quality perceived by the customers because they are high-level and network-based metrics. Instead, Quality of Experience (QoE) monitoring of the most common mobile applications can help MNOs to determine when and where customer experience is degraded. In this paper, a customized tool based on Big Data Streaming is proposed to solve the needs of customer experience monitoring in a real-life MNO and to overcome the challenges of processing a large amount of data collected in 3G and 4G mobile networks. Moreover, real-life case studies of value creation through Big Data Analytics for telecommunication industry are also defined. Results show that the streaming data processing enables new opportunities for the MNO to take actions focused on customer experience improvement in near real-time.},
keywords={Quality of experience;Big Data;Monitoring;Streaming media;Real-time systems;Tools;Big Data Analytics;customer experience management;mobile networks;quality of experience;streaming data processing},
doi={10.1109/BigData.2018.8622590},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8725744,
author={Xiang, Zheng and Xiaofang, Liu and Weigang, Gao},
booktitle={2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Analysis of the Application of Military Big Data in Equipment Quality Information Management},
year={2019},
volume={},
number={},
pages={68-71},
abstract={This At present, big data has risen to the national strategy. Big data is fully integrated into the military field, becoming the driving force of military scientific research, the core element of construction management, and an important resource for war success. This paper mainly expounds the basic connotation of big data technology and military big data, and analyzes the application of military big data in equipment quality information management, and proposes information collection, storage, analysis, processing, exchange and feedback on equipment quality information management. The countermeasures provide methods and basis for military big data in equipment information management.},
keywords={Big Data;Military equipment;Information management;Distributed databases;Data analysis;Maintenance engineering;military big data;equipment quality information;management},
doi={10.1109/ICCCBDA.2019.8725744},
ISSN={},
month={April},}
@INPROCEEDINGS{8686099,
author={Abdallah, Mohammad},
booktitle={2019 International Conference on Big Data and Computational Intelligence (ICBDCI)},
title={Big Data Quality Challenges},
year={2019},
volume={},
number={},
pages={1-3},
abstract={Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.},
keywords={Big Data;Quality Measurement;Quality Model;Quality Assurance},
doi={10.1109/ICBDCI.2019.8686099},
ISSN={},
month={Feb},}
@INPROCEEDINGS{9377734,
author={Arruda, Darlan and Laigner, Rodrigo},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Requirements Engineering Practices and Challenges in the Context of Big Data Software Development Projects: Early Insights from a Case Study},
year={2020},
volume={},
number={},
pages={2012-2019},
abstract={This paper reports on the results of an exploratory case study on a large-scale Big Data systems development project in the Oil&Gas domain within a non-profit organisation. The aim of this study was to investigate the RE practices and challenges in such projects, currently bereft in the scientific literature. This investigation was focused on: (a) RE practices; (b) sources and distribution of requirements; (c) the role of Big Data characteristics and technologies in RE and systems design; and (d) RE challenges in engineering Big Data Systems. The main results show that (a) there is a lack of specific project tailored RE practices, tools, and frameworks for elicitation, specification and modelling, analysis, and prioritisation of requirements; (b) 40% of the system's requirements are considered Big Data-related from which 75% are identified from internal sources; (c) Big Data characteristics and technologies play an important role in defining quality requirements and system's architecture; (d) five challenges in eliciting, documenting, and analysing Big Data related requirements were identified and discussed. The findings suggest academics and practitioners opportunities to engage in further research in this area.},
keywords={Conferences;Big Data;Tools;Software;Data models;Requirements engineering;System analysis and design;Big Data Systems;Requirements Engineering;Case Study;Big Data Challenges;Big Data Requirements},
doi={10.1109/BigData50022.2020.9377734},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7363987,
author={Priebe, Torsten and Markus, Stefan},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Business information modeling: A methodology for data-intensive projects, data science and big data governance},
year={2015},
volume={},
number={},
pages={2056-2065},
abstract={This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.},
keywords={Business;Data models;Terminology;Big data;Data warehouses;Wheels;Standards organizations;Data Modeling;Project Methodology;Data Governance;Metadata;Information Catalog},
doi={10.1109/BigData.2015.7363987},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9323615,
author={Han, Weiguo and Jochum, Matthew},
booktitle={IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium},
title={A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System},
year={2020},
volume={},
number={},
pages={3101-3103},
abstract={In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.},
keywords={Data integrity;Machine learning;Earth;Satellites;Big Data;Process control;Monitoring;Big Data;Machine Learning;Earth Observation Data;Data management;Data Quality;Random Forest},
doi={10.1109/IGARSS39084.2020.9323615},
ISSN={2153-7003},
month={Sep.},}
@INPROCEEDINGS{8416208,
author={Blanquer, Ignacio and Meira, Wagner},
booktitle={2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)},
title={EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform},
year={2018},
volume={},
number={},
pages={47-48},
abstract={This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.},
keywords={Big Data;Data analysis;Data privacy;Quality of service;Data models;Security;Biological system modeling;Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics},
doi={10.1109/DSN-W.2018.00023},
ISSN={2325-6664},
month={June},}
@INPROCEEDINGS{7207219,
author={Taleb, Ikbal and Dssouli, Rachida and Serhani, Mohamed Adel},
booktitle={2015 IEEE International Congress on Big Data},
title={Big Data Pre-processing: A Quality Framework},
year={2015},
volume={},
number={},
pages={191-198},
abstract={With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.},
keywords={Big data;Data integration;Accuracy;Distributed databases;Data analysis;Business;Big Data;Data Quality;pre-processing},
doi={10.1109/BigDataCongress.2015.35},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{8465129,
author={Juddoo, Suraj and George, Carlisle},
booktitle={2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)},
title={Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.},
keywords={Data integrity;Big Data;Semantics;Industries;Bibliographies;Libraries;Production;Big Data;data quality;health data;data quality dimensions;latent semantic analysis},
doi={10.1109/ICABCD.2018.8465129},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8621924,
author={Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example},
year={2018},
volume={},
number={},
pages={5328-5329},
abstract={Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.},
keywords={Springs;Databases;Service-oriented architecture;Data integrity;Tools;Servers;Big Data;auto configuration;CRUD;java framework;service-oriented architecture;REST;spring boot},
doi={10.1109/BigData.2018.8621924},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9073586,
author={Khaleel, Majida Yaseen and Hamad, Murtadha M.},
booktitle={2019 12th International Conference on Developments in eSystems Engineering (DeSE)},
title={Data Quality Management for Big Data Applications},
year={2019},
volume={},
number={},
pages={357-362},
abstract={Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.},
keywords={Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.},
doi={10.1109/DeSE.2019.00072},
ISSN={2161-1351},
month={Oct},}
@INPROCEEDINGS{8258338,
author={Assefi, Mehdi and Behravesh, Ehsun and Liu, Guangchi and Tafti, Ahmad P.},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Big data machine learning using apache spark MLlib},
year={2017},
volume={},
number={},
pages={3492-3498},
abstract={Artificial intelligence, and particularly machine learning, has been used in many ways by the research community to turn a variety of diverse and even heterogeneous data sources into high quality facts and knowledge, providing premier capabilities to accurate pattern discovery. However, applying machine learning strategies on big and complex datasets is computationally expensive, and it consumes a very large amount of logical and physical resources, such as data file space, CPU, and memory. A sophisticated platform for efficient big data analytics is becoming more important these days as the data amount generated in a daily basis exceeds over quintillion bytes. Apache Spark MLlib is one of the most prominent platforms for big data analysis which offers a set of excellent functionalities for different machine learning tasks ranging from regression, classification, and dimension reduction to clustering and rule extraction. In this contribution, we explore, from the computational perspective, the expanding body of the Apache Spark MLlib 2.0 as an open-source, distributed, scalable, and platform independent machine learning library. Specifically, we perform several real world machine learning experiments to examine the qualitative and quantitative attributes of the platform. Furthermore, we highlight current trends in big data machine learning research and provide insights for future work.},
keywords={Sparks;Big Data;Libraries;Data models;Computer architecture;Machine learning algorithms;Apache Spark MLlib;Big Data Machine Learning;Big Data Analytics;Machine Learning},
doi={10.1109/BigData.2017.8258338},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8942297,
author={Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif, Abdelaziz},
booktitle={2019 Third International Conference on Intelligent Computing in Data Sciences (ICDS)},
title={Towards a multi-agents model for errors detection and correction in big data flows},
year={2019},
volume={},
number={},
pages={1-5},
abstract={The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.},
keywords={Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction of data errors},
doi={10.1109/ICDS47004.2019.8942297},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7502278,
author={Shanmugam, Srinivasan and Seshadri, Gokul},
booktitle={2016 IEEE 2nd International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing (HPSC), and IEEE International Conference on Intelligent Data and Security (IDS)},
title={Aspects of Data Cataloguing for Enterprise Data Platforms},
year={2016},
volume={},
number={},
pages={134-139},
abstract={As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.},
keywords={Context;Business;Metadata;Big data;Electronic mail;Reliability;Indexes;Enterprise data;Data catalogue;Metadata of data;Data context;Data quality;Data governance;Data as a service},
doi={10.1109/BigDataSecurity-HPSC-IDS.2016.52},
ISSN={},
month={April},}
@ARTICLE{7299603,
author={Immonen, Anne and Pääkkönen, Pekka and Ovaska, Eila},
journal={IEEE Access},
title={Evaluating the Quality of Social Media Data in Big Data Architecture},
year={2015},
volume={3},
number={},
pages={2028-2043},
abstract={The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.},
keywords={Big data;Social network services;Computer architecture;Meta data;Online services;architecture;big data;metadata;quality attribute;quality of data;Architecture;big data;metadata;quality attribute;quality of data},
doi={10.1109/ACCESS.2015.2490723},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8372743,
author={Chang, Yue Shan and Lin, Kuan-Ming and Tsai, Yi-Ting and Zeng, Yu-Ren and Hung, Cheng-Xiang},
booktitle={2018 27th Wireless and Optical Communication Conference (WOCC)},
title={Big data platform for air quality analysis and prediction},
year={2018},
volume={},
number={},
pages={1-3},
abstract={With the advance of industry, air quality (AQ) is increasingly becoming worse. There are increasingly AQ monitors device have been deployed around country for monitoring air-quality all year long. To estimate and predict AQ, such as PM (particulate matter) 2.5, become an important issue for government to improve people's quality of life. As we can know, there are many factors can affect the AQ, such as traffic, factory exhaust emissions, weather, incineration of garbage, and so on. In most well-developed countries, these pollution sources are monitored for future environmental policy making. In this paper, we will propose a semantic ETL (Extract-Transform-Load) framework on cloud platform for AQ prediction. In the platform, we exploit ontology to concretize the relationship of PM 2.5 from various data sources and to merge those data with the same concept but different naming into the unified database. We implement the ETL framework on the cloud platform, which includes computing nodes and storage nodes. The computing nodes are used to execute data mining algorithms for predicting, and storage modes are used to store retrieved, preprocessed, and analyzed data. We utilize restful web service as the front end API to retrieve analyzed data, and finally we exploit browser to show the visualized result to demonstrate the estimation and prediction. It shows that the big data access framework on the cloud platform can work well for air quality analysis.},
keywords={Semantics;Big Data;Air quality;Data mining;Urban areas;Monitoring;Government;Air Quality;Big Data;Prediction;Cloud Environment},
doi={10.1109/WOCC.2018.8372743},
ISSN={2379-1276},
month={April},}
@INPROCEEDINGS{9297009,
author={Juddoo, Suraj and George, Carlisle},
booktitle={2020 3rd International Conference on Emerging Trends in Electrical, Electronic and Communications Engineering (ELECOM)},
title={A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry},
year={2020},
volume={},
number={},
pages={58-66},
abstract={Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.},
keywords={Industries;Machine learning algorithms;Data integrity;Clustering algorithms;Medical services;Big Data;Tools;Big Data;Data Quality;Data Inaccuracy;Data incompleteness;Machine Learning},
doi={10.1109/ELECOM49001.2020.9297009},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7840595,
author={Haryadi, Adiska Fardani and Hulstijn, Joris and Wahyudi, Agung and van der Voort, Haiko and Janssen, Marijn},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Antecedents of big data quality: An empirical examination in financial service organizations},
year={2016},
volume={},
number={},
pages={116-121},
abstract={Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.},
keywords={Big data;Industries;Insurance;Companies;Finance;big data;data quality;big data quality;antecedents;finance},
doi={10.1109/BigData.2016.7840595},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7823519,
author={Li, Tao and He, Yihai and Zhu, Chunling},
booktitle={2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)},
title={Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry},
year={2016},
volume={},
number={},
pages={181-186},
abstract={The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.},
keywords={Indexes;Mathematical model;Data models;Manufacturing industries;Computational modeling;Big data;Customer satisfaction;macro-quality evaluation;big data;macro-quality index;customer satisfaction index;PLS-SEM},
doi={10.1109/ICIICII.2016.0052},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8085555,
author={Huang, Xiaotao and Qin, Niannian and Zhang, Xiaofang and Wang, Fen},
booktitle={2017 12th International Conference on Computer Science and Education (ICCSE)},
title={Experimental teaching design and practice on big data course},
year={2017},
volume={},
number={},
pages={566-569},
abstract={With the rapid development of big data technology and the rapid growth of big data industry market, big data talent demand is also a substantial increase in China. In order to cultivate more talented people satisfying the needs of the community, we have designed the big data course for undergraduates. The big data course stresses not only on many theories but also lots of practice. The project of “big data talent development trend analysis” is designed in the experimental teaching on big data. By doing this project, students can master all the technologies of big data processing lifecycle, including data collection, data preprocessing, data mining and data visualization. We evaluate students who master big data core technology with a multi-evaluation method and design the experiment evaluation system on big data. Through our two years' practice, the results show that all these designs have achieved the good effect and improved the teaching quality.},
keywords={Big Data;Education;Data mining;Clustering algorithms;Data visualization;Classification algorithms;Industries;Experimental teaching;Big data course;Big data processing lifecycle;Big data project;Big data},
doi={10.1109/ICCSE.2017.8085555},
ISSN={2473-9464},
month={Aug},}
@INPROCEEDINGS{7840935,
author={Haug, Frank S.},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Bad big data science},
year={2016},
volume={},
number={},
pages={2863-2871},
abstract={As hardware and software technologies have improved, our definition of a “manageable amount of data” has increased in its scope dramatically. The term “big data” can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term “data science” refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for “bad big data science”, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.},
keywords={Big data;Metadata;Distributed databases;Stakeholders;Data science;Big Data;Data Quality;Data Warehousing;Distributed Database;Metadata},
doi={10.1109/BigData.2016.7840935},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8075482,
author={Shao, Jun and Xu, Daqi and Feng, Chun and Chi, Mingmin},
booktitle={2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)},
title={Big data challenges in China centre for resources satellite data and application},
year={2015},
volume={},
number={},
pages={1-4},
abstract={China Centre for Resources Satellite Data and Application (abbreviate as CRESDA) is a core platform to store, process, analyze, and distribute land observing satellite data in China. It can provide high quality and effective services for the State Council and the relevant departments of government and local authorities. In the era of big data, the data center benefits from big data opportunities as well as suffering from big data challenges. In the paper, the big data challenges of the CRESDA are summarized. In particular, four major challenges are comprised of the 3V dimensions of big data (i.e. Volume, Variety, and Velocity) and one specific challenge (i.e., extensibility) in the data center.},
keywords={Satellites;Remote sensing;Big Data;Monitoring;Land observing satellites;data center;big data;data challenges},
doi={10.1109/WHISPERS.2015.8075482},
ISSN={2158-6276},
month={June},}
@INPROCEEDINGS{8996332,
author={Wang, Jinghan and Zhang, Jinnan and Yuan, XueGuang and Tang, Yu and Hao, Hongyu and Zuo, Yong and Tan, Zebin and Qiao, Min and Cao, Yang Hua and Ai, Lingmei and Wan, Yihang and Chen, Hao},
booktitle={2019 Chinese Automation Congress (CAC)},
title={Air quality data analysis and forecasting platform based on big data},
year={2019},
volume={},
number={},
pages={2042-2046},
abstract={Nowadays, with the continuous development of big data technology, various industries use big data technology to process and mine massive data, and realize the value of data efficiently. In terms of air quality data processing, big data technology can also play a certain advantage. The platform is based on big data technology to design an air quality data analysis and prediction platform including data layer, business layer, interaction layer and visualization platform. Data is cleaned, calibrated, and stored in the data layer to ensure data consistency, integrity, and security. The air quality data is analyzed and predicted at the business layer. The interaction layer includes the functions of algorithm management, data query, and the data visualization platform provides intuitive information display. This design is a significant application for fully exploiting environmental data information. It has powerful data processing functions and scalability, which is a reliable data analysis and prediction platform.},
keywords={Air quality;Big Data;Data mining;Photonics;Optical fiber communication;Telecommunications;Clustering algorithms;Air quality;Big data;Data mining;Data visualization},
doi={10.1109/CAC48633.2019.8996332},
ISSN={2688-0938},
month={Nov},}
@INPROCEEDINGS{7840586,
author={Challa, Jagat Sesh and Goyal, Poonam and Nikhil, S. and Mangla, Aditya and Balasubramaniam, Sundar S. and Goyal, Navneet},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={DD-Rtree: A dynamic distributed data structure for efficient data distribution among cluster nodes for spatial data mining algorithms},
year={2016},
volume={},
number={},
pages={27-36},
abstract={Parallelizing data mining algorithms has become a necessity as we try to mine ever increasing volumes of data. Spatial data mining algorithms like Dbscan, Optics, Slink, etc. have been parallelized to exploit a cluster infrastructure. The efficiency achieved by existing algorithms can be attributed to spatial locality preservation using spatial indexing structures like k-d-tree, quad-tree, grid files, etc. for distributing data among cluster nodes. However, these indexing structures are static in nature, i.e., they need to scan the entire dataset to determine the partitioning coordinates. This results in high data distribution cost when the data size is large. In this paper, we propose a dynamic distributed data structure, DD-Rtree, which preserves spatial locality while distributing data across compute nodes in a shared nothing environment. Moreover, DD-Rtree is dynamic, i.e., it can be constructed incrementally making it useful for handling big data. We compare the quality of data distribution achieved by DD-Rtree with one of the recent distributed indexing structure, SD-Rtree. We also compare the efficiency of queries supported by these indexing structures along with the overall efficiency of DBSCAN algorithm. Our experimental results show that DD-Rtree achieves better data distribution and thereby resulting in improved overall efficiency.},
keywords={Data structures;Clustering algorithms;Data mining;Indexing;Distributed databases;Algorithm design and analysis;Data mining;data distribution;spatial locality;neighborhood queries;k-NN queries;density based clustering},
doi={10.1109/BigData.2016.7840586},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9148148,
author={Bai, Zhongxian and Zhuo, Rongqing},
booktitle={2020 International Conference on Computer Information and Big Data Applications (CIBDA)},
title={Quality Management of Crowd Sensing Data Based on Machine Learning},
year={2020},
volume={},
number={},
pages={185-188},
abstract={Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.},
keywords={Time series analysis;Temperature sensors;Mobile handsets;Temperature distribution;Databases;Data models;Machine Learning;Crowd Sensing;Big Data Analysis;Abnormal Data Detection Management;Clustering Method},
doi={10.1109/CIBDA50819.2020.00049},
ISSN={},
month={April},}
@INPROCEEDINGS{7406335,
author={Debattista, Jeremy and Lange, Christoph and Scerri, Simon and Auer, Sören},
booktitle={2015 IEEE/ACM 2nd International Symposium on Big Data Computing (BDC)},
title={Linked 'Big' Data: Towards a Manifold Increase in Big Data Value and Veracity},
year={2015},
volume={},
number={},
pages={92-98},
abstract={The Web of Data is an increasingly rich source of information, which makes it useful for Big Data analysis. However, there is no guarantee that this Web of Data will provide the consumer with truthful and valuable information. Most research has focused on Big Data's Volume, Velocity, and Variety dimensions. Unfortunately, Veracity and Value, often regarded as the fourth and fifth dimensions, have been largely overlooked. In this paper we discuss the potential of Linked Data methods to tackle all five V's, and particularly propose methods for addressing the last two dimensions. We draw parallels between Linked and Big Data methods, and propose the application of existing methods to improve and maintain quality and address Big Data's veracity challenge.},
keywords={Big data;Semantics;Resource description framework;Data models;Encyclopedias;Vocabulary;linked data;Web of Data;Veracity;Value;Big Data dimension},
doi={10.1109/BDC.2015.34},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8695373,
author={Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita},
booktitle={2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)},
title={A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control},
year={2019},
volume={},
number={},
pages={463-466},
abstract={Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.},
keywords={Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control},
doi={10.1109/MIPR.2019.00093},
ISSN={},
month={March},}
@INPROCEEDINGS{7877056,
author={Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Marín-Tordera, Eva},
booktitle={2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT)},
title={Towards a Comprehensive Data LifeCycle Model for Big Data Environments},
year={2016},
volume={},
number={},
pages={100-106},
abstract={A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource, however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation. 2. Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity, and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.},
keywords={Data models;Adaptation models;Biological system modeling;Big Data;Organizations;Data acquisition;Computational modeling;Big Data;Data LifeCycle;Data Management;Data Organization;Data Complexity;Vs Challenges},
doi={},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8843451,
author={Doku, Ronald and Rawat, Danda B. and Liu, Chunmei},
booktitle={2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI)},
title={Towards Federated Learning Approach to Determine Data Relevance in Big Data},
year={2019},
volume={},
number={},
pages={184-192},
abstract={In the past few years, data has proliferated to astronomical proportions; as a result, big data has become the driving force behind the growth of many machine learning innovations. However, the incessant generation of data in the information age poses a needle in the haystack problem, where it has become challenging to determine useful data from a heap of irrelevant ones. This has resulted in a quality over quantity issue in data science where a lot of data is being generated, but the majority of it is irrelevant. Furthermore, most of the data and the resources needed to effectively train machine learning models are owned by major tech companies, resulting in a centralization problem. As such, federated learning seeks to transform how machine learning models are trained by adopting a distributed machine learning approach. Another promising technology is the blockchain, whose immutable nature ensures data integrity. By combining the blockchain's trust mechanism and federated learning's ability to disrupt data centralization, we propose an approach that determines relevant data and stores the data in a decentralized manner.},
keywords={Data models;Blockchain;Machine learning;Mobile handsets;Data privacy;Cryptography;Big Data;Federated Learning Approach, Data Relevance, Big Data Analytics, Machine Learning},
doi={10.1109/IRI.2019.00039},
ISSN={},
month={July},}
@INPROCEEDINGS{9314391,
author={Faroukhi, Abou Zakaria and El Alaoui, Imane and Gahi, Youssef and Amine, Aouatif},
booktitle={2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)},
title={Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.},
keywords={Big Data;Security;Data integrity;Data models;Organizations;Decision making;Reliability;Big Data Value Chain;Data Management;Data Quality;Data Security;Process Integration;Orchestration},
doi={10.1109/ICECOCS50124.2020.9314391},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7384166,
author={Chenran, Xiong and Youde, Wu},
booktitle={2015 International Conference on Intelligent Transportation, Big Data and Smart City},
title={The Geographic Environment Analysis of Regional Economic Development of Yunnan Province of China Based on the Big Data Technology},
year={2015},
volume={},
number={},
pages={869-872},
abstract={This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.},
keywords={Transportation;Big data;Smart cities;analysis of big data;Yunnan province of China;regional economic development;geographic environment},
doi={10.1109/ICITBS.2015.220},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8257913,
author={Benbernou, Salima and Ouziri, Mourad},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Enhancing data quality by cleaning inconsistent big RDF data},
year={2017},
volume={},
number={},
pages={74-79},
abstract={We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.},
keywords={Resource description framework;Ontologies;Sparks;Cleaning;Inference algorithms;Automobiles;Jacobian matrices;Big data;Data quality;Inconsistency;Logical inference;RDF data cleaning;Apache Spark ecosystems},
doi={10.1109/BigData.2017.8257913},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8453066,
author={Scavuzzo, Marco and Di Nitto, Elisabetta and Ardagna, Danilo},
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)},
title={[Journal First] Experiences and Challenges in Building a Data Intensive System for Data Migration},
year={2018},
volume={},
number={},
pages={93-93},
abstract={Recent analyses[2, 4, 5] report that many sectors of our economy and society are more and more guided by data-driven decision processes (e.g., health care, public administrations, etc.). As such, Data Intensive (DI) applications are becoming more and more important and critical. They must be fault-tolerant, they should scale with the amount of data, and be able to elastically leverage additional resources as and when these last ones are provided [3]. Moreover, they should be able to avoid data drops introduced in case of sudden overloads and should offer some Quality of Service (QoS) guarantees. Ensuring all these properties is, per se, a challenge, but it becomes even more difficult for DI applications, given the large amount of data to be managed and the significant level of parallelism required for its components. Even if today some technological frameworks are available for the development of such applications (for instance, think of Spark, Storm, Flink), we still lack solid software engineering approaches to support their development and, in particular, to ensure that they offer the required properties in terms of availability, throughput, data loss, etc. In fact, at the time of writing, identifying the right solution can require several rounds of experiments and the adoption of many different technologies. This implies the need for highly skilled persons and the execution of experiments with large data sets and a large number of resources, and, consequently, a significant amount of time and budget. To experiment with currently available approaches, we performed an action research experiment focusing on developing- testing-reengineering a specific DI application, Hegira4Cloud, that migrates data between widely used NoSQL databases, including so-called Database as a Service (DaaS), as well as on-premise databases. This is a representative DI system because it has to handle large volumes of data with different structures and has to guarantee that some important characteristics, in terms of data types and transactional properties, are preserved. Also, it poses stringent requirements in terms of correctness, high performance, fault tolerance, and fast and effective recovery. In our action research, we discovered that the literature offered some high level design guidelines for DI applications, as well as some tools to support modelling and QoS analysis/simulation of complex architectures, however the available tools were not yet. suitable to support DI systems. Moreover, we realized that the available big data frameworks we could have used were not flexible enough to cope with all possible application-specific aspects of our system. Hence, to achieve the desired level of performance, fault tolerance and recovery, we had to adopt a time-consuming, experiment-based approach [1, 6], which, in our case, consisted of three iterations: (1) the design and implementation of a Mediation Data Model capable of managing data extracted from different databases, together with a first monholitical prototype of Hegira4Cloud; (2) the improvement of performance of our prototype when managing and transferring huge amounts of data; (3) the introduction of fault-tolerant data extraction and management mechanisms, which are independent from the targeted databases. Among the others, an important issue that has forced us to reiterate in the development of Hegira4Cloud concerned the DaaS we interfaced with. In particular these DaaS, which are well-known services with a large number of users: (1) were missing detailed information regarding the behaviour of their APIs; (2) did not offer a predictable service; (3) were suffering of random downtimes not correlated with the datasets we were experimenting with. In this journal first presentation, we describe our experience and the issues we encountered that led to some important decisions during the software design and engineering process. Also, we analyse the state of the art of software design and verification tools and approaches in the light of our experience, and identify weaknesses, alternative design approaches and open challenges that could generate new research in these areas. More details can be found in the journal publication.},
keywords={Big Data;Databases;Quality of service;Tools;Fault tolerance;Fault tolerant systems;Software;Data intensive applications;Experiment driven action research;Big data;Data migration},
doi={10.1145/3180155.3182534},
ISSN={1558-1225},
month={May},}
@INPROCEEDINGS{9006187,
author={Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={DQA: Scalable, Automated and Interactive Data Quality Advisor},
year={2019},
volume={},
number={},
pages={2913-2922},
abstract={Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.},
keywords={Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science},
doi={10.1109/BigData47090.2019.9006187},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8386523,
author={Cao, Rui and Gao, Jing},
booktitle={2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Research on reliability evaluation of big data system},
year={2018},
volume={},
number={},
pages={261-265},
abstract={The application of big data system is now more pervasive. The reliability of the large data system is crucial to both the academic and the industry. However, to date there are few studies on the reliability of the big data system, and lack of evaluation model. This paper uses the fault tree to model the reliability of the big data system on the cloud. The type of faults is summarized and the cause of fault is analyzed by experiments. The fault tree analysis (FTA) is used to evaluate the reliability of the big data system, which can provide reference for the fault processing and quality assurance of big data system.},
keywords={Big Data;Fault trees;Software;Software reliability;Data models;Hardware;big data system;reliability;fault tree;evaluation},
doi={10.1109/ICCCBDA.2018.8386523},
ISSN={},
month={April},}
@INPROCEEDINGS{7363865,
author={Rettig, Laura and Khayati, Mourad and Cudré-Mauroux, Philippe and Piórkowski, Michal},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Online anomaly detection over Big Data streams},
year={2015},
volume={},
number={},
pages={1113-1122},
abstract={Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.},
keywords={Entropy;Measurement;Correlation;Big data;Data structures;Yttrium;Sparks},
doi={10.1109/BigData.2015.7363865},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7363900,
author={Stojanovic, Nenad and Dinic, Marko and Stojanovic, Ljiljana},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Big data process analytics for continuous process improvement in manufacturing},
year={2015},
volume={},
number={},
pages={1398-1407},
abstract={One of the most important challenges in manufacturing is the continuous process improvement that requires new insights about the behavior/quality control of processes in order to understand the optimization/improvement potential. The paper elaborates on usage of big data-driven clustering for an efficient discovering of real-time unusualities in the process and their route-cause analysis. Our approach extends traditional clustering algorithms (like k-Means) with methods for better understanding the nature of clusters and provides a very efficient big data realization. We argue that this approach paves the way for a new generation of quality management tools based on big data analytics that will extend traditional statistical process control and empower Lean Six Sigma through big data processing. The proposed approach has been applied for improving process control in Whirlpool (washing machine tests, factory in Italy) and we present the most important finding from the evaluation study.},
keywords={Clustering algorithms;Big data;Process control;Partitioning algorithms;Manufacturing;Six sigma;Analytical models;Big data;manufactoring;quality control},
doi={10.1109/BigData.2015.7363900},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8596389,
author={Ezzine, Imane and Benhlima, Laila},
booktitle={2018 IEEE 5th International Congress on Information Science and Technology (CiSt)},
title={A Study of Handling Missing Data Methods for Big Data},
year={2018},
volume={},
number={},
pages={498-501},
abstract={Improving data quality is not a recent field but in the context of big data this is a challenging area as there is a crucial need for data quality to, for example, increase the accuracy of big data analytics or avoid storing redundant data. Missing data is one of the major problem that faces the quality of data. There are several methods and approaches that have been used in relational databases to handle missing data most of which have been adapted to big data. This paper aims to provide an overview of some methods and approaches for handling missing data in big data contexts.},
keywords={Data integrity;Big Data;Data models;Dictionaries;Machine learning algorithms;Machine learning;Measurement;data quality;missing data;big data;functional dependancy;master data;machine learning},
doi={10.1109/CIST.2018.8596389},
ISSN={2327-1884},
month={Oct},}
@INPROCEEDINGS{8421858,
author={Jiang, Wei and Ning, Xiuli and Xu, Yingcheng},
booktitle={2018 5th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2018 4th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)},
title={Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods},
year={2018},
volume={},
number={},
pages={95-102},
abstract={Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.},
keywords={Big Data;Inspection;Data integration;Semantics;Hidden Markov models;Thesauri;Support vector machines;Big data of quality inspection;data fusion;named entity recognition;entity resolution;data conflict resolution},
doi={10.1109/CSCloud/EdgeCom.2018.00025},
ISSN={},
month={June},}
@INPROCEEDINGS{8276745,
author={Micic, Natasha and Neagu, Daniel and Campean, Felician and Zadeh, Esmaeil Habib},
booktitle={2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)},
title={Towards a Data Quality Framework for Heterogeneous Data},
year={2017},
volume={},
number={},
pages={155-162},
abstract={Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.},
keywords={Data integrity;Measurement;Warranties;Metadata;Cleaning;Big Data;Heterogeneous Data Sets;Data Quality;Metadata;Data Cleaning;Data Quality Assessment},
doi={10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28},
ISSN={},
month={June},}
@INPROCEEDINGS{8258270,
author={Karafili, Erisa and Lupu, Emil C. and Cullen, Alan and Williams, Bill and Arunkumar, Saritha and Calo, Seraphin},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Improving data sharing in data rich environments},
year={2017},
volume={},
number={},
pages={2998-3005},
abstract={The increasing use of big data comes along with the problem of ensuring correct and secure data access. There is a need to maximise the data dissemination whilst controlling their access. Depending on the type of users different qualities and parts of data are shared. We introduce an alteration mechanism, more precisely a restriction one, based on a policy analysis language. The alteration reflects the level of trust and relations the users have, and are represented as policies inside the data sharing agreements. These agreements are attached to the data and are enforced every time the data are accessed, used or shared. We show the use of our alteration mechanism with a military use case, where different parties are involved during the missions, and they have different relations of trust and partnership.},
keywords={Big Data;Drones;Data privacy;Electronic mail;Access control;Big data;data sharing;data access;usage control;DSAs;drone systems;military scenario},
doi={10.1109/BigData.2017.8258270},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7944957,
author={Zhang, Pengcheng and Zhou, Xuewu and Li, Wenrui and Gao, Jerry},
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)},
title={A Survey on Quality Assurance Techniques for Big Data Applications},
year={2017},
volume={},
number={},
pages={313-319},
abstract={With the rapid advance of big data and cloud computing, building high quality big data systems in different application fields has gradually became a popular research topic in academia and industry as well as government agencies. However, more quality problems lead to application errors. Although the current research work has discussed how to ensure the quality of big data applications from several aspects, there is no systematic discussion on how to ensure the quality of large data applications. Therefore, a systematic study on big data application quality assurance is very necessary and critical. This paper focuses on the survey of quality assurance techniques of big data applications, and it introduces big data properties and quality attributes. It mainly discusses the key approaches to ensure the quality of big data applications and they are testing, model-driven architecture (MDA), monitoring, fault tolerance, verification and also prediction techniques. In addition, this paper also discusses the impact of big data characteristics on big data applications.},
keywords={Big Data applications;Quality assurance;Testing;Data models;Monitoring;Computer architecture;Quality Assurance;Big data;Big data application;MDA;Testing;Verification;Fault tolerance;Monitoring;Prediction},
doi={10.1109/BigDataService.2017.42},
ISSN={},
month={April},}
@ARTICLE{8787229,
author={Kumar, Sunil and Singh, Maninder},
journal={Big Data Mining and Analytics},
title={A novel clustering technique for efficient clustering of big data in Hadoop Ecosystem},
year={2019},
volume={2},
number={4},
pages={240-247},
abstract={Big data analytics and data mining are techniques used to analyze data and to extract hidden information. Traditional approaches to analysis and extraction do not work well for big data because this data is complex and of very high volume. A major data mining technique known as data clustering groups the data into clusters and makes it easy to extract information from these clusters. However, existing clustering algorithms, such as k-means and hierarchical, are not efficient as the quality of the clusters they produce is compromised. Therefore, there is a need to design an efficient and highly scalable clustering algorithm. In this paper, we put forward a new clustering algorithm called hybrid clustering in order to overcome the disadvantages of existing clustering algorithms. We compare the new hybrid algorithm with existing algorithms on the bases of precision, recall, F-measure, execution time, and accuracy of results. From the experimental results, it is clear that the proposed hybrid clustering algorithm is more accurate, and has better precision, recall, and F-measure values.},
keywords={Clustering algorithms;Big Data;Ocean temperature;Data mining;Meteorology;Partitioning algorithms;Temperature control;clustering;Hadoop;big data;k-means;hierarchical},
doi={10.26599/BDMA.2018.9020037},
ISSN={2096-0654},
month={Dec},}
@INPROCEEDINGS{9368701,
author={Wang, Xin and Zhao, Xinbin and Yu, Liling},
booktitle={2020 IEEE 2nd International Conference on Civil Aviation Safety and Information Technology (ICCASIT},
title={Data Mining on the Flight Quality of an Airline based on QAR Big Data},
year={2020},
volume={},
number={},
pages={955-958},
abstract={At present, the airlines have made some achievements in event analysis and investigation by using their quick access record (QAR) data. But where each airline's flight quality is in the industry, and whether there is a problem in itself, the airline can't find. In order to help airlines discover the existing flight quality problems, this article uses the QAR big data of the flight operational quality assurance (FOQA) Station of CAAC, and compares the industry-wide QAR data with the QAR data of individual airlines, and founds that the take-off pitch angle of a certain aircraft of A321 models is too small, by using mathematical statistics t test to verify, found the airline's the take-off pitch angle and the industry's the take-off pitch angle exist significant difference. The correlative speed at rotation and the speed at liftoff are also analyzed, and the significant difference is found. The FOQA Station of CAAC feeds back the problem to the airline and the authority. After the investigation of the airline and the authority, there are problems with the airline. And the airline immediately starts to rectify it.},
keywords={Quality assurance;Atmospheric modeling;Big Data;Data models;Time measurement;Safety;Mathematical model;flight quality;pitch;QAR data;normal distribution;t test},
doi={10.1109/ICCASIT50869.2020.9368701},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8078802,
author={Xu, Birong and Wang, Weijiang and Wu, Yuyan and Shi, Yueting and Lu, Chang},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
title={Internet of things and big data analytics for smart oil field malfunction diagnosis},
year={2017},
volume={},
number={},
pages={178-181},
abstract={With the rapid development of information technology and digital communication, the data types are more abundant by integration of various technologies. In this paper, based on the analysis of a large number of historical data of oil and water wells, the changes of some important parameters of the wells can be monitored and then used in the trend prediction and the early warning system. Subsequently, we use 6 Sigma algorithm to process the historical data, and by the big data trend analysis combining with various parameters, we can diagnose six operating conditions, such as sand production, abnormal of moisture content etc. Through experiments, the algorithm is stable and reliable in practical application, and it has great significance to ensure the normal production of oil field and improve the management ability for oil field.},
keywords={Oils;Production;Standards;Fluids;Moisture;Big Data;Algorithm design and analysis;internet of thing;warning thresholds scheme;oil field fault diagnosis;big data;6 sigma},
doi={10.1109/ICBDA.2017.8078802},
ISSN={},
month={March},}
@INPROCEEDINGS{9134140,
author={Gan, Wenting},
booktitle={2020 International Conference on E-Commerce and Internet Technology (ECIT)},
title={Design of Network Precision Marketing Based on Big Data Analysis Technology},
year={2020},
volume={},
number={},
pages={77-81},
abstract={In the process of big data processing, big data analysis is the core work content. After obtaining a large amount of data, we use related analysis technology to perform data processing and analysis to obtain knowledge. Its related content includes visual analysis, data mining, predictive analysis, semantic analysis and data quality management. How to obtain big data, classify and store according to data types, mine valuable information from big data, and effectively apply big data in the field of precision marketing are hot topics of research. On the basis of researching the key technologies of big data analysis, this paper uses Hadoop big data to analyze and store the massive online logs generated by users' mobile terminals, and calculates and builds user characteristic databases. We use relevant analysis technology to analyze the user's location information, browsing and usage habits, hobbies, and focus content. At the same time, a precise marketing model is established according to user behavior characteristics and attributes, thereby improving the marketing effect of the enterprise.},
keywords={Visualization;Data analysis;Databases;Data integrity;Semantics;Big Data;Internet;Big Data;E-commerce;Internet marketing;Marketing design},
doi={10.1109/ECIT50008.2020.00026},
ISSN={},
month={April},}
@INPROCEEDINGS{8258221,
author={Hee, Kim},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Is data quality enough for a clinical decision?: Apply machine learning and avoid bias},
year={2017},
volume={},
number={},
pages={2612-2619},
abstract={This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.},
keywords={Data mining;Medical diagnostic imaging;Currencies;Tools;Medical services;Measurement;data quality;decision quality;healthcare;clinical data;EHRs;data quality assurance;stratified sampling;bias},
doi={10.1109/BigData.2017.8258221},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9172875,
author={Zhang, Guobao},
booktitle={2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC)},
title={A data traceability method to improve data quality in a big data environment},
year={2020},
volume={},
number={},
pages={290-294},
abstract={In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.},
keywords={Data Governance;Data Credibility;Data Traceability},
doi={10.1109/DSC50466.2020.00051},
ISSN={},
month={July},}
@INPROCEEDINGS{8625289,
author={Guler, Emine Rumeysa and Ozdemir, Suat},
booktitle={2018 International Congress on Big Data, Deep Learning and Fighting Cyber Terrorism (IBIGDELFT)},
title={Applications of Stream Data Mining on the Internet of Things: A Survey},
year={2018},
volume={},
number={},
pages={51-55},
abstract={In the era of the Internet of Things (IoT), enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices result in big or fast/real time data streams. The analytics technique on the subject matter used to discover new information, anticipate future predictions and make decisions on important issues makes IoT technology valuable for both the business world and the quality of everyday life. In this study, first of all, the concept of IoT and its architecture and relation with big and streaming data are emphasized. Information discovery process applied to the IoT streaming data is investigated and deep learning frameworks covered by this process are described comparatively. Finally, the most commonly used tools for analyzing IoT stream data are introduced and their characteristics are revealed.},
keywords={Big Data;Deep learning;Python;Cyber terrorism;Mathematical model;Data mining;Nanoelectromechanical systems;IoT;Big Data Analytics;Deep Learning;Stream Data Mining;Data Processing Platforms},
doi={10.1109/IBIGDELFT.2018.8625289},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8669651,
author={Feng, Peilu},
booktitle={2019 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={Big Data Analysis of E-Commerce Based on the Internet of Things},
year={2019},
volume={},
number={},
pages={345-347},
abstract={In the era of big data, while providing massive information, it also challenges the development of related activities in the overall environment. In the context of the rapid development of e-commerce, the opportunities of the development of the Internet of things technology are analyzed from the aspects of logistics distribution, quality control and facilities promotion. Electronic commerce is a new form of trade under the development of modern information technology, while cloud computing and the Internet of Things provide related services. Under the exertion of their related functions, the revolutionary improvement of e-commerce mode has been realized, and to a certain extent, it has promoted the development and operation of modern market economy. This article analyzes the development strategy of e-commerce based on Internet of things and cloud computing under the overall environment of big data era.},
keywords={Conferences;Transportation;Big Data;Smart cities;ISO;Big Data;Internet of things;electronic commerce},
doi={10.1109/ICITBS.2019.00091},
ISSN={},
month={Jan},}
@INPROCEEDINGS{9150244,
author={Zhang, Lanlan and Zou, Du},
booktitle={2020 International Conference on Big Data and Informatization Education (ICBDIE)},
title={Product quality prediction of rolling mill in big data environment},
year={2020},
volume={},
number={},
pages={34-38},
abstract={With the wide use of rolling mill in iron and steel industry, the quality of rolling mill products has become the primary goal of people. However, due to design defects and manufacturing quality problems, the quality of steel products is seriously affected, and the surface roughness and thickness of steel plate are important quality indicators. In this paper, by analyzing a large number of monitoring data of rolling mill condition and using BP neural network model [1], the discrete system model between monitoring data and “surface roughness” and “thickness error” of rolling steel plate is further established.},
keywords={Data models;Neural networks;Analytical models;Predictive models;Rough surfaces;Surface roughness;Mathematical model;Product quality;BP neural network;data analysis;surface roughness;thickness error},
doi={10.1109/ICBDIE50010.2020.00015},
ISSN={},
month={April},}
@INPROCEEDINGS{8109143,
author={Chengzan, Li and Yanfei, Hou and Jianhui, Li and Lili, Zhang},
booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)},
title={ScienceDB: A Public Multidisciplinary Research Data Repository for eScience},
year={2017},
volume={},
number={},
pages={248-255},
abstract={Research data repositories are necessary infrastructures that ensure the data generated for research are accessible, stable, reliable, and reusable. Based on years of accumulated data work experience, the Computer Network Information Center of the Chinese Academy of Sciences has built a multi-disciplinary data repository ScienceDB for research users and teams using its big data storage, analysis and computing environments. This paper firstly introduces the motivation to develop ScienceDB and gives a profile to it. Then the overall technical framework of ScienceDB is introduced, and the key technologies such as the support for multidiscipline extensibility, data collaboration and data recommendation are analyzed deeply. And then this paper presents the functions and features of ScienceDB's current version and discusses some issues such as its data policy, data quality assurance measures, and current application status. Finally, it summarizes and puts forward that it needs to carry out more in-depth research and practice of ScienceDB in order to meet the higher requirements of eScience in terms of thorough data association and fusion, data analysis and mining, data evaluation, and so on.},
keywords={Metadata;Business;Data visualization;Data mining;Distributed databases;Collaboration;Computer architecture;research data repository;technical framework;multidiscipline;data recommendation;data collaboration;open data},
doi={10.1109/eScience.2017.38},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7248139,
author={Chen, Zeqiang and Chen, Nengcheng and Gong, Jianya},
booktitle={2015 Fourth International Conference on Agro-Geoinformatics (Agro-geoinformatics)},
title={Design and implementation of the real-time GIS data model and Sensor Web service platform for environmental big data management with the Apache Storm},
year={2015},
volume={},
number={},
pages={32-35},
abstract={An abstract real-time GIS data model and Sensor Web service platform was proposed to manage real-time environmental data. With the development of sensor technology, more and more sensor networks are deployed to monitor our environment, and then generate environmental big data. How to improve the real-time GIS data model and Sensor Web service platform for real-time environmental big data manage is a problem. In this paper, the Apache Storm is adopted to deal with the question. A design and implementation of the real-time GIS data model and Sensor Web service platform for environmental big data management with Apache Storm is proposed. The main studied contents include integrating the Apache Strom with the Sensor Web service as the Sensor Observation Service, and processing the environmental big data timely. To test the feasibility of the design and implementation, two use cases of real-time air quality monitoring and real-time soil moisture monitoring based on the real-time GIS data model in the Sensor Web service platform are realized and demonstrated. The experimental results show that the implementation of real-time GIS data model and Sensor Web Service Platform with the Apache Storm is an effective way to manage real-time environmental big data.},
keywords={Real-time systems;Storms;Data models;Web services;Big data;Air quality;Soil moisture;real-time GIS data model;Sensor Web;environmental big data;Apache Storm},
doi={10.1109/Agro-Geoinformatics.2015.7248139},
ISSN={},
month={July},}
@INPROCEEDINGS{8787394,
author={Vidal, Maria-Esther and Jozashoori, Samaneh and Sakor, Ahmad},
booktitle={2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)},
title={Semantic Data Integration Techniques for Transforming Big Biomedical Data into Actionable Knowledge},
year={2019},
volume={},
number={},
pages={563-566},
abstract={FAIR principles and the Open Data initiatives have motivated the publication of large volumes of data. Specifically, in the biomedical domain, the size of the data has increased exponentially in the last decade, and with the advances in the technologies to collect and generate data, a faster growth rate is expected for the next years. The available collections of data are characterized by the dominant dimensions of big data, i.e., they are not only large in volume, but they can be also heterogeneous and present quality issues. These data complexity problems impact on the typical tasks of data management, and particularly, in the task of integrating big biomedical data sources. We tackle the problem of big data integration and present a knowledge-driven framework able to extract and integrate data collected from structured and unstructured data sources. The proposed framework resorts to Natural Language Processing techniques to extract knowledge from unstructured data and short text. Furthermore, ontologies and controlled vocabularies, e.g., UMLS, are utilized to annotate the extracted entities and relations with terms from the ontology or controlled vocabulary. The annotated data is integrated into a knowledge graph. A unified schema is used to describe the meaning of the integrated data as well as the main properties and relations. As proof of concept, we show the results of applying the proposed framework to integrate clinical records from lung cancer patients with data extracted from open data sources like Drugbank and PubMed. The created knowledge graph enables the discovery of interactions between drugs in the treatments prescribed to lung cancer patients.},
keywords={Drugs;Data mining;Semantics;Data integration;Unified modeling language;Ontologies;Bioinformatics;Semantic Data Integration;Big Data;Knowledge Graph;Biomedical Data;Natural Language Processing},
doi={10.1109/CBMS.2019.00116},
ISSN={2372-9198},
month={June},}
@INPROCEEDINGS{7848697,
author={Tan, Julian SK and Ang, Ai Kiar and Lu, Liu and Gan, Sheena WQ and Corral, Marilyn G},
booktitle={2016 IEEE Region 10 Conference (TENCON)},
title={Quality Analytics in a Big Data supply chain: Commodity data analytics for quality engineering},
year={2016},
volume={},
number={},
pages={3455-3463},
abstract={While the world is experiencing a global shortage of natural resources, a new one in the form of Digital Data has emerged! The ability to harness this new resource has become a renewed basis for competitive advantage where leveraging Big Data effectively means winning in the marketplace. It is going to transform industries and professions around the world. However, traditional data management techniques and analytical methodologies that has taken us from the late 20th century and into the early 21st century are not sustainable in today's business environment where organizations are constantly being challenged to right size the work force, increase labor productivity, increase customer satisfaction and at the same time improving product quality and reliability.},
keywords={Supply chains;Big data;Manufacturing;Data visualization;Market research;Industries;Supply Chain;Analytics;Industrie 4.0;IoT;Big Data;Internet of Things;Predictive;Prescriptive;Cognitive;Descriptive;Data Management;Data Source;Systems of Engagement;Systems of Records;Quality;Commodity},
doi={10.1109/TENCON.2016.7848697},
ISSN={2159-3450},
month={Nov},}
@INPROCEEDINGS{7840777,
author={Stojanovic, Ljiljana and Dinic, Marko and Stojanovic, Nenad and Stojadinovic, Aleksandar},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Big-data-driven anomaly detection in industry (4.0): An approach and a case study},
year={2016},
volume={},
number={},
pages={1647-1652},
abstract={In this paper we present a novel approach for data-driven Quality Management in industry processes that enables a multidimensional analysis of the anomalies that can appear and their real-time detection in the running system. The approach revolutionizes the way how quality control (and esp. anomaly detection) will be realized in production processes influenced by many parameters that can be in complex nonlinear correlations. It consists of two main steps: learning the normal behavior of the system (based on past data) and detecting an anomalous behavior in the real-time (by processing real-time data). The approach is especially suitable for modern industry systems that follow Industry 4.0 principles of ubiquity sensing and proactive responding. One of the main advantages is the self-adaptive nature of the approach due to its data-driven orientation, so that the model and parameters of the approach will be continuously updated to the dynamicity of data. The approach has been applied in the process of manufacturing microwave ovens (Whirlpool) and in this paper we present results for the data-driven quality control of one of the most critical parts - microwave oven fan. Due to the high speed of the rotation, every item has to be very precisely produced (according to the CAD model), which requires very strong quality control process.},
keywords={Real-time systems;Quality control;Process control;Production;Monitoring;Industries;Big data;Big Data;Anomaly Detection;CEP;Data-driven Quality Control;Industrie 4.0},
doi={10.1109/BigData.2016.7840777},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8029373,
author={Zhou, Lixiao and Huang, Maohai},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={Challenges of Software Testing for Astronomical Big Data},
year={2017},
volume={},
number={},
pages={529-532},
abstract={Astronomy has been one of the first areas of science to embrace and learn from big data. The amount of data we have on our universe is doubling every year thanks to big telescopes and better light detectors. Most leading research is based on data from a handful of very expensive telescopes. Undoubtedly, the data quality is the key basis for the leading scientific findings. It is imperative that software testers understand that big data is about far more than simply data volume. This paper will analyze characteristics, types, methods, strategies, problems, challenges and propose some possible solutions of software testing for astronomical big data.},
keywords={Big Data;Software testing;Software;Astronomy;Data analysis;Distributed databases;software testing; big data; astronomical software},
doi={10.1109/BigDataCongress.2017.91},
ISSN={},
month={June},}
@INPROCEEDINGS{7034765,
author={Nowling, Ronald J. and Vyas, Jay},
booktitle={2014 IEEE Fourth International Conference on Big Data and Cloud Computing},
title={A Domain-Driven, Generative Data Model for Big Pet Store},
year={2014},
volume={},
number={},
pages={49-55},
abstract={Generating large amounts of semantically-rich data for testing big data workflows is paramount for scalable performance benchmarking and quality assurance in modern machine-learning and analytics workloads. The most obvious use case for such a generative algorithm is in conjunction with a big data application blueprint, which can be used by developers (to test their emerging big data solutions) as well as end users (as a starting point for validating infrastructure installations, building novel applications, and learning analytics methods). We present a new domain-driven, generative data model for Big Pet Store, a big data application blueprint for the Hadoop ecosystem included in the Apache Big Top distribution. We describe the model and demonstrate its ability to generate semantically-rich data at variable scale ranging from a single machine to a large cluster. We validate the model by using the generated data to answer questions about customer locations and purchasing habits for a fictional targeted advertising campaign, a common business use case.},
keywords={Hidden Markov models;Data models;Probability density function;Big data;Benchmark testing;Generators;big data;synthetic data sets;data generation;benchmarking;testing;probabilistic models},
doi={10.1109/BDCloud.2014.38},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7374131,
author={Juddoo, Suraj},
booktitle={2015 International Conference on Computing, Communication and Security (ICCCS)},
title={Overview of data quality challenges in the context of Big Data},
year={2015},
volume={},
number={},
pages={1-9},
abstract={Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.},
keywords={Big data;Context;Quality management;Frequency measurement;Organizations;Big Data;Data quality;Data profiling;Data cleansing;data quality rules;dimensions;metrics},
doi={10.1109/CCCS.2015.7374131},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8094109,
author={Lincy, S.S. Blessy Trencia and Kumar, N. Suresh},
booktitle={2017 International Conference on Innovations in Green Energy and Healthcare Technologies (IGEHT)},
title={An enhanced pre-processing model for big data processing: A quality framework},
year={2017},
volume={},
number={},
pages={1-7},
abstract={With the ever growing trends and technologies a huge volume of data is being evolved each and every second big data has become a supreme approach in data inception, accession, processing and analyzing the heterogeneous, huge amount of data so as to derive useful insights out of it. With data and without quality there is no point in having the data. Thus, data with quality is required to use or leverage the data in a more appropriate manner. With the evolution of big data many technologies are being developed. The input to it must be processed in such a way that the quality data yields quality effective results. An effective pre-processing model is proposed in this paper for the processing of the big data. Using relief algorithm and fast mRMR together as a hybrid approach can be used for the pre-processing of the data. Analysis shows that this hybrid approach is more effective and can greatly enhance the quality of the data. This approach can yield better performance upon the big data platform using the Spark framework.},
keywords={Big Data;Data models;Algorithm design and analysis;Medical services;Data analysis;Brain modeling;Feature extraction;Big data;pre-processing;relief algorithm;fast mRMR;SparkR},
doi={10.1109/IGEHT.2017.8094109},
ISSN={},
month={March},}
@INPROCEEDINGS{9239752,
author={Wang, Wenjing and Yang, Shengquan},
booktitle={2020 International Conference on Computer Network, Electronic and Automation (ICCNEA)},
title={Research on Air Quality Forecasting Based on Big Data and Neural Network},
year={2020},
volume={},
number={},
pages={180-184},
abstract={Aiming at the problem that existing air quality prediction models cannot efficiently and accurately predict air quality in a big data environment, an air quality prediction method based on a big data platform to implement a distributed neural network is proposed. Collect historical data of the six pollutant concentrations that affect the air quality index and use it as input to a neural network model; A distributed neural network model containing the AQI change rule in the distributed neural network structure is adopted to realize the short-term prediction of the AQI. Experimental results show that air quality prediction models based on big data and neural networks can reveal the development trend of air quality through self-learning characteristics. And has higher prediction accuracy, It can provide a scientific basis for the degree of urban air pollution and help people make appropriate measures for different AQI levels.},
keywords={Air quality;Predictive models;Atmospheric modeling;Data models;Big Data;Biological neural networks;AQI Prediction;Big Data;Neural Network},
doi={10.1109/ICCNEA50255.2020.00045},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9403810,
author={Huimin, Li and Guomin, Song},
booktitle={2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)},
title={Research on the Teaching Reform of Finance and Accounting Major under the Background of Big Data},
year={2020},
volume={},
number={},
pages={73-77},
abstract={With the development of information and intelligent technology such as Internet, big data and cloud computing, the concept of big data in education and teaching has also been widely concerned by the society and colleges and universities. The application of big data technology broadens educational resources and teaching channels, and provides a new space for teaching reform and talent training of accounting majors. Based on the background of big data and combined with the characteristics of finance and accounting majors, this paper integrates big data thinking into the teaching reform process of finance and accounting majors, integrates the needs of big data information resources and application ability cultivation of finance and accounting majors, improves the training quality of finance and accounting professionals, and promotes the teaching reform of finance and accounting majors.},
keywords={Training;Technological innovation;Cloud computing;Education;Finance;Information services;Big Data;Big data;Major in Accounting;Teaching reform},
doi={10.1109/ICBASE51474.2020.00023},
ISSN={},
month={Oct},}
@ARTICLE{8782595,
author={Wang, Songyun and Yuan, Jiabin and Li, Xin and Qian, Zhuzhong and Arena, Fabio and You, Ilsun},
journal={IEEE Access},
title={Active Data Replica Recovery for Quality-Assurance Big Data Analysis in IC-IoT},
year={2019},
volume={7},
number={},
pages={106997-107005},
abstract={QoS-aware big data analysis is critical in Information-Centric Internet of Things (IC-IoT) system to support various applications like smart city, smart grid, smart health, intelligent transportation systems, and so on. The employment of non-volatile memory (NVM) in cloud or edge system provides good opportunity to improve quality of data analysis tasks. However, we have to face the data recovery problem led by NVM failure due to the limited write endurance. In this paper, we investigate the data recovery problem for QoS guarantee and system robustness, followed by proposing a rarity-aware data recovery algorithm. The core idea is to establish the rarity indicator to evaluate the replica distribution and service requirement comprehensively. With this idea, we give the lost replicas with distinguishing priority and eliminate the unnecessary replicas. Then, the data replicas are recovered stage by stage to guarantee QoS and provide system robustness. From our extensive experiments and simulations, it is shown that the proposed algorithm has significant performance improvement on QoS and robustness than the traditional direct data recovery method. Besides, the algorithm gives an acceptable data recovery time.},
keywords={Nonvolatile memory;Quality of service;Data analysis;Data centers;Bandwidth;Big Data;Robustness;Big data analysis;data recovery;IC-IoT;NVM;QoS improvement},
doi={10.1109/ACCESS.2019.2932259},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7364047,
author={Wang, Jianwu and Crawl, Daniel and Purawat, Shweta and Nguyen, Mai and Altintas, Ilkay},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Big data provenance: Challenges, state of the art and opportunities},
year={2015},
volume={},
number={},
pages={2509-2516},
abstract={Ability to track provenance is a key feature of scientific workflows to support data lineage and reproducibility. The challenges that are introduced by the volume, variety and velocity of Big Data, also pose related challenges for provenance and quality of Big Data, defined as veracity. The increasing size and variety of distributed Big Data provenance information bring new technical challenges and opportunities throughout the provenance lifecycle including recording, querying, sharing and utilization. This paper discusses the challenges and opportunities of Big Data provenance related to the veracity of the datasets themselves and the provenance of the analytical processes that analyze these datasets. It also explains our current efforts towards tracking and utilizing Big Data provenance using workflows as a programming model to analyze Big Data.},
keywords={Big data;Data models;Distributed databases;Sparks;Engines;Programming;Context;Big Data;provenance;workflows;distributed data-parallel programming models},
doi={10.1109/BigData.2015.7364047},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9006234,
author={Harris, Daniel R. and Delcher, Chris},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={bench4gis: Benchmarking Privacy-aware Geocoding with Open Big Data},
year={2019},
volume={},
number={},
pages={4067-4070},
abstract={Geocoding, the process of translating addresses to geographic coordinates, is a relatively straight-forward and well-studied process, but limitations due to privacy concerns may restrict usage of geographic data. The impact of these limitations are further compounded by the scale of the data, and in turn, also limits viable geocoding strategies. For example, healthcare data is protected by patient privacy laws in addition to possible institutional regulations that restrict external transmission and sharing of data. This results in the implementation of “in-house” geocoding solutions where data is processed behind an organization's firewall; quality assurance for these implementations is problematic because sensitive data cannot be used to externally validate results. In this paper, we present our software framework called bench4gis which benchmarks privacy-aware geocoding solutions by leveraging open big data as surrogate data for quality assurance; the scale of open big data sets for address data can ensure that results are geographically meaningful for the locale of the implementing institution.},
keywords={Medical services;Benchmark testing;Geospatial analysis;Big Data;Data privacy;Open source software;geographic information systems;geospatial analysis;big data applications},
doi={10.1109/BigData47090.2019.9006234},
ISSN={},
month={Dec},}