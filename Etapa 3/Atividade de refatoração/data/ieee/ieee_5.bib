@INPROCEEDINGS{8079923,
author={Yusuf-Asaju, Ayisat W. and Dahalin, Zulkhairi B. and Ta'a, Azman},
booktitle={2017 8th International Conference on Information Technology (ICIT)},
title={Mobile network quality of experience using big data analytics approach},
year={2017},
volume={},
number={},
pages={658-664},
abstract={Traditionally, Quality of experience is mostly examined in a laboratory experiments to enable a fixed contextual factor. While the results present an estimated mean opinion score representing perceived QoE. It is imperative to estimate mean opinion score employing large data (big data) gathered from the mobile network comprising of different user's location and time for a specific service. Because time and location can have a huge influence on the user perceived quality of experience. Therefore, this paper proposed a framework for modelling perceived QoE through big data analytics. The proposed framework describes the process of estimating perceived quality of experience to assist the mobile network operators effectively manage the network performance and aid satisfactory provision of mobile internet services.},
keywords={Mobile communication;Mobile computing;Big Data;Telecommunication traffic;Data models;Neural networks;Mean Opinion Score;Big data analytics;Mobile Network Operator;Telecoms},
doi={10.1109/ICITECH.2017.8079923},
ISSN={},
month={May},}
@INPROCEEDINGS{7231398,
author={Bayati, Shahabedin and Parsons, David and Susnjak, Teo and Heidary, Marzieh},
booktitle={2015 3rd International Conference on Information and Communication Technology (ICoICT)},
title={Big data analytics on large-scale socio-technical software engineering archives},
year={2015},
volume={},
number={},
pages={65-69},
abstract={Given the fast growing nature of software engineering data in online software repositories and open source communities, it would be helpful to analyse these assets to discover valuable information about the software engineering development process and other related data. Big Data Analytics (BDA) techniques and frameworks can be applied on these data resources to achieve a high-performance and relevant data collection and analysis. Software engineering is a socio-technical process which needs development team collaboration and technical knowledge to develop a high-quality application. GitHub, as an online social coding foundation, contains valuable information about the software engineers' communications and project life cycles. In this paper, unsupervised data mining techniques are applied on the data collected by general Big Data approaches to analyse GitHub projects, source codes and interactions. Source codes and projects are clustered using features and metrics derived from historical data in repositories, object oriented programming metrics and the influences of developers on source codes.},
keywords={Software;Data mining;Software engineering;Big data;Measurement;Encoding;Feature extraction;Big Data;GitHub Mining;Clustering;Mining Software Repositories (MSR);Empirical Software Engineering},
doi={10.1109/ICoICT.2015.7231398},
ISSN={},
month={May},}
@INPROCEEDINGS{7796948,
author={Bendechache, Malika and Kechadi, M-Tahar and Le-Khac, Nhien-An},
booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
title={Efficient Large Scale Clustering Based on Data Partitioning},
year={2016},
volume={},
number={},
pages={612-621},
abstract={Clustering techniques are very attractive for extracting and identifying patterns in datasets. However, their application to very large spatial datasets presents numerous challenges such as high-dimensionality data, heterogeneity, and high complexity of some algorithms. For instance, some algorithms may have linear complexity but they require the domain knowledge in order to determine their input parameters. Distributed clustering techniques constitute a very good alternative to the big data challenges (e.g.,Volume, Variety, Veracity, and Velocity). Usually these techniques consist of two phases. The first phase generates local models or patterns and the second one tends to aggregate the local results to obtain global models. While the first phase can be executed in parallel on each site and, therefore, efficient, the aggregation phase is complex, time consuming and may produce incorrect and ambiguous global clusters and therefore incorrect models. In this paper we propose a new distributed clustering approach to deal efficiently with both phases, generation of local results and generation of global models by aggregation. For the first phase, our approach is capable of analysing the datasets located in each site using different clustering techniques. The aggregation phase is designed in such a way that the final clusters are compact and accurate while the overall process is efficient in time and memory allocation. For the evaluation, we use two well-known clustering algorithms, K-Means and DBSCAN. One of the key outputs of this distributed clustering technique is that the number of global clusters is dynamic, no need to be fixed in advance. Experimental results show that the approach is scalable and produces high quality results.},
keywords={Clustering algorithms;Partitioning algorithms;Biological system modeling;Algorithm design and analysis;Spatial databases;Distributed databases;Data mining;Big Data;spatial data;clustering;distributed mining;data analysis;k-means;DBSCAN},
doi={10.1109/DSAA.2016.70},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7373958,
author={Ong, Vincent Koon},
booktitle={2015 IIAI 4th International Congress on Advanced Applied Informatics},
title={Big Data and Its Research Implications for Higher Education: Cases from UK Higher Education Institutions},
year={2015},
volume={},
number={},
pages={487-491},
abstract={With the rise of big data, its impact has becoming apparent even in the Higher Education sector. The strategic use and applications of big data in higher education would lead to higher educational quality and better student and staff experience. Using the output from UK JISC's BI projects, this paper reviews and outlines some cases of big data analytics in the UK Higher Education institutions, followed by some research implications for future big data research in Higher Education.},
keywords={Bismuth;Big data;Education;Data visualization;Decision making;Artificial intelligence;big data analytics;business intelligence;big data models;visualization},
doi={10.1109/IIAI-AAI.2015.178},
ISSN={},
month={July},}
@INPROCEEDINGS{9045779,
author={Zhang, Hong-xia and Ji, Jian-dong},
booktitle={2019 6th International Conference on Dependable Systems and Their Applications (DSA)},
title={Research on the Safety Prediction Method of Long Jump in Big Data},
year={2020},
volume={},
number={},
pages={223-230},
abstract={The long jump has a great influence on sports events, but the long jump athletes are vulnerable in training and their safety cannot be guaranteed. Therefore, this paper puts forward the research on the safety prediction method of long jump movement under big data. Through the research on the sport form characteristics of long jump athletes, this paper analyzes the athletes' physical quality and training mode, so as to develop the method to guarantee the safety prediction of long jump athletes. This paper makes a comparative analysis on the safety prediction methods of big data long jumpers. The experimental data shows that the safety prediction methods of big data long jumpers are 16.8% higher than the traditional methods. The prediction accuracy is better, the prediction time is shortened, and the safety of athletes can be better guaranteed(Abstract).},
keywords={Training;Prediction methods;Big Data;Safety;Sports;Big data;The long jump;Safety prediction;Training methodst(key words)},
doi={10.1109/DSA.2019.00035},
ISSN={},
month={Jan},}
@INPROCEEDINGS{6973836,
author={Bennani, Nadia and Ghedira-Guegan, Chirine and Musicante, Martin A. and Solar, Genoveva Vargas},
booktitle={2014 IEEE 7th International Conference on Cloud Computing},
title={SLA-Guided Data Integration on Cloud Environments},
year={2014},
volume={},
number={},
pages={934-935},
abstract={Existing data integration techniques have to be revisited to query big data collections on the Cloud. Service Level Agreements implement the contracts between the cloud provider and the users, and between the cloud and service providers. Given SLA heterogeneity and data integration scalability problems, we propose an SLA guided data integration for querying data on multiple clouds.},
keywords={Data integration;Contracts;Quality of service;Cloud computing;Big data;Monitoring;Data models;SLA;Cloud Computing;Data Integration},
doi={10.1109/CLOUD.2014.130},
ISSN={2159-6190},
month={June},}
@INPROCEEDINGS{8464770,
author={Adrian, Cecilia and Abdullah, Rusli and Atan, Rodziah and Jusoh, Yusmadi Yah},
booktitle={2018 Fourth International Conference on Information Retrieval and Knowledge Management (CAMP)},
title={Expert Review on Big Data Analytics Implementation Model in Data-driven Decision-Making},
year={2018},
volume={},
number={},
pages={1-5},
abstract={Data-driven decision-making can offer improved insights for information value and create new business opportunity. The purpose of this paper is to present the findings of expert opinion in verifying the influencing factors in big data analytics (BDA) implementation that are beneficial in developing the BDA implementation assessment model. The study was carried out by conducting face-to-face approach sessions with three academicians and four industry experts who have vast experience in big data research and its implementation. The findings from these exercises has confirmed and verified the content of the ten factors that include organization dimension (such as big data strategy, top management support, resource commitment, organizational relationship), people dimension (such as analytics skills, managerial skills and analytics culture) and technology dimension (includes data infrastructures, information processing and quality) were appropriate for the research model. It was described using descriptive analysis such as frequency, mean and standard deviation. Once the verification process is complete, the research model will be validated through survey in the future work.},
keywords={Big Data;Organizations;Decision making;Standards organizations;Analytical models;expert review;decision-making;big data analytics implementation;factors},
doi={10.1109/INFRKM.2018.8464770},
ISSN={},
month={March},}
@INPROCEEDINGS{9070242,
author={Park, Minwoo and Lee, Ryong and Jang, Rae-young and Lee, Sang-hwan},
booktitle={2020 IEEE International Conference on Big Data and Smart Computing (BigComp)},
title={KISTI Vehicle-Based Urban Sensing Dataset},
year={2020},
volume={},
number={},
pages={601-604},
abstract={Recent smart city projects are rapidly expanding their technical boundary up to the Internet of Things, Big Data, and AI technology to realize the data-oriented urban situation awareness and making lots of sophisticated decision makings for various social issues such as traffic, air quality, boosting local economy, etc. In terms of cyber-physical systems, sensing data would be the first step to enable whole urban ecology to live and derive numerous benefits. In this work, in order to satisfy growing demands on urban environments by researchers and government officials, we have developed a vehicle-based urban sensing system to construct the basis to create urban sensing datasets. In this paper, we introduce our effort to collect urban sensing data collected in Daejeon City, Korea with two exclusive vehicles for three months (Sept.-Nov., 2019). We also describe what we can obtain from the collected datasets in terms of public benefits for establishing data-oriented smart cities.},
keywords={Sensors;Urban areas;Real-time systems;Roads;Servers;Air quality;Automobiles;Urban Sensing Dataset, Vehicle-based IoT Data Collection, Sensing Data, Vision Data},
doi={10.1109/BigComp48618.2020.00019},
ISSN={2375-9356},
month={Feb},}
@INPROCEEDINGS{9322987,
author={Cherviakov, Leonid M. and Sheptunov, Sergey A. and Oleynik, Andrey V. and Bychkova, Natalia A.},
booktitle={2020 International Conference Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)},
title={Digitalization of Quality Management of the Strategic Decision-Making Process},
year={2020},
volume={},
number={},
pages={193-196},
abstract={Strategic decisions are linked to long-term goals and priorities. At the same time, current trends point to an increasing dynamics of ongoing events that affect these priorities. To improve the quality of the decisions made, we propose to add a tactical loop that is able to track changes in events in a relatively short period of time to a system with a strategic management loop. To improve the quality of the relevant strategic decisions, we propose to use the block for analyzing big data, reflecting the state of the control object. The dynamics of the process is provided by intelligent analysis of the digital twin of the control object. Application of the approach under consideration relates to the field of strategic decision making as an additional solver.},
keywords={Decision making;Big Data;Task analysis;Planning;Process control;Technological innovation;Monitoring;digitalization;automation;making decisions;scientific and technological development;strategic decisions;big data;artificial intelligence},
doi={10.1109/ITQMIS51053.2020.9322987},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8754114,
author={Lo, Dan and Kevin Tiba, Karl and Buciumas, Sergiu and Ziller, Frank},
booktitle={2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)},
title={An Emperical Study on Application of Big Data Analytics to Automate Service Desk Business Process},
year={2019},
volume={2},
number={},
pages={670-675},
abstract={The maturity of the Big Data analytics allows enterprises to enhance their business processes using a data-driven approach. From retailers, E-commerce, risk analysis to pharmaceuticals, Bioinformatics, healthcare informatics, and others, business owners are thinking about mining business data into information for making better decisions to drive business value. However, the lack of reverse engineers and data analysts makes it harder for enterprises to apply Big Data analytics in their business data with a goal to enhance their operational processes. Ultimately, these processes can be automated to reduce human errors. In this paper, we study an empirical dataset containing real world service desk tickets using machine learning with a goal to automate and improve business processes, which will benefit to enterprises and customers in increasing quality of service.},
keywords={Business;Big Data;Machine learning;Clustering algorithms;Data analysis;Natural language processing;Genetic algorithms;Big Data Analytics, Machine Learning, Business Process, HPCC},
doi={10.1109/COMPSAC.2019.10285},
ISSN={0730-3157},
month={Jul},}
@INPROCEEDINGS{8784484,
author={Zhang, Xupeng and Liang, Du},
booktitle={2019 IEEE 9th International Conference on Electronics Information and Emergency Communication (ICEIEC)},
title={Construction of Elevator Inspection Quality Evaluation System Based on Big Data},
year={2019},
volume={},
number={},
pages={238-242},
abstract={Elevator inspection information has typical big data characteristics. This paper points out that the elevator inspection data introduces the method of elevator inspection big data analysis. Taking elevator inspection as an example, it lists several kinds of big data analysis methods for inspection data, including the risk points describing the basic information of the elevator, the scanning inspection process and the inspection quality. Based on frequency analysis of active factors, outlier test, quality assessment, correlation analysis. Using big data technology, it can make statistical analysis on the data obtained by elevator inspection, make the inspection situation more intuitive, help the management organization to understand the overall elevator quality and elevator inspection, and build an elevator inspection quality evaluation system to make the work more transparent and management more precise. Find more accurate questions, deeper supervision, and more scientific government decisions.},
keywords={Elevators;Inspection;Big Data;Safety;Market research;Testing;Monitoring;Elevator;inspection;big data;quality evaluation},
doi={10.1109/ICEIEC.2019.8784484},
ISSN={2377-844X},
month={July},}
@INPROCEEDINGS{9006288,
author={Yang, Hongzhi and Chiang, Chieh-Feng and Arbee L.P., Chen},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Discovering High Demanding Bus Routes Using Farecard Data},
year={2019},
volume={},
number={},
pages={3832-3837},
abstract={Having an effective public transport system is one of the most important factors to improve the quality of urban residents' life and to bring a sustainable development in urban areas. In this paper, we detect high demanding region pairs with inconvenient bus route design, such as taking circuitous routes or having too many stops, etc., to improve the utilization efficiency of public transportation services, according to people's real demands. The detected results consist of 1) region pairs with significant bus route design problems, and 2) the linking structure as well as the correlation among these region pairs. We compare these results to some existing and future urban planning, such as MRT lines, and study whether this planning reduces the current problems.},
keywords={Public transportation;Heuristic algorithms;Bars;Merging;Urban planning;Big Data;Urban computing;bus routing;big data;human mobility},
doi={10.1109/BigData47090.2019.9006288},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7724441,
author={Aggarwal, Ankur},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)},
title={Identification of quality parameters associated with 3V's of Big Data},
year={2016},
volume={},
number={},
pages={1135-1140},
abstract={Big Data approach uses an empirical process that does not lie on the understanding of underlying mechanisms, but lies on the observation of facts. Achieving high quality in Big Data is a critical issue for both the database researchers and practitioners. More explicit consideration must be given to data quality since data increasingly outlives the application for which it was initially designed. In this paper, identification of quality parameters is done which are compatible to the 3V's of big data which will further provide enhancement in achieving quality data to be stored in the repository. Good utilization of Big Data strengthens the performance and competitiveness of the firms by enabling better and faster results to its customer needs. In this paper GQM (Goal Question Metric) methodology is proposed to measure quality using metrics. It describes how to include data quality metrics to project, progress and maintain levels of quality in an organization. It helps to make a decision whether or not our current data satisfies our quality prospects.},
keywords={Big data;Organizations;Conferences;Social network services;Electronic mail;Databases;Big Data;Quality;Volume;Variety;Velocity},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{8004394,
author={Mondek, Dušan and Blažek, Rudolf B. and Zahradnický, Tomáš},
booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
title={Security Analytics in the Big Data Era},
year={2017},
volume={},
number={},
pages={605-606},
abstract={This paper discusses the reality of the state-of-theartof existing information security systems that often providesenseless functions based on “buzz-words”. It points out real-liferequirements that these systems tend to ignore. It proposes thatdynamically changing understandable use cases should becreated in collaboration with corporate management to addressobjectives that are essential for the businesses. Big data analyticmethodologies should be utilized to assist the design of efficientimplementations of these use cases. Big data approaches shouldalso be used for heuristic detection of unknown attacks andanomalies, for data enrichment, and for post-hoc forensicanalysis. The main goal for information security systems that areusable in real-life should be that corporate decision makers areonly provided trustworthy and actionable insights that arerelevant to the business and provided services.},
keywords={Information security;Big Data;Companies;Computer security;Collaboration;information security systems;buzz-words;real-life},
doi={10.1109/QRS-C.2017.136},
ISSN={},
month={July},}
@INPROCEEDINGS{7846371,
author={Fujita, Hamido},
booktitle={2016 IEEE 17th International Symposium on Computational Intelligence and Informatics (CINTI)},
title={Big data-based clouds health-care and risk predictions based on ensemble classifiers and subjective projection},
year={2016},
volume={},
number={},
pages={000011-000012},
abstract={Discovering patterns from big data attracts a lot of attention due to its importance in discovering accurate patterns and features that are used in predictions of decision making. The challenges in big data analytics are the high dimensionality and complexity in data representation. Granular computing and feature selection are among the challenge to deal with big data analytics that is used for Decision making. We will discuss these challenges in this talk and provide new projection on ensemble learning for health care risk prediction. In decision making most approaches are taking into account objective criteria, however the subjective correlation among different ensembles provided as preference utility is necessary to be presented to provide confidence preference additive among it reducing ambiguity and produce better utility preferences measurement for good quality predictions. Most models in Decision support systems are assuming criteria as independent. Different type of data (time series, linguistic values, interval data, etc.) imposes some difficulties to data analytics due to preprocessing and normalization processes which are expensive and difficult when data sets are raw and imbalanced. We will highlight these issues though project applied to health-care for elderly, by merging heterogeneous metrics for providing health care predictions for elderly at home. We have utilized ensemble learning as multi-classification techniques on multi-data streams that collected from multi-sensing devices. Subjectivity (i.e., service personalization) would be examined based on correlations between different contextual structures that are reflecting the framework of personal context, for example in nearest neighbor based correlation analysis fashion. Some of the attributes incompleteness also may lead to affect the approximation accuracy. Attributes with preference-ordered domain relations properties become one aspect in ordering properties in rough approximations. We outline issues on Virtual Doctor Systems, and highlights its innovation in interactions with elderly patients, also discuss these challenges in granular computing and decision support systems research domains. In this talk I will present the current state of art and focus it on health care risk analysis with examples from our experiments.},
keywords={Medical services;Big data;Correlation;Senior citizens;Decision making;Computational intelligence;Informatics},
doi={10.1109/CINTI.2016.7846371},
ISSN={2471-9269},
month={Nov},}
@INPROCEEDINGS{9526682,
author={XiangLi, Peng and YiXi, Wang and Fei, Long and Hao, Feng and Fen, Liu},
booktitle={2020 13th International Conference on Intelligent Computation Technology and Automation (ICICTA)},
title={Research On Parallel Clustering Algorithm Of Feature Hiding Big Data In Heterogeneous Networks},
year={2020},
volume={},
number={},
pages={424-429},
abstract={Under the background of big data, network data presents a complex and multi structural feature. In the computer network, clustering is the key feature of big data hiding in heterogeneous network. In a word, the data chain in the network is more compact, and the data link between structures is more evacuation. As an important network data application method and traditional data development tool, clustering algorithm has been widely used in academia and society. Based on this, a parallel clustering algorithm for feature hiding big data in heterogeneous networks is proposed. On the premise of establishing the fuzzy equivalent constraint Association of hidden data, the heterogeneous measurement of mixed data is calculated, and online clustering is realized by reconstructing data structure. The experimental results show that the parallel clustering algorithm designed in this paper can achieve good clustering results in data sets, and can measure the differences between data and classes more accurately and reasonably. The new algorithm overcomes the shortcomings of traditional clustering algorithm which classifies attributes according to the overall size of data set or the dispersion degree within the cluster. Compared with other data clustering algorithms, the algorithm proposed in this paper has higher practicability and higher clustering quality.},
keywords={Automation;Clustering algorithms;Big Data;Tools;Data structures;Big Data applications;Heterogeneous networks;Heterogeneous network;hiding large data;parallel clustering;Constraint Association},
doi={10.1109/ICICTA51737.2020.00096},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8517556,
author={Charvat, Karel and Junior, Karel Charvat and Reznik, Tomas and Lukas, Vojtech and Jedlicka, Karel and Palma, Raul and Berzins, Raitis},
booktitle={IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium},
title={Advanced Visualisation of Big Data for Agriculture as Part of Databio Development},
year={2018},
volume={},
number={},
pages={415-418},
abstract={There is an increasing tension in agriculture between the requirements to assure full safety on the one hand and keep costs under control on the other hand, both with respect to (inter)national strategies. Farmers need to measure and understand the impact of huge amount and variety of data which drive overall quality and yield in their fields. Among others, those are local weather data, Global Navigation System of Systems data, orthophotos and satellite imagery, data on soil specifics etc. A strong need to secure Big Data arises due to various repositories and heterogeneous sources. Data storage and visualisation requirements are in some cases competing as they are a common interest as well as a threat that helps one part of a value chain to gain a higher profit. As demonstrated in this paper, handling (Big) data is therefore a sensitive topic, where trust of producers on data security is essential.},
keywords={Agriculture;Data visualization;Data models;Productivity;Big Data;Three-dimensional displays;Unified modeling language;precision agriculture;big data;yield productivity zones;visualisation},
doi={10.1109/IGARSS.2018.8517556},
ISSN={2153-7003},
month={July},}
@ARTICLE{8641478,
author={Zhang, Meifan and Wang, Hongzhi and Li, Jianzhong and Gao, Hong},
journal={IEEE Access},
title={One-Pass Inconsistency Detection Algorithms for Big Data},
year={2019},
volume={7},
number={},
pages={22377-22394},
abstract={Data in the real world is often dirty. Inconsistency is an important kind of dirty data; before repairing inconsistency, we need to detect them first. The time complexities of the current inconsistency detection algorithms are super-linear to the size of data and not suitable for the big data. For the inconsistency detection of big data, we develop an algorithm that detects inconsistency within the one-pass scan of the data according to both the functional dependency (FD) and the conditional functional dependency (CFD) in our previous work. In this paper, we propose inconsistency detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are more expressive than FDs and CFDs. Developing the algorithm to detect the violation of DCs increases the applicability of our inconsistency detection algorithms. We compare the performance of our algorithm with the performance of implementing SQL queries in MySQL and BigQuery. The experimental results indicate the high efficiency of our algorithms.},
keywords={Big Data;Data integrity;Detection algorithms;Databases;Time complexity;Hazards;Business;Inconsistency detection;big data;one-pass algorithm;data quality;denial constraint},
doi={10.1109/ACCESS.2019.2898707},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7325916,
author={Shen, Yang and Wang, Yong and Lv, Haitao},
booktitle={2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
title={Thin cloud removal for Landsat 8 OLI data using independent component analysis},
year={2015},
volume={},
number={},
pages={921-924},
abstract={Using independent component analysis (ICA) coupled with the quality assessment (QA) band of Landsat 8, an approach for thin cloud removal in Landsat 8 operational land imager (OLI) data was developed. After the ICA transformation of the visible, near infrared, short-wavelength and cirrus bands of OLI data, cloud component was identified by the mixing matrix. Then, a cloud mask derived from the analysis of the QA band was formed such that an image pixel with and without cloud cover was delineated. The cloud component and cloud mask were used to remove the thin clouds. Thin clouds disappeared visually within the OLI data. Using another cloud-free image acquired in the previous overflight as the reference, we assessed the accuracy level of the cloud removal. Before and after the cloud removal, the spatial correlation coefficients increased from 0.69 to 0.83 in band 1, 0.75 to 0.86 in band 2, 0.81 to 0.88 in band 3, 0.87 to 0.91 in band 4, and no change in bands 5, 6, and 7 for pixels identified with cloud cover.},
keywords={Clouds;Remote sensing;Satellites;Earth;Image color analysis;Independent component analysis;Algorithm design and analysis;Independent component analysis (ICA);Landsat 8;Operational land imager (OLI) data;Quality assessment (QA) band;Thin clouds and their removal},
doi={10.1109/IGARSS.2015.7325916},
ISSN={2153-7003},
month={July},}
@ARTICLE{8714078,
author={Kosen, Ian and Huang, Can and Chen, Zhi and Zhang, Xuechen and Min, Liang and Zhou, Dao and Zhu, Lin and Liu, Yilu},
journal={IEEE Transactions on Smart Grid},
title={UPS: Unified PMU-Data Storage System to Enhance T+D PMU Data Usability},
year={2020},
volume={11},
number={1},
pages={739-748},
abstract={The emerging distribution-level phasor measurement unit (D-PMU) is expected to play an important role in enhancing distribution system observability and situational-awareness. It is a demanding yet challenging task to develop advanced D-PMU data management and analytics tools to improve D-PMU data usability and further promote D-PMU projects deployment. This paper focuses on D-PMU data processing and storage. It presents a brief review of existing D-PMU data storage systems and points out their limitations on high-performance, flexibility, and scalability. To overcome the limitations, a unified PMU-data storage system (UPS) is proposed. Specifically, a unified I/O interface between storage servers and computing jobs is developed to effectively reduce the overhead of managing various computing jobs and data analytics over multiple storage infrastructures; and PMUCache with PMUCache partitioning and PMUCache replacement algorithms are designed to support in-situ data processing and shared distributed data storage and further serve the computing jobs/queries with different quality of service (QoS) requirements. Through a series of experiments, it is demonstrated that UPS achieves high performance on fast and big data processing and storage, and efficiently increases the flexibility and scalability of PMU data management systems.},
keywords={Phasor measurement units;Uninterruptible power systems;Quality of service;Real-time systems;Data storage systems;Servers;Data analysis;Database storage;data management;phasor measurement unit (PMU);quality of service (QoS);cache partitioning;cache replacement},
doi={10.1109/TSG.2019.2916570},
ISSN={1949-3061},
month={Jan},}
@INPROCEEDINGS{6597123,
author={Ramaswamy, Lakshmish and Lawson, Victor and Gogineni, Siva Venkat},
booktitle={2013 IEEE International Congress on Big Data},
title={Towards a Quality-centric Big Data Architecture for Federated Sensor Services},
year={2013},
volume={},
number={},
pages={86-93},
abstract={As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.},
keywords={Feeds;Clouds;Wireless sensor networks;Computer architecture;Fluid flow measurement;Markup languages;Data models;Internet of Things;Federated Sensor Clouds;Data Quality;Sensor Virtualization},
doi={10.1109/BigData.Congress.2013.21},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{9407474,
author={Huang, Haiyan and Wei, Bizhong and Dai, Jian and Ke, Wenlong},
booktitle={2020 16th International Conference on Computational Intelligence and Security (CIS)},
title={Data Preprocessing Method For The Analysis Of Incomplete Data On Students In Poverty},
year={2020},
volume={},
number={},
pages={248-252},
abstract={Data mining is the focus of big data applications in various fields. Data pre-processing is a crucial step in the data mining process. With the development of the information society and the application of databases, the educational data has seen explosive growth, and the data on poor students has become informative. However, the actual student financial aid management system collects the data on poor students which generally has problems such as missing values, attributes redundancy, and noise. To solve this problem, we proposed a novel method called DPBP to preprocess data. The proposed DPBP approach consists of four stages: the preparation of data, the scoping of characteristics, the combination of characteristics, and the filtering of missing number. Firstly, we prepare the dataset by extracting data. Next, the characteristic range is limited by choosing experimental results of feature selection algorithm. Then, third stage performs feature combination to obtain the feature decomposition sets. Finally, based on accuracy and missing number, we gain the optimal dataset. Series of experiments result show that our proposed method significantly improves the data quality and stability.},
keywords={Filtering;Databases;Data integrity;Redundancy;Data preprocessing;Feature extraction;Stability analysis;data mining;data preprocessing;feature selection},
doi={10.1109/CIS52066.2020.00060},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9243611,
author={Liu, Jian},
booktitle={2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)},
title={Information reconstruction and data modeling and the historical dilemma of contemporary literature with information mining},
year={2020},
volume={},
number={},
pages={891-894},
abstract={Information reconstruction and data modeling and the historical dilemma of contemporary literature with information mining are studied in this paper. From the technology level, the novelties are summarized as follows. (1) Big data basic information, functional information, and collaborative linkage information to then establish the meta-description of big data bodies that need to be pre-processed. (2) There is a core certain difference in the amplitude value between the reconstructed data and the original data, but it is a systematic difference and will not affect the quality of the reconstructed data and imaging effect. Then, the proposed methodology is applied to the application and the performance is evaluated. The experimental results have shown that the proposed model has better robustness.},
keywords={Systematics;Scholarships;Big Data;Data models;Robustness;Data mining;Image reconstruction;Information reconstruction;data modeling;information mining;text recognition},
doi={10.1109/I-SMAC49090.2020.9243611},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8336582,
author={Sundararajan, Aditya and Sarwat, Arif I.},
booktitle={2017 International Conference on Big Data, IoT and Data Science (BID)},
title={Roadmap to prepare distribution grid-tied photovoltaic site data for performance monitoring},
year={2017},
volume={},
number={},
pages={110-115},
abstract={One of the key analytics conducted on a gridtied Photovoltaic (PV) system is the periodic monitoring of its performance. It is expected that with increased PV penetration into the distribution smart grid in the future, quality and integrity of the data required to conduct such analytics will be crucial. While data processing and management tools for smart grid in the literature use cloud, distributed file management and parallel processing, the latency and computation requirements specific to performance monitoring need more lightweight, descriptive methods. This paper provides a systematic roadmap to analyze data collected from a real distribution grid-tied 1.4MW PV power plant for completeness, consistency and integrity, with the objective of using it for performance monitoring. To ensure the data's integrity is not compromised, the distribution of processed data is compared with that of the raw data. This paper makes one of the first few attempts to provide a comprehensive approach for data scientists to clean and prepare grid-tied PV data for site-level performance monitoring.},
keywords={Monitoring;Smart grids;Correlation;Maximum likelihood estimation;Inverters;Temperature;Big Data;smart grid;PV big data;performance monitoring;data processing},
doi={10.1109/BID.2017.8336582},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9397990,
author={Labeeb, Kashshaf and Chowdhury, Kuraish Bin Quader and Riha, Rabea Basri and Abedin, Mohammad Zoynul and Yesmin, Sarmila and Khan, Mohammad Nasfikur Rahman},
booktitle={2020 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE)},
title={Pre-Processing Data In Weather Monitoring Application By Using Big Data Quality Framework},
year={2020},
volume={},
number={},
pages={284-287},
abstract={In this research, we are working with Big Data for obtaining, preparing and analyzing data-based information to make use of the data retrieved which will benefit any organization. It is a progressing part of all divisions of industry and business. All organizations in any field, for example, oil, money, fabricating hardware and so forth produce big data, which can show incredibly helpful designs to business directors to make and develop their organizations, when the information is gathered and analyzed accurately. It permits us to gather, store, and decipher immense measures of big data to produce useful outcomes. Data quality is affected by the information that is gathered to be analyzed as that data will make sure whether in the long run a specific method of conducting the ongoing process is useful or not. Consequently, the consistency of big data very important. Here, we propose that the various types of raw information should be analyzed to expand its precision in the pre-handling stage, as those pieces of information are not utilized later in the process. During investments, we break down and model the big data to decrease overhead expenses to create and add to a solid understanding of results to improve information consistency.},
keywords={Solid modeling;Oils;Organizations;Big Data;Solids;Monitoring;Meteorology;heterogeneous data;noise filter;convergence;climate change},
doi={10.1109/WIECON-ECE52138.2020.9397990},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7783229,
author={Qaiyum, Sana and Aziz, Izzatdin A. and Jaafar, Jafreezal Bin},
booktitle={2016 3rd International Conference on Computer and Information Sciences (ICCOINS)},
title={Analysis of Big Data and Quality-of-Experience in High-Density Wireless Network},
year={2016},
volume={},
number={},
pages={287-292},
abstract={The proliferation of smart devices, along with the availability of bandwidth-intensive applications are generating huge volumes of data that create challenges to IT industries. Data handling becomes more troublesome when mobile user's gather in tens and thousands of quantity at confine locations and generates Big Data. Analysis and storing of this huge, varied and complex data put great challenges to the Network Service Providers (NSP). Thus, service providers are facing problems in managing big data in dense environment and maintaining user's Quality of Experience (QoE). However, big data also provide great opportunities to NSP. The accurate analysis of big data in real-time reveals the user experience of network services which helps the service providers to take timely action to improve user QoE. Thus, this paper presents an overview of Big Data and QoE in High-Density Wireless Network (HDWN) environment.},
keywords={Big data;Mobile communication;Quality of service;Computers;Wireless networks;Streaming media;Mobile computing;Big Data;QoE;HDWN},
doi={10.1109/ICCOINS.2016.7783229},
ISSN={},
month={Aug},}
@ARTICLE{8561268,
author={Huang, Yuan and Zhao, Qiang and Zhou, Qianyu and Jiang, Wanchang},
journal={IEEE Access},
title={Air Quality Forecast Monitoring and Its Impact on Brain Health Based on Big Data and the Internet of Things},
year={2018},
volume={6},
number={},
pages={78678-78688},
abstract={Brain health quality pre-monitoring has become an urgent need, and this is a system of complex engineering. From the perspective of intelligent decision-making based on big data, the intelligent air index prediction is introduced, the popular classification algorithm is introduced, the hidden information of historical data is mined, and the brain health quality prediction is realized. The brain health quality monitoring system based on the Internet of Things is constructed, and the classification algorithm is used to realize real-time acquisition, intelligent processing of data. In order to improve the data processing speed and enhance the real-time performance of brain health quality prediction, this paper introduces cloud computing technology to accelerate data processing. In order to enable users to understand the air index, anytime and anywhere, it is also designed based on the problem of large historical data of air index and real-time data collection. The Android platform develops an air index forecast client.},
keywords={Monitoring;Big Data;Forecasting;Indexes;Cloud computing;Real-time systems;Brain modeling;Brain health quality;intelligent forecasting;air quality forecast monitoring;big data;Internet of Things},
doi={10.1109/ACCESS.2018.2885142},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8916276,
author={Wang, Haifeng and Cao, Yunpeng},
booktitle={2019 Seventh International Conference on Advanced Cloud and Big Data (CBD)},
title={Communication Optimization of Compute-Intensive Clusters Based on Software-Defined Networks},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Data transmission among nodes in compute-intensive cluster with MapReduce is a major performance bottleneck. In order to optimize network communication performance, a distributed data mapping model is constructed for big data computing jobs. Firstly, the mapping model extracts the spatio-temporal features of the input data of computing jobs, and optimizes job data layout by replacing storage locality with communication locality. Secondly, decision-space transformation is used to classify calculation jobs, and big data calculation jobs are divided into two categories: communication-intensive and non-intensive. Finally, the data communication of computing cluster is managed by software-defined network, and communication-intensive jobs are mapped to nodes of high-quality link by using global sensing capability of software-defined network to improve the communication performance of intermediate data. Experiments show that the model has better communication optimization effect for data communication-intensive jobs, and data transmission delay is reduced from 4.2% to 5.7%. Therefore, this communication optimization scheme is suitable for data center or large-scale cluster communication optimization, and adapts to various big data scheduling strategies and multiple network topologies.},
keywords={Task analysis;Optimization;Computational modeling;Data communication;Data models;Big Data;Distributed databases;compute-intensive cluster, communication optimization, MapReduce, software-defined network},
doi={10.1109/CBD.2019.00011},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8078853,
author={Ikram, Amir and Su, Qin and Fiaz, Muhammad and Khadim, Sahar},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
title={Big data in enterprise management: Transformation of traditional recruitment strategy},
year={2017},
volume={},
number={},
pages={414-419},
abstract={Technological advancements and growth of social media prompted organizations to deploy better ways of doing business. Social big data analytics provide a platform to facilitate the job seekers and job providers. The study highlights the significance of social networking sites and its role in transforming traditional recruitment strategy. The transitioning to e-recruitment signifies the importance of social networking sites as a source of recruitment mix. Primary data was collected through questionnaire from 110 employees of commercial banks operating in the city of Lahore, Pakistan. Simple random sampling technique and willingness to participate on the part of respondents generated productive findings. Cronbach's alpha, descriptive statistics, correlation matrix, and regression analysis were used as data analysis tools to endorse reliability of data collection and test hypotheses. Information quality, popularity, and security of social media found to have significant impact on the effectiveness of e-recruitment. The findings of the research suggest that big data applications not only results in better business performance management, but also enhances accessibility and reputation of the enterprise through cyber-social media. Management issues of social network big data with respect to HR managers, IT professionals and academia are also discussed.},
keywords={Recruitment;Big Data;Reliability;Facebook;Organizations;big data;social networking sites;information quality;e-recruitment},
doi={10.1109/ICBDA.2017.8078853},
ISSN={},
month={March},}
@ARTICLE{8412190,
author={El Kassabi, Hadeel T. and Serhani, Mohamed Adel and Dssouli, Rachida and Benatallah, Boualem},
journal={IEEE Access},
title={A Multi-Dimensional Trust Model for Processing Big Data Over Competing Clouds},
year={2018},
volume={6},
number={},
pages={39989-40007},
abstract={Cloud computing has emerged as a powerful paradigm for delivering data-intensive services over the Internet. Cloud computing has enabled the implementation and success of big data, a recent phenomenon handling huge data being generated from different sources. Competing clouds have made it challenging to select a cloud provider that guarantees quality of cloud service (QoCS). Also, cloud providers' claims of guaranteeing QoCS are exaggerated for marketing purposes; hence, they cannot often be trusted. Therefore, a comprehensive trust model is necessary to evaluate the QoCS prior to making any selection decision. In this paper, we propose a multi-dimensional trust model for big data workflow processing over different clouds. It evaluates the trustworthiness of cloud providers based on: the most up-to-date cloud resource capabilities, the reputation evidence measured by neighboring users, and a recorded personal history of experiences with the cloud provider. The ultimate goal is to ensure an efficient selection of trustworthiness cloud provider who eventually will guarantee high QoCS and fulfills key big data workflow requirements. Various experiments were conducted to validate our proposed model. The results show that our model captures the different components of trust, ensures high QoCS, and effectively adapts to the dynamic nature of the cloud.},
keywords={Cloud computing;Big Data;Computational modeling;Data models;Adaptation models;Task analysis;Big data;big data processing;cloud computing;cloud selection;trust model;quality of cloud services;service evaluation;community},
doi={10.1109/ACCESS.2018.2856623},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7166053,
author={Mirakhorli, Mehdi and Chen, Hong-Mei and Kazman, Rick},
booktitle={2015 IEEE/ACM 1st International Workshop on Big Data Software Engineering},
title={Mining Big Data for Detecting, Extracting and Recommending Architectural Design Concepts},
year={2015},
volume={},
number={},
pages={15-18},
abstract={An architecture recommender system can help programmers make better design choices to address their architectural quality attribute concerns while doing their daily programming tasks. We mine big data to detect and extract a large set of architectural design concepts, such as design patterns, design tactics, architecture styles, etc., to be used in our architecture recommender system called ARS. However, mining big data poses many practical challenges for system implementation. The volume, velocity and variety of our data set, like all other big data systems, requires careful planning. This first challenge is to select appropriate technologies from the large number of available products for our system implementation. Building on these technologies our greatest challenge is to custom-fit our algorithms to the parallel processing platform we have selected for ARS, to meet our performance goals.},
keywords={Computer architecture;Data mining;Recommender systems;Big data;Software engineering;Software;Algorithm design and analysis},
doi={10.1109/BIGDSE.2015.11},
ISSN={},
month={May},}
@INPROCEEDINGS{9101191,
author={Li, Dacan and Gong, Yuanyuan and Ren, Minmin and Li, Dezheng},
booktitle={2020 5th IEEE International Conference on Big Data Analytics (ICBDA)},
title={The Research and Design of Trust Business Management and Analysis System Based on Big Data Technology},
year={2020},
volume={},
number={},
pages={68-72},
abstract={China's trust companies are gradually moving towards the international market and directly participating in international competition. Facing the increasingly complex macroeconomic environment and more prudent external regulatory requirements, the trust industry has both opportunities and challenges. On the one hand, the number of high-end customers and financial needs have increased dramatically, providing many high-quality customers for trust companies. On the other hand, trust product allocation lacks flexibility. How to balance the balance between competition and customers' changing needs, give full play to the asset management and allocation functions of trust companies, improve product innovation ability, become the top priority of trust companies' operation, Informatization and risk prevention become the key to the success of the new business model and management model.In the era of information revolution, big data has become an important driving force for financial innovation. Trust companies should actively seize the new opportunities for the vigorous development of financial market, accelerate the innovation of trust functions, focus on building core capabilities, and actively plan for the development of new layout.Therefore, China's trust industry needs to actively adopt big data technology to achieve efficient management and intelligent analysis of trust business.},
keywords={Big Data;Contracts;Servers;Companies;Security;Databases;trust;big data technology;cloud platform;Precise analysis},
doi={10.1109/ICBDA49040.2020.9101191},
ISSN={},
month={May},}
@INPROCEEDINGS{6984221,
author={Bruballa, Eva and Taboada, Manel and Cabrera, Eduardo and Rexachs, Dolores and Luque, Emilio},
booktitle={2014 International Conference on Future Internet of Things and Cloud},
title={Simulation and Big Data: A Way to Discover Unusual Knowledge in Emergency Departments: Work-in-Progress Paper},
year={2014},
volume={},
number={},
pages={367-372},
abstract={Here a work in progress is reported on within research that aims to obtain knowledge about variables which may influence a hospital emergency department's performance and quality of service. Knowledge discovery will be achieved through the analysis of intensive data generated by the simulation of any possible scenario in the real system. The challenge is to provide knowledge of critical, non-usual or extreme situations. Simulation is the only way to obtain information about these kinds of situations, as it is not possible to test such scenarios in the real system. We show how simulation of the real system through advanced computing is a source of big data, as it allows rapid and massive data generation. The potential of high performance computing makes it possible to generate a very large amount of data within a reasonable time, store this data, then process and analyze it to obtain knowledge. We describe the methodology proposed for this goal, which is based on the use of the simulator as a sensor of the real system, and so as the main source of data. The application of data mining techniques will open the doors to knowledge. To verify that the proposed methodology works, we propose a case study in which the aim is to obtain knowledge from a set of data already available, obtained from the simulation of a reduced set of scenarios of the real system.},
keywords={Data models;Data mining;Computational modeling;Hospitals;Analytical models;Big data;Agent-Based Modeling and Simulation (ABMS);Big Data;Data Mining (DM);Decision Support Systems (DSS);Emergency Department (ED);Knowledge Discovery},
doi={10.1109/FiCloud.2014.65},
ISSN={},
month={Aug},}
@ARTICLE{8012376,
author={Xu, Xiaoya and Hua, Qingsong},
journal={IEEE Access},
title={Industrial Big Data Analysis in Smart Factory: Current Status and Research Strategies},
year={2017},
volume={5},
number={},
pages={17543-17551},
abstract={Under the background of cyber-physical systems and Industry 4.0, intelligent manufacturing has become an orientation and produced a revolutionary change. Compared with the traditional manufacturing environments, the intelligent manufacturing has the characteristics as highly correlated, deep integration, dynamic integration, and huge volume of data. Accordingly, it still faces various challenges. In this paper, we summarize and analyze the current research status in both domestic and aboard, including industrial big data collection, modeling of the intelligent product lines based on ontology, the predictive diagnosis based on industrial big data, group learning of product line equipment and the product line reconfiguration of intelligent manufacturing. Based on the research status and the problems, we propose the research strategies, including acquisition schemes of industrial big data under the environment of intelligent, ontology modeling and deduction method based intelligent product lines, predictive diagnostic methods on production lines based on deep neural network, deep learning among devices based on cloud supplements and 3-D selforganized reconfiguration mechanism based on the supplements of cloud. In our view, this paper will accelerate the implementation of smart factory.},
keywords={Manufacturing;Big Data;Ontologies;Production facilities;Cloud computing;Data models;Machine learning;Industrial big data;smart factory;data analysis;cyber-physical systems},
doi={10.1109/ACCESS.2017.2741105},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7371810,
author={Sowe, Sulayman K. and Zettsu, Koji},
booktitle={2015 Seventh International Conference on Knowledge and Systems Engineering (KSE)},
title={Towards an Open Data Development Model for Linking Heterogeneous Data Sources},
year={2015},
volume={},
number={},
pages={344-347},
abstract={Open data is providing opportunities as well as challenges for research and development in the field of knowledge and systems engineering. The prospects for both researchers and application developers accessing and integrating publicly available data from various sources are unprecedented. However, many open data platforms are built in such a way that it is difficult or, sometimes, impossible to extract value from the data, integrate data from various sources, transform or improve the quality of the data through continuous use and reuse, track provenance, and share research findings. In this paper we present open data development model (ODDM) that addresses some these difficulties. The model shows how various stakeholders can check-out data from an open data repository, use and reuse the data, add new functionalities, appraise, transform, and publish or check-in the improved data for other researchers to benefit. We discuss how we intend to apply the model in practice to build a new generation of open data repository for interdisciplinary research, and highlight key issues for debate and discussion.},
keywords={Data models;Sensors;Computational modeling;Government;Joining processes;Data mining;Open data;Big data;Data repositories;Open data model;Cloud computing},
doi={10.1109/KSE.2015.56},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9076536,
author={Jha, Bineet Kumar and Sivasankari, G G and Venugopal, K R},
booktitle={2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC)},
title={Fraud Detection and Prevention by using Big Data Analytics},
year={2020},
volume={},
number={},
pages={267-274},
abstract={A retail sector is a group of organization or people who sell goods or services for gaining income. Fraud is wrongful or criminal activities for the economic and personal benefits. Fraud detection is finding actual or expected fraud which takes place in an organization and in the retail market is one of the challenging aspects. Fraud is mischievous activities occur in retail sector includes shoplifting, skimming, replicating cards from skimmed data, counterfeiting, bar code or POS(Point-of-Sale) manipulation, contamination, mislabeling, substitute cheaper ingredients instead of high-quality ingredients. Fraudulent activities occur in the retail sector by both consumer and supplier. Analyzing financial crimes related to fraudulent activities is difficult where traditional data mining techniques fail to address all of them. Big data analytics is used to identify an unusual pattern to detect and prevent fraud in the retail sector. Various predictive analytics tools are used to handle massive data and their pattern.},
keywords={Organizations;Machine learning;Big Data;Tools;Data aggregation;Real-time systems;Security;Big Data Analytics;Fraud Detection;Fraud Prevention and Retail Transactions Analysis},
doi={10.1109/ICCMC48092.2020.ICCMC-00050},
ISSN={},
month={March},}
@INPROCEEDINGS{7828513,
author={Zhu, Xiaolu and Li, Jinglin and Liu, Zhihan and Yang, Fangchun},
booktitle={2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
title={A Joint Grid Segmentation Based Affinity Propagation Clustering Method for Big Data},
year={2016},
volume={},
number={},
pages={1232-1233},
abstract={Clustering is useful for discovering underlying groups and identifying interesting patterns in scientific data and engineering systems. Affinity propagation (AP) is an effective clustering algorithm which has been successfully applied to broad areas of computer science. To generate high quality clusters, AP iteratively performs information propagation on the full similarity matrix and requires excessive time to exchange messages between data points. This paper proposes a novel AP clustering method based on grid segmentation. The main ideas of our approach are: (1) to partition the data points into multiple non-overlapping sub-sets to simplify representation of huge data points into smaller sub-sets, (2) to construct sparse similarity matrix to decrease the unnecessary message exchanges. Experimental evaluations on large-scale real-world datasets demonstrate our proposed method has superior performance in effectiveness and efficiency.},
keywords={Clustering methods;Clustering algorithms;Big data;Conferences;Sparse matrices;Algorithm design and analysis;Public transportation;affinity propagation;clustering;grid segmentation},
doi={10.1109/HPCC-SmartCity-DSS.2016.0172},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8603594,
author={Farooqi, M. Mashab and Ali Khattak, Hasan and Imran, Muhammad},
booktitle={2018 14th International Conference on Emerging Technologies (ICET)},
title={Data Quality Techniques in the Internet of Things: Random Forest Regression},
year={2018},
volume={},
number={},
pages={1-4},
abstract={Internet of Things (IoTs) is one of the most promising fields in computer science. It consists of physical devices, automobiles, home appliances, embedded hardware, sensors and actuators which empowers these objects to interface and share information with other devices over the network. The data gathered from these devices is used to make intelligent decisions. If the data quality is poor, decisions are likely to be flawed. A little work has been carried out regarding data quality in the Internet of Things, but there is no scheme which is experimentally proved. In this paper we will identify data quality challenges in the Internet of Things domain and propose a model which ensure data quality standards provided by ISO 8000. We evaluated our model on the weather dataset and used the random forest prediction method to calculate the accuracy of our data. Results show that when compared with the baseline model the proposed system improves accuracy by 38.88%.},
keywords={Data integrity;Data models;Internet of Things;Cleaning;Meteorology;Sensors;Predictive models;data quality;internet of things;big data;machine learning},
doi={10.1109/ICET.2018.8603594},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8386522,
author={Lu, Ming and Zhou, Xu},
booktitle={2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={A big data on private cloud agile provisioning framework based on OpenStack},
year={2018},
volume={},
number={},
pages={253-260},
abstract={On the bases of the OpenStack private cloud delivery big data platform, numerous entities yearn for attaining agile and standardized big data delivery platform, reclaiming the resources, managing the total cost of ownership (TCO) and adapting to multiple big data open source or commercial off-the-shelf (COTS) solutions. Nevertheless, as regards the big data platform running on cloud computing, the big data platform is disintegrated from the cloud computing system by virtual machines since neither being based on OpenStack private cloud nor on big data platform can achieve end-to-end resource delivery, together with ensuring that it is quite convenient for the long-term operations. Accordingly, establishing an across framework between private cloud and big data platform is quite essential. The big data on cloud agile provision framework could realize fast resource delivery based on predefined orchestration template of private cloud, operating system, big data platform, monitor, inspection system, etc. Through the deployment of this framework, it is capable of attaining the delivery of agile, low cost, standardized and high adaptability the big data on cloud, as well as the high-quality operation of the big data on cloud with the help of integration configuration management database (CMDB) with the automatic inspection system.},
keywords={Big Data;Cloud computing;Virtual machining;Technological innovation;Testing;Inspection;cloud computing;agile resource provisioning;big data platform orchestration;inspection and rule engine},
doi={10.1109/ICCCBDA.2018.8386522},
ISSN={},
month={April},}
@ARTICLE{8395004,
author={Singh, Amritpal and Garg, Sahil and Kaur, Kuljeet and Batra, Shalini and Kumar, Neeraj and Choo, Kim-Kwang Raymond},
journal={IEEE Transactions on Industrial Informatics},
title={Fuzzy-Folded Bloom Filter-as-a-Service for Big Data Storage in the Cloud},
year={2019},
volume={15},
number={4},
pages={2338-2348},
abstract={With the ongoing trend of smart and Internet-connected objects being deployed across a broad range of applications, there is also a corresponding increase in the amount of data movement across different geographical regions. This, in turn, poses a number of challenges with respect to big data storage across multiple locations, including cloud computing platform. For example, the underlying distributed file system has a large number of directories and files in the form of gigantic trees, which are difficult to parse in polynomial time. Moreover, with the exponential increase of big data streams (i.e., unbounded sets of continuous data flows), challenges associated with indexing and membership queries are compounded. The capability to process such significant amount of data with high accuracy can have significant impact on decision-making and formulation of business and risk-related strategies, particularly in our current Industrial Internet of Things environment (IIoT). However, existing storage solutions are deterministic in nature. In other words, they tend to consume considerable memory and CPU time to yield accurate results. This necessitates the design of efficient quality of service-aware IIoT applications that are able to deal with the challenges of data storage and retrieval in the cloud computing environment. In this paper, we present an effective space-effective strategy for massive data storage using bloom filter (BF). Specifically, in the proposed scheme, the standard BF is extended to incorporate fuzzy-enabled folding approach, hereafter referred to as fuzzy folded BF (FFBF). In FFBF, fuzzy operations are used to accommodate the hashed data of one BF into another to reduce storage requirements. Evaluations on UCI ML AReM and Facebook datasets demonstrate the efficacy of FFBF, in terms of dealing with approximately 1.9 times more data as compared to using the standard BF. This is also achieved without affecting the false positive rate and query time.},
keywords={Cloud computing;Memory;Big Data;Sensors;Complexity theory;Internet of Things;Distributed databases;Big data storage;bloom filter (BF);cloud storage;double hashing;probabilistic data structures (PDS)},
doi={10.1109/TII.2018.2850053},
ISSN={1941-0050},
month={April},}
@INPROCEEDINGS{9523914,
author={Shufang, Xiao and Ping, Xu},
booktitle={2020 International Conference on Robots & Intelligent System (ICRIS)},
title={Construction Strategy of University Intelligent Art Education Ecosystem based on Big Data},
year={2020},
volume={},
number={},
pages={253-256},
abstract={In order to overcome the problems existing in the development of smart art education in colleges and universities, and to improve the informatization level of art education in colleges and universities, this paper puts forward a construction strategy of the ecosystem of smart art education in colleges and universities based on big data. Based on this, the system realizes the combination of art education and wisdom education from multiple dimensions, which are teaching activities, teaching concepts and teaching subjects. In addition, the system can also give full play to the interaction and immersion of big data technology and artificial intelligence technology, comprehensively improve the quality of art education, and build a perfect education ecological environment. The results show that the construction strategy can strengthen the deep integration of art education and information science and technology, and improve the information level of art education in colleges and universities.},
keywords={Information science;Art;Education;Ecosystems;Big Data;Intelligent systems;Information technology;Big data;Educational resources;Art education in colleges and universities;Smart education},
doi={10.1109/ICRIS52159.2020.00070},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8592252,
author={Zheng, Z.Y. and Li, Z. and Yu, S.F. and Ke, D.F. and Lao, Z.J. and Lin, J.Q. and Chen, J.J.},
booktitle={2018 China International Conference on Electricity Distribution (CICED)},
title={A New Capacity Inspection Method for Distribution Transformer based on Big Data},
year={2018},
volume={},
number={},
pages={38-41},
abstract={The quality of distribution network equipment is the basis of the construction of intrinsically safe distribution system, and the sample inspection of delivered equipment is an important means of the quality control of distribution network equipment. Centering on the problem that distribution transformer capacity is hard to be inspected accurately at present, this paper proposes a new capacity inspection method for distribution transformer based on big data. This paper accumulates a large number of test data of nearly one thousand distribution transformers from inspection work, and analyzes the distribution of volume, weight, DC resistance, no-load loss, load loss and short-circuit impedance of distribution transformer in different capacity levels through the statistical method, and finally makes a capacity assessment process. This method is simple, practical and reliable, and it can be spread and applied in production practice.},
keywords={Inspection;Resistance;Impedance;Oil insulation;Big Data;Statistical analysis;Safety;Distribution transformer;capacity inspection;big date;sample inspection of delivered equipment},
doi={10.1109/CICED.2018.8592252},
ISSN={2161-749X},
month={Sep.},}
@INPROCEEDINGS{7836014,
author={Luna, Gladys Alicia Tenesaca and Chicaiza, Janneth and Belén Mora Arciniegas, María and Torres, Juan Pablo Ureña and Faggioni, Verónica Alexandra Segarra and Ludeña, Marlon Santiago Viñan},
booktitle={2016 35th International Conference of the Chilean Computer Science Society (SCCC)},
title={Contribution of big data in E-leaming. A methodology to process academic data from heterogeneous sources},
year={2016},
volume={},
number={},
pages={1-12},
abstract={Big Data covers a wide spectrum of technologies, which tends to support the processing of big amounts of heterogeneous data. The paper identifies the powerful benefits and the application areas of Big Data in the on-line education context. Considering the boom of academic services on-line, and the free access to the educative content, a great amount of data is being generated in the formal educational field as well as in less formal contexts. In this sense, Big Data can help stakeholders, involved in education decision making, to reach the objective of improving the quality of education and the learning outcomes. In this paper, a methodology is proposed to process big amounts of data coming from the educational field. The current study ends with a specific case study where the data of a well-known Ecuadorian institution that has more than 80 branches is analyzed.},
keywords={Big data;Sparks;Software;Education;Context;Google;Data models;Big Data;Hadoop;Data Web;Education;Dataset;UTPL},
doi={10.1109/SCCC.2016.7836014},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8538098,
author={Liu, Yang and Lingling, Xv and Jiang, Peng and Yutong, Li and Mingtai, Shi and Haosong, Li and Zhongping, Xv and Jin, Li and Shuai, Wang and Dongliang, Hu and Jia, Wu and Dan, Su},
booktitle={2018 International Symposium in Sensing and Instrumentation in IoT Era (ISSI)},
title={Study of Data Integration Architecture for WideArea Distributed Power Quality of Power Grid},
year={2018},
volume={},
number={},
pages={1-6},
abstract={With the increasing degree of interconnection between regional power grids and the diversification of power quality interference sources, the problem of power quality has become a complex problem across provinces and regions. It is necessary to provide an analysis method for solving complex power quality problems between regions by carrying out analysis technology research based on the monitoring data of whole network power quality and exploring the correlation of interregional power quality problems. A new wide-area distributed power quality data fusion architecture is proposed in this paper. It solves the data source problem of the big data analysis of the power quality, and realizes the sharing of the data and information of the whole network power quality, and lays the theoretical foundation for the depth application of the power quality data by researching and designing the architecture aiming at multi-source, heterogeneous and distributed data integration technology and wide area distributed data storage technology.},
keywords={Power quality;Data integration;Distributed databases;Monitoring;Companies;Web services;Real-time systems;Power Quality Wide-Area Distribution Data Integration},
doi={10.1109/ISSI.2018.8538098},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7577067,
author={Al Najada, Hamzah and Mahgoub, Imad},
booktitle={2016 International Wireless Communications and Mobile Computing Conference (IWCMC)},
title={Big vehicular traffic Data mining: Towards accident and congestion prevention},
year={2016},
volume={},
number={},
pages={256-261},
abstract={In 2013, 32,719 people died in traffic crashes in the USA. Almost 90 people on average lose their lives every day and more than 250 are injured every hour. Road safety could be enhanced by decreasing the traffic crashes. Traffic crashes cause traffic congestion as well, which has become unbearable, especially in mega-cities In addition, direct and indirect loss from traffic congestion only is over $124 billion. The existence of the Big Data of traffic crashes, as well as the availability of Big Data analytics tools can help us gain useful insights to enhance road safety and decrease traffic crashes. In this paper we use H2O and WEKA mining tools. We apply the feature selection techniques to find the most important predictors. In addition, we tackle the problem of class imbalance by employing bagging and using different quality measures. Furthermore, we evaluate the performance of five classifiers to: (1) Conduct Big Data analysis on a big traffic accidents dataset of 146322 examples, find useful insight and patterns from the data, and forecast possible accidents in advance (2) Conduct Big Data analysis on a big vehicular casualties dataset of 194477 examples, to study the driver's behavior on the road. From the driver's behavior mining we can predict the driver age, sex as well as the accident severity. The aforementioned analyses, can be used by decision makers and practitioners to develop new traffic rules and policies, in order to prevent accidents, and increase roadway safety.},
keywords={Accidents;Data mining;Big data;Vehicles;Roads;Computer crashes;Intelligent Transportation System (ITS);Vehicular Ad-hoc Network (VANET);Traffic Engineering;Big Data;Machine Learning and Data Mining},
doi={10.1109/IWCMC.2016.7577067},
ISSN={2376-6506},
month={Sep.},}
@ARTICLE{8057859,
author={Yu, Zhibin and Xiong, Wen and Eeckhout, Lieven and Bei, Zhendong and Mendelson, Avi and Xu, Chengzhong},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={MIA: Metric Importance Analysis for Big Data Workload Characterization},
year={2018},
volume={29},
number={6},
pages={1371-1384},
abstract={Data analytics is at the foundation of both high-quality products and services in modern economies and societies. Big data workloads run on complex large-scale computing clusters, which implies significant challenges for deeply understanding and characterizing overall system performance. In general, performance is affected by many factors at multiple layers in the system stack, hence it is challenging to identify the key metrics when understanding big data workload performance. In this paper, we propose a novel workload characterization methodology using ensemble learning, called Metric Importance Analysis (MIA), to quantify the respective importance of workload metrics. By focusing on the most important metrics, MIA reduces the complexity of the analysis without losing information. Moreover, we develop the MIA-based Kiviat Plot (MKP) and Benchmark Similarity Matrix (BSM) which provide more insightful information than the traditional linkage clustering based dendrogram to visualize program behavior (dis)similarity. To demonstrate the applicability of MIA, we use it to characterize three big data benchmark suites: HiBench, CloudRank-D and SZTS. The results show that MIA is able to characterize complex big data workloads in a simple, intuitive manner, and reveal interesting insights. Moreover, through a case study, we demonstrate that tuning the configuration parameters related to the important metrics found by MIA results in higher performance improvements than through tuning the parameters related to the less important ones.},
keywords={Measurement;Big Data;Benchmark testing;Hardware;Software;Support vector machines;Big data;benchmarking;workload characterization;performance measurement;MapReduce/hadoop},
doi={10.1109/TPDS.2017.2758781},
ISSN={1558-2183},
month={June},}
@INPROCEEDINGS{7364062,
author={Berti-Équille, Laure},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Data veracity estimation with ensembling truth discovery methods},
year={2015},
volume={},
number={},
pages={2628-2636},
abstract={Estimation of data veracity is recognized as one of the grand challenges of big data. Typically, the goal of truth discovery is to determine the veracity of multi-source, conflicting data and return, as outputs, a veracity label and a confidence score for each data value, along with the trustworthiness score of each source claiming it. Although a plethora of methods has been proposed, it is unlikely a technique dominates all others across all data sets. Furthermore, the performance evaluation of the methods entirely depends on the availability of labeled ground truth data (i.e., data whose veracity has been manually checked). In the context of Big Data, acquiring the complete ground truth data is out-of-reach. In this paper, we propose an ensembling method that mitigates the two problems of method selection and ground truth data sparsity. Our approach combines the results of a set of truth discovery methods and preliminary experiments suggest that it improves the quality performance over the single methods when samples of ground truth data are used.},
keywords={Big data;Estimation;Google;Measurement;Conferences;Electronic mail;Context},
doi={10.1109/BigData.2015.7364062},
ISSN={},
month={Oct},}
@ARTICLE{8002577,
author={Chi, Mingmin and Sun, Zhongyi and Qin, Yiqing and Shen, Jinsheng and Benediktsson, Jón Atli},
journal={Proceedings of the IEEE},
title={A Novel Methodology to Label Urban Remote Sensing Images Based on Location-Based Social Media Photos},
year={2017},
volume={105},
number={10},
pages={1926-1936},
abstract={With the rapid development of the internet and popularization of intelligent mobile devices, social media is evolving fast and contains rich spatial information, such as geolocated posts, tweets, photos, video, and audio. Those location-based social media data have offered new opportunities for hazards and disaster identification or tracking, recommendations for locations, friends or tags, pay-per-click advertising, etc. Meanwhile, a massive amount of remote sensing (RS) data can be easily acquired in both high temporal and spatial resolution with a multiple satellite system, if RS maps can be provided, to possibly enable the monitoring of our location-based living environments with some devices like charge-coupled device (CCD) cameras but on a much larger scale. To generate the classification maps, usually, labeled RS image pixels should be provided by RS experts to train a classification system. Traditionally, labeled samples are obtained according to ground surveys, image photo interpretation or a combination of the aforementioned strategies. All the strategies should be taken care of by domain experts, in a means which is costly, time consuming, and sometimes of a low quality due to reasons such as photo interpretation based on RS images only. These practices and constraints make it more challenging to classify land-cover RS images using big RS data. In this paper, a new methodology is proposed to classify urban RS images by exploiting the semantics of location-based social media photos (SMPs). To validate the effectiveness of this methodology, an automatic classification system is developed based on RS images as well as SMPs via big data analysis techniques including active learning, crowdsourcing, shallow machine learning, and deep learning. As the labels of RS training data are given by ordinary people with a crowdsourcing technique, the developed system is named Crowd4RS. The quantitative and qualitative experiments confirm the effectiveness of the proposed Crowd4RS system as well as the proposed methodology for automatically generating RS image maps in terms of classification results based on big RS data made up of multispectral RS images in a high spatial resolution and a large amount of photos from social media sites, such as Flickr and Panoramio.},
keywords={Social network services;Crowdsourcing;Big data;Spatial resolution;Remote sensing;Satellites;Urban areas;Social network services;Mobile communication;Machine learning;Big data;crowdsourcing;deep learning;remote sensing;social media},
doi={10.1109/JPROC.2017.2730585},
ISSN={1558-2256},
month={Oct},}
@INPROCEEDINGS{6602860,
author={Weiqiang Sun and Fengqin Li and Wei Guo and Yaohui Jin and Weisheng Hu},
booktitle={2013 15th International Conference on Transparent Optical Networks (ICTON)},
title={Store, schedule and switch - A new data delivery model in the big data era},
year={2013},
volume={},
number={},
pages={1-4},
abstract={The big data era is posing unprecedented challenges on the existing network infrastructure. In today's networks, data are transferred across the network as a combination of a series of packets, delivered one by one, without considering the data in their entirety with respective service level requirements. The so called elephant data, which may be less sensitive to transfer delay, compete precious network resources with mice data, in most cases from interactive and delay sensitive applications. Consequently, the Quality of Service (QoS) of interactive applications is hard to provision, and the utility of network is low. We propose a new data transfer model to complement the existing per-packet forwarding paradigm. In the new data transfer model, a service level requirement is assigned (by the data source) to each big data transfer request. Instead of transferring these data on per-packet bases immediately upon entering the network, the network stores the data until it find necessary, or enough network resource is available for that transfer. The scheduled data delivery is realized through the use of dynamic circuit switching. We also present some preliminary simulation results of SSS networks.},
keywords={Information management;Data handling;Data storage systems;Optical switches;Delays;Data models;Big Data;OpenFlow;Data Delivery;Network Resource Management;Dynamic Circuit Switching},
doi={10.1109/ICTON.2013.6602860},
ISSN={2161-2064},
month={June},}
@INPROCEEDINGS{7301625,
author={Khan, Abdul Rauf and Schiøler, Henrik and Knudsen, Torben and Kulahci, Murat},
booktitle={2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA)},
title={Statistical data mining for efficient quality control in manufacturing},
year={2015},
volume={},
number={},
pages={1-4},
abstract={Extensive use of machines, flexible/re-configurable manufacturing and transition towards the fully automated factories call for intelligent use of information recorded during the manufacturing process. Modern manufacturing processes produce Terabytes of information during different stages of the process e.g sensor measurements, machine readings etc, and the major contributor of these big data sets are different quality control processes. In this article we will present methodology to extract valuable insight from manufacturing data. The proposed methodology is based on comparison of probabilities and extension of likelihood principles in statistics as a performance function for Genetic Algorithm.},
keywords={Quality control;Manufacturing processes;Genetic algorithms;Data mining;Process control},
doi={10.1109/ETFA.2015.7301625},
ISSN={1946-0759},
month={Sep.},}
@INPROCEEDINGS{8258503,
author={Lieberman, Joshua and Leidner, Alan and Percivall, George and Rönsdorf, Carsten},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Using big data analytics and IoT principles to keep an eye on underground infrastructure},
year={2017},
volume={},
number={},
pages={4592-4601},
abstract={A Concept Development Study by the Open Geospatial Consortium (OGC) has highlighted the importance of high-quality feature data for underground urban infrastructure (UGI). Analysis of large survey datasets, including both visual and non-visual methods, is essential for creating and maintaining UGI geodata. Connecting hidden features with diverse, high-velocity sensing streams and realistic predictive models that effectively characterize them is key to lower construction costs, efficient infrastructure operation, sound disaster preparedness, and new smart city services. IoT principles that combine OGC geodata and Sensor Web observation standards may offer the best chance for working towards functional “digital twins” of such hidden infrastructure that are both cost effective and scalable with the increasing complexity and instrumentation of the underground built environment. Technical and policy challenges remain, however, before this can be achieved.},
keywords={Maintenance engineering;Data models;Monitoring;Smart cities;infrastructure;underground;utility;network;geospatial;sensing;iot},
doi={10.1109/BigData.2017.8258503},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8256208,
author={Xian, Xiaochen and Wang, Andi and Liu, Kaibo},
booktitle={2017 13th IEEE Conference on Automation Science and Engineering (CASE)},
title={A nonparametric adaptive sampling strategy for online monitoring of big data streams},
year={2017},
volume={},
number={},
pages={844-846},
abstract={With the rapid development of sensor techniques, we often face the challenges of monitoring big data streams in modern quality control, which consist of massive series of real-time, continuously and sequentially ordered observations. For example, in manufacturing industries, hundreds or thousands of variables are observed during online production for quality insurance. Also, smart grid infrastructure needs to simultaneously monitor massive access points for intrusion and threat detection. As another example, an image sensing device continuously collects high-resolution images at high frequency for video surveillance and object movement tracking. Ideally, in those applications, it is preferable to detect assignable causes as early as possible, while maintaining a prespecified in-control Average Run Length (ARL).},
keywords={Monitoring;Big Data;Process control;Automation;Computer aided software engineering;Real-time systems;Production;Distribution-free;Multivariate CUSUM Procedure;Partial Observations;Process Change Detection;Statistical Process Control},
doi={10.1109/COASE.2017.8256208},
ISSN={2161-8089},
month={Aug},}
@INPROCEEDINGS{7029280,
author={Earley, Seth},
booktitle={2014 IT Professional Conference},
title={Presentation 1. Information governance in the age of big data},
year={2014},
volume={},
number={},
pages={1-3},
abstract={Organizations have understood the value of their structured data — mostly financial transactions — since the first mainframes were developed in the 40's. Data quality issues have always been a challenge and the increasing numbers of applications consuming and producing structured transactional data has grown exponentially. Unstructured information has been given less significance and strategic importance and therefore fewer resources and less attention on the part of leadership. All of that has changed and is changing faster than anyone imagined. Unstructured content is what humans produce. They create the documents: strategies, proposals, support documents, marketing content, white papers, engineering specifications, etc. that form the intelligence and core knowledge capital of the enterprise. Many organizations have left business units to fend for themselves and “go figure it out” with little guidance or support. These has led to terabytes of content that people cannot find their way through and that leaves the organization open to risks, liabilities and costs of e discovery. According to the Minnesota Journal of Law, Science & Technology, a gig of data costs $30,000 in e discovery costs. The cost of storage is ten cents. The problem is that the enterprise does not understand the hidden costs of not making data accessible and usable — lost time, lost IP, inefficiency, poor customer service that can lead to lost customers, slower growth, etc. As newer collaboration technologies are deployed, they expose the bad habits and sins of the past. Deploying a new search engine shows that the content is not curated. Standing up a new content management application like SharePoint reveals the haphazard shanty town of an information architecture with inconsistencies in models, terminology and applications. Today's landscape of marketing and customer experience technologies is complex and interconnected and requires those upstream knowledge processes that produce unstructured content as the fuel. Customer experience entails everything that happens before you purchase (marketing, education and outreach), when you purchase (e commerce with product content and data), and after you purchase (self-service systems and knowledge bases that support the call center). This is the customer lifecycle and at each step in the process systems and tools need to be harmonized as they gather information about the users using attribute models that are consistent and that serve the business and the customer. They take content and data as input and then output more data. One applications exhaust is another applications fuel. Organizations are also purchasing data streams to enrich their internal information sources. Social media is an enormous virtually untapped reservoir of data about customers and what they think about organizations. This can be mined for sentiment and to gauge marketing effectiveness. Increasingly, much of this is being placed into the hands of the marketing organization. In fact, a study by Gartner Group said that by 2017 the CMO will spend more money on IT than the CIO. What all of this means is that information, content and data governance need to be considered as part of a whole and not as separate initiatives. Elements of good information governance include: • Deployment and Operationalization • Alignment with User Needs • Business Value • Buy-In and Change Management • Sponsorship and Accountability Leads to the following outcomes: • Manages conflicts in business priorities (between initiatives, business units, drivers, etc) • Allows for ongoing input from various stakeholders and constituencies in order to evolve capabilities with the needs of the business • Prioritizes efforts and allocation of resources • Assigns roles and responsibilities with accountability to critical functions • Takes into consideration various levels of maturity in the organization — no one size fits all • Ensures that investments in systems, processes and tools are providing sufficient return to the business • Balances centralized standards with decentralized decision making • Aligns incentives to use a system with business goals This session will review governance concepts, discuss how they apply to various types of data and content and provide a framework for developing governance processes and structures.},
keywords={Organizations;Standards organizations;Information architecture;Fuels;Information management;Big data},
doi={10.1109/ITPRO.2014.7029280},
ISSN={},
month={May},}
@INPROCEEDINGS{7207234,
author={Yip, Benjamin and Hirai, Hoyee W. and Kuo, Yong-Hong and Meng, Helen M. and Wong, Samuel and Tsoi, Kelvin K.F.},
booktitle={2015 IEEE International Congress on Big Data},
title={Blood Pressure Management with Data Capturing in the Cloud among Hypertensive Patients: A Monitoring Platform for Hypertensive Patients},
year={2015},
volume={},
number={},
pages={305-308},
abstract={Hypertension is a significant modifiable risk factor for cardiovascular and kidney disease, and blood pressure (BP) control is a very important step for cardiovascular risk management. Recently, home telemonitoring BP has been suggested as an effective tool for BP control and been commonly used in Western countries. Application of technology for healthcare management becomes a trend. Health data is usually longitudinal and voluminous, an effective data management would improve the quality of healthcare service. In order to deal with the volume, variety and velocity of medical data, cloud technology has opened a new horizon, especially data for medical research. Boosting the current home telemonitoring BP system with an automatic data capturing cloud technology along with healthcare provider alert function would be a pioneer. In this study, a cloud-connected personal-based BP meter will be transformed to a research-based BP data capturing cloud platform and will observe daily use of BP measurement and upload data to the cloud through a USB hub and internet-connected personal computer. All personal identity can be decoded and a study identity number will be assigned to each user for data privacy protection. The cloud platform enables easy access for different parties from anywhere, high speed performance, strong infrastructure support and vigorous data analysis power.},
keywords={Hypertension;Blood pressure;Biomedical monitoring;Monitoring;Universal Serial Bus;Market research;Cloud Platform;Hypertension;Blood Pressure;Big Data Processing},
doi={10.1109/BigDataCongress.2015.50},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{8332761,
author={Hou, Zhijie},
booktitle={2018 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={Data Mining Method and Empirical Research for Extension Architecture Design},
year={2018},
volume={},
number={},
pages={275-278},
abstract={For extension data mining of architecture design, the process and method to convert extension architectural design data to structural data with high quality are discussed in this article. In special knowledge mining of extension architectural design data, the problems of architectural space design, form design, extension knowledge mining for structural design are studied according to cross-industry standard process for data mining(CRISP-DM). They are integrated with operations of extenics, geometry, statistical theory to provide scientific and reasonable design. The results of case study show that our scheme can effectively convert extension architectural design data to computable structural data with unified format and high reliability.},
keywords={Conferences;Transportation;Big Data;Smart cities;data mining;extension architecture;element;quantification;feature},
doi={10.1109/ICITBS.2018.00077},
ISSN={},
month={Jan},}
@ARTICLE{8840830,
author={Nazir, Shah and Nawaz, Muhammad and Adnan, Awais and Shahzad, Sara and Asadi, Shahla},
journal={IEEE Access},
title={Big Data Features, Applications, and Analytics in Cardiology—A Systematic Literature Review},
year={2019},
volume={7},
number={},
pages={143742-143771},
abstract={In today's digital world the information surges with the widespread use of the internet and global communication systems. Healthcare systems are also facing digital transformations with the enhancement in the utilization of healthcare information systems, electronic records in medical, wearable, smart devices, handheld devices, and so on. A bulk of data is produced from these digital transformations. The recent increase in medical big data and the development of computational techniques in the field of cardiology enables researchers and practitioners to extract and visualize medical big data in a new spectrum. The role of medical big data in cardiology becomes a challenging task. Early decision making in cardiac healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Therefore, to facilitate this process a detailed report of the existing literature will be feasible to help the doctors and practitioners in decision making for the purpose of identifying and treating cardiac diseases. This detailed study will summarize results from the existing literature on big data in the field cardiac disease. This research uses the systematic literature protocol as presented by Kitchenham et al. The data was collected from the published materials from 2008 to 2018 as conference or journal publications, books, magazines and other online sources. 190 papers were included relying on the defined inclusion, exclusion, and checking the quality criteria. The current study helped to identify medical big data features, the application of medical big data, and the analytics of the big data in cardiology. The results of the proposed research shows that several studies exist that are associated to medical big data specifically to cardiology. This research summarizes and organizes the existing literature based on the defined keywords and research questions. The analysis will help doctors to make more authentic decisions, which ultimately will help to use the study as evidence for treating patients with heart related diseases.},
keywords={Big Data;Cardiology;Diseases;Protocols;Data visualization;Bibliographies;Big data;big data features;analytics in cardiology;healthcare;systematic literature review (SLR)},
doi={10.1109/ACCESS.2019.2941898},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7450804,
author={Xu, Donna and Wu, Dongyao and Xu, Xiwei and Zhu, Liming and Bass, Len},
booktitle={2015 11th International ACM SIGSOFT Conference on Quality of Software Architectures (QoSA)},
title={Making real time data analytics available as a service},
year={2015},
volume={},
number={},
pages={73-82},
abstract={Conducting (big) data analytics in an organization is not just about using a processing framework (e.g. Hadoop/Spark) to learn a model from data currently in a single file system (e.g. HDFS). We frequently need to pipeline real time data from other systems into the processing framework, and continually update the learned model. The processing frameworks need to be easily invokable for different purposes to produce different models. The model and the subsequent model updates need to be integrated with a product that may require a real time prediction using the latest trained model. All these need to be shared among different teams in the organization for different data analytics purposes. In this paper, we propose a real time data-analytics-as-service architecture that uses RESTful web services to wrap and integrate data services, dynamic model training services (supported by big data processing framework), prediction services and the product that uses the models. We discuss the challenges in wrapping big data processing frameworks as services and other architecturally significant factors that affect system reliability, real time performance and prediction accuracy. We evaluate our architecture using a log-driven system operation anomaly detection system where staleness of data used in model training, speed of model update and prediction are critical requirements.},
keywords={Real-time systems;Data models;Big data;Predictive models;Training;Data analysis;Computer architecture;Software Architecture;Big Data Processing;Data Analytics Service},
doi={10.1145/2737182.2737186},
ISSN={},
month={May},}
@INPROCEEDINGS{9357200,
author={Ezzine, Imane and Benhlima, Laila},
booktitle={2020 6th IEEE Congress on Information Science and Technology (CiSt)},
title={Technology against COVID-19 A Blockchain-based framework for Data Quality},
year={2020},
volume={},
number={},
pages={84-89},
abstract={The effects of COVID-19 have quickly spread around the world, testing the limits of the population and the public health sector. High demand on medical services are offset by disruptions in daily operations as hospitals struggle to function in the face of overcapacity, understaffing and information gaps. Faced with these problems, new technologies are being deployed to fight this pandemic and help medical staff governments to reduce its spread. Among these technologies, we find blockchains and Big Data which have been used in tracking, prediction applications and others. However, despite the help that these new technologies have provided, they remain limited if the data with which they are fed are not of good quality. In this paper, we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic and some data quality issues that still present challenges to decision making. Finally we present a general Blockchain-based framework for data governance that aims to ensure a high level of data trust, security, and privacy.},
keywords={COVID-19;Pandemics;Data integrity;Blockchain;Big Data;Statistics;Testing;Covid-19;Blockchain;Big Data;Data Quality;data governance},
doi={10.1109/CiSt49399.2021.9357200},
ISSN={2327-1884},
month={June},}
@INPROCEEDINGS{7840920,
author={Liu, Ruilin and Yang, Kai and Sun, Yanjia and Quan, Tao and Yang, Jin},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Spark-based rare association rule mining for big datasets},
year={2016},
volume={},
number={},
pages={2734-2739},
abstract={Nowadays, the quality of wireless network becomes critical for the network service providers (NSP). A poor performed network may lead to customer complaints, even loss of revenue from user churns. To ensure the quality of service, the key quality indicators (KQI) which reflect the quality of specific use cases have been collected alongside the network performance counters (NPC) for network performance analysis. To start, the NSP mines the network performance data to discover KQI anomalies which may cause poor user experience. If there is any KQI anomaly has been detected, the NSP investigates the associated NPCs to identify the possible root causes. Since the number of use cases increases dramatically and the volume of collected network performance data grows tremendously everyday, the wireless network anomaly root cause analysis becomes extremely challenging. How to efficiently discover the relationship between NPCs and KQI outliers becomes the key to identify the root causes of anomalies in the wireless network. To solve this problem, in this paper, we propose an efficient rare association rule mining algorithm called Spark-based Rare Association Rule Mining (SRAM) which leverages not only the efficiency of FP-growth algorithm but also the powerful big data processing mechanism of Spark platform. We have implemented our algorithm on the Spark platform and tested with various of data sets. The result shows our method can efficiently mine rare association rules from big volume of data.},
keywords={Big data;Conferences;Dairy products},
doi={10.1109/BigData.2016.7840920},
ISSN={},
month={Dec},}
@ARTICLE{8368242,
author={Li, Yanhua and Liu, Guanxiong and Zhang, Zhi-Li and Luo, Jun and Zhang, Fan},
journal={IEEE Transactions on Big Data},
title={CityLines: Designing Hybrid Hub-and-Spoke Transit System with Urban Big Data},
year={2019},
volume={5},
number={4},
pages={576-587},
abstract={Rapid urbanization has posed significant burden on urban transportation infrastructures. In today's cities, both private and public transits have clear limitations to fulfill passengers' needs for quality of experience (QoE): Public transits operate along fixed routes with long wait time and total transit time; Private transits, such as taxis, private shuttles and ride-hailing services, provide point-to-point transits with high trip fare. In this paper, we propose CityLines, a transformative urban transit system, employing hybrid hub-and-spoke transit model with shared shuttles. Analogous to Airlines services, the proposed CityLines system routes urban trips among spokes through a few hubs or direct paths, with travel time as short as private transits and fare as low as public transits. CityLines allows both point-to-point connection to improve the passenger QoE, and hub-and-spoke connection to reduce the system operation cost. To evaluate the performance of CityLines, we conduct extensive data-driven experiments using one-month real-world trip demand data (from taxis, buses and subway trains) collected from Shenzhen, China. The results demonstrate that CityLines reduces 12.5-44 percent average travel time, and aggregates 8.5-32.6 percent more trips with ride-sharing over other implementation baselines.},
keywords={Public transportation;Quality of experience;Urban areas;Urban planning;Data analysis;Big Data;Hub-and-spoke network;urban computing;spatio-temporal data analytics},
doi={10.1109/TBDATA.2018.2840222},
ISSN={2332-7790},
month={Dec},}
@INPROCEEDINGS{9378228,
author={Suárez-Otero, Pablo and Mior, Michael J. and José Suárez-Cabal, Maria and Tuya, Javier},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Maintaining NoSQL Database Quality During Conceptual Model Evolution},
year={2020},
volume={},
number={},
pages={2043-2048},
abstract={Database schemas evolve over time to satisfy changing application requirements. If this evolution is not performed correctly, some quality attributes are at risk such as data integrity, functional correctness, or maintainability. To help developer teams in the design of database schemas, several design methodologies for NoSQL databases have proposed to use conceptual models during this process. The use of an explicit conceptual model can also help developers in the tasks of schema evolution. In this work-in-progress paper, we propose a framework that, given a change in the conceptual model, identifies what must be modified in a NoSQL database schema and the underlying data. We researched several open source projects that use Apache Cassandra to study the benefits of using a conceptual model during the schema evolution process as well as to understand how these models evolve. In this first work, we have focused on studying seven types of conceptual model changes identified in these projects. For each change we describe the transformation required in the database schema to maintain the consistency between the schema and the model as well as the migration of data required to the new schema version.},
keywords={Databases;NoSQL databases;Design methodology;Data integrity;Big Data;Data models;Task analysis;quality;database evolution;NoSQL;column-oriented databases;conceptual model},
doi={10.1109/BigData50022.2020.9378228},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9182621,
author={Meijuan, Shan and Kaili, Yang},
booktitle={2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)},
title={Design and Development of self-Adaptive Learning System Based on Data Analysis},
year={2020},
volume={},
number={},
pages={709-711},
abstract={In the era of rapid development of artificial intelligence and big data technology, the education departments are also experiencing the transformation from the traditional classroom to the blending learning mode mixed with online learning and offline classroom. In order to promote the modernization of education and realize the inclusiveness and individuation of future education, the self-adaptive learning platform is rising gradually, which emphasizes service orientation and provides personalized learning contents to students based on data collections and analyses, enhancing the pertinence and efficiency of students' learning, and improving students' learning experience. In this paper, the writer applies to big data and artificial intelligence technology into the educational environment and attempts to build an intelligent self-adaptive learning system to realize personalized teaching model for the purpose of serving for teachers and students. Besides, the writer also carries out some innovative explorations on personalized teaching based on learner-centered and improves teaching qualities with remindings and early warnings.},
keywords={Adaptive learning;Big Data;Learning (artificial intelligence);Adaptation models;Learning systems;education big data;online machine learning system;artificial intelligence},
doi={10.1109/ICAICA50127.2020.9182621},
ISSN={},
month={June},}
@INPROCEEDINGS{7004497,
author={Warren, Robert and Liu, Bo},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Language, cultural influences and intelligence in historical gazetteers of the Great War},
year={2014},
volume={},
number={},
pages={70-72},
abstract={Historical gazetteers trace locations that have been long forgotten while allowing for the cross-referencing of locations across different documents. In this work, we present the problem of managing a gazetteer of geometries, features and names during the Great War on the Western Front. The careful tracking of provenance information and the novel use of existing semantic web standards allows for the discovery of both the quality of the cartographic work done by both sides and the cultural influences between belligerents.},
keywords={Geometry;Semantic Web;Big data;Vocabulary;Cultural differences;Ontologies;Shape;Historical Gazetteers;Linked Open Big Data},
doi={10.1109/BigData.2014.7004497},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6903303,
author={Chircu, Alina M. and Sultanow, Eldar and Chircu, Flavius C.},
booktitle={2014 IEEE World Congress on Services},
title={Cloud Computing for Big Data Entrepreneurship in the Supply Chain: Using SAP HANA for Pharmaceutical Track-and-Trace Analytics},
year={2014},
volume={},
number={},
pages={450-451},
abstract={This paper focuses on the real-world use of cloud-based analytics for supporting medication track-and-trace and monitoring safety and quality in the pharmaceutical supply chain. The paper describes a track-and-trace system developed by German start-up XQS-Service GmbH and adopted by manufacturers, wholesalers, pharmacies and clinics in Germany. The paper also discusses the benefits of using in-memory computing to process large amounts of big data to support efficient and effective medication track-and-trace.},
keywords={Supply chains;Big data;Cloud computing;Clouds;Innovation management;Drugs;big data analytics;cloud computing;entrepreneurship;pharmaceutical supply chain;RFID;SAP HANA;start-up;track-and-trace},
doi={10.1109/SERVICES.2014.84},
ISSN={2378-3818},
month={June},}
@INPROCEEDINGS{9384635,
author={Jia, Cuijiao and Xu, Zhijin},
booktitle={2020 International Conference on Modern Education and Information Management (ICMEIM)},
title={Analyses of the Research Status Quo of Xi Jinping's Thought on Ecological Civilization Based on Big Data Method},
year={2020},
volume={},
number={},
pages={240-243},
abstract={Xi Jinping's thought on ecological civilization, as the guiding ideology of China's ecological civilization construction in the new era, has become one of the hot spots in the academic research. Applying the big data method with the help of VOSviewer and SmartAnalyze, the paper has performed a series of analyses of knowledge graph, including the annual publication trends, the subject term expression changes, the subject distribution, the authors co-occurrence and the keywords co-occurrence on 335 published papers from core journals and CSSCI periodicals database regarding the theme of Xi Jinping's thought on ecological civilization. By analyzing the knowledge map, we find that the quantity and quality of research literatures have the tendency of growth, and subject terms become more and more standardized, but the research perspective needs to be broadened. There are more and more exchanges and cooperation between the authors, but in low frequency. Their topics mainly centre around the five aspects of value, connotation, background, path and theory origin of Xi jinping's thought on ecological civilization.},
keywords={Economics;Social sciences;Education;Big Data;Market research;Safety;Information management;thought of ecological civilization;Xi Jinping;research status quo;big data method},
doi={10.1109/ICMEIM51375.2020.00062},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9422005,
author={Xiong, Xiaoli},
booktitle={2020 2nd International Conference on Information Technology and Computer Application (ITCA)},
title={Research on Chinese Individualized Reading Teaching Based on "Big Data" Education Platform},
year={2020},
volume={},
number={},
pages={668-671},
abstract={In the practice of Chinese teaching, reading teaching is a very core content. Reading teaching can scientifically and comprehensively improve students' reading literacy. In order to effectively optimize the reading ability of students, teachers should actively and comprehensively carry out personalized reading teaching. At the same time, teachers also need to rely on the "big data" education platform to effectively and efficiently improve the overall effectiveness and level of personalized reading, guarantee the quality of students' reading in an all-round way, and better promote the growth and development of students.},
keywords={Education;Computer applications;Big Data;Information technology;Big Data Education Platform;Personalized Chinese Reading;Teaching},
doi={10.1109/ITCA52113.2020.00145},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9282614,
author={Zhang, Chi and Qiao, Xiangjie and Chen, Xianfeng},
booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
title={A Big Data based Decision Framework for Public Management and Service in Tourism},
year={2020},
volume={},
number={},
pages={550-555},
abstract={In view of the characteristics of the complexity of public management and service content, the universality of service objects and the diversity of demands faced by current tourism industry development into a new stage of popularisation and industrialisation, this paper proposes a big data driven decision-making model to innovate tourism public management and service, and discusses the connotation, decision-making and implementation process under this decision-making mode. Through the construction of tourism public management and service framework based on big data, this paper discusses the elements, environment characteristics and promotion mode of the framework operation. The mode of tourism public management and service are reformed with decision-making and management based on big data. The problems solution efficiency, quality and services in current tourism industry are improved. Further, tourism public service in the sustainable development in tourism industry worldwide is promoted.},
keywords={Decision making;Tourism industry;Software quality;Big Data;Data models;Software reliability;Sustainable development;Big Data;Tourism Public Management and Service;Big Data Driven Decision-Making Model},
doi={10.1109/QRS-C51114.2020.00096},
ISSN={},
month={Dec},}
@ARTICLE{9146409,
author={He, Xiaoming and Wang, Kun and Lu, Haodong and Xu, Wenyao and Guo, Song},
journal={IEEE Network},
title={Edge QoE: Intelligent Big Data Caching via Deep Reinforcement Learning},
year={2020},
volume={34},
number={4},
pages={8-13},
abstract={In mobile edge networks (MENs), big data caching services are expected to provide mobile users with better quality of experience (QoE) than normal scenarios. However, the increasing types of sensors and devices are producing an explosion of big data. Extracting valuable contents for caching is becoming a vital issue for the satisfaction of QoE. Therefore, it is urgent to propose some rational strategies to improve QoE, which is the major challenge for content-centric caching. This article introduces a novel big data architecture consisting of data management units for content extraction and caching decision, improving quality of service and ensuring QoE. Then a caching strategy is proposed to improve QoE, including three parts: (1) the caching location decision, which means the method of deploying caching nodes to make them closer to users; (2) caching capacity assessment, which aims to seek suitable contents to match the capacity of caching nodes; and (3) caching priority choice, which leads to contents being cached according to their priority to meet user demands. With this architecture and strategy, we particularly use a caching algorithm based on deep reinforcement learning to achieve lower cost for intelligent caching. Experimental results indicate that our schemes achieve higher QoE than existing algorithms.},
keywords={Quality of experience;Big Data;Computer architecture;Quality of service;Streaming media;Reinforcement learning;Data mining},
doi={10.1109/MNET.011.1900393},
ISSN={1558-156X},
month={July},}
@INPROCEEDINGS{8771733,
author={Radhakrishnan, Asha and Das, Sarasij},
booktitle={2018 20th National Power Systems Conference (NPSC)},
title={Quality Assessment of Smart Grid Data},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Enormous amount of data gets generated in the Smart Grids (SGs) due to the large number of measuring devices, higher measurement rates and various types of sensors. Smart grid data contains important and critical information about the grid. Data driven applications are being developed for better planning, monitoring and operation of SGs. The outcome of data analytics heavily depends on the quality of SG data. However, not much work has been reported on the quality assessment of SG data. This paper addresses the objective assessment of SG data quality. Various dimensions of SG data quality are identified in this paper. Mathematical formulations are proposed to quantify the SG data quality. Proposed data quality metrics have been applied on the SCADA and PMU measurements collected from the Southern Regional Grid of India to demonstrate their effectiveness.},
keywords={Data integrity;Smart grids;Phasor measurement units;Big Data;Quality assessment;Big data;data quality;power system;smart grid},
doi={10.1109/NPSC.2018.8771733},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7364061,
author={Abboura, Asma and Sahrl, Soror and Ouziri, Mourad and Benbernou, Salima},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={CrowdMD: Crowdsourcing-based approach for deduplication},
year={2015},
volume={},
number={},
pages={2621-2627},
abstract={Matching dependencies (MDs) were recently introduced as quality rules for data cleaning and entity resolution. They are rules that specify what values should be considered duplicates, and have to be matched. Defining such quality rules on a database instance, is a very expensive and a time consuming process, and requires huge efforts to analyse the whole database. In this demo paper, we present CrowdMD, a hybrid machine-crowd system for generating MDs. It first asks the crowd to determine whether a given pair, from training sample pairs, match or not. Then, it uses data mining techniques to generate attributes constituting an MD. Using a Restaurant database, we will show how the crowders can help to generate MDs by labelling the training sample through the CrowdMD user interface and how MDs can be mined from this training set.},
keywords={Big data;Databases;Labeling;Hybrid power systems;Crowdsourcing;Training;Cleaning;matching rules;deduplication;entity resolution;big data quality},
doi={10.1109/BigData.2015.7364061},
ISSN={},
month={Oct},}
@ARTICLE{8946609,
author={Sarabia-Jácome, David and Palau, Carlos E. and Esteve, Manuel and Boronat, Fernando},
journal={IEEE Access},
title={Seaport Data Space for Improving Logistic Maritime Operations},
year={2020},
volume={8},
number={},
pages={4372-4382},
abstract={The maritime industry expects several improvements to efficiently manage the operation processes by introducing Industry 4.0 enabling technologies. Seaports are the most critical point in the maritime logistics chain because of its multimodal and complex nature. Consequently, coordinated communication among any seaport stakeholders is vital to improving their operations. Currently, Electronic Data Interchange (EDI) and Port Community Systems (PCS), as primary enablers of digital seaports, have demonstrated their limitations to interchange information on time, accurately, efficiently, and securely, causing high operation costs, low resource management, and low performance. For these reasons, this contribution presents the Seaport Data Space (SDS) based on the Industrial Data Space (IDS) reference architecture model to enable a secure data sharing space and promote an intelligent transport multimodal terminal. Each seaport stakeholders implements the IDS connector to take part in the SDS and share their data. On top of SDS, a Big Data architecture is integrated to manage the massive data shared in the SDS and extract useful information to improve the decision-making. The architecture has been evaluated by enabling a port authority and a container terminal to share its data with a shipping company. As a result, several Key Performance Indicators (KPIs) have been developed by using the Big Data architecture functionalities. The KPIs have been shown in a dashboard to allow easy interpretability of results for planning vessel operations. The SDS environment may improve the communication between stakeholders by reducing the transaction costs, enhancing the quality of information, and exhibiting effectiveness.},
keywords={Seaports;Big Data;Industries;Stakeholders;Logistics;Data models;Data mining;Analytics;big data;industry 4.0;industrial data spaces;Internet of Things;maritime;seaport;intelligent transport},
doi={10.1109/ACCESS.2019.2963283},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7300845,
author={Qanbari, Soheil and Rekabsaz, Navid and Dustdar, Schahram},
booktitle={2015 3rd International Conference on Future Internet of Things and Cloud},
title={Open Government Data as a Service (GoDaaS): Big Data Platform for Mobile App Developers},
year={2015},
volume={},
number={},
pages={398-403},
abstract={The next web of open and linked data leverages governmental data openness to improve the quality of social services. This data is a national asset. In this study, we elaborate on this emerging open government movement, together with the underlying data transparency to drive novel business models which utilize these assets under a functioning platform called Open Government Data as a Service (GoDaaS). These business models actively engage civic-minded programmers in developing sustainable applications, contextualizing and utilizing the government open data resources. This leads to an expansive government marketplace, with many civic-minded developers might be new to doing business with the federal or state government. By means of a consultation service prototype, we provide development advices for programmers on how to work out the specific details of their applications business model. Having the business models in focus, this study also proposes a novel abstraction unit called Gov. Data Compute Unit (DCU), so that governments are able to feed developers with formalized, structured and programmable data resource units rather than just data catalogs. Such DCUs enable developers to cope with an increasing heterogeneity of state government data sets, by providing a unified interface on top of diverse data schemata from various states.},
keywords={Government;Data models;Biological system modeling;Stakeholders;Programming;Adaptation models;Government Open Data;Data as a Service;Business model;Data compute unit},
doi={10.1109/FiCloud.2015.34},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8332775,
author={Gao, Liuxin},
booktitle={2018 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={Research on Application of Data Mining in Virtual Community of Foreign Language Learning},
year={2018},
volume={},
number={},
pages={331-334},
abstract={The construction of virtual community in foreign language learning is a comprehensive foreign language learning environment integrated with foreign language vocabulary database construction and vocabulary retrieval, combining the virtual reality technology to construct the language environment of foreign language learning. The virtual community of foreign language learning can improve the sense of language authenticity in foreign language learning and improve the quality of foreign language teaching. A method of building a virtual community for foreign language learning is proposed based on data mining technology, data acquisition and feature preprocessing model for building semantic vocabulary of foreign language learning is constructed, the linguistic environment characteristics of the semantic vocabulary data of foreign language learning is analyzed, and the semantic noumenon structure model is obtained. Fuzzy clustering method is used for vocabulary clustering and comprehensive retrieval in the virtual community of foreign language learning, the performance of vocabulary classification in foreign language learning is improved, the adaptive semantic information fusion method is used to realize the vocabulary data mining in the virtual community of foreign language learning, information retrieval and access scheduling for virtual communities in foreign language learning are realized based on data mining results. The simulation results show that the accuracy of foreign language vocabulary retrieval is good, improve the efficiency of foreign language learning.},
keywords={Conferences;Transportation;Big Data;Smart cities;data mining;foreign language learning;virtual community;language environment;fuzzy clustering},
doi={10.1109/ICITBS.2018.00091},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8258391,
author={Liu, Ning and Kumara, Soundar and Reich, Eric},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Explainable data-driven modeling of patient satisfaction survey data},
year={2017},
volume={},
number={},
pages={3869-3876},
abstract={In the personalized patient-centered healthcare, self-reported patient satisfaction survey data plays an important role. Given the patient survey data, it is necessary to identify the drivers of patient satisfaction and explain them so that such patterns can be used in future as well as necessary corrective actions can be taken. In healthcare, both accuracy and interpretability are important criteria for choosing a reliable predictive model for analyzing patient data. Usually, complex models such as Random Forest, neural networks can achieve high prediction accuracy but lack necessary interpretation to their prediction results. In this paper, we address this problem by proposing a local explanation method to interpret complex model prediction results. First, we build a predictive model using Random Forest to fit the patient satisfaction data. Second, we utilize local explanation method to provide insights into the Random Forest prediction results so as to discover true reasons behind patient experiences and overall ratings. Specifically, our approach allows us to interpret patient's overall rating of a hospital at the individual level, and find out the set of the most influential factors for each patient. We focus on all unhappy patients to investigate the top reasons for patient dissatisfaction. Our approach and findings will help to establish guidelines for a quality healthcare.},
keywords={Hospitals;Predictive models;Data models;Training data;Pain;Vegetation;explainable machine learning;data mining;lazy lasso;patient satisfaction;healthcare management},
doi={10.1109/BigData.2017.8258391},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7371861,
author={Laranjeiro, Nuno and Soydemir, Seyma Nur and Bernardino, Jorge},
booktitle={2015 IEEE 21st Pacific Rim International Symposium on Dependable Computing (PRDC)},
title={A Survey on Data Quality: Classifying Poor Data},
year={2015},
volume={},
number={},
pages={179-188},
abstract={Data is part of our everyday life and an essential asset in numerous businesses and organizations. The quality of the data, i.e., the degree to which the data characteristics fulfill requirements, can have a tremendous impact on the businesses themselves, the companies, or even in human lives. In fact, research and industry reports show that huge amounts of capital are spent to improve the quality of the data being used in many systems, sometimes even only to understand the quality of the information in use. Considering the variety of dimensions, characteristics, business views, or simply the specificities of the systems being evaluated, understanding how to measure data quality can be an extremely difficult task. In this paper we survey the state of the art in classification of poor data, including the definition of dimensions and specific data problems, we identify frequently used dimensions and map data quality problems to the identified dimensions. The huge variety of terms and definitions found suggests that further standardization efforts are required. Also, data quality research on Big Data appears to be in its initial steps, leaving open space for further research.},
keywords={Standards organizations;Industries;Training;Companies;Decision making;Poor data quality;dirty data;poor data classification;data quality problems},
doi={10.1109/PRDC.2015.41},
ISSN={},
month={Nov},}
@ARTICLE{8766970,
author={Aggarwal, Apeksha and Toshniwal, Durga},
journal={IEEE Access},
title={Frequent Pattern Mining on Time and Location Aware Air Quality Data},
year={2019},
volume={7},
number={},
pages={98921-98933},
abstract={With the advent of big data era, enormous volumes of data are generated every second. Varied data processing algorithms and architectures have been proposed in the past to achieve better execution of data mining algorithms. One such algorithm is extracting most frequently occurring patterns from the transactional database. Dependency of transactions on time and location further makes frequent itemset mining task more complex. The present work targets to identify and extract the frequent patterns from such time and location-aware transactional data. Primarily, the spatio-temporal dependency of air quality data is leveraged to find out frequently co-occurring pollutants over several locations of Delhi, the capital city of India. Varied approaches have been proposed in the past to extract frequent patterns efficiently, but this work suggests a generalized approach that can be applied to any numeric spatio-temporal transactional data, including air quality data. Furthermore, a comprehensive description of the algorithm along with a sample running example on air quality dataset is shown in this work. A detailed experimental evaluation is carried out on the synthetically generated datasets, benchmark datasets, and real world datasets. Furthermore, a comparison with spatio-temporal apriori as well as the other state-of-the-art non-apriori-based algorithms is shown. Results suggest that the proposed algorithm outperformed the existing approaches in terms of execution time of algorithm and memory resources.},
keywords={Data mining;Itemsets;Air quality;Data structures;Urban areas;Location awareness;Air quality;data mining;frequent;itemset;spatio-temporal},
doi={10.1109/ACCESS.2019.2930004},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9035250,
author={Mylavarapu, Goutam and Viswanathan, K. Ashwin and Thomas, Johnson P.},
booktitle={2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA)},
title={Assessing Context-Aware Data Consistency},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Data analysis is a demanding task that involves extracting deep insights hidden in data. Many businesses enforce data analysis irrespective of the domain, as it is crucial in minimizing developmental risks. Raw data cannot be used to perform any analysis as poor-quality data leads to erroneous decision-making. This makes data quality assessment a necessary function before data analysis. Data quality is a multi-dimensional factor that affects the analysis in multiple ways. Among all the dimensions, consistency is one of the most critical dimensions to assess. Context of data plays an important role in consistency assessment, as the records are inherently related within a dataset. Existing studies are computationally expensive and do not consider the context of data. In this paper, we propose a comprehensive context-aware data consistency assessment tool that uses machine learning to evaluate the consistency of data. Our model was developed on Apache Hadoop and Apache Spark to support big data, as well as to boost some computationally intensive algorithms.},
keywords={Feature extraction;Data analysis;Data integrity;Data models;Machine learning algorithms;Context modeling;Task analysis;Data analysis;data context;data quality;data consistency;machine learning;word embeddings;approximate dependencies;mutual information;apache hadoop;apache spark},
doi={10.1109/AICCSA47632.2019.9035250},
ISSN={2161-5330},
month={Nov},}
@INPROCEEDINGS{7314320,
author={Ho, Thi Thao Nguyen and Pernici, Barbara},
booktitle={2015 IEEE Conference on Technologies for Sustainability (SusTech)},
title={A data-value-driven adaptation framework for energy efficiency for data intensive applications in clouds},
year={2015},
volume={},
number={},
pages={47-52},
abstract={The emerging of cloud computing and Big Data has been presenting to the world both grand opportunities and challenges. However, the increasing trend in energy consumption in clouds due to the fast growing quantity of data to be transmitted and processed has made cloud computing, together with Big Data phenomenon, becoming the dominant contributor in energy consumption, and consequently in CO2 emission. In this paper, we propose an adaptation framework for data-intensive applications aiming to improve energy efficiency. The adaptation mechanism is driven by the data value extracted from datasets or data streams of the applications. Our main contribution lies in the proposal of treating large amount of data according to their value, i.e., their level of importance.},
keywords={Monitoring;Big data;Measurement;Engines;Data mining;Throughput;Quality of service;data value;energy efficiency;Big Data;Cloud computing;MapReduce},
doi={10.1109/SusTech.2015.7314320},
ISSN={},
month={July},}
@INPROCEEDINGS{7009043,
author={Pankowska, Malgorzata},
booktitle={International Conference on Information Society (i-Society 2014)},
title={Service science facing Big Data},
year={2014},
volume={},
number={},
pages={207-212},
abstract={The Big Data is a modification of the traditional view of information organization, particularly view of the data warehouses and databases. Nowadays, business organizations must address a mix of structured, unstructured and streaming data that supports queries and reports. Business recognized the wealth of untapped information in open social media data. Therefore, the goal of this paper is to present the procedural approach on how to cope with massive data sets' management. The proposal included in this paper covers service science application.},
keywords={Big data;Quality of service;Monitoring;Decision making;Organizations;Data analysis;Big Data;information governance;service science;Service Level Management;service quality},
doi={10.1109/i-Society.2014.7009043},
ISSN={},
month={Nov},}
@ARTICLE{8993712,
author={Ullah, Hidayat and Wan, Wanggen and Haidery, Saqib Ali and Khan, Naimat Ullah and Ebrahimpour, Zeinab and Muzahid, A. A. M.},
journal={IEEE Access},
title={Spatiotemporal Patterns of Visitors in Urban Green Parks by Mining Social Media Big Data Based Upon WHO Reports},
year={2020},
volume={8},
number={},
pages={39197-39211},
abstract={Green parks in urban areas are believed to enhance the well-being of residents. The importance of green spaces to support health and fitness in urban areas has recently regained interest. Reports released in 2010-2016 by the World Health Organization (WHO) on urban planning, environment, and health stated that green spaces can have a positive impact on physical activity, social and mental well-being, enhance air quality and decrease noise exposure. We analyzed the number of check-ins in various parks of Shanghai by utilizing geotagged social media network check-in data. This article presents a descriptive study using social media data by obtaining the three-year comparison of spatial and temporal patterns of park visits to raise public awareness that green parks provide a healthy environment that can be beneficial for the well-being of urban citizens. We investigated the visitor spatiotemporal behavior in more than 115 green parks in 10 districts of Shanghai with approximately 250,000 check-ins. We examined 3 years of geotagged data and our main findings are: (i) the spatial and temporal variations of users in urban green parks (ii) the gender differences in space and time with relation to urban green parks. The main objective of this article is to present evident data for policymakers on the advantages of providing green spaces access to urban citizens and to facilitate cities with systematic approaches to provide green space access to improve the health of urban citizens.},
keywords={Green products;Urban areas;Social networking (online);Spatiotemporal phenomena;Data mining;Big Data;Air quality;Urban green parks;big data;social networks;spatiotemporal;KDE;data mining},
doi={10.1109/ACCESS.2020.2973177},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7207278,
author={Hahanov, Vladimir and Gharibi, Wajeb and Litvinova, Eugenia and Chumachenko, Svetlana},
booktitle={2015 IEEE International Congress on Big Data},
title={Big Data Driven Cyber Analytic System},
year={2015},
volume={},
number={},
pages={615-622},
abstract={An infrastructure for parallel analyzing big data in order to search, pattern recognition and decision-making based on the use of the Boolean metric of cyberspace measurement is proposed. It is characterized by using only logical xor-operation for determining the cyber-distance by means of cyclic closing at least one object, which allows significantly increasing the speed of analysis of large data. A new approach to vector-logical big data processing based on the total removal of arithmetic operations, which impact on the performance and hardware complexity, is offered, the approach can be efficiently implemented based on modern multiprocessor digital systems-on-chips and virtual parallel processors, operating under the control of cyber-physical systems or cloud service-filters. A qubit-vector model of computing automaton is proposed, it is characterized by the transactional interaction of memory components, which represent the combinational and sequential elements and are implemented in the form of a qubit or "quantum" primitives needed to create parallel virtual computers and cloud-focused processors. A new structural model for analyzing big data is developed, it is characterized by the use of cloud services, cyber-physical and search systems, virtual parallel multiprocessors with a minimal set of vector-logical operations for accurate information retrieval based on the proposed Boolean metric and non-numerical quality criteria, which makes it possible to create a semantic infrastructure of cyberspace by the competence classification and metric ordering big data across the cyber-ecosystem of the planet.},
keywords={Big data;Cyberspace;Process control;Measurement;Computers;Planets;big data;cyberspace analytics;cyber-physical system;cyber metric for information retrieval},
doi={10.1109/BigDataCongress.2015.94},
ISSN={2379-7703},
month={June},}
@ARTICLE{6774768,
author={Gorton, Ian and Klein, John},
journal={IEEE Software},
title={Distribution, Data, Deployment: Software Architecture Convergence in Big Data Systems},
year={2015},
volume={32},
number={3},
pages={78-85},
abstract={Big data applications are pushing the limits of software engineering on multiple horizons. Successful solutions span the design of the data, distribution, and deployment architectures. The body of software architecture knowledge must evolve to capture this advanced design knowledge for big data systems. This article is a first step on this path. Our research is proceeding in two complementary directions. First, we're expanding our collection of architecture tactics and encoding them in an environment that supports navigation between quality attributes and tactics, making crosscutting concerns for design choices explicit. Second, we're linking tactics to design solutions based on specific big data technologies, enabling architects to rapidly relate a particular technology's capabilities to a specific set of tactics.},
keywords={Big data;Distributed databases;Computer architecture;Software engineering;Software architecture;Data management;Data models;software architecture;big data;distributed systems;data management;NoSQL;software engineering},
doi={10.1109/MS.2014.51},
ISSN={1937-4194},
month={May},}
@ARTICLE{8913543,
author={Li, Xiang and Zhang, Zijia},
journal={IEEE Access},
title={Research and Analysis for Real-Time Streaming Big Data Based on Controllable Clustering and Edge Computing Algorithm},
year={2019},
volume={7},
number={},
pages={171621-171632},
abstract={Aiming at the low efficiency, poor performance and weak stability of traditional clustering algorithms and the poor response to the processing of massive data in real time, a real-time streaming controllable clustering edge computing algorithm (SCCEC) is proposed. First, the data tuples that arrive in real time are pre-processed by coarse clustering, the number of clusters, and the position of the center point are determined, and a set formed by macro clusters having differences is formed. Secondly, the macro cluster set obtained by the coarse clustering is sampled, and then K-means parallel clustering is performed with the largest and smallest distances, thereby realizing fine clustering of data. Finally, the completely clustering algorithm and the edge-computing algorithm are combined to realize the clustering analysis under the edge-computing framework. The experimental results show that the proposed algorithm has the advantages of high efficiency, good quality, and strong stability. It can quickly obtain the global optimal solution, and deal with massive data with high real-time performance. It can be used for real-time streaming data aggregation under big data background.},
keywords={Clustering algorithms;Edge computing;Real-time systems;Big Data;Cloud computing;Internet of Things;Classification algorithms;Real-time streaming data;clustering;edge computing;algorithm},
doi={10.1109/ACCESS.2019.2955992},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{6932715,
author={Zhang, Lichen},
booktitle={2014 IEEE International Conference on Information and Automation (ICIA)},
title={A framework to specify big data driven complex cyber physical control systems},
year={2014},
volume={},
number={},
pages={548-553},
abstract={Big data technology is a new technology that aims to efficiently obtain value from very big volumes of a wide variety of data, by enabling high velocity capture, process, store, discovery and/or analysis, while ensuring their veracity by an automatic quality control in order to obtain a big value and make decision. Big Data is described by what is often represented as a multi-V model. In multi-V model, volume, velocity and variety are the items most commonly recognized. Big data driven cyber physical systems refer to such cyber physical systems that use large quantities of complex data to perform their functions. Data is very important to the correct specification, analysis, design and implementation and operation of these big data driven cyber physical systems. he design of big data driven cyber physical systems requires that new concepts are used to model classical data structures, 4V features of big data, spatio-temporal constraints and moving object, and the dynamic continuous behavior of the physical world. In this paper, we propose an approach to integrate Architecture Analysis & Design Language (AADL) [6], Modelicaml and Hybrid Relation Calculus for big data driven cyber physical system development. We illustrate the proposed method by specifying and modeling the Vehicular Ad doc NETwork (VANET).},
keywords={Data models;Big data;Mathematical model;Unified modeling language;Analytical models;Ports (Computers);Solid modeling;Big data;AADL;Cyber physical systems;Specification;modeling;VANET},
doi={10.1109/ICInfA.2014.6932715},
ISSN={},
month={July},}
@INPROCEEDINGS{7373260,
author={Omar, Ankur},
booktitle={2015 9th International Conference on Next Generation Mobile Applications, Services and Technologies},
title={Improving Data Extraction Efficiency of Cache Nodes in Cognitive Radio Networks Using Big Data Analysis},
year={2015},
volume={},
number={},
pages={305-310},
abstract={In cognitive radio networks, unlicensed users are allowed to use underutilized licensed spectrum until licensed users' transmission quality of service is not compromised. As soon as the conflict goes beyond a certain limit, SU must leave the spectrum and move to the other nearby free band. At the time of interruption, sensing the nearby free channels and switching to them will take some time, hence the ongoing data will be interrupted, which will delay the data transmission. To minimize this delay, creating cache of the SU signal at multiple nodes in a cluster has shown significant improvement in reducing the transmission delay if cache placement is done systematically. This systematic and accurate placement of cache is possible if the data accumulated is accessed and processed quickly. Taking into account the vastness of cluster networks, a huge amount of data will be required to be accessed and processed. Cognitive Radio networks are very complex structures when it comes to the information sharing amongst the secondary users and with the cluster head. Taking into account, whether unlicensed users share their information with other secondary users, and in case if they do, how much proportion of it they allow the fusion center to process, several big data scenarios exist. This paper discusses the possible information sharing scenarios in cognitive radio network systems and their possible Big Data Solutions.},
keywords={Cognitive radio;Delays;Big data;Data mining;Open systems;Quality of service;Sensors;Cognitive Radio Networks;Cache Nodes;Big Data;MapReduce},
doi={10.1109/NGMAST.2015.15},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7167407,
author={Casale, Giuliano and Ardagna, Danilo and Artac, Matej and Barbier, Franck and Di Nitto, Elisabetta and Henry, Alexis and Iuhasz, Gabriel and Joubert, Christophe and Merseguer, Jose and Munteanu, Victor Ion and Perez, Juan Fernando and Petcu, Dana and Rossi, Matteo and Sheridan, Craig and Spais, Ilias and Vladuic, Daniel},
booktitle={2015 IEEE/ACM 7th International Workshop on Modeling in Software Engineering},
title={DICE: Quality-Driven Development of Data-Intensive Cloud Applications},
year={2015},
volume={},
number={},
pages={78-83},
abstract={Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.},
keywords={Unified modeling language;Big data;Data models;Computational modeling;Analytical models;Reliability;Software;Big Data;quality assurance;model-driven engineering},
doi={10.1109/MiSE.2015.21},
ISSN={2156-7891},
month={May},}
@INPROCEEDINGS{7214117,
author={Alavi, Zohreh and Sharma, Sagar and Zhou, Lu and Chen, Keke},
booktitle={2015 IEEE 8th International Conference on Cloud Computing},
title={Scalable Euclidean Embedding for Big Data},
year={2015},
volume={},
number={},
pages={773-780},
abstract={Euclidean embedding algorithms transform data defined in an arbitrary metric space to the Euclidean space, which is critical to many visualization techniques. At big-data scale, these algorithms need to be scalable to massive data-parallel infrastructures. Designing such scalable algorithms and understanding the factors affecting the algorithms are important research problems for visually analyzing big data. We propose a framework that extends the existing Euclidean embedding algorithms to scalable ones. Specifically, it decomposes an existing algorithm into naturally parallel components and non-parallelizable components. Then, data parallel implementations such as MapReduce and data reduction techniques are applied to the two categories of components, respectively. We show that this can be possibly done for a collection of embedding algorithms. Extensive experiments are conducted to understand the important factors in these scalable algorithms: scalability, time cost, and the effect of data reduction to result quality. The result on sample algorithms: Fast Map-MR and LMDS-MR shows that with the proposed approach the derived algorithms can preserve result quality well, while achieving desirable scalability.},
keywords={Algorithm design and analysis;Big data;Approximation algorithms;Measurement;Parallel processing;Scalability;Complexity theory;data visualization;Euclidean embedding algorithms;big data;data reduction;parallel processing},
doi={10.1109/CLOUD.2015.107},
ISSN={2159-6190},
month={June},}
@INPROCEEDINGS{9071249,
author={Liu, Hong and Sang, Zhenhua and Karali, Sameer},
booktitle={2019 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={Approximate Quality Assessment with Sampling Approaches},
year={2019},
volume={},
number={},
pages={1306-1311},
abstract={Data is useful to the extent that it can be quickly analyzed to reveal valuable information. With high-quality data, we can increase revenue, reduce cost, and reduce risk. On the other hand, the consequences of poor-quality data can be severe. It has been estimated that poor quality customer data costs U.S. businesses $611 billion annually in postage, printing, and staff overhead. These issues make data quality assessment a necessary and critical step in any data-related systems. Big data brings new challenges to data quality assessment due to the scale of data, streaming data, and different forms of data. Therefore, we proposed a sampling-based approximate quality assessment model on large data. Sampling large datasets can make all quality assessment processes cheaper and more feasible because of data reduction. The protocol of this work: First, the sample size is determined for estimating a large dataset. Next, sampling techniques are applied to collect samples. Then, these samples are used to estimate the quality of the large dataset. The objective of quality assessment in this work is to evaluate the completeness, accuracy, and timeliness of data and to return fast and approximate scores. Using different sample sizes and different sampling methods, we obtained 72 sets of data and compared them. These results show that the proposed approach is efficient and provides some insight into the quality assessment with samples.},
keywords={Data integrity;Quality assessment;Writing;Gaussian distribution;Big Data;Sampling methods;Time-frequency analysis;Data Quality, Quality Assessment, Sampling},
doi={10.1109/CSCI49370.2019.00244},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7322471,
author={Hubbell, Matthew and Moran, Andrew and Arcand, William and Bestor, David and Bergeron, Bill and Byun, Chansup and Gadepally, Vijay and Michaleas, Peter and Mullen, Julie and Prout, Andrew and Reuther, Albert and Rosa, Antonio and Yee, Charles and Kepner, Jeremy},
booktitle={2015 IEEE High Performance Extreme Computing Conference (HPEC)},
title={Big Data strategies for Data Center Infrastructure management using a 3D gaming platform},
year={2015},
volume={},
number={},
pages={1-6},
abstract={High Performance Computing (HPC) is intrinsically linked to effective Data Center Infrastructure Management (DCIM). Cloud services and HPC have become key components in Department of Defense and corporate Information Technology competitive strategies in the global and commercial spaces. As a result, the reliance on consistent, reliable Data Center space is more critical than ever. The costs and complexity of providing quality DCIM are constantly being tested and evaluated by the United States Government and companies such as Google, Microsoft and Facebook. This paper will demonstrate a system where Big Data strategies and 3D gaming technology is leveraged to successfully monitor and analyze multiple HPC systems and a lights-out modular HP EcoPOD 240a Data Center on a singular platform. Big Data technology and a 3D gaming platform enables the relative real time monitoring of 5000 environmental sensors, more than 3500 IT data points and display visual analytics of the overall operating condition of the Data Center from a command center over 100 miles away. In addition, the Big Data model allows for in depth analysis of historical trends and conditions to optimize operations achieving even greater efficiencies and reliability.},
keywords={Three-dimensional displays;Games;Big data;Cooling;Monitoring;Data visualization;Sensors},
doi={10.1109/HPEC.2015.7322471},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8525121,
author={Bataev, Alexey V.},
booktitle={2018 IEEE International Conference "Quality Management, Transport and Information Security, Information Technologies" (IT&QM&IS)},
title={Analysis of the Application of Big Data Technologies in the Financial Sphere},
year={2018},
volume={},
number={},
pages={568-572},
abstract={The last years' global development is characterized by the mass introduction of information and communication technologies in all spheres of economic development. The implementation of digital technologies acquired explosive character recently. It is no coincidence that the term "digital economy", indicating the use of the most advanced digital technologies in various sectors of the world economy, has emerged in past years. Today, digital transformation occurs in virtually all areas of economic development, for example, Industry 4.0 in industry, Fintech in the financial sector. The main breakthrough technologies of digital transformation are cloud computing technology, cyber - physical systems, artificial intelligence, and technologies for analysis and processing of big data. One of the drivers of digital modernization is the big data direction. The rapid development of this sphere is explained by the intermittent growth of information, which leads to the impossibility of applying classical methods and tools for processing available data. The paper analyzes the technologies and methods of processing information in the field of big data. Future development of the sectors of the field of big data is given. The research of the big data market in various sectors of economic development was conducted; the leaders in this field are identified. The main directions of using big data in financial institutions as one of the key players in the market are analyzed. The study of the big data market in the financial sphere was carried out; financial indicators, dynamics of the market growth were determined. The main reasons that prevent the big data introduction in financial institutions are shown. The prognosis for the further development of the use of big data in the financial sphere is given.},
keywords={Big Data;Economics;Tools;Companies;Investment;Industries;information technology management;digital transformation;big data;financial institutions;implementation perspectives},
doi={10.1109/ITMQIS.2018.8525121},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8777571,
author={Jakhar, Karan and Hooda, Nishtha},
booktitle={2018 4th International Conference on Computing Communication and Automation (ICCCA)},
title={Big Data Deep Learning Framework using Keras: A Case Study of Pneumonia Prediction},
year={2018},
volume={},
number={},
pages={1-5},
abstract={Big Data predictive analytics using machine learning techniques is currently a much active area of research in medical science. With increasing size and complexity of medical data like X-rays, deep learning gained huge success in prediction of many fatal diseases like pneumonia. In this research work, DCNN (deep convolutional neural networks) an efficient predicting model for big data, having deep layers is a proposed, which can classify whether a person is having a pneumonia or not. The experiments are carried after extracting the features of high quality X-ray images data and achieved an prediction accuracy of 84% and AUC of Promising results are found, when the results of the DCNN framework is compared with the regular classifiers like SVM, random forest, etc. using different evaluation metrics like accuracy, sensitivity, etc. With the appearance of increasing cases of pneumonia, tactful implementation of deep learning can play a big part in improving the performance of prediction of many fatal diseases in the future.},
keywords={Diseases;Lung;Deep learning;Measurement;Biomedical imaging;Predictive models;big data;machine learning;prediction;deep learning;pneumonia},
doi={10.1109/CCAA.2018.8777571},
ISSN={2642-7354},
month={Dec},}
@INPROCEEDINGS{8706202,
author={Corallo, Angelo and Crespino, Annamaria and Dibiccari, Carla and Lazoi, Mariangela and Lezzi, Marianna},
booktitle={2018 14th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)},
title={Processing Big Data in Streaming for Fault Prediction: An Industrial Application},
year={2018},
volume={},
number={},
pages={730-736},
abstract={In the current data-driven industrial scenario, Big Data processing plays a leading role in enhancing business performance. An ever increasing number of working machines are equipped with smart devices (such as sensors and actuators) which are in charge of monitoring machine status in real time and implement corrective actions before the workpiece quality is compromised or machine is damaged. However, many manufacturing companies do not take advantage of the use of Big Data coming from their production systems. In some cases, Big Data analytics is an un-explored issue since it is considered time and resources consuming. Moreover, the real benefits of processing, in real time, industrial data are usually underestimated. The European project TOREADOR wants to extend and facilitate the diffusion of Big Data analytics within industrial contexts, in order to generate greater value for companies. Focusing on the aerospace components manufacturing process (as one of the case studies of the Project), the paper aims to describe the main developments and lessons learned by the customization of TOREADOR Big Data analytics platform to process and analyze data from CNC machines sensors.},
keywords={Big Data;Manufacturing;Companies;Analytical models;Data models;Aerospace industry;big data, fault detection, aerospace industry, Toreador project},
doi={10.1109/SITIS.2018.00117},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8728990,
author={Wang, Zihui and Haihong, E and Song, Meina and Ren, Zhijun},
booktitle={2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)},
title={Time-varying Data Visual Analysis method based on Parallel Coordinate System},
year={2019},
volume={},
number={},
pages={1256-1260},
abstract={With the advent of the era of big data, there are more and more ways to obtain data. The collected data generally is large-scale, multi-dimensional and time-varying. To explore and analyse time-varying data comprehensively, this paper proposes a visual method for processing multidimensional time-varying data based on parallel coordinate system. Firstly, we use MDS algorithm to turn the original multidimensional time data to distribute in one-dimension space in chronological order, constructing the time-varying parallel coordinate system. Then, we cluster the projection coordinate points of each time axis, and each type of data is drawn in different colours to differentiate. At last, edge bundling method is used to cluster data polylines in each time segment, helping to reduce visual confusion and improve information expression efficiency. The experimental analysis of the air quality dataset from Krakow Poland shows that the method can comprehensively analyse the multi-dimensional time-varying data, helping users to explore the implied regularity hidden in the dataset effectively.},
keywords={Data visualization;Time-varying systems;Visualization;Air quality;Dimensionality reduction;Sensor phenomena and characterization;time-varying data;multidimensional data;data visualization;parallel coordinate system;multidimensional scaling},
doi={10.1109/ITNEC.2019.8728990},
ISSN={},
month={March},}
@INPROCEEDINGS{8754345,
author={Xia, Hong and Zhao, Mingdao and Chen, Yanping and Wang, Zhongmin and Yu, Zhong and Yang, Jingwei},
booktitle={2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)},
title={Multi-Source Heterogeneous Core Data Acquisition Method in Edge Computing Nodes},
year={2019},
volume={1},
number={},
pages={382-385},
abstract={As the volume of data grows exponentially, big data brings an unprecedented burden to the current computing infrastructure. How to deal with big data efficiently and concisely and reduce the burden of computing infrastructure has always been a big challenge. Therefore, this paper proposes a high-quality core data extraction method in edge computing nodes. Firstly, heterogeneous data are fused into a unified model, the data characteristics of the original data are retained. Then, a Lanzcos-based incremental tensor decomposition method is proposed to extracted the high quality core tensor dynamically. Finally, the model algorithm is verified using real data. The experimental results show that the approximate tensor reconstructed from the tensor containing 15% of the core data can guarantee 90% accuracy. At the same time, IncLHOSVD is significantly better than non-incremental HOSVD in execution time in guaranteeing the accuracy of approximate equal error.},
keywords={Edge computing;Internet of Things;Data mining;Matrix decomposition;Computational modeling;Data models;Edge Computing, Tensor, HOSVD, dimensionality reduction, Big data},
doi={10.1109/COMPSAC.2019.00062},
ISSN={0730-3157},
month={Jul},}
@INPROCEEDINGS{6681858,
author={McHann, Stanley E.},
booktitle={2013 IEEE Rural Electric Power Conference (REPC)},
title={Grid analytics: How much data do you really need?},
year={2013},
volume={},
number={},
pages={C3-1-C3-4},
abstract={Electric utilities are experiencing a technology transformation, which includes the deployment of distributed intelligent devices, two-way communications systems, and information technologies to enable greater monitoring and control of their distribution systems. These technologies will increase the volume of data flowing into a utility's Information Systems, which in turn will need to be stored and managed. The amount of data that can be generated from AMI/AMR, SCADA, and other intelligent devices can be significant. One strategy, which seems to be a popular option, is to collect all of the data possible and figure out what to do with the data at a later date. As the amount of data increases that needs to be stored, there is a corresponding increase in Information Technology infrastructure, skill sets, and cost. If collecting all data were possible, it is doubtful the data would be stored or structured in a way that would be useable in the future. An alternative to capturing and storing all data possible is to use Information Engineering methodologies to focus on what data is needed for a given task or application. Information Engineering is defined as a set if interrelated disciplines that are needed to build a computerized enterprise based on information systems. The focus of Information Engineering is the data that is stored and maintained by computers and the information that is distilled from this data. [1] It is foundational to understand that data and information are terms used interchangeably, but are distinctly different terms with different meanings. A kWh is a piece of data that is useful information when added to other data such as meter number, account name, or service location. The amount and kind of data required, as well as the necessary time frames, (i.e., historical, real-time, predicted) depend upon the applications, which may include: 1. Geospatial Information System 2. Meter/billing information 3. Asset management 4. Work and workforce management 5. Network modeling and analysis for planning (e.g., voltage drop, power flow, short circuit, arc flash, contingency studies, reliability metrics, loss analysis, protective device coordination) 6. Operations (outage management, fault location, Volt/VAR control, power quality, demand management, distributed generation and storage) 7. Real-time, active grid management, grid analytics. This paper will discuss how to apply Information Engineering principles to turn data into useful information for a utility as an alternative to the "Big Data" approach to capturing and storing data. Using kWh, and Voltage as examples, we will outline the Information Engineering process to turn these data elements into useful information. Once you have the information engineered, the next step is to use data management methodologies to manage the data that is being gathered. Data Management is a detailed topic on its own and will not be covered on this paper. What data has been captured needs to be managed across the enterprise.},
keywords={Information management;Data handling;Data storage systems;Radio frequency;Power grids;Information technology;Advanced Metering Infrastructure (AMI);Data Analytics;Smart Grid;Smart Meters;SCADA (System Control and Data Acquisition);Distribution Analysis;Distribution Automation;Distribution State Estimation;Load Flow Analysis;Real Time Distribution Feeder Analysis;Active Grid Management;Big Data},
doi={10.1109/REPCon.2013.6681858},
ISSN={2153-3636},
month={April},}
@ARTICLE{7723926,
author={Bharill, Neha and Tiwari, Aruna and Malviya, Aayushi},
journal={IEEE Transactions on Big Data},
title={Fuzzy Based Scalable Clustering Algorithms for Handling Big Data Using Apache Spark},
year={2016},
volume={2},
number={4},
pages={339-352},
abstract={A huge amount of digital data containing useful information, called Big Data, is generated everyday. To mine such useful information, clustering is widely used data analysis technique. A large number of Big Data analytics frameworks have been developed to scale the clustering algorithms for big data analysis. One such framework called Apache Spark works really well for iterative algorithms by supporting in-memory computations, scalability etc. We focus on the design and implementation of partitional based clustering algorithms on Apache Spark, which are suited for clustering large datasets due to their low computational requirements. In this paper, we propose Scalable Random Sampling with Iterative Optimization Fuzzy c-Means algorithm (SRSIO-FCM) implemented on an Apache Spark Cluster to handle the challenges associated with big data clustering. Experimental studies on various big datasets have been conducted. The performance of SRSIO-FCM is judged in comparison with the proposed scalable version of the Literal Fuzzy c-Means (LFCM) and Random Sampling plus Extension Fuzzy c-Means (rseFCM) implemented on the Apache Spark cluster. The comparative results are reported in terms of time and space complexity, run time and measure of clustering quality, showing that SRSIO-FCM is able to run in much less time without compromising the clustering quality.},
keywords={Clustering algorithms;Big data;Algorithm design and analysis;Sparks;Partitioning algorithms;Data mining;Optimization;Fuzzy clustering;iterative computation;big data;Apache Spark;distributed machine learning},
doi={10.1109/TBDATA.2016.2622288},
ISSN={2332-7790},
month={Dec},}
@INPROCEEDINGS{8308334,
author={Hamdi, Sana and Bouazizi, Emna and Faiz, Sami},
booktitle={2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)},
title={A Speculative Concurrency Control in Real-Time Spatial Big Data Using Real-Time Nested Spatial Transactions and Imprecise Computation},
year={2017},
volume={},
number={},
pages={534-540},
abstract={Real-time spatial applications have become more and more important. These applications are innovating in the way we live. As a result, there is a huge amount of real-time spatial data generated everyday, accessed simultaneously by two types of transactions, Update transactions and User transactions (continuous requests). In real-time spatial Big Data, the performance can be increased by allowing concurrent execution of transactions. This activity is called concurrency control. The concurrency control algorithm must be used to ensure serializability of transaction scheduling and to maintain data consistency.Several works have been done in this area, but without holding into account the existence of a huge volume of data. In this paper, we apply the technique of imprecise computation for real-time spatial nested transactions. We consider that imprecise transaction consist that a User transaction is decomposed logically into a one mandatory sub-transaction and one or more optional sub-transactions and an Update transaction consists only of a single mandatory sub-transaction. We propose an improvement of an existing Two-Shadow Speculative Concurrency Control (SCC-2S) with priority with the use of the imprecise real-time spatial transaction. Our main objectives are: to guarantee the data freshness, to enhance the deadline miss ratio even in the presence of conflicts and unpredictable workloads and finally to satisfy the requirements of users by the improving of the quality of service (QoS).},
keywords={Real-time systems;Concurrency control;Spatial databases;Data models;Big Data;Computational modeling;Real-Time Spatial Big Data;User Transaction;Up-date Transaction;Nested Transaction;Speculative Concurrency Control;Imprecise Computation;Quality of Service},
doi={10.1109/AICCSA.2017.8},
ISSN={2161-5330},
month={Oct},}
@INPROCEEDINGS{7530040,
author={Morshed, Sarwar Jahan and Rana, Juwel and Milrad, Marcelo},
booktitle={2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
title={Open Source Initiatives and Frameworks Addressing Distributed Real-Time Data Analytics},
year={2016},
volume={},
number={},
pages={1481-1484},
abstract={The continuous evolution of digital services, is resulting in the generation of extremely large data sets that are created in almost real time. Exploring new opportunities for improving the quality of these digital services, as well as providing better-personalized experiences to digital users are two major challenges to be addressed. Different methods, tools, and techniques existed today to generate actionable insights from digital services data. Traditionally, big data problems are handled on historical data-sets. However, there is a growing demand on real-time data analytics to offer new services to users and to provide pro-active customers' care, personalized ads, emergency aids, just to give a few examples. Spite of the fact that there are few existing frameworks for real-time analytics, however, utilizing those for solving distributed real-time big data analytical problems stills remains a challenge. Existing real-time data analytics (RTDA) frameworks are not covering all the features that requires for distributed computation in real-time. Therefore, in this paper, we present a qualitative overview and analysis on some of the mostly used existing RTDA frameworks. Specifically, Apache Spark, Apache Flink, Apache Storm, and Apache Samza are covered and discussed in this paper.},
keywords={Real-time systems;Sparks;Data analysis;Distributed databases;Storms;Big data;Real-time;data analytics;big data;streaming data;data analytics framework;distributed real-time data analysis},
doi={10.1109/IPDPSW.2016.152},
ISSN={},
month={May},}
@INPROCEEDINGS{8859457,
author={Lin, Jun and Liu, Lan},
booktitle={2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
title={Research on Security Detection and Data Analysis for Industrial Internet},
year={2019},
volume={},
number={},
pages={466-470},
abstract={Industrial Internet platform needs to solve a series of problems, such as access of multi-type industrial equipment, multi-source industrial data integration, massive data management and processing, industrial Internet security and so on. This paper builds industrial big data analysis algorithm library based on domain knowledge modeling and big data analysis of industrial data. Through the analysis of the behavior characteristics of industrial internet network traffic data, this paper studies the method of selecting traffic characteristics of events in the industrial Internet; establishes the propagation and evolution model of security events in the industrial Internet, and builds a traceability map of security event propagation; This study combines the characteristics of large data volume and centralized control of future industrial Internet to reduce the complexity of security event detection and analysis. It has reference value for industrial Internet controller to formulate node routing strategy.},
keywords={Internet;Security;Big Data;Data models;Analytical models;Production;Classification algorithms;Industrial Internet, Future network, Big Data, Security Detection},
doi={10.1109/QRS-C.2019.00089},
ISSN={},
month={July},}
@INPROCEEDINGS{9377995,
author={Singh, Maninder and Anu, Vaibhav},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Graph Based CIA in Requirements Engineering},
year={2020},
volume={},
number={},
pages={5828-5830},
abstract={We developed a novel Vertical Breadth-First Search All-path graph algorithm that utilizes vertical data structures to find all-length paths (including shortest paths) for all pairs of vertices in a graph. In the current research we propose an approach to apply our All-path algorithm to perform Change Impact Analysis during Requirements Engineering. This article presents a roadmap of our future research plan to apply our All-path algorithm for software quality improvement.},
keywords={Conferences;Software algorithms;Software quality;Big Data;Data structures;Requirements engineering},
doi={10.1109/BigData50022.2020.9377995},
ISSN={},
month={Dec},}