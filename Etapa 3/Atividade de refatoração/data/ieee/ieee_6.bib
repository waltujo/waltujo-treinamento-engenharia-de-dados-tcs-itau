@INPROCEEDINGS{8121916,
author={Sattart, Farook and McQuay, Colter and Driessen, Peter F.},
booktitle={2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)},
title={Marine mammal sound anomaly and quality detection using multitaper spectrogram and hydrophone big data},
year={2017},
volume={},
number={},
pages={1-6},
abstract={This paper proposes a novel method for anomaly and quality detection of marine mammal sounds using multitaper spectrogram and hydrophone big data. The proposed method is aimed to automatically detect anomaly, such as high-frequency vessel noise, Doppler noise, in sperm whale (SPW) sound as well as the quality of the sound. A new signature function derived from a multi-taper spectrogram is able to detect the anomaly in the data and a new anomaly distortion measure can detect the sound quality into good/bad. The proposed method, is tested with 1905 minutes of data spanning a single year, and using a human operator's annotations. The experimental results reveal that the proposed multitaper spectrogram based approach is efficient in detecting anomaly as well as sperm whale sound quality for hydrophone big data and high detection accuracy (>85%) is achieved for raw input hydrophone data.},
keywords={Spectrogram;Whales;Sonar equipment;Big Data;Acoustic distortion;Anomaly detection;Anomaly detection;Hydrophone Big data;Quality detection;Multitaper spectrogram;Marine mammal sound;Sperm whale},
doi={10.1109/PACRIM.2017.8121916},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7876389,
author={Pandey, Manish Kumar and Subbiah, Karthikeyan},
booktitle={2016 IEEE International Conference on Computer and Information Technology (CIT)},
title={A Novel Storage Architecture for Facilitating Efficient Analytics of Health Informatics Big Data in Cloud},
year={2016},
volume={},
number={},
pages={578-585},
abstract={Analytics of health big data are very crucial for providing cost effective quality health care. Over recent years, the analytics on healthcare big data has evolved into a challenging task for getting insights into a very large data set for improving the health services. This enormous amount of data, which is being generated incessantly over a long period of time, has put a great deal of stress on the write performance as well as on scalability. Moreover, there is a requirement of efficient storage and meaningful processing of these data which is an another challenging issue. The traditional relational databases, which were used in the storage of health data, are now unable to handle due to its massive and varied nature. Besides, these databases have some inherent weakness in terms of scalability, storing varied data format, etc. So there is a necessity for a new kind of data storage management system. This paper proposes a new big data storage architecture consisting of application cluster and a storage cluster to facilitate read/write/update speedup as well as data optimization. The application cluster is used to provide efficient storage and retrieval functions from the users. The storage services will be provided through the storage cluster.},
keywords={Medical services;Big data;Cloud computing;Computer architecture;Data models;Informatics;Scalability;Health Big Data;In Memory Storage System;In Memory Data Processing System},
doi={10.1109/CIT.2016.86},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9321955,
author={Riznyk, Volodymyr},
booktitle={2020 IEEE 15th International Conference on Computer Sciences and Information Technologies (CSIT)},
title={Methods of Big Vector Data Processing Under Toroidal Coordinate Systems},
year={2020},
volume={1},
number={},
pages={105-108},
abstract={Methods of big vector data (BVD) processing under toroidal coordinate systems for development of high performance vector information technologies with improving the quality indices of the technologies presents in this paper. These methods involve novel mathematical principle relating to the minimizing of structural elements in toroidal coordinate systems, including the appropriate algebraic constructions such as vector different sets of cyclic groups and "GlorytoUkraineStar" (GUS) combinatorial configurations, namely the concept of Perfect Toroidal Codes (PTC)s. These codes form binary coding system under two- or multidimensional ring axes of toroidal reference grid, which provides big data processing of multidimensional arrays, using the smallest possible basis.},
keywords={Data processing;Big Data;Two dimensional displays;Data models;Spatial databases;Manifolds;Encoding;big data;combinatorial configuration;toroidal system;reference grid;vector code;entity-attribute-value list;optimization},
doi={10.1109/CSIT49958.2020.9321955},
ISSN={2766-3639},
month={Sep.},}
@INPROCEEDINGS{7459050,
author={Krishnan, Shankar M.},
booktitle={2016 32nd Southern Biomedical Engineering Conference (SBEC)},
title={Application of Analytics to Big Data in Healthcare},
year={2016},
volume={},
number={},
pages={156-157},
abstract={In the current age of smart phones and wearable devices, vast amounts of patient health data files forming Big Data are being placed into large databases where they can be accessed by multiple users including doctors, caregivers and patients. The estimated spending on healthcare in 2015 in the U.S. is around $3.2 trillion, which triggers the question of improvement of patient care while containing the costs. The objective of the present study is to review a few applications of analytics of Big Data in the healthcare field and the associated outcomes. Big Data is generally characterized by the volume, velocity, variety and veracity of complex data. Many hospitals have applied analytics to big data from various sources including patient health records to achieve overall improvement in healthcare. Operationally, most of the pertinent data of patients are made available on demand so doctors can see how other treatments have worked globally and apply relevant results to facilitate better decision making and interventions. Making proper use of big data analytics in healthcare can lead to improvement in care delivery coupled with significant cost savings. Concurrent challenges to be addressed include accessibility, privacy, security, usability, implementation costs, transportability, interoperability, and standardization. In conclusion, employing efficient and streamlined analytics to big data will contribute to quick and accurate diagnosis, appropriate treatment, reduced costs and improved overall healthcare quality.},
keywords={Big data;Hospitals;Cancer;Diseases;Medical diagnostic imaging;Heart;Big Data},
doi={10.1109/SBEC.2016.88},
ISSN={},
month={March},}
@INPROCEEDINGS{8258329,
author={Itoh, Masahiko and Yokoyama, Daisaku and Toyoda, Masashi and Kitsuregawa, Masaru},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Optimal viewpoint finding for 3D visualization of spatio-temporal vehicle trajectories on caution crossroads detected from vehicle recorder big data},
year={2017},
volume={},
number={},
pages={3426-3434},
abstract={Traffic accidents are still troubling our society. The number of drive recorders sold has increased, and therefore we can collect large-scale vehicle recorder data to be used to support traffic safety. We have developed a system for detecting potentially risky crossroads on the basis of vehicle recorder data, road shapes, and weather information. Visualization combining space and time in a single display called a “space time cube (STC)” helps us to understand and analyze spatio-temporal mobility data on caution crossroads. The STC enables us to simultaneously explore not only shapes and positions of vehicle trajectories but also their temporal distributions. However, it is difficult for users to manually find good viewpoints for understanding such characteristics of trajectories. In this paper, we propose an optimal viewpoint selection method for visualizing spatio-temporal characteristics of vehicle trajectories on a large set of crossroads using an STC. Major contributions of this paper are as follows: (1) We provide an algorithm based on viewpoint entropy weighted by angles of trajectories with a horizontal line as a measure of a viewpoint quality on a projected 2D image. (2) We demonstrate our solution can be adapted to crossroads with different trajectory shapes. We also extend the proposed method to find an optimal viewpoint for multiple crossroads. (3) We verify the proposed method through users' evaluations. (4) We construct an overviewing catalog of potentially risky crossroads detected from real vehicle recorder big data to discuss and analyze them with stakeholders.},
keywords={Trajectory;Shape;Three-dimensional displays;Data visualization;Two dimensional displays;Entropy;Big Data},
doi={10.1109/BigData.2017.8258329},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6755345,
author={Liu, Chang and Ranjan, Rajiv and Zhang, Xuyun and Yang, Chi and Georgakopoulos, Dimitrios and Chen, Jinjun},
booktitle={2013 IEEE 16th International Conference on Computational Science and Engineering},
title={Public Auditing for Big Data Storage in Cloud Computing -- A Survey},
year={2013},
volume={},
number={},
pages={1128-1135},
abstract={Data integrity is an important factor to ensure in almost any data and computation related context. It serves not only as one of the qualities of service, but also an important part of data security and privacy. With the proliferation of cloud computing and the increasing needs in big data analytics, verification of data integrity becomes increasingly important, especially on outsourced data. Therefore, research topics related to data integrity verification have attracted tremendous research interest. Among all the metrics, efficiency and security are two of the most concerned measurements. In this paper, we provide an analysis on authenticator-based efficient data integrity verification. we will analyze and provide a survey on the main aspects of this research problem, summarize the research motivations, methodologies as well as main achievements of several of the representative approaches, then try to bring forth a blueprint for possible future developments.},
keywords={Information management;Data handling;Data storage systems;Security;Cloud computing;Distributed databases;Cascading style sheets;cloud computing;big data;data security;integrity verification;public auditing},
doi={10.1109/CSE.2013.164},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7004392,
author={Bhardwaj, Ruchie and Sethi, Adhiraaj and Nambiar, Raghunath},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Big data in genomics: An overview},
year={2014},
volume={},
number={},
pages={45-49},
abstract={Studies show that healthcare industry in U.S. alone could save billions of dollars by utilizing big data and analytics technologies. Big Data can improve operational efficiencies, help predict and plan responses to disease epidemics, improve the quality of monitoring of clinical trials, and optimize healthcare spending at all levels from patients to hospital systems to governments. Another key area is genomics sequencing which is expected to be the future of healthcare. In this paper the authors look at the opportunities, work in progress and challenges of genomics with emerging big data and analytics.},
keywords={Bioinformatics;Genomics;Big data;Sequential analysis;DNA;Diseases;Big Data;Healthcare;Genomics},
doi={10.1109/BigData.2014.7004392},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7285809,
author={Lambert-Torres, Germano and Rossi, Ronaldo and Coutinho, Maurilio Pereira and de Moraes, Carlos Henrique Valerio and Borges da Silva, Luiz Eduardo},
booktitle={2015 IEEE Power & Energy Society General Meeting},
title={Some discussions about data in the new environment of power systems},
year={2015},
volume={},
number={},
pages={1-5},
abstract={The objective of this paper is to present some discussions about data treatment in the new environment existing in the power systems and smart-grids. The paper presents some aspects of the data acquisition, communication, and security, and data amount, management, and mining. In this paper, some ideas are proposed to inspire discussions about the above themes. The goal of these discussions is to provide energy with a better quality to the consumers without disruption and mitigating all sorts of contingencies involving the distribution and transmission systems. Also some discussions about data treatment in power control center are also made involved aspects of big data.},
keywords={Data acquisition;Data mining;Big data;Companies;Smart grids;Databases;Big data;Data acquisition;Data communication;Load management;System monitoring},
doi={10.1109/PESGM.2015.7285809},
ISSN={1932-5517},
month={July},}
@ARTICLE{9082104,
author={Darwish, Tasneem S. J. and Bakar, Kamalrulnizam Abu and Kaiwartya, Omprakash and Lloret, Jaime},
journal={IEEE Transactions on Vehicular Technology},
title={TRADING: Traffic Aware Data Offloading for Big Data Enabled Intelligent Transportation System},
year={2020},
volume={69},
number={7},
pages={6869-6879},
abstract={Todays' Intelligent Transportation System (ITS) applications majorly depend on either limited neighbouring traffic data or crowd sourced stale traffic data. Enabling big traffic data analytics in ITS environments is a step closer towards utilizing significant traffic patterns and trends for making more precise and intelligent decisions particularly in connected autonomous vehicular environments. Towards this end, this paper presents a Traffic Aware Data Offloading (TRADING) approach for big traffic data centric ITS applications in connected autonomous vehicular environments. Specifically, TRADING balances offloading data traffic among gateways focusing on vehicular traffic and network status in the vicinity of gateways. In addition, TRADING mitigates the effect of gateway advertisement overhead to liberate the transmission channels for traffic big data transmission. The performance of TRADING is comparatively evaluated in a realistic simulation environment by considering gateway access overhead, load distribution among gateways, data offloading delay, and data offloading success ratio. The comparative performance evaluation results show some significant developments towards enabling big traffic data centric ITS.},
keywords={Logic gates;Big Data;Quality of service;Delays;Real-time systems;Safety;Roads;Big data;gateway;intelligent transportation systems;VANET;vehicle-to-internet},
doi={10.1109/TVT.2020.2991372},
ISSN={1939-9359},
month={July},}
@INPROCEEDINGS{9378457,
author={Fulp, Megan Hickman and Biswas, Ayan and Calhoun, Jon C.},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Combining Spatial and Temporal Properties for Improvements in Data Reduction},
year={2020},
volume={},
number={},
pages={2654-2663},
abstract={Due to I/O bandwidth limitations, intelligent in situ data reduction methods are needed to enable post-hoc workflows. Current state-of-the-art sampling methods save data points if they deem them spatially or temporally important. By analyzing the properties of the data values at each time-step, two consecutive steps may be very similar. This research follows the notion that if neighboring time-steps are very similar, samples from both are unnecessary, which leaves storage for adding more useful samples. Here, we present an investigation of the combination of spatial and temporal sampling to drastically reduce data size without the loss of valuable information. We demonstrate that, by reusing samples, our reconstructed data set reduces the overall data size while achieving a higher post-reconstruction quality over other reduction methods.},
keywords={Conferences;Bandwidth;Big Data;Sampling methods;Spatial databases;Data Reduction;Data Sampling;Importance Sampling;Feature Preservation},
doi={10.1109/BigData50022.2020.9378457},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8029304,
author={Ding, Junhua and Wang, Jiabin and Kang, Xiaojun and Hu, Xin-Hua},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={Building an SVM Classifier for Automated Selection of Big Data},
year={2017},
volume={},
number={},
pages={15-22},
abstract={The quality of big data could great impact the value extracted from the data. Automated filtering of noisy data from big data is an ideal approach for improving the quality of big data. However, due to large volume and variety of big data, automated filtering of noisy data from big data is a grand challenging task. In this paper, we propose a support vector machine based approach for automated classification of big data so that the noisy data are classified as separated categories from the regular data. In order to improve the classification accuracy and training performance, we design an experiment for improving the classification model through finding the optimized learning feature set and an approach for iteratively improving the quality of the training data set. We conducted a thorough experimental study of automated classification of massive image data of biology cells to explain the approach of automated selection of big data and demonstrate its effectiveness. Finally, we compare the performance of the SVM based classification and a deep learning based classification of the same data set. The proposed approach and experience collected from the experimental study can help big data researchers and practitioners to design strategies for improving the quality of big data, designing high performance classifier, and building tools for automated selection of big data.},
keywords={Diffraction;Support vector machines;Big Data;Noise measurement;Training data;Machine learning;Buildings;machine learning;support vector machine;diffraction image;GLCM;feature selection;deep learning;big data},
doi={10.1109/BigDataCongress.2017.12},
ISSN={},
month={June},}
@INPROCEEDINGS{9257251,
author={Noskievičová, Darja and Smajdorová, Tereza and Tylečková, Eva},
booktitle={2020 21th International Carpathian Control Conference (ICCC)},
title={Statistical Process Control in Big Data Environment},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Big data analysis tools are an inevitable part of instruments and methods for monitoring and predicting the longitudinal performance of the processes in the production systems of the future, based on the deep automatization and overall digitalization. From this point of view statistical process control (SPC) will continue to be very effective method for meeting these goals. But there must be done some modifications. This paper deals with such possible modifications of SPC. In the first part of the paper the stress is put on various methods that can be integrated into SPC to meet new challenges in collecting, analysing and interpreting data (control charts for high yield processes, multivariable approaches, profile monitoring, data mining tools including machine learning methods, nonparametric control charts). SW for the selected discussed methods is also mentioned. The second part of the paper is devoted to the nonparametric methods of SPC and to the methodology of their practical application.},
keywords={Monitoring;Control charts;Process control;Data mining;Unsupervised learning;Mood;Big Data;big data;statistical process control;automatization;digitalization},
doi={10.1109/ICCC49264.2020.9257251},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6846857,
author={Zhong, Ray Y. and Huang, George Q. and Dai, Qingyun},
booktitle={Proceedings of the 2014 IEEE 18th International Conference on Computer Supported Cooperative Work in Design (CSCWD)},
title={A big data cleansing approach for n-dimensional RFID-Cuboids},
year={2014},
volume={},
number={},
pages={289-294},
abstract={Radio Frequency Identification (RFID) technology has been widely used in manufacturing sites for supporting the shopfloor management. Huge amount of RFID-enabled production data has been generated. In order to discover invaluable information and knowledge from the RFID big data, it is necessary to cleanse such dataset since there is large number of noises. This paper uses n-dimensional RFID-Cuboids to establish the data warehouse. A big data cleansing approach is proposed to detect, remove and tidy the RFID-Cuboids so that the reliability and quality of dataset could be ensured before knowledge discovery. Experiments and discussions are carried out for validating the proposed approach. It is observed that the proposed big data cleansing approach outperforms other methods like statistics analysis in terms of finding incomplete and missing cuboids.},
keywords={Radiofrequency identification;Materials;Big data;Manufacturing;Data warehouses;Logistics;big data;data cleansing;RFID;cuboid;n-dimensional},
doi={10.1109/CSCWD.2014.6846857},
ISSN={},
month={May},}
@INPROCEEDINGS{9434655,
author={Xing, Qian},
booktitle={2020 2nd International Conference on Economic Management and Model Engineering (ICEMME)},
title={CFO’s working as the Board Secretary concurrently and corporate disclosure quality: Based on Big Data Samples and Economic Model},
year={2020},
volume={},
number={},
pages={911-914},
abstract={This paper took the selected data listed companies in Shenzhen Stock Exchange in 2008-2015 as big data samples to study the relationship between the CFO's working as the Board Secretary concurrently and corporate disclosure quality. Through statistical analysis and economic model, it transforms qualitative questions into quantitative questions. The results indicate that the CFO's doubling as the Board Secretary can distinctly improve the quality of corporate disclosure in listed companies.},
keywords={Analytical models;Statistical analysis;Companies;Transforms;Big Data;Data models;Stock markets;CFO;the Board Secretary;Corporate Disclosure Quality},
doi={10.1109/ICEMME51517.2020.00185},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8769451,
author={Baeissa, Omar and Noaman, Amin Y. and Ragab, Abdul Hamid M. and Hagag, Asmaa},
booktitle={2019 2nd International Conference on Computer Applications & Information Security (ICCAIS)},
title={Minimizing Prediction Time for Catheter-Associated Urinary Tract Infection Using Big Data Mining Model},
year={2019},
volume={},
number={},
pages={1-5},
abstract={This paper focuses on minimizing prediction time for Catheter-Associated Urinary Tract Infection (CAUTI) as one of the main types of Healthcare Associated Infections (HAIs) through a big data analytics model. Big data raises the bar as a result of additional features. It is mainly characterized by tremendous amount of data that is composed of different forms. It also deals with the rapid data flow rate that is generated from multiple sources, and to top it off the quality of the data is questionable. Data mining (DM) approach consumes significantly less time, provides higher accuracy, and prevents personal subjective decisions. The paper evaluates seven data mining algorithms with real patients dataset. It includes more than 28,000 cases for a period of five years. The modeling process considers the latest definition of the Centers for Disease Control and Prevention (CDC), published in January 2019. The model is evaluated through calculations of different assessment factors such as accuracy, computation speed, and precision (true positive and true negative). The research results show the best suitable algorithm is Naïve Bayes. It overcomes the other data mining techniques utilized in similar works.},
keywords={Medical services;Data mining;Prediction algorithms;Big Data;Data models;Sensitivity;Biomedical imaging;Big Data Analytics;Data Mining;Healthcare Associated Infections;Catheter-Associated Urinary Tract Infection},
doi={10.1109/CAIS.2019.8769451},
ISSN={},
month={May},}
@INPROCEEDINGS{9263243,
author={Yousfi, Aola and El Yazidi, Moulay Hafid and Zellou, Ahmed},
booktitle={2020 International Conference on Advanced Computer Science and Information Systems (ICACSIS)},
title={HASSO: A Highly-Automated Source Selection and Ordering System Based on Data Quality Factors},
year={2020},
volume={},
number={},
pages={155-164},
abstract={Big data integration gives access to a large number of data sources through a unified user interface. Answers include high-quality data, medium-quality data, and low-quality data. Selecting a subset of high accurate and consistent data sources and ordering them appropriately is critical to obtain as many high-quality answers as possible right after querying few data sources. However, the process of selecting and ordering data sources can be quite complicated and can present several challenges. The main challenge faced during that process is identifying the most adequate data quality factors to consider. In this paper, we present HASSO, a Highly-Automated Source Selection and Ordering System based on data quality factors. To produce consistent and high accurate answers, HASSO identifies, for each data source, its domain, data consistency and data accuracy using the schema matches. To maximize the total number of complete and non-redundant answers returned right after querying a small number of data sources, HASSO orders data sources in terms of their data overlap and in a decreasing order of their overall coverage. Experimental results in real-world domains show that HASSO produces high-quality answers at high speed.},
keywords={Computer science;Data integrity;User interfaces;Big Data;Information systems;Source Selection;Source Ordering;Data Quality;Big Data Integration;Semantic Similarity},
doi={10.1109/ICACSIS51025.2020.9263243},
ISSN={2330-4588},
month={Oct},}
@INPROCEEDINGS{8592556,
author={Hao, Jiao and Jinming, Chen and Yajuan, Guo},
booktitle={2018 China International Conference on Electricity Distribution (CICED)},
title={Data-driven lean Management for Distribution Network},
year={2018},
volume={},
number={},
pages={701-705},
abstract={This paper proposes a concept of “data-driven, lean-oriented and closed-loop” management for distribution network and explain how to implement this kind of management, as shown in fig.1 Firstly, a big data platform is constructed to integrate and combine the multi-source data. Secondly, big data analysis technologies such as data mining, machine learning and data visualization are applied to solve problems in distribution network production. For example, accurate location of the fault can be found with help of multisource information from different devices and systems. And we can also be aware of the risk points in distribution network through history data analysis. Finally, this Paper explains how to promote lean management of distribution network in the fields of asset, operation, maintenance and investment based on the big data platform and big data analysis methods. In addition, the feedback procedure sets up a bridge between application and data collecting, which further improve the data quality. Those management measure have been piloted in several cities in Jiangsu. The result proves that they can improve power supply reliability and reduce operating costs significantly. Two practical cases are given to show how they work.},
keywords={Big Data;Maintenance engineering;Data models;Investment;Fault diagnosis;Poles and towers;data-driven;lean management;closed-loop;big data analysis},
doi={10.1109/CICED.2018.8592556},
ISSN={2161-749X},
month={Sep.},}
@INPROCEEDINGS{7364133,
author={AlJadda, Khalifeh and Korayem, Mohammed and Grainger, Trey},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Improving the quality of semantic relationships extracted from massive user behavioral data},
year={2015},
volume={},
number={},
pages={2951-2953},
abstract={As the ability to store and process massive amounts of user behavioral data increases, new approaches continue to arise for leveraging the wisdom of the crowds to gain insights that were previously very challenging to discover by text mining alone. For example, through collaborative filtering, we can learn previously hidden relationships between items based upon users' interactions with them, and we can also perform ontology mining to learn which keywords are semantically-related to other keywords based upon how they are used together by similar users as recorded in search engine query logs. The biggest challenge to this collaborative filtering approach is the variety of noise and outliers present in the underlying user behavioral data. In this paper we propose a novel approach to improve the quality of semantic relationships extracted from user behavioral data. Our approach utilizes millions of documents indexed into an inverted index in order to detect and remove noise and outliers.},
keywords={Cleaning;Semantics;Data mining;Indexes;Big data;Collaboration;Dentistry},
doi={10.1109/BigData.2015.7364133},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9179628,
author={Tsoumakos, Dimitrios and Giannakopoulos, Ioannis},
booktitle={2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Content-Based Analytics: Moving beyond Data Size},
year={2020},
volume={},
number={},
pages={33-40},
abstract={Efforts on Big Data technologies have been highly directed towards the amount of data a task can access or crunch. Yet, for content-driven decision making, it is not (only) about the size, but about the "right" data: The number of available datasets (a different type of volume) can reach astronomical sizes, making a thorough evaluation of each input prohibitively expensive. The problem is exacerbated as data sources regularly exhibit varying levels of uncertainty and velocity/churn. To date, there exists no efficient method to quantify the impact of numerous available datasets over different analytics tasks and workflows. This visionary work puts the spotlight on data content rather than size. It proposes a novel modeling, planning and processing research bundle that assesses data quality in terms of analytics performance. The main expected outcome is to provide efficient, continuous and intelligent management and execution of content-driven data analytics. Intelligent dataset selection can achieve massive gains on both accuracy and time required to reach a desired level of performance. This work introduces the notion of utilizing dataset similarity to infer operator behavior and, consequently, be able to build scalable, operator-agnostic performance models for Big Data tasks over different domains. We present an overview of the promising results from our initial work with numerical and graph data and respective operators. We then describe a reference architecture with specific areas of research that need to be tackled in order to provide a data-centric analytics ecosystem.},
keywords={Data models;Analytical models;Task analysis;Predictive models;Uncertainty;Biological system modeling;Numerical models;modeling;data quality;big data;Machine Learning;scheduling},
doi={10.1109/BigDataService49289.2020.00013},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8469275,
author={Wang, Jing and Ren, Dayong},
booktitle={2018 2nd IEEE Advanced Information Management,Communicates,Electronic and Automation Control Conference (IMCEC)},
title={Research on Software Testing Technology Under the Background of Big Data},
year={2018},
volume={},
number={},
pages={2679-2682},
abstract={With the progress of science and technology, computer technology continues to improve, the Internet has produced a large amount of data information, mankind has entered the era of “big data”, has great influence in this era of complex software industry development, the software product has penetrated into various industries in various fields of society. Under such a background, one of the problems that people need to solve is the quality, stability and reliability of software products. The key to solve this problem is software testing. This paper describes the current situation of the software testing technology under the background of big data, the purpose of the test, the testing principles, the testing methods, the development process of software testing technology based on the UML model, and the prospect of the development.},
keywords={Software;Software testing;Unified modeling language;Tools;Big Data;Data models;Big data era;software testing;Software testing methods;UML model;Development prospect},
doi={10.1109/IMCEC.2018.8469275},
ISSN={},
month={May},}
@INPROCEEDINGS{9012549,
author={Vandana, B and Kumar, S Sathish},
booktitle={2018 3rd IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)},
title={A Novel Approach using Big Data Analytics to Improve the Crop Yield in Precision Agriculture},
year={2018},
volume={},
number={},
pages={824-827},
abstract={Agriculture is the main work field in India. Farming industry adopts less innovative technology compared to other industries. Information and Communication Technologies provides simple and cost effective techniques for farmers to enable precision agriculture. The work propose a state of the art model in agriculture field which will guide the rural farmers to use Information and Communication technologies (ICT) in agriculture fields. Big data analytics is used to improve the crop yield. It can be customized for precision agriculture to improve the quality of crops which improves the overall production rate.},
keywords={Agriculture;Big Data;Temperature sensors;Internet of Things;Data models;Information and communication technology;Precision Agriculture;Big Data Analytics;ICT},
doi={10.1109/RTEICT42901.2018.9012549},
ISSN={},
month={May},}
@INPROCEEDINGS{8983192,
author={Wang, Shenjie and Wang, Jiayin and Xiao, Xiao and Zhang, Xuanping and Wang, Xuwen and Zhu, Xiaoyan and Lai, Xin},
booktitle={2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
title={GSDcreator: An Efficient and Comprehensive Simulator for Genarating NGS Data with Population Genetic Information},
year={2019},
volume={},
number={},
pages={1868-1875},
abstract={In recent decades, NGS data analysis has become a major research field in bioinformatics, which presents great advantages in many application scenarios. Many algorithms and software were designed for analyzing the NGS data, while simulation datasets are urgently needed for testing software and optimizing their parameter configurations. Thus, a series of NGS data simulators have been published. However, the existing simulators cannot satisfy the requirements from many specific scenarios. First, they do not support many newly discovered variations. Second, complex structural variations are difficult to generate. In addition, along with the increase of population data, it is urgent to increase population information simulation. In this paper, we propose GSDcreator, a comprehensive NGS simulator that overcome the three weaknesses mentioned above. It can produce all known types of variation, where the complex of variations are also supported. Furthermore, it can capture many important real data features including population polymorphism, insert size distribution, adjacent site depth distribution, overall depth distribution, quality score distribution, amplification bias, sequencing errors and so on. It's highlighted that 1000 Genomes Project Database is taken as a reference and integrates population genetic information to simulate population polymorphism. To test the performance, we did a lot of experiments and found that simulated data produced by GSDcreator are quit mimic to the real sequencing data.},
keywords={population genomics;next-generation sequencing data analysis;data simulator;population information},
doi={10.1109/BIBM47256.2019.8983192},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8257990,
author={Axel-Cyrille and Ngomo, Ngonga and Hoffmann, Michael and Usbeck, Ricardo and Jha, Kunal},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Holistic and scalable ranking of RDF data},
year={2017},
volume={},
number={},
pages={746-755},
abstract={The volume and number of data sources published using Semantic Web standards such as RDF grows continuously. The largest of these data sources now contain billions of facts and are updated periodically. A large number of applications driven by such data sources requires the ranking of entities and facts contained in such knowledge graphs. Hence, there is a need for time-efficient approaches that can compute ranks for entities and facts simultaneously. In this paper, we present the first holistic ranking approach for RDF data. Our approach, dubbed HARE, allows the simultaneous computation of ranks for RDF triples, resources, properties and literals. To this end, HARE relies on the representation of RDF graphs as bi-partite graphs. It then employs a time-efficient extension of the random walk paradigm to bi-partite graphs. We show that by virtue of this extension, the worst-case complexity of HARE is O(n5) while that of PageRank is O(n6). In addition, we evaluate the practical efficiency of our approach by comparing it with PageRank on 6 real and 6 synthetic datasets with sizes up to 108 triples. Our results show that HARE is up to 2 orders of magnitude faster than PageRank. We also present a brief evaluation of HARE's ranking accuracy by comparing it with that of PageRank applied directly to RDF graphs. Our evaluation on 19 classes of DBpedia demonstrates that there is no statistical difference between HARE and PageRank. We hence conclude that our approach goes beyond the state of the art by allowing the ranking of all RDF entities and of RDF triples without being worse w.r.t. the ranking quality it achieves on resources. HARE is open-source and is available at http://github.com/dice-group/hare.},
keywords={Resource description framework;Runtime;Data science;Electronic mail;Indexes;Scalability;Semantic Web;Ranking;PageRank;Data Volume;Scalability},
doi={10.1109/BigData.2017.8257990},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8740576,
author={Zan, Songting and Zhang, Xu},
booktitle={2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)},
title={Medical Data Quality Assessment Model Based on Credibility Analysis},
year={2018},
volume={},
number={},
pages={940-944},
abstract={The current main problem of medical data is low accuracy. Although existing data quality evaluation and audit can meet the needs, network security problem will be more serious in the future. As a result, the data quality evaluation model must include data credibility. This paper proposes the medical big data quality evaluation model based on credibility analysis and Analytic Hierarchy Process(AHP). Firstly, analyze the data credibility. After eliminating the unreliable data, calculate the data quality dimensions respectively. Then obtain the data quality evaluation result by integrating all dimensions with AHP. Through simulation, the data quality evaluation model can effectively identify the data that affects the credibility. The data quality evaluation results are improved after removing the untrusted data, besides the amount of data is reduced, which is applicable to the big data scenario.},
keywords={Data models;Data integrity;Big Data;Mathematical model;Analytical models;Analytic hierarchy process;Surgery;data quality evaluation;medical;credibility analysis;analytic hierarchy process},
doi={10.1109/ITOEC.2018.8740576},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9258830,
author={Hu, Qiliang},
booktitle={2020 International Conference on Communications, Information System and Computer Engineering (CISCE)},
title={The Vocational Skills Competition Based on Big Data Analysis Promotes the Research of Students' Vocational Ability},
year={2020},
volume={},
number={},
pages={279-282},
abstract={With the progress of information technology, big data gradually shows its extraordinary value. Big data is used in all walks of life, including education and teaching. The big data on the vocational Skills Competition can analyze the students' mastery of vocational ability. Through big data analysis, I was informed of the projects of China Vocational Skills Competition and the awards of various provinces and cities. It is concluded that the competition of vocational skills is difficult and can train students' various abilities. Therefore, it is proposed to take the vocational skill contest as an opportunity to improve students' vocational ability and teaching quality and promote the development of vocational education.},
keywords={Urban areas;Information systems;Indexes;Handheld computers;Education;Conferences;Big Data;big data;bocational skills competition;vocational ability;ascension},
doi={10.1109/CISCE50729.2020.00062},
ISSN={},
month={July},}
@INPROCEEDINGS{7004210,
author={Mall, Raghvendra and Jumutc, Vilen and Langone, Rocco and Suykens, Johan A.K.},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Representative subsets for big data learning using k-NN graphs},
year={2014},
volume={},
number={},
pages={37-42},
abstract={In this paper we propose a deterministic method to obtain subsets from big data which are a good representative of the inherent structure in the data. We first convert the large scale dataset into a sparse undirected k-NN graph using a distributed network generation framework that we propose in this paper. After obtaining the k-NN graph we exploit the fast and unique representative subset (FURS) selection method [1], [2] to deterministically obtain a subset for this big data network. The FURS selection technique selects nodes from different dense regions in the graph retaining the natural community structure. We then locate the points in the original big data corresponding to the selected nodes and compare the obtained subset with subsets acquired from state-of-the-art subset selection techniques. We evaluate the quality of the selected subset on several synthetic and real-life datasets for different learning tasks including big data classification and big data clustering.},
keywords={Big data;Kernel;Entropy;Standards;Predictive models;Training;Communities},
doi={10.1109/BigData.2014.7004210},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8289792,
author={El-Ghafar, Randa M. Abd and Gheith, Mervat H. and El-Bastawissy, Ali H. and Nasr, Eman S.},
booktitle={2017 13th International Computer Engineering Conference (ICENCO)},
title={Record linkage approaches in big data: A state of art study},
year={2017},
volume={},
number={},
pages={224-230},
abstract={Record Linkage aims to find records in a dataset that represent the same real-world entity across many different data sources. It is a crucial task for data quality. With the evolution of Big Data, new difficulties appeared to deal mainly with the 5Vs of Big Data properties; i.e. Volume, Variety, Velocity, Value, and Veracity. Therefore Record Linkage in Big Data is more challenging. This paper investigates ways to apply Record Linkage algorithms that handle the Volume property of Big Data. Our investigation revealed four major issues. First, the techniques used to resolve the Volume property of Big Data mainly depend on partitioning the data into a number of blocks. The processing of those blocks is parallelly distributed among many executers. Second, MapReduce is the most famous programming model that is designed for parallel processing of Big Data. Third, a blocking key is usually used for partitioning the big dataset into smaller blocks; it is often created by the concatenation of the prefixes of chosen attributes. Partitioning using a blocking key may lead to unbalancing blocks, which is known as data skew, where data is not evenly distributed among blocks. An uneven distribution of data degrades the performance of the overall execution of the MapReduce model. Fourth, to the best of our knowledge, a small number of studies has been done so far to balance the load between data blocks in a MapReduce framework. Hence more work should be dedicated to balancing the load between the distributed blocks.},
keywords={Couplings;Big Data;Programming;Data integration;Task analysis;Data models;Databases;Big Data;Big Data Integration;blocking;entity matching;entity resolution;Hadoop;machine learning;MapReduce;Record Linkage},
doi={10.1109/ICENCO.2017.8289792},
ISSN={2475-2320},
month={Dec},}
@ARTICLE{8667006,
author={Xu, Xuefang and Lei, Yaguo and Li, Zeda},
journal={IEEE Transactions on Industrial Electronics},
title={An Incorrect Data Detection Method for Big Data Cleaning of Machinery Condition Monitoring},
year={2020},
volume={67},
number={3},
pages={2326-2336},
abstract={The presence of incorrect data leads to the decrease of condition-monitoring big data quality. As a result, unreliable or misleading results are probably obtained by analyzing these poor-quality data. In this paper, to improve the data quality, an incorrect data detection method based on an improved local outlier factor (LOF) is proposed for data cleaning. First, a sliding window technique is used to divide data into different segments. These segments are considered as different objects and their attributes consist of time-domain statistical features extracted from each segment, such as mean, maximum and peak-to-peak value. Second, a kernel-based LOF (KLOF) is calculated using these attributes to evaluate the degree of each segment being incorrect data. Third, according to these KLOF values and a threshold value, incorrect data are detected. Finally, a simulation of vibration data generated by a defective rolling element bearing and three real cases concerning a fixed-axle gearbox, a wind turbine, and a planetary gearbox are used to verify the effectiveness of the proposed method, respectively. The results demonstrate that the proposed method is able to detect both missing segments and abnormal segments, which are two typical incorrect data, effectively, and thus is helpful for big data cleaning of machinery condition monitoring.},
keywords={Big Data;Machinery;Feature extraction;Condition monitoring;Data integrity;Fault diagnosis;Cleaning;Condition-monitoring big data;data cleaning;data quality;incorrect data;local outlier factor (LOF)},
doi={10.1109/TIE.2019.2903774},
ISSN={1557-9948},
month={March},}
@INPROCEEDINGS{8258196,
author={Marcu, Ovidiu-Cristian and Costan, Alexandru and Antoniu, Gabriel and Pérez-Hernández, María S. and Tudoran, Radu and Bortoli, Stefano and Nicolae, Bogdan},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Towards a unified storage and ingestion architecture for stream processing},
year={2017},
volume={},
number={},
pages={2402-2407},
abstract={Big Data applications are rapidly moving from a batch-oriented execution model to a streaming execution model in order to extract value from the data in real-time. However, processing live data alone is often not enough: in many cases, such applications need to combine the live data with previously archived data to increase the quality of the extracted insights. Current streaming-oriented runtimes and middlewares are not flexible enough to deal with this trend, as they address ingestion (collection and pre-processing of data streams) and persistent storage (archival of intermediate results) using separate services. This separation often leads to I/O redundancy (e.g., write data twice to disk or transfer data twice over the network) and interference (e.g., I/O bottlenecks when collecting data streams and writing archival data simultaneously). In this position paper, we argue for a unified ingestion and storage architecture for streaming data that addresses the aforementioned challenge. We identify a set of constraints and benefits for such a unified model, while highlighting the important architectural aspects required to implement it in real life. Based on these aspects, we briefly sketch our plan for future work that develops the position defended in this paper.},
keywords={Throughput;Big Data;Data models;Distributed databases;Runtime;Routing;Scalability;Big Data;Streaming;Storage;Ingestion;Unified Architecture},
doi={10.1109/BigData.2017.8258196},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6691553,
author={Han, Rui and Nie, Lei and Ghanem, Moustafa M. and Guo, Yike},
booktitle={2013 IEEE International Conference on Big Data},
title={Elastic algorithms for guaranteeing quality monotonicity in big data mining},
year={2013},
volume={},
number={},
pages={45-50},
abstract={When mining large data volumes in big data applications users are typically willing to use algorithms that produce acceptable approximate results satisfying the given resource and time constraints. Two key challenges arise when designing such algorithms. The first relates to reasoning about tradeoffs between the quality of data mining output, e.g. prediction accuracy for classification tasks and available resource and time budgets. The second is organizing the computation of the algorithm to guarantee producing better quality of results as more budget is used. Little work has addressed these two challenges together in a generic way. In this paper, we propose a novel framework for developing elastic big data mining algorithms. Based on Shannon's entropy, an information-theoretic approach is introduced to reason about how result quality is affected by the allocated budget. This is then used to guide the development of algorithms that adapt to the available time budgets while guaranteeing producing better quality results as more budgets are used. We demonstrate the application of the framework by developing elastic k-Nearest Neighbour (kNN) classification and collaborative filtering (CF) recommendation algorithms as two examples. The core of both elastic algorithms is to use a naïve kNN classification or CF algorithm over R-tree data structures that successively approximate the entire datasets. Experimental evaluation was performed using prediction accuracy as quality metric on real datasets. The results show that elastic mining algorithms indeed produce results with consistent increase in observable qualities, i.e., prediction accuracy, in practice.},
keywords={Approximation algorithms;Data mining;Encoding;Classification algorithms;Algorithm design and analysis;Prediction algorithms;Accuracy;elastic data mining algorithms;quality monotonicity;entropy;R-tree},
doi={10.1109/BigData.2013.6691553},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7363784,
author={Kandogan, Eser and Roth, Mary and Schwarz, Peter and Hui, Joshua and Terrizzano, Ignacio and Christodoulakis, Christina and Miller, Renée J.},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={LabBook: Metadata-driven social collaborative data analysis},
year={2015},
volume={},
number={},
pages={431-440},
abstract={Open data analysis platforms are being adopted to support collaboration in science and business. Studies suggest that analytic work in an enterprise occurs in a complex ecosystem of people, data, and software working in a coordinated manner. These studies also point to friction between the elements of this ecosystem that reduces user productivity and quality of work. LabBook is an open, social, and collaborative data analysis platform designed explicitly to reduce this friction and accelerate discovery. Its goal is to help users leverage each other's knowledge and experience to find the data, tools and collaborators they need to integrate, visualize, and analyze data. The key insight is to collect and use more metadata about all elements of the analytic ecosystem by means of an architecture and user experience that reduce the cost of contributing such metadata. We demonstrate how metadata can be exploited to improve the collaborative user experience and facilitate collaborative data integration and recommendations. We describe a specific use case and discuss several design issues concerning the capture, representation, querying and use of metadata.},
keywords={Metadata;Collaboration;Bioinformatics;Ecosystems;Business;Data analysis;Semantics;metadata;collaboration;data discovery;data analytics},
doi={10.1109/BigData.2015.7363784},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9377972,
author={Celona, John and Halamek, Louis and Seiver, Adam},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Making "Magic" with Engineered Decisions, Data, and Processes: A Hospital Operations Center},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Healthcare in the U.S. is the most expensive in the world by any measure, but not the best. The reasons are many and apply worldwide. The short summary is that technology in healthcare is Space Age, but processes echo medieval artisan work (for a variety of reasons). Big Data is the latest technology to promise vast improvement, but may follow many other technologies in making little or no significant improvement and possibly making matters worse. Decision analysis supplies a philosophy, theory, methodology, and tool set for making better decisions in novel, complex, or uncertain situations. It can be applied to individual or group decisions, including Big Data team projects. Hospital operations centers may achieve the sought-after "Quadruple Aim," with simultaneous improvements in healthcare quality, patient and provider satisfaction, and reduction in cost. Decision analysis offers a framework for the system engineering efforts required, which may include harnessing Big Data. Current thoughts for a development and implementation pathway are described.},
keywords={Hospitals;Conferences;Big Data;Tools;Systems engineering and theory;Decision analysis;Big Data;healthcare;decision analysis;operations center;system engineering;team processes},
doi={10.1109/BigData50022.2020.9377972},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9282471,
author={Orlov, Ivan and Falise, Frédéric},
booktitle={2020 China Semiconductor Technology International Conference (CSTIC)},
title={Quality Control in Sapphire Growing: From Automated Defect Detection to Big Data Approach},
year={2020},
volume={},
number={},
pages={1-3},
abstract={We illustrate how automated scanners visualise internal defects in raw sapphire prior to its processing, and present some defect statistics that Scientific Visual has collected over five years of serving key sapphire suppliers in Europe and Asia. The article illustrates use of defect location and morphology data to reveal trends in sapphire quality, compare production modes, and to find out the optimal parameters for sapphire growth.},
keywords={Visualization;Morphology;Europe;Data visualization;Quality control;Production;Market research},
doi={10.1109/CSTIC49141.2020.9282471},
ISSN={},
month={June},}
@INPROCEEDINGS{7004349,
author={Ahnn, Jong Hoon},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Scalable big data computing for the personalization of machine learned models and its application to automatic speech recognition service},
year={2014},
volume={},
number={},
pages={1-8},
abstract={We observe that the recent advances in big data computing have empowered model-based services such as speech recognition, face recognition, context-aware service, and many other services. Various sources of user's logs can be utilized in remodeling or adapting existing models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner. Recently advances in ASR and big data technologies drive more personalized services in many areas of services. A speaker adaptation is one good example which requires huge computation cost in creating a personalized acoustic model and corresponding language model over 100s millions of Samsung product users. We propose a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. We study an optimal set of execution environments by executing jobs running either on Hadoop 1 or Hadoop 2 cluster, and move forward performance optimization strategies: workflow compaction, file compression, best file system selection among several distributed file systems. We devise a metric for the cost of personalized model creation to compare the efficiency of one cluster with the other cluster, and it provides the estimated total execution time for the given number of machines. We finally introduce our in-house object storage and data storage design, and their high performance compared to state-of-the art systems, optimized for voice-enabled services to effectively support small and large files.},
keywords={Adaptation models;Speech;Computational modeling;Acoustics;Memory;Speech recognition;Big data},
doi={10.1109/BigData.2014.7004349},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9371793,
author={Fernando, Pattinige Ravindra R},
booktitle={2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications (CITISIA)},
title={Improving the quality of education system using Data Science Technologies: Survey},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Education is the most important and silent weapon in a country for both individual and country's economy. However lower level of adoption in the education system, poor decision making with less accuracy levels, adoption to new curriculums or subjects, teaching and learning styles are the main issues in education systems. These factors also have further long-term consequences for a country such as unemployment rates rises high, lack of suitable workforce for the demanding fields, individual dissatisfaction while being unemployed as well as in the community and socially. Unemployment rates are risen in Australia from past few years and this as a factor will be an ongoing issue if the government does not take any further actions to overcome these issues will definitely be direct hit to their economy in relation to work force in the present and future. Therefore the right technology should be implemented in order to obtain extract insights, obtain accurate decisions and high level adoption in education sector, as an example technologies such as data warehousing, big data, data mining, business intelligence and data analytics are in the peak of other industries such as aviation, retail, banking and other financial institutions. The main objective of this project is to facilitate a guide or a review for having data science technologies implemented in education sector in order to accomplish better education, as well as emphasis potential advantages of data technologies if it has been implemented in and around education systems.},
keywords={Education;Warehousing;Decision making;Data science;Data warehouses;Data mining;Unemployment;Data Science;Data warehousing in education;data warehouse potentiality in education sector;big data in education;data mining in education;business intelligence},
doi={10.1109/CITISIA50690.2020.9371793},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9260294,
author={Peng, Xie and Hongmei, Zhang and Lijie, Cui and Ying, Hu},
booktitle={2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Analysis of Computer Network Information Security under the Background of Big Data},
year={2020},
volume={},
number={},
pages={409-412},
abstract={In today's society, under the comprehensive arrival of the Internet era, the rapid development of technology has facilitated people's production and life, but it is also a “double-edged sword”, making people's personal information and other data subject to a greater threat of abuse. The unique features of big data technology, such as massive storage, parallel computing and efficient query, have created a breakthrough opportunity for the key technologies of large-scale network security situational awareness. On the basis of big data acquisition, preprocessing, distributed computing and mining and analysis, the big data analysis platform provides information security assurance services to the information system. This paper will discuss the security situational awareness in large-scale network environment and the promotion of big data technology in security perception.},
keywords={Quality function deployment;Xenon;Smart grids;OWL;Hafnium compounds;Frequency locked loops;Conferences;Big Data;Network Security;Situation Awareness;Parallel computing},
doi={10.1109/ICSGEA51094.2020.00094},
ISSN={},
month={June},}
@ARTICLE{7587347,
author={Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D. and Venugopalan, Janani and Hoffman, Ryan and Wang, May D.},
journal={IEEE Transactions on Biomedical Engineering},
title={–Omic and Electronic Health Record Big Data Analytics for Precision Medicine},
year={2017},
volume={64},
number={2},
pages={263-273},
abstract={Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of -omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present -omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Results: To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating -omic information into EHR. Conclusion: Big data analytics is able to address -omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of -omic and EHR data to improve healthcare outcome. It has long lasting societal impact.},
keywords={Big data;Bioinformatics;Genomics;Data mining;Medical services;DNA;Feature extraction;Big data analytics;bioinformatics;electronic health records (EHRs);health informatics;–omic data;precision medicine},
doi={10.1109/TBME.2016.2573285},
ISSN={1558-2531},
month={Feb},}
@INPROCEEDINGS{7273312,
author={Wang, May D.},
booktitle={2015 IEEE 39th Annual Computer Software and Applications Conference},
title={Biomedical Big Data Analytics for Patient-Centric and Outcome-Driven Precision Health},
year={2015},
volume={3},
number={},
pages={1-2},
abstract={Rapid advancements in biotechnologies such as -omic (genomics, proteomics, metabolomics, lipidomics etc.), next generation sequencing, bio-nanotechnologies, molecular imaging, and mobile sensors etc. accelerate the data explosion in biomedicine and health wellness. Multiple nations around the world have been seeking novel effective ways to make sense of "big data" for evidence-based, outcome-driven, and affordable 5P (Patient-centric, Predictive, Preventive, Personalized, and Precise) healthcare. My main research focus is on multi-modal and multi-scale (i.e. molecular, cellular, whole body, individual, and population) biomedical data analytics for discovery, development, and delivery, including translational bioinformatics in biomarker discovery for personalized care; imaging informatics in histopathology for clinical diagnosis decision support; bionanoinformatics for minimally-invasive image-guided surgery; critical care informatics in ICU for real-time evidence-based decision making; and chronic care informatics for patient-centric health. In this talk, first, I will highlight major challenges in biomedical and health informatics pipeline consisting of data quality control, information feature extraction, advanced knowledge modeling, decision making, and proper action taking through feedback. Second, I will present informatics methodological research in (i) data integrity and integration; (ii) case-based reasoning for individualized care; and (iii) streaming data analytics for real-time decision support using a few mobile health case studies (e.g. Sickle Cell Disease, asthma, pain management, rehabilitation, diabetes etc.). Last, there is big shortage of data scientists and engineers who are capable of handling Big Data. In addition, there is an urgent need to educate healthcare stakeholders (i.e. patients, physicians, payers, and hospitals) on how to tackle these grant challenges. I will discuss efforts such as patient-centric educational intervention, community-based crowd sourcing, and Biomedical Data Analytics MOOC development. Our research has been supported by NIH, NSF, Georgia Research Alliance, Georgia Cancer Coalition, Emory-Georgia Tech Cancer Nanotechnology Center, Children's Health Care of Atlanta, Atlanta Clinical and Translational Science Institute, and industrial partners such as Microsoft Research and HP.},
keywords={Biomedical imaging;Informatics;Bioinformatics;Big data;Cancer;Decision making},
doi={10.1109/COMPSAC.2015.343},
ISSN={0730-3157},
month={July},}
@ARTICLE{6879577,
author={Slavakis, Konstantinos and Giannakis, Georgios B. and Mateos, Gonzalo},
journal={IEEE Signal Processing Magazine},
title={Modeling and Optimization for Big Data Analytics: (Statistical) learning tools for our era of data deluge},
year={2014},
volume={31},
number={5},
pages={18-31},
abstract={With pervasive sensors continuously collecting and storing massive amounts of information, there is no doubt this is an era of data deluge. Learning from these large volumes of data is expected to bring significant science and engineering advances along with improvements in quality of life. However, with such a big blessing come big challenges. Running analytics on voluminous data sets by central processors and storage units seems infeasible, and with the advent of streaming data sources, learning must often be performed in real time, typically without a chance to revisit past entries. Workhorse signal processing (SP) and statistical learning tools have to be re-examined in todays high-dimensional data regimes. This article contributes to the ongoing cross-disciplinary efforts in data science by putting forth encompassing models capturing a wide range of SP-relevant data analytic tasks, such as principal component analysis (PCA), dictionary learning (DL), compressive sampling (CS), and subspace clustering. It offers scalable architectures and optimization algorithms for decentralized and online learning problems, while revealing fundamental insights into the various analytic and implementation tradeoffs involved. Extensions of the encompassing models to timely data-sketching, tensor- and kernel-based learning tasks are also provided. Finally, the close connections of the presented framework with several big data tasks, such as network visualization, decentralized and dynamic estimation, prediction, and imputation of network link load traffic, as well as imputation in tensor-based medical imaging are highlighted.},
keywords={Big data;Data models;Sparse matrices;Signal processing algorithms;Information technology;Data storage;Storage automation;Statistical analysis},
doi={10.1109/MSP.2014.2327238},
ISSN={1558-0792},
month={Sep.},}
@INPROCEEDINGS{6906823,
author={Ismail, Leila and Masud, Mohammad M. and Khan, Latifur},
booktitle={2014 IEEE International Congress on Big Data},
title={FSBD: A Framework for Scheduling of Big Data Mining in Cloud Computing},
year={2014},
volume={},
number={},
pages={514-521},
abstract={Cloud computing is seen as an emerging technology for big data mining and analytics. Cloud computing can provide data mining results in the form of a Software As a Service (SAS). Both performance and quality of mining are fundamentals criteria for the use of a data mining application provided by a Cloud computing environment. In this paper, we propose a Cloud computing framework, which is responsible to distribute and schedule a Cluster-Based data mining application and its data set. The main goal of our proposed framework for scheduling of Big Data Mining (FSBD) is to decrease the overall execution time of the application with minimum loss in mining quality. We consider the Cluster-based data mining technique as a pilot application for our framework. The results show an important speedup with a minimum loss in quality of mining. We obtained a ratio of 2 of the normalized actual makespan vis-a-vis the ideal makespan. The quality of mining scales well with the number of clusters and the increasing size of the dataset. The results are promising, encouraging the adoption of the framework by Cloud providers.},
keywords={Data mining;Cloud computing;Big data;Clustering algorithms;Computational modeling;Scheduling algorithms;Distributed Systems;Cloud Computing;Divisible Load Application;Autonomous Computing;Scheduling;High Performance Computing;Data Mining},
doi={10.1109/BigData.Congress.2014.81},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7474377,
author={El Kassabi, Hadeel T. and Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle={2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Policy-Based QoS Enforcement for Adaptive Big Data Distribution on the Cloud},
year={2016},
volume={},
number={},
pages={225-233},
abstract={Big Data distribution has benefited from the Cloud resources to accommodate application's QoS requirements. In this paper, we propose Big Data distribution scheme that matches the Cloud available resources to guarantee application's QoS given the continuously dynamic and varying resources of the Cloud infrastructure. We developed Two-Level QoS Policies (TLPS) for selecting clusters and nodes while satisfying the client's application QoS. We also proposed an adaptive data distribution algorithm to cope with changing QoS requirements. Experiments have been conducted to evaluate both the effectiveness and the communication overhead of our proposed distribution scheme and the results we have reported are convincing. Other experiments evaluated our TLPS algorithm against other single-based QoS data distribution algorithms and the results show that TLPS algorithm adapts to the customer QoS requirements.},
keywords={Quality of service;Big data;Clustering algorithms;Cloud computing;Distributed databases;Memory;Dynamic scheduling;Big Data;Cloud;QoS;Data placement;selection policy;distributed data centers;cost minimization},
doi={10.1109/BigDataService.2016.16},
ISSN={},
month={March},}
@INPROCEEDINGS{9006596,
author={Shalaginov, Andrii and Kotsiuba, Igor and Iqbal, Asif},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Cybercrime Investigations in the Era of Smart Applications: Way Forward Through Big Data},
year={2019},
volume={},
number={},
pages={4309-4314},
abstract={The omnipresence of smart devices in many aspects of modern everyday life has helped to achieve an enormous level of automation, has ensured sustainable development, and improved quality of life. Over the last decade, such small and portable devices became cheap and easy to deploy in any kind of application. With the full range of versatile connectivity, such technological development also brings multiple challenges related to the security of infrastructure and data. Many individuals, companies, and states worldwide experience the previously unseen scale and scope of the attacks using novel approaches. All these smart applications have also increased the overall attack surface leading to multiple attack vectors available through vulnerabilities. Lack of standards, insufficient security awareness, and new technological landscape does not help either. Considering this, one needs to enhance forensics investigation methodologies, employ novel tools, combine threat intelligence, and integrate forensic readiness. Such measures will help to reduce the total cyber risk through a high level of preparedness for anticipated data-driven crimes in smart applications. We believe that this paper will help in bringing novel focus to existing digital forensics methodologies with a focus on smart applications.},
keywords={Computer crime;Big Data;Smart devices;Digital forensics;smart cities;smart applications;big data;cybersecurity;cyber investigtions},
doi={10.1109/BigData47090.2019.9006596},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9005636,
author={Vo, Phuong-Binh and Phan, Trong-Dat and Dao, Minh-Son and Zettsu, Koji},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Association Model between Visual Feature and AQI Rank Using Lifelog Data},
year={2019},
volume={},
number={},
pages={4197-4200},
abstract={Air Quality Index (AQI) is an indicator of the rank of air pollution that is very vital for the environmental impacts to the public health. In this paper, we propose an association model between visual feature and AQI rank of lifelog data. Visual data (i.e., environmental pictures) and numerical data (i.e., environmental AQI measurements) of lifelog are utilized for the data training stage. The features of the visual data are extracted using a CNN-based method, where the latter are calculated using the standard AQI ranking. The extracted visual features and ranked AQI are combined as the input data for a deep neural network MLP (Multi-layer Perception) to study the association relationship between visual feature and AQI rank. The experimental results show that the proposed method can provide accurate predictions of good or unhealthy AQI ranks from lifelog visual data.},
keywords={Visualization;Feature extraction;Training;Data models;Data mining;Sensors;Numerical models;lifelog data;AQI rank;visual feature extraction;multi-layer perception;deep learning},
doi={10.1109/BigData47090.2019.9005636},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7207250,
author={Xu, Brian and Kumar, Sathish Alampalayam},
booktitle={2015 IEEE International Congress on Big Data},
title={Big Data Analytics Framework for System Health Monitoring},
year={2015},
volume={},
number={},
pages={401-408},
abstract={In this paper, we present our Machine Learning (ML) based big data analytics framework that we tested to improve the quality and performance of Auxiliary Power Units (APU) health monitoring services. We are motivated to develop and apply practical and useful big data analytics technologies for industrial applications in aerospace and aviation. Key contributions of our work include the development and use of our ML algorithms that have been tested and used to analyze multiple data sources and to provide useful insights and increase the ability to predict (1) APU wear from 39%to 56% and (2) APU shutdown events from 19% to 60%. Such system health monitoring can be integrated with the widely used condition based maintenance (CBM) services. Users can use this cloud based analytic toolset and access the big data through any devices (PCs, Tablets, smart phones) anytime and anywhere.},
keywords={Big data;Monitoring;Distributed databases;Maintenance engineering;Data mining;Data models;Aircraft;Big Data Analytics;Machine Learning Algorithms;Map-Reduce;System health Monitoring;CBM},
doi={10.1109/BigDataCongress.2015.66},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7474370,
author={Liu, Hong and Kumar T.K., Ashwin and Thomas, Johnson P. and Hou, Xiaofei},
booktitle={2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Cleaning Framework for BigData: An Interactive Approach for Data Cleaning},
year={2016},
volume={},
number={},
pages={174-181},
abstract={Data is a valuable resource. Proper use of high-quality data can help people make better predictions, analyses and decisions. However, no matter how much effort we put into collecting a good dataset, errors will inevitably creep into the data, making it necessary for data cleaning. This becomes a concern particularly when large-scale heterogeneous data from multiple sources are integrated for other purposes. Data cleaning can be complicated, time-consuming, and expensive, but it is a necessary step in any data-related system since poor-quality data may not be suitable to achieve the intended purposes. The core of our data cleaning system is data association and repairing. Association aims to identify the same object and link with the most associated objects, and repairing is to make a database reliable by fixing errors in the data. For big data applications, we don't necessarily need to use all the data. In most situations, we only need a small subset of the most relevant data. So the goal of association is to convert big raw data into a small subset of the most relevant data that are most useful for a particular application. After we obtain a small amount of relevant data, we also need to further analyze the data to help people digest the data and turn the data into knowledge. We use a number of techniques to associate the data to get useful knowledge for data repairing. Our research shows that data association can effectively help with data repairing. To capture the interaction, we provide a uniform framework that unifies the association and repairing process seamlessly based on context patterns, usage patterns, metadata, and repairing rules.},
keywords={Cleaning;Context;Metadata;Urban areas;Pragmatics;Big data;Maintenance engineering;Data Cleaning;Data Association;Data Repairing},
doi={10.1109/BigDataService.2016.41},
ISSN={},
month={March},}
@INPROCEEDINGS{9140534,
author={Molinari, Andrea and Nollo, Giandomenico},
booktitle={2020 IEEE 20th Mediterranean Electrotechnical Conference ( MELECON)},
title={The quality concerns in health care Big Data},
year={2020},
volume={},
number={},
pages={302-305},
abstract={Health information technology is showing an impressive growing interest towards Big Data. Big Data Analytics is expected to bring important achievements for building sophisticated models, methods and tools that are expected to improve healthcare services and citizen health and wellbeing. In spite of these expectations data quality and analytics methods are not getting the attention they deserve. In this short paper, we aimed to highlight the issues of data quality in the context of Big Data Healthcare Analytics. The common sources of errors, the consequence of these errors, and potential solutions that should be considered to mitigate errors and pitfalls are discussed in the healthcare context.},
keywords={Big Data;Medical services;Data integrity;Biomedical monitoring;Business;Big Data;Analytics;healthcare quality;entity reconciliation},
doi={10.1109/MELECON48756.2020.9140534},
ISSN={2158-8481},
month={June},}
@ARTICLE{8373692,
author={Mohammadi, Mehdi and Al-Fuqaha, Ala and Sorour, Sameh and Guizani, Mohsen},
journal={IEEE Communications Surveys & Tutorials},
title={Deep Learning for IoT Big Data and Streaming Analytics: A Survey},
year={2018},
volume={20},
number={4},
pages={2923-2960},
abstract={In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.},
keywords={Machine learning;Big Data;Data analysis;Economics;Internet of Things;Data mining;Tutorials;Deep learning;deep neural network;Internet of Things;on-device intelligence;IoT big data;fast data analytics;cloud-based analytics},
doi={10.1109/COMST.2018.2844341},
ISSN={1553-877X},
month={Fourthquarter},}
@INPROCEEDINGS{7586377,
author={Joseph, Priyanka and Pamila, J.C.Miraclin Joyce},
booktitle={2016 3rd International Conference on Advanced Computing and Communication Systems (ICACCS)},
title={Survey on incremental and iterative models in big data mining environment},
year={2016},
volume={01},
number={},
pages={1-5},
abstract={It has become increasingly popular to mine big data in order to gain insights to help business decisions or to provide more desirable personalized, higher quality services. They usually include data sets with sizes beyond the ability of commonly used software tools to retrieve, manage, and process data within an adequate elapsed time. So there is big demand for distributed computing framework. As new data and updates are constantly arriving, the results of data mining applications become incomplete over time. In such situations it is desirable to periodically refresh the mined data in order to keep it up-to-date. This paper describes the existing approaches to big data mining which uses these frameworks in an incremental approach that saves and reuses the previous states of computations. It also explores several enhancements introduced in this same framework with iterative mapping characteristics. Gaps in the current methods are identified in this literature review.},
keywords={Data mining;Big data;Computational modeling;Servers;Pipelines;Communication systems;Data models;Big data mining;Incremental data;Iterative computation},
doi={10.1109/ICACCS.2016.7586377},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8991080,
author={Guo, Peng and Yang, Guosheng and Wang, Wenhuan and Yan, Zhoutian and Zhang, Lie and Zhang, Hanfang},
booktitle={2019 International Conference on Electronic Engineering and Informatics (EEI)},
title={Relay Protection Data Integrity Check Method Based on Big Data Association Algorithm},
year={2019},
volume={},
number={},
pages={506-508},
abstract={Relay protection big data creates good conditions for the improvement of professional applications, and data integrity is an important aspect that reflects data quality. The association of relay protection big data is intense. This paper applies Apriori algorithm to mine data relevance and generate association rules. Based on this, the integrity of relay protection data is checked, and the incomplete data is predicted. Taking the relay protection defect data as an example, the paper explores the correlation among 251 items of the six dimensions of protection relay defect data such as type of protection, the severity of the defect, whether the protection is out of operation, the defect location, the cause of the defect, and the equipment manufacturer, completing the processing of incomplete data with great application results.},
keywords={Apriori algorithm;relay protection;defect data;integrity check},
doi={10.1109/EEI48997.2019.00115},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8126163,
author={Attigeri, Girija and Manohara Pai, M.M. and Pai, Radhika M.},
booktitle={2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},
title={Analysis of feature selection and extraction algorithm for loan data: A big data approach},
year={2017},
volume={},
number={},
pages={2147-2151},
abstract={Fraudulent activities in financial institutes can break the economic system of the country. These activities can be identified using clustering and classification algorithms. Effectiveness of these algorithms depend on quality of the input data. Moreover, financial data comes from various sources and forms such as financial statements, stakeholders activities and others. This data from various sources is very vast and unstructured big data. Hence, parallel distributed pre-processing is very significant to improve the quality of the data. Objective of this work is dimensionality reduction considering feature selection and extraction algorithm for large volume of financial data. In this paper an attempt is made to understand the implications of feature extraction and transformation algorithm using Principal Feature Analysis on the financial data. Effect of reduced dimension is studied on various classification algorithms for financial loan data. Parallel and distributed implementation is carried out on IBM Bluemix cloud platform with spark notebook. The results show that reduction of features has significantly improved execution time without compromising the accuracy.},
keywords={Feature extraction;Big Data;Algorithm design and analysis;Support vector machines;Prediction algorithms;Logistics;Mathematical model;Classification;Financial big data;Feature selection and extraction;Support Vector Machine;Logistic regression},
doi={10.1109/ICACCI.2017.8126163},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8817262,
author={Silvestri, Stefano and Esposito, Angelo and Gargiulo, Francesco and Sicuranza, Mario and Ciampi, Mario and De Pietro, Giuseppe},
booktitle={2019 IEEE World Congress on Services (SERVICES)},
title={A Big Data Architecture for the Extraction and Analysis of EHR Data},
year={2019},
volume={2642-939X},
number={},
pages={283-288},
abstract={In the current Italian eHealth scenario, a national IT platform has been designed and developed with the purpose of ensuring the interoperability between the various Electronic Health Record (EHR) systems that have been adopted in the different regions of the country, according to the requirements provided by Italian Laws. In this way, the healthcare providers and the policy makers can acquire and process the data of a patient despite its initial format and source, allowing an improved quality of patient care and optimizing the management of the financial resources. To further exploit this huge resource of health and social data, it is very important to allow the extraction of the complex information buried under the Big Data source enabled by the EHRs, providing the physicians, the researchers and public health policy makers with innovative instruments. Meeting this need is not a trivial task, due to the difficulties of processing different document formats and processing Natural Language text, alongside to the problems related to the data size. In this paper we propose a Big Data architecture that is able to extract information from the documents acquired by the EHRs, integrate and process them, providing a set of valuable data for both physicians and patients, as well as decision makers.},
keywords={Data mining;Interoperability;Standards;Medical services;Task analysis;Big Data;Data integration;Big Data Analytics;Electronic Health Record Interoperability;Natural Language Processing},
doi={10.1109/SERVICES.2019.00082},
ISSN={2642-939X},
month={July},}
@INPROCEEDINGS{8035018,
author={Bhuyan, Fahima Amin and Lu, Shiyong and Ruan, Dong and Zhang, Jia},
booktitle={2017 IEEE International Conference on Services Computing (SCC)},
title={Scalable Provenance Storage and Querying Using Pig Latin for Big Data Workflows},
year={2017},
volume={},
number={},
pages={459-466},
abstract={Provenance refers to the information about the derivation history of a data product. It is important for evaluating the quality and trustworthiness of a data product and ensuring the reproducibility of scientific discoveries. Much research has been done on storing and querying scientific workflow provenance - provenance that is produced in the execution of data-centric scientific workflows. To address the challenges of big data in increasing volume, velocity and variety, a new generation of scientific workflows, called big data workflows are under active research. As both data and workflows increase in their scale, the scale of provenance naturally increases, calling for a new scalable storage and querying infrastructure. This paper leverages Pig Latin, a high-level platform for creating programs that run on Apache Hadoop, and OPQL, a graph-level provenance query language, to build a scalable provenance storage and querying system for big data workflows. Our main contributions are: i) we propose algorithms to translate OPQL constructs to equivalent Pig Latin programs, ii) we extend OPQL, to support the W3C PROV-DM standard provenance model, iii) we develop and evaluate our system on provenance datasets from the UTPB benchmark, and (iv) we create some visual OPQL constructs in the DATAVIEW big data workflow system to facilitate the easy creation of complex OPQL queries in a visual workflow style. Our preliminary experimental study shows the feasibility of our framework for big-data-scale provenance storage and querying.},
keywords={Data models;Big Data;Database languages;Engines;Xenon;Visualization;Standards;Provenance;Pig;Big data;Hadoop Distributed File System;Query language},
doi={10.1109/SCC.2017.65},
ISSN={2474-2473},
month={June},}
@INPROCEEDINGS{9378498,
author={Conti, Christopher J. and Varde, Aparna S. and Wang, Weitian},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Task quality optimization in collaborative robotics},
year={2020},
volume={},
number={},
pages={5652-5654},
abstract={This paper addresses commonsense knowledge (CSK) to enhance human-robot collaboration (HRC) in large scale smart manufacturing. As big data in collaborative robotics grows, CSK in useful to achieve task optimization as depicted in our simulation studies and laboratory experiments, extendable to real-world applications.},
keywords={Collaboration;Big Data;Data models;Task analysis;Robots;Optimization;Smart manufacturing},
doi={10.1109/BigData50022.2020.9378498},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7840916,
author={Zhuang, Yu},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Symmetric repositioning of bisecting K-means centers for increased reduction of distance calculations for big data clustering},
year={2016},
volume={},
number={},
pages={2709-2715},
abstract={Clustering is one of the fundamental data mining procedures. Bisecting K-means (BKM) clustering has been studied to have higher computing efficiency and better clustering quality when compared with the basic Lloyd version of the K-means clustering. Elkan's method of utilizing triangle inequality significantly reduces distance calculations, and is applicable to each K-means iteration without affecting the clustering result of the iteration. Thus, it is natural to think of using the Elkan method to further improve the efficiency of the already efficient bisecting K-means. In this paper, we find that the bisecting K-means allows the repositioning of the two centers of a to-be-bisected cluster to further reduce distance calculations. Based on our heuristic analysis we investigate a repositioning strategy with a set of repositioning parameters, and incorporated the repositioning techniques into the Elkan method for bisecting K-means algorithms. We tested these new algorithms on three big datasets with millions of data points and compared with the straightforwardly combined Elkan-BKM without center repositioning. The experimental results show that the center-repositioned algorithms have fewer distance calculations than the Elkan-BKM algorithms without center repositioning in almost all cases. While our repositioning parameters produced good results for the tested datasets, these parameter sets are derived based on our intuitive thinking and hence they are by no means optimal. The experimental data presented in this paper suggest the potential of achieving higher efficiency if better repositioning parameters can be discovered.},
keywords={Decision support systems;Big data;Conferences;Clustering algorithms;Testing;clustering;bisecting K-means;the Elkan method;cluster center repositioning;large datasets},
doi={10.1109/BigData.2016.7840916},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9060361,
author={Woodbridge, Diane and Hua, Nina and Suarez, Victoria and Reilly, Rebecca and Trinh, Philip and Intrevado, Paul},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={The Impact of Bike-Sharing Ridership on Air Quality: A Scalable Data Science Framework},
year={2019},
volume={},
number={},
pages={1950-1957},
abstract={This research explores the relationship between daily air quality indicator (AQI) values and the daily intensity of bike-share ridership in New York City. The authors designed and deployed a distributed data science framework on which to process and run Elastic Net, Random Forest Regression, and Gradient Boosted Regression Trees. Nine gigabytes of CitiBike ridership data, along with one gigabyte of air quality indicator (AQI) data were employed. All machine learning algorithms identified bike-share ridership intensity as either the most important or the second most important feature in predicting future daily AQIs. The authors also empirically demonstrated that although a distributed platform was necessary to ingest and pre-process the raw 10 gigabytes of data, the actual execution time of all three machine learning algorithms on cleaned, joined, and aggregated data was far faster on a local, commodity computer than on its distributed counterpart.},
keywords={Air quality;Distributed databases;Servers;Urban areas;Cloud computing;Vegetation;Indexes;Distributed computing;Distributed information systems;Distributed databases;Machine learning;Air pollution;Air quality;Intelligent transportation systems},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00341},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7463700,
author={Yang, Zhi and Zhang, Chunping and Hu, Mu and Lin, Feng},
booktitle={2015 IEEE International Conference on Smart City/SocialCom/SustainCom (SmartCity)},
title={OPC: A Distributed Computing and Memory Computing-Based Effective Solution of Big Data},
year={2015},
volume={},
number={},
pages={50-53},
abstract={The Big Data computing is one of hot spots of the internet of things and cloud computing. How to compute efficiently on the Big Data is the key of improving performance. By means of distributed computing or memory computing, many companies and institutions provide some technologies and produces. But they are invalid in the scene in which there are real-time demands in the low-configure cluster. To deal with the problem, this paper provides a distributed computing and memory computing-based effective solution (Objectification Parallel Computing, OPC). In the solution, the data can be formatted into object. Then the objects are distributed stored in the computer memories and parallel compute to complete tasks. The OPC is applied to the Electric Asset Quality Supervision Manage System (EAQSMS) of State Grid of China, the result shows that with PCs the system is efficiently available, reliable, and flexible expansible.},
keywords={Computers;Big data;Parallel processing;Real-time systems;Databases;Distributed computing;Memory management;Big Data;Distributed Memory Computing;Parallel Computing;Objectification Parallel Computing;Architecture of Objectification Parallel Computing},
doi={10.1109/SmartCity.2015.46},
ISSN={},
month={Dec},}
@ARTICLE{6574863,
author={Liu, Chang and Chen, Jinjun and Yang, Laurence T. and Zhang, Xuyun and Yang, Chi and Ranjan, Rajiv and Kotagiri, Ramamohanarao},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={Authorized Public Auditing of Dynamic Big Data Storage on Cloud with Efficient Verifiable Fine-Grained Updates},
year={2014},
volume={25},
number={9},
pages={2234-2244},
abstract={Cloud computing opens a new era in IT as it can provide various elastic and scalable IT services in a pay-as-you-go fashion, where its users can reduce the huge capital investments in their own IT infrastructure. In this philosophy, users of cloud storage services no longer physically maintain direct control over their data, which makes data security one of the major concerns of using cloud. Existing research work already allows data integrity to be verified without possession of the actual data file. When the verification is done by a trusted third party, this verification process is also called data auditing, and this third party is called an auditor. However, such schemes in existence suffer from several common drawbacks. First, a necessary authorization/authentication process is missing between the auditor and cloud service provider, i.e., anyone can challenge the cloud service provider for a proof of integrity of certain file, which potentially puts the quality of the so-called ‘auditing-as-a-service’ at risk; Second, although some of the recent work based on BLS signature can already support fully dynamic data updates over fixed-size data blocks, they only support updates with fixed-sized blocks as basic unit, which we call coarse-grained updates. As a result, every small update will cause re-computation and updating of the authenticator for an entire file block, which in turn causes higher storage and communication overheads. In this paper, we provide a formal analysis for possible types of fine-grained data updates and propose a scheme that can fully support authorized auditing and fine-grained update requests. Based on our scheme, we also propose an enhancement that can dramatically reduce communication overheads for verifying small updates. Theoretical analysis and experimental results demonstrate that our scheme can offer not only enhanced security and flexibility, but also significantly lower overhead for big data applications with a large number of frequent small updates, such as applications in social media and business transactions.},
keywords={Cascading style sheets;Cloud computing;Security;Information management;Data handling;Data storage systems;Scalability;Cloud computing;big data;data security;provable data possession;authorized auditing;fine-grained dynamic data update},
doi={10.1109/TPDS.2013.191},
ISSN={1558-2183},
month={Sep.},}
@INPROCEEDINGS{9102068,
author={Wang, Jiye and Li, Yang and Guo, Jian and Cao, Junwei and Hua, Haochen and Xing, Chunxiao and Qi, Caijuan and Pi, Zhixian},
booktitle={2020 IEEE 7th International Conference on Industrial Engineering and Applications (ICIEA)},
title={Data Quality Analysis Framework and Evaluation Methods for Power System Operation with High Proportion of Renewable Energy Penetration},
year={2020},
volume={},
number={},
pages={687-692},
abstract={Global climate crisis in 21st century pushed countries to move towards energy transformation in generation and consumption. To achieve green and low-carbon energy transformation goals, it is necessary that a large number of renewable energy resources such as wind and solar to be consumed. Renewable energy with intermittent fluctuations in time dimension and agglomerations in spatial dimension increases the complexity of green energy consumption friendly. Therefore, comprehensive data and advanced predictive analysis methods are required to guarantee safety of operation and transactions for renewable energy plants and stations. We can even say that quality of renewable energy data determines the accuracy of prediction and analysis. Firstly, this article analyzes the operation and transaction characteristics of distributed renewable energy plants, and data quality analysis framework for distributed renewable energy operations and transactions was built on the new energy cloud platform. Data information were classified into model parameter and status instance, which are related to dispatching and energy power transaction businesses such as equipment model management, operation monitoring and security analysis, measurement statistics etc. The importance between them is determined according to pairwise comparison. Finally, analytic hierarchy process (AHP) theory was applied to calculate weights for data integrity, accuracy, consistency and timeliness, data quality assessment process and calculation methods were designed, and load series data was used to verify its correctness.},
keywords={Data integrity;Renewable energy sources;Data models;Analytical models;Production;Monitoring;Clouds;renewable energy cloud;data quality analysis;AHP;evaluation metrics},
doi={10.1109/ICIEA49774.2020.9102068},
ISSN={},
month={April},}
@INPROCEEDINGS{9421436,
author={Wang, Xiaofeng and Jiang, Yong and Zhan, Gaofeng and Zhao, Tong},
booktitle={2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)},
title={Quality Analysis and Evaluation Method for Multisource Aggregation Data based on Structural Equation Model},
year={2020},
volume={},
number={},
pages={1279-1282},
abstract={In the era of big data, how to evaluate the data quality of multi-source aggregation data is very important. The reason is that uneven data quality will directly lead to inaccurate or ambiguous data in the database, and indirectly lead to the deviation of subsequent data mining and decision-making. In this paper, structural equation model(SEM) is introduced to explore the effectiveness of various data quality evaluation indicators in data aggregation and finding out internal relationship between them. A new quality evaluation method of multi-source aggregation data is proposed, based on the regression's significance analysis and factor loads of each observation index in the SEM model. The case analysis shows that the proposed method is feasible and can be used to evaluate the quality of multi-source aggregation data adaptively for a long time.},
keywords={Analytical models;Adaptation models;Numerical analysis;Data integrity;Computational modeling;Urban areas;Data aggregation;Data Aggregation;Data Analysis;Quality Evaluation;Structural Equation Model},
doi={10.1109/ICMCCE51767.2020.00280},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9378192,
author={Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Gondalia, Shlok and Duggan, Jerry and Kahn, Michael G.},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={An Autocorrelation-based LSTM-Autoencoder for Anomaly Detection on Time-Series Data},
year={2020},
volume={},
number={},
pages={5068-5077},
abstract={Data quality significantly impacts the results of data analytics. Researchers have proposed machine learning based anomaly detection techniques to identify incorrect data. Existing approaches fail to (1) identify the underlying domain constraints violated by the anomalous data, and (2) generate explanations of these violations in a form comprehensible to domain experts. We propose IDEAL, which is an LSTM-Autoencoder based approach that detects anomalies in multivariate time-series data, generates domain constraints, and reports subsequences that violate the constraints as anomalies. We propose an automated autocorrelation-based windowing approach to adjust the network input size, thereby improving the correctness and performance of constraint discovery over manual and brute-force approaches. The anomalies are visualized in a manner comprehensible to domain experts in the form of decision trees extracted from a random forest classifier. Domain experts can then provide feedback to retrain the learning model and improve the accuracy of the process. We evaluate the effectiveness of IDEAL using datasets from Yahoo servers, NASA Shuttle, and Colorado State University Energy Institute. We demonstrate that IDEAL can detect previously known anomalies from these datasets. Using mutation analysis, we show that IDEAL can detect different types of injected faults. We also demonstrate that the accuracy improves after incorporating domain expert feedback.},
keywords={NASA;Manuals;Big Data;Servers;Decision trees;Anomaly detection;Random forests;Anomaly detection;Autocorrelation;Data quality tests;Explainability;LSTM-Autoencoder;Time series},
doi={10.1109/BigData50022.2020.9378192},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7508137,
author={Bhatia, Mantek Singh and Jaiswal, Arunima},
booktitle={2016 6th International Conference - Cloud System and Big Data Engineering (Confluence)},
title={Empirical analysis of data acquisition techniques: PAPI vs. CAPI},
year={2016},
volume={},
number={},
pages={326-330},
abstract={Data acquisition refers to the act of collecting data on a large scale. This paper presents a qualitative and quantitative analysis of two data acquisition techniques, namely, Pen and Paper Interviewing (PAPI) and Computer-Assisted Personal Interviewing (CAPI). It cites two case studies to clearly define the difference between the two techniques. They have been compared on four factors, namely, cost, time consumed during the whole process, productivity of each interviewer and quality assessment of each techniques (average number of errors occurred per interview). It emphasizes on providing absolute numbers to clearly show the difference between the two techniques.},
keywords={Data acquisition;Interviews;Data collection;Productivity;Data mining;Quality assessment;Mobile handsets;PAPI;CAPI;Data Acquisition Techniques;Data mining;Data Analytics},
doi={10.1109/CONFLUENCE.2016.7508137},
ISSN={},
month={Jan},}
@INPROCEEDINGS{7819372,
author={Niño, Mikel and Sáenz, Fernando and Blanco, José Miguel and Illarramendi, Arantza},
booktitle={2016 IEEE 14th International Conference on Industrial Informatics (INDIN)},
title={Requirements for a big data capturing and integration architecture in a distributed manufacturing scenario},
year={2016},
volume={},
number={},
pages={1326-1329},
abstract={Big Data is one of the key enabling technologies in smart manufacturing, where manufacturing companies aim at leveraging the data generated throughout their processes. The potential of Big Data Analytics is particularly significant in the context of manufacturing companies distributed worldwide. These companies own several manufacturing plants operating the same process in different environments and conditions. This generates massive amounts of data that could be analyzed in order to improve process efficiency and product quality. This paper presents the requirements for an architecture to capture, integrate and analyze the large-scale volumes of data generated in a real-world manufacturing business scenario - a chemical manufacturing sector distributed worldwide-. This scenario serves as a case study for an applied research project on Big Data Analytics. The business nature of this scenario provides those real-life requirements the architecture has to deal with. Existing approaches can be extended to fulfill these requirements, in order to be effectively applied in similar manufacturing business contexts.},
keywords={Manufacturing;Big data;Computer architecture;Business;Cloud computing;Context;Production;Big Data Analytics;Industry 4.0;Smart Manufacturing;Cloud Computing;Predictive Analytics;Prescriptive Control;Decision-Guidance Systems},
doi={10.1109/INDIN.2016.7819372},
ISSN={2378-363X},
month={July},}
@INPROCEEDINGS{8258543,
author={Liu, Lixin and Chen, Jun},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={The influences of deep-sea vision data quality on observational analysis},
year={2017},
volume={},
number={},
pages={4789-4791},
abstract={Deep-sea study by optical observation method is an interdisciplinary subject and faces plenty of difficulties. To find out the influences of vision data quality, characteristic of vision data for deep-sea observation is analyzed, and a deep-sea landing experiment has been implemented. Data quality analyzing based on real deep-sea vision data that collected in the in-situ observation platform is actualized. It is expected that the research on influence mechanism of deep-sea vision quality is beneficial to the detection of region of interest, the judging of animal existence, the classification of species, and the trajectories labeling. Further analyzing on unsupervised deep-sea vision data quality control is necessary.},
keywords={Scattering;Optical sensors;Optical imaging;Fish;Backscatter;deep-sea observation;vision data quality;automatic judging},
doi={10.1109/BigData.2017.8258543},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8845059,
author={Nie-fang, Yu and Xiao-ning, Peng and Chun-qiao, Mi and Xiao-mei, Li},
booktitle={2019 14th International Conference on Computer Science & Education (ICCSE)},
title={Reform of University Computer Foundation Course Based on Mobile Terminals in Big Data Era},
year={2019},
volume={},
number={},
pages={925-928},
abstract={Under the background of big data era, the rise of mobile terminals has also put forward new requirements for the teaching of university computer foundation course. This thesis puts forward some common problems in the teaching practice, and discusses how to promote informatization education under the background of big data based on mobile terminals, thus improving teaching quality and cultivating students into individuals with independent learning, cooperative spirit and innovative ability.},
keywords={Big Data;Testing;Online services;Real-time systems;Computer science;Courseware;Mobile Terminal;Big Data;Informatization Education},
doi={10.1109/ICCSE.2019.8845059},
ISSN={2473-9464},
month={Aug},}
@INPROCEEDINGS{8126976,
author={Han, Weiguo and Jochum, Matthew},
booktitle={2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
title={Latency analysis of large volume satellite data transmissions},
year={2017},
volume={},
number={},
pages={384-387},
abstract={A wide array of time-sensitive satellite data is required in the research and development activities for natural hazard assessment, storms and weather prediction, hurricane tracking, disaster and emergency response, and so on. Identifying and analyzing the latencies of large volumes of real-time and near real-time satellite data is very useful and helpful for detecting transmission issues, managing IT resources, and configuring and optimizing data management systems. This paper introduces how to monitor and collect important timestamps of data transmissions, organize them in a NoSQL database, and explore data latency via a user-friendly dashboard. Taking Sentinel series satellite data as an example, data transmission issues are illustrated and investigated further. Latency analysis and explorations help data providers and managers improve data transmission and enhance data management.},
keywords={Satellites;Data communication;Real-time systems;Databases;Browsers;Big Data;Big Data;Satellite Data;Data Latency;Data Quality;NoSQL;MongoDB},
doi={10.1109/IGARSS.2017.8126976},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{8075416,
author={Xie, Huan and Tong, Xiaohua and Meng, Wen and Wang, Fang and Xu, Xiong},
booktitle={2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)},
title={Multiple stratified sampling strategy for assessing the big remote sensing products},
year={2015},
volume={},
number={},
pages={1-4},
abstract={The number and volume of remote sensing data and its derived products, which are regarded as typical “big data”, have grown exponentially. How to assess the quality of these big remote sensing products become a challenge. As an importance technique, spatial sampling is regarded to be necessary for the quality assessment of remote sensing derived products. This paper proposes an approach of multiple stratified spatial sampling for assessing the remote sensing products, with the aim of resolving the issue of the quality inspection of big remote sensing products. The proposed method improves the sampling accuracy without increasing the sampling size, and the whole procedure is repeatable and easily adopted for the quality inspection of remote sensing derived products.},
keywords={Remote sensing;Inspection;Sampling methods;Sociology;Big Data;Quality assessment;multiple stratified;spatial sampling;quality assessment;remote sensing products;big data},
doi={10.1109/WHISPERS.2015.8075416},
ISSN={2158-6276},
month={June},}
@INPROCEEDINGS{8258222,
author={Lazar, Alina and Jin, Ling and Spurlock, C. Anna and Wu, Kesheng and Sim, Alex},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Data quality challenges with missing values and mixed types in joint sequence analysis},
year={2017},
volume={},
number={},
pages={2620-2627},
abstract={The goal of this paper is to investigate the impact of missing values in categorical time series sequences on common data analysis tasks. Being able to more effectively identify patterns in socio-demographic longitudinal data is an important component in a number of social science settings. However, performing fundamental analytical operations, such as clustering for grouping these data based on similarity patterns, is challenging due to the categorical and multi-dimensional nature of the data, and their corruption by missing and inconsistent values. To study these data quality issues, we employ longitudinal sequence data representations, a similarity measure designed for categorical and longitudinal data, together with state-of-the art clustering methodologies reliant on hierarchical algorithms. The key to quantifying the similarity and difference among data records is a distance metric. Given the categorical nature of our data, we employ an “edit” type distance using Optimal Matching (OM). Because each data record has multiple variables of different types, we investigate the impact of mixing these variables in a single similarity measure. Between variables with binary values and those with multiple nominal values, we find that the ability to overcome missing data problems is harder in the nominal domain versus the binary domain. Additionally, artificial clusters introduced by the alignment of leading missing values can be resolved by tuning the missing value substitution cost parameter.},
keywords={Trajectory;Sequences;Time series analysis;Education;Employment;joint sequence analysis;optimal matching;missing values;time series clustering;data quality},
doi={10.1109/BigData.2017.8258222},
ISSN={},
month={Dec},}
@ARTICLE{7070679,
author={Gu, Lin and Zeng, Deze and Guo, Song and Xiang, Yong and Hu, Jiankun},
journal={IEEE Transactions on Computers},
title={A General Communication Cost Optimization Framework for Big Data Stream Processing in Geo-Distributed Data Centers},
year={2016},
volume={65},
number={1},
pages={19-29},
abstract={With the explosion of big data, processing large numbers of continuous data streams, i.e., big data stream processing (BDSP), has become a crucial requirement for many scientific and industrial applications in recent years. By offering a pool of computation, communication and storage resources, public clouds, like Amazon's EC2, are undoubtedly the most efficient platforms to meet the ever-growing needs of BDSP. Public cloud service providers usually operate a number of geo-distributed datacenters across the globe. Different datacenter pairs are with different inter-datacenter network costs charged by Internet Service Providers (ISPs). While, inter-datacenter traffic in BDSP constitutes a large portion of a cloud provider's traffic demand over the Internet and incurs substantial communication cost, which may even become the dominant operational expenditure factor. As the datacenter resources are provided in a virtualized way, the virtual machines (VMs) for stream processing tasks can be freely deployed onto any datacenters, provided that the Service Level Agreement (SLA, e.g., quality-of-information) is obeyed. This raises the opportunity, but also a challenge, to explore the inter-datacenter network cost diversities to optimize both VM placement and load balancing towards network cost minimization with guaranteed SLA. In this paper, we first propose a general modeling framework that describes all representative inter-task relationship semantics in BDSP. Based on our novel framework, we then formulate the communication cost minimization problem for BDSP into a mixed-integer linear programming (MILP) problem and prove it to be NP-hard. We then propose a computation-efficient solution based on MILP. The high efficiency of our proposal is validated by extensive simulation based studies.},
keywords={Semantics;Cloud computing;Minimization;Big data;Computational modeling;Electronic mail;Flow graphs;big data;stream processing;network cost minimization;VM placement;geo-distributed data centers;Big data;stream processing;network cost minimization;VM placement;geo-distributed data centers},
doi={10.1109/TC.2015.2417566},
ISSN={1557-9956},
month={Jan},}
@INPROCEEDINGS{7363071,
author={dos Anjos, Julio C.S. and Assunção, Marcos D. and Bez, Jean and Geyer, Claudio and de Freitas, Edison Pignaton and Carissimi, Alexandre and Costa, João Paulo C. L. and Fedak, Gilles and Freitag, Felix and Markl, Volker and Fergus, Paul and Pereira, Rubem},
booktitle={2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing},
title={SMART: An Application Framework for Real Time Big Data Analysis on Heterogeneous Cloud Environments},
year={2015},
volume={},
number={},
pages={199-206},
abstract={The amount of data that human activities generate poses a challenge to current computer systems. Big data processing techniques are evolving to address this challenge, with analysis increasingly being performed using cloud-based systems. Emerging services, however, require additional enhancements in order to ensure their applicability to highly dynamic and heterogeneous environments and facilitate their use by Small & Medium-sized Enterprises (SMEs). Observing this landscape in emerging computing system development, this work presents Small & Medium-sized Enterprise Data Analytic in Real Time (SMART) for addressing some of the issues in providing compute service solutions for SMEs. SMART offers a framework for efficient development of Big Data analysis services suitable to small and medium-sized organizations, considering very heterogeneous data sources, from wireless sensor networks to data warehouses, focusing on service composability for a number of domains. This paper presents the basis of this proposal and preliminary results on exploring application deployment on hybrid infrastructure.},
keywords={Big data;Cloud computing;Real-time systems;Monitoring;Data models;Quality of service;Performance evaluation;Big Data;Data Analytics;Cloud Computing;SMEs;Hybrid Clouds},
doi={10.1109/CIT/IUCC/DASC/PICOM.2015.29},
ISSN={},
month={Oct},}
@ARTICLE{7549067,
author={Zhao, Liang and Chen, Zhikui and Hu, Yueming and Min, Geyong and Jiang, Zhaohua},
journal={IEEE Transactions on Big Data},
title={Distributed Feature Selection for Efficient Economic Big Data Analysis},
year={2018},
volume={4},
number={2},
pages={164-176},
abstract={With the rapidly increasing popularity of economic activities, a large amount of economic data is being collected. Although such data offers super opportunities for economic analysis, its low-quality, high-dimensionality and huge-volume pose great challenges on efficient analysis of economic big data. The existing methods have primarily analyzed economic data from the perspective of econometrics, which involves limited indicators and demands prior knowledge of economists. When embracing large varieties of economic factors, these methods tend to yield unsatisfactory performance. To address the challenges, this paper presents a new framework for efficient analysis of high-dimensional economic big data based on innovative distributed feature selection. Specifically, the framework combines the methods of economic feature selection and econometric model construction to reveal the hidden patterns for economic development. The functionality rests on three pillars: (i) novel data pre-processing techniques to prepare high-quality economic data, (ii) an innovative distributed feature identification solution to locate important and representative economic indicators from multidimensional data sets, and (iii) new econometric models to capture the hidden patterns for economic development. The experimental results on the economic data collected in Dalian, China, demonstrate that our proposed framework and methods have superior performance in analyzing enormous economic data.},
keywords={Big data;Econometrics;Feature extraction;Distributed databases;Analytical models;Economic indicators;Feature selection;big data;subtractive clustering;collaborative theory;economy;urbanization},
doi={10.1109/TBDATA.2016.2601934},
ISSN={2332-7790},
month={June},}
@INPROCEEDINGS{9377812,
author={Kanamori, Kenta and Tsubouchi, Kota and Sato, Junichi and Higurashi, Tatsuru},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Location YardStick: Calculation of the Location Data Value Depending on the Users’ Context},
year={2020},
volume={},
number={},
pages={1545-1554},
abstract={These days, many apps acquire location data as a way of estimating the user's behavior. As such, there are privacy concerns in using location data. In particular, users who are concerned about privacy may reduce the frequency of location acquisition or turn off the function, even though it degrades the quality of service. On the other hand, the only options available to users are yes-no or either-or ones such as "Always permit background acquisition" or "Permit only while using the app". For example, users who give permission to "Permit only while using the app" are themselves unable to understand how far their own veil of privacy will be lifted. That is, there are no metrics that can help users to understand the value of their own location data. How should the value of location data be determined? This study attempts to answer that question. The difficulty is that the value of a single point of location data depends on the context, such as how much other location data the app holds or when the location data was obtained. We propose a "Location YardStick" (LYS) that calculates the value of location information fairly in context. We confirmed that the LYS score is close to the user's expectations by comparing its results with those of a large online survey of 1300 people, and we conducted case studies in which we calculated LYS on location data acquired in various actual contexts.},
keywords={Privacy;Waste materials;Data privacy;Quality of service;Big Data;Frequency measurement;Task analysis;Location YardStick;context-aware data collection;location data value},
doi={10.1109/BigData50022.2020.9377812},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8966806,
author={Ganasan, Jaya R},
booktitle={2019 17th International Conference on ICT and Knowledge Engineering (ICT&KE)},
title={Big Data Mining: Managing the Costs of Data Mining},
year={2019},
volume={},
number={},
pages={1-4},
abstract={The amount of data collected and stored in various industries has grown exponentially in the last decade. Data is collected and stored from industries consisting of large consumers such as telecommunications, banking or financial sectors. Further, given the advent of cloud computing and software availability in the cloud being cheaper, smaller industries are utilizing data storage for competitive advantage. Companies increasingly rely on analysis of huge amounts of data to gain a strategic advantage, improving on product quality and providing better services to their end users be it the employee, consumer or customer. A combination of statistical techniques and file management tools once sufficed for analyzing mounds of data. The costs of analysis are often charged out at very high rates for companies that require data analysis and the output is dependent very much on analyzing the correct attributes within large databases to ensure the data analyzed provides the relevant result. The most known technique or tools are the subject of the growing field of knowledge discovery in databases (KDD) [1]. Using business process data mapping (BPDM) to define the targeted data along with the process of knowledge discovery mapping in the database may provide a more targeted approach with much lest costs expended.},
keywords={data mining;process mapping;data analysis;knowledge discovery mapping},
doi={10.1109/ICTKE47035.2019.8966806},
ISSN={2157-099X},
month={Nov},}
@ARTICLE{8537879,
author={Sakr, Sherif and Maamar, Zakaria and Awad, Ahmed and Benatallah, Boualem and Van Der Aalst, Wil M. P.},
journal={IEEE Access},
title={Business Process Analytics and Big Data Systems: A Roadmap to Bridge the Gap},
year={2018},
volume={6},
number={},
pages={77308-77320},
abstract={Business processes represent a cornerstone to the operation of any enterprise. They are the operational means for such organizations to fulfill their goals. Nowadays, enterprises are able to gather massive amounts of event data. These are generated as business processes are executed and stored in transaction logs, databases, e-mail correspondences, free form text on (enterprise) social media, and so on. Taping into these data, enterprises would like to weave data analytic techniques into their decision making capabilities. In recent years, the IT industry has witnessed significant advancements in the domain of Big Data analytics. Unfortunately, the business process management (BPM) community has not kept up to speed with such developments and often rely merely on traditional modeling-based approaches. New ways of effectively exploiting such data are not sufficiently used. In this paper, we advocate that a good understanding of the business process and Big Data worlds can play an effective role in improving the efficiency and the quality of various data-intensive business operations using a wide spectrum of emerging Big Data systems. Moreover, we coin the term process footprint as a wider notion of process data than that is currently perceived in the BPM community. A roadmap towards taking business process data intensive operations to the next level is shaped in this paper.},
keywords={Big Data;Task analysis;Data models;Organizations;Databases;Data mining;Business process analytics;Big Data systems;process data-intensive operations},
doi={10.1109/ACCESS.2018.2881759},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9384689,
author={Zheng, Yabo and Li, Wenzhe and Dong, Xiaoming and Zhou, Ming and Xu, Linzhou},
booktitle={2020 International Conference on Modern Education and Information Management (ICMEIM)},
title={Application Research on Big Data of Military Training in Military Academy Teaching},
year={2020},
volume={},
number={},
pages={357-361},
abstract={As an important driving force for future social development, big data technology is promoting the development of science, technology, industry, military, education and other fields. In the era of big data, military education faces new opportunities and challenges. How to combine big data of military training with the military education has become a new and important topic. At present, the military education is still confronted with such problems as decoupling of teaching content from the military training, single teaching method and low teaching effect. In order to improve the training quality of military talents, and make education closer to the reality of military training, this paper applies big data of military training to military academy education. By using teaching methods such as battle plan making, case teaching, tactic discussion and training evaluation, it can promote the transformation of military training achievements in military academy teaching, realize the comprehensive and deep integration of military training and military academy teaching, improve the cultivating level of military talents in military academy, and achieve modernization of military education based on big data of military training.},
keywords={Training;Industries;Force;Big Data;Information management;Faces;big data;military training;military academy teaching;achievement transformation},
doi={10.1109/ICMEIM51375.2020.00088},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7364123,
author={Maddipatla, Ratna Madhuri and Hadzikadic, Mirsad and Misra, Dipti Patel and Yao, Lixia},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={30 Day hospital readmission analysis},
year={2015},
volume={},
number={},
pages={2922-2924},
abstract={Readmissions to a hospital after procedures are costly and considered to be an indication of poor quality. As Per the Affordable Care Act of 2010, hospitals may be reimbursed at a reduced rate for patients readmitted to a hospital within 30 days of discharge. In this project, we used statistical and machine-learning methods to analyze the Nationwide Inpatient Sample dataset provided by HCUP (Healthcare Cost and Utilization Project) to identify various clinical, demographic and socio-economic factors that play crucial roles in predicting the revenue loss due to readmissions. Three medical conditions, namely chronic obstructive pulmonary disorder (COPD), total hip arthroplasty (THA), and total knee arthroplasty (TKA) have been primarily used for this purpose. Our analysis builds on both non-parametric and parametric statistical models and machine learning techniques such as Decision Tree, Gradient Boosting, Logistic Regression and Neural Networks. We evaluated and compared these models based on Area under ROC (AUC) and misclassification rate. By including visual analytics, this analysis not only enables the hospitals to compute the loss of revenue but also monitors their quality of service in a real-time fashion.},
keywords={Hospitals;Predictive models;Data models;Analytical models;Decision trees;Heart;Multi Model Evaluation;Hospital Readmissions;Area under ROC (AUC);Misclassification rate;Data Visualization},
doi={10.1109/BigData.2015.7364123},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9315078,
author={Cherrington, Marianne and Airehrour, David and Lu, Joan and Xu, Qiang and Wade, Steven and Dunn, Ihaka},
booktitle={2020 30th International Telecommunication Networks and Applications Conference (ITNAC)},
title={Indigenous Big Data Implications in New Zealand},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Our world is dynamic and digital. It is irrefutable; big data are reshaping commerce, healthcare and governmental decisions. Individuals forgo awareness or even desire to uncover or appreciate how big data are affecting our society. This raises distinct data issues of huge effect, especially for indigenous communities. The novel proposition of this research, is that by considering advances in big data optimisation and issues from an indigenous perspective, a broad appreciation of decision-making significances will result. The microcosm in Aotearoa, New Zealand is relatively simple. It permits both a clear and leading view, especially of complex indigenous effects, due to its foundations in Maoridom. As big data advancements impact the quality of life for us all, there is an enormous responsibility, opportunity and incentive to focus research and awareness into nascent data design fields. Through an indigenous lens, unique perspectives and insights are shaped and presented in this paper, implicating feature selection in high-dimensional data.},
keywords={Big Data;Artificial intelligence;Business;Market research;Feature extraction;Cultural differences;Government;high-dimensional data;Māori data sovereignty;indigenous;feature selection;big data;algorithmic bias;PSO},
doi={10.1109/ITNAC50341.2020.9315078},
ISSN={2474-154X},
month={Nov},}
@INPROCEEDINGS{8601548,
author={Zhong, Qing and Yao, Wenlin and Lin, Linxue and Wang, Gang and Xu, Zhong},
booktitle={2018 International Conference on Power System Technology (POWERCON)},
title={Data Analysis and Applications of the Power Quality Monitoring},
year={2018},
volume={},
number={},
pages={4035-4039},
abstract={This paper presents three applications to transform the power quality (PQ) monitoring data into the useful information. With the increasing volume of the PQ monitoring data, mining the values of the data is very important for the power system operations. Three applications are introduced with the PQ monitoring data in Guangzhou grid, China. Firstly, the cumulative probability of PQ monitoring data is applied to certificate the PQ limits according to the national standards. Secondly, three types of voltage sags are counted by the PQ monitoring data to show the severity of voltage sags in local grid. Thirdly, the correlation analysis is applied to show the impact of PQ problem on the device malfunctions. The correlation coefficients between the PQ monitoring data and the device malfunction data can show the impacts of PQ problems on the devices directly. The malfunctions of capacitors/inductors are relevant to the voltage deviation and harmonic distortion obviously which is shown by the correlation coefficients. It is a good attempt to translate the PQ monitoring data into the useful information, which can help the operators decide.},
keywords={Monitoring;Power quality;Big Data;Standards;Correlation coefficient;Correlation;Big data;Power quality;Data analysis;Correlation coefficient},
doi={10.1109/POWERCON.2018.8601548},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9322890,
author={Vasiliev, Victor A. and Aleksandrova, Svetlana V.},
booktitle={2020 International Conference Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)},
title={The Prospects for the Creation of a Digital Quality Management System DQMS},
year={2020},
volume={},
number={},
pages={3-5},
abstract={The development of digital technologies can give a new impetus to the development of quality management (QM). The development of new approaches based on the integration of quality management methods and digital technologies creates prerequisites for the digital transformation of the entire product lifecycle. The difficulty of creating an effective quality management system using digital technologies is not only in the absence of specialists in two areas of knowledge simultaneously, but also in the lack of integration of modern quality management methods with existing software products. In most ready-made solutions, quality management is limited to controlling process parameters and product quality. Automatic registration of process parameters with real-time data analysis should be additionally enabled in DQMS. This will allow you to organize monitoring and control of processes at each automated workplace. The accumulated analysis results will help you make decisions in difficult situations. A set of processes with digital control and analysis ensures quality assurance at all stages of the product lifecycle.},
keywords={Quality management;Process control;Digital transformation;Information technology;Task analysis;Information security;Companies;digital technologies;quality;quality management;digital quality management system DQMS;aerospace;life cycle;digital transformation;Big data;control of technological processes;Internet of things},
doi={10.1109/ITQMIS51053.2020.9322890},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9523967,
author={Cheng, Wanqing},
booktitle={2020 International Conference on Robots & Intelligent System (ICRIS)},
title={Location Selection Method of Real Estate Developers Based on Big Data Technology},
year={2020},
volume={},
number={},
pages={426-429},
abstract={In order to provide consumers with high-quality real estate products and meet their purchase needs, this paper proposes a novel location selection method of real estate developers based on big data technology. This method combines the big data technology, and gives full play to the characteristics and advantages of big data technology, especially data acquisition and data mining. In addition, based on literature research and questionnaire survey, this paper constructs structural equation model from the influencing factors of consumers' purchase choice, and then provides feasible path for real estate open development site selection. Based on this, the model constructed by this method is also combined with quantitative analysis method, and then the specific factor structure is obtained, and the relationship between variables is fitted. The experimental results show that the method can help real estate developers to choose a reasonable development location, and better meet the purchase preferences of consumers.},
keywords={Analytical models;Visualization;Statistical analysis;Numerical analysis;Transportation;Big Data;Safety;Structural Equation;House Purchase;Optimal Path},
doi={10.1109/ICRIS52159.2020.00110},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8029535,
author={Dierckens, Karl E. and Harrison, Adrian B. and Leung, Carson K. and Pind, Adrienne V.},
booktitle={2017 IEEE Trustcom/BigDataSE/ICESS},
title={A Data Science and Engineering Solution for Fast K-Means Clustering of Big Data},
year={2017},
volume={},
number={},
pages={925-932},
abstract={With advances in technology, high volumes of a wide variety of valuable data of different veracity can be easily collected or generated at a high velocity in the current era of big data. Embedded in these big data are implicit, previously unknown and potentially useful information. Hence, fast and scalable big data science and engineering solutions that mine and discover knowledge from these big data are in demand. A popular and practical data mining task is to group similar data into clusters (i.e., clustering). To cluster very large data or big data, k-means based algorithms have been widely used. Although many existing k-means algorithms give quality results, they also suffer from some problems. For instance, there are risks associated with randomly selecting the k centroids, there is a tendency to produce roughly equal circular clusters, and the runtime complexity is very high. To deal with these problems, we present in this paper a big data science and engineering solution that applies heuristic prototype-based algorithm. Evaluation results show the efficiency and scalability of this solution.},
keywords={Data models;Data mining;Classification algorithms;Clustering algorithms;Partitioning algorithms;Big data;data mining;clustering;k-means},
doi={10.1109/Trustcom/BigDataSE/ICESS.2017.332},
ISSN={2324-9013},
month={Aug},}
@INPROCEEDINGS{8972078,
author={Borrison, Reuben and Kloepper, Benjamin and Mullen, Jennifer},
booktitle={2019 IEEE 17th International Conference on Industrial Informatics (INDIN)},
title={Data Preparation for Data Mining in Chemical Plants using Big Data},
year={2019},
volume={1},
number={},
pages={1185-1191},
abstract={Data preparation for data mining in industrial applications is a key success factor which requires considerable repeated efforts. Although the required activities need to be repeated in very similar fashion across many projects, details of their implementation differ and require both application understanding and experience. As a result, data preparation is done by data mining experts with a strong domain background and a good understanding of the characteristics of the data to be analyzed. Experts with these profiles usually have an engineering background and no strong expertise in distributed programming or big data technology. Unfortunately, the amount of data can be so large that distributed algorithms are required to allow for inspection of results and iteration of preparation steps. This contribution introduces an interactive data preparation workflow for signal data from chemical plants enabling domain experts without background in distributed computing and extensive programming experience to leverage the power of big data technologies.},
keywords={Data quality;Soft sensors;Big data},
doi={10.1109/INDIN41052.2019.8972078},
ISSN={2378-363X},
month={July},}
@INPROCEEDINGS{8748630,
author={Saha, Ajitesh Kumar and Kumar, Ashwini and Tyagi, Vishu and Das, Sanjoy},
booktitle={2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)},
title={Big Data and Internet of Things: A Survey},
year={2018},
volume={},
number={},
pages={150-156},
abstract={In this digital era, here the sum of data is generate with store has prolonged inside a less period of time. The data in this era generated high speed leads to many challenges. The size is primarily and periodically, just the measurement that bounces in the big data position. In this survey, we have attempt to give a broad explanation of big data that captures its other unique and important features. We have discussed 4V's model. Also, the latest technologies uses in big data, like Hadoop, HDFS, MapReduce and different type of methods used by big data. Finally various benefits of using big data analysis and include some feature of cloud computing.},
keywords={Big Data;Reliability;Cloud computing;Social networking (online);Sensors;Videos;Data integrity;Big Data;Big Data Analytics;Big Data Quality;Data Reliability;IOT;Hadoop;HDFS;Cloud Computing},
doi={10.1109/ICACCCN.2018.8748630},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8456931,
author={Tikhonyuk, A. I. and Erokhin, S. D. and Chadov, T. A.},
booktitle={2018Systems of Signal Synchronization, Generating and Processing in Telecommunications (SYNCHROINFO)},
title={Big Data application on signal processing systems},
year={2018},
volume={},
number={},
pages={1-3},
abstract={One of the tasks of system identification within the modern signal processing system is the task of identification of unknown error-correction code (ECC). System identification can be done using various methods. Anyway, the more knowledge one has about the possible system structure provides better solution for system identification. Here we take a look into digital communication systems uses ECC, which are the major part of modern communication systems are. We create a new practical approach for identification of a priori unknown ECC being used over the transmission channel of the communication system which is examined for identification purposes. The approach is based as Big Data application which can provide new source for the study. Shown that usage of new approach provides better speed for system identification making the process of identification faster, more reliable, with better quality for solution found and with lower requirements for quality of the transmission channel.},
keywords={Big Data;System identification;Error correction codes;Task analysis;Communication systems;Machine learning;Data mining;error correction codes;ECC;FEC;synchronization;identification;big data;machine learning;deep learning;block codes;convolutional codes;system identification;deep neural networks;automation},
doi={10.1109/SYNCHROINFO.2018.8456931},
ISSN={},
month={July},}
@INPROCEEDINGS{9384718,
author={Zhening, Xu and Xue, Pan},
booktitle={2020 International Conference on Modern Education and Information Management (ICMEIM)},
title={Study on Construction Path of Curriculum Ideological and Political Education in Higher Vocational Colleges under the Background of Big Data},
year={2020},
volume={},
number={},
pages={202-205},
abstract={Ideological and political education in colleges and universities is carried by curriculum ideological and political education in the development process of the new period, which is an important means to improve the effect of teaching and educating in order to fully reflect the core idea of ideological and political course. It is necessary to pay attention to improving the level of ideological and political construction in the process of carrying out ideological and political education in higher vocational colleges, and meanwhile to innovate the teaching methods and means of ideological and political course. Only in this way can the teaching quality of ideological and political courses be improved. In the course of curriculum ideological and political construction in higher vocational colleges, it is necessary to analyze the existing problems, understand the influence of big data technology on curriculum ideological and political teaching, and put forward relevant strategies for curriculum ideological and political construction in higher vocational colleges, which can really improve the quality of curriculum ideological and political construction and ensure the smooth progress of ideological and political education in higher vocational colleges.},
keywords={Education;Information services;Big Data;Information management;Data mining;big data technology;higher vocational colleges;curriculum ideological and political education;construction strategy},
doi={10.1109/ICMEIM51375.2020.00053},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6821019,
author={Essa, Youssef M. and Attiya, Gamal and El-Sayed, Ayman},
booktitle={2013 International Conference on Cloud Computing and Big Data},
title={Mobile Agent Based New Framework for Improving Big Data Analysis},
year={2013},
volume={},
number={},
pages={381-386},
abstract={The rising number of applications serving millions of users and dealing with terabytes of data need to a faster processing paradigms. Recently, there is growing enthusiasm for the notion of big data analysis. Big data analysis becomes a very important aspect for growth productivity, reliability and quality of services (QoS). Processing of big data using a powerful machine is not efficient solution. So, companies focused on using Hadoop software for big data analysis. This is because Hadoop designed to support parallel and distributed data processing. However, Hadoop has several drawbacks effect on its performance and reliability against big data analysis. In this paper, a new framework is proposed to improve big data analysis and overcome the drawbacks of Hadoop. The proposed framework is called MapReduce Agent Mobility (MRAM). MRAM is developed by using mobile agent and MapReduce paradigm under Java Agent Development Framework (JADE).},
keywords={Mobile agents;Servers;Fault tolerance;Fault tolerant systems;Software;Mobile Agent;JADE;Big Data Analysis;HDFS;Fault Tolerance},
doi={10.1109/CLOUDCOM-ASIA.2013.75},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8456108,
author={Tse, Daniel and Chow, Chung-kin and Ly, Ting-pong and Tong, Chung-yan and Tam, Kwok-wah},
booktitle={2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)},
title={The Challenges of Big Data Governance in Healthcare},
year={2018},
volume={},
number={},
pages={1632-1636},
abstract={Big data starts to be employed in some industries but not yet widely or properly adopted in healthcare industry. This research paper aims at studying the usages and challenges of big data in healthcare sector. Governance of big data will include the domains of strategy, process, people, policy and technology and automation. Among the challenges identified in the healthcare sector, reliability and integrity are especially important because it is related to life and death. Big data governance for policy maker, authentication for data integrity, and future development of healthcare big data governance are discussed here. Moreover, some future development questions are raised in this paper for further study, which will improve the quality of life and lead to a better and healthier world under the proper and adequate big data governance environment.},
keywords={Handheld computers;Conferences;Security;Data privacy;Big Data;big data;governance;healthcare;challenges},
doi={10.1109/TrustCom/BigDataSE.2018.00240},
ISSN={2324-9013},
month={Aug},}
@INPROCEEDINGS{7363904,
author={Oneto, Luca and Orlandi, Ilenia and Anguita, Davide},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Performance assessment and uncertainty quantification of predictive models for smart manufacturing systems},
year={2015},
volume={},
number={},
pages={1436-1445},
abstract={We review in this paper several methods from Statistical Learning Theory (SLT) for the performance assessment and uncertainty quantification of predictive models. Computational issues are addressed so to allow the scaling to large datasets and the application of SLT to Big Data analytics. The effectiveness of the application of SLT to manufacturing systems is exemplified by targeting the derivation of a predictive model for quality forecasting of products on an assembly line.},
keywords={Predictive models;Support vector machines;Uncertainty;Big data;Manufacturing systems;Data models;Biological system modeling;Performance Assessment;Uncertainty Quantification;Smart Manufacturing Systems;Big Data;Supervised Learning;Statistical Learning Theory},
doi={10.1109/BigData.2015.7363904},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9064371,
author={Gao, Zhiheng and Gong, Yunping},
booktitle={2019 IEEE 5th International Conference on Computer and Communications (ICCC)},
title={A Study of Real Time Raster Graphic Visualization Based on Big Data Technology},
year={2019},
volume={},
number={},
pages={485-489},
abstract={The grid sizes were designed to provide best quality for raster graphics. The grid data was regionally grouped into blocks and stored by partition, providing a data structure suitable for distributed parallel processing. The whole procedure from data query to grid block sub-graphic generation was fully parallel processed by an independently developed cluster, achieving high performance visualization of pixel-level raster graphics under full-HD screen resolution. The solution provides reference for the construction of similar systems.},
keywords={Data visualization;Distributed databases;Image color analysis;Time factors;Real-time systems;Telecommunications;big data;HBase;raster graphics;graphics visualization},
doi={10.1109/ICCC47050.2019.9064371},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7603605,
author={Lin, Shunfu and Xie, Chao and Tang, Bo and Liu, Ronghui and Pan, Aiqiang},
booktitle={2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)},
title={The data mining application in the power quality monitoring data analysis},
year={2016},
volume={},
number={},
pages={338-342},
abstract={It is a main issue to find valuable information from the power quality data because of its big volume, heterogeneity and low value density in the power quality monitoring system of the grid. An analysis system of the power quality analysis based on the data mining technologies is presented in this paper, consisting of the technologies of data cleaning, data fusion, cluster analysis, correlation analysis, and etc. The proposed analysis system is applied in the power quality data analysis of a certain city power quality monitoring system. The meaningful variation laws of the power quality indices are obtained, which can provide valuable reference to the grid planning, dispatch and operation.},
keywords={Power quality;Correlation;Monitoring;Indexes;Data mining;Voltage fluctuations;Data analysis;power quality;data mining;cluster analysis;correlation analysis},
doi={10.1109/ICIEA.2016.7603605},
ISSN={2158-2297},
month={June},}
@INPROCEEDINGS{8241335,
author={Mejía-Lavalle, Manuel and Meusel, Winfrid and Tavira, Jonathan Villanueva and Cruz, Mirian Calderón},
booktitle={2017 International Conference on Mechatronics, Electronics and Automotive Engineering (ICMEAE)},
title={Effective Data Quality Diagnostic Schema for Big Data},
year={2017},
volume={},
number={},
pages={163-168},
abstract={Big Data environment is a computing area with a great growth. Today it is common that we hear about databases with huge volumes of information and also we hear about Data Mining and Business Intelligence projects related with these huge databases. However, in general, little attention has been given to the quality of the data. Here we propose and present innovative metrics and schema designed to perform a basic task related to the Data Quality issue, this is, the diagnostic. The preliminary results that we obtained when we apply our approaches to Big Data encourage us to continue this work.},
keywords={Databases;Big Data;Measurement;Cleaning;Data mining},
doi={10.1109/ICMEAE.2017.29},
ISSN={},
month={Nov},}
@ARTICLE{8715359,
author={Ramzan, Shabana and Bajwa, Imran Sarwar and Ramzan, Bushra and Anwar, Waheed},
journal={IEEE Access},
title={Intelligent Data Engineering for Migration to NoSQL Based Secure Environments},
year={2019},
volume={7},
number={},
pages={69042-69057},
abstract={In an era of super computing, data is increasing exponentially requiring more proficiency from the available technologies of data storage, data processing, and analysis. Such continuous massive growth of structured and unstructured data is referred to as a “Big data”. The processing and storage of big data through a conventional technique is not possible. Due to improved proficiency of Big Data solution in handling data, such as NoSQL caused the developers in the previous decade to start preferring big data databases, such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology that is designed to provide scalability to support voluminous data, leading to the rise of NoSQL as the most viable database solution. These modern databases aim to overcome the limitations of relational databases such as unlimited scalability, high performance, data modeling, data distribution, and continuous availability. These days, the larger enterprises need to shift NoSQL databases due to their more flexible models. It is a great challenge for business organizations and enterprises to transform their existing databases to NoSQL databases considering heterogeneity and complexity in relational data. In addition, with the emergence of big data, data cleansing has become a great challenge. In this paper, we proposed an approach that has two modules: data transformation and data cleansing module. The first phase is the transformation of a relational database to Oracle NoSQL database through model transformation. The second phase provides data cleansing ability to improve data quality and prepare it for big data analytics. The experiments show the proposed approach successfully transforms the relational database to a big data database and improve data quality.},
keywords={Big Data;NoSQL databases;Transforms;Scalability;Servers;Tools;Relational databases;NoSQL;big data;data cleansing},
doi={10.1109/ACCESS.2019.2916912},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9202833,
author={Salvadori, Ivan and Huf, Alexis and Siqueira, Frank},
booktitle={2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)},
title={Data Linking as a Service: An Infrastructure for Generating and Publishing Linked Data on the Web},
year={2020},
volume={},
number={},
pages={262-271},
abstract={Companies, government, and even ordinary people have been producing and publishing huge amounts of data. This phenomena, known as big data, leveraged the interest in advanced analytics and data science. Many observers, though, are pointing out that extracting knowledge from such datasets requires suitable tools for handling and integrating data. Research in the last years has shown that taking into account the semantics of data is crucial for fostering data integration solutions. However, there is a lack of solutions for data publishing that follow the best practices for exposing and connecting data. With this regard, this work proposed DLaaS, an infrastructure for generating and publishing linked data on the Web. It aims at facilitating the execution of necessary processes to properly publish high quality linked data. Its main goal is to improve the reuse of data by connecting entities based on heterogeneous datasets that share a certain level of data intersection or semantic relationship.},
keywords={Linked data;Semantics;Publishing;Web services;Data models;Ontologies;Standards;Semantic Web, Linked Data, Microservices, Web Services, Data-driven Services},
doi={10.1109/COMPSAC48688.2020.00042},
ISSN={0730-3157},
month={July},}
@ARTICLE{9235580,
author={Sun, Maohua and Li, Yuangang},
journal={IEEE Access},
title={Eco-Environment Construction of English Teaching Using Artificial Intelligence Under Big Data Environment},
year={2020},
volume={8},
number={},
pages={193955-193965},
abstract={Application of big data and artificial intelligence has become one influence factor of English teaching, which have broken the balance of the teaching Eco-environment for English. In this article, the artificial intelligence and big data are introduced into English teaching to propose a new teaching Eco-environment construction method to meet the needs of the social development and international communication in English. In the proposed method, the characteristics of English teaching under big data environment are analyzed in detail. Then the big data technology is used to construct a new Eco-environment of English teaching to improve the teaching and learning quality. The data mining method is one of artificial intelligence methods, which is used to analyze the relationship of interdependence and mutual restriction among various factors in English teaching in order to build and implement a new Eco-environment with the information sharing, quality teaching and personalized learning of English. Finally, through the practical application of the constructed Eco-environment, the experiment results show that the proposed method can help students update their learning concepts, methods and contents of English, inspire their interest and initiative by comparing with some existed teaching methods, so as to improve their learning effects and application ability of English. Therefore, the constructed Eco-environment provides a new idea and direction for English teaching reform by application of big data and artificial intelligence.},
keywords={Education;Big Data;Data mining;Information technology;Symbiosis;Learning (artificial intelligence);Eco-environment;English teaching;big data;data mining;influence factor;comprehensive ability},
doi={10.1109/ACCESS.2020.3033068},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9263667,
author={Xiangwei, Kong},
booktitle={2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
title={Evaluation of Flight Test Data Quality Based on Rough Set Theory},
year={2020},
volume={},
number={},
pages={1053-1057},
abstract={With the continuous development of flight test technology, the test system is filled with massive, multi-structured, and multi-dimensional data resources. The value of big data has been fully recognized by the society. How to tap the value of data has become an application in various research fields and industries. The most concerned issue of the field. Whether the data is rubbish or treasure, the most important question is whether the data to be analyzed and mined is of high quality. A low-quality data source will not only fail to reflect the value of the data, but may also run counter to the actual situation, which has side effects. In order to effectively evaluate the quality of flight test data, according to the characteristics of test data in flight test, a test data quality evaluation method based on rough set theory is proposed, standard test methods and test indicators for flight test data quality are proposed, and data quality evaluation is given. The method of rule extraction realizes the quality evaluation of flight test data.},
keywords={Data integrity;Rough sets;Feature extraction;Image color analysis;Shape;Packet loss;Data mining;Flight test;data quality;rough set;quality evaluation},
doi={10.1109/CISP-BMEI51763.2020.9263667},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7755397,
author={Vinod, D. Franklin and Vasudevan, V.},
booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)},
title={A filter based feature set selection approach for big data classification of patient records},
year={2016},
volume={},
number={},
pages={3684-3687},
abstract={The flourishing fame and development of big data in recent years made researchers to have a detailed study. Of the all entire emerging big data research topics, classification of data from big data is identified as a great challenge to address as of our analysis. The Classification is the process of categorizing data for its most effective and efficient use. While analyzing large scale patient records, hierarchical learning approach which is tree structured that train max-margin classifier will give better classification results and also it is computationally efficient. The quality of features has an effect on the performance of hierarchical learning approach for classification of patient records. So we have to extract discriminative features for training hierarchical classifier. In this paper Highly Correlated Feature Set Selection (HCFS) algorithm is proposed to combine with the hierarchical leaning approach to improve its performance. This algorithm identifies the good feature subsets which will improve the classification accuracy.},
keywords={Classification algorithms;Correlation;Big data;Algorithm design and analysis;Medical treatment;Clustering algorithms;Filtering algorithms;Big Data;Classification;Feature set;tree structured classifier},
doi={10.1109/ICEEOT.2016.7755397},
ISSN={},
month={March},}
@INPROCEEDINGS{8359833,
author={Sun, Huabo and Jiao, Yang and Han, Jingru and Wang, Chun},
booktitle={2017 IEEE 17th International Conference on Communication Technology (ICCT)},
title={A novel temporal-spatial analysis system for QAR big data},
year={2017},
volume={},
number={},
pages={1238-1241},
abstract={The current analysis restricts to the statistics of different exceedance events in flight operational quality assurance (FOQA). Statistical methods lack effective correlation to flight data at different time and space, and lack deep-level mining and application of flight quality monitoring information. For questions raised above, this paper presents a novel approach for FOQA based on temporal geography information system (T-GIS). We construct a time-snapshot-> time series-> space-time evolution model, and design a dynamic spatiotemporal statistical analysis algorithm. The results show that the system can be used to deal with large flight data to find the unsafety events spatial-temporal distribution of the whole civil aviation industry. It also provides a new research idea to raise the level of FOQA.},
keywords={Airports;Correlation;Springs;Safety;Statistical analysis;Geographic information systems;Aircraft;flight operational quality assurance;temporal GIS;spatial-temporal analysis;QAR big data},
doi={10.1109/ICCT.2017.8359833},
ISSN={2576-7828},
month={Oct},}
@INPROCEEDINGS{8337434,
author={Gao, Shanchun},
booktitle={2018 10th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)},
title={Research on Engineering Teaching Mode of "Introduction" Course in Engineering Colleges under the Background of Big Data},
year={2018},
volume={},
number={},
pages={486-488},
abstract={The development of information technology, such as Internet, Internet of things and cloud computing, has brought the storage of massive information resources for human beings, and human beings have entered the era of big data.Big data has changed people's values, lifestyles and ways of thinking.The big data brings challenges to the teaching of "Introduction"course, but also brings opportunities to the teaching of "Introduction"course.For the teaching of "Introduction"course of engineering colleges, we must deepen teaching reform, actively meet the challenges and opportunities brought by big data, and innovate engineering teaching mode, so as to continuously improve teaching quality.},
keywords={Education;Big Data;Cloud computing;Production;Knowledge engineering;Information services;Big data;Engineering;Teaching model},
doi={10.1109/ICMTMA.2018.00123},
ISSN={2157-1481},
month={Feb},}
@ARTICLE{8890933,
author={Yan, Yan and Zhang, Lianxiu and Sheng, Quan Z. and Wang, Bingqian and Gao, Xin and Cong, Yiming},
journal={IEEE Access},
title={Dynamic Release of Big Location Data Based on Adaptive Sampling and Differential Privacy},
year={2019},
volume={7},
number={},
pages={164962-164974},
abstract={Data releasing is a key part bridging between the collection of big data and their applications. Traditional methods release the static version of dataset or publish the snapshot with a fixed sampling interval, which cannot meet the dynamic query requirements and query precision for big data. Moreover, the quality of published data cannot reflect the characteristics of the dynamic changes of big data, which often leads to subsequent data analysis and mining errors. This paper proposes an adaptive sampling mechanism and privacy protection method for the release of big location data. In order to reflect the dynamic change of data in time, we design an adaptive sampling mechanism based on the proportional-integral-derivative (PID) controller according to the temporal and spatial correlation of the location data. To ensure the privacy of published data, we propose a heuristic quad-tree partitioning method as well as a corresponding privacy budget allocation strategy. Experiments and analysis prove that the adaptive sampling mechanism proposed in this paper can effectively track the trend of dynamic changes of data, and the designed differential privacy method can improve the accuracy of counting query and enhance the availability of published data under the premise of certain privacy intensity. The proposed methods can also be readily extended to other areas of big data release applications.},
keywords={Differential privacy;Publishing;Big Data;Data models;Vehicle dynamics;Big location data;privacy preserving data publishing;adaptive sampling;differential privacy;heuristic quad-tree partitioning},
doi={10.1109/ACCESS.2019.2951364},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8386595,
author={Shih, Dong-Her and Shih, Po-Yuan and Wu, Ting-Wei},
booktitle={2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={An infrastructure of multi-pollutant air quality deterioration early warning system in spark platform},
year={2018},
volume={},
number={},
pages={648-652},
abstract={In recent years, the increase of multi-pollutant air pollution index has a direct impact on people's health. Relevant studies have pointed out that the exposure of air pollutants has a strong connection with asthma and other unhealthy diseases such as the lungs. The air pollution has brought health issue, but also caused a huge medical costs each year derived from the disease. Although there are many different approaches to forecasting air pollution, most of them are based on historical data and the environment of local server devices. Some of the data records may differ from those of real-time situations and those limitation of traditional hardware devices. Therefore, this project tries to adopt Azure, a cloud computing platform with advantages of scalability, reliability and agility, to set up a multi-pollutant air quality deterioration warning system. This project is divided into three parts. The first part is the collection of multi-pollutant data and the prediction of multi-pollutant air pollution quality. After the government and private data are collected and integrated into the database, the air quality of multi-pollutant are predicted. The second part is big data analysis of air quality indicators for pollutants indicator AQI, AAQI, and HAQI AIDE, a real-time deterioration warning system for deteriorating, was popularized in Spark for big data analytics framework and various reports were generated. The third part will be a combination of blockchain and perpetual system performance assessment. The overall system will integrate with the blockchain and conduct a continuous evaluation of the system.},
keywords={Air pollution;Indexes;Standards;Monitoring;Alarm systems;Predictive models;air pollution;multi-pollutant;deterioration warning system;air quality indicators;blockchain;big data analysis},
doi={10.1109/ICCCBDA.2018.8386595},
ISSN={},
month={April},}
@INPROCEEDINGS{7004306,
author={Chung, Wei-Chun and Chang, Yu-Jung and Lee, D. T. and Ho, Jan-Ming},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Using geometric structures to improve the error correction algorithm of high-throughput sequencing data on MapReduce framework},
year={2014},
volume={},
number={},
pages={784-789},
abstract={Next-generation sequencing (NGS) data are a rapidly growing example of big data and a source of new knowledge in science. However, sequencing errors remain unavoidable and reduce the quality of NGS data. Error correction, therefore, is a critical step in the successful utilization of NGS data, including de novo genome assembly and DNA resequencing. Since NGS throughput doubles approximately every five months and the length of NGS records (i.e., reads) is increasing, improvements in efficiency and effectiveness of computational strategies are needed. In this study, we aim to improve the performance of CloudRS, an open-source MapReduce application designed to correct sequencing errors in NGS data. We introduce the readmessage (RM) diagram to represent the set of messages, i.e., the key-value pairs generated on each read. We also present the Gradient-number Votes (GNV) scheme in order to trim off portions of the RM diagram, thereby reducing the total size of messages associated with each read. Experimental results show that the GNV scheme successfully reduce execution time and improve the quality of the de novo genome assembly.},
keywords={Sequential analysis;Bioinformatics;Error correction;Genomics;Assembly;Big data;DNA;big data;error correction;geometric structure;mapreduce;next-generation sequencing},
doi={10.1109/BigData.2014.7004306},
ISSN={},
month={Oct},}