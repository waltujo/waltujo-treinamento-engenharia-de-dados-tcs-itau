@INPROCEEDINGS{9104168,
author={Balaji, Sathya and Prasathkumar, V.},
booktitle={2020 International Conference on Computer Communication and Informatics (ICCCI)},
title={Dynamic Changes by Big Data in Health Care},
year={2020},
volume={},
number={},
pages={1-4},
abstract={The healthcare sector is expanding and industry- related data is also growing In this controlled situation, the sufficiency of the data becomes more essential. In particular, the evaluation of patient care and the development of new healthcare are important. To this end, healthcare is progressively realizing the importance of emerging technologies and making proper use of data collected by patient acceptance, background of patient safety, declarations of protection, and popular acceptance for drugs. With the use of big data in specific, healthcare is now experiencing significant and ground-breaking shifts. This motto has implemented a fundamental shift to such a degree that payers, single hospitals and pharmaceutical companies rely on a growing pool of health data to better communicate conditions. Despite the shattered basis of economic challenges, the healthcare industry has been able to work. Every second generates a huge amount of physical data of unpredictable intricacy and validity. This volume of data with the innate to alter the course of the healthcare industry can be caught with state-of - the-art data analytics software. Caregivers and managers can now make greater financial and medical decisions while giving ever-increasing quality of clinical care with the wealth of data produced by healthcare data analytics.},
keywords={Data Analytics;Machine Learning;Artificial Intelligence;Image Processing.},
doi={10.1109/ICCCI48352.2020.9104168},
ISSN={2329-7190},
month={Jan},}
@INPROCEEDINGS{7840961,
author={Zuo, Liudong and Zhu, Michelle Mengxia},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Bandwidth provision strategies for reliable data movements in dedicated networks},
year={2016},
volume={},
number={},
pages={3069-3078},
abstract={Sheer volumes of data are being generated in extreme-scale distributed scientific applications, and need to be transferred remotely in fast, predictable and reliable way for data storage and analysis purpose. Reserving bandwidth along selected paths in high-performance networks (HPNs) has proved to be an effective way to satisfy the high-demanding performance requirements of such data transfer. However, node and link failures within the HPNs potentially degrade the quality of data transfer. In this paper, we focus on the scheduling of two generic types of bandwidth reservation requests concerning data transfer reliability: (i) to achieve the highest data transfer reliability under a given data transfer deadline, and (ii) to achieve the earliest data transfer completion time while satisfying a given data transfer reliability requirement. Poisson distribution is used to model the node and failures within the HPNs, and two periodic bandwidth reservation algorithms with rigorous optimality proofs are proposed.},
keywords={Bandwidth;Data transfer;Reliability;Scheduling;Big data;Distributed databases;Mathematical model;Bandwidth reservation;bandwidth scheduling;high-performance networks;fault tolerance},
doi={10.1109/BigData.2016.7840961},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7207238,
author={Chong, Marc and Wang, Maggie and Lai, Xin and Zee, Benny and Hong, Fung and Yeoh, Ek and Wong, Eliza and Yam, Carrie and Chau, Patsy and Tsoi, Kelvin and Graham, Colin},
booktitle={2015 IEEE International Congress on Big Data},
title={Patient Flow Evaluation with System Dynamic Model in an Emergency Department: Data Analytics on Daily Hospital Records},
year={2015},
volume={},
number={},
pages={320-323},
abstract={The big data in Accident and Emergency services in hospitals worth being analyzed to provide clinical decision support to clinicians and medical information to patients. System Dynamics modelling is a technique used for modelling complex behaviors of organizational and social systems. In this study, a system dynamics approach is used to model the patient flow in Accident and Emergency in Hong Kong. The study aims to examine the trade-offs of various safety and quality outcomes in an Emergency Department: waiting time and occupancy (acute beds and waiting room) in primary, on the adjustments of various factors (e.g. Admission volumes and staff numbers) in order to evaluate how an accident and emergency system in Hong Kong could be more efficiently operated.},
keywords={Mathematical model;Hospitals;Accidents;Data models;Big data;System dynamics;emergency department;patient flow;accident and emergency},
doi={10.1109/BigDataCongress.2015.54},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{9006114,
author={Yuasa, Shigeaki and Nakai, Takumi and Maruichi, Takanori and Landsmann, Manuel and Kise, Koichi and Matsubara, Masaki and Morishima, Atsuyuki},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Towards Quality Assessment of Crowdworker Output Based on Behavioral Data},
year={2019},
volume={},
number={},
pages={4659-4661},
abstract={In this paper, we show preliminary results on the quality assessment of crowdworker output based on the movements of the mouse and the eyes while the task is performed. We assume that the mouse and the eyes stop longer if the quality is lower due to the lack of knowledge, or confidence, etc. Because the mouse- and eye-stopping duration follows lognormal distribution, we estimate its parameters (mean and standard deviation) to evaluate the quality. Results of preliminary experiments with 10 participants show that the parameters of correct outputs are different from those of incorrect ones. As compared to the task duration, which is often used as a feature for assessment, we have found that the mouse-and the eyestopping duration is advantageous and complementary for the assessment.},
keywords={Task analysis;Mice;Crowdsourcing;Standards;Quality assessment;Quality control;Log-normal distribution;quality assessment;crowdsourcing;eye tracking;mouse movement;log-normal},
doi={10.1109/BigData47090.2019.9006114},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8215621,
author={Zhao, Xiangyu and Xu, Tong and Fu, Yanjie and Chen, Enhong and Guo, Hao},
booktitle={2017 IEEE International Conference on Data Mining (ICDM)},
title={Incorporating Spatio-Temporal Smoothness for Air Quality Inference},
year={2017},
volume={},
number={},
pages={1177-1182},
abstract={It is well recognized that air quality inference is of great importance for environmental protection. However, due to the limited monitoring stations and various impact factors, e.g., meteorology, traffic volume and human mobility, inference of air quality index (AQI) could be a difficult task. Recently, with the development of new ways for collecting and integrating urban, mobile, and public service data, there is a potential to leverage spatial relatedness and temporal dependencies for better AQI estimation. To that end, in this paper, we exploit a novel spatio-temporal multi-task learning strategy and develop an enhanced framework for AQI inference. Specifically, both time dependence within a single monitoring station, and spatial relatedness across all the stations will be captured, and then well trained with effective optimization to support AQI inference tasks. As air-quality related features from cross-domain data have been extracted and quantified, comprehensive experiments based on real-world datasets validate the effectiveness of our proposed framework with significant margin compared with several state-of-the-art baselines, which support the hypothesis that our spatio-temporal multi-task learning framework could better predict and interpret AQI fluctuation.},
keywords={Air quality;Monitoring;Real-time systems;Feature extraction;Data mining;Optimization;Urban Computing;AQI Prediction;Multi-task Learning},
doi={10.1109/ICDM.2017.158},
ISSN={2374-8486},
month={Nov},}
@INPROCEEDINGS{9005982,
author={Krishnamurthy, Vikram and Nezafati, Kusha and Bae, Juhyun and Gursoy, Emre and Zhong, Mian and Singh, Vikrant},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Classification of Driving Behavior Events Utilizing Kinematic Classification and Machine Learning for Down Sampled Time Series Data},
year={2019},
volume={},
number={},
pages={3789-3796},
abstract={The proliferation of connected cars globally has the potential to produce torrents of Big Data that will enable improvements in driver safety, new location based services, improvements in vehicle quality, and optimized vehicle designs. One aspect of connected car data involves driving behavior data and its use for Usage Based Insurance (UBI). UBI has become one of the most widely used applications of driving behavior data. Currently, the transmission and processing of high frequency driving behavior data from the connected car to the cloud is limited by wireless data costs and in-vehicle hardware complexity. To alleviate these issues, we detail the development of a machine learning framework utilizing a kinematic classification methodology applied to down sampled time series vehicle data sets for accurate imputation of driving behavior events in UBI applications. The down-sampled data, consisting of 5 second frames with data fields of timestamp, vehicle speed, and vehicle acceleration is classified into unique kinematic clusters to standardize any driving behavior data distributions. Subsequently, machine learning is used to impute harsh driving events for each 5 second frame in select kinematic clusters. This novel machine learning methodology reduced data set sizes by 75%, utilized a limited set of five attributes, and achieved an average precision and recall of 84.5% and 63.5% for two distinct connected car data sets with 1/5 Hz down-sampled data.},
keywords={Acceleration;Automobiles;Kinematics;Machine learning;Telematics;Insurance;Hardware;Machine Learning;Usage Based Insurance;Kinematics;Clustering;Neural Networks},
doi={10.1109/BigData47090.2019.9005982},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8079968,
author={Karim, Shakir and Al-Tawara, Abdallah and Gide, Ergun and Sandu, Raj},
booktitle={2017 8th International Conference on Information Technology (ICIT)},
title={Is big data too big for SMEs in Jordan?},
year={2017},
volume={},
number={},
pages={914-922},
abstract={With the worldwide appearance of Big Data and its prospective to citizens in all its endeavours, there has been a developing requirements for research on the business drivers influencing the adoption of Big Data services. This paper therefore focuses on Small to Midsized Enterprises-SMEs in Jordan; hoping to have a better delivery of Big Data services with increased transparency and availability of technology, and the improved interaction with SMEs. This paper reviewed the relationships between Big Data and SMEs in general in Jordan. In addition, it examines the barriers for adopting Big Data among SMEs in Jordan. The paper mainly has used secondary research data and methods to provide a broad investigation of the issues relevant to Big Data, the reasons of those issues for SMEs in Jordan. The research is subject to academic journal articles, project reports, media articles, government and corporation based documents and other appropriate information. The study found that currently Jordan has been capitalizing comprehensively in developing its ICT sector, aiming at improving the performance of its public and private sector organizations, effectiveness, accurateness, time and fulfillment. However, Jordan needs to overcome barriers for adopting Big Data among SMEs which will improve efficiency and organizational effectiveness. By this way, SMEs in Jordan will be able to deliver information, high-quality customer centric and performance-driven services, and procedures, using information and communication technologies (ICT) to change the way they engage with customers and other businesses.},
keywords={Big Data;Social network services;Internet;Industries;Government;Mobile handsets;Big Data;Small to Midsized Enterprises-SMEs;Jordan;Motivators and Barriers;E-Business},
doi={10.1109/ICITECH.2017.8079968},
ISSN={},
month={May},}
@INPROCEEDINGS{7363794,
author={Li, Tonglin and Wang, Ke and Zhao, Dongfang and Qiao, Kan and Sadooghi, Iman and Zhou, Xiaobing and Raicu, Ioan},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={A flexible QoS fortified distributed key-value storage system for the cloud},
year={2015},
volume={},
number={},
pages={515-522},
abstract={In the era of big data and cloud, distributed key-value stores are increasingly used as building blocks of large-scale applications. Comparing to traditional relational databases, key-value stores are particularly compelling due to their low latency and excellent scalability. Many big companies, such as Facebook and Amazon, run multiple different applications and services on top of a single key-value store deployment to reduce the deployment and maintenance complexity as well as economic cost. However, every application has its performance requirement but most current key-value store systems are designed to serve every application request equally. This design works well when a single application accesses the key-value store, but it is not as good for the emerging concurrent multi-application scenario. In this paper, we present ZHT/Q, a flexible QoS (Quality of Service) fortified distributed key-value storage system for clouds and data centers. It improves the overall throughput by an order of magnitude and still satisfies different applications' latency requirements with QoS using dynamic and adaptive request batching mechanisms. The experiment results show that our new system delivers up to 28 times higher throughput than the base solution while more than 99% of requests' latency requirements are satisfied.},
keywords={Servers;Quality of service;Throughput;Monitoring;Heuristic algorithms;Big data;Facebook;Distributed key-value store;QoS;NoSQL database;Request batching;Distributed systems},
doi={10.1109/BigData.2015.7363794},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7802106,
author={Kalan, Reza Shokri and Ünalir, Murat Osman},
booktitle={2016 6th International Conference on Computer and Knowledge Engineering (ICCKE)},
title={Leveraging big data technology for small and medium-sized enterprises (SMEs)},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Wisdom aligns with technology is the key factor for sustainable business development. By increasing amount of public and private data, organizations need to find new solutions to manage data and information which lead to knowledge, better decision making, and value. In the big data-bang, smart organization surfing on-line technology and start planning big data strategy. However, many organizations do not yet have a big data strategy. A challenge facing SMEs is that they may not have the same capacity as large companies to analysis new data sets. Also, traditional data processing tools are not capable for SMEs decision making because of volume, velocity and variety if data. For address this problem we need new leveraging technology, tools and talent. SMEs which have risen to leveraging the value of big data are using advantage of cloud computing and open-source software to realize various goals. The main goal of this investment is about value as a new concept in a big data era. In this study, we focus on emerging trends and future requirement: technology and tools for SMEs.},
keywords={Organizations;Parallel processing;Computer architecture;Security;Privacy;big data;data analytic;data quality;SMEs;business intelligence;cloud computing},
doi={10.1109/ICCKE.2016.7802106},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8850938,
author={Muttaqi, Syukron Rifa'il and Ahmad, Tohari},
booktitle={2019 12th International Conference on Information & Communication Technology and System (ICTS)},
title={A New Data Hiding Method for Protecting Bigger Secret Data},
year={2019},
volume={},
number={},
pages={146-151},
abstract={Security plays a vital role in business. To improve security, hiding secret data in an image can be employed when sending it in an electronic form. Some images are sensitive to modification, such as military and medical images. Thus, reversible data hiding needs to be utilized to prevent permanent distortion. We employ difference expansion, modulus function, and reduced difference expansion to conceal secret data into an image. Unlike previous methods, our scheme can conceal data in any block with any difference pixel value as long as they do not cause underflow or overflow. The result shows that this proposed method can increase the performance, especially the capacity of the payload. As the trade-off, the quality decreases, depending on the cover being used.},
keywords={Payloads;Data mining;Image restoration;Distortion;Business;Security;Biomedical imaging;data hiding;data security;data protection;difference expansion;modulus function},
doi={10.1109/ICTS.2019.8850938},
ISSN={},
month={July},}
@INPROCEEDINGS{8547029,
author={Borgi, Tawfik and Zoghlami, Nesrine and Abed, Mourad and Naceur, Mohamed Saber},
booktitle={2017 6th IEEE International Conference on Advanced Logistics and Transport (ICALT)},
title={Big Data for Operational Efficiency of Transport and Logistics: A Review},
year={2017},
volume={},
number={},
pages={113-120},
abstract={In the new information and communication era, digital transformation and adoption of recent technological advances have become a must for all transport and logistics providers who aim to significantly improve their activities. Consequently, this digitalization is inevitably giving birth to voluminous and rapidly growing sets of large-scale data generated from heterogeneous data sources, also known as Big Data. With particular management infrastructures and advanced data analysis methodologies, these huge amounts of data can be efficiently harvested to optimize the logistics and transport operations and provide a higher quality of service. This paper provides a review of the application of Big Data technologies in improving the operational efficiency of transport and logistics, exposes the main use cases and identifies some future research challenges.},
keywords={Big Data;Logistics;Data mining;Sensor phenomena and characterization;Global Positioning System;Conferences;Big Data;Transport;Logistics},
doi={10.1109/ICAdLT.2017.8547029},
ISSN={},
month={July},}
@INPROCEEDINGS{9057068,
author={Jiménez, Alicia and Nieve, José and Estrada, Fernando and Vázquez-Gálvez, Felipe A. and Hernández, Israel},
booktitle={2019 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)},
title={Management of heterogeneous data in the Red Climatológica UACJ in a NoSQL environment},
year={2019},
volume={},
number={},
pages={1-6},
abstract={The meteorological data in networks are susceptible to errors and irregularities, one of the reasons is because they are collected by different types of meteorological stations, the possibility of heterogeneous data, so they limit their assimilation to models and interpretation platforms. This paper presents a project that aims to assist the Laboratorio de Climatología y Calidad del Aire in the analysis of the Red-Clima UACJ, through the creation of a NoSQL environment in which there is a process for the quality assessment of the data, as well as the necessary management for the homogenization and visualization of it. The quality assessment using a series of quality control algorithms can detect possible anomalies in the behavior of the data for later review and modification. For the management it is necessary to create a framework with the objective to homogenize the climatic information contained in MongoDB, providing as a result a dataset in JSON, CSV or XML format.},
keywords={XML;Big Data;Python;Atmospheric modeling;Quality assessment;Data visualization;Biological system modeling;NoSQL;MongoDB;linear regression;heterogeneous data;homogenization},
doi={10.1109/ROPEC48299.2019.9057068},
ISSN={2573-0770},
month={Nov},}
@INPROCEEDINGS{9260374,
author={Xu, Yan},
booktitle={2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Design System Construction of Intelligent Enterprise Management System Based on Big Data Technology},
year={2020},
volume={},
number={},
pages={363-366},
abstract={In order to improve the work efficiency and intelligence of enterprise management system, this paper puts forward a novel design system of intelligent enterprise management system based on big data technology. The design system is mainly based on big data technology and breaks the limitations of the traditional enterprise management system. The research results show that the design system can not only improve the work efficiency of the enterprise management system, but also promote the combination of big data technology and enterprise management system, and realize the sustainable development of the enterprise.},
keywords={Zirconium;Xenon;Smart grids;Quality function deployment;Hafnium;Conferences;Automation;Big data technology;Artificial intelligence technology;Intelligent system;Enterprise management system},
doi={10.1109/ICSGEA51094.2020.00084},
ISSN={},
month={June},}
@INPROCEEDINGS{7977009,
author={Joglekar, Prajakta and Kulkarni, Vrushali},
booktitle={2017 International Conference on Emerging Trends & Innovation in ICT (ICEI)},
title={Data oriented view of a smart city: A big data approach},
year={2017},
volume={},
number={},
pages={51-55},
abstract={Smart City concept is being adopted by many governments to implement sustainable solutions and improve the quality of life of the citizens. This paper presents a data oriented view of a smart city by studying the characteristics of data generated by various sources in the city. This data would be used to build smart city components related to transport, healthcare, energy and environment. Datafication of smart city reveals that the data to be managed is Big Data. Hence, we further review how big data can be used for smart city solutions, and provide a process flow and mapping for some of the smart city components.},
keywords={Smart cities;Big Data;Government;Monitoring;Intelligent sensors;Smart city;Big data;Urban computing;Governance;Datafication;technology},
doi={10.1109/ETIICT.2017.7977009},
ISSN={},
month={Feb},}
@INPROCEEDINGS{9109853,
author={Xia, Shaoqiong},
booktitle={2020 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={Research on the Influence of Information Technology on Education Under the Background of Big Data},
year={2020},
volume={},
number={},
pages={612-616},
abstract={In the big data environment, information technology has penetrated into all aspects of education, and has a profound impact on education. It is of great significance to study the impact of information technology on education and improve the quality of education in the context of big data. Through the elaboration of big data and information technology, this paper analyzes the impact of information technology on education and teaching, and its positive and negative impact on curriculum design, education methods, education innovation, etc. The purpose is to give full play to the positive impact of information technology on education and teaching and avoid the negative impact of information technology on education. Using information technology to improve the quality and level of education and teaching, and promote the overall development of talents.},
keywords={Technological innovation;Smart cities;Design methodology;Education;Big Data;Information technology;Big data;Information technology;Education;Influence},
doi={10.1109/ICITBS49701.2020.00135},
ISSN={},
month={Jan},}
@INPROCEEDINGS{7207222,
author={Liu, Hong and Ashwin Kumar, T.K. and Thomas, Johnson P.},
booktitle={2015 IEEE International Congress on Big Data},
title={Cleaning Framework for Big Data - Object Identification and Linkage},
year={2015},
volume={},
number={},
pages={215-221},
abstract={Data is a valuable resource. The proper use of high-quality data can help make better predictions, analysis and decisions. Poor-quality data is detrimental to data analytics. Data from different sources may provide the same entities, but different identities. This becomes a concern particularly when large-scale heterogeneous data from multiple sources are integrated for other purposes. This paper aims to identify same or similar objects and link these associated objects together so that the data can be cleaned and combined efficiently. Our research harnesses both context and usage patterns of data items to determine relationships among objects. Our experimental results show that efficient linkage among multiple sources can be constructed using context and usage patterns.},
keywords={Context;Cleaning;Object recognition;Generators;Couplings;Markov processes;Data Cleaning;Object Identification;Object Linkage;Data Context;Usage Patterns},
doi={10.1109/BigDataCongress.2015.38},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7379507,
author={Krawczyk, Bartosz and Wozniak, Michal},
booktitle={2015 IEEE International Conference on Systems, Man, and Cybernetics},
title={Weighted Naïve Bayes Classifier with Forgetting for Drifting Data Streams},
year={2015},
volume={},
number={},
pages={2147-2152},
abstract={Mining massive data streams in real-time is one of the contemporary challenges for machine learning systems. Such a domain encompass many of difficulties hidden beneath the term of Big Data. We deal with massive, incoming information that must be processed on-the-fly, with lowest possible response delay. We are forced to take into account time, memory and quality constraints. Our models must be able to quickly process large collection of data and swiftly adapt themselves to occurring changes (shifts and drifts) in data streams. In this paper, we propose a novel version of simple, yet effective Naïve Bayes classifier for mining streams. We add a weighting module, that automatically assigns an importance factor to each object extracted from the stream. The higher the weight, the bigger influence given object exerts on the classifier training procedure. We assume, that our model works in the non-stationary environment with the presence of concept drift phenomenon. To allow our classifier to quickly adapt its properties to evolving data, we imbue it with forgetting principle implemented as weight decay. With each passing iteration, the level of importance of previous objects is decreased until they are discarded from the data collection. We propose an efficient sigmoidal function for modeling the forgetting rate. Experimental analysis, carried out on a number of large data streams with concept drift prove that our weighted Naïve Bayes classifier displays highly satisfactory performance in comparison with state-of-the-art stream classifiers.},
keywords={Training;Adaptation models;Data mining;Memory management;Detectors;Data models;Probability;machine learning;data stream;concept drift;big data;incremental learning;forgetting},
doi={10.1109/SMC.2015.375},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7840696,
author={Mukherjee, Tathagata and Parajuli, Biswas and Kumar, Piyush and Pasiliao, Eduardo},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={TruthCore: Non-parametric estimation of truth from a collection of authoritative sources},
year={2016},
volume={},
number={},
pages={976-983},
abstract={Truth Finding is the problem of determining correct information from several conflicting sources and is required for data aggregation. Existing algorithms solve the problem by simultaneously estimating source qualities and fact confidences, working on either numeric or non-numeric data. However, in practice, datasets are a mixture of several different data types. In this work we present a unified framework for finding truth from a collection of conflicting, authoritative sources. We assume that a small subset of independent reliable sources are selected by a preprocessing step. We formulate truth finding as an outlier removal problem, by modeling the similarities between the values reported by these sources. Our algorithm works in two stages: it first models the similarity graph between the sources and then finds the truth by invoking an outlier removal algorithm. We report experiments on several datasets including results for fixing records in Open Library; an open, editable library catalog.},
keywords={Measurement;Silicon;Open systems;Numerical models;Big data;Machine learning algorithms;Data models;Data Mining;Machine Learning;Big Data;Geometric Algorithms},
doi={10.1109/BigData.2016.7840696},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8530821,
author={Jing, Wang and Chenglong, He and Wei, Wu and Guilin, Zhang},
booktitle={2018 Sixth International Conference on Advanced Cloud and Big Data (CBD)},
title={Track Quality Evaluation Method Research on Tactical Data Link},
year={2018},
volume={},
number={},
pages={89-92},
abstract={Track quality (TQ) evaluation of tactical data link is most crucial content in track processing, and is also the basis of tactical data link. In this paper, a new assessment method is presented to calculate the TQ. At first, TQ area equation defined in the U.S. Army tactical data link formatted message standard (TADIL) 6016B is introduced. Meanwhile, according to the educing and analyzing, this paper shows that the track quality in U.S. Army data link is defined according to the ellipse expression based on the χ2 distributing, and the TQ acreage value equals to the acreage of ellipse. Moreover, a new method of distance measurement on TQ value is proposed. At last, simulations show the affection of the geography location error, radar measurement error and extrapolation error on the TQ value.},
keywords={Radar tracking;Azimuth;Extrapolation;Navigation;Target tracking;Reliability;TQ(Track Quality) Tactical Data link, x^2 distributing},
doi={10.1109/CBD.2018.00025},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8260155,
author={Bagheri, Azam and Bollen, Math H.J. and Gu, Irene Y.H.},
booktitle={2017 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe)},
title={Big data from smart grids},
year={2017},
volume={},
number={},
pages={1-5},
abstract={This paper gives a general introduction to “Big Data” in general and to Big Data in smart grids in particular. Large amounts of data (Big Data) contains a lots of information, however developing the analytics to extract such information is a big challenge due to some of the particular characteristics of Big Data. This paper investigates some existing analytic algorithms, especially deep learning algorithms, as tools for handling Big Data. The paper also explains the potential for deep learning application in smart grids.},
keywords={Big Data;Smart grids;Machine learning;Complexity theory;Classification algorithms;Power quality;Feature extraction;Smart grid;Big Data;deep learning;power quality;voltage dip},
doi={10.1109/ISGTEurope.2017.8260155},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7841699,
author={Mosleh, Susanna and Liu, Lingjia and Hou, Hongyan and Yi, Yang},
booktitle={2016 IEEE Global Communications Conference (GLOBECOM)},
title={Coordinated Data Assignment: A Novel Scheme for Big Data over Cached Cloud-RAN},
year={2016},
volume={},
number={},
pages={1-6},
abstract={A cloud radio access network (Cloud-RAN) is a network architecture that holds onto the promise of meeting the explosive growth of mobile data traffic. Cloud-RAN consists a central processor (CP) connecting to multiple multi-antenna base stations (BSs) via finite-capacity backhaul links. To reduce the backhaul traffic, BS-level caching technique is utilized in which the popular contents are pre-fetched in memories at each BS. This technique plays an important role in future wireless big data processing due to its simplicity, low cost, and natural integration with big data analytical tools. Considered the tradeoff between the backhaul and the transmission power cost, in this paper we define the network cost of the system as a normalized weighted sum. The problem of minimizing the network cost with respect to both the precoding matrix and the cache placement matrix is formulated subject to the quality of service (QoS), peak transmission power, and cache capacity constraints. The l0-norm in the objective function along with the QoS constraints renders the optimization problem non-convex. Additionally, since the entries of cache placement matrix take binary values, the optimization problem falls into a mixed integer nonlinear programming (MINLP) which is a NP-hard problem. An iterative coordinated data assignment algorithm is introduced which achieves a stationary point of the problem. Simulations are conducted to illustrate the performance of introduced algorithm. It suggests that the introduced scheme can significantly reduce the total network cost of the underlying Cloud-RAN network and demonstrate the importance of considering the designing of cache placement matrix.},
keywords={Quality of service;Interference;Array signal processing;Wireless communication;Optimization;Algorithm design and analysis;Signal to noise ratio},
doi={10.1109/GLOCOM.2016.7841699},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8332871,
author={Li, Fei},
booktitle={2018 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={University Ideological Education Quality Assessment Based on Fuzzy Comprehensive Evaluation},
year={2018},
volume={},
number={},
pages={725-728},
abstract={How to improve ideological teaching quality to enhance the competitiveness of the university has become the important subject of each university. In this paper, through the study of the characteristics of teacher quality, teaching evaluation and the main factors influencing the teaching evaluation, on the basis of analyzing the advantages and disadvantages of the existing teaching evaluation system, we formulate a set of feasible evaluation index system for ideological teaching. Then, we use the fuzzy comprehensive evaluation method to calculate the evaluation data, which ensures that the results of evaluation is scientific and accurate. The struts 2, JSP, and Hibernate technology is used to develop a kind of online teaching evaluation system based on B/S structure. The experiment results show that the designed system has good stability.},
keywords={Conferences;Transportation;Big Data;Smart cities;ideological teaching quality;fuzzy comprehensive evaluation;evaluation index},
doi={10.1109/ICITBS.2018.00187},
ISSN={},
month={Jan},}
@INPROCEEDINGS{9196495,
author={Jin, Li},
booktitle={2020 International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)},
title={Exploring the Determinants of Massive Open Online Courses Reusage Intention in Humanities},
year={2020},
volume={},
number={},
pages={238-241},
abstract={With the rapid improvement of information technology and big data analytics, massive open online courses (MOOCs) have become important in the learning process and have been increasingly adopted in higher education. Meanwhile, MOOCs application reusage intention as part of learning persistence in the context of MOOCs has gained considerable attention from digital learning because of its influence on learning performance and interruption. However, few studies investigated MOOCs application reusage intention in humanities domain. Thus, an integrated model was proposed based on survey-type experiment, focusing on the exploration MOOC education usage toward both structured data. Firstly, an experiment survey was conducted, and responses from university students were analyzed using structural equation modeling. The findings reveal that perception factors are determinants of MOOCs application reusage intention. Furthermore, the results improve the understanding of the relationships among perception factors, relationship quality, and reusage intention. Relationship quality not only exerts direct influence on reusage intention but also plays indirect influence between perception factors and reusage intention. Therefore, this study contributes to investigating the determinants of reusage intention from individual perception factors and relationship quality of MOOCs application learning intention in humanities disciplines (e.g. music education). Accordingly, MOOCs learners' experience will be enhanced due to the implication and discussion.},
keywords={Big Data;Artificial intelligence;Internet of Things;Data models;Conferences;Semantics;massive open online courses;learner features;structured data model;reusage intention},
doi={10.1109/ICBAIE49996.2020.00056},
ISSN={},
month={June},}
@INPROCEEDINGS{9231625,
author={Beny Pangestu, M and Ridho Barakbah, Ali and Hadiah Muliawati, Tri},
booktitle={2020 International Electronics Symposium (IES)},
title={Data Analytics for Hotel Reviews in Multi-Language based on Factor Aggregation of Sentiment Polarization},
year={2020},
volume={},
number={},
pages={324-331},
abstract={Online booking platform eases customer to book hotel easily prior to the arrival date. However, problem arises if customer thinks hotel quality is not as good as promised on online booking platform. Hotel rating which is presented on online booking platform is not sufficient to fully represent hotel quality in terms of services and facilities. To fill in the gap, hotel reviews can be used to depict hotel quality in details. However, hotel reviews can be written in multi-language and the amount is too much, which makes it harder to be understood. On the other hand, hotel's management also needs to regularly monitor hotel quality perceived by the customer in order to maintain and improve hotel quality. Therefore, this research proposes system to analyze, classify, and predict hotel quality using aggregation of hotel rating and review. The proposed system uses factor aggregation of sentiment polarization approach in vaderSentiment and SentiwordNet which uses different methods (Lexicon-based and Rule-based) to calculate sentiment degree of hotel reviews. Before being analyzed using sentiment analysis, multi-language review is standardized automatically into English. Descriptive and predictive analysis are conducted in big data environment using Hierarchical K-Means Clustering and Smoothing Exponential method respectively. The results of sentiment aggregation that are used as a feature in descriptive and predictive method enriches the analysis for the better result Performance of this approach is 12,47% for prediction error and 4,61% error rate for clustering model. It produced better result hotel segmentation and predictions of hotel quality. This study can give an advantage for both the customer and the hotel management in analyzing the development of hotel quality.},
keywords={Sentiment analysis;Big Data;Task analysis;Data analysis;Classification algorithms;Social networking (online);Computer architecture;hotel reviews;data analytics;multi-language sentiment analysis;big-data;agregation of sentiment polarization},
doi={10.1109/IES50839.2020.9231625},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9338437,
author={Bi, Haoyang and Ma, Haiping and Huang, Zhenya and Yin, Yu and Liu, Qi and Chen, Enhong and Su, Yu and Wang, Shijin},
booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
title={Quality Meets Diversity: A Model-Agnostic Framework for Computerized Adaptive Testing},
year={2020},
volume={},
number={},
pages={42-51},
abstract={Computerized Adaptive Testing (CAT) is emerging as a promising testing application in many scenarios, such as education, game and recruitment, which targets at diagnosing the knowledge mastery levels of examinees on required concepts. It shows the advantage of tailoring a personalized testing procedure for each examinee, which selects questions step by step, depending on her performance. While there are many efforts on developing CAT systems, existing solutions generally follow an inflexible model-specific fashion. That is, they need to observe a specific cognitive model which can estimate examinee's knowledge levels and design the selection strategy according to the model estimation. In this paper, we study a novel model-agnostic CAT problem, where we aim to propose a flexible framework that can adapt to different cognitive models. Meanwhile, this work also figures out CAT solution with addressing the problem of how to generate both high-quality and diverse questions simultaneously, which can give a comprehensive knowledge diagnosis for each examinee. Inspired by Active Learning, we propose a novel framework, namely Model-Agnostic Adaptive Testing (MAAT) for CAT solution, where we design three sophisticated modules including Quality Module, Diversity Module and Importance Module. Specifically, at one CAT selection step, Quality Module first quantifies the informativeness of questions and generates candidate subset with the highest quality. Then, Diversity Module selects one question at each step that maximizes the concept coverage. Additionally, we propose Importance Module to estimate the importance of concepts that optimizes the CAT selection. Under MAAT, we prove that the goal of maximizing both quality and diversity is NP-hard, but we provide efficient algorithms by exploiting the inherent submodular property. Extensive experimental results on two real-world datasets clearly demonstrate that our MAAT can support CAT with guaranteeing both quality and diversity perspectives.},
keywords={Adaptation models;Cats;Computational modeling;Estimation;Games;Testing;Recruitment;Computerized Adaptive Testing, Model-Agnostic, Quality, Diversity},
doi={10.1109/ICDM50108.2020.00013},
ISSN={2374-8486},
month={Nov},}
@INPROCEEDINGS{8034995,
author={Wang, Congjie and Lu, Zhihui and Wu, Ziyan and Wu, Jie and Huang, Shalin},
booktitle={2017 IEEE International Conference on Services Computing (SCC)},
title={Optimizing Multi-Cloud CDN Deployment and Scheduling Strategies Using Big Data Analysis},
year={2017},
volume={},
number={},
pages={273-280},
abstract={With the substantial incensement of broadband networks, Internet applications have shifted from simple web browsing to content-centric applications. From the perspective of the content distributor, how to reduce the cost while satisfying quality of service and how to respond timely to users are of great concern. Currently, using multi-cloud technology is a feasible solution to provide more agile and scalable services. Meanwhile, big data techniques, such as Spark and Hadoop, can help content distributors make load-direct decisions more timely and accurately. In this paper, we present a multi-cloud architecture-supported resource allocation and scheduling optimized strategy through CDN (content delivery network) operation big data analysis. We firstly analyze quantities of CDN operation log data on Spark to evaluate quality of service (QoS) between end users and multi-cloud-based CDN operator. Then we perform a long-term resource deployment algorithm to book the minimum resources to meet users' requests with higher QoS and lower cost. We apply the prediction model ARIMA on Spark to predict the short term demand through analyzing a longer time series data. When the predicted resources cannot satisfy burst demand, we design a new multi-cloud extension algorithm to schedule additional cloud resource to handle overload requests and use precopying algorithm to select media contents to be stored in the new prepared cloud. We implement and evaluate our scheme with real operation logs data provided by China's biggest CDN distributor to show the efficiency of our algorithms.},
keywords={Cloud computing;Quality of service;Prediction algorithms;Urban areas;Sparks;Algorithm design and analysis;Computer architecture;CDN;QoS;Multi-cloud;cloud resource prediction;resource scheduling;Big Data;Spark},
doi={10.1109/SCC.2017.42},
ISSN={2474-2473},
month={June},}
@ARTICLE{7439865,
author={Berberidis, Dimitris and Kekatos, Vassilis and Giannakis, Georgios B.},
journal={IEEE Transactions on Signal Processing},
title={Online Censoring for Large-Scale Regressions with Application to Streaming Big Data},
year={2016},
volume={64},
number={15},
pages={3854-3867},
abstract={On par with data-intensive applications, the sheer size of modern linear regression problems creates an ever-growing demand for efficient solvers. Fortunately, a significant percentage of the data accrued can be omitted while maintaining a certain quality of statistical inference with an affordable computational budget. This work introduces means of identifying and omitting less informative observations in an online and data-adaptive fashion. Given streaming data, the related maximum-likelihood estimator is sequentially found using first- and second-order stochastic approximation algorithms. These schemes are well suited when data are inherently censored or when the aim is to save communication overhead in decentralized learning setups. In a different operational scenario, the task of joint censoring and estimation is put forth to solve large-scale linear regressions in a centralized setup. Novel online algorithms are developed enjoying simple closed-form updates and provable (non)asymptotic convergence guarantees. To attain desired censoring patterns and levels of dimensionality reduction, thresholding rules are investigated too. Numerical tests on real and synthetic datasets corroborate the efficacy of the proposed data-adaptive methods compared to data-agnostic random projection-based alternatives.},
keywords={Linear regression;Maximum likelihood estimation;Signal processing algorithms;Distributed databases;Big data;Approximation algorithms;Parameter estimation;least squares;big data;Parameter estimation;least squares;stochastic approximation;big data;support vector machines;data reduction;censoring;LMS;RLS;random projections},
doi={10.1109/TSP.2016.2546225},
ISSN={1941-0476},
month={Aug},}
@INPROCEEDINGS{7175897,
author={Camacho, David},
booktitle={2015 IEEE 2nd International Conference on Cybernetics (CYBCONF)},
title={Bio-inspired clustering: Basic features and future trends in the era of Big Data},
year={2015},
volume={},
number={},
pages={1-6},
abstract={Clustering is perhaps one of the most popular approaches used in unsupervised machine learning. There's a huge number of different methods and algorithms that have been designed in the last decades related to this “blind pattern search”, some of these approaches are based on bio-inspired methods such as Evolutionary Computation, Swarm Intelligence or Neural Networks among others. In the last years, and due to the fast growing of Big Data problems, some interesting advances and new approaches are currently being developed in this area, new algorithms like online clustering and streaming clustering are appearing. These new algorithms try to solve classical problems in Clustering and deal with the new features of these new kind of problems. This keynote lecture will provide some basics on both, Clustering methods and bio-inspired computation, and how they have been combined to improve the quality of these algorithms, to later show the main features that Big Data needs to obtain reliable clustering approaches. Finally, some practical examples and applications will be described to show how these new algorithms are evolving to be used in the near future in complex and dynamic environments.},
keywords={Clustering algorithms;Algorithm design and analysis;Big data;Data mining;Particle swarm optimization;Data analysis;Clustering algorithms;Bio-inspired algorithms;Big Data methods and applications},
doi={10.1109/CYBConf.2015.7175897},
ISSN={},
month={June},}
@ARTICLE{6613488,
author={Dou, Wanchun and Zhang, Xuyun and Liu, Jianxun and Chen, Jinjun},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={HireSome-II: Towards Privacy-Aware Cross-Cloud Service Composition for Big Data Applications},
year={2015},
volume={26},
number={2},
pages={455-466},
abstract={Cloud computing promises a scalable infrastructure for processing big data applications such as medical data analysis. Cross-cloud service composition provides a concrete approach capable for large-scale big data processing. However, the complexity of potential compositions of cloud services calls for new composition and aggregation methods, especially when some private clouds refuse to disclose all details of their service transaction records due to business privacy concerns in cross-cloud scenarios. Moreover, the credibility of cross-clouds and on-line service compositions will become suspicional, if a cloud fails to deliver its services according to its “promised” quality. In view of these challenges, we propose a privacy-aware cross-cloud service composition method, named HireSome-II (History record-based Service optimization method) based on its previous basic version HireSome-I. In our method, to enhance the credibility of a composition plan, the evaluation of a service is promoted by some of its QoS history records, rather than its advertised QoS values. Besides, the k-means algorithm is introduced into our method as a data filtering tool to select representative history records. As a result, HireSome-II can protect cloud privacy, as a cloud is not required to unveil all its transaction records. Furthermore, it significantly reduces the time complexity of developing a cross-cloud service composition plan as only representative ones are recruited, which is demanded for big data processing. Simulation and analytical results demonstrate the validity of our method compared to a benchmark.},
keywords={History;Cloud computing;Quality of service;Information management;Data handling;Data storage systems;Clustering algorithms;Cloud;service composition;QoS;big data;transaction history records},
doi={10.1109/TPDS.2013.246},
ISSN={1558-2183},
month={Feb},}
@INPROCEEDINGS{8574573,
author={Lv, Yirong and Sun, Bin and Luo, Qingyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai},
booktitle={2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
title={CounterMiner: Mining Big Performance Data from Hardware Counters},
year={2018},
volume={},
number={},
pages={613-626},
abstract={Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running in a "24/7/365" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance. In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.},
keywords={Hardware;Time series analysis;Program processors;Cloud computing;Microarchitecture;Data mining;Benchmark testing;Performance;big data;computer architecture;performance counters;data mining},
doi={10.1109/MICRO.2018.00056},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6693469,
author={Lu, Tianbo and Guo, Xiaobo and Xu, Bing and Zhao, Lingling and Peng, Yong and Yang, Hongyu},
booktitle={2013 International Conference on Social Computing},
title={Next Big Thing in Big Data: The Security of the ICT Supply Chain},
year={2013},
volume={},
number={},
pages={1066-1073},
abstract={In contemporary society, with supply chains becoming more and more complex, the data in supply chains increases by means of volume, variety and velocity. Big data rise in response to the proper time and conditions to offer advantages for the nodes in supply chains to solve prewiously difficult problems. For any big data project to succeed, it must first depend on high-quality data but not merely on quantity. Further, it will become increasingly important in many big data projects to add external data to the mix and companies will eventually turn from only looking inward to also looking outward into the market, which means the use of big data must be broadened considerably. Hence the data supply chains, both internally and externally, become of prime importance. ICT (Information and Telecommunication) supply chain management is especially important as supply chain link the world closely and ICT supply chain is the base of all supply chains in today's world. Though many initiatives to supply chain security have been developed and taken into practice, most of them are emphasized in physical supply chain which is addressed in transporting cargos. The research on ICT supply chain security is still in preliminary stage. The use of big data can promote the normal operation of ICT supply chain as it greatly improve the data collecting and processing capacity and in turn, ICT supply chain is a necessary carrier of big data as it produces all the software, hardware and infrastructures for big data's collection, storage and application. The close relationship between big data and ICT supply chain make it an effective way to do research on big data security through analysis on ICT supply chain security. This paper first analyzes the security problems that the ICT supply chain is facing in information management, system integrity and cyberspace, and then introduces several famous international models both on physical supply chain and ICT supply chain. After that the authors describe a case of communication equipment with big data in ICT supply chain and propose a series of recommendations conducive to developing secure big data supply chain from five dimensions.},
keywords={big data;ICT supply chain model;information security;system integrity;cyberattacks},
doi={10.1109/SocialCom.2013.172},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7918019,
author={Kaur, Navneet and Bawa, Navneet},
booktitle={2016 2nd International Conference on Contemporary Computing and Informatics (IC3I)},
title={Algorithm for fuzzy based compression of gray JPEG images for big data storage},
year={2016},
volume={},
number={},
pages={518-523},
abstract={In the current era of big data and data mining, efficient storage of images is very important. In the collected datasets from blogs or social network sites, images are one of the prominent part of that datasets. Images took very large space for storage and even loading when required by the third party. So, image compression is one of the important part in big data and effective data mining. These applications require rapid image processing both at the front and back end. So, one of the most important step in storing and retrieving images is the effective compression of images. Images should be stored in compressed form and compression should not decrease the quality of the image. Many standards for compression of grey images are available. However, this area is still open for research purpose. Moreover, increase in variety of images over the internet demand the use of fuzzy based compression techniques, as also mentioned. In this paper, fuzzy based technique is used for compressing the grey JPEG images. It provides high level of compression and reduced level of errors in the images. Proposed technique also reduced different type of artifacts such as blocking artifacts, ringing artifacts and false contouring. Proposed compression can be effectively used for big data based storing and retrieval of images.},
keywords={Handheld computers;Decision support systems;Informatics;Conferences;JPEG;Big Data;Image Compression;Fuzzy Based Compression},
doi={10.1109/IC3I.2016.7918019},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8982563,
author={Yoo, Yeisol and Yoo, Jin Soung},
booktitle={2019 IEEE International Conferences on Ubiquitous Computing & Communications (IUCC) and Data Science and Computational Intelligence (DSCI) and Smart Computing, Networking and Services (SmartCNS)},
title={RFID Data Warehousing and OLAP with Hive},
year={2019},
volume={},
number={},
pages={476-483},
abstract={Radio Frequency Identification (RFID) technology is used in many applications for monitoring object movement. The use of RFID in supply chain management systems enables to track the movement of products from suppliers to warehouses, store backrooms, and eventually points of sale. The vast amount of data resulting from the proliferation of RFID readers and tags poses challenges for data management and analytics. RFID data warehousing can enhance data quality and consistency, and give great potential benefits for Online Analytical Processing (OLAP) applications. Traditional data warehouses are built primarily on relational database management systems. However, the size of RFID data being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hive is an open-source data warehousing solution built on top of Hadoop which is a popular Big Data computing framework. This paper presents alternative RFID data warehouse designs which can handle a large amount of RFID data and support a variety of OLAP queries. The proposed approaches are implemented on Hive and evaluated for query performance in cloud computing environment.},
keywords={RFID data warehousing;OLAP;Hive;Cloud computing},
doi={10.1109/IUCC/DSCI/SmartCNS.2019.00105},
ISSN={},
month={Oct},}
@ARTICLE{8546785,
author={Hou, Boyi and Chen, Qun and Chen, Zhaoqiang and Nafa, Youcef and Li, Zhanhuai},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={r-HUMO: A Risk-Aware Human-Machine Cooperation Framework for Entity Resolution with Quality Guarantees},
year={2020},
volume={32},
number={2},
pages={347-359},
abstract={Even though many approaches have been proposed for entity resolution (ER), it remains very challenging to enforce quality guarantees. To this end, we propose a risk-aware HUman-Machine cOoperation framework for ER, denoted by r-HUMO. Built on the existing HUMO framework, r-HUMO similarly enforces both precision and recall guarantees by partitioning an ER workload between the human and the machine. However, r-HUMO is the first solution that optimizes the process of human workload selection from a risk perspective. It iteratively selects human workload by real-time risk analysis based on the human-labeled results as well as the prespecified machine metric. In this paper, we first introduce the r-HUMO framework and then present the risk model to prioritize the instances for manual inspection. Finally, we empirically evaluate r-HUMO's performance on real data. Our extensive experiments show that r-HUMO is effective in enforcing quality guarantees, and compared with the state-of-the-art alternatives, it can achieve desired quality control with reduced human cost.},
keywords={Erbium;Measurement;Crowdsourcing;Inspection;Risk analysis;Man-machine systems;Task analysis;Entity resolution;human-machine cooperation;risk analysis;quality guarantee},
doi={10.1109/TKDE.2018.2883532},
ISSN={1558-2191},
month={Feb},}
@INPROCEEDINGS{9006206,
author={Angriman, Eugenio and van der Grinten, Alexander and Meyerhenke, Henning},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Local Search for Group Closeness Maximization on Big Graphs},
year={2019},
volume={},
number={},
pages={711-720},
abstract={In network analysis and graph mining, closeness centrality is a popular measure to infer the importance of a vertex. Computing closeness efficiently for individual vertices received considerable attention. The NP-hard problem of group closeness maximization, in turn, is more challenging: the objective is to find a vertex group that is central as a whole and state-of the-art heuristics for it do not scale to very big graphs yet.In this paper, we present new local search heuristics for group closeness maximization. By using randomized approximation techniques and dynamic data structures, our algorithms are often able to perform locally optimal decisions efficiently. The final result is a group with high (but not optimal) closeness centrality. We compare our algorithms to the current state-of-the-art greedy heuristic both on weighted and on unweighted real-world graphs. For graphs with hundreds of millions of edges, our local search algorithms take only around ten minutes, while greedy requires more than ten hours. Overall, our new algorithms are between one and two orders of magnitude faster, depending on the desired group size and solution quality. For example, on weighted graphs and k =10, our algorithms yield solutions of 12.4% higher quality, while also being 793. 6× faster. For unweighted graphs and k =10, we achieve solutions within 99.4% of the state-of-the-art quality while being 127. 8× faster.},
keywords={Approximation algorithms;Greedy algorithms;Heuristic algorithms;Big Data;Linear programming;Data structures;Acceleration;centrality;group closeness;graph mining;network analysis},
doi={10.1109/BigData47090.2019.9006206},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8117153,
author={ur Rehman, Shafiq and Hark, Andre and Gruhn, Volker},
booktitle={2017 8th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)},
title={A framework to handle big data for cyber-physical systems},
year={2017},
volume={},
number={},
pages={72-78},
abstract={The use of big data for cyber-physical systems (CPS) is gaining more importance due to the ever-increasing amount of collectable data. Due to the decreasing cost of sensors and the growth of embedded systems, which are increasingly used in the industries as well as in the private sectors, new methods are needed to evaluate and process the collected data. Therefore, in this paper we proposed a framework to handle big data for cyber-physical systems. The framework considered the possible solutions that would be standardization, cloud computing, online and data stream learning, a methodology to process data and multi-agent systems for CPS. Furthermore, we examine the security challenges and big data issues of cyber-physical systems.},
keywords={Big Data;Cyber-physical systems;Safety;Real-time systems;Sensors;Big data;cyber-physical system (CPS);security;real-time;standardization;infrastructure;data quality},
doi={10.1109/IEMCON.2017.8117153},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7237115,
author={Klous, Sander},
booktitle={2015 International Conference on High Performance Computing & Simulation (HPCS)},
title={A new reality requiers new ecosystems},
year={2015},
volume={},
number={},
pages={665-666},
abstract={We live in a time where borders between organizations are vanishing. A time where division equals multiplication. Where Open Source is the new alternative for a patent application. And in a time where Big Data offers new ways to add value to business and society. Such a time requires new ways of collaboration and organization of partners in the value chain. What we need is ecosystems that assure responsible application of data analysis and stimulate quality and innovation. In such a way that participants can easily plug in and out.},
keywords={Big data;Ecosystems;Standards organizations;Companies;Patents;Big Data;Data Analytics;Ecosystems;Privacy},
doi={10.1109/HPCSim.2015.7237115},
ISSN={},
month={July},}
@INPROCEEDINGS{8514403,
author={de Souza, Paulo R.R. and Matteussi, Kassiano J. and dos Anjos, Julio C.S. and dos Santos, Jobe D.D. and Geyer, Claudio Fernando Resin and da Silva Veith, Alexandre},
booktitle={2018 International Conference on High Performance Computing & Simulation (HPCS)},
title={Aten: A Dispatcher for Big Data Applications in Heterogeneous Systems},
year={2018},
volume={},
number={},
pages={585-592},
abstract={Stream Processing Engines (SPEs) have to support high data ingestion to ensure the quality and efficiency for the end-user or a system administrator. The data flow processed by SPE fluctuates over time, and requires real-time or near real-time resource pool adjustments (network, memory, CPU and other). This scenario leads to the problem known as skewed data production caused by the non-uniform incoming flow at specific points on the environment, resulting in slow down of applications caused by network bottlenecks and inefficient load balance. This work proposes Aten as a solution to overcome unbalanced data flows processed by Big Data Stream applications in heterogeneous systems. Aten manages data aggregation and data streams within message queues, assuming different algorithms as strategies to partition data flow over all the available computational resources. The paper presents preliminary results indicating that is possible to maximize the throughput and also provide low latency levels for SPEs.},
keywords={Big Data;Real-time systems;Throughput;Data models;Parallel processing;Engines;Batch production systems},
doi={10.1109/HPCS.2018.00098},
ISSN={},
month={July},}
@INPROCEEDINGS{9005715,
author={Gjoreski, Martin and Kochev, Stefan and Reščič, Nina and Gregorič, Matej and Eftimov, Tome and Seljak, Barbara Koroušič},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Exploring Dietary Intake Data collected by FPQ using Unsupervised Learning},
year={2019},
volume={},
number={},
pages={5126-5130},
abstract={Populations in countries undergoing rapid transition are experiencing food- and nutrition-related problems. To acquire high-quality nutrition information, we need beside adequate data about food consumption, also efficient methods for the extraction of information from the collected data. Our aim was to develop a methodology for analyzing and reasoning about dietary intake data collected by a food propensity questionnaire (FPQ) and dependent 24-hour recalls (24HRs). We analysed a subset of data (about 197 participants) in the SI.Menu survey carried out in 2016/17 in Slovenia. The participants completed FPQs and 24HRs. We were able to identify four clusters. Two clusters represented participants with more healthy habits, e.g., low intake of animal fats, high breakfast frequency, and high intake of fruits and vegetables. The other two clusters represented participants with less healthy habits, e.g., high intake of animal fats, low breakfast frequency and increased BMI. The four clusters can be well separated by only four variables. This interesting discovery could lead to simplified FFQ questionnaires, which could significantly decrease the participants' burden and could ensure participant compliance in similar studies. Having big national data set related to nutrition should ease the process of creating sustainable policies that will ultimately benefit agriculture, human health and the environment.},
keywords={Fats;Data mining;Clustering algorithms;Dairy products;Unsupervised learning;Proteins;Big Data;FPQ;24HRs;clustering;healthy habits},
doi={10.1109/BigData47090.2019.9005715},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7463863,
author={Zheng, Xinying and Cai, Yu},
booktitle={2015 IEEE International Conference on Smart City/SocialCom/SustainCom (SmartCity)},
title={Energy-Efficient Statistical Live Virtual Machine Placement for Big Data Information Systems in Cloud Computing Environments},
year={2015},
volume={},
number={},
pages={1053-1058},
abstract={With the increasing applications of big data computing on large scale cloud platforms, virtual machines (VM) are utilized to provide flexibility and availability for big data information systems. Energy efficient VM management and distribution on cloud platforms has become an important research subject. Mapping VMs "correctly" into PMs (Physical Machines) requires knowing the capacity of each PM and the resource requirements of the VMs. It must also take into accounts VM operation overheads, the reliability of PMs, Quality of Service (QoS) in addition to energy efficiency. However, the current VM placement approaches are mostly built in a server cluster with homogeneous nodes. Moreover, those approaches are not effective for live migrations with multiple considerations at the same time. In this paper, we propose an energy efficient statistical live VM placement scheme in a heterogeneous server clusters. Our scheme supports VM requests scheduling and live migration to minimize the number of active servers in order to save the overall energy in a virtualized server cluster. Specifically, the proposed VM placement scheme incorporates all the VM operation overheads in the dynamic migration process. In addition, it considers other important factors in relation to energy consumption, and it is ready to be extended with more considerations on users demand. We conduct extensive evaluations based on HPC jobs in a simulated environment. The results prove the effectiveness of our scheme.},
keywords={Servers;Cloud computing;Reliability;Quality of service;Big data;Virtual machining;Resource management;Cloud computing;virtual machine placement;energy efficiency;big data;information systems},
doi={10.1109/SmartCity.2015.208},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9260278,
author={Yu, Liu},
booktitle={2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Analysis on The Demand Evolution of Cloud Accounting Information System in The Era of Big Data},
year={2020},
volume={},
number={},
pages={350-353},
abstract={Big data can provide effective support for enterprise's financial decision-making, but how to build accounting information system(AIS) under big data environment is the primary issue that cloud accounting suppliers must consider with. Based on the analysis of the impact of cloud AIS, the implementation framework of AIS with cloud accounting is established, and the role of each module in the framework is described. Then, the complex network theory is introduced to model the AIS products of cloud accounting and calculate the structural eigenvalues. The relationship between the structural eigenvalues and the credibility attributes of the AIS products of cloud accounting is also established by the structural eigenvalues. Finally, through the analysis of cloud AIS products provided by cloud accounting suppliers, according to the needs of enterprise users as a specific sample software, it verifies the structure and credibility of the cloud accounting AIS products proposed in this paper, as well as the effectiveness of management methods.},
keywords={Smart grids;Quality function deployment;Optical fibers;Hafnium;Conferences;Automation;AIS;big data;propagation;trusted requirement;evolution},
doi={10.1109/ICSGEA51094.2020.00081},
ISSN={},
month={June},}
@ARTICLE{7579568,
author={Conforti, Raffaele and Rosa, Marcello La and Hofstede, Arthur H.M. ter},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Filtering Out Infrequent Behavior from Business Process Event Logs},
year={2017},
volume={29},
number={2},
pages={300-314},
abstract={In the era of “big data”, one of the key challenges is to analyze large amounts of data collected in meaningful and scalable ways. The field of process mining is concerned with the analysis of data that is of a particular nature, namely data that results from the execution of business processes. The analysis of such data can be negatively influenced by the presence of outliers, which reflect infrequent behavior or “noise”. In process discovery, where the objective is to automatically extract a process model from the data, this may result in rarely travelled pathways that clutter the process model. This paper presents an automated technique to the removal of infrequent behavior from event logs. The proposed technique is evaluated in detail and it is shown that its application in conjunction with certain existing process discovery algorithms significantly improves the quality of the discovered process models and that it scales well to large datasets.},
keywords={Big data;Data mining;Data models;Business;Information analysis;Behavioral science;Filtering;Data analysis;Business process management;process mining;infrequent behavior},
doi={10.1109/TKDE.2016.2614680},
ISSN={1558-2191},
month={Feb},}
@INPROCEEDINGS{7004450,
author={Conlan, Owen and O'Connor, Alexander and Loinsigh, Órla Ní and Munnelly, Gary and Lawless, Séamus and Murphy, Rachel},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Revolutionary entities: Turning data into knowledge to drive personalized exploration of The irish rising of 1916},
year={2014},
volume={},
number={},
pages={32-38},
abstract={‘Big Data’ can mean something quite different in the context of Humanities. The way Humanities scholars frame their inquiries often leverages collections that are an order of magnitude smaller than the full, industrial scale, there is significant value to be found in the Humanistic sense of ‘Big’. In particular, the variety of the data, and the richness of the explorations, means that high-quality knowledge systems are required. More meaning is needed than the surface analytics often demonstrated in other ‘Big’ scenarios. This paper examines how a specific collection related to the 1916 Rising in Ireland was analyzed. The result was a process to extract entities that underpinned a highly-effective personalized knowledge-driven exploration of that collection by users. It demonstrates the mutual benefit of natural language at scale with rich humanistic inquiry to communicate improved experiences for a much broader range of users than would otherwise be possible.},
keywords={Materials;Cultural differences;Big data;History;Global communication;Communities;Transducers;Entity Extraction;Personalized Exploration;Historical Corpora},
doi={10.1109/BigData.2014.7004450},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6927666,
author={Ba, Huafeng and Gao, Xiaoming and Zhang, Xiaofeng and He, Zhenyu},
booktitle={2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)},
title={Protecting Data Privacy from Being Inferred from High Dimensional Correlated Data},
year={2014},
volume={2},
number={},
pages={495-502},
abstract={In the era of big data, privacy becomes a challenging issue which already attracts a good number of research efforts. In the literature, most of existing privacy preserving algorithms focus on protecting users' privacy from being disclosed by making the set of designated semi-id features indiscriminate. However, how to automatically determine the appropriate semi-id features from high-dimensional correlated data is seldom studied. Therefore, in this paper we first theoretically study the problem and propose the IPFS algorithm to find all possible features forming the candidate semi-id feature set which can infer users' privacy. Then, the KIPFS algorithm is proposed to find the key features from the candidate semi-id feature set. By anonymizing the key feature set, called as key inferring privacy features (KIPFS), users' privacy is protected. To evaluate the effectiveness and the efficacy of the proposed approach, two state-of-the-art algorithms, i.e., K-anonymity and t-closeness, applied on the designated semi-id feature set are chose as the baseline algorithms and their revised versions are applied on the KIPFS for the performance comparison. The promising results showed that by anonymizing the identified KIPFS, both aforementioned algorithms can achieve better performance than the original ones in terms of efficiency and data quality.},
keywords={Privacy;Data privacy;Algorithm design and analysis;Educational institutions;Equations;Computer science;Data analysis;privacy preserving data mining;data publishing;algorithm},
doi={10.1109/WI-IAT.2014.139},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9345133,
author={Yuan, Fang and Hong, Xianbin and Yuan, Cheng and Fei, Xiang and Guan, Sheng-Uei and Liu, Dawei and Wang, Wei},
booktitle={2020 IEEE 6th International Conference on Computer and Communications (ICCC)},
title={Keywords-oriented Data Augmentation for Chinese},
year={2020},
volume={},
number={},
pages={2006-2012},
abstract={In natural language processing tasks, data is very important, but data collection is not cheap. Large volume data can well serve a series of tasks, especially for deep learning tasks. Data augmentation methods are solutions to data problems, which can work well on rising data quality and quantity, such as generating text without meaning changing and expanding the diversity of data distribution. A user-friendly method of the data augmentation is to sample words in a text then augmenting them. The sampling method is often implemented by a random probability. Although the performance of this solution has been proved over the past few years, random sampling is not the best choice for the data augmentation as it has a chance of randomly introducing some noise into initial data, like stop words. The generated data could interfere with the subsequent tasks and drop the accuracy of the tasks' solutions. Hence, this paper aims to introduce a novel data augmentation method that could avoid involving such noisy data. The strategy is keywords-oriented data augmentation for Chinese (KDA). The KDA proposed in this paper indicates a method of extracting keywords based on category labels, and an augmenting method based on the keywords. In contrast to randomness, the proposed technique firstly selects the key information data, then expands the selected data. The experimental section is compared with another two typical data augmentation techniques on three Chinese data sets for text classification tasks. The result shows that the KDA technique has a better performance in the data augmentation task than the compared two.},
keywords={Deep learning;Data integrity;Text categorization;Sampling methods;Natural language processing;Noise measurement;Task analysis;Data Augmentation;Chinese;Classification},
doi={10.1109/ICCC51575.2020.9345133},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7179453,
author={Zhu, Julie Yixuan and Sun, Chenxi and Li, Victor O.K.},
booktitle={2015 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)},
title={Granger-Causality-based air quality estimation with spatio-temporal (S-T) heterogeneous big data},
year={2015},
volume={},
number={},
pages={612-617},
abstract={This paper considers city-wide air quality estimation with limited available monitoring stations which are geographically sparse. Since air pollution is highly spatio-temporal (S-T) dependent and considerably influenced by urban dynamics (e.g., meteorology and traffic), we can infer the air quality not covered by monitoring stations with S-T heterogeneous urban big data. However, estimating air quality using S-T heterogeneous big data poses two challenges. The first challenge is due to with the data diversity, i.e., there are different categories of urban dynamics and some may be useless and even detrimental for the estimation. To overcome this, we first propose an S-T extended Granger causality model to analyze all the causalities among urban dynamics in a consistent manner. Then by implementing non-causality test, we rule out the urban dynamics that do not “Granger” cause air pollution. The second challenge is due to the time complexity when processing the massive volume of data. We propose to discover the region of influence (ROI) by selecting data with the highest causality levels spatially and temporally. Results show that we achieve higher accuracy using “part” of the data than “all” of the data. This may be explained by the most influential data eliminating errors induced by redundant or noisy data. The causality model observation and the city-wide air quality map are illustrated and visualized using data from Shenzhen, China.},
keywords={Air pollution;Monitoring;Atmospheric modeling;Training;Estimation;Big data},
doi={10.1109/INFCOMW.2015.7179453},
ISSN={},
month={April},}
@INPROCEEDINGS{8668106,
author={Manjunatha and Annappa, B},
booktitle={2018 International Conference on Communication, Computing and Internet of Things (IC3IoT)},
title={Real Time Big Data Analytics in Smart City Applications},
year={2018},
volume={},
number={},
pages={279-284},
abstract={Technological revolution in the recent past has enabled the concept of Smart City for urban development. Smart City concept is conceived with the objectives of providing better services to the citizens and improves the quality of life. Information and Communication Technology (ICT) and Internet of Things (IoT) made smart city applications as much simpler and effective. Big data technologies play a major role in smart city applications. This paper gives an overview of the role of big data in building smart city applications and proposes a framework for real time big data analytics. Real-time big data analytics help in making better decisions and more accurate predictions at right time to offer better services to the citizens. Here, we discussed some of important solutions and services for the smart city where the real-time big data analytics helps in improving the quality of services in smart city applications.},
keywords={Real-time systems;Smart cities;Big Data;Internet of Things;Monitoring;Information and communication technology;Smart City;Big Data;Real-Time Analytics;Internet of Things},
doi={10.1109/IC3IoT.2018.8668106},
ISSN={},
month={Feb},}
@INPROCEEDINGS{8953033,
author={Li, Zheng and Pino, Esteban J.},
booktitle={2019 IEEE 12th Conference on Service-Oriented Computing and Applications (SOCA)},
title={D&D: A Distributed and Disposable Approach to Privacy Preserving Data Analytics in User-Centric Healthcare},
year={2019},
volume={},
number={},
pages={176-183},
abstract={Benefiting from the modern information and communication technologies, user centricity has become a clear evolution trend in healthcare. Unfortunately, given the high sensitivity of health data and the uncertainty in user environments, user-centric healthcare systems inevitably suffer from more frequent privacy threats, not to mention that technologies and business of data exploitation have generally outpaced the current privacy regulations and laws. Although there exist well-defined privacy preserving mechanisms, such as Data Encryption, Data Perturbation, and De-identification, they have been considered inadequate to address the diverse privacy challenges in big healthcare data analytics. Our argument is that, before considering any sophisticated mechanism, practitioners should first try to imitate human memory's forgetting process as an intrinsic privacy preserving strategy in user-centric healthcare. Technically, we implement this strategy by changing traditional data analytics routines into a distributed and disposable manner, so as to naturally exclude the data owners' sensitive information. The technical implementation essentially acts as a concrete How-To solution to satisfying a fundamental principle of privacy law, i.e. data minimization. We have initially applied our work to a smart bed project for sleep quality analytics, and received positive feedback on the effectiveness of privacy preservation in suitable homecare scenarios.},
keywords={Data privacy;Medical services;Privacy;Big Data;Data analysis;Distributed databases;Memory;big data analytics, homecare, microservice, privacy preservation, smart bed, user centric healthcare},
doi={10.1109/SOCA.2019.00033},
ISSN={2163-2871},
month={Nov},}
@INPROCEEDINGS{9378343,
author={Moon, Aekyeung and Woo Son, Seung and Jung, Jiuk and Jeong Song, Yun},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Understanding Bit-Error Trade-off of Transform-based Lossy Compression on Electrocardiogram Signals},
year={2020},
volume={},
number={},
pages={3494-3499},
abstract={The growing demand for recording longer ECG signals to improve the effectiveness of IoT-enabled remote clinical healthcare is contributing large amounts of ECG data. While lossy compression techniques have shown potential in significantly lowering the amount of data, investigation on how to trade-off between data reduction and data fidelity on ECG data received relatively less attention. This paper gives insight into the power of lossy compression to ECG signals by balancing between data quality and compression ratio. We evaluate the performance of transformed-based lossy compressions on the ECG datasets collected from the Biosemi ActiveTwo devices. Our experimental results indicate that ECG data exhibit high energy compaction property through transformations like DCT and DWT, thus could improve compression ratios significantly without hurting data fidelity much. More importantly, we evaluate the effect of lossy compression on ECG signals by validating the R-peak in the QRS complex. Our method can obtain low error rates measured in PRD (as low as 0.3) and PSNR (up to 67) using only 5% of the transform coefficients. Therefore, R-peaks in the reconstructed ECG signals are almost identical to ones in the original signals, thus facilitating extended ECG monitoring.},
keywords={Performance evaluation;Measurement uncertainty;Transforms;Medical services;Electrocardiography;Big Data;Monitoring;Transform coding;Lossy compression;IoT;Health care;R-peak;data fidelity},
doi={10.1109/BigData50022.2020.9378343},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8526677,
author={Melnykova, Nataliia and Marikutsa, Uliana and Kryvenchuk, Uriy},
booktitle={2018 IEEE 13th International Scientific and Technical Conference on Computer Sciences and Information Technologies (CSIT)},
title={The New Approaches of Heterogeneous Data Consolidation},
year={2018},
volume={1},
number={},
pages={408-411},
abstract={The article is suggested the approaches to the process of data consolidation, namely data of various origin of large sizes for analysis and prediction of the object's behavior of the studied area. The author identifies the problems of heterogeneous data consolidation and proposes stages of integration. The author evaluated the quality and usefulness of the consolidated data of the information product for quality decision-making. The basic stages of the procedure for assessing the quality of consolidated data are also identified.},
keywords={Data models;Decision making;Data warehouses;Semantics;Forecasting;Databases;Data analysis;Data Consolidation;States Spaces;Heterogeneous Data;Big Date;The Quality of Decision-Making},
doi={10.1109/STC-CSIT.2018.8526677},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9149151,
author={Guo, Weisi},
booktitle={ICC 2020 - 2020 IEEE International Conference on Communications (ICC)},
title={Partially Explainable Big Data Driven Deep Reinforcement Learning for Green 5G UAV},
year={2020},
volume={},
number={},
pages={1-7},
abstract={UAV enabled terrestrial wireless networks enables targeted user-centric service provisioning to en-richen both deep urban coverage and target various rural challenge areas. However, UAVs have to balance the energy consumption of flight with the benefits of wireless capacity delivery via a high dimensional optimisation problem. Classic reinforcement learning (RL) cannot meet this challenge and here, we propose to use deep reinforcement learning (DRL) to optimise both aggregate and minimum service provisioning. In order to achieve a trusted autonomy, the DRL agents have to be able to explain its actions for transparent human-machine interrogation. We design a Double Dueling Deep Q-learning Neural Network (DDDQN) with Prioritised Experience Replay (PER) and fixed Q-targets to achieve stable performance and avoid over-fitting, offering performance gains over naive DQN algorithms. We then use a big data driven case study and found that UAVs battery size determines the nature of its autonomous mission, ranging from an efficient exploiter of one hotspot (100% reward gain) to a stochastic explorer of many hotspots (60-150% reward gain). Using a variety of telecom and social media data, we infer driving Quality-of-Experience (QoE) and Quality-of-Service (QoS) metrics that are in contention with UAV power and communication constraints. Our greener UAVs (30-40% energy saved) address both quantitative QoS and qualitative QoE issues. Partial interpretability in the reinforcement learning is achieved using data features extracted in the hidden layers, offering an initial step for explainable AI (XAI) connecting machine intelligence with human expertise.},
keywords={5G mobile communication;Batteries;Wireless communication;Big Data;Machine learning;Optimization;big data;machine learning;deep reinforcement learning;radio resource management;UAV;energy efficiency;XAI},
doi={10.1109/ICC40277.2020.9149151},
ISSN={1938-1883},
month={June},}
@INPROCEEDINGS{9005445,
author={Lakshminarayan, Choudur and Ramakrishnan, Thiagarajan and Al-Omari, Awny and Bouaziz, Khaled and Ahmad, Faraz and Raghavan, Sri and Agarwal, Prama},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Model Management and Handwritten Character Recognition: An Enterprise Solution},
year={2019},
volume={},
number={},
pages={6110-6112},
abstract={Ease-of-use analytics at scale is the holy grail of industrial strength machine learning. In order to reap benefits from the mother-lode of business related data; tools, technologies, and analytical functions should operate in perpetual symphony to derive insightful business outcomes. While there have been advances in APIs, algorithms, and user interfaces, building an end to end workflow spanning data ingestion, data preparation, model training, model scoring, visualization and finally continuous improvement and model management received limited investment. In this paper we demonstrate an analytical workflow that integrates multiple analytical tools and techniques for image recognition wrapped in the model management framework. As analytics in industry is maturing, analytics implementations are no longer one-off, but are components of Analytics Operations known as AnalyticsOps. We introduce the notion of Model Quality Index (MQI) to track model performance. The MQI is similar to Process Capability Index (PCI) common in 6σ programs in manufacturing. Our solution combines relational databases (Teradata DB), Machine Learning (Teradata/Aster), Deep Learning (TensorFlow), Hadoop Distributed File System (HDFS), and user interface tools over a communication fabric (Teradata QueryGrid). In particular, we demonstrate a hand written word recognition use-case for an enterprise customer cast in a model management workflow for repeatable deployments across a range of businesses.},
keywords={Data models;Character recognition;Tools;Analytical models;Machine learning;Image recognition;Adaptation models;Character Recognition;Image Processing;Convolutional Neural Networks;Model Management;Model Quality Index;and 6 sigma},
doi={10.1109/BigData47090.2019.9005445},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7570892,
author={Dubey, Vigya and Agrawal, Pranjal},
booktitle={2016 Symposium on Colossal Data Analysis and Networking (CDAN)},
title={Cloud computing and data management},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Since the world is getting digitalized, cloud computing has become a core part of it. Huge Data on daily basis is proceed, stored, and transferred over the internet. Cloud computing is become quite popular because of its superlative quality but there are some issues related with security of data and how to manage the big data. Data security and data residency are the key concern of cloud. This paper deals with the one of the solution on security of data called “Encryption”. We are focused on the mathematical and logical solution of RES, one type of encryption. Later we deal with the management of big data using “Hadoop”.},
keywords={Cloud computing;Servers;Encryption;Operating systems;Software as a service;Hardware;cloud computing;data Security;encryption;RES;hadoop},
doi={10.1109/CDAN.2016.7570892},
ISSN={},
month={March},}
@INPROCEEDINGS{9513432,
author={Abdo, Asmaa S. and Salem, Rashed K. and Abdul-Kader, Hatem M.},
booktitle={2015 25th International Conference on Computer Theory and Applications (ICCTA)},
title={Efficient Dependable Rules Generation Approach for Data Quality Enhancement},
year={2015},
volume={},
number={},
pages={91-96},
abstract={In area of data quality research, enhancing data quality is still big challenge, especially in large databases. Data mining techniques can be efficiently utilized in data cleaning process. Databases are often suffered from data inconsistency, which has no vital solution up to now. In this paper, we tackle the problem of detecting data inconsistency from large databases. We propose an approach for discovering dependable rules from databases themselves. Such generated rules are minimal and non-redundant that covers all rules among patterns in database. The proposed approach focuses mainly on generating precise dependable data quality rules through extracting maximal frequent patterns, as effective pruning mechanism to reduce the search space domain. The proposed approach is validated against several datasets from different application domains. Experimental Results demonstrate that our approach outperform other approaches in terms of the efficiency, accuracy and scalability using both real-life and synthetic datasets.},
keywords={Databases;Data integrity;Scalability;Cleaning;Data mining;Data Quality;Data Mining;Data Cleaning;Functional Dependency (FD);Conditional Functional Dependency (CFD);Association Rule Mining (ARM);Maximal Frequent Patterns (MFP)},
doi={10.1109/ICCTA37466.2015.9513432},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8433710,
author={Globa, L.S. and Novogrudska, R.L. and Koval, A.V.},
booktitle={2018 IEEE International Black Sea Conference on Communications and Networking (BlackSeaCom)},
title={Ontology Model of Telecom Operator Big Data},
year={2018},
volume={},
number={},
pages={1-5},
abstract={The paper describes the approach to the representation of telecom operator Big Data by means of ontology model. Big Data characteristics are described as ontology classes, attributes and exemplars that are linked by relations. It was proposed to divide general ontology of telecom operator Big Data into three components: ontology of billing system, ontology of services quality and ontology of monitoring system. In the research all elements of each component are described. Based on ontology searching engine and logical inference is performed.},
keywords={Telecommunications;Ontologies;Big Data;Task analysis;Data models;Monitoring;Data analysis;Big Data;telecom operator;ontology;subject domain;comunication proces;telecom analitics;billing system;services quality},
doi={10.1109/BlackSeaCom.2018.8433710},
ISSN={},
month={June},}
@INPROCEEDINGS{7973772,
author={Santos, Walter dos and Carvalho, Luiz F. M. and Avelar, Gustavo de P. and Silva, Átila and Ponce, Lucas M. and Guedes, Dorgival and Meira, Wagner},
booktitle={2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)},
title={Lemonade: A Scalable and Efficient Spark-Based Platform for Data Analytics},
year={2017},
volume={},
number={},
pages={745-748},
abstract={Data Analytics is a concept related to pattern and relevant knowledge discovery from large amounts of data. In general, the task is complex and demands knowledge in very specific areas, such as massive data processing and parallel programming languages. However, analysts are usually not versed in Computer Science, but in the original data domain. In order to support them in such analysis, we present Lemonade — Live Exploration and Mining Of a Non-trivial Amount of Data from Everywhere — a platform for visual creation and execution of data analysis workflows. Lemonade encapsulates storage and data processing environment details, providing higher-level abstractions for data source access and algorithms coding. The goal is to enable batch and interactive execution of data analysis tasks, from basic ETL to complex data mining algorithms, in parallel, in a distributed environment. The current version supports HDFS (the Hadoop filesystem), local filesystems and distributed environments such as Apache Spark, the state-of-art framework for Big Data analysis.},
keywords={Data visualization;Sparks;Data mining;Data analysis;Quality of service;Visualization;Monitoring;Data analytics;cloud;Spark},
doi={10.1109/CCGRID.2017.142},
ISSN={},
month={May},}
@INPROCEEDINGS{7164483,
author={Moyne, James and Samantaray, Jamini and Armacost, Mike},
booktitle={2015 26th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC)},
title={Big data emergence in semiconductor manufacturing advanced process control},
year={2015},
volume={},
number={},
pages={130-135},
abstract={As requirements on data volumes, rates, quality, merging and analytics increase exponentially in the digital universe, semiconductor manufacturers are faced with a need for new approaches to data management and use across the fab. These are often termed “big data” challenges. In our industry big data solutions will be key to scaling Advanced Process Control (APC) solutions to finer levels of control and diagnostics. However the main impact will be to better enable more effective predictive technologies such as Predictive Maintenance (PdM), Virtual Metrology (VM) and yield prediction, all of which utilize data from APC capabilities. PdM represents one area where big data solutions are generating significant benefits across a variety of process types. Moving to big data solutions involves addressing the aforementioned requirements either with enhancements of existing systems or moving to more big data friendly platforms. The latter applied to APC systems provides quantifiable cost-of-ownership and speed improvements, thereby better enabling high quality prediction solutions.},
keywords={Big data;Maintenance engineering;Metrology;Manufacturing;Process control;Real-time systems;Data models;Big Data;Prediction;Predictive Maintenance;Virtual Metrology;yield prediction},
doi={10.1109/ASMC.2015.7164483},
ISSN={2376-6697},
month={May},}
@INPROCEEDINGS{8119365,
author={Molka-Danielsen, Judith and Engelseth, Per and Olešnaníková, Veronika and Šarafín, Peter and Žalman, Róbert},
booktitle={2017 5th International Conference on Enterprise Systems (ES)},
title={Big Data Analytics for Air Quality Monitoring at a Logistics Shipping Base via Autonomous Wireless Sensor Network Technologies},
year={2017},
volume={},
number={},
pages={38-45},
abstract={The indoor air quality in industrial workplace buildings, e.g. air temperature, humidity and levels of carbon dioxide (CO2), play a critical role in the perceived levels of workers' comfort and in reported medical health. CO2 can act as an oxygen displacer, and in confined spaces humans can have, for example, reactions of dizziness, increased heart rate and blood pressure, headaches, and in more serious cases loss of consciousness. Specialized organizations can be brought in to monitor the work environment for limited periods. However, new low cost wireless sensor network (WSN) technologies offer potential for more continuous and autonomous assessment of industrial workplace air quality. Central to effective decision making is the data analytics approach and visualization of what is potentially, big data (BD) in monitoring the air quality in industrial workplaces. This paper presents a case study that monitors air quality that is collected with WSN technologies. We discuss the potential BD problems. The case trials are from two workshops that are part of a large on-shore logistics base a regional shipping industry in Norway. This small case study demonstrates a monitoring and visualization approach for facilitating BD in decision making for health and safety in the shipping industry. We also identify other potential applications of WSN technologies and visualization of BD in the workplace environments; for example, for monitoring of other substances for worker safety in high risk industries and for quality of goods in supply chain management.},
keywords={Air quality;Temperature measurement;Wireless sensor networks;Employment;Buildings;Monitoring;Temperature sensors;Big Data analytics;Wireless Sensor Network Technologies;health and safety;shipping industry;air quality},
doi={10.1109/ES.2017.14},
ISSN={2572-6609},
month={Sep.},}
@INPROCEEDINGS{8104433,
author={Jia, Zhuang},
booktitle={2017 International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Intelligent Evaluation Method for Urban Public Space in the Big Data Era},
year={2017},
volume={},
number={},
pages={489-492},
abstract={One of the key issues in urban public space design is how to establish an evaluation system for its usage. The modernization for urban public space management cannot be achieved without the decision support service by intelligent evaluation system. In the 21st century, more attention has been paid to human activities and needs in public space design, and with the development of mobile information technology, it become possible to have access to a large number of network data (social network, theme website, or search engines etc.), reflecting urban space quality and peoples behavior characteristics. Based on the idea of Big Data, which uses cloud computing, this paper attempts to establish an intelligent evaluation method for urban public space by means of mobile information devices such as GPS, intelligent phone, social network, IC card and RFID communication technology.},
keywords={Smart grids;Automation;big data;public space;intelligent evaluation system},
doi={10.1109/ICSGEA.2017.49},
ISSN={},
month={May},}
@INPROCEEDINGS{7350636,
author={Li, Qingyong and Zhong, Zhangdui and Liang, Zhengping and Liang, Yong},
booktitle={2015 18th International Conference on Network-Based Information Systems},
title={Rail Inspection Meets Big Data: Methods and Trends},
year={2015},
volume={},
number={},
pages={302-308},
abstract={Rail inspection is one of the most important tasks for rail industry, in order to guarantee the safety of railway systems and control their cost. Rail are systematically inspected for defects using various non-destructive evaluation (NDE) techniques, which include ultrasonic inspection, visual detection, magnetic flux leakage method, acoustic emission inspection etc. The data obtained by these NDE devices are going to increase in both quality and quantity, therefore big data has emerged as a potential challenge for rail inspection. This paper reviews the advanced NDE techniques for rail inspection, and brings forward a new framework of rail inspection based on big data, according to the characteristics of inspection data.},
keywords={Rails;Inspection;Acoustics;Surface cracks;Visualization;Surface treatment;Big data;rail inspection;big data;non-destructive evaluation;rail defect},
doi={10.1109/NBiS.2015.47},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8855480,
author={Xu, Rui and Xiong, Qingyu and Yi, Hualing and Wu, Chao and Ye, Jianxin},
booktitle={2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
title={Research on Water Quality Prediction Based on SARIMA-LSTM: A Case Study of Beilun Estuary},
year={2019},
volume={},
number={},
pages={2183-2188},
abstract={As water environment is an important part of mangrove ecosystem, an efficient prediction of water quality is the foundation for judging the health of wetland ecosystem. And it also contributes a lot to the smooth development of environmental protection work. Based on the data of water quality and weather in Beilun Estuary, this paper chooses permanganate index and the content of ammonia nitrogen, which can reflect the water quality, as forecasting targets. We propose a multi-feature prediction method called SARIMA-LSTM on the basis of seasonal autoregressive integrated moving average model and long short-term memory. Through the combination of linear and non-linear model, this method can possess a better prediction effect considering the influence of weather on water quality. And the experimental results of four models show that this method has higher accuracy, stability and reliability.},
keywords={Predictive models;Water pollution;Meteorology;Prediction algorithms;Training;Wetlands;Data models;time series model;water quality prediction;seasonal autoregressive integrated moving average;long short-term memory},
doi={10.1109/HPCC/SmartCity/DSS.2019.00302},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8912104,
author={Lu, Qing},
booktitle={2019 International Conference on Computer Network, Electronic and Automation (ICCNEA)},
title={Research on Precise Quality Testing of Sponge City Construction Under Big Data},
year={2019},
volume={},
number={},
pages={99-105},
abstract={The traditional detection management system lacks certain stability in practical work. Therefore, big data technology is introduced to put forward the precise quality detection system of sponge city construction project. The hardware design of the system is a functional framework layer created based on the Internet system. On this basis, the system is divided into data processing module, testing function module and engineering management module with the idea of modularity, which is used to test the quality of construction projects, and to complete the design of precise quality testing system for sponge city construction projects under the big data technology. Finally, the test system is built to test the system proposed by big data technology. The experimental results show that the system has strong stability and meets the design requirements.},
keywords={Testing;Urban areas;XML;Big Data;Internet;Process control;Inspection;Construction Projects;Accurate Detection;Data Analysis;Quality Control},
doi={10.1109/ICCNEA.2019.00029},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7997238,
author={Xia, Qiufen and Liang, Weifa and Xu, Zichuan},
booktitle={2017 IEEE International Conference on Communications (ICC)},
title={QoS-aware data replications and placements for query evaluation of big data analytics},
year={2017},
volume={},
number={},
pages={1-7},
abstract={Enterprise users at different geographic locations generate large-volume data and store their data at different geographic datacenters. These users may also issue ad hoc queries of big data analytics on the stored data to identify valuable information in order to help them make strategic decisions. However, it is well known that querying such large-volume big data usually is time-consuming and costly. Sometimes, users are only interested in timely approximate rather than exact query results. When this approximation is the case, applications must sacrifice either timeliness or accuracy by allowing either the latency of delivering more accurate results or the accuracy error of delivered results based on the samples of the data, rather than the entire set of data itself. In this paper, we study the QoSaware data replications and placements for approximate query evaluation of big data analytics in a distributed cloud, where the original (source) data of a query is distributed at different geo-distributed datacenters. We focus on placing the samples of the source data with certain error bounds at some strategic datacenters to meet users' stringent query response time. We propose an efficient algorithm for evaluating a set of big data analytic queries with the aim to minimize the evaluation cost of the queries while meeting their response time requirements. We demonstrate the effectiveness of the proposed algorithm through experimental simulations. Experimental results show that the proposed algorithm is promising.},
keywords={Big Data;Time factors;Quality of service;Query processing;Distributed databases;Delays},
doi={10.1109/ICC.2017.7997238},
ISSN={1938-1883},
month={May},}
@INPROCEEDINGS{9384596,
author={Hou, Shuai and Jin, Qiancheng and Li, Jichao and Jiang, Jiang},
booktitle={2020 6th International Conference on Big Data and Information Analytics (BigDIA)},
title={Comparative Analysis of Scientific Research between China and the United States Based on Academic Big Data},
year={2020},
volume={},
number={},
pages={421-427},
abstract={The ability of scientific and technological innovation is an important part of a country's comprehensive strength. In recent years, the competition between China and the United States in the field of science and technology has attracted the attention of many scholars. As an important carrier of science and technology, academic data lays a foundation for quantitative analysis of the differences between China and the United States in the field of science and technology. Based on the data of Web of Science, this paper makes a comprehensive comparative analysis of the science and technology level of China and the United States, including the aspects of scientific research input, scientific research output, paper quality and field differences etc. We find that although there are still large gaps between the scientific and technological capabilities of China and the United States, these gaps are gradually narrowing due to the efforts in recent years. The results of this paper can be used for reference to accurately grasp the scientific and technological level of the two countries and formulate relevant policies in the future.},
keywords={Technological innovation;Statistical analysis;Big Data;Academic data;Scientometrics;Science and technology competition;Knowledge map},
doi={10.1109/BigDIA51454.2020.00075},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9006321,
author={Lakshminarayan, Choudur and Ramakrishnan, Thiagarajan and Al-Omari, Awny and Bouaziz, Khaled and Ahmad, Faraz and Raghavan, Sri and Agarwal, Prama},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Enterprise-wide Machine Learning using Teradata Vantage: An Integrated Analytics Platform},
year={2019},
volume={},
number={},
pages={2043-2046},
abstract={Big data characterized by variety can be divided into 3 principal categories: numeric structured data, semi-structured data, and unstructured multimedia data involving audio, video, and text. Decision making requires multiple analytical engines suitable for each type of data, programming languages, algorithms, visualization tools, and user interfaces. More often than not, industrial analytics is conducted ad hoc by lashing together analytics components such as distributed data sources, analytics engines, and algorithms. This kind of piecemeal approach ignores scale, security, governance, reliability, model management and fault tolerance that are paramount for industrial strength analytics. A unified, versatile, and robust architecture that combines various components in a single integrated platform is the need of the hour. Teradata Vantage (TD Vantage) is such a platform for delivering production quality enterprise analytics at scale. In this paper, we outline the proposed TD Vantage (available in the market and under continuous development) that unifies data, engines, and algorithms operating in a seamless symphony. We will demonstrate its capabilities through three proofs of concept biz: image data using TensorFlow, text data using Spark, and transaction data using Aster (now renamed Machine Learning Engine or MLE), with Teradata orchestrating interactions among the various components.},
keywords={Engines;Sparks;Machine learning;Tools;Maximum likelihood estimation;Distributed databases;Integrated Analytics Platform;Machine Learning;Artificial Intelligence;Analytics Engines;Text Analysis by NLP;Image Recognition},
doi={10.1109/BigData47090.2019.9006321},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7584947,
author={Zhuang, Zhenyun and Feng, Tao and Pan, Yi and Ramachandra, Haricharan and Sridharan, Badri},
booktitle={2016 IEEE International Congress on Big Data (BigData Congress)},
title={Effective Multi-stream Joining in Apache Samza Framework},
year={2016},
volume={},
number={},
pages={267-274},
abstract={Increasing adoption of Big Data in business environments have driven the needs of stream joining in realtime fashion. Multi-stream joining is an important stream processing type in today's Internet companies, and it has been used to generate higher-quality data in business pipelines. Multi-stream joining can be performed in two models: (1) All-In-One (AIO) Joining and (2) Step-By-Step (SBS) Joining. Both models have advantages and disadvantages with regard to memory footprint, joining latency, deployment complexity, etc. In this work, we analyze the performance tradeoffs associated with these two models using Apache Samza.},
keywords={Data models;Containers;Companies;Big data;LinkedIn;Complexity theory;Multi-stream joining;Samza;Stream processing;Big Data},
doi={10.1109/BigDataCongress.2016.41},
ISSN={},
month={June},}
@INPROCEEDINGS{9006082,
author={Berns, Fabian and Schmidt, Kjeld and Grass, Alexander and Beecks, Christian},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={A New Approach for Efficient Structure Discovery in IoT},
year={2019},
volume={},
number={},
pages={4152-4156},
abstract={Complex, multivariate data streams frequently comprise subjacent behavioral patterns, which are subsumable by a process of statistical structure discovery. Revealing these hidden patterns from raw data is a major challenge in abstracting information and thus for new opportunities of efficient data analysis at scale. State-of-the-art approaches, such as CKS and ABCD, leverage statistical data models and Gaussian Processes in order to abstract from raw data and to describe their major data characteristics by means of kernel-decomposed covariance functions. The process of identifying the most appropriate covariance function is a performance bottleneck due to its super-quadratic computation time complexity for model selection and evaluation. In this paper, we thus propose a new approach for the computation of large-scale statistical data models. To this end, we propose to bound the complexity of the statistical data model and develop a sequential agglomerative approach to reduce the computational load of the required evaluative calculations. Our performance analysis indicates that our proposal is able to outperform state-of-the-art kernel search algorithms such as CKS and ABCD with respect to the qualities of efficiency and accuracy.},
keywords={Kernel;Data models;Gaussian processes;Complexity theory;Big Data;Computational modeling;Market research;Internet of Things;Machine Learning;Gaussian Process;Data Modeling;Structure Discovery},
doi={10.1109/BigData47090.2019.9006082},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8672375,
author={Li, Xin and Lian, Zhen and Qin, Xiaolin and Abawajyz, Jemal},
booktitle={2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)},
title={Delay-Aware Resource Allocation for Data Analysis in Cloud-Edge System},
year={2018},
volume={},
number={},
pages={816-823},
abstract={There is a strong need for data analysis in information systems to support various services. Traditional cloud data centers provide powerful ability to conduct data analysis jobs. However, the data transmission consumes a large amount of time and leads to a long service delay. The QoS (Quality of Service) caused by long service delay is unacceptable for real-time services or applications. The collaboration with edge computing is an opportunity for service delay reduction. In this paper, we investigate the task placement problem for reducing service delay in cloud-edge system. We use the W-DAG (Weighted Directed Acyclic Graph) to model the data-intensive service or business logic. We analyze the data and resource requirements for the tasks, which constitute the integrated service, and make resource allocation between cloud data center and edge nodes. Then, we propose the task placement algorithm to achieve shorter service delay. The core idea is to make a tradeoff between data transmission time and data analysis time. The simulation results show that our algorithm has significant performance improvement on service delay reduction.},
keywords={Task analysis;Cloud computing;Delays;Data communication;Data centers;Data analysis;Resource management;delay-aware;data-driven;edge computing;resource allocation;task placement},
doi={10.1109/BDCloud.2018.00122},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8731479,
author={Guerraoui, Rachid and Kermarrec, Anne-Marie and Ruas, Olivier and Taïani, François},
booktitle={2019 IEEE 35th International Conference on Data Engineering (ICDE)},
title={Fingerprinting Big Data: The Case of KNN Graph Construction},
year={2019},
volume={},
number={},
pages={1738-1741},
abstract={We propose fingerprinting, a new technique that consists in constructing compact, fast-to-compute and privacy-preserving binary representations of datasets. We illustrate the effectiveness of our approach on the emblematic big data problem of K-Nearest-Neighbor (KNN) graph construction and show that fingerprinting can drastically accelerate a large range of existing KNN algorithms, while efficiently obfuscating the original data, with little to no overhead. Our extensive evaluation of the resulting approach (dubbed GoldFinger) on several realistic datasets shows that our approach delivers speedups of up to 78.9% compared to the use of raw data while only incurring a negligible to moderate loss in terms of KNN quality.},
keywords={Motion pictures;Force;Fingerprint recognition;Indexes;Hash functions;Big Data;Acceleration;KNN graphs;fingerprint;similarity},
doi={10.1109/ICDE.2019.00186},
ISSN={2375-026X},
month={April},}
@INPROCEEDINGS{7976069,
author={Vieira, Vanessa and Pedrosa, Isabel and Soares, Bruno Horta},
booktitle={2017 12th Iberian Conference on Information Systems and Technologies (CISTI)},
title={Big data & analytics: An approach using audit experts' interviews},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Big Data is one of the great trends in the short and medium term in organizations. There is a growing concern in provid solutions to address this trend, to methodical analysis of data and to do better decisions. The amount of data becomes less relevant when there is efficiency in Analytics. Internal auditors need to be in compliance with technology evolution. This research main objective is to understand how are internal auditors perceiving Big Data & Analytics' and which are the opportunities and difficulties pointed to address that challenge. To achieve this main goal, semi-structured interviews were conducted focused on internal auditors group. Those interviews intend to analyze and classify respondents' contributions in order to provide more insights for the present research. As a result, the main opportunities listed were greater information security and greater efficiency in data processing. Pointed obstacles were data quality, security and users' training.},
keywords={Big Data;Interviews;Market research;Organizations;Surges;Software;Big Data;organizations;information;Audit;technology},
doi={10.23919/CISTI.2017.7976069},
ISSN={},
month={June},}
@INPROCEEDINGS{7377712,
author={Wickramage, Chathurika and Sahama, Tony and Wickramanayake, Gihan N.},
booktitle={2015 Fifteenth International Conference on Advances in ICT for Emerging Regions (ICTer)},
title={Information Accountability of Healthcare Big Data},
year={2015},
volume={},
number={},
pages={279-279},
abstract={The use of Electronic Health Records (EHR) and its substantial growth of associated healthcare related data generate increasing volume of health information (volume) at an exponential velocity. These huge amounts of data are generated from a variety of information sources while the data sources originate from a veracity of clinical information systems and corporate data warehouses. The data derived from these data sources are used for analysis and trending purposes thus playing an influential role as a real time decision-making tool.},
keywords={Electronic Health Records (EHR);Big Data;Information Accountability;Healthcare},
doi={10.1109/ICTER.2015.7377712},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9539463,
author={Zhu, Ligang and Liu, Zongang and Lu, Jiaguo},
booktitle={2020 International Conference on Virtual Reality and Intelligent Systems (ICVRIS)},
title={Research on Information Integration Method of Intelligent Manufacturing Resources Based on Big Data Mining Technology},
year={2020},
volume={},
number={},
pages={720-724},
abstract={One of the goals of networked manufacturing is to achieve the integration and sharing of manufacturing resources, which is based on the effective release and mining of manufacturing resources. However, the existing networked manufacturing information resource processing module lacks the effective description and integration of manufacturing resource information. On this basis, the intelligent manufacturing resource information integration method based on big data mining technology is studied. By optimizing the characteristics and functions of the modern manufacturing market, effectively integrating intelligent manufacturing resources, effectively increasing the utilization of manufacturing resources, shortening product development cycles, reducing costs, and improving product quality. Practice shows that the intelligent manufacturing resource information integration method based on big data mining technology is feasible and fully meets the research requirements.},
keywords={Information resources;Costs;Virtual reality;Big Data;Product development;Product design;Manufacturing;Big data mining;intelligent manufacturing;information integration;resource utilization},
doi={10.1109/ICVRIS51417.2020.00176},
ISSN={},
month={July},}
@INPROCEEDINGS{7510775,
author={Liu, Yunshu and Chen, Xuanyu and Chen, Cailian and Guan, Xingping},
booktitle={2016 IEEE International Conference on Communications (ICC)},
title={Traffic big data analysis supporting vehicular network access recommendation},
year={2016},
volume={},
number={},
pages={1-6},
abstract={With the explosive growth of Internet of Vehicles (IoV), it is undoubted that vehicular demands for real-time Internet access would get a surge in the near future. Therefore, it is foreseeable that the cars within the IoV will generate enormous data. On the one hand, the huge volume of data mean we could get much information (e.g., vehicle's condition and real-time traffic distribution) through the big data analysis. On the other hand, the huge volume of data will overload the cellular network since the cellular infrastructure still represents the dominant access methods for ubiquitous connections. The vehicular ad hoc network (VANET) offloading is a promising solution to alleviate the conflict between the limited capacity of cellular network and big data collection. In a vehicular heterogeneous network formed by cellular network and VANET, an efficient network selection is crucial to ensure vehicles' quality of service. To address this issue, we develop an intelligent network recommendation system supported by traffic big data analysis. Firstly, the traffic model for network recommendation is built through big data analysis. Secondly, vehicles are recommended to access an appropriate network by employing the analytic framework which takes traffic status, user preferences, service applications and network conditions into account. Furthermore an Android application is developed, which enables individual vehicle to access network automatically based on the access recommender. Finally, extensive simulation results show that our proposal can effectively select the optimum network for vehicles, and network resource is fully utilized at the same time.},
keywords={Vehicles;Vehicular ad hoc networks;Roads;Big data;Quality of service;Internet;Real-time systems},
doi={10.1109/ICC.2016.7510775},
ISSN={1938-1883},
month={May},}
@INPROCEEDINGS{8590192,
author={Fernández-Cerero, Damian and Fernández-Montes, Alejandro and Kolodziej, Joanna and Lefèvre, Laurent},
booktitle={2018 11th International Conference on the Quality of Information and Communications Technology (QUATIC)},
title={Quality of Cloud Services Determined by the Dynamic Management of Scheduling Models for Complex Heterogeneous Workloads},
year={2018},
volume={},
number={},
pages={210-219},
abstract={The quality of services in Cloud Computing (CC) depends on the scheduling strategies selected for processing of the complex workloads in the physical cloud clusters. Using the scheduler of the single type does not guarantee of the optimal mapping of jobs onto cloud resources, especially in the case of the processing of the big data workloads. In this paper, we compare the performances of the cloud schedulers for various combinations of the cloud workloads with different characteristics. We define several scenarios where the proper types of schedulers can be selected from a list of scheduling models implemented in the system, and used to schedule the concrete workloads based on the workloads' parameters and the feedback on the efficiency of the schedulers. The presented work is the first step in the development and implementation of an automatic intelligent scheduler selection system. In our simple experimental analysis, we confirm the usefulness of such a system in today's data-intensive cloud computing.},
keywords={Cloud computing;Task analysis;Processor scheduling;Dynamic scheduling;Big Data;Job shop scheduling;Servers;Big Data;Quality of Big Data;Scheduling;Cloud scheduling;Dynamic cloud scheduling},
doi={10.1109/QUATIC.2018.00039},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9243395,
author={Kumar Sadineni, Praveen},
booktitle={2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)},
title={Developing a Model to Enhance the Quality of Health Informatics using Big Data},
year={2020},
volume={},
number={},
pages={1267-1272},
abstract={Data world which is ruling us today not only established its identity in the field of computer science but also other interdisciplinary sectors such as healthcare, economics, media and communication etc. The speed and the size at which data is being produced by various applications led to the concept of Big Data which has transformed conventional data world to Digital data world. Big Data Analytics is an advanced technique which helps in analyzing large, distinct datasets which comprise structured, unstructured and semi-structured data of different sizes collected from several sources to find out unseen patterns, associations and other intuitions. Its applications are not only limited to banking, communiqué, social media, education, social services, trade and shipping but also extended to healthcare sectors. It provides healthcare services such as patients prediction to expand staff employment, Electronic Health Records (EHRs), Telemedicine etc. This paper elaborates on how machine learning techniques such as Decision Trees, Support Vector Machine (SVM) and K nearest Neighbor can be integrated with Big Data Analytics to enhance the quality of healthcare services such as Heart Disease examination. Performance comparison of the techniques is evaluated using metrics such as Accuracy, Recall and Specificity.},
keywords={Support vector machines;Social networking (online);Telemedicine;Medical services;Machine learning;Big Data;Diseases;Healthcare;Big Data Analytics;Electronic Health Records;Health Monitoring Devices;Health Analytics},
doi={10.1109/I-SMAC49090.2020.9243395},
ISSN={},
month={Oct},}
@ARTICLE{7364226,
author={Qi, Lianyong and Dou, Wanchun and Hu, Chunhua and Zhou, Yuming and Yu, Jiguo},
journal={IEEE Transactions on Cloud Computing},
title={A Context-Aware Service Evaluation Approach over Big Data for Cloud Applications},
year={2020},
volume={8},
number={2},
pages={338-348},
abstract={Cloud computing has promoted the success of big data applications such as medical data analyses. With the abundant resources provisioned by cloud platforms, the quality of service (QoS) of services that process big data could be boosted significantly. However, due to unstable network or fake advertisement, the QoS published by service providers is not always trusted. Therefore, it becomes a necessity to evaluate the service quality in a trustable way, based on the services' historical QoS records. However, the evaluation efficiency would be low and cannot meet users' quick response requirement, if all the records of a service are recruited for quality evaluation. Moreover, it may lead to `Lagging Effect' or low evaluation accuracy, if all the records are treated equally, as the invocation contexts of different records are not exactly the same. In view of these challenges, a novel approach named Partial Historical Records-based service evaluation approach (Partial-HR) is put forward in this paper. In Partial-HR, each historical QoS record is weighted based on its service invocation context. Afterwards, only partial important records are employed for quality evaluation. Finally, a group of experiments are deployed to validate the feasibility of our proposal, in terms of evaluation accuracy and efficiency.},
keywords={Quality of service;Big data;Cloud computing;Context;Business;Concrete;Mathematical model;Big data;cloud;context-aware service evaluation;historical QoS record;weight},
doi={10.1109/TCC.2015.2511764},
ISSN={2168-7161},
month={April},}
@INPROCEEDINGS{9257527,
author={Donzia, Symphorien Karl Yoki and Kim, Haeng-kon},
booktitle={2020 20th International Conference on Computational Science and Its Applications (ICCSA)},
title={Architecture Design of a Smart Farm System Based on Big Data Appliance Machine Learning},
year={2020},
volume={},
number={},
pages={45-52},
abstract={The size of the world's population increased at a Revolution. The modern expansion of human numbers started but environmental degradation with lack of urban services. To satisfy the growing of human food, worldwide demand for grain the area under production should be increased, and productivity must be improved on yields area firstly. To evaluate the Smart Farming sub-use cases' overall outcome, each economic and environmental benefits, social aspects, and the technical evolution path were evaluated. We have like an significant improvement in the economic outcome of the farm. This paper proposed an implementation of BMS (Big Data Application Machine Learning-based Smart Farm System) with an emphasis on crop productivity and the importance of farmers' income increase. Increasing crop productivity is also important to increase essentials' income, enhance farmer field-level insights, and actionable knowledge to produce when the crop is of the best quality or selling it with a good price. Therefore, in the Smart Farm system proposed in this paper specially in case of big data science, we need to consider data analysis and machine learning as the most important steps and then we can include the value of big data science. Machine learning is an essential ability to learn from data and provide data-driven information, decisions, and forecasts. Traditional approaches to machine learning were developed in a different era, like the data set that fully integrates memory. In addition to the characteristics of Big Data, they create obstacles to traditional techniques. One of the objectives of this document is to summarize the challenges of machine learning with Big Data.},
keywords={Big Data;Machine learning;Agriculture;Real-time systems;Productivity;Sensors;Green products;Smart Farm;Big data;Machine Learning;BMS;Monitor;software stack},
doi={10.1109/ICCSA50381.2020.00019},
ISSN={},
month={July},}
@INPROCEEDINGS{6873776,
author={Ukil, Abhisek and Zivanovic, Rastko},
booktitle={2014 IEEE Innovative Smart Grid Technologies - Asia (ISGT ASIA)},
title={Automated analysis of power systems disturbance records: Smart Grid big data perspective},
year={2014},
volume={},
number={},
pages={126-131},
abstract={Analysis of faults and disturbances play crucial roles in secure and reliable electrical power supply. Digital fault recorders (DFR) enable digital recording of the power systems transient events with high quality and huge quantity. However, transformation of data to information, expectedly in an automated way, is a big challenge for the power utilities worldwide. This is a key focus for realizing the `Smart Grid'. In this paper, the architecture and specifications for the primary and the secondary information for the automated systems are described. This provides qualitative and quantitative guidelines about the information to derive out of the disturbance data. A quantified estimate of big data for the substations, has been estimated in the paper. Possible ways of reducing the big data by utilizing intelligent segmentation techniques are described, substantiated by real example. Utilization of centralized protection and remote disturbance analysis for reducing big disturbance data are also discussed.},
keywords={Relays;Standards;Circuit faults;Smart grids;Asia;Big data;Analytics;big data;centralized protection;fault analysis;high voltage;HV;medium voltage;MV;radial network;remote relay testing;smart grid;web service},
doi={10.1109/ISGT-Asia.2014.6873776},
ISSN={2378-8542},
month={May},}
@INPROCEEDINGS{7363807,
author={Wang, Yang and Ma, Kwan-Liu},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Revealing the fog-of-war: A visualization-directed, uncertainty-aware approach for exploring high-dimensional data},
year={2015},
volume={},
number={},
pages={629-638},
abstract={Dimensionality Reduction (DR) is a crucial tool to facilitate high-dimensional data analysis. As the volume and the variety of features used to describe a phenomenon keeps increasing, DR has become not only desirable but paramount. However, DR can result in unreliable depictions of data. The uncertainties involved in DR may stem from the selection of methods, parameter configurations, and the constraints imposed by the user. To address these uncertainties, various means of DR quality assessment have been proposed in the literature. Nevertheless, how to optimize the trade-off between the quantification efficiency and accuracy is yet to be further studied. The purpose of this paper is to present a general technique, in the context of visual analytics, to support efficient uncertainty-aware high-dimensional data exploration. We model the uncertainty based on how well neighborhood geometries are preserved during DR. We employ approximated nearest neighbor (ANN) search algorithms to speed up the quantification process with marginal decrease in accuracy. We then visualize the quantified uncertainties in the form of augmented scatter plot. We test our technique with three real world datasets against several well-known DR techniques, and discuss possible underlying causes that lead to certain embedding patterns. Our results show that our approach is effective and beneficial for both DR assessment and user-centered data exploration.},
keywords={Uncertainty;Measurement;Geometry;Data visualization;Approximation algorithms;Quantization (signal);Nearest neighbor searches;Visual Analytics;Dimension Reduction;Uncertainty Analysis;Quality Assessment},
doi={10.1109/BigData.2015.7363807},
ISSN={},
month={Oct},}
@ARTICLE{8867944,
author={Shen, Yinghua and Pedrycz, Witold and Chen, Yuan and Wang, Xianmin and Gacek, Adam},
journal={IEEE Transactions on Fuzzy Systems},
title={Hyperplane Division in Fuzzy C-Means: Clustering Big Data},
year={2020},
volume={28},
number={11},
pages={3032-3046},
abstract={Big data with a large number of observations (samples) have posed genuine challenges for fuzzy clustering algorithms and fuzzy C-means (FCM), in particular. In this article, we propose an original algorithm referred to as a hyperplane division method to split the entire data set into disjoint subsets. By disjoint subsets, we mean that the data subspaces (parts of the entire data space), each of which is supported or spanned by the data points in the corresponding subset, do not overlap each other. The disjoint subsets turned out to be beneficial to the improvement of the quality of the clusters formed by the clustering algorithms. Moreover, considering that either a large number (say, thousands) or a small number (say, a few) of clusters may be pursued in the clustering task, we propose corresponding strategies (based on the hyperplane division method) to make clustering processes feasible, efficient, and effective. By validating the proposed strategies on both synthetic and publicly available data, we show their superiority (in terms of both efficiency and effectiveness) manifested in a visible way over the method of clustering the entire data and over some representative big data clustering methods.},
keywords={Clustering algorithms;Big Data;Indexes;Prototypes;Task analysis;Clustering methods;Data structures;Big data clustering;clustering requirements;fuzzy C-means (FCM);hyperplane division;many clusters},
doi={10.1109/TFUZZ.2019.2947231},
ISSN={1941-0034},
month={Nov},}
@INPROCEEDINGS{7003687,
author={Han, Li},
booktitle={2014 7th International Conference on Intelligent Computation Technology and Automation},
title={Citizens' Perception of Civil Engineering Quality Based on Data Mining},
year={2014},
volume={},
number={},
pages={932-935},
abstract={Quality of the civil engineering in a city can demonstrate the level of economic development, city image and the ability of governance of this specific area. Utilizing data mining method and algorithm, as well as the quality index of the civil engineering constructed from a large scale consumer survey, the results revealed that, as a whole, the quality of civil engineering in China is not quite good. The quality scores of Public Square, theatre, municipal pipeline, and so on, are fall behind the scores of some industry products. Moreover, there is big discrepancy existing in the results collected from different residents and different regions. The quality score of civil engineering in urban area is higher than that of rural area, same result applies from big city, medium city, and to small city as well. Marketization level, economic duality and government governance ability can be the reasons lead to diversity. So, government should reform the mechanism of quality regulation, adopt new strategy of governance to deal with the quality issues in civil engineering.},
keywords={Civil engineering;Cities and towns;Government;Quality assessment;Product design;Industries;Educational institutions;Quality;Civil Engineering;Empirical Investigation;Consumer Survey;Data mining},
doi={10.1109/ICICTA.2014.224},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9353794,
author={Xia, Hong and Zhang, YongKang and Wang, Han and Chen, YanPing and Wang, ZhongMin},
booktitle={2020 International Conference on Networking and Network Applications (NaNA)},
title={Crowdsourcing Answer Integration Algorithm For Big Data Environment},
year={2020},
volume={},
number={},
pages={335-341},
abstract={Crowdsourcing is an emerging distributed computing model that is widely used. Aiming at the uneven quality of crowdsourcing answers due to different workers' capabilities and attitudes, it is necessary to effectively study the hotspot issue of crowdsourcing answer integration. A crowdsourced answer integration algorithm based on “filter-evaluate-vote” is proposed. This algorithm is implemented using MapReduce parallel programming model in the Hadoop platform, and experiments are performed on multiple data sets. The results show that the proposed algorithm can be effective. It improves the accuracy of crowdsourced answers, and has high computing performance and horizontal scalability, which is suitable for answer integration in a big data environment.},
keywords={Crowdsourcing;Computational modeling;Scalability;Big Data;Quality assessment;Time factors;Task analysis;Crowdsourcing;quality assessment;answer integration;MapReduce},
doi={10.1109/NaNA51271.2020.00064},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7538393,
author={Pasupathi, Chitra and Kalavakonda, Vijaya},
booktitle={2016 2nd International Conference on Advances in Electrical, Electronics, Information, Communication and Bio-Informatics (AEEICB)},
title={Evidence Based health care system using Big Data for disease diagnosis},
year={2016},
volume={},
number={},
pages={743-747},
abstract={Health care analysis poses a consistent challenge to physicians and is the area of research in which trillions of amounts are being spent by all countries. The Institute of Medicine proposes several recommendations to increase the quality of health care. Different data mining algorithms have been applied on the voluminous health records to aid decision making process. In this proposed system, the patient health conditions are analyzed and exact disease is diagnosed using Big Data analytics with Evidence Based methodology. Patient's medical history and current evidences are considered to diagnose and drug suggestion. Patient analysis report is generated to monitor the recovery and patient feedback of the suggested drug is recorded to witness the success-rate of the diagnosis process.},
keywords={Diseases;Drugs;Big data;Medical diagnostic imaging;History;Data models;Decision support system;decision making;Drug therapy;Big Data Analytics},
doi={10.1109/AEEICB.2016.7538393},
ISSN={},
month={Feb},}
@INPROCEEDINGS{8342967,
author={Shah, Fadia and Li, Jianping and Shah, Yasir and Shah, Faiza},
booktitle={2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)},
title={Extended definition of medical big data},
year={2017},
volume={},
number={},
pages={513-516},
abstract={The human life is always experiencing problems related to health. The survival is difficult if not treated well. For a better recovery, health problems are solved by treatment plans and medications; which are by health care professional called specialists, doctors or medical practitioners. Whatever these professionals recommend, it is well maintained in the form of reports. The collection of all this record makes Medical Big Data (MBD). Based upon the medical problem, this MBD includes medical history and prescription reports, test reports, X-Ray, CT Scan, and some other types of medical diagnosis. Traditional systems are now improved after the enhancements in telecommunication modes and innovation of smart devices with latest 5G technology has a huge contribution in every field of science. Regarding health care systems, in developed countries, E-Health and Telemedicine systems being developed to improve the quality of treatment. Such systems have many enhanced features like data management, reliable diagnoses; among them a distinct aspect is load reduction for the patient about data availability and management. Since MBD is increasing for every patient as the time passes and the patient consults again and again. Many schemes with efficient models are proposed to make this MBD available over the network by compression and network management tools. This exponential expansion of MBD has unmitigated the domains of MBD generating sources. In this paper, MBD collection, sources are discussed which ensure directly or indirectly how some domains are responsible to increase MBD more than normal ways.},
keywords={Medical diagnostic imaging;Big Data;Medical services;Smart devices;Robots;Prediction algorithms;Big Data;Medical Big Data Diversity;IoT;Wavelets Compression},
doi={10.1109/ICSESS.2017.8342967},
ISSN={2327-0594},
month={Nov},}
@INPROCEEDINGS{7830238,
author={Lathiya, Piyush and Rani, Rinkle},
booktitle={2016 International Conference on Inventive Computation Technologies (ICICT)},
title={Improved CURE clustering for big data using Hadoop and Mapreduce},
year={2016},
volume={3},
number={},
pages={1-5},
abstract={In the Era of Information, Extracting useful information out of massive amount of data and process them in less span of time has become crucial part of Data mining. CURE is very useful hierarchical algorithm which has ability to identify cluster of arbitrary shape and able to identify outliers. In this paper we have implemented CURE clustering algorithm over distributed environment using Apache Hadoop. Now a days, to handle large store and handle huge data, Mapreduce has become very popular paradigm. Mapper and Reducer routines are designed for CURE algorithm. We have also discussed how different parameters affect quality of clusters by removing outliers.},
keywords={Clustering algorithms;Algorithm design and analysis;Data mining;Partitioning algorithms;Distributed databases;Shape;Classification algorithms;Clustering;Data mining;CURE;Mapreduce;Outliers;Hadoop},
doi={10.1109/INVENTIVE.2016.7830238},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9006289,
author={Rocha, Lais M. A. and Bessa, Aline and Chirigati, Fernando and OFriel, Eugene and Moro, Mirella M. and Freire, Juliana},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Understanding Spatio-Temporal Urban Processes},
year={2019},
volume={},
number={},
pages={563-572},
abstract={Increasingly, decisions are based on insights and conclusions derived from the results of data analysis. Thus, determining the validity of these results is of paramount importance. In this paper, we take a step towards helping users identify potential issues in spatio-temporal data and thus gain trust in the results they derived from these data. We focus on processes that are captured by relationships among datasets that serve as the data exhaust for different components of urban environments. In this scenario, debugging data involves two important challenges: the inherent complexity of spatio-temporal data, and the number of possible relationships. We propose a framework for profiling spatio-temporal relationships that automatically identifies data slices that present a significant deviation from what is expected, and thus, helps focus a user's attention on slices of the data that may have quality issues and/or that may affect the conclusions derived from the analysis' results. We describe the profiling methodology and how it derives relationships, identifies candidate deviations, assesses their statistical significance, and measures their magnitude. We also present a series of cases studies using real datasets from New York City which demonstrate the usefulness of spatio-temporal profiling to build trust on data analysis' results.},
keywords={Urban areas;Correlation;Spatial resolution;Data analysis;Mathematical model;Public transportation;Standards;data quality;data profiling;urban data},
doi={10.1109/BigData47090.2019.9006289},
ISSN={},
month={Dec},}
@ARTICLE{8254302,
author={Chindanur, Narendra Babu and Sure, Pallaviram},
journal={Computing in Science & Engineering},
title={Low-Dimensional Models for Traffic Data Processing Using Graph Fourier Transform},
year={2018},
volume={20},
number={2},
pages={24-37},
abstract={The reliability of services offered by intelligent transportation systems is attributed to the accuracy and timely availability of road-network traffic information. However, in the present era of big data, compliance with anticipated service quality requirements mandates consistent real-time processing of big spatiotemporal traffic data. Thus, development of low-dimensional models is a crucial challenge in traffic data processing. The authors developed such representations using data graph framework and graph Fourier transform (GFT) approaches. Experimental results on California daily network traffic data showed that, even with a 15:1 compression ratio, GFT-based models offered less than 6 percent reconstruction error (RE), instigating a less than 2 percent increase in mean absolute percentage error of corresponding predictions. The authors also proposed a 3D graph framework, which reduced RE by almost 2 percent compared to its 2D counterpart.},
keywords={Data models;Matrix decomposition;Computational modeling;Real-time systems;Eigenvalues and eigenfunctions;Intelligent transportation systems;Three-dimensional displays;Information processing;Information systems;intelligent transportation systems;road network traffic data;low-dimensional model;dimensionality reduction;data graph framework;graph Fourier transform;intelligent information processing;scientific computing},
doi={10.1109/MCSE.2018.110111913},
ISSN={1558-366X},
month={Mar},}
@ARTICLE{8105799,
author={Hossain, M. Shamim and Muhammad, Ghulam},
journal={IEEE Internet of Things Journal},
title={Emotion-Aware Connected Healthcare Big Data Towards 5G},
year={2018},
volume={5},
number={4},
pages={2399-2406},
abstract={The recent development of big data-oriented wireless technologies in terms of emerging 5G, edge computing, interconnected devices of the Internet of Things (IoT), and data analytics, as well as techniques, have enabled connected healthcare services for a happier and healthier life. Although, the quality of the healthcare services can be enhanced through big data-oriented wireless technologies, however, the challenges remain for not considering emotional care, especially for children, elderly, and mentally ill people. In this paper, we propose an emotion-aware connected healthcare system using a powerful emotion detection module. Different IoT devices are used to capture speech and image signals of a patient in a smart home scenario. These signals are used as the input to the emotion detection module. Speech and image signals are processed separately, and classification scores using these signals are fused to produce a final score to take a decision about the emotion. If the emotion is detected as pain, caregivers can visit the patient. Several experiments were performed to validate the proposed system, and good accuracies, up to 99.87%, were achieved for emotion detection. The proposed framework would greatly contribute personalized and seamless emotion-aware healthcare services toward 5G.},
keywords={Medical services;Emotion recognition;Big Data;5G mobile communication;Speech;Feature extraction;Wireless sensor networks;5G;big data;emotion recognition;emotion-aware healthcare;Internet of Things (IoT);local binary pattern (LBP)},
doi={10.1109/JIOT.2017.2772959},
ISSN={2327-4662},
month={Aug},}
@INPROCEEDINGS{8692131,
author={Cheriyan, Anish and Gondkar, Raju Ramakrishna and Gopal, Thiyagu and S., Suresh Babu},
booktitle={2018 IEEE 8th International Advance Computing Conference (IACC)},
title={Quality Assurance Practices in Continuous Delivery - an implementation in Big Data Domain},
year={2018},
volume={},
number={},
pages={7-13},
abstract={This paper provides the details about the Quality Assurance practices and techniques to be followed by the QA professional (also called SQA-Software Quality Assurance) in continuous delivery mode of software development. QA professionals are responsible for the process definition, audit, training and other assurance activites in the project. The paper provides a QA model named 'ACID-QA' model which comprises of key practices which can be used by the QA professional in continuous delivery mode of software development. The objective of the 'ACID-QA' model is to provide a working model for the SQA which can be used during the planning, requirement, design, coding, testing, continuous integration, audit and release activities of the project. The paper provides an overview of each of the practice areas of the model in the further sections. This model is implemented in Big Data Hadoop File system and Map Reduce and it is found that the product quality issues found by SQA Professionals are improved by 100%. The audit findings are further detailed down in the paper.},
keywords={Pipelines;Quality assurance;Testing;Big Data;Tools;Security;Cloud computing;Quality Assurance;Continuous Delivery;DevOps;QA Professional;SQA;Continuous Integration},
doi={10.1109/IADCC.2018.8692131},
ISSN={2473-3571},
month={Dec},}
@INPROCEEDINGS{9331122,
author={Qin, Taichun and Liu, Shouwen and Huang, Shouqing and Zhou, Yuege},
booktitle={2020 7th International Conference on Dependable Systems and Their Applications (DSA)},
title={The test process date based quality defect detection strategy for spacecraft components},
year={2020},
volume={},
number={},
pages={550-554},
abstract={A large amount of data has been generated during laboratory tests of the spacecraft components. However, due to the lack of systematic data mining strategy, these data were remaining underutilized. This paper proposes a quality defect detection strategy, which can extract abnormal information from the test process data. Specifically, an overall process is proposed and related data analysis methods is analyzed. In addition, a case study is carried out, and the results shows that the quality defect detection strategy can build the successful envelope with the test data of historical products, and effectively discriminate whether the quality of the current test product meets requirement.},
keywords={Space vehicles;Systematics;Data analysis;Data mining;big data;laboratory test;data mining;quality defect detection},
doi={10.1109/DSA51864.2020.00093},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8411892,
author={Du, Jianju and Wang, Lili},
booktitle={2018 IEEE Third International Conference on Data Science in Cyberspace (DSC)},
title={Ownership Concentration, Internal Control Quality and Real Earnings Management},
year={2018},
volume={},
number={},
pages={438-444},
abstract={With the development of information technology in various industries, we have entered the era of "Big data". The value embodied in "Big data" is also incalculable. It provides a convenient condition for comprehensively reflecting the financial status of enterprises. Therefore, this article uses Big data to sample the A-share listed companies in the Shanghai and Shenzhen stock markets from 2011 to 2015, using descriptive statistics, Pearson correlation test, and multiple regression analysis to study the relationship between ownership concentration, internal control quality, and real earnings management. This article reflects the concept of "Big data" in terms of data acquisition and processing methods. This provides empirical data for optimizing the ownership structure and improving the internal control quality of listed companies to effectively suppress the level of real earnings management.},
keywords={Conferences;Data science;Cyberspace;Big data;ownership concentration;internal control quality;real earnings management},
doi={10.1109/DSC.2018.00071},
ISSN={},
month={June},}
@INPROCEEDINGS{9443818,
author={Shao, Peng and Tao, Ming and Zhou, Min and Wang, Tian},
booktitle={2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)},
title={Ontology-Based Modeling and Semantic Query for Mobile Trajectory Data},
year={2020},
volume={},
number={},
pages={1183-1188},
abstract={In the era of big data, the development of mobile Internet and the popularization of mobile terminals have formed massive mobile trajectory data. Reasonable usage of the data will greatly improve the service quality and experience of end users. To analyze hidden activity patterns of end-user in the data, big data query is an important operation and how to enhance the query efficiency remains a challenge issue. However, different data analysis approaches have different applications in different fields, and it is necessary to mine hidden data relationships. In addition, query time is one of important factors to evaluate query efficiency, some researches however mainly focus on query result rather than evaluating query efficiency through multiple contrast approaches. To address these issues, an ontology-based modeling and semantic query strategy for mobile trajectory data is investigated in this paper. First, we respectively employ cosine similarity, point-wise mutual information (PMI) and containment probability model to mine association relationship and containment relationship hidden in the data. Subsequently, an ontology-based model is built to visualize end-user's activity through taxonomy and comparison approaches. Finally, four semantic query methods, e.g., basic query, join query, containment query and combination (join & containment) query, are defined through SPARQL (SPARQL Protocol and RDF Query Language) to evaluate query time, and the query efficiency achieved by these investigations has been demonstrated through the conducted experiments.},
keywords={Protocols;Numerical analysis;Semantics;Taxonomy;Big Data;Data models;Resource description framework;ontology;mobile trajectory data;semantic query},
doi={10.1109/ISPA-BDCloud-SocialCom-SustainCom51426.2020.00176},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9362933,
author={Mhatre, Apurva and Mahalingam, Avantika and Narayanan, Mahadevan and Nair, Akash and Jaju, Suyash},
booktitle={2020 2nd International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)},
title={Predicting Employee Attrition along with Identifying High Risk Employees using Big Data and Machine Learning},
year={2020},
volume={},
number={},
pages={269-276},
abstract={"It takes a lot of time and energy to build a great employee and only a second to lose one." Employee turnover is a perennial challenge faced by all the major companies across the globe, performance of a company is directly proportional to the quality of employees retained by them. Whenever a good employee quits the organization it leads to financial losses, gaps in company's execution capability, re-recruiting costs and loss of productivity. The success of a company lies not only in impeding the attrition rate but also in retaining the right talent. According to NASSCOM, the global employee churn rate as of 2019 is 18-20 percent, which is what makes it necessary to alleviate the business risks associated with the turnover using statistical analysis. This research aims to foresee potential attrition (specifically in the B.P.O. sector) by mining turnover trends amongst employees and use supervised classification techniques to cluster out vulnerable employees".},
keywords={Productivity;Statistical analysis;Companies;Machine learning;Big Data;Market research;Data mining;Big data;turnover;data mining;patterns;supervised learning;machine learning;concept drift},
doi={10.1109/ICACCCN51052.2020.9362933},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8258457,
author={Jitkajornwanich, Kulsawasd and Vateekul, Peerapon and Gupta, Upa and Kormongkolkul, Teeranai and Jirakittayakorn, Arnon and Lawawirojwong, Siam and Srisonphan, Siwapon},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Ocean surface current prediction based on HF radar observations using trajectory-oriented association rule mining},
year={2017},
volume={},
number={},
pages={4293-4300},
abstract={HF (high frequency) coastal radar system is used to capture the surface current behavior - in terms of velocity and direction - in the ocean near the coast. 18 HF coastal radar stations were implemented along the Gulf of Thailand in order to monitor for disasters (e.g., Tsunami) as well as relevant risks. The HF systems are also to serve other life-critical applications, such as water quality control and monitoring, chemical spill backtracking, and marine navigation. However, not all the applications can benefit from this near-real-time HF data; some applications in different domains require forecast values. The examples include search-and-rescue system and hazardous materials spill trajectory prediction. Therefore, in this paper, we propose a predictive model for future current data based on historical HF coastal radar data sets, utilizing association rule mining combined with an object dispersion concept. So, the full potential of HF radar systems can be exploited. The spatial and temporal dimensions are taken into account when designing our predictive system, which consists of two phases: ocean surface current track formulation and spatio-temporal association rule mining. The experiments are performed on a two-year HF radar dataset (2014-2015) using Google Cloud Platform. The resulting forecast current values: velocity and direction are then compared with testing datasets (using 10-fold cross validation) of the actual recorded values and evaluated based on percentage accuracy and RMSE, respectively.},
keywords={Sea surface;Radar tracking;Trajectory;Sea measurements;Data mining;ocean surface current;trajectory prediction;HF radar;spatio-temporal data mining;association rule mining},
doi={10.1109/BigData.2017.8258457},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8511985,
author={Ferrara, Enrico and Liotta, Antonio and Erhan, Laura and Ndubuaku, Maryleen and Giusto, Daniele and Richardson, Miles and Sheffield, David and McEwan, Kirsten},
booktitle={2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)},
title={A Pilot Study Mapping Citizens’ Interaction with Urban Nature},
year={2018},
volume={},
number={},
pages={836-841},
abstract={The capabilities offered by smart sensing (the Internet of Things) and data science, create new opportunities to carry out large-scale studies involving social science and human factors. We report here our findings on a pilot study aimed at better understanding how citizens interact with urban green areas, identify relevant features, spot interaction patterns and, ultimately, recommend interventions to increase well-being. Our study was carried out in Sheffield (UK), where we tracked 1,870 subjects for two different periods (7 and 30 days), covering 760 digitally geo-fenced green areas. Through a smartphone App, we collected both subjective data (personal feelings, type of social interactions, type of activity, and perception of space) and objective data (sensor data, location, time, and photos). We employed data science methods to filter, correlate, cluster, and visualize the data, doing text analysis to extract semantic information from the subjects' responses. Looking at the intensity of interaction between citizens and green spaces, we found a stronger correlation with the quality of the green areas (diversity of natural features, trees, and birds), rather than their size (half of the top visited areas included small squares and gardens). Looking at the type of social interaction taking place within the green areas (lone visits or with friends and family), we found that different social interaction patterns correlate to different types of green area. Interestingly, most of the interactions correlate strongly to the proximity to the city centre, the presence of facilities (sport, parking), and architectural features (listed building, artistic/monumental icons), which indicate that targeted small-scale intervention into central built areas may have more immediate impact towards the citizen's well-being than the more peripheral parks.},
keywords={Urban areas;Data science;Green buildings;Tools;Correlation;Internet of Things;data-analysis;urban-analytics;urban-planning;smart-cities;data-science;social-science},
doi={10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00-21},
ISSN={},
month={Aug},}
@ARTICLE{8667817,
author={He, Zhenzhen and He, Yihai and Liu, Fengdi and Zhao, Yixiao},
journal={IEEE Access},
title={Big Data-Oriented Product Infant Failure Intelligent Root Cause Identification Using Associated Tree and Fuzzy DEA},
year={2019},
volume={7},
number={},
pages={34687-34698},
abstract={Infant failure analyzing is an effective approach to improve production quality continuously. The root causes of infant failure have always been a puzzle to manufacturers. To satisfy the increasing demand for the fuzzy root cause analysis of product infant failure in the era of big data, a novel root cause identification approach based on the associated tree and fuzzy data envelopment analysis (DEA) is presented for product infant failure. First, to decrease fuzziness with regard to the mechanism of infant failure, the associated tree is adapted to guide the analysis process for possible root causes based on axiomatic domain mapping. Second, considering the fuzzy mechanism and massive data, the fuzzy DEA technique is adopted to cluster all the potential factors of functional parameters, physical parameters, and process parameters from big data regarding product life cycle. Third, the ranking method of decision-making unit efficiency in fuzzy DEA is used to model and rank the weight of each node in the established associated tree of infant failure. Finally, a case study of root cause identification for a typical infant failure of the vibration and noise of a washing machine is presented to demonstrate the feasibility and validity of the proposed method.},
keywords={Big Data;Reliability;Manufacturing;Failure analysis;Analytical models;Production;Computational modeling;Infant failure;big data;root cause analysis;associated tree;fuzzy DEA},
doi={10.1109/ACCESS.2019.2904759},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7581555,
author={Zhang, Guigang and Yang, Yi and Zhai, Xiaoshuang and Yao, Qi and Wang, Jian},
booktitle={2016 11th International Conference on Computer Science & Education (ICCSE)},
title={Online education big data platform},
year={2016},
volume={},
number={},
pages={58-63},
abstract={Online education is a popular way for distance education in the world. The data of online education platform is explosively growing. The Big Data technologies can process and analyze the massive data and facilitate the user experience and teaching quality. This paper proposes an online education Big Data platform that aims at improving the quality of education by analyzing a large amount of data generated during the online education using big data technologies. This paper presents the concepts and design of the platform. Finally, an application scenario is showed for describing the use of the platform as well.},
keywords={Computer science;Education;Online learning;Big Data;Online education;Analysis;Cloud Computing;Application},
doi={10.1109/ICCSE.2016.7581555},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9006370,
author={Baughman, Matt and Chakubaji, Nifesh and Truong, Hong-Linh and Kreics, Krists and Chard, Kyle and Foster, Ian},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Measuring, Quantifying, and Predicting the Cost-Accuracy Tradeoff},
year={2019},
volume={},
number={},
pages={3616-3622},
abstract={Exponentially increasing data volumes, coupled with new modes of analysis have created significant new opportunities for data scientists. However, the stochastic nature of many data science techniques results in tradeoffs between costs and accuracy. For example, machine learning algorithms can be trained iteratively and indefinitely with diminishing returns in terms of accuracy. In this paper we explore the cost-accuracy tradeoff through three representative examples: we vary the number of models in an ensemble, the number of epochs used to train a machine learning model, and the amount of data used to train a machine learning model. We highlight the feasibility and benefits of being able to measure, quantify, and predict cost accuracy tradeoffs by demonstrating the presence and usability of these tradeoffs in two different case studies.},
keywords={Training;Machine learning;Data models;Pipelines;Computational modeling;Machine learning algorithms;Training data;performance;optimization;big data analytics;machine learning;quality of analytics},
doi={10.1109/BigData47090.2019.9006370},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6906820,
author={Papageorgiou, Apostolos and Zahn, Manuel and Kovacs, Ernö},
booktitle={2014 IEEE International Congress on Big Data},
title={Auto-configuration System and Algorithms for Big Data-Enabled Internet-of-Things Platforms},
year={2014},
volume={},
number={},
pages={490-497},
abstract={Internet of Things (IoT) platforms that handle Big Data might perform poorly or not according to the goals of their operator (in terms of costs, database utilization, data quality, energy-efficiency, throughput) if they are not configured properly. The latter configuration refers mainly to system parameters of the data-collecting gateways, e.g., polling intervals, capture intervals, encryption schemes, used protocols etc. However, re-configuring the platform appropriately upon changes of the system context or the operator targets is currently not taking place. This happens because of the complexity or unawareness of the synergies between system configurations and various aspects of the Big Data-handling IoT platform, but also because of the human resources that an efficient re-configuration would require. This paper presents an auto-configuration solution based on interpretable configuration suggestions, focusing on the algorithms for computing the mentioned suggested configurations. Five such algorithms are contributed, while a thorough evaluation reveals which of these algorithms should be used in different operation scenarios in order to achieve high fulfillment of the operator's targets.},
keywords={Logic gates;Big data;Measurement;Heuristic algorithms;Complexity theory;Standards;Optimization;M2M;IoT;configuration;gateway;autonomic;self-management},
doi={10.1109/BigData.Congress.2014.78},
ISSN={2379-7703},
month={June},}
@ARTICLE{7298399,
author={Lu, Qinghua and Li, Zheng and Kihl, Maria and Zhu, Liming and Zhang, Weishan},
journal={IEEE Access},
title={CF4BDA: A Conceptual Framework for Big Data Analytics Applications in the Cloud},
year={2015},
volume={3},
number={},
pages={1944-1952},
abstract={Building big data analytics (BDA) applications in the cloud introduces inevitable challenges, such as loss of control and uncertainty. To address the existing challenges, numerous efforts have been made on BDA application engineering to optimize the quality of BDA applications in the cloud, such as performance and reliability. However, there is still a lack of systematic view on engineering BDA applications in the cloud. Therefore, in this paper, we present a conceptual framework named CF4BDA to analyze the existing work on BDA applications from two perspectives: 1) the lifecycle of BDA applications and 2) the objects involved in the context of BDA applications in the cloud. The framework can help researchers and practitioners identify the research opportunities in a structured way and guide implementing BDA applications in the cloud. We perform a preliminary evaluation of the usefulness of CF4BDA by applying it to analyze a set of representative studies.},
keywords={Big data;Data analystics;Software engineering;Cloud computing;Reliability engineering;Semantics;Context modeling;Big data analytics;Cloud computing;conceptual framework;software engineering;Big data analytics;cloud computing;conceptual framework;software engineering},
doi={10.1109/ACCESS.2015.2490085},
ISSN={2169-3536},
month={},}