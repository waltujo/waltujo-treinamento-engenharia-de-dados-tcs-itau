@INPROCEEDINGS{7435456,
author={Yang, Sha and Yu, Wei and Hu, Yahui and Wang, Kai and Wang, Jun and Li, Shijun},
booktitle={2015 Third International Conference on Advanced Cloud and Big Data},
title={An Automatic Discovery Framework of Cross-Source Data Inconsistency for Web Big Data},
year={2015},
volume={},
number={},
pages={73-79},
abstract={The vigorous growth of big data has triggered both opportunities and challenges in business and industry. However, Web big data distributed in diverse sources with multiple data structures frequently conflict with each other, i.e. inconsistency in cross-source Web big data. In this paper, we propose a state-of-the-art architecture of auto-discovering inconsistency with Web big data. Our contributions include: (1) we classify the inconsistency features to formalize inconsistency data and establish an algebraic operation system, (2) we propose three algorithms to auto-discover inconsistency, including constraint-based, SDA-based and HPDM-based method and (3) we conduct experiments on real-world dataset to compare aforesaid schemes with Oracle-based inconsistency detection framework. The empirical results show that our methods outperform traditional framework both on accuracy and efficiency under Web big data.},
keywords={Big data;Data models;Computers;Data mining;Industries;Algorithm design and analysis;Distributed databases;Web Big Data;Data Consistency;Web Data Management;Data Quality Assessment;Data Analysis},
doi={10.1109/CBD.2015.22},
ISSN={},
month={Oct},}
@ARTICLE{8822937,
author={Li, Xin and Fan, Xiaoping and Qu, Xilong and Sun, Guang and Yang, Chen and Zuo, Biao and Liao, Zhifang},
journal={IEEE Access},
title={Curriculum Reform in Big Data Education at Applied Technical Colleges and Universities in China},
year={2019},
volume={7},
number={},
pages={125511-125521},
abstract={With the boom in data science, big data education has received increasing attention from all kinds of colleges and universities in China, and many of them are in a rush to offer big data education. This paper first analyzes the major areas of big data capability training and the Chinese market needs for various kinds of data science talent. Then, it discusses the curriculum design process for the “Data Science & Big Data Technology” bachelor's degree program, and summarizes some detailed approaches to improving teaching experiments. Finally, this paper proposes a graduating student profile for big data education at applied technical colleges and universities in China. The authors' main ideas include that, at the applied technical colleges and universities, a) a suitable graduating student orientation should be determined as the big data talent needs are hierarchical; b) the redesigned curriculum in big data education should provide students more practical capabilities and knowledge; c) the teaching of the existing mainstream big data technologies and tools should be significant components in the syllabi of big data education.},
keywords={Data science;Data visualization;Economics;Training;Big Data applications;Applied Technical Colleges and Universities;big data education;curriculum reform},
doi={10.1109/ACCESS.2019.2939196},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9148224,
author={Qin, Fang and Zeng, Weijia and Li, Lin and Zhao, Rong},
booktitle={2020 International Wireless Communications and Mobile Computing (IWCMC)},
title={Construction of Big Data Monitoring Platform for Teaching Quality under Intelligent Education},
year={2020},
volume={},
number={},
pages={1594-1597},
abstract={To a great extent, the quality of teaching determines the level of trained talents. Now it has entered the era of intelligent education, coupled with the rapid development of the Internet, has produced a large number of teaching data, which also makes the monitoring and evaluation of teaching quality become particularly difficult. In view of the above problems, this paper proposes the construction of big data monitoring platform for teaching quality under intelligent education. In this paper, the OPC UA unified architecture is used for communication between devices, and the information configuration is based on the spring boot framework to achieve data collection. Then, data processing is based on GRU neural network, and spark distributed computing framework is used to improve the efficiency of data operation. Finally, the monitoring effect is realized by constructing the evaluation system.},
keywords={Education;Big Data;Monitoring;Sparks;Distributed computing;Computer architecture;intelligent education;teaching quality big data;monitoring platform construction},
doi={10.1109/IWCMC48107.2020.9148224},
ISSN={2376-6506},
month={June},}
@INPROCEEDINGS{7046920,
author={Fan, Lang and Ma, Hui},
booktitle={2014 International Conference on Management of e-Commerce and e-Government},
title={Comparative Study of Products Quality Control System of Countries in the Era of Big Data},
year={2014},
volume={},
number={},
pages={211-214},
abstract={Product quality control is an important outcome of the development of human society and production management of core areas, while its system is an important content of the quality and safety system. National regulatory system for product quality very seriously, however, product quality and safety are occurring. What is a quality management system? Regulatory information and what is the relationship between implementation of the system? Internet and "big data" but also can lead to changes in the regulatory process and innovation of social governance and strict regulatory regime covering the whole process? This series of quality control problems have been highlighted. Thus, drawing on the experience of other countries, along the historical context, reflections on both sorting and summarizing, and building more accurate inference and description are very urgent and necessary.},
keywords={Safety;Quality assessment;Product design;Standards;Control systems;Big data;Educational institutions;Product quality control;Big data;Regulatory regime},
doi={10.1109/ICMeCG.2014.51},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9298378,
author={Desai, Vinod and H A, Dinesha},
booktitle={2020 IEEE International Conference for Innovation in Technology (INOCON)},
title={A Hybrid Approach to Data Pre-processing Methods},
year={2020},
volume={},
number={},
pages={1-4},
abstract={This is an era of big data, as data is growing exponentially and resources are running out of infrastructure, so it is required to accommodate all the data that gets generated. We collect data in enormous amounts to derive meaningful conclusions, perform effective data analytics and improve decision making. As we don't have enough infrastructures to support data storage for huge volumes, it is needed to clean the data in compulsion. It is a mandatory to carry out a step before doing anything with the data. We call it pre-processing of data and this is carried out in various steps. Pre-processing includes data cleaning, data integration, data filtering, and data transformation and so on. As such preprocessing is not limited to the number of steps or a number of methods or definitive methods. We must innovatively preprocess the data before it is being consumed for data analytics. It has become a responsibility for every data analyst or big data researcher to handpick data for his or her analytics. Considering all these techniques in mind we are proposing a hybrid technique to leverage various algorithms available to pre-process our data along with minor modifications such as at the run time, choosing an algorithm or technique wisely based on the data that we have.},
keywords={Big Data;Data integrity;Error analysis;Data mining;Data analysis;Transforms;Time series analysis;Big Data;Data Pre-processing;Data Quality checks},
doi={10.1109/INOCON50539.2020.9298378},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9434497,
author={XU, Jinying},
booktitle={2020 International Conference on Big Data and Social Sciences (ICBDSS)},
title={Research on Multidimensional Teaching Mode of College English Based on Data Mining},
year={2020},
volume={},
number={},
pages={1-4},
abstract={The rapid development of computer technology and Internet technology has promoted the development of the era of big data. The acquisition and application of data information is an important foundation for improving the level of data application in the era of big data. In the current college English teaching process, the multi-dimensional English teaching model based on data mining has a positive effect on improving college English teaching efficiency and ensuring teaching quality. In the context of big data, teachers not only need to understand the problems in college English teaching, but also teachers must analyze the way of multi-dimensional college English teaching mode based on data mining, so as to effectively improve the quality of English teaching.},
keywords={Computational modeling;Education;Social sciences;Big Data;Big Data applications;Data models;Internet;Big Data Mining Technology;College English Teaching;Multidimensional Teaching Model},
doi={10.1109/ICBDSS51270.2020.00008},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8923233,
author={Muenzberg, Alexander and Sauer, Janina and Hein, Andreas and Roesch, Norbert},
booktitle={2019 International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)},
title={Checking the Plausibility of Nutrient Data in Food Datasets Using KNIME and Big Data},
year={2019},
volume={},
number={},
pages={1-4},
abstract={As there is no standardized food database with all products available in Europe, many developers of health apps fall back on databases of communities whose quality is often insufficient. In health apps, the quality of the data sets is critical, as poor quality lowers the user's confidence. This paper examines the plausibility of nutrient data from such data sources using similarity analysis, decision support methods and Big Data technology. During a special developed process, the plausibility of the data is to be increased. Finally, the methods used will be evaluated on the basis of test data.},
keywords={Decision trees;Databases;Conferences;Wireless communication;Big Data;Data mining;Food products;Food Data;Nutrient Data;Data Analysis;Big Data;Data Mining},
doi={10.1109/WiMOB.2019.8923233},
ISSN={2160-4894},
month={Oct},}
@ARTICLE{8950481,
author={Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu, Liang},
journal={Chinese Journal of Electrical Engineering},
title={A missing power data filling method based on improved random forest algorithm},
year={2019},
volume={5},
number={4},
pages={33-39},
abstract={Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.},
keywords={Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data mining;Big data cleaning;missing data filling;data preprocessing;random forest;data quality},
doi={10.23919/CJEE.2019.000025},
ISSN={2096-1529},
month={Dec},}
@INPROCEEDINGS{4424094,
author={Vatra, Fanica and Poida, Ana and Stanescu, Carmen},
booktitle={2007 9th International Conference on Electrical Power Quality and Utilisation},
title={Data system for the monitoring of power quality in the transmission substations supplying big consumers},
year={2007},
volume={},
number={},
pages={1-5},
abstract={During 2006, at CN Transelectrica - OMEPA Branch request, ISPE - Power Systems Department has conceived the designing documentations (Feasibility Study and Tender Documents) for “Power Quality Analyzing System at the big consumers”. The present paper reports the purpose and technical endowment proposed by ISPE for “Power Quality Monitoring and Analyzing System” that will be developed at OMEPA.},
keywords={Data systems;Monitoring;Power quality;Substations;Power measurement;Electric variables measurement;Data analysis;Current measurement;Frequency measurement;Data acquisition;power quality;data acquisition;monitoring;data system;big consumers},
doi={10.1109/EPQU.2007.4424094},
ISSN={2150-6655},
month={Oct},}
@INPROCEEDINGS{9005952,
author={Qiang, Yang},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Federated Recommendation Systems},
year={2019},
volume={},
number={},
pages={1-1},
abstract={Despite its great progress so far, artificial intelligence (AI) is facing a serious challenge in the availability of high-quality Big Data. In many practical applications, data are in the form of isolated islands. Efforts to integrate the data are increasingly difficult partly due to serious concerns over user privacy and data security. The problem is exacerbated by strict government regulations such as Europe's General Data Privacy Regulations (GDPR). In this talk, I will review these challenges and describe efforts to address them in recommendation systems area. In particular, I will give an overview of recent advances in federated learning and then focus on developments of “federated recommendation systems”, which aims to build high-performance recommendation systems by bridging data repositories without compromising data security and privacy.},
keywords={Big Data;Data privacy;Data security;Learning (artificial intelligence);Conferences;Government},
doi={10.1109/BigData47090.2019.9005952},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7821610,
author={Kush, Ashwani and Hwang, C. Jinshong and Dattana, Vishal},
booktitle={2016 Future Technologies Conference (FTC)},
title={Big data analytics on MANET routing standardization using quality assurance metrics},
year={2016},
volume={},
number={},
pages={192-198},
abstract={An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Routing in ad-hoc network is a challenging issue. Big data analytics have been suggested for proper evaluation and decision making in routing and placement of ad hoc network nodes. This Paper analyses the performance of AODV and DSR routing protocols for the quality assurance metrics. The performance differentials of AODV and DSR protocols are analyzed using NS-2 which is the main network simulator, NAM (Network Animator) and compared in terms of scales applied on Packet Delivery Ratio (PDR), in different environments specified by varying pause time, speed and number of nodes.},
keywords={Routing protocols;Routing;Big data;Mobile ad hoc networks;Quality assurance;AODV;Big Data;PDR;Metrics;Quality},
doi={10.1109/FTC.2016.7821610},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8669529,
author={Song, Guijuan and Wen, Yandong and Jia, Yue and Liu, Hongbo},
booktitle={2019 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={Research on Medical Service System Based on Big Data Technology},
year={2019},
volume={},
number={},
pages={302-304},
abstract={The purpose of the medical information sharing system based on big data in this paper is promoting the development of medical informatization, providing technical solutions and supporting conditions for the construction of regional medical informatization in China. The system is bound to promote the quality of medical information service by the application of big data and cloud computing and solve the "high cost but low efficiency" problem. This paper designs a service pattern that takes the patients as the center and realizes the reasonable distribution and sharing of medical resources. The system also provides experience and references for the medical service innovation in China. In this paper, a medical service system BDMSP which based on big data technology, is introduced. It describes the design of BDMSP from system architecture, key technologies, its application and etc. It may be a helpful research for application of big data technology.},
keywords={Data mining;Big Data;Medical diagnostic imaging;Data models;Computational modeling;Computer architecture;BDMSP;Medical Service;Big Data},
doi={10.1109/ICITBS.2019.00079},
ISSN={},
month={Jan},}
@INPROCEEDINGS{9378008,
author={Tasnim, Jarin and Mondal, Debajyoti},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Data Reduction and Deep-Learning Based Recovery for Geospatial Visualization and Satellite Imagery},
year={2020},
volume={},
number={},
pages={5276-5285},
abstract={The storage, retrieval, and distribution of data are some critical aspects of big data management. Data scientists and decision-makers often need to share large datasets and make decisions on archiving or deleting historical data to cope with resource constraints. A potential approach to mitigate such problems is to reduce big datasets into smaller ones, which will not only lower storage requirements but also allow light load transfer over the network. Carefully prepared data by removing redundancies, along with a machine learning model capable of reconstructing the whole dataset from its reduced version, can improve the storage scalability, data transfer, and speed up the overall data management pipeline. In this paper, we explore some data reduction strategies for big datasets, while ensuring that the data can be transferred and used ubiquitously by all stakeholders, i.e., the entire dataset can be reconstructed with high quality whenever necessary. Our approach guarantees a minimum of 75% data size reduction, where the reconstruction accuracy observed is as high as 98.75% on an average for geospatial meteorological data (e.g., soil moisture and albedo), and 99.09% for satellite imagery. We propose a novel variance based reduction technique that can further reduce the data size without losing the accuracy significantly, and adopt various deep learning approaches for high-quality reconstruction.},
keywords={Deep learning;Satellites;Data visualization;Big Data;Geospatial analysis;Stakeholders;Image reconstruction;Data Reduction;Reconstruction;Deep Learning;SRGAN;Image Inpainting;Geospatial Visualization},
doi={10.1109/BigData50022.2020.9378008},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7944943,
author={Kang, Gaganjot and Gao, Jerry Zeyu and Xie, Gang},
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Data-Driven Water Quality Analysis and Prediction: A Survey},
year={2017},
volume={},
number={},
pages={224-232},
abstract={Water quality becomes one of the important quality factors for the quality life in smart cities. Recently, water quality has been degraded due to diverse forms of pollution caused by disposal of human wastes, industrial wastes, automobile wastes. The increasing pollution affects water quality and the quality of people's life. Hence, water quality evaluation, monitoring, and prediction become an important and hot research subject. In the past, many environmental researchers have dedicated their research efforts on this subject using conventional approaches. Recently, many researchers begin to use the big data analytics approach to studying, evaluating, and predicting water quality due to the advances of big data applications and the availability of environmental sensing networks and sensor data. This paper reviews the published research results relating to water quality evaluation and prediction. Moreover, the paper classifies and compares the applied big data analytics approaches and big data based prediction models for water quality assessment. Furthermore, the paper also discusses the future research needs and challenges.},
keywords={Water pollution;Water resources;Big Data;Analytical models;Data models;Water;Indexes;Water quality evaluation;big data analytics;data-driven water quality evaluation;and water quality prediction},
doi={10.1109/BigDataService.2017.40},
ISSN={},
month={April},}
@INPROCEEDINGS{8029349,
author={Kim, Hee Young and Cho, June-Suh},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={Data Governance Framework for Big Data Implementation with a Case of Korea},
year={2017},
volume={},
number={},
pages={384-391},
abstract={Big Data governance requires a data governance that can satisfy the needs for corporate governance, IT governance, and ITA/EA. While the existing data governance focuses on the processing of structured data, Big Data governance needs to be established in consideration of a broad sense of Big Data services including unstructured data. To achieve the goals of Big Data, strategies need to be established together with goals that are aligned with the vision and objective of an organization. In addition to the preparation of the IT infrastructure, a proper preparation of the components is required to effectively implement the strategy for Big Data services. We propose the Big Data Governance Framework in this paper. The Big Data governance framework presents criteria different from existing criteria at the data quality level. It focuses on timely, reliable, meaningful, and sufficient data services, focusing on what data attributes should be achieved based on the data attributes of Big Data services. In addition to the quality level of Big Data, the personal information protection strategy and the data disclosure/accountability strategy are also needed to achieve goals and to prevent problems. This paper performed case analysis based on the Big Data Governance Framework with the National Pension Service of South Korea. Big Data services in the public sector are an inevitable choice to improve the quality of people's life. Big Data governance and its framework are the essential components for the realization of Big Data service.},
keywords={Big Data;Government;Data privacy;Reliability;Social network services;Big data;Data governance;Data governance framework;Case analysis},
doi={10.1109/BigDataCongress.2017.56},
ISSN={},
month={June},}
@INPROCEEDINGS{9407044,
author={Zhong, Qi and Wang, Yuxin},
booktitle={2020 International Conference on Big Data Economy and Information Management (BDEIM)},
title={Analysis of Factors Affecting the Sales of Popular Science Books Based on Big Data},
year={2020},
volume={},
number={},
pages={66-69},
abstract={The consumption data of popular science books on Taobao is obtained by using the technology of web crawler, and an empirical study is carried out on the indexes that affect the sales. Through the research, it is found that the sales rank of search engines, the price, the quantity and quality and content of online reviews have a significant impact on the sales of popular science books. Although the basic variable is an important reference index for consumers to make decisions, it has no significant impact on sales. As an important manifestation of the value of popular science books, price has a positive impact on sales, but buyers are usually willing to pay a certain premium for high-quality products. Sales rank is an important function of major e-commerce websites, which has a strong correlation and boosting effect with sales. Different variables have different effects on the sales of popular science books.},
keywords={Correlation;Decision making;Crawlers;Big Data;Search engines;Boosting;Data models;Data Mining;Big Data;Web Crawler;Popular Science Books},
doi={10.1109/BDEIM52318.2020.00024},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8939061,
author={Ambigavathi, M. and Sridharan, D.},
booktitle={2018 Tenth International Conference on Advanced Computing (ICoAC)},
title={Big Data Analytics in Healthcare},
year={2018},
volume={},
number={},
pages={269-276},
abstract={The pace of both digital innovation and technology disruption is refining the healthcare industry at an exponential rate. The large volume of healthcare data continues to mount every second, making it harder and very difficult to find any form of useful information. Recently, big data is shifting the traditional way of data delivery into valuable insights using big data analytics method. Big data analytics provides a lot of benefits in the healthcare sector to detect critical diseases at the initial stage and deliver better healthcare services to the right patient at the right time so that it improves the quality of life care. Big data analytics tools play an essential role to analyze and integrate large volumes of structured, semi-structured and unstructured vital data rapidly produced by the various clinical, hospitals, other social web sources and medical data lakes. However, there are several issues to be addressed in the current health data analytics platforms that offer technical mechanisms for data collection, aggregation, process, analysis, visualization, and interpretation. Due to lack of detailed study in the previous literature, this article inspects the promising field of big data analytics in healthcare. This article examines the unique characteristics of big data, big data analytical tools, different phases followed by the healthcare economy from data collection to the data delivery stage. Further, this article briefly summarizes the open research challenges with feasible findings, and then finally offers the conclusion.},
keywords={Medical services;Big Data;Tools;Biomedical imaging;Data mining;Data analysis;Data models;Big data;Big data analytics;Big data analytics tools;Healthcare applications},
doi={10.1109/ICoAC44903.2018.8939061},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7474371,
author={Ayyalasomayajula, Haripriya and Gabriel, Edgar and Lindner, Peggy and Price, Daniel},
booktitle={2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Air Quality Simulations Using Big Data Programming Models},
year={2016},
volume={},
number={},
pages={182-184},
abstract={Forecasts of daily pollutant levels have become a standard part of weather predictions in television, on-line, and in newspapers. Research groups also need to analyze larger timeframes across more locations to correlate long term developments for different pollutants with multiple serious health effects such as asthma. This paper presents a comparison of the Hadoop MapReduce and Spark programing models for air quality simulations, guiding future code development for the research groups interested in these analyses. Two use cases have been used, namely (i) calculating the eight hour rolling average of pollutants in a restricted region, (ii) identifying clusters of sensors showing similar patterns in pollutant concentration over multiple years in the state of Texas. The data set used in this analysis is air pollution data collected over fifteen years at 179 monitor sites across the state of Texas for a variety of pollutants. Our results reveal 20-25% performance benefits for the Spark solutions over MapReduce. Furthermore, it documents performance benefits of the Spark MLlib machine learning library over the Mahout library which is based on the MapReduce programing model.},
keywords={Sparks;Atmospheric modeling;Air quality;Analytical models;Sensors;Computational modeling;Data models;Air Quality Simulations;MapReduce;Spark},
doi={10.1109/BigDataService.2016.26},
ISSN={},
month={March},}
@INPROCEEDINGS{8258218,
author={Colborne, Adrienne and Smit, Michael},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Identifying and mitigating risks to the quality of open data in the post-truth era},
year={2017},
volume={},
number={},
pages={2588-2594},
abstract={Big Data analysis often relies on open data, integrating it with large private data sets, using it as ground truth information, or providing it as part of the input to large simulations. Data can be released openly by governments to achieve various objectives: transparency, informing citizen engagement, or supporting private enterprise, to name a few. To the latter objective, Big Data analytics algorithms rely on high-quality, timely access to various data sources, including open data. Examples include retail analytics drawing on open demographic data and weather forecast systems drawing on open weather and climate data. In this paper, we describe the rise of post-truth in society, and the risks this poses to the quality, integrity, and authenticity of open data. We also discuss approaches to identifying, assessing, and mitigating these risks, and suggest future steps to manage this data quality concern.},
keywords={Big Data;Meteorology;Portals;Voting;open data;post-truth;fake news;risk identification;risk mitigation;data quality assurance},
doi={10.1109/BigData.2017.8258218},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8862445,
author={Malik, Vinita and Singh, Sukhdip},
booktitle={2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)},
title={Cloud, Big Data & IoT: Risk Management},
year={2019},
volume={},
number={},
pages={258-262},
abstract={The heart of research pumps for analyzing risks in today's competitive business environment where big, massive computations are performed on interconnected devices pervasively. Advanced computing environments i.e. Cloud, big data and Internet of things are taken under consideration for finding and analyzing business risks developed from evolutionary, interoperable and digital devices communications with massive volume of data generated. Various risks in advanced computational environment have been identified in this research and are provided with risks mitigation strategies. We have also focused on how risk management affects these environments and how that effect can be mitigated for software and business quality improvement.},
keywords={Cloud computing;Internet of Things;Big Data;Risk management;Security;Distributed databases;Big data;Internet of Things;Cloud Computing;Risk Management},
doi={10.1109/COMITCon.2019.8862445},
ISSN={},
month={Feb},}
@INPROCEEDINGS{7004319,
author={Malcolm, Rohan and Morrison, Cherrelle and Grandison, Tyrone and Thorpe, Sean and Christie, Kimron and Wallace, Akim and Green, Damian and Jarrett, Julian and Campbell, Arnett},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Increasing the accessibility to Big Data systems via a common services API},
year={2014},
volume={},
number={},
pages={883-892},
abstract={Despite the plethora of polls, surveys, and reports stating that most companies are embracing Big Data, there is slow adoption of Big Data technologies, like Hadoop, in enterprises. One of the primary reasons for this is that companies have significant investments in legacy languages and systems and the process of migrating to newer (Big Data) technologies would represent a substantial commitment of time and money, while threatening the ir short-term service quality and revenue goals. In this paper, we propose a possible solution that enables existing infrastructure to access Big Data systems via a services application programming interface (API); minimizing the migration drag and (possibly negative) business repercussions.},
keywords={Big data;Semantics;XML;Arrays;Business;Standards organizations;Big Data;Hadoop;API;Big Data Systems},
doi={10.1109/BigData.2014.7004319},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9361012,
author={Qiuju, Huang},
booktitle={2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)},
title={Influence of Big Data on Modern Risk-oriented Audit and Countermeasures},
year={2020},
volume={},
number={},
pages={319-322},
abstract={The level of economic development determines the actual state of social development. With the advent of the era of fragmentation, science and technology are constantly evolving. Big data and multiple linkage technologies are newly presented in response to the trend of the new era. At the same time, big data can be widely used in various fields. It can also play an effective role in risk-oriented audit. Therefore, this article starts from reality, fully explores the problems of big data in modern risk-oriented audit, and then proposes corresponding solutions.},
keywords={Economics;Couplings;Machine learning;Big Data applications;Market research;Business intelligence;big data;modern risk-oriented audit;work quality},
doi={10.1109/MLBDBI51377.2020.00068},
ISSN={},
month={Oct},}
@ARTICLE{8667300,
author={Lee, Doyoung},
journal={IEEE Access},
title={Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea},
year={2019},
volume={7},
number={},
pages={36294-36299},
abstract={In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.},
keywords={Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical measurement;Big data;data quality;data traceability;metrology;standard reference data;uncertainty},
doi={10.1109/ACCESS.2019.2904286},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{6597164,
author={Zheng, Zibin and Zhu, Jieming and Lyu, Michael R.},
booktitle={2013 IEEE International Congress on Big Data},
title={Service-Generated Big Data and Big Data-as-a-Service: An Overview},
year={2013},
volume={},
number={},
pages={403-410},
abstract={With the prevalence of service computing and cloud computing, more and more services are emerging on the Internet, generating huge volume of data, such as trace logs, QoS information, service relationship, etc. The overwhelming service-generated data become too large and complex to be effectively processed by traditional approaches. How to store, manage, and create values from the service-oriented big data become an important research problem. On the other hand, with the increasingly large amount of data, a single infrastructure which provides common functionality for managing and analyzing different types of service-generated big data is urgently required. To address this challenge, this paper provides an overview of service-generated big data and Big Data-as-a-Service. First, three types of service-generated big data are exploited to enhance system performance. Then, Big Data-as-a-Service, including Big Data Infrastructure-as-a-Service, Big Data Platform-as-a-Service, and Big Data Analytics Software-as-a-Service, is employed to provide common big data related services (e.g., accessing service-generated big data and data analytics results) to users to enhance efficiency and reduce cost.},
keywords={Information management;Data handling;Data storage systems;Quality of service;Data visualization;Fault tolerance;Fault tolerant systems;big data;service computing;Big Data-as-a-Service},
doi={10.1109/BigData.Congress.2013.60},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7836747,
author={Isetani, Hideki},
booktitle={2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)},
title={The Prominent Role of Probe Data and Big Data in Modern Technology},
year={2016},
volume={},
number={},
pages={788-791},
abstract={In today's society, the management of probe data and big data is becoming increasingly important with new technological advancements. This can be demonstrated in the ongoing development of automated driving systems and the dynamic map. The safety of these new systems depends on high quality data and a reliable shared platform. As the transition to artificial intelligence continues, it is the responsibility of humans to create a system that can effectively control information with minimal risks.},
keywords={Vehicle dynamics;Safety;Roads;Probes;Big data;Industries;Vehicles;big data;dynamic map;probe data;shared platform},
doi={10.1109/ICDMW.2016.0116},
ISSN={2375-9259},
month={Dec},}
@INPROCEEDINGS{8275101,
author={Setiadi, Yazid and Uluwiyah, Ana},
booktitle={2017 International Workshop on Big Data and Information Security (IWBIS)},
title={Improving data quality through big data: Case study on big data-mobile positioning data in Indonesia tourism statistics},
year={2017},
volume={},
number={},
pages={43-48},
abstract={Big Data is a new concept that has become widely popularised in recent years. The revolutionized meaning of information communication technologies and Internet technologies refers to mobile communications which enable individuals to move and generate, transmit and receive different kinds of information. There are many communication options where users can search, interact and share information with other users such as website, social media, online communities blogs, and email called as Digital transformation. In Digital transformation era, over 95 % of travellers today use digital resources. Digital traveler can be as data source for official statistics. One of methods to capture the number of tourist can use Mobile Positioning Data (MPD). This method is considered able to improve the quality of survey data. This article will discuss further how to improve the quality of survey data through Big Data with case studies of MPD users in Indonesian Tourism Statistics.},
keywords={1/f noise;Mobile communication;Big Data;Accuracy;Mobile Positioning Data;Tourism Statistics},
doi={10.1109/IWBIS.2017.8275101},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9360973,
author={Tian, Yan},
booktitle={2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)},
title={Research on How to Strengthen Ideological and Political Education in Colleges and Universities in the Era of Big Data},
year={2020},
volume={},
number={},
pages={274-277},
abstract={Analyzing, judging and sorting out data through big data technology helps us grasp the prospects and laws of the development of things. Especially for the future development direction of things, more accurate calculations can be made, and scientific and reasonable measures can be taken to promote the development of things according to the calculation results. At present, in the process of ideological and political education in my country's colleges and universities, the teaching methods are mainly traditional teaching methods. In classroom teaching, teachers instill the corresponding curriculum theory, the teaching method is relatively simple, and the curriculum content is boring, which seriously affects the teaching efficiency and teaching quality of college ideological and political classrooms. The use of big data technology to build a curriculum network system that is more suitable for students' learning needs. Besides, the use of some simple extracurricular activities to assist students' education and teaching can mobilize students' enthusiasm and enthusiasm for participating in ideological and political education in colleges and universities, which is conducive to giving full play to ideological and political education in colleges and universities positive effect. Therefore, we need to pay attention to the positive role of big data technology in college ideological and political education, and use big data technology to strengthen college ideological and political education.},
keywords={Education;Machine learning;Big Data;Business intelligence;Sorting;Big Data Era;Ideological and Political Education in Colleges and Universities;Innovation Strategy;Problem Analysis},
doi={10.1109/MLBDBI51377.2020.00058},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9434447,
author={Wang, Siyou and Ma, Guoqiang and Tian, Yunchen},
booktitle={2020 International Conference on Big Data and Social Sciences (ICBDSS)},
title={Research on the Development Trend of Aquaculture Based on Big Data Analysis},
year={2020},
volume={},
number={},
pages={169-172},
abstract={Aquaculture economy plays an important role in guaranteeing the food supply and quality protein output of our residents. By cleaning, aggregating and analyzing the big data of China, Japan and the United States from 2010 to 2017, published by the fisheries and aquaculture department of FAO, we study the development trend of aquaculture production and value in the three countries and make a prediction in 5 years by linear regression fitting. Finally, suggestions were put forward for the aquaculture development. This paper found that China's aquaculture production and value have been developing steadily, and would continue to stay progression in five years, but the growth rate would decline. The variety of aquaculture species is diversified, the production and value of aquaculture farming species are relatively balanced. China should strengthen aquaculture resources and environmental conservation and vigorously develop high technology in aquaculture. As for Japan, development prospects are not bright in the near future. Due to the combined effects of tsunami and aging fishery communities, the aquaculture is generally showing a declining trend. Japan should take measures to restore and strengthen fishery resources and revitalize fishing communities. In the United States, aquaculture production declined gradually after 2004 due mainly to the shrinking scale of channel catfish farming. Since then, it has remained quite stable. Besides, the aquaculture species are relatively poor, the production and value of the farming species are extremely uneven. At present, the United States should expand aquaculture species, substantially increase aquaculture production and maintain a growing trend.},
keywords={Proteins;Silver;Social sciences;Linear regression;Production;Big Data;Aging;Aquaculture Production and Value;Big Data Analysis;Development Trend},
doi={10.1109/ICBDSS51270.2020.00045},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9403835,
author={Xuan, Liu and Chang, Liu},
booktitle={2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)},
title={Analysis of Computer Science Based on Big-Data Mining},
year={2020},
volume={},
number={},
pages={94-97},
abstract={The scientific construction of a first-class discipline construction evaluation system is of great significance to the promotion of discipline construction. As an important evaluation reference system, the third-party evaluation system must pay attention to its underlying data sources and calculation methods. Using the massive underlying data of the Scopus database, through the analysis of the computer disciplines of four Chinese universities, the development trend is discussed from the aspects of overall academic output, scientific research quality, and hot topics.},
keywords={Computer science;Statistical analysis;Distributed databases;Tools;Big Data;Market research;Performance analysis;Underlying data sources;Computer Science;Research output quality;research topics},
doi={10.1109/ICBASE51474.2020.00028},
ISSN={},
month={Oct},}
@ARTICLE{9259196,
author={Zhai, Guanlin and Yang, Yan and Wang, Heng and Du, Shengdong},
journal={Big Data Mining and Analytics},
title={Multi-attention fusion modeling for sentiment analysis of educational big data},
year={2020},
volume={3},
number={4},
pages={311-319},
abstract={As an important branch of natural language processing, sentiment analysis has received increasing attention. In teaching evaluation, sentiment analysis can help educators discover the true feelings of students about the course in a timely manner and adjust the teaching plan accurately and timely to improve the quality of education and teaching. Aiming at the inefficiency and heavy workload of college curriculum evaluation methods, a Multi-Attention Fusion Modeling (Multi-AFM) is proposed, which integrates global attention and local attention through gating unit control to generate a reasonable contextual representation and achieve improved classification results. Experimental results show that the Multi-AFM model performs better than the existing methods in the application of education and other fields.},
keywords={Two dimensional displays;Task analysis;Sentiment analysis;Data models;Data mining;Context modeling;Analytical models;educational big data;sentiment analysis;aspect-level;attention},
doi={10.26599/BDMA.2020.9020024},
ISSN={2096-0654},
month={Dec},}
@INPROCEEDINGS{7951917,
author={Xu Xiao-tao and Yang Chen and Dai Guang-hua and Ma Hua-long},
booktitle={2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Information service quality evaluation study of cloud computing environment based on big data},
year={2017},
volume={},
number={},
pages={236-240},
abstract={Give full play to the big data in the cloud computing environment application advantages has become an important information service mode of the era of Internet +, paper with information service quality evaluation as the main line, using fuzzy comprehensive evaluation method to analyze a set of cloud computing environment information service quality evaluation process, at the same time, using the case of project construction example analysis, the evaluation of cloud computing environment based on large data information service quality has important reference value.},
keywords={Information services;Indexes;Cloud computing;Reliability;Security;Transforms;big data;cloud computing environment;information service;quality evaluation},
doi={10.1109/ICCCBDA.2017.7951917},
ISSN={},
month={April},}
@INPROCEEDINGS{9092466,
author={Xueli, Cao and Hongna, Gao and Guanyu, kou},
booktitle={2019 International Conference on Information Technology and Computer Application (ITCA)},
title={Research of the Integration of Humanistic Quality in Education under the Background of Intelligent Big Data},
year={2019},
volume={},
number={},
pages={289-291},
abstract={In the context of the new era, it is very indispensable to integrate humanistic quality education into college and university. However, there are still some problems because of the impact of big data information, which is not conducive to students' humanity and even affects their future development. The following is an in-depth analysis of the integration of humanistic quality in education under the background of big data information. The purpose of this paper is to effectively promote the integration of humanistic quality between college and university education, in order to improve the humanistic quality of students as well as better development of students.},
keywords={Education;Big Data;Cloud computing;Media;Information technology;Streaming media;big data information;humanistic qualities;college and university education},
doi={10.1109/ITCA49981.2019.00070},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9377918,
author={Inibhunu, Catherine and McGregor, Carolyn},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Edge Computing with Big Data Cloud Architecture: A Case Study in Smart Building},
year={2020},
volume={},
number={},
pages={3387-3393},
abstract={The growth of buildings embedded with technologies that can monitor the internal building environment with respect to energy consumption such as heating, ventilation, air conditioning, wind, motion as well occupancy have an immense potential. From energy management, occupancy administration, security maintenance as well as improving the health and quality of life for humans in indoor or outdoor spaces. These potentials can be realized by a clear understanding of the interplay between vast environmental conditions, humans and their health as well as the many smart products they interact with in their lives. This is a complex process that requires thorough testing and evaluation within smart buildings simulation environments where multiple buildings data can be generated and then effectively analyzed. This can be facilitated by a robust data management process that utilizes big data computing technologies to harness large volumes, variety and velocity of data that can be captured within smart buildings while maintain the security and privacy of data sources.In this paper we describe a smart building architecture that has been designed and developed for management of data from a smart building. In particular the architecture enables acquisition, processing and distribution of simulated environmental building data to multiple consumers and workflows for further processing and analysis locally and in a high performance cloud computing platform. The research premise is that such an architecture enables effective management of multiple data sources within climatic based simulated testing in smart buildings to further research.},
keywords={Cloud computing;Smart buildings;Architecture;Computer architecture;Big Data;Data models;Testing;Climatic Facilities;Smart Buildings Simulations;Big Data Architecture;IoT Cloud Frameworks;Edge Computing},
doi={10.1109/BigData50022.2020.9377918},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8622229,
author={Austin, Claire C.},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={A Path to Big Data Readiness},
year={2018},
volume={},
number={},
pages={4844-4853},
abstract={"Big Data readiness" begins at the source where data are first created and extends along a path through an organization to the outside world. This paper focuses on practical solutions to common problems experienced when integrating diverse datasets from disparate sources. Following the Introduction, Section 2 situates Big Data in the larger context of open government, open science, science integrity, and Standards, internationally and in Canada. Section 3 analyses the Big Data problem space, while Section 4 proposes a Big Data solution space. Section 5 proposes eight data checklist modules and suggests implementation strategies to effectively meet a variety of organizational needs. Section 6 summarizes conclusions and describes future work.},
keywords={Big Data;Government;Standards organizations;Europe;Tools;Big Data;data quality;data checklist;data repository;open science;open government;data science},
doi={10.1109/BigData.2018.8622229},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9209633,
author={Byabazaire, John and O’Hare, Gregory and Delaney, Declan},
booktitle={2020 29th International Conference on Computer Communications and Networks (ICCCN)},
title={Using Trust as a Measure to Derive Data Quality in Data Shared IoT Deployments},
year={2020},
volume={},
number={},
pages={1-9},
abstract={Recent developments in Internet of Things have heightened the need for data sharing across application domains to foster innovation. As most of these IoT deployments are based on heterogeneous sensor types, there is increased scope for sharing erroneous, inaccurate or inconsistent data. This in turn may lead to inaccurate models built from this data. It is important to evaluate this data as it is collected to establish its quality. This paper presents an analysis of data quality as it is represented in Internet of Things (IoT) systems and some of the limitations of this representation. The paper then introduces the use of trust as a heuristic to drive data quality measurements. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework for representing data quality within the big data model. To demonstrate the application of a trust backed framework, we used data collected from a IoT deployment of sensors to measure air quality in which a low cost sensor was co-located with a gold reference sensor. Using data streams modeled based on a dataset from an IoT deployment, our initial results show that the framework's trust score are consistent with the accuracy measure of the machine learning models.},
keywords={Data integrity;Data models;Big Data;Biological system modeling;Measurement;Standards;Internet of Things;Data Quality;Internet of Things (IoT);Trust;Big Data Model;Machine learning},
doi={10.1109/ICCCN49398.2020.9209633},
ISSN={2637-9430},
month={Aug},}
@INPROCEEDINGS{9403816,
author={Peng, Lina},
booktitle={2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)},
title={A Research on the Improvement of the College English Classroom Study in the Context of New Media and Big Data},
year={2020},
volume={},
number={},
pages={15-18},
abstract={As a basic subject, college English is of great significance to the all-around development of college students. But traditionally, there exist certain problems, such as undiversified teaching methods, boredom in classroom teaching, lack of learning interests and ineffective classroom learning among students, facing college English teaching. With the develop of science and technology, the technology of new media has become an important part of people's life and work. In the age of new media and big data, the new media technology with its openness, diversity, plurality, and interactivity, can be combined with and facilitate classroom teaching. Such a combination not only provides abundance of learning materials, but also promotes students' learning interest as well as motivation. What's more, it could stimulate students' full development. Under this background, this paper will probe into the college English teaching status quo and the existing problems first, and then give advices regarding the improvement of students' classroom learning efficiency and quality by utilizing the new media technology.},
keywords={Mood;Education;Media;Big Data;Probes;Software engineering;New Media;Big Data;College English teaching;Improvement of Learning Quality},
doi={10.1109/ICBASE51474.2020.00010},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7724931,
author={Sangeeta and Sharma, Kapil},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)},
title={Quality issues with big data analytics},
year={2016},
volume={},
number={},
pages={3589-3591},
abstract={The promise of data-driven decision-making is now being recognized broadly, and there is growing enthusiasm for the notion of “Big Data.” In this paper techniques related with the big data pipelining or processing are discussed. There are number of issues related with the analytical results of the big data and the big data processing software. The major issues are included in this paper and will be helpful for the new beginners in this area of big data analytics.},
keywords={Big data;Software;Software reliability;Data models;Data mining;Computers;Big Data Reliability;Big Data Analytic},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{7575978,
author={Ye, Yuan and Daming, Zhu and Jingting, Hu},
booktitle={2016 China International Conference on Electricity Distribution (CICED)},
title={Discussion of application about using of big data technology in harmonic monitoring platform},
year={2016},
volume={},
number={},
pages={1-4},
abstract={At present, several provinces' power supply companies have established harmonic monitoring platform in China, by real-time monitoring the voltage and current of power grid, then upload these data to the main station to analyze and settle. The platform can collect about 1 TB(1TB=1000GB) data every day[12], these data contain all the indexes of power quality, but we can only use this platform to do some basal data analysis work, but not processing analysis with multi-indexes, so that it does not show strong supporting effort to the power grid operation. With the development of energy internet and the third industry, more and more unstructured data are impacting the power grid operation, how to make better use of big data technology to deal with the data from monitoring platform has became the focus of the power grid company. In this paper, some introduces of contents and characteristics of harmonic real-time monitoring platform and big data technology has been made, by comparing the way of using and settling data of platform to the big data technology, meanwhile by combining with the characteristics of power quality data, we analyze the combination of the two prospects. At last, the development direction of the advanced application for the harmonic line monitoring system is discussed in this paper.},
keywords={Monitoring;Power grids;Big data;Power quality;Harmonic analysis;Power system harmonics;Companies;harmonic real-time monitoring platform;big data technology;power quality},
doi={10.1109/CICED.2016.7575978},
ISSN={2161-749X},
month={Aug},}
@INPROCEEDINGS{8070826,
author={Panda, Minerva and Ali, Syed Mohd and Panda, Sanjog Kumar},
booktitle={2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC)},
title={Big data in health care: A mobile based solution},
year={2017},
volume={},
number={},
pages={149-152},
abstract={In the present Indian scenario, healthcare information is independently maintained by hospitals, institutions and not readily accessible in a centralized, informed manner. This greatly limits the health providers' efforts to improve quality and efficiency. Through this paper, we address this issue on bringing various information from many sources into one place in realtime which can be truly life saving. Also, low ratio of doctor to patient and the low per capita income in India hikes the medical expenses thereby increasing the patient's inaccessibility to receive proper health care in their reach especially for people in the rural areas. A means by which the bridge between the patients and doctors can be gapped and how patients can be treated at a lower expense is the prime concern. This paper focuses on the development of a mobile/web application, through which patients sends their symptomatic query to the doctors through a server. The mobile application will be equipped with first aid instructions, according to the nature and severity of the symptoms, either the patients are directed to respective departments or given emergency help for further treatment. Within the time huge amount of data is collected from users and doctors, this big data will be used to train machines to automate the tasks to some extent. The information gained from analyzing massive amounts of aggregated health data can provide useful insight to improve quality and efficiency for providers and insurers alike. This makes the patients reach out for healthcare solutions easily and cheaply and makes healthcare a easy reach for the unprivileged also. Thus, this unified model can serve as a data collection, delivery as well as an analytic tool in the healthcare domain.},
keywords={Medical diagnostic imaging;Big Data;Diseases;Analytical models;Sociology;Health Big Data;Mobile Application;Centralized health database;Health analytics;Remote Healthcare},
doi={10.1109/ICBDACI.2017.8070826},
ISSN={},
month={March},}
@INPROCEEDINGS{9403757,
author={Jia, Jianling},
booktitle={2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)},
title={Evaluation of Livable and Resilient Urban Design based on big data},
year={2020},
volume={},
number={},
pages={1-4},
abstract={In the background of the information age, modern information technology is constantly updated and improved, especially the rapid development of mobile information technology, which provides great convenience for the use of big data. In the context of big data, the livability of cities has attracted more and more attention. The construction of livable city is not only the best interpretation of the construction of resource-saving and environment-friendly social policies, but also the most urgent desire of urban residents for high-level economic development and high-quality living environment. This paper analyzes the ideas and methods of urban planning evaluation under the background of big data, which can further strengthen the function of urban space urban planning and promote the construction and development of urban modernization.},
keywords={Temperature sensors;Temperature distribution;Urban planning;Big Data;Thermal conductivity;Land surface temperature;Thermal pollution;big data;urban design;evaluation},
doi={10.1109/ICBASE51474.2020.00007},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9196305,
author={Zou, Jie and Gong, Wenkai and Guo, Zheng and Lin, Zhichi and Liu, Zekun},
booktitle={2020 International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)},
title={Discussion on the Transformation of Computer Remote Network Communication Technology in the Era of Big Data},
year={2020},
volume={},
number={},
pages={106-109},
abstract={With the advent of the big data era, computer network communication technology has brought certain conveniences to the development of people and enterprises, and it has also broken through the traditional time and geographical limitations. However, under the influence of the big data era, China s computers There are still many problems in the development of long-distance network communication technology. The existence of these problems will not only affect the data transmission speed and quality, but also have a certain negative impact on people’s production and life. Therefore, in the context of the era of big data, relevant departments should revolutionize the network communication technology of the Computer College. This article describes the relative advantages and existing problems of computer remote network communication technology in the era of big data, and puts forward reasonable suggestions for computer network remote communication technology.},
keywords={Handheld computers;Security;Maintenance engineering;Big Data;Artificial intelligence;Internet of Things;big data era;computer;remote network communication technology},
doi={10.1109/ICBAIE49996.2020.00028},
ISSN={},
month={June},}
@INPROCEEDINGS{9137451,
author={Xiaofeng, Luo and Jing, Luo},
booktitle={2020 3rd International Conference on Artificial Intelligence and Big Data (ICAIBD)},
title={Research on Big Data Reference Architecture Model},
year={2020},
volume={},
number={},
pages={205-209},
abstract={For ISO / / IEC TS 25011:2017 some deficiencies of big data reference system. In this paper, we redesigned the big data reference system structure model, and proposed a goal (big data information service quality), three chains (information value chain, information technology value chain and information assurance value chain) and five roles (big data application provider, big data framework provider and big data information assurance provider) 1-3-5 model of data provider and data consumer, and all the attributes of the model are described. In the part of big data information service quality, the deficiencies in ISO / / IEC TS 25011:2017 are corrected. Then, the basic system of big data is proposed. Under the big data reference architecture model proposed in this paper, compared with some other typical models, the conclusion is that other typical models can be replaced completely.},
keywords={ISO;Information services;Big Data;Big Data applications;Data models;IEC Standards;Information technology;big data;reference model;value chain;QoIs;role introduction},
doi={10.1109/ICAIBD49809.2020.9137451},
ISSN={},
month={May},}
@INPROCEEDINGS{8532518,
author={Burkhardt, Andrew and Berryman, Sheila and Brio, Ashley and Ferkau, Susan and Hubner, Gloria and Lynch, Kevin and Mittman, Susan and Sonderer, Kathy},
booktitle={2018 IEEE AUTOTESTCON},
title={Measuring Manufacturing Test Data Analysis Quality},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Manufacturing test data volumes are constantly increasing. While there has been extensive focus in the literature on big data processing, less focus has existed on data quality, and considerably less focus has been placed specifically on manufacturing test data quality. This paper presents a fully automated test data quality measurement developed by the authors to facilitate analysis of manufacturing test operations, resulting in a single number used to compare manufacturing test data quality across programs and factories, and focusing effort cost-effectively. The automation enables program and factory users to see, understand, and improve their test data quality directly. Immediate improvements in test data quality speed manufacturing test operation analysis, reducing elapsed time and overall spend in test operations. Data quality has significant financial impacts to businesses [1]. While manufacturing cost models are well understood, data quality cost models are less well understood (see Eppler & Helfert [2] who review manufacturing cost models and create a taxonomy for data quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary data quality cost calculation is described in [4]. Haug et al. [5] describe a classification of costs for poor data quality, and while they do not provide a cost calculation, they do define optimality for data quality. Laranjeiro et al. [6] have a recent survey of poor data quality classification. Ge & Helfert [7] extend the work in [2], and provide an updated review of data quality costs. Test data is specifically addressed in the context of data processing in [8]. Big data quality efforts are reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements for data quality metrics are identified in [12]. Data inconsistencies are detailed in [13], while categorical data inconsistencies are explained in [14]. In the current work, manufacturing test data quality is directly correlated to the speed of manufacturing test operations analysis. A measurement for manufacturing test data quality indicates the speed at which analysis can be performed, and increases in the test data quality score have precipitated increases in the speed of analysis, described herein.},
keywords={Data integrity;Manufacturing;Measurement;Decision making;Production facilities;Data models;manufacturing test;data quality;test data quality;cost of data quality},
doi={10.1109/AUTEST.2018.8532518},
ISSN={1558-4550},
month={Sep.},}
@INPROCEEDINGS{8769141,
author={Subbalakshmi, Sakineti and Prabhu, CSR},
booktitle={2018 International Conference on Computational Techniques, Electronics and Mechanical Systems (CTEMS)},
title={Protagonist of Big Data and Predictive Analytics using data analytics},
year={2018},
volume={},
number={},
pages={276-279},
abstract={Big Data has created as a fundamental locale of eagerness of study and research among experts and academicians. Movement in advancement is making it fiscally conceivable to store and examine gigantic proportions of information. Enormous Data consolidates a mix of composed, semi-sorted out and unstructured progressing information starting from combination of sources. Farsighted Analytics gives method in tapping learning from broad information files. Various visionary associations, for instance, Google, Amazon, etc have comprehended the ability of Big Data and Analytics in expanding upper hand. These techniques give a couple of chances like discovering precedents or better improvement figurings. Directing and breaking down. Big data similarly sets up couple of troubles - specifically estimate, quality, steadfast quality and satisfaction of data. This paper gives an expansive review of composing on Big Data and Predictive Analytics. It gives unpretentious components of urgent thoughts in this rising field. Finally, we have completed up with disclosures of our examination and structure future research orientation in this field.},
keywords={Big Data;Data mining;Organizations;Consumer electronics;Machine learning;Data analysis;Big Data;Big Data and Analytics;Data Science;Predictive Analytics;Advanced Analytics;Communal Broadcasting Analytics},
doi={10.1109/CTEMS.2018.8769141},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7207237,
author={Lai, Xin and Liu, Liu and Lai, Paul B.S. and Tsoi, Kelvin and Wang, Haitian and Chong, Ka Chun and Zee, Benny},
booktitle={2015 IEEE International Congress on Big Data},
title={Risk-Adjusted Monitoring Method for Surgical Data: Methodology for Data Analytics (Work in Progress)},
year={2015},
volume={},
number={},
pages={317-319},
abstract={Hospital Authority (HA) has launched a Surgical Outcome Monitoring and Improvement Program (SOMIP), which is used to audit the surgical performance of all public hospitals in Hong Kong. One of the most important information provided by the annual SOMIP report is the changes of 30-day mortality and identify whether and when there is a significant deterioration for each hospital. However, the routine monitoring method used such as Variable Life-adjusted Display (VLAD) and Cumulative Sum Charting (CUSUM) may not be able to detect the change of surgical performance efficiently. Expected improvement or deterioration in surgical outcome may not be exactly the same as the truth. In this paper, we develop a more effective risk-adjusted monitoring method to detect the change in surgical performance. By adapting this method to SOMIP, the proposed monitoring procedure is expected to not only benefit frontline surgeons, anaesthetists and intensivists when they decide on the operations for patients, but also help managers in HA to evaluate the surgical performance and further improve surgical quality.},
keywords={Surgery;Monitoring;Hospitals;Biomedical monitoring;Big data;Logistics;monitoring performance;Surgical big data;risk adjustment;EWMA},
doi={10.1109/BigDataCongress.2015.53},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7498367,
author={Sadiq, Shazia and Papotti, Paolo},
booktitle={2016 IEEE 32nd International Conference on Data Engineering (ICDE)},
title={Big data quality - whose problem is it?},
year={2016},
volume={},
number={},
pages={1446-1447},
abstract={The increased reliance on data driven enterprise has seen an unprecedented investment in big data initiatives. Organizations averaged US$8M in investments in big data-related initiatives and programs in 2014, with 70% of large enterprises and 56% of small and medium enterprises (SMEs) having already deployed, or planning to deploy, big-data projects [1]. As companies intensify their efforts to get value from big data, the growth in the amount of data being managed continues at an exponential rate, leaving organizations with a massive footprint of unexplored, unfamiliar datasets. On February 8th, 2015, a group of global thought leaders from the database research community outlined the grand challenges in getting value from big data [2]. The key message was the need to develop the capacity to `understand how the quality of data affects the quality of the insight we derive from it'.},
keywords={Big data;Cleaning;Computer science;Databases;Investment},
doi={10.1109/ICDE.2016.7498367},
ISSN={},
month={May},}
@INPROCEEDINGS{6755349,
author={Freitas, Patrícia Alves de and Reis, Everson Andrade dos and Michel, Wanderson Senra and Gronovicz, Mauro Edson and Rodrigues, Márcio Alexandre de Macedo},
booktitle={2013 IEEE 16th International Conference on Computational Science and Engineering},
title={Information Governance, Big Data and Data Quality},
year={2013},
volume={},
number={},
pages={1142-1143},
abstract={The value of information as a competitive differential has been taken into consideration in companies all over the world for some time already. In recent years, there has been heated debate about some terms originated from new concepts related to information, such as big data, due to the promise that such topic might revolutionise world trade. Hence, data and information governance and quality have been increasingly discussed in the business world.},
keywords={Companies;Information management;Data handling;Data storage systems;Computer architecture;Information Governance;Big Data;Data Quality},
doi={10.1109/CSE.2013.168},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8085513,
author={Li, Yuqian and Li, Peng and Zhu, Feng and Wang, Ruchuan},
booktitle={2017 12th International Conference on Computer Science and Education (ICCSE)},
title={Design of higher education quality monitoring and evaluation platform based on big data},
year={2017},
volume={},
number={},
pages={337-342},
abstract={Through the continuous collection and in-depth analysis of the quality monitoring data of colleges and universities, we combine the efficiency processing of big data and data evaluation, monitor the status of higher education normally, and construct a higher education quality monitoring and evaluation platform based on Spark. This platform is teaching centered with schools as its basis, including subsystems of data acquisition, data analysis, machine learning, data storage, data analysis and other areas. Through the application of the higher education quality monitoring platform, we can understand the current situation of the development of higher education scientifically, and provide the basis for the macro-decision of education administration department.},
keywords={Education;Monitoring;Big Data;Data mining;Servers;Memory;Indexes;big data;monitoring and evaluation;system design},
doi={10.1109/ICCSE.2017.8085513},
ISSN={2473-9464},
month={Aug},}
@INPROCEEDINGS{7383964,
author={Chen, Mao and Xugang, Zhong and Guansen, Wang and Jianxiao, Ma},
booktitle={2015 International Conference on Intelligent Transportation, Big Data and Smart City},
title={A Preliminary Discussion on the Application of Big Data in Urban Residents Travel Guidance},
year={2015},
volume={},
number={},
pages={47-50},
abstract={This paper discusses the application of big data in the residents travel guidance system. The system is based on traffic service information platform and tries to make the information collection and analysis system suitable for more types of data through applying the big data with characteristics of volume, variety, velocity. The application of big data processing technology in this system will form a complete transportation guidance system with the big data, geographic information system and the cloud storage. The big data processing technology will be used to simplify the program of residents travel guidance and raise the quality of urban traffic information.},
keywords={Transportation;Big data;Smart cities;Big data;Traffic service information;Route guidance;Intelligent parking},
doi={10.1109/ICITBS.2015.18},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7489625,
author={Garg, Anil and Bindal, Mukesh},
booktitle={2015 International Conference on Soft Computing Techniques and Implementations (ICSCTI)},
title={Enhancing QOS and QOE using big data in IPTV domain},
year={2015},
volume={},
number={},
pages={163-165},
abstract={Big Data analytics has brought remarkable difference in the way businesses are operating these days. IPTV service providers are also looking for ways to augment the value of their offerings by using the power of Big Data Analytics. For IPTV Service Providers, Enhancing Quality of Service (QoS) and Quality of Experience (QoE) are some of the crucial areas which always have significant scope of improvement. These can be directly related to higher customer satisfaction and subsequently to the revenues. Using Big Data Analytics, IPTV Service Providers can anticipate several risks and observe user behavior. This can help the Service Providers to take proactive/appropriate actions which leads to better QoS and QoE.},
keywords={Big data;IPTV;Quality of service;Data mining;Servers;Distributed databases;Motion pictures;Big Data Analytics;IPTV;QoS;QoE},
doi={10.1109/ICSCTI.2015.7489625},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9407223,
author={Gao, Hang},
booktitle={2020 International Conference on Big Data Economy and Information Management (BDEIM)},
title={Research on the Development of Retail E-commerce in China from the Perspective of Big Data},
year={2020},
volume={},
number={},
pages={87-90},
abstract={After more than 20 years of development, China's retail e-commerce industry shows an explosive growth trend. At the present stage, the over-speed growth of the retail e-commerce industry has gradually leveled off, with many platforms showing a trend of giant standing. The rapid rise of emerging technologies such as big data, artificial intelligence and cloud computing has brought new opportunities and challenges to the development of retail e-commerce, and further exposed the problems of retail e-commerce. In order to satisfy consumers' high-quality and high-demand experience in the future, this paper puts forward countermeasures to improve the development of retail e-commerce in China based on the analysis of big data.},
keywords={Industries;Economics;Cloud computing;Big Data;Market research;Explosives;Manufacturing;retail e-commerce;online retail;big data;network economy;economic development},
doi={10.1109/BDEIM52318.2020.00029},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8070831,
author={Jayalakshmi, G. and Anuradha, T.},
booktitle={2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC)},
title={Big Data Technologies for Predicting Epidemics and Enhancing the Quality of Human Life},
year={2017},
volume={},
number={},
pages={173-177},
abstract={With a lot of medical data coming from various sources, steer decisions can be made from the insights expand through big data by using various Machine Learning Algorithms. Traditionally, physicians use their knowledge while making treatment assessments, but in the last few years there has been a shift towards evidence-based medicine. This involves systematical appraisal of clinical data and making treatment decisions based on the finest accessible information. In this paper, we apply the predictive analysis technique in Hadoop/Map Reduce background to predict and classify the type of diabetes. And also focuses on how MapReduce is used, how map and reduce evaluations are adapted, implemented in various situations such as in medical field to produce medical reports by processing large medical data sets. This system provides capable way to care and cure the patients at low cost with improved outcomes like affordability and accessibility.},
keywords={Big Data;Organizations;Data mining;Distributed databases;Hospitals;medical;MapReduce;analytics;framework},
doi={10.1109/ICBDACI.2017.8070831},
ISSN={},
month={March},}
@INPROCEEDINGS{7841003,
author={Naeemi, Maitham D and Ren, Johnny and Hollcroft, Nathan and Alessio, Adam M and Roychowdhury, Sohini},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Application of big data analytics for automated estimation of CT image quality},
year={2016},
volume={},
number={},
pages={3422-3431},
abstract={With the increasing applications of Big Data analytics in medical image processing systems, there has been a growing need for quantitative medical image quality assessment techniques. Specifically for computed tomography (CT) images, quantitative image assessment can allow for benchmarking image processing methods and optimization of image acquisition parameters. In this work, large volumes of CT images from phantoms and patients are analyzed using 3 data models that vary in their implementation time complexities. The goal here is to identify the optimal method that scales across data set variabilities for predictive modeling of CT image quality (CTIQ). The first two models rely on spatial segmentation of regions-of-interest (ROIs) and estimate CTIQs in terms of segmented pixel variabilities. The third, convolutional neural network (CNN) model relies on error back-propagation from the training set of images to learn the regions indicative of CTIQ. We observe that for 70/30 data split, the average multi-class classification accuracies for CTIQ prediction using the 3 data models range from 73.6-100% and 50-100% for the phantom and patient CT images, respectively. Using variance of pixels within the segmented ROIs as a CTIQ classification parameter, the spatial segmentation data models are found to be more generalizable that the CNN model. However, the CNN model is found to be more suitable for CT image texture classification in the absence of structural variabilities. Our analysis demonstrates that spatial ROI segmentation data models are consistent CTIQ estimators while the CNN models are consistent identifiers of structural similarities for CT image data sets.},
keywords={Computed tomography;Data models;Image segmentation;Image quality;Big data;Lungs;Measurement;Region of interest;Convolutional neural network;Image variability;CT image},
doi={10.1109/BigData.2016.7841003},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7848472,
author={Yeo Chii, Yvonne and Xue, Feng and Low, Wen Wei and Yoon, Jung H. and Gold, Steve},
booktitle={2016 IEEE Region 10 Conference (TENCON)},
title={A big data approach for memory quality management},
year={2016},
volume={},
number={},
pages={2448-2452},
abstract={As memory technology scaling continues to advance to sub 20nm technology and memory capacity becomes higher, memory quality management becomes more challenging to achieve client's quality expectations especially in this new era of computing. Transformation of traditional quality management approaches becomes necessary to drive memory quality improvements. A big data analytics approach is presented in this paper to demonstrate its application on end to end quality management process to drive continuous memory quality improvements.},
keywords={Data analysis;Memory management;Manufacturing;Engines;Random access memory;Quality management;Data mining;Big data;Analytics;Memory Quality;Supplier Quality},
doi={10.1109/TENCON.2016.7848472},
ISSN={2159-3450},
month={Nov},}
@INPROCEEDINGS{9378487,
author={Rager, Jack and Liu, Fang Cherry},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Facilitating the HPC Data Center Host efficiency through Big Data Analytics},
year={2020},
volume={},
number={},
pages={3280-3287},
abstract={Quality of service is important feature for a High Performance Computing Center (HPC) center like Partnership for an Advanced Computing Environment (PACE) center in Georgia Institute of Technology (Georgia Tech). The user's job fails running on a HPC center may due to a spectral of reasons, one of major contributor is the hardware and network failure. Reducing the hardware failure rate can significantly increase a data center's quality of service as well as reducing the cost of human intervention. This is critical during PACE's transition to a fee-based service model in which uptime correlates directly with revenue. PACE has around 9 millions jobs each year with 12% of job failure rate. In order to extend service life of hardware and reduce the potential failure and data center's cost, we present a machine learning method to understand the center's host usage pattern. By clustering the hosts based on multiple features, we reshuffle the host list to avoid the hosts being overused over time. We build a test framework which runs the complex combination of experiments, and presents the ad hoc comparisons. We intend to make the machine learning method in a rack aware fashion, and show the meaningful result with rack information included.},
keywords={Machine learning algorithms;Torque;Clustering algorithms;Machine learning;Quality of service;Big Data;Hardware;High Performance Computing;Host Analysis;Unsupervised Machine Learning;Data Center},
doi={10.1109/BigData50022.2020.9378487},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8465132,
author={Segooa, Mmatshuene Anna and Kalema, Billy Mathias},
booktitle={2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)},
title={Improve Decision Making Towards Universities Performance Through Big Data Analytics},
year={2018},
volume={},
number={},
pages={1-5},
abstract={The technology Big Bang has seen organizations universities inclusive generating big volumes of data in various formats and at high speed than they used to do. Such data is referred to as Big Data. This voluminous data can be of great significance to organizations if better insights are drawn for management to improve decision making. However, to draw valued insight from Big Data, advanced forms of analytics need to be employed and such techniques are commonly known as Big Data analytics (BDA), This paper sough to report on analysis of factors influencing the leverage of BDA to improve performance in universities.},
keywords={Big Data;Organizations;Decision making;Standards organizations;Learning management systems;Market research;Big Data;Big Data Analytics;universities decision making;data quality},
doi={10.1109/ICABCD.2018.8465132},
ISSN={},
month={Aug},}
@ARTICLE{8038820,
author={Edstrom, Jonathon and Chen, Dongliang and Gong, Yifu and Wang, Jinhui and Gong, Na},
journal={IEEE Transactions on Big Data},
title={Data-Pattern Enabled Self-Recovery Low-Power Storage System for Big Video Data},
year={2019},
volume={5},
number={1},
pages={95-105},
abstract={The growing popularity of powerful mobile devices such as smart phones and tablet devices has resulted in the exponential growth of demand for video applications. However, due to the large video data size and intensive computation, mobile video applications require frequent embedded memory access, which consumes a large amount of power and limits battery life. In this paper, we present a low-cost self-recovery video storage system by investigating meaningful data patterns hidden in big video data, by introducing data mining techniques to the hardware design process. We propose a two-dimensional data-pattern approach to explore horizontal data-association and vertical data-correlation characteristics. Such data relationship discovery and pattern identification enable a new dimension for the hardware design space and bring self-recovery ability to memories in the presence of bitcell failures. Based on the identified optimal data patterns, we present a low-cost and efficient SRAM design to enable data self-recovery at low voltages. A 45nm 32 kb SRAM is implemented that delivers good video quality at near-threshold voltage (0.5 V) with negligible area overhead (7.94 percent).},
keywords={Random access memory;Hardware;Mobile communication;Big Data;Memory management;Data mining;Two dimensional displays;Videos;data mining;data pattern;low-power;self-recovery;on-chip memory},
doi={10.1109/TBDATA.2017.2750699},
ISSN={2332-7790},
month={March},}
@INPROCEEDINGS{7474375,
author={Ciancarini, Paolo and Poggi, Francesco and Russo, Daniel},
booktitle={2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Big Data Quality: A Roadmap for Open Data},
year={2016},
volume={},
number={},
pages={210-215},
abstract={Open Data (OD) is one of the most discussed issue of Big Data which raised the joint interest of public institutions, citizens and private companies since 2009. However, the massive amount of freely available data has not yet brought the expected effects: as of today, there is no application that has fully exploited the potential provided by large and distributed information sources in a non-trivial way, nor any service has substantially changed for the better the lives of people. The era of a new generation applications based on OD is far to come. In this context, we observe that OD quality is one of the major threats to achieving the goals of the OD movement. The starting point of this case study is the quality of the OD released by the five Constitutional offices of Italy. Our exploratory case study aims to assess the quality of such releases and the real implementations of OD. The outcome suggests the need of a drastic improvement in OD quality. Finally we highlight some key quality principles for OD, and propose a roadmap for further research.},
keywords={Big data;Metadata;Government;ISO Standards;Distributed databases;Open Data Quality;Information Modeling;E-Government;Big Data Knowledge Extraction},
doi={10.1109/BigDataService.2016.37},
ISSN={},
month={March},}
@INPROCEEDINGS{9115136,
author={Wentong, Zou},
booktitle={2020 Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)},
title={A Big Data Analysis Based System for the Comprehensive Evaluation of the Cultivation Quality of the Innovation and Entrepreneurship Skills among Chinese College Students},
year={2020},
volume={},
number={},
pages={392-394},
abstract={The big data analysis based model for tracking the cultivation quality of college students’ innovation and entrepreneurship skills relies on the matching and synergy between elements such as diversified participants, funding, technology and network design, and institutional systems to achieve its swift and efficient operation. The creation and integration of these elements would be the future focus of this project. Through diversified data collection from existing systems and other databases, we employ the indigenously developed data scheduling system (DataX) to carry out centralized scheduling of data extraction, thereby building a big data analysis based system for the comprehensive evaluation of the cultivation quality of innovation and entrepreneurship skills.},
keywords={Computers;Technological innovation;Databases;Image processing;Buildings;Entrepreneurship;Big Data;big data analysis;innovation and entrepreneurship skills among college students;evaluation system},
doi={10.1109/IPEC49694.2020.9115136},
ISSN={},
month={April},}
@INPROCEEDINGS{8047206,
author={Yanchun, Luo and Lin, Lu},
booktitle={2016 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={The Optimization Design of Pro-poor Tourism Information System in Sichuan Area with Introduction of Big Data Analysis},
year={2016},
volume={},
number={},
pages={482-485},
abstract={For any countries and regions, and all sectors of society, poverty has always been the generally concerned and long-term content. The devotion of the area has fully embodied the social common development and the humanities. Sichuan is an important tourism province. The development value space of tourism resource is extremely broad. The pro-poor tourism has a strong maneuverability. The distribution of tourism resources and the concentration distribution of poor population in Sichuan area are highly overlapped. The poverty-stricken area is the most potential area of tourist resources. The distribution of tourism resources is more concentrated, and the quality of resources is higher. Therefore, it is an effective way to combine the tourism development with the anti-poverty organically. For this purpose, the big data analysis is introduced into the optimization design of Sichuan area pro-poor tourism information system, and the guarantee system of pro-poor tourism is established creatively.},
keywords={Transportation;Big Data;Smart cities;Big data analysis;Sichuan area;Pro-poor tourism;System design},
doi={10.1109/ICITBS.2016.69},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9232648,
author={Hossen, Md Ismail and Goh, Michael and Hossen, Abid and Rahman, Md. Armanur},
booktitle={2020 11th IEEE Control and System Graduate Research Colloquium (ICSGRC)},
title={A Study on the Aspects of Quality of Big Data on Online Business and Recent Tools and Trends Towards Cleaning Dirty Data},
year={2020},
volume={},
number={},
pages={209-213},
abstract={The reliability, efficiency, and accuracy of e-business depend on the quality of data that is associated with a buyer, seller, brokers, e-business portals, admins, managers, decision-makers and so on. However, maintaining the quality of data in e-business is very challenging. It is because e-business data typically comes from different communication channels and sources. Integrating and managing the data quality of different sources is generally much troublesome than dealing with traditional business data. Even though there are several data cleaning methods and tools exist those methods and tools have some constraints. None of them directly working, particularly on e-business data that motivates to do research to highlight the aspects of big data quality related to e-business. Therefore, this research demonstrates the problems related to data quality related to online business, discusses the existing literature of data quality, the current tools and techniques that are being used for data quality and provides a research finding highlighting the weaknesses of current tools to address the problem of online business.},
keywords={Data integrity;Tools;Companies;Cleaning;Task analysis;Machine learning;Regulation;E-business;Big data;data quality;dirty data;machine learning},
doi={10.1109/ICSGRC49013.2020.9232648},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8622487,
author={Király, Péter and Büchler, Marco},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Measuring completeness as metadata quality metric in Europeana},
year={2018},
volume={},
number={},
pages={2711-2720},
abstract={Europeana, the European digital platform for cultural heritage, has a heterogeneous collection of metadata records ingested from more than 3200 data providers. The original nature and context of these records were different. In order to create effective services upon them we should know the strength and weakness or in other words the quality of these data. This paper proposes a method and an open source implementation to measure some structural features of these data, such as completeness, multilinguality, uniqueness, record patterns, to reveal quality issues.},
keywords={Metadata;Europe;Cultural differences;Measurement;Tools;Feature extraction;Big Data;Big data applications;Data analysis;Data collection;Quality of service;Quality management;Metadata;Data integration},
doi={10.1109/BigData.2018.8622487},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9378024,
author={Janusz, Andrzej and Hao, Guohua and Kałuża, Daniel and Li, Tony and Wojciechowski, Robert and Ślęzak, Dominik},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Predicting Escalations in Customer Support: Analysis of Data Mining Challenge Results},
year={2020},
volume={},
number={},
pages={5519-5526},
abstract={We summarize IEEE Big Data Cup: Predicting Escalations in Customer Support - a data mining competition organized jointly by companies Information Builders and QED Software at the KnowledgePit platform, in the frame of the 2020 IEEE International Conference on Big Data. We discuss the motivation for organizing this event and highlight the factors that make it such a challenging topic. We describe the data provided to participants and formulate the competition task. We also provide an overview of competition results with a detailed analysis of a few selected solutions. Finally, we present a novel functionality of the KnowledgePit platform - an analytic module that allows organizers to investigate selected solutions using a convenient GUI and provides in-depth insights about their quality.},
keywords={Training;Analytical models;Big Data;Data models;Software;Data mining;Task analysis;Data mining competitions;customer support analytics;NLP;feature engineering;visual data analysis},
doi={10.1109/BigData50022.2020.9378024},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7944952,
author={Xie, Chunli and Gao, Jerry and Tao, Chuanqi},
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Big Data Validation Case Study},
year={2017},
volume={},
number={},
pages={281-286},
abstract={With the advent of big data, data is being generated, collected, transformed, processed and analyzed at an unprecedented scale. Since data is created at a fast velocity and with a large variety, the quality of big data is far from perfect. Recent studies have shown that poor quality can bring serious erroneous data costs on the result of big data analysis. Data validation is an important process to recognize and improve data quality. In this paper, a case study that is relevant to big data quality is designed to study original big data quality, data quality dimension, data validation process and tools.},
keywords={Big Data;Tools;Databases;Electronic mail;Reliability;Temperature measurement;Null value;big data quality;big data validation;data checklist;big data tool;case study},
doi={10.1109/BigDataService.2017.44},
ISSN={},
month={April},}
@INPROCEEDINGS{8258380,
author={Fu, Qian and Easton, John M.},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Understanding data quality: Ensuring data quality by design in the rail industry},
year={2017},
volume={},
number={},
pages={3792-3799},
abstract={The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.},
keywords={Industries;Rails;Data models;Rail transportation;Systematics;Decision making;data quality;rail;quality by design;data quality schema},
doi={10.1109/BigData.2017.8258380},
ISSN={},
month={Dec},}
@ARTICLE{7273904,
author={Dang, Depeng and Liu, Ying and Zhang, Xiaoran and Huang, Shihang},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={A Crowdsourcing Worker Quality Evaluation Algorithm on MapReduce for Big Data Applications},
year={2016},
volume={27},
number={7},
pages={1879-1888},
abstract={Crowdsourcing is a new emerging distributed computing and business model on the backdrop of Internet blossoming. With the development of crowdsourcing systems, the data size of crowdsourcers, contractors and tasks grows rapidly. The worker quality evaluation based on big data analysis technology has become a critical challenge. This paper first proposes a general worker quality evaluation algorithm that is applied to any critical tasks such as tagging, matching, filtering, categorization and many other emerging applications, without wasting resources. Second, we realize the evaluation algorithm in the Hadoop platform using the MapReduce parallel programming model. Finally, to effectively verify the accuracy and the effectiveness of the algorithm in a wide variety of big data scenarios, we conduct a series of experiments. The experimental results demonstrate that the proposed algorithm is accurate and effective. It has high computing performance and horizontal scalability. And it is suitable for large-scale worker quality evaluations in a big data environment.},
keywords={Accuracy;Crowdsourcing;Algorithm design and analysis;Big data;Computational modeling;Quality control;Data models;crowdsourcing systems;quality control;Big data;MapReduce;Hadoop;Crowdsourcing systems;quality control;big data;mapreduce;hadoop},
doi={10.1109/TPDS.2015.2457924},
ISSN={1558-2183},
month={July},}
@INPROCEEDINGS{9006446,
author={Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Kahn, Michael G},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={An Interactive Data Quality Test Approach for Constraint Discovery and Fault Detection},
year={2019},
volume={},
number={},
pages={200-205},
abstract={Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.},
keywords={Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing feature maps;Inspection;Big Data;Data quality tests;Explainable learning;Interactive learning;Unsupervised learning},
doi={10.1109/BigData47090.2019.9006446},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8343025,
author={Cai, Hong-xia and Wei, Zhuang-yu},
booktitle={2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)},
title={Analysis of civil aircraft quality data under the support of big data},
year={2017},
volume={},
number={},
pages={766-770},
abstract={In the production assembly manufacturing process, a large amount of quality data has been generated by civil aircraft equipment system. With the passage of time and the accumulation of data, these massive data cannot be dealt with effectively using traditional statistical analysis of discrete manufacturing industry. To solve this problem, the method of quality data analysis for unsupervised learning presented in this paper was developed, after evaluating the generating characteristics of the civil aircraft quality data and the problems associated with the processing of the traditional quality data analysis. On this basis, in this paper, according to the disorder association, complex structure and large amount of data of the civil aircraft quality data, the data mining association analysis Apriori algorithm and the big data Splunk platform are introduced to effectively reduce the complexity of the quality data analysis through the complementary advantages of both, and put the data in an orderly, coherent state. The results show that the developed method is effective with high efficiency value.},
keywords={Aircraft;Aircraft manufacture;Production;Data mining;Big Data;Itemsets;quality oriented;aircraft;data analysis;Apriori algorithm;big data Splunk platform},
doi={10.1109/ICSESS.2017.8343025},
ISSN={2327-0594},
month={Nov},}
@INPROCEEDINGS{9150161,
author={Sun, Jian and Wang, Ting and Luo, Ming},
booktitle={2020 International Conference on Big Data and Informatization Education (ICBDIE)},
title={Research on the Construction and Innovation of Lifelong Education System Under the Background of Big Data},
year={2020},
volume={},
number={},
pages={30-33},
abstract={The development of information technology provides technical support for the construction of lifelong education think tanks. It is an urgent need for lifelong education development to create a new type of life think tanks in combination with regional development. The advent of the era of big data has profoundly changed the social structure, social relations, social production and lifestyle, and people's way of thinking. The entire education ecosystem has been reshaped, information and knowledge are constantly changing, and continuous learning has become the basic needs of society members. In addition, China is in a period of social transformation, the quality of the population needs to be improved, the issue of equity in education needs to be solved, and the vitality of social innovation needs to be activated. The unique advantages of big data will help solve the above problems. Therefore, building a learning society for the whole people, lifelong and comprehensive learning is not only an effective way to promote China's sustainable development, but also an inevitable choice in the era of big data.},
keywords={Big Data;Qualifications;Business;Learning systems;Standards;big data;lifelong education;system building},
doi={10.1109/ICBDIE50010.2020.00014},
ISSN={},
month={April},}
@INPROCEEDINGS{7841033,
author={Derbeko, Philip and Dolev, Shlomi and Gudes, Ehud and Ullman, Jeffrey D.},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Concise essence-preserving big data representation},
year={2016},
volume={},
number={},
pages={3662-3665},
abstract={Controversially, more data is not necessary better than less data. The explosion of the data lead to a number of interesting practical and theoretical problems. Among those problems are the need to filter, process, verify, index, distribute, protect and make redundant copies of the data. This data “massaging” usually take a lot of time and processing power. However, the quantity of the collected data does not necessary mean quality, as a lot of data is repetitive or does not contain any new information. Nevertheless, it still has to be processed, filtered, consumes high communication volume, has to be protected from breaches and from storage failures. In this position paper we propose to perform data reduction techniques on the collected (big) data prior to gathering of the data in a single location. In many cases (exemplified by two use-cases), especially in Internet-of-Things (IoT), those techniques might save tremendous amounts of power, processing time and network traffic.},
keywords={Big Data;Data Reduction;Big Data Analysis;Big Data Performance},
doi={10.1109/BigData.2016.7841033},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7051902,
author={Zillner, Sonja and Oberkampf, Heiner and Bretschneider, Claudia and Zaveri, Amrapali and Faix, Werner and Neururer, Sabrina},
booktitle={Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014)},
title={Towards a technology roadmap for big data applications in the healthcare domain},
year={2014},
volume={},
number={},
pages={291-296},
abstract={Big Data technologies can be used to improve the quality and efficiency of healthcare delivery. The highest impact of Big Data applications is expected when data from various healthcare areas, such as clinical, administrative, financial, or outcome data, can be integrated. However, as of today, the seamless access to the various healthcare data pools is only possible in a very constrained and limited manner. For enabling the seamless access several technical requirements, such as data digitalization, semantic annotation, data sharing, data privacy and security as well as data quality need to be addressed. In this paper, we introduce a detailed analysis of these technical requirements and show how the results of our analysis lead towards a technical roadmap for Big Data in the healthcare domain.},
keywords={Medical services;Big data;Semantics;Data privacy;Standards;Biomedical imaging;Security;Big Data;technical requirements;data digitalization;semantic annotation;data integration;data privacy and security;data quality},
doi={10.1109/IRI.2014.7051902},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7004459,
author={Wang, Xin and Sun, Ang and Kardes, Hakan and Agrawal, Siddharth and Chen, Lin and Borthwick, Andrew},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Probabilistic estimates of attribute statistics and match likelihood for people entity resolution},
year={2014},
volume={},
number={},
pages={92-99},
abstract={For big data practitioners, data integration/entity resolution/record linkage is one of the key challenges we face from day to day. Entity resolution/record linkage with high precision and recall on a large graph with billions of nodes, and hundreds of times more edges poses significant scalability challenges. Similarity based graph partition is still the most scalable method available. This paper presents a probabilistic method to approximate the match likelihood of a pair of records by incorporating values of different attributes and their aggregates/statistics. The quality of the approximates depend on the accuracy of the estimates of the aggregated values. The paper adapts the GTM model described in [1] to obtain the estimates. We present experimental results based on real world commercial data sources to show that the estimates obtained via GTM model is better than the baseline. Our experimental results also showed that the approximate match likelihood can improve the recall of the similarity function.},
keywords={Frequency estimation;Sociology;Cities and towns;Adaptation models;Couplings;Clustering algorithms;Big Data Demographic Information;Approximate Probabilistic Estimates;Record Linkage;Data Integration;Entity Resolution;Data Fusion},
doi={10.1109/BigData.2014.7004459},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8754071,
author={Ved, Mohit and B., Rizwanahmed},
booktitle={2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)},
title={Big Data Analytics in Telecommunication using State-of-the-art Big Data Framework in a Distributed Computing Environment: A Case Study},
year={2019},
volume={1},
number={},
pages={411-416},
abstract={Predictive Analytics is of great interest when it comes to enhancing Business Intelligence. Businesses have already started to use Big Data Analytics, particularly predictive and prescriptive analytics, to strengthen and increase their business yields. Not only has analytics resulted in business growth, but has also provided a significant competitive edge over others. The voluminous data generated from various resources is highly unstructured in nature and adding a structure to it would leverage the actual potential of the data. New techniques and frameworks should serve as human aids in automatically and intelligently analyzing large datasets in order to acquire useful information. In this paper, we attempt to perform Big Data Analytics on data from one of the most important and growing sources, namely, Telecommunication. To keep pace with the growing telecommunication market and ever increasing demands of the consumers for quality service, the telecom service providers are required to observe and estimate various trends in customer's usage to plan future upgrades and deployments driven by real data. We have attempted to use several data mining techniques to find hidden and interesting patterns from the telecom data generated by Telecoms Italia cellular network for the city of Milano, Italy. K-means clustering is used to categorize the usage statistics while several machine learning algorithms like Decision Tree, Random Forest, Logistic Regression and SVM are used for predicting the usage of telecom services. In the end, a performance comparison matrix is generated to rate the performance of these algorithms for the given dataset. All these experiments are performed on the big data environment set up at the supercomputing infrastructure of C-DAC. Given such a matrix, the result can be applied to similar dataset pertaining to other domains as well.},
keywords={Big Data;Telecommunications;Market research;Urban areas;Industries;Predictive Analytics, Telecom Analytics, Machine Learning, Big Data and Data Analytics},
doi={10.1109/COMPSAC.2019.00066},
ISSN={0730-3157},
month={Jul},}
@INPROCEEDINGS{8622589,
author={Deibe, David and Amor, Margarita and Doallo, Ramón},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Big data storage technologies: a case study for web-based LiDAR visualization},
year={2018},
volume={},
number={},
pages={3831-3840},
abstract={Big data technologies have been growing up quickly during past years. New storage and computing solutions appear while those already established in the market are improved with new features and better performance. Along with this growth also rises the number of applications and fields where the inclusion of big data technologies provides a large number of benefits, from the reduction in computational costs and economic resources to the improvement in the quality of the services provided which has a direct impact on the customers satisfaction. LiDAR (Light Detection and Ranging) data processing is one of the topics that could benefit from the adoption of these kind of technologies due to the massive datasets that are being gathered nowadays, with applications in archaeology, geography, geology or forestry, among many others. An efficient management of this volume of data becomes a key point especially in visualization, computing and analytic processes. In this paper, we analyse how web applications for the visualization of LiDAR data can benefit from the adoption of big data storage technologies, as well as the advantages and disadvantages that may determine the choice of one of them.},
keywords={Big Data;Data visualization;Laser radar;Servers;Software;Three-dimensional displays;Metadata;LiDAR;big data;storage technologies;web applications},
doi={10.1109/BigData.2018.8622589},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7747827,
author={Cheng, Shenghui and Wang, Bing and Zhong, Wen and Xie, Cong and Mahmood, Salman and Wang, Jun and Mueller, Klaus},
booktitle={2016 New York Scientific Data Summit (NYSDS)},
title={Model-driven visual analytics for big data},
year={2016},
volume={},
number={},
pages={1-2},
abstract={The growth of digital data is tremendous. Any aspect of life and matter is being recorded and stored on cheap disks, either in the cloud, in businesses, or in research labs. We can now afford to explore very complex relationships with many variables playing a part. But for this we need powerful tools that allow us to be creative, to sculpt this intricate insight formulated as models from the raw block of data. High-quality visual feedback plays a decisive role here. The subject of this poster is a framework we have developed over the years to make the exploration of large multivariate data more intuitive and direct. The components of this framework were conceived in tight collaborations with domain experts in the fields of climate science, health informatics, computer systems, and others.},
keywords={Correlation;Context;Visualization;Data visualization;Layout;Data models;visualization;high-dimensional data;data science},
doi={10.1109/NYSDS.2016.7747827},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8916246,
author={Shen, Ziyu and Zhang, Xusheng and Liu, Binghui and Xia, Bin and Liu, Zheng and Li, Yun and Long, Saiqin},
booktitle={2019 Seventh International Conference on Advanced Cloud and Big Data (CBD)},
title={PCP-2LSTM: Two Stacked LSTM-Based Prediction Model for Power Consumption in Data Centers},
year={2019},
volume={},
number={},
pages={13-18},
abstract={As the size of data centers and cloud computing continue to expand, power consumption in data centers is rapidly increasing. It has a great significance to predict and analyze power consumption in the data center because power consumption prediction can help data center operators perform workflow scheduling, manage energy efficiency, provide high quality-of-service (QoS), and meet the requirements of green energy use. The current methods are mainly divided into two scopes: the one is establishing a static relationship between power consumption and relevant components/applications, and the other one is treating power consumption as sequential temporal data. However, the first scope does not consider the dynamic fluctuation of power, and the other one ignores the characteristics of the power consumption data. To solve these issues, in this paper, we present a power consumption prediction framework called PCP-2LSTM based on the mean smoothing and long short-term memory (LSTM) network. We first build a power consumption system to collect data and analyze the stationary of the power series. Then we use the mean smoothing to remove the noise from the time series of power consumption. After data preprocessing, because the time for workflow and container scheduling is usually 30 seconds, we use a stacked LSTM model to predict 30s power consumption in the future. The experimental result indicates that our approach outperforms other baselines.},
keywords={Power demand;Data centers;Predictive models;Time series analysis;Data models;Servers;Correlation;Data center, time series prediction, energy efficiency, power consumption},
doi={10.1109/CBD.2019.00013},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9196257,
author={Ye, Zijun},
booktitle={2020 International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)},
title={Identification of the Residential Areas for Urban Renewal Based on Big Data : Take Guangzhou as an example},
year={2020},
volume={},
number={},
pages={36-39},
abstract={Urban renewal is an important direction for urban development in China. Affected by urban diseases and lack of land resources, megacities urgently need to carry out urban renewal. The residential areas in Guangzhou also face problems such as low efficiency, poor quality, and insufficient vitality. This paper uses big data to identify the urban residential areas that is in urgent need of transformation and provides a reference for the planning and decision-making of Guangzhou urban renewal.},
keywords={Conferences;Big Data;Artificial intelligence;Internet of Things;Planning;Big data;urban renewal;residential area;identification},
doi={10.1109/ICBAIE49996.2020.00014},
ISSN={},
month={June},}
@INPROCEEDINGS{8622335,
author={Tang, Yujian and Tasnim, Samia and Pissinou, Niki and Iyengar, S.S. and Shahid, Abdur},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Reputation-Aware Data Fusion and Malicious Participant Detection in Mobile Crowdsensing},
year={2018},
volume={},
number={},
pages={4820-4828},
abstract={Mobile crowdsensing, an emerging sensing paradigm, promotes scalability and reduction in the deployment of specialized sensing devices for large-scale data collection in a decentralized fashion. However, its open structure allows malicious entities to interrupt a system by reporting fabricated or erroneous data, making trust evaluation a highly important issue in mobile crowdsensing applications. The goal of this research is to show that an introduction of a reputation system in the process of correlated sensor-based data fusion will enhance the overall quality of the sensed data. To do so, we design a reputation-aware data fusion mechanism to ensure data integrity. We use Gompertz function in our reputation method to rate the trustworthiness of the data reported by a crowdsensing participant. The proposed mechanism, on one hand, is capable of defending a data corruption attack and identifying malicious or honest participants based on their reported data in real time. On the other hand, this mechanism yields more accurate data prediction in terms of lower data prediction error. We conducted experiments using two different real-world datasets. We compare our correlated data and reputation-aware data prediction (CDR) method with other popular methods, and the results show that our effective method incurs lower data prediction error.},
keywords={Sensors;Data integration;Data integrity;Big Data;Real-time systems;Air quality;Monitoring;Big data analytics;anomaly detection;data fusion;mobile crowdsensing;Spatial-temporal data analysis},
doi={10.1109/BigData.2018.8622335},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9458106,
author={Meli, Matthew and Gatt, Edward and Casha, Owen and Grech, Ivan and Micallef, Joseph},
booktitle={2020 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={A Low Cost LoRa-based IoT Big Data Capture and Analysis System for Indoor Air Quality Monitoring},
year={2020},
volume={},
number={},
pages={376-381},
abstract={This paper presents a low cost LoRa-based IoT big data capture and analysis system for indoor air quality monitoring. This system is presented as an alternative solution to expensive and bulky indoor air quality monitors. It enables multiple low cost nodes to be distributed within a building such that extensive location-based indoor air quality data is generated. This data is captured by a gateway and forwarded to a cloud-based LoRaWAN network which in turn publishes the received data via MQTT. A cloud-based data forwarding server is used to capture, format and store this big data on a cloud-based document-oriented database. Cloud-based services are used for data visualization and analysis. Periodic indoor air quality graphs along with air quality index and thermal comfort index heat maps are generated.},
keywords={Temperature measurement;Temperature sensors;Cloud computing;Data visualization;Big Data;Logic gates;Particle measurements;IoT;LPWAN;Indoor Air Quality;Big Data;LoRa},
doi={10.1109/CSCI51800.2020.00070},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7508117,
author={Chauhan, Ritu and Jangade, Rajesh},
booktitle={2016 6th International Conference - Cloud System and Big Data Engineering (Confluence)},
title={A robust model for big healthcare data analytics},
year={2016},
volume={},
number={},
pages={221-225},
abstract={The big data technologies are offering varied challenges for research and scientists in healthcare application domain to improve the quality of life in patients. Yet in this phase of technology explosion, retrieving effective and efficient information from big data is a value care for future medical diagnostic. However. unprecedented growth in data analytical technology has proven fruitful for discovery of hidden patterns from such databases. The purpose of this study is to discuss current developments in big data analytics with healthcare application domain. However, paper explains the emerging role of predictive data analytics with focused study on patient's quality care with several states of examples. Further, comprehensive suggestive novel framework is discussed with the approach to offer significant benefits to computing technology for effective patient care diagnosis.},
keywords={Decision support systems;Big data;Hospitals;Conferences;Data mining;Satellites;Mobile communication;Big Data;Predictive Data Analytic;Data Mining;Healthcare Databases},
doi={10.1109/CONFLUENCE.2016.7508117},
ISSN={},
month={Jan},}
@INPROCEEDINGS{7979931,
author={Wen, Hongsheng and Chen, Zhiqiang and Gu, Jianping and Zhu, Qiangqiang},
booktitle={2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
title={Big Data Analysis on Radiographic Image Quality},
year={2016},
volume={},
number={},
pages={341-346},
abstract={Mass data generated from in-service radiographic product contain assignable information on Image Quality (IQ). Analyzing data from routine work might supplement the time-consuming Image Quality Assurance Test Procedure (IQATP) to evaluate IQ and to know product type performance on site, which can also locate risks and give manufacturer directions for the further actions as well. This article illustrates methodologies of extracting IQ information from mass data and visual quality track, analysis, control, and risk mitigation in Big Data environments.},
keywords={Detectors;Image edge detection;Radiography;Standards;Image quality;X-ray imaging;Indexes;image quality;in-service;radiographic product;routine data;quality control},
doi={10.1109/CCBD.2016.073},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9378153,
author={Uygun, Yasin and Oguz, Ramazan Faruk and Olmezogullari, Erdi and Aktas, Mehmet S.},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={On the Large-scale Graph Data Processing for User Interface Testing in Big Data Science Projects},
year={2020},
volume={},
number={},
pages={2049-2056},
abstract={In functional User Interface testing, test scenarios are written with respect to the requirements that are specified by test analysts. Usually, a test analyst focuses on base URLs and HTML components while collecting requirements of User Interface test scenarios. A base URL is essentially a unit segment of large scale graph data. It has mostly dynamic shape and is used to navigate pages amongst application's pages. We argue that even though dynamic URLs have additional important information about the content of the page, they are not being utilized in generating User Interface test scenarios. In this study, we address this lack of capability and focus on the development of a methodology that can support the usage of large-scale dynamic URL datasets in UI test script generation. Our proposed methodology is designed as an add-on tool that can be used on the top of the existing UI test automation tools to improve testing quality. We introduce a higher quality testing methodology to make the results more accurate, and we discuss the proposed methodology and give an overview of the implementation details followed by the evaluation results. We perform various performance evaluations to investigate how well the proposed algorithms scale under increasing data sizes. The results are promising and show the usability of the proposed methodology.},
keywords={Uniform resource locators;Shape;User interfaces;Big Data;Tools;Usability;Testing;Big Data Science Projects;Large-scale Data Processing;Dynamic URLs;NLP;Clustering;HDFS;Map Reduce;Word2vec;Software Testing},
doi={10.1109/BigData50022.2020.9378153},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7951899,
author={Barril, José Farnesio Huesca and Qing Tan},
booktitle={2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Integrating privacy in architecture design of student information system for big data analytics},
year={2017},
volume={},
number={},
pages={139-144},
abstract={Educational Data Mining (EDM) is an area of growing interest in academia with significant challenges and tremendous opportunities. Most EDM initiatives are based on finding patterns to aid and enhance student learning and performance, while others focus on program efficiency, service improvement, and college readiness. This paper is related to a case study being conducted at Sta. Teresa School, a high school in the Northern District of a West African country looking to improve service quality. By combining its Relational Database School Management System data sets with its anticipated online community forum, and aggregating it with Social media data, the school is expected to gain new actionable insights to enhance student services. In this paper, we present a model for incorporating privacy into big data analytics architecture integration with Social media, discuss some of the school's concern related to privacy and security, and offer some delivery options for its online community forum initiative.},
keywords={Authorization;Cryptography;Privacy;Business;educational data mining;big data analytics;data security and privacy;opinion mining;online community data mining;digital education;big data analytics architecture;cloud},
doi={10.1109/ICCCBDA.2017.7951899},
ISSN={},
month={April},}
@INPROCEEDINGS{9095695,
author={Chen, Yuepeng and Fu, Qingwen and Zhu, Jiahui},
booktitle={2020 IEEE 5th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA)},
title={Finding Next High-Quality Passenger Based on Spatio-Temporal Big Data},
year={2020},
volume={},
number={},
pages={447-452},
abstract={Finding high-quality passenger can provide timely recommendations for taxi drivers, thus decreasing the waiting time of passengers and increasing the cab driver's efficiency. This paper proposes a high-quality passenger recommendation model which combines the value evaluation formula of non-occupied status and clustering. Firstly, the GPS big data of taxi is preprocessed to get the trajectory data by Map Reduce, which is simple, clean and labeled with status. Then, every trajectory pick-up point is extracted, and the profits of pick-up point set is calculated by using the formula considering non-occupied status. At the same time, the density based clustering method DBSCAN clustering is used for different value of pickup points. Finally, the top high probability passenger area with high value is extracted as the recommended result. In this paper, the real 10357 taxis equipping GPS in Beijing collected the big trajectory data as the experimental dataset, using our method to calculate and recommend, the results show that it can accurately predict the high-quality passenger area, further to significantly improve the income of taxi drivers and reduce the waiting time of passengers.},
keywords={Public transportation;Trajectory;Feature extraction;Big Data;Data mining;Global Positioning System;Clustering methods;big data;GPS trajectory;DBScan;finding passenger},
doi={10.1109/ICCCBDA49378.2020.9095695},
ISSN={},
month={April},}
@INPROCEEDINGS{9434482,
author={Lixin, Yao and Jiaxun, You and Wenbin, Wang},
booktitle={2020 International Conference on Big Data and Social Sciences (ICBDSS)},
title={Research and Application on the Governance of Passenger Car Product Data Resources},
year={2020},
volume={},
number={},
pages={46-49},
abstract={With the development of digital economy and big data technology, data resources owned by enterprises have become one of the important production factors in the era of digital economy. Via data empowerment, enterprises analyze the intrinsic value of data, and then realize their own business transformation and innovation and development. After more than ten years of rapid development, China is now the world's largest automobile production country. Behind the huge automobile market, there are a lot of data resources related to the market and products. However, due to the problems of scattered data sources, inconsistent statistical caliber and untimely updating, it brings great inconvenience to the researchers engaged in automobile industry. This paper proposes a set of passenger car product data governance framework, which aims to provide high-quality passenger car product database for enterprises in the process of studying the market and technology of passenger car products, mining the value of data assets, and assisting managers in decision-making.},
keywords={Industries;Technological innovation;Databases;Social sciences;Decision making;Production;Big Data;Passenger car;Data Governance;Metadata;Master Data},
doi={10.1109/ICBDSS51270.2020.00018},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8393094,
author={Wang, Zhenhai and Xu, Bo},
booktitle={2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
title={Design and implementation of the water environmental quality monitoring system based on big data},
year={2017},
volume={},
number={},
pages={2092-2096},
abstract={To realize the comprehensive scientific decision-making of ecological environment, a hybrid storage structure based on a combination of big data technology and traditional database is designed. First, HBase has high expansibility and reliability to the data storage capacity in big environmental monitoring data. Second, relational database MySql provides powerful SQL query language and data analysis capabilities, which can provide a real time statistical analysis and multi dimension display on recent environmental quality monitoring data. Given method can effectively improve the data storage size and query speed, compared with the traditional method.},
keywords={Monitoring;Indexes;Big Data;Memory;Cloud computing;Relational databases;Rivers;Environmental big data;Hybrid storage;Data import},
doi={10.1109/FSKD.2017.8393094},
ISSN={},
month={July},}
@INPROCEEDINGS{9258802,
author={Wang, Ya and Yang, Yanmei},
booktitle={2020 International Conference on Communications, Information System and Computer Engineering (CISCE)},
title={Research on Higher Vocational Teaching Quality Improvement Based on Educational Big Data},
year={2020},
volume={},
number={},
pages={227-230},
abstract={The vigorous development of educational big data already has great potential to promote educational reform, which provides conditions and opportunities for the improvement of teaching quality. Education big data has the characteristics of real-time, multi-dimensionality, authenticity and so on. This paper analyzes the current problems in the teaching quality of higher vocational education, and with the continuous improvement of data mining and learning analysis technology, puts forward the path of educational big data to promote the quality of teaching.},
keywords={Education;Handheld computers;Big Data;Decision making;Voltage control;Uniform resource locators;Tools;education;big data;higher vocational education;teaching},
doi={10.1109/CISCE50729.2020.00051},
ISSN={},
month={July},}
@INPROCEEDINGS{7840906,
author={Angryk, Rafal A. and Galarus, Douglas E.},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={The SMART approach to comprehensive quality assessment of site-based spatial-temporal data},
year={2016},
volume={},
number={},
pages={2636-2645},
abstract={There is a need for comprehensive solutions to address the challenges of spatio-temporal data quality assessment. Emphasis is often placed on the quality assessment of individual observations from sensors but not on the sensors themselves nor upon site metadata such as location and timestamps. The focus of this paper is on the development and evaluation of a representative and comprehensive, interpolation-based methodology for assessment of spatio-temporal data quality. We call our method the SMART method, short for Simple Mappings for the Approximation and Regression of Time series. When applied to a real-world, meteorological data set, we show that our method outperforms standard interpolators and we identify numerous problematic sites that otherwise would not have been flagged as bad. We further identify sites for which metadata is incorrect. We believe that there are many problems with real data sets like these and, in the absence of an approach like ours, these problems have largely gone unidentified. Our results bring into question the validity of provider-based quality control indicators. In addition to providing a comprehensive solution, our approach is novel for the simple but effective way that it accounts for spatial and temporal variation.},
keywords={Sensors;Quality control;Metadata;Meteorology;Interpolation;Quality assessment;Time series analysis;data quality;data stream processing;spatial-temporal data;quality control;interpolation},
doi={10.1109/BigData.2016.7840906},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7100317,
author={Singh, Saravjeet and Singh, Jaiteg},
booktitle={2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)},
title={SSMDM: An approach of big data for semantically Master Data Management},
year={2015},
volume={},
number={},
pages={586-590},
abstract={Master data is critical for any business organization. Big organizations like Oracle, Infosys, IBM, Google, Facebook and TCS started working on Master Data Management (MDM) in early 20's. Multinational corporations spend millions of dollars for Managing their Master Data, so as to ensure quality of service and customer retention as well. Unlike big organizations, Small and Midsized Enterprises (SME's), because of their limited resources, are unable to exploit the economies of scale associated with master data management. In this paper a Synthetic Semantic Master Data Modeler (SSMDM) has been proposed, this modeler primarily uses the concept of Google's knowledge graph to identify semantics within data sets. Using SSMDM, synthetic yet realistic master data was generated to find out probable ontologies within synthetic data sets. Based on these ontologies, some rules were framed to produce synthetic facts. These synthetic facts were further used to decide services and cuisines to be offered at a newly opened eating joint. Since inception, we keep on collecting actual customer data. Once sufficient data was available we statistically analyzed the facts originated from actual data with those created synthetically. The results were promising and justify the use of SSMDM for the purpose of policy making in SMEs.},
keywords={Big data;Semantics;Ontologies;Organizations;Google;Facebook;Data models;Master data management;ontology;semantics;SME;SSMD},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{8121809,
author={Liu, He and Chen, Jiangqi and Huang, Fupeng and Li, Han},
booktitle={2017 14th International Symposium on Pervasive Systems, Algorithms and Networks & 2017 11th International Conference on Frontier of Computer Science and Technology & 2017 Third International Symposium of Creative Computing (ISPAN-FCST-ISCC)},
title={An Electric Power Sensor Data Oriented Data Cleaning Solution},
year={2017},
volume={},
number={},
pages={430-435},
abstract={With the development of Smart Grid Technology, more and more electric power sensor data are utilized in various electric power systems. To guarantee the effectiveness of such systems, it is necessary to ensure the quality of electric power sensor data, especially when the scale of electric power sensor data is large. In the field of large-scale electric power sensor data cleaning, the computational efficiency and accuracy of data cleaning are two vital requirements. In order to satisfy these requirements, this paper presents an electric power sensor data oriented data cleaning solution, which is composed of a data cleaning framework and a data cleaning method. Based on Hadoop, the given framework is able to support large-scale electric power sensor data acquisition, storage and processing. Meanwhile, the proposed method which achieves outlier detection and reparation is implemented on the basis of a time-relevant k-means clustering algorithm in Spark. The feasibility and effectiveness of the proposed method is evaluated on a data set which originates from charging piles. Experimental results show that the proposed data cleaning method is able to improve the data quality of electric power sensor data by finding and repairing most outliers. For large-scale electric power sensor data, the proposed data cleaning method has high parallel performance and strong scalability.},
keywords={Power systems;Cleaning;Clustering algorithms;Big Data;Sparks;Data acquisition;Algorithm design and analysis;electric power senser data;data cleaning;k-means clustering;outlier;Spark},
doi={10.1109/ISPAN-FCST-ISCC.2017.29},
ISSN={2375-527X},
month={June},}
@ARTICLE{6949519,
author={O'Leary, Daniel E.},
journal={IEEE Intelligent Systems},
title={Embedding AI and Crowdsourcing in the Big Data Lake},
year={2014},
volume={29},
number={5},
pages={70-73},
abstract={Daniel E. O'Leary examines the notion of the Big Data Lake and contrasts it with decision support-based data warehouses. In addition, some of the risks of the emerging Lake concept that ultimately require data governance are analyzed. O'Leary investigates using different AI and crowdsourcing (human intelligence) applications in that lake in order to integrate disparate data sources, facilitate master data management and analyze data quality. Although data governance often is not seen as a technology issue, it is seen as a critical component of making the Big Data Lake "work".},
keywords={Crowdsourcing;Artificial intelligence;Big data;Data warehouses;Decision support systems;Databases;Business;Big Data Lake;data warehouses;artificial intelligence;crowdsourcing;data governance;master data management;intelligent systems},
doi={10.1109/MIS.2014.82},
ISSN={1941-1294},
month={Sep.},}
@ARTICLE{8371209,
author={Xu, Chenhan and Wang, Kun and Sun, Yanfei and Guo, Song and Zomaya, Albert Y.},
journal={IEEE Transactions on Network Science and Engineering},
title={Redundancy Avoidance for Big Data in Data Centers: A Conventional Neural Network Approach},
year={2020},
volume={7},
number={1},
pages={104-114},
abstract={As the innovative data collection technologies are applying to every aspect of our society, the data volume is skyrocketing. Such phenomenon poses tremendous challenges to data centers with respect to enabling storage. In this paper, a hybrid-stream big data analytics model is proposed to perform multimedia big data analysis. This model contains four procedures, i.e., data pre-processing, data classification, data recognition and data load reduction. Specifically, an innovative multi-dimensional Convolution Neural Network (CNN) is proposed to assess the importance of each video frame. Thus, those unimportant frames can be dropped by a reliable decision-making algorithm. In order to ensure video quality, minimal correlation and minimal redundancy (MCMR) are combined to optimize the decision-making algorithm. Simulation results show that the amount of processed video is significantly reduced, and the quality of video is preserved due to the addition of MCMR. The simulation also proves that the proposed model performs steadily and is robust enough to scale up to accommodate the big data crush in data centers.},
keywords={Streaming media;Big Data;Data centers;Redundancy;Multimedia communication;Data models;Analytical models;Data centers;redundancy avoidance;multimedia;storage;big data;convolution neural network},
doi={10.1109/TNSE.2018.2843326},
ISSN={2327-4697},
month={Jan},}
@INPROCEEDINGS{8085517,
author={Lingfeng, Zhang and Feng, Feng and Heng, Huang},
booktitle={2017 12th International Conference on Computer Science and Education (ICCSE)},
title={Wine quality identification based on data mining research},
year={2017},
volume={},
number={},
pages={358-361},
abstract={For the quality of the wine big data identification technology, the introduction of data mining classification algorithm, effectively according to the content of several impact compounds in wine level identification;Are introduced including the Logistic regression and BP neural network and SVM classification algorithm, in view of the three algorithms identify the modeling analysis of wine quality. Data mining is closely related to big data, applying data mining to the wine in the quality detection of big data, can quickly to the quality of the wine.},
keywords={Logistics;Data models;Classification algorithms;Data mining;Algorithm design and analysis;Support vector machines;Analytical models;big data;data mining;quality identification},
doi={10.1109/ICCSE.2017.8085517},
ISSN={2473-9464},
month={Aug},}
@INPROCEEDINGS{7811391,
author={Noorwali, Ibtehal and Arruda, Darlan and Madhavji, Nazim H.},
booktitle={2016 IEEE/ACM 2nd International Workshop on Big Data Software Engineering (BIGDSE)},
title={Understanding Quality Requirements in the Context of Big Data Systems},
year={2016},
volume={},
number={},
pages={76-79},
abstract={While the domain of big data is anticipated to affect many aspects of human endeavour, there are numerous challenges in building big data applications among which is how to address big data characteristics in quality requirements. In this paper, we propose a novel, unified, approach for specifying big data characteristics (e.g., velocity of data arrival) in quality requirements (i.e., those requirements specifying attributes such as performance, reliability, availability, security, etc.). Several examples are given to illustrate the integrated specifications. As this is early work, further experimentation is needed in different big data situations and quality requirements and, beyond that, in a variety of project settings.},
keywords={Big data;Security;Context;Requirements engineering;Data privacy;Reliability;Software;Quality requirements; big data; specification;requirements engineering;software engineering},
doi={10.1145/2896825.2896838},
ISSN={},
month={May},}
@INPROCEEDINGS{9006190,
author={Hillman, Velislava and Ganesh, Varunram},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Kratos: A secure, authenticated and publicly verifiable system for educational data using the blockchain},
year={2019},
volume={},
number={},
pages={5754-5762},
abstract={Growing interest in educational data mining (EDM) and learning analytics (LA) to leverage big data and to benefit education and the science of learning has made data ownership an important focus point for institutions and students. While EDM and LA can provide important information that help enhance the quality of teaching and learning, it has become critical to ensure data privacy and student agency over data. In this paper, we introduce Kratos: an immutable and publicly verifiable data management system that enables EDM and LA, while maintaining data privacy and empowering students with a user interface for data governance and participation in school processes. The system aims to achieve data interoperability, which facilitates EDM and LA as incentives to educational stakeholders (policy makers, educators, developers of education technologies, etc.), while prioritizing student agency over their data. Our system gives students and schools an immutable log along with comprehensive access to data that is otherwise scattered across systems and vendors. The underlying set of rules of the system are defined in a set of smart contracts, codified from existing non-virtual agreements [1] between schools and education technology (edutech) vendors. We propose the smart contracts to be deployed on a public blockchain (like Ethereum or Bitcoin), for notarizing and time-stamping various interactions which users of Kratos may have with data. Third parties requesting access to school data have a unique virtual token assigned to them on the blockchain which helps keep track of data modifications, access and use.},
keywords={Standards;Education;Contracts;Data privacy;Interoperability;Stakeholders;educational data mining;learning analytics;data privacy;blockchain;distributed data management systems},
doi={10.1109/BigData47090.2019.9006190},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7518308,
author={Tesfagiorgish, Dawit G. and JunYi, Li},
booktitle={2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)},
title={Big Data Transformation Testing Based on Data Reverse Engineering},
year={2015},
volume={},
number={},
pages={649-652},
abstract={During the transformation of huge volume of data, there might exist data mismatch, miscalculation and/or loss of useful data that leads to an unsuccessful data transformation. To check out the occurrence of such possible errors, testing is a crucial requirement. The existing quality testing methods are either unreliable, return biased results, fail to provide answers for data differences or have several limitations which does not treat each and every part of the data into the process. We propose an approach of big data transformation testing based on the concept of data reverse engineering. It is a comprehensive approach that reverse the whole transformation process and does a comparison testing on each and every entry of the data if the original source data can be constructed back from the target data, once successful ETL process is done.},
keywords={Testing;Databases;Big data;Reverse engineering;Data mining;Companies;Data reverse engineering;data transformation;ETL;Big data testing},
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.129},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8405689,
author={Ebrahimi, Mahdi and Mohan, Aravind and Lu, Shiyong},
booktitle={2018 IEEE Fourth International Conference on Big Data Computing Service and Applications (BigDataService)},
title={Scheduling Big Data Workflows in the Cloud under Deadline Constraints},
year={2018},
volume={},
number={},
pages={33-40},
abstract={With the advent of cloud computing, an unbound number of compute resources can be leased from the cloud providers. In such an environment, the number of assigned resources to a workflow can be elastically scaled in and out on a demand basis using the added Quality of Service (QoS) constraints such as the budget and the deadline. The heterogeneous nature of the cloud resources makes the decision of selecting resource type for each workflow a challenging problem. Although there are several existing research studies that propose both static and dynamic scheduling algorithms for both homogeneous and heterogeneous cloud resource types, they do not take advantage of the data dependency information that is part of the workflow structure during the scheduling process. There is still room for improvement, since the scheduling problem is an NP-hard problem. In this paper we propose a new Big data wOrkflow scheduleR undeR deadlIne conStraint (BORRIS) that is used to minimize the execution cost of the workflow under a provided deadline constraint in a heterogeneous cloud computing environment. We have implemented the proposed algorithm in our big data workflow system called DATAVIEW and the experimental results show the competitive advantage of our approach.},
keywords={Task analysis;Cloud computing;Big Data;Data communication;Computational modeling;Cost function;Processor scheduling;big data workflows;big data;scheduling;BORRIS},
doi={10.1109/BigDataService.2018.00014},
ISSN={},
month={March},}
@INPROCEEDINGS{8102206,
author={Alqarni, Mohammed A.},
booktitle={2017 14th International Conference on Smart Cities: Improving Quality of Life Using ICT & IoT (HONET-ICT)},
title={Benefits of SDN for Big data applications},
year={2017},
volume={},
number={},
pages={74-77},
abstract={Big data applications depend on underlying networks that make the transfer of information possible. These networks may be real (conventional) or virtual (in case of services hosted in data centers). Either way, the responsibility of smooth execution of the application, despite increasing traffic volume, lies with the service provider. The service providers face many challenges with respect to providing a high quality of service. It is therefore in the best interest of the service providers that efficiency of the applications is increased. SDN has the potential to improve big data application performance. In this paper we have a look at the recent advancements in technology that helps improve big data applications using SDN and discuss our observations.},
keywords={Big Data applications;Optimization;Servers;Protocols;Multimedia communication;SDN;Network Virtualization;Software Defined Networks;Big data},
doi={10.1109/HONET.2017.8102206},
ISSN={1949-4106},
month={Oct},}
@INPROCEEDINGS{8997699,
author={Pan, Lingling and Liu, Jun and Li, Feng},
booktitle={2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)},
title={Multi-dimensional Index Construction of Electric Power Multi-source Measurement Data considering Spatio-temporal Correlation},
year={2019},
volume={1},
number={},
pages={2386-2390},
abstract={The operation of complex AC/DC power grid changes rapidly and dynamically, which objectively puts forward higher requirements for on-line analysis, and it is urgent to improve the basic data quality of power grid. Because of low quality and poor synchronization of the basic data of power grid, it is impossible to accurately map the actual operation of the power grid. At the same time, the cross-system data matching degree is low and the data correlation is poor, so it can not support the multi-scale data analysis for all kinds of applications. In this paper, the associated method of multi-source heterogeneous data in the power grid is studied. Combined with big data's access characteristics, big data storage, big data retrieval and artificial intelligence technology, the high-speed data storage and index architecture of power big data are constructed, and a multi-dimensional index reflecting the associated relationship of operating data is established from the dimensions of time, space, application, device and so on. It is easier to analyze multi-source data, to improve the basic data quality of power grid, which provides effective support for accurate data analysis and evaluation of power grid.},
keywords={Big Data;Power grids;Indexes;Power measurement;Time measurement;Memory;power big data;multi-source heterogeneous data;spatio-temporal correlation;data storage;multi-dimensional index},
doi={10.1109/IAEAC47372.2019.8997699},
ISSN={2381-0947},
month={Dec},}
@INPROCEEDINGS{7756075,
author={Almoqren, Nuha and Altayar, Mohammed},
booktitle={2016 4th Saudi International Conference on Information Technology (Big Data Analysis) (KACSTIT)},
title={The motivations for big data mining technologies adoption in saudi banks},
year={2016},
volume={},
number={},
pages={1-8},
abstract={Significant shifts in the business environment, economic instability, changes in the desires and expectations of customers and employees, led the banking sector to find new technology strategies that corresponded with these changes. The Banking sector realized the need for innovative Information Technology solutions, and this has led to the use of Big Data and Data Mining tools, where both are playing significant and effective roles in creating business value in banking products and services. This research seeks to investigate the motivational factors affecting the implementation of data mining techniques to harness big data in Saudi banks. According to the findings, the adoption and implementation of data mining to harness big data is affected by motivational factors including: system quality, information quality, service quality and perceived benefits. The paper highlights the importance of these issues and their role in the adoption and implementation of big data mining technology.},
keywords={Big data;Data mining;Banking;Data visualization;Business;Reliability;Big Data;Data Mining;Banks;Saudi Banks;IT adoption and implementation},
doi={10.1109/KACSTIT.2016.7756075},
ISSN={},
month={Nov},}