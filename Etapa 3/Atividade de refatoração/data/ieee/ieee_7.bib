@INPROCEEDINGS{8622101,
author={Li, Sihuan and Di, Sheng and Liang, Xin and Chen, Zizhong and Cappello, Franck},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Optimizing Lossy Compression with Adjacent Snapshots for N-body Simulation Data},
year={2018},
volume={},
number={},
pages={428-437},
abstract={Today's N-body simulations are producing extremely large amounts of data. The Hardware/Hybrid Accelerated Cosmology Code (HACC), for example, may simulate trillions of particles, producing tens of petabytes of data to store in a parallel file system, according to the HACC users. In this paper, we design and implement an efficient, in situ error-bounded lossy compressor to significantly reduce the data size for N-body simulations. Not only can our compressor save significant storage space for N-body simulation researchers, but it can also improve the I/O performance considerably with limited memory and computation overhead. Our contribution is threefold. (1) We propose an efficient data compression model by leveraging the consecutiveness of the cosmological data in both space and time dimensions as well as the physical correlation across different fields. (2) We propose a lightweight, efficient alignment mechanism to align the disordered particles across adjacent snapshots in the simulation, which is a fundamental step in the whole compression procedure. We also optimize the compression quality by exploring best-fit data prediction strategies and optimizing the frequencies of the space-based compression vs. time-based compression. (3) We evaluate our compressor using both a cosmological simulation package and molecular dynamics simulation data-two major categories in the N-body simulation domain. Experiments show that under the same distortion of data, our solution produces up to 43% higher compression ratios on the velocity field and up to 300% higher on the position field than do other state-of-the-art compressors (including SZ, ZFP, NUMARCK, and decimation). With our compressor, the overall I/O time on HACC data is reduced by up to 20% compared with the second-best compressor.},
keywords={Data models;Computational modeling;Data compression;Big Data;Distortion;Supercomputers;Mathematical model;Error-bounded lossy compression;N-body simulation;large science data;I/O performance},
doi={10.1109/BigData.2018.8622101},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7460348,
author={Ismail, Ashraf},
booktitle={2016 3rd MEC International Conference on Big Data and Smart City (ICBDSC)},
title={Utilizing big data analytics as a solution for smart cities},
year={2016},
volume={},
number={},
pages={1-5},
abstract={Cities generate huge volumes of data that are diverse in nature and are generated on a daily basis. These data types involve environment, energy, transport and economic data. This includes structured and unstructured data which needs to be processed and understood to become valuable. By managing and analyzing big data, smart cities will be able to tackle formidable problems and ascend to a state where better and more informed decisions will be taken resulting in enhanced economic and environment outcomes leading to better quality of life. This paper reviews utilizing big data in cities by using big data analytics as a solution for smart cities and conceiving related issues being faced.},
keywords={Big data;Smart cities;Intelligent sensors;Economics;Business;key Big Data;Big Data Analytics;Smart Cities},
doi={10.1109/ICBDSC.2016.7460348},
ISSN={},
month={March},}
@INPROCEEDINGS{9050350,
author={Yang, Xueqin and Zan, Tao and Wang, Lan and Li, Deli},
booktitle={2020 12th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)},
title={Research on Nursing Management Based on Big Data},
year={2020},
volume={},
number={},
pages={770-772},
abstract={With the rapid development of the Internet, the era of science and technology has arrived, and big data in information technology has become the driving force behind scientific and technological progress. Applying big data and intelligence to the clinical nursing quality management system, constructing a nursing management and control platform that integrates nursing quality indicators, nursing event reporting, and nursing risk management, and realizes the dynamic and intelligent management and control of nursing quality throughout the process. After applying big data to the nursing quality management system, the time for entering and analyzing nursing quality problems is significantly shortened, the quality of nursing services is improved, and patient satisfaction is effectively improved. The establishment and application of mobile nursing quality management system reduces time cost, improves nursing work efficiency, promotes the full participation of nursing quality management, and improves the effectiveness of nursing quality management and patient satisfaction.},
keywords={Mechatronics;Hospitals;Process control;Medical services;System integration;Quality of service;Big Data;big data;nursing;satisfaction;work efficiency},
doi={10.1109/ICMTMA50254.2020.00168},
ISSN={2157-1481},
month={Feb},}
@ARTICLE{7116439,
author={Nepal, Surya and Ranjan, Rajiv and Choo, Kim-Kwang Raymond},
journal={IEEE Cloud Computing},
title={Trustworthy Processing of Healthcare Big Data in Hybrid Clouds},
year={2015},
volume={2},
number={2},
pages={78-84},
abstract={A 2015 Gartner report noted that data processing technologies haven't kept pace with the significant increase in the volume of digital healthcare data, and an integrated and trustworthy healthcare analytics solution can facilitate more effective decision making in patient care and risk management, improving quality of life, optimizing performance of services, and so on. The challenge is how to ensure data confidentiality and integrity when storing such data but still make it highly available, process it to extract actionable information for decision makers, including medical professionals, and share it with collaborators, while preserving the privacy of individual patients and giving them the full control of their data at all times. This challenge calls for a trustworthy big data processing platform.},
keywords={Cloud computing;Big data;Biomedical imaging;Medical services;Cryptography;Data privacy;cloud computing;big data;healthcare;big data processing;Internet-connected devices;privacy},
doi={10.1109/MCC.2015.36},
ISSN={2325-6095},
month={Mar},}
@INPROCEEDINGS{8070878,
author={Liyanage, Nandika Habaraduwa},
booktitle={2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC)},
title={Advanced query model design concept to support multi-dimensional data analytics for relational database management systems},
year={2017},
volume={},
number={},
pages={432-435},
abstract={Industries use information for decision making to help process changers and business expansions. They use historical data to produce the required information. Preparation of information varies with the way of arranging, retrieving and processing data accurately. Decision support systems use many different approaches to design and data manipulation. Existing small to medium systems still use relational databases and basic query models as primary data analysis technique. But large scaled enterprise systems use different analytical, data warehousing and business intelligent systems rather than using databases due to the numerous RDBMS limitations. As described in (Ballard, et al., 1998, February) data warehousing concept evolve the easy access method of quality data in a structured store for decision making. Objective of this paper is to introduce an advanced database and query model design to support data analysis, business intelligence and data visualization over relational database management systems.},
keywords={Big Data;Computational intelligence;Analytical models;Data models;Data analysis;Relational databases;Two dimensional displays;Multi-Dimensional Key (MDK);Multi-Dimensional Cursor (MDC);Primary Multi-Dimension Key (PMDK);Foreign (Reference) Multi-Dimension Key (FMDK)},
doi={10.1109/ICBDACI.2017.8070878},
ISSN={},
month={March},}
@INPROCEEDINGS{8690369,
author={Xiaorong, Feng and Shizhun, Jia and Songtao, Mai},
booktitle={2018 IEEE International Conference of Safety Produce Informatization (IICSPI)},
title={The Research and Analysis of Big Data Application on Distribution Network},
year={2018},
volume={},
number={},
pages={331-335},
abstract={Big data used in intelligent power grid operation is gradually getting more and more widely acknowledged and emphasized. Due to the fast development of smart grid and distribution network, big data application on the traditional power industry is undergoing profound changes, which promotes the decentralization, digitalization and intellectualization of distribution system devices, where power users could significantly make use of the information amount of system running state. Through the analysis of big data derived from distribution network, this paper discusses the concept of big data characteristics in modern intelligent distribution network, clarifies data source of distributed network system and illustrates the significance of big data application. Finally, effective big data analysis methods are next employed to demonstrate the special implement of big data technologies in distribution network environment.},
keywords={Distribution networks;Big Data;Power quality;Distributed databases;Monitoring;Power grids;distribution network;smart grid;big data;characteristic analysis;assessment and forecasting},
doi={10.1109/IICSPI.2018.8690369},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9607195,
author={Zhao, Haicheng},
booktitle={2020 2nd International Conference on Applied Machine Learning (ICAML)},
title={Analysis of Big Data Cleaning Algorithm Research and System Platform Construction},
year={2020},
volume={},
number={},
pages={187-190},
abstract={In recent years., due to the rapid development of industry., science and technology., the sources of big data with important industrial significance are increasingly diversified., showing an exponential growth trend. In today's social production., if we want to obtain useful quality data about fast-growing and complex data., we need to clean up these data. In the era of Industry 4.0., the global manufacturing industry is developing rapidly., and the data in the production process has increased., showing an exponential trend. Pay more attention to the use of these data in production. How to make effective use of public industrial big data from networked and intelligent manufacturing is the main solution of manufacturing industry. To this end, in order to better use the big data cleaning algorithm in production and production., this paper studies the design., system construction and implementation of the big data platform.},
keywords={Manufacturing industries;Machine learning algorithms;Production;Machine learning;Big Data;Market research;Cleaning;Big data cleaning;System platform construction;manufacturing},
doi={10.1109/ICAML51583.2020.00045},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9079066,
author={Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and Alhroob, Aysh},
booktitle={2020 11th International Conference on Information and Communication Systems (ICICS)},
title={An Approach to Improve Data Quality from Big Data Aspect by Sensitive Cost and Time},
year={2020},
volume={},
number={},
pages={022-026},
abstract={Big data is term of dataset with characteristic volume, value and veracity that lead to challenges unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the appropriate resources of organization in many phases by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope depends on high trust which is getting high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally choose from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation arranging between them start by project scope as strongest one then cost, product and finally time is weakest between them, in the final when select best quality use two sides generally from quality degree and be middle-quality interval and especially from relative distance with the strongest factor.},
keywords={Costs;Correlation;Statistical analysis;Data integrity;Volume measurement;Project management;Big Data;Big Data;Project Management;Sensitive Rule;Quality},
doi={10.1109/ICICS49469.2020.239526},
ISSN={2573-3346},
month={April},}
@INPROCEEDINGS{8711912,
author={Suleykin, Alexander and Panfilov, Peter},
booktitle={2019 24th Conference of Open Innovations Association (FRUCT)},
title={Distributed Big Data Driven Framework for Cellular Network Monitoring Data},
year={2019},
volume={},
number={},
pages={430-436},
abstract={The smart monitoring system (SMS) vision relies on the use of ICT to efficiently manage and maximize the utility of network infrastructures and services in order to improve the quality of service and network performance. Many aspects of SMS projects are dynamic data driven application system where data from sensors monitoring the system state are used to drive computations that in turn can dynamically adapt and improve the monitoring process as the complex system evolves. In this context, a research and development of new paradigm of Distributed Big Data Driven Framework (DBDF) for monitoring data in mobile network infrastructures entails the ability to dynamically incorporate more accurate information for network monitoring and controlling purposes through obtaining real-time measurements from the base stations, user demands and claims, and other sensors (for weather conditions, etc.). The proposed framework consists of network probes, data parsing application, Message-Oriented Middleware, real-time and offline data models, Big Data storage and Decision layers., and Other data sources. Each Big Data layer might be implemented using comparative analysis of the most effective Big Data solutions. In addition, as a proof of concept, the roaming users detection model was created based on Apache Spark application. The model filters streaming protocols data, deserializes it into Json format and finally sends it to Kafka application. The experiments with the model demonstrated and acknowledged the capacities of the Apache Spark in building foundation for Big Data hub as a basic application for online mobile network data processing.},
keywords={Big Data;Real-time systems;Cellular networks;Monitoring;Distributed databases;Data models;Computer architecture},
doi={10.23919/FRUCT.2019.8711912},
ISSN={2305-7254},
month={April},}
@INPROCEEDINGS{9005596,
author={Abdoli, Alireza and Murillo, Amy C. and Gerry, Alec C. and Keogh, Eamonn J.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Time Series Classification: Lessons Learned in the (Literal) Field while Studying Chicken Behavior},
year={2019},
volume={},
number={},
pages={5962-5964},
abstract={Poultry farms are a major contributor to the human food chain. However, around the world, there have been growing concerns about the quality of life for the livestock in poultry farms; and increasingly vocal demands for improved standards of animal welfare. Recent advances in sensing technologies and machine learning allow the possibility of monitoring birds, and employing the lessons learned to improve the welfare for all birds. This task superficially appears to be easy, yet, studying behavioral patterns involves collecting enormous amounts of data, justifying the term Big Data. Before the big data can be used for analytical purposes to tease out meaningful, well-conserved behavioral patterns, the collected data needs to be preprocessed. The pre-processing refers to processes for cleansing and preparing data so that it is in the format ready to be analyzed by downstream algorithms, such as classification and clustering algorithms. However, as we shall demonstrate, efficient preprocessing of chicken big data is both non-trivial and crucial towards success of further analytics.},
keywords={Time series analysis;Big Data;Classification algorithms;Data mining;Birds;Timing;Time Series;Classification;Big Data;Machine Learning;Poultry Welfare},
doi={10.1109/BigData47090.2019.9005596},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9005600,
author={Janjic, V. and Bowles, J.K.F. and Vermeulen, A.F. and Silvina, A. and Belk, M. and Fidas, C. and Pitsillides, A. and Kumar, M. and Rossbory, M. and Vinov, M. and Given-Wilson, T. and Legay, A. and Blackledge, E. and Arredouani, R. and Stylianou, G. and Huang, W.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={The SERUMS tool-chain: Ensuring Security and Privacy of Medical Data in Smart Patient-Centric Healthcare Systems},
year={2019},
volume={},
number={},
pages={2726-2735},
abstract={Future-generation healthcare systems will be highly distributed, combining centralised hospital systems with decentralised home-, work-and environment-based monitoring and diagnostics systems. These will reduce costs and injury-related risks whilst both improving quality of service, and reducing the response time for diagnostics and treatments made available to patients. To make this vision possible, medical data must be accessed and shared over a variety of mediums including untrusted networks. In this paper, we present the design and initial implementation of the SERUMS tool-chain for accessing, storing, communicating and analysing highly confidential medical data in a safe, secure and privacy-preserving way. In addition, we describe a data fabrication framework for generating large volumes of synthetic but realistic data, that is used in the design and evaluation of the tool-chain. We demonstrate the present version of our technique on a use case derived from the Edinburgh Cancer Centre, NHS Lothian, where information about the effects of chemotherapy treatments on cancer patients is collected from different distributed databases, analysed and adapted to improve ongoing treatments.},
keywords={Medical services;Distributed databases;Security;Data privacy;Fabrication;Data models;Medical diagnostic imaging;Medical data;Smart Healthcare;Data Sharing;Privacy;Security;Personalised Medicine},
doi={10.1109/BigData47090.2019.9005600},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7363909,
author={Karimov, Jeyhun and Ozbayoglu, Murat},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={High quality clustering of big data and solving empty-clustering problem with an evolutionary hybrid algorithm},
year={2015},
volume={},
number={},
pages={1473-1478},
abstract={Achieving high quality clustering is one of the most well-known problems in data mining. k-means is by far the most commonly used clustering algorithm. It converges fairly quickly, but achieving a good solution is not guaranteed. The clustering quality is highly dependent on the selection of the initial centroid selections. Moreover, when the number of clusters increases, it starts to suffer from "empty clustering". The motivation in this study is two-fold. We not only aim at improving the k-means clustering quality, but at the same time not being effected by the empty cluster issue. For achieving this purpose, we developed a hybrid model, H(EC)2S, Hybrid Evolutionary Clustering with Empty Clustering Solution. Firstly, it selects representative points to eliminate Empty Clustering problem. Then, the hybrid algorithm uses only these points during centroid selection. The proposed model combines Fireworks and Cuckoo-search based evolutionary algorithm with some centroid-calculation heuristics. The model is implemented using a Hadoop Mapreduce algorithm for achieving scalability when faced with a Big Data clustering problem. The advantages of the developed model is particularly attractive when the amount, dimensionality and number of cluster parameters tend to increase. The results indicate that considerable clustering quality performance improvement is achieved using the proposed model.},
keywords={Clustering algorithms;Mathematical model;Big data;Explosions;Sparks;Evolutionary computation;Arrays;clustering;k-means;evolutionary algorithms;Cuckoo search;Fireworks algorithm;Hadoop;Mapreduce},
doi={10.1109/BigData.2015.7363909},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8622178,
author={Bahri, Maroua and Maniu, Silviu and Bifet, Albert},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={A Sketch-Based Naive Bayes Algorithms for Evolving Data Streams},
year={2018},
volume={},
number={},
pages={604-613},
abstract={A well-known learning task in big data stream mining is classification. Extensively studied in the offline setting, in the streaming setting - where data are evolving and even infinite - it is still a challenge. In the offline setting, training needs to store all the data in memory for the learning task; yet, in the streaming setting, this is impossible to do due to the massive amount of data that is generated in real-time. To cope with these resource issues, this paper proposes and analyzes several evolving naive Bayes classification algorithms, based on the well-known count-min sketch, in order to minimize the space needed to store the training data. The proposed algorithms also adapt concept drift approaches, such as ADWIN, to deal with the fact that streaming data may be evolving and change over time. However, handling sparse, very high-dimensional data in such framework is highly challenging. Therefore, we include the hashing trick, a technique for dimensionality reduction, to compress that down to a lower dimensional space, which leads to a large memory saving.We give a theoretical analysis which demonstrates that our proposed algorithms provide a similar accuracy quality to the classical big data stream mining algorithms using a reasonable amount of resources. We validate these theoretical results by an extensive evaluation on both synthetic and real-world datasets.},
keywords={Prediction algorithms;Approximation algorithms;Hash functions;Big Data;Training;Task analysis;Data mining;Data stream classification;Naive Bayes;Count-min sketch;Hashing trick;Concept drift},
doi={10.1109/BigData.2018.8622178},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8657115,
author={Zan, Khant Ko},
booktitle={2019 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)},
title={Prospects For Using Big Data To Improve The Effectiveness Of An Education Organization},
year={2019},
volume={},
number={},
pages={1777-1779},
abstract={Today, teaching methods have changed significantly. With new innovations in the field of information technology, work in the classroom becomes virtual, and cloud knowledge. Although these new technologies have been adopted in the field of education, the informative organizations still face the challenge of improving the quality of education and reducing dropout rates. This article will examine the analysis of big data that can be used in education and how it affects the improvement of the quality of education.},
keywords={Big Data;Education;Hardware;Information technology;Software;Monitoring;Organizations;Big Data;Big Data Analytics;Teaching Methods;Higher Education;Sociotechnical Approach},
doi={10.1109/EIConRus.2019.8657115},
ISSN={2376-6565},
month={Jan},}
@INPROCEEDINGS{9101464,
author={Swami, Arun and Vasudevan, Sriram and Huyn, Joojay},
booktitle={2020 IEEE 36th International Conference on Data Engineering (ICDE)},
title={Data Sentinel: A Declarative Production-Scale Data Validation Platform},
year={2020},
volume={},
number={},
pages={1579-1590},
abstract={Many organizations process big data for important business operations and decisions. Hence, data quality greatly affects their success. Data quality problems continue to be widespread, costing US businesses an estimated $600 billion annually. To date, addressing data quality in production environments still poses many challenges: easily defining properties of high-quality data; validating production-scale data in a timely manner; debugging poor quality data; designing data quality solutions to be easy to use, understand, and operate; and designing data quality solutions to easily integrate with other systems. Current data validation solutions do not comprehensively address these challenges. To address data quality in production environments at LinkedIn, we developed Data Sentinel, a declarative production-scale data validation platform. In a simple and well-structured configuration, users declaratively specify the desired data checks. Then, Data Sentinel performs these data checks and writes the results to an easily understandable report. Furthermore, Data Sentinel provides well-defined schemas for the configuration and report. This makes it easy for other systems to interface or integrate with Data Sentinel. To make Data Sentinel even easier to use, understand, and operate in production environments, we provide Data Sentinel Service (DSS), a complementary system to help specify data checks, schedule, deploy, and tune data validation jobs, and understand data checking results. The contributions of this paper include the following: 1) Data Sentinel, a declarative production-scale data validation platform successfully deployed at LinkedIn 2) A generic design to build and deploy similar systems for production environments 3) Experiences and lessons learned that can benefit practitioners with similar objectives.},
keywords={Data integrity;Production;LinkedIn;Big Data;Business;Debugging;Schedules},
doi={10.1109/ICDE48307.2020.00140},
ISSN={2375-026X},
month={April},}
@INPROCEEDINGS{6903145,
author={Sowe, Sulayman K. and Kimata, Takashi and Dong, Mianxiong and Zettsu, Koji},
booktitle={2014 IEEE 38th International Computer Software and Applications Conference Workshops},
title={Managing Heterogeneous Sensor Data on a Big Data Platform: IoT Services for Data-Intensive Science},
year={2014},
volume={},
number={},
pages={295-300},
abstract={Big data has emerged as a key connecting point between things and objects on the internet. In this cyber-physical space, different types of sensors interact over wireless networks, collecting data and delivering services ranging from environmental pollution monitoring, disaster management and recovery, improving the quality of life in homes, to enabling smart cities to function. However, despite the perceived benefits we are realizing from these sensors, the dawn of the Internet of Things (IoT) brings fresh challenges. Some of these have to do with designing the appropriate infrastructure to capture and store the huge amount of heterogeneous sensor data, finding practical use of the collected sensor data, and managing IoT communities in such a way that users can seamlessly search, find, and utilize their sensor data. In order to address these challenges, this paper describes an integrated IoT architecture that combines the functionalities of Service-Controlled Networking (SCN) with cloud computing. The resulting community-driven big data platform helps environmental scientists easily discover and manage data from various sensors, and share their knowledge and experience relating to air pollution impacts. Our experience in managing the platform and communities provides a proof of concept and best practice guidelines on how to manage IoT services in a data-intensive research environment.},
keywords={Communities;Big data;Data visualization;Computer architecture;Pollution;Middleware;Internet of Things;Internet of Things;Big Data;Sensor data;IoT architecture;Service-Controlled Networking;Data-intensive science},
doi={10.1109/COMPSACW.2014.52},
ISSN={},
month={July},}
@INPROCEEDINGS{9260288,
author={Xiao, Wei},
booktitle={2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Quality-Centered Mobile Terminal Teaching Assessment System under the Background of Big Data},
year={2020},
volume={},
number={},
pages={446-449},
abstract={The teaching system of mobile terminals is a teaching environment that has emerged with the popularity of computer technology and mobile devices. It is the main research object in this article. It is different from the traditional classroom teaching environment. It completely uses mobile terminals instead of traditional paper textbooks. In this classroom environment, students and teachers each have a handheld mobile device that connects to the wireless network through a server. The information is processed dynamically and statically through the data acquisition layer, and the processed data is analyzed dynamically and statically, so that the data storage layer stores the information. Then the data storage layer feeds back the data information to students, service organizations, operators and teachers through the precise teaching engine, so as to meet the needs of teaching. Students acquire personalized teaching needs, and establish a precise large data analysis and data mining model.},
keywords={Zirconium;Smart grids;Radio frequency;Quality function deployment;Germanium;Conferences;Automation;big data;mobile terminal;teaching evaluation;data mining},
doi={10.1109/ICSGEA51094.2020.00102},
ISSN={},
month={June},}
@INPROCEEDINGS{8855387,
author={ahmad, sahan and Zobaed, SM and Gottumukkala, Raju and Salehi, Mohsen Amini},
booktitle={2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
title={Edge Computing for User-Centric Secure Search on Cloud-Based Encrypted Big Data},
year={2019},
volume={},
number={},
pages={662-669},
abstract={Cloud service providers offer a low-cost and convenient solution to host unstructured data. However, cloud services act as third-party solutions and do not provide control of the data to users. This has raised security and privacy concerns for many organizations (users) with sensitive data to utilize cloud-based solutions. User-side encryption can potentially address these concerns by establishing user-centric cloud services and granting data control to the user. Nonetheless, user-side encryption limits the ability to process (e.g., search) encrypted data on the cloud. Accordingly, in this research, we provide a framework that enables processing (in particular, searching) of encrypted multiorganizational (i.e., multi-source) big data without revealing the data to cloud provider. Our framework leverages locality feature of edge computing to offer a user-centric search ability in a realtime manner. In particular, the edge system intelligently predicts the user's search pattern and prunes the multi-source big data search space to reduce the search time. The pruning system is based on efficient sampling from the clustered big dataset on the cloud. For each cluster, the pruning system dynamically samples appropriate number of terms based on the user's search tendency, so that the cluster is optimally represented. We developed a prototype of a user-centric search system and evaluated it against multiple datasets. Experimental results demonstrate 27% improvement in the pruning quality and search accuracy.},
keywords={Big Data;Encryption;Semantic search;Cloud computing;Edge computing;Data privacy;Edge Computing, User-based Sampling, Markov Chain, Privacy-Preserving Big Data, Encrypted Clustering},
doi={10.1109/HPCC/SmartCity/DSS.2019.00100},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7377714,
author={Dantanarayana, GGT and Sahama, Tony and Wikramanayake, GN},
booktitle={2015 Fifteenth International Conference on Advances in ICT for Emerging Regions (ICTer)},
title={Quality of information for quality of life: Healthcare big data analytics},
year={2015},
volume={},
number={},
pages={281-281},
abstract={Business intelligence and analytics, and big data analytics have become increasingly important in describing the data sets and analytical techniques in software applications that are so large and complex. These two techniques have been used as analytics by several e-commerce communities. For example, vendors such as Amazon and eBay have adapted these techniques to significantly advance in innovative and highly recommended scalable e-commerce platforms and product systems to target potential customers thus increasing business revenues. In a similar context, the health community have experienced not only more complex and large data content, but also information systems that contain a large number of data sources with interrelated data. Furthermore, due to the increasing diversity and differentiation of expansions by service providers in the form of primary or nursing care, a variety of service organisations in the public and private hospital networks including new medical specialist facilities have resulted in challenging, and highly dynamic environments resulting in the creation of big data with its enumerate complexities, for instance sharing information with expected security requirements of stakeholders. Therefore, the health community will have to adapt the concept of big data analytics in order to solve major issues that have occurred due to complex shared information.},
keywords={Electronic Health Record (EHR);Quality of Information;Value Co-creation;Healthcare Big Data Analytics},
doi={10.1109/ICTER.2015.7377714},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8258220,
author={Ding, Junhua and Li, XinChuan and Gudivada, Venkat N.},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Augmentation and evaluation of training data for deep learning},
year={2017},
volume={},
number={},
pages={2603-2611},
abstract={Deep learning is an important technique for extracting value from big data. However, the effectiveness of deep learning requires large volumes of high quality training data. In many cases, the size of training data is not large enough for effectively training a deep learning classifier. Data augmentation is a widely adopted approach for increasing the amount of training data. But the quality of the augmented data may be questionable. Therefore, a systematic evaluation of training data is critical. Furthermore, if the training data is noisy, it is necessary to separate out the noise data automatically. In this paper, we propose a deep learning classifier for automatically separating good training data from noisy data. To effectively train the deep learning classifier, the original training data need to be transformed to suit the input format of the classifier. Moreover, we investigate different data augmentation approaches to generate sufficient volume of training data from limited size original training data. We evaluated the quality of the training data through cross validation of the classification accuracy with different classification algorithms. We also check the pattern of each data item and compare the distributions of datasets. We demonstrate the effectiveness of the proposed approach through an experimental investigation of automated classification of massive biomedical images. Our approach is generic and is easily adaptable to other big data domains.},
keywords={Diffraction;Machine learning;Training data;Big Data;Support vector machines;Training;Noise measurement;big data;machine learning;neural network;deep learning;convolutional neural network;support vector machine;diffraction image},
doi={10.1109/BigData.2017.8258220},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8718981,
author={Duan, Panting and He, Yihai and Zhang, Anqing and Cui, Jiaming and Liu, Fengdi},
booktitle={2018 12th International Conference on Reliability, Maintainability, and Safety (ICRMS)},
title={Big Data Oriented Root Cause Heuristic Identification Approach Based on FWARM for Quality Accident},
year={2018},
volume={},
number={},
pages={7-12},
abstract={With the advent of big data era, data driven quality management decision-making has become an important means to seek new quality improvement opportunity for manufacturers. Quality accidents (QAs) can cause severely economic and reputational damage to manufacturer, accurate identification of root causes for severe quality accident is the primary task and always a big challenge for quality managers. Because of the fuzzy nature of incomplete and noisy data for big data, the fuzzy concept is proposed, and the fuzzy weighted association rule mining (FWARM) method is adopted into the root cause identification of quality accident novelly. Firstly, Quality Accident formation mechanism and modeling of big data for quality accident are defined; Secondly, root cause identification framework of quality accident based on the relevance tree is established; Then, the FWARM method is used to identify product functional defects and physical defects through big data for quality accident. Finally, a case study of root cause identification of quality accident is adopted to validate the proposed approach.},
keywords={Accidents;Big Data;Data mining;Reliability;Manufacturing;Data models;Product design;Quality Accidents, Root Cause, Heuristic Identification, Data Mining, FWARM},
doi={10.1109/ICRMS.2018.00012},
ISSN={2575-2642},
month={Oct},}
@INPROCEEDINGS{9384673,
author={Yang, Chunlin},
booktitle={2020 International Conference on Modern Education and Information Management (ICMEIM)},
title={The Construction of “Dual-qualified” Teachers in Applied Colleges Based on Big Data},
year={2020},
volume={},
number={},
pages={194-197},
abstract={Big data mainly refers to large-scale or ultra-large-scale data sets, which are called “massive data” or “massive data”. Nowadays, with the development of information technology, such as mobile Internet, cloud computing, Internet of Things, and data mining, a variety of information is gradually being widely used in various industries. Moreover, they can provide important forces to promote national innovation, scientific development, economic take-off, and educational reform. The characteristics of big data are mainly manifested as massive, fast, diversified and high value. Based on the quality requirements of “double-qualified” teachers in applied colleges, schools need to strengthen the construction of the teaching team and create good conditions to actively cultivate high-quality talents.},
keywords={Industries;Technological innovation;Education;Big Data;Information management;Internet of Things;Information technology;Big Data;Applied Colleges;“Dual-qualified” Teachers;Team Building},
doi={10.1109/ICMEIM51375.2020.00051},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9284399,
author={Mystakidis, Aristeidis and Tjortjis, Christos},
booktitle={2020 11th International Conference on Information, Intelligence, Systems and Applications (IISA},
title={Big Data Mining for Smart Cities: Predicting Traffic Congestion using Classification},
year={2020},
volume={},
number={},
pages={1-8},
abstract={This paper provides an analysis and proposes a methodology for predicting traffic congestion. Several machine learning algorithms and approaches are compared to select the most appropriate one. The methodology was implemented using Data Mining and Big Data techniques along with Python, SQL, and GIS technologies and was tested on data originating from one of the most problematic, regarding traffic congestion, streets in Thessaloniki, the 2nd most populated city in Greece. Evaluation and results have shown that data quality and size were the most critical factors towards algorithmic accuracy. Result comparison showed that Decision Trees were more accurate than Logistic Regression.},
keywords={Machine learning algorithms;Smart cities;Big Data;Prediction algorithms;Data mining;Traffic congestion;Regression tree analysis;Data Mining;Big Data;Machine learning;Smart Cities;Prediction;Classification;Traffic Congestion},
doi={10.1109/IISA50023.2020.9284399},
ISSN={},
month={July},}
@INPROCEEDINGS{9006422,
author={Grueneberg, K. and Calo, S. and Dewan, P. and Verma, D. and O’Gorman, Tristan},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={A Policy-based Approach for Measuring Data Quality},
year={2019},
volume={},
number={},
pages={4025-4031},
abstract={With the growing importance of data in all aspects of the functioning of an enterprise, having good quality of data is crucial in support of business processes. However, there do not exist good metrics to measure the quality of data that is available within an enterprise. While there are several data quality standards, their complexity and their required customization makes them difficult to use in real-world industrial scenarios. In this paper, we discuss the challenges encountered in measuring data quality within asset management systems. We propose a policy-based approach for measuring data quality, and show how such an approach can be customized and interpreted easily by practitioners in the field.},
keywords={Data integrity;Standards;Measurement;Asset management;Data models;Complexity theory;Data Quality;Asset Management Systems;Policy based Data Management},
doi={10.1109/BigData47090.2019.9006422},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9047259,
author={Shen, Ziyu and Zhang, Xusheng and Xia, Bin and Liu, Zheng and Li, Yun},
booktitle={2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)},
title={Multi-Granularity Power Prediction for Data Center Operations via Long Short-Term Memory Network},
year={2019},
volume={},
number={},
pages={194-201},
abstract={The increasing numbers of the applications and requirement of cloud computing have made huge power consumption in data centers, which brings the problems of the high cost and resource waste. This problem attracts significant attention from academia and industry. A critical approach to solve this problem is constructing an intelligent energy management system for data centers. Furthermore, an efficient assessment and prediction module of power consumption in data centers is an essential part of the management system. It facilitates cloud service providers to perform workflow scheduling at the minimal cost and energy efficiency management with the requirement of QoS. Since the assessment and prediction of power consumption correlate, this paper presents a multi-granularity approach for power consumption prediction in data centers, which combines multi-task learning with the LSTM network. We first transfer a multi-granularity power prediction problem into a multi-task regression problem to assess and predict the power consumption of data center system maintenance and scheduling operations. Due to the time requirement for workflow and container scheduling, the prediction interval is 30 seconds. Then we propose an efficient long short-term memory network for the multigranularity prediction. The experimental results show our model outperforms other prediction models on the real datasets.},
keywords={Power demand;Data centers;Task analysis;Predictive models;Data models;Time series analysis;Servers;Data center, Time series prediction, Energy efficiency, Power consumption},
doi={10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00037},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9378483,
author={Hossain, Md Monir and Sebestyen, Mark and Mayank, Dhruv and Ardakanian, Omid and Khazaei, Hamzeh},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Large-scale Data-driven Segmentation of Banking Customers},
year={2020},
volume={},
number={},
pages={4392-4401},
abstract={This paper presents a novel big data analytics framework for creating explainable personas for retail and business banking customers. These personas are essential to better tailor financial products and improve customer retention. This framework is comprised of several components including anomaly detection, binning and aggregation of contextual data, clustering of transaction time series, and mining association rules that map contextual data to cluster identifiers. Leveraging rich transaction and contextual data available from nearly 60,000 retail and 90,000 business customers of a financial institution, we empirically evaluate this framework and describe how the identified association rules can be used to explain and refine existing customer classes, and identify new customer classes and various data quality issues. We also analyze the performance of the proposed framework and show that it can easily scale to millions of banking customers.},
keywords={Data integrity;Conferences;Time series analysis;Banking;Big Data;Anomaly detection;Business;customer segmentation;clustering;association rules mining;anomaly detection},
doi={10.1109/BigData50022.2020.9378483},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8457746,
author={Ardagna, Claudio A. and Bellandi, Valerio and Ceravolo, Paolo and Damiani, Ernesto and Di Martino, Beniamino and D'Angelo, Salvatore and Esposito, Antonio},
booktitle={2018 IEEE International Congress on Big Data (BigData Congress)},
title={A Fast and Incremental Development Life Cycle for Data Analytics as a Service},
year={2018},
volume={},
number={},
pages={174-181},
abstract={Big Data does not only refer to a huge amount of diverse and heterogeneous data. It also points to the management of procedures, technologies, and competencies associated with the analysis of such data, with the aim of supporting high-quality decision making. There are, however, several obstacles to the effective management of a Big Data computation, such as data velocity, variety, and veracity, and technological complexity, which represent the main barriers towards the full adoption of the Big Data paradigm. The goal of this work is to define a new software Development Life Cycle for the design and implementation of a Big Data computation. Our proposal integrates two model-driven methods: a first method based on pre-configured services that reduces the cost of deployment and a second method based on custom component development that provides an incremental process of refinement and customization. The proposal is experimentally evaluated by clustering a data set of the distribution of the population in the United States based on contextual criteria.},
keywords={Big Data;Data models;Computational modeling;Data analysis;Complexity theory;Tuning;Proposals;Big Data Analytics;Model-Driven Development;Software Development Life Cycle},
doi={10.1109/BigDataCongress.2018.00030},
ISSN={},
month={July},}
@ARTICLE{7225163,
author={Xia, Qiufen and Xu, Zichuan and Liang, Weifa and Zomaya, Albert Y.},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={Collaboration- and Fairness-Aware Big Data Management in Distributed Clouds},
year={2016},
volume={27},
number={7},
pages={1941-1953},
abstract={With the advancement of information and communication technology, data are being generated at an exponential rate via various instruments and collected at an unprecedented scale. Such large volume of data generated is referred to as big data, which now are revolutionizing all aspects of our life ranging from enterprises to individuals, from science communities to governments, as they exhibit great potentials to improve efficiency of enterprises and the quality of life. To obtain nontrivial patterns and derive valuable information from big data, a fundamental problem is how to properly place the collected data by different users to distributed clouds and to efficiently analyze the collected data to save user costs in data storage and processing, particularly the cost savings of users who share data. By doing so, it needs the close collaborations among the users, by sharing and utilizing the big data in distributed clouds due to the complexity and volume of big data. Since computing, storage and bandwidth resources in a distributed cloud usually are limited, and such resource provisioning typically is expensive, the collaborative users require to make use of the resources fairly. In this paper, we study a novel collaboration- and fairness-aware big data management problem in distributed cloud environments that aims to maximize the system throughout, while minimizing the operational cost of service providers to achieve the system throughput, subject to resource capacity and user fairness constraints. We first propose a novel optimization framework for the problem. We then devise a fast yet scalable approximation algorithm based on the built optimization framework. We also analyze the time complexity and approximation ratio of the proposed algorithm. We finally conduct experiments by simulations to evaluate the performance of the proposed algorithm. Experimental results demonstrate that the proposed algorithm is promising, and outperforms other heuristics.},
keywords={Distributed databases;Big data;Collaboration;Bandwidth;Servers;Approximation algorithms;Optimization;Big data management;dynamic data placement;fair resource allocation;collaborative users;distributed clouds;data sharing;Big data management;dynamic data placement;fair resource allocation;collaborative users;distributed clouds;data sharing},
doi={10.1109/TPDS.2015.2473174},
ISSN={1558-2183},
month={July},}
@INPROCEEDINGS{7321730,
author={Ahnn, Jong Hoon},
booktitle={2014 IEEE/ACM International Symposium on Big Data Computing},
title={A Practical Approach to Scalable Big Data Computing for the Personalization of Services at Samsung},
year={2014},
volume={},
number={},
pages={64-73},
abstract={We observe that the recent advances in big data computing have empowered the personalization of service including model-based services such as speech recognition, face recognition, and context-aware service. Various sources of user's logs can be utilized in remodeling, adapting, and personalizing pretrained models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner on top of Samsung' big data infrastructure. An automatic speech recognition (ASR) service such as Samsung's S-Voice, Apple's SIRI is one of the representative examples. Recently advances in ASR married with big data technologies drive more personalized services in many areas of services. A speaker adaptation is now a well-accepted technology that requires huge computation cost in creating a personalized acoustic model and corresponding language model over several billions of Samsung product users. We implement a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. An optimal set of performance optimization is suggested to have the best performance such as workflow compaction, file compression, best file system selection among several distributed file systems.},
keywords={Adaptation models;Speech;Acoustics;Computational modeling;Engines;Big data;Speech recognition;Big Data;Cloud Computing;Hadoop;Speech Recognition;Scalability;Personalization},
doi={10.1109/BDC.2014.11},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6991919,
author={Huang Zhiwei and Gao Tian and Zhang Huaving and Han Xu and Cao Junwei and Hu Ziheng and Yao Senjing and Zhu Zhengguo},
booktitle={2014 China International Conference on Electricity Distribution (CICED)},
title={Transient power quality assessment based on big data analysis},
year={2014},
volume={},
number={},
pages={1308-1312},
abstract={A transient power quality assessment method is proposed in this paper, using Naive Bayes classification method which is based on big data processing architecture, in this architecture, data sources will be extended to the aspects of power grid monitoring data, the power customer data and the public data, and the assessment severity will be classified into the normal state, the abnormal state, the critical state, and the failed state, according to the Naive Bayes classification results. Based on the data type of transient power quality assessment, big data processing architecture used in this paper can be able to process distributed data and streaming data, so that it can ensure not only updates classifier rules regularly, but also the real-time condition assessment. In the classifier training phase, we use the massive historical data as the distributed learning object, and generate assessment rules periodically. In the state assessment phase, each assessment node will update the assessment rules generated by training phase, generate real- time evaluation of samples from stream processing framework, and evaluate the power quality state according to the current rule. On this basis, this paper designs a Naive Bayes classification method based on MapReduce processing, and realizes the map and reduce process method to compute the priori probability and the conditional probability in distributed way. Experiments show that the transient power quality evaluation method based on the big data analysis presented in this paper is feasible, and achieve good results both in classification accuracy and processing speed.},
keywords={Monitoring;Transient analysis;Data mining;Power quality;Data analysis;Abstracts;Accuracy;Big data;MapReduce;Distributed data mining;Naive bayes classification},
doi={10.1109/CICED.2014.6991919},
ISSN={2161-749X},
month={Sep.},}
@INPROCEEDINGS{8622362,
author={Werner, Sebastian and Kuhlenkamp, Jörn and Klems, Markus and Müller, Johannes and Tai, Stefan},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Serverless Big Data Processing using Matrix Multiplication as Example},
year={2018},
volume={},
number={},
pages={358-365},
abstract={Serverless computing, or Function-as-a-Service (FaaS), is emerging as a popular alternative model to on-demand cloud computing. Function services are executed by a FaaS provider; a client no longer uses cloud infrastructure directly as in traditional cloud consumption. Is serverless computing a feasible and beneficial approach to big data processing, regarding performance, scalability, and cost effectiveness? In this paper, we explore this research question using matrix multiplication as example. We define requirements for the design of serverless big data applications, present a prototype for matrix multiplication using FaaS, and discuss and synthesize insights from results of extensive experimentation. We show that serverless big data processing can lower operational and infrastructure costs without compromising system qualities; serverless computing can even outperform cluster-based distributed compute frameworks regarding performance and scalability.},
keywords={Cloud computing;Scalability;FAA;Prototypes;Big Data applications;Task analysis;serverless;big data;cloud;matrix multiplication},
doi={10.1109/BigData.2018.8622362},
ISSN={},
month={Dec},}
@ARTICLE{8456550,
author={Perrot, Alexandre and Auber, David},
journal={IEEE Transactions on Big Data},
title={Cornac: Tackling Huge Graph Visualization with Big Data Infrastructure},
year={2020},
volume={6},
number={1},
pages={80-92},
abstract={The size of available graphs has drastically increased in recent years. The real-time visualization of graphs with millions of edges is a challenge but is necessary to grasp information hidden in huge datasets. This article presents an end-to-end technique to visualize huge graphs using an established Big Data ecosystem and a lightweight client running in a Web browser. For that purpose, levels of abstraction and graph tiles are generated by a batch layer and the interactive visualization is provided using a serving layer and client-side real-time computation of edge bundling and graph splatting. A major challenge is to create techniques that work without moving data to an ad hoc system and that take advantage of the horizontal scalability of these infrastructures. We introduce two novel scalable algorithms that enable to generate a canopy clustering and to aggregate graph edges. These two algorithms are both used to produce levels of abstraction and graph tiles. We prove that our technique guarantee a quality of visualization by controlling both the necessary bandwidth required for data transfer and the quality of the produced visualization. Furthermore, we demonstrate the usability of our technique by providing a complete prototype. We present benchmarks on graphs with millions of elements and we compare our results to those obtained by state of the art techniques. Our results show that new Big Data technologies can be incorporated into visualization pipeline to push out the size limits of graphs one can visually analyze.},
keywords={Data visualization;Visualization;Big Data;Clustering algorithms;Ecosystems;Distributed databases;Sparks;Information visualization;big data;graphs;hadoop;spark},
doi={10.1109/TBDATA.2018.2869165},
ISSN={2332-7790},
month={March},}
@INPROCEEDINGS{8035055,
author={Albertoni, Riccardo and De Martino, Monica and Quarati, Alfonso},
booktitle={2017 International Conference on High Performance Computing & Simulation (HPCS)},
title={Linked Thesauri Quality Assessment and Documentation for Big Data Discovery},
year={2017},
volume={},
number={},
pages={37-44},
abstract={Thesauri are knowledge systems which may ease Big Data access, fostering their integration and re-use. Currently several Linked Data thesauri covering multi-disciplines are available. They provide a semantic foundation to effectively support cross-organization and cross-disciplinary management and usage of Big Data. Thesauri effectiveness is affected by their quality. Diverse quality measures are available taking into account different facets. However, an overall measure is needed to compare several thesauri and to identify those more qualified for a proper reuse. In this paper, we propose a Multi Criteria Decision Making based methodology for the documentation of the quality assessment of linked thesauri as a whole. We present a proof of concept of the Analytic Hierarchy Process adoption to the set of Linked Data thesauri for the Environment deployed in LusTRE. We discuss the step-by-step practice to document the overall quality measurements, generated by the quality assessment, with the W3C promoted Data Quality Vocabulary.},
keywords={Thesauri;Metadata;Vocabulary;Measurement;Quality assessment;Big Data;quality;linked data;thesauri;AHP;metadata;DQV},
doi={10.1109/HPCS.2017.16},
ISSN={},
month={July},}
@ARTICLE{8805062,
author={Nazir, Shah and Nawaz Khan, Muhammad and Anwar, Sajid and Adnan, Awais and Asadi, Shahla and Shahzad, Sara and Ali, Shaukat},
journal={IEEE Access},
title={Big Data Visualization in Cardiology—A Systematic Review and Future Directions},
year={2019},
volume={7},
number={},
pages={115945-115958},
abstract={The digital transformations and use of healthcare information system, electronic medical records, wearable technology, and smart devices are increasing with the passage of time. A variety of sources of big data in healthcare are available, such as biometric data, registration data, electronic health record, medical imaging, patient reported data, biomarker data, clinical data, and administrative data. Visualization of data is a key tool for producing images, diagrams, or animations to convey messages from the viewed insight. The role of cardiology in healthcare is obvious for living and life. The function of heart is the control of blood supply to the entire parts of the body. Recent speedy growth in healthcare and the development of computation in the field of cardiology enable researchers and practitioners to mine and visualize new insights from patient data. The role of visualization is to capture the important information from the data and to visualize it for the easiness of doctors and practitioners. To help the doctors and practitioners, the proposed study presents a detailed report of the existing literature on visualization of data in the field of cardiology. This report will support the doctors and practitioners in decision-making process and to make it easier. This detailed study will eventually summarize the results of the existing literature published related to visualization of data in the cardiology. This research uses the systematic literature protocol and the data was collected from the studies published during the year 2009 to 2018 (10 years). The proposed study selected 53 primary studies from different repositories according to the defined exclusion, inclusion, and quality criteria. The proposed study focused mainly on the research work been done on visualization of big data in the field of cardiology, presented a summary of the techniques used for visualization of data in cardiology, and highlight the benefits of visualizations in cardiology. The current research summarizes and organizes the available literature in the form of published materials related to big data visualization in cardiology. The proposed research will help the researchers to view the available research studies on the subject of medical big data in cardiology and then can ultimately be used as evidence in future research. The results of the proposed research show that there is an increase in articles published yearly wise and several studies exist related to medical big data in cardiology. The derivations from the studies are presented in the paper.},
keywords={Data visualization;Cardiology;Big Data;Medical services;Systematics;Protocols;Libraries;Big data;medical big data;visualization;healthcare;cardiology;systematic literature review},
doi={10.1109/ACCESS.2019.2936133},
ISSN={2169-3536},
month={},}
@ARTICLE{6527249,
author={Wigan, Marcus R. and Clarke, Roger},
journal={Computer},
title={Big Data's Big Unintended Consequences},
year={2013},
volume={46},
number={6},
pages={46-53},
abstract={Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article "Big Data's Big Unintended Consequences" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.},
keywords={Information management;Data handling;Data storage systems;Government policies;Databases;Business;Legal aspects;Data privacy;policy;privacy;data;social impact;big data;private data commons},
doi={10.1109/MC.2013.195},
ISSN={1558-0814},
month={June},}
@INPROCEEDINGS{9607151,
author={Yang, Shenghong},
booktitle={2020 2nd International Conference on Applied Machine Learning (ICAML)},
title={In The Era of Big Data, Training the Practical Innovation Ability of Applied Talents with the Carrier of Mathematical Modeling Competition},
year={2020},
volume={},
number={},
pages={224-227},
abstract={In recent years, with the gradual development of innovative technology, innovative technology has become the main driving force for the development of mathematics major in China. With China gradually stepping into the era of big data, it cultivates a large number of mathematics professionals for China through the use of innovative technology. This paper aims to train a large number of outstanding talents for Our country by adopting mathematics competition mode, and improve students' data processing ability under the background of big data, so as to comprehensively improve students' mathematical innovation ability. This paper mainly takes data modeling contest as the carrier and conducts analysis and research from three aspects: competition platform, teacher formulation and student management, so as to cultivate innovative talents in mathematics. This paper also builds a mathematical innovation practice platform by cultivating students' innovative thinking ability and practical ability, so as to improve students' mathematical innovation ability and cultivate high-quality mathematical talents.},
keywords={Training;Technological innovation;Solid modeling;Force;Machine learning;Big Data;Solids;big data;mathematical modeling contest;innovative application;talent training},
doi={10.1109/ICAML51583.2020.00054},
ISSN={},
month={Oct},}
@ARTICLE{8658160,
author={Singh, Nitin and Lai, Kee-Hung and Vejvar, Markus and Cheng, T. C. E.},
journal={IEEE Engineering Management Review},
title={Big Data Technology: Challenges, Prospects, and Realities},
year={2019},
volume={47},
number={1},
pages={58-66},
abstract={We attempt to demonstrate the value of big data to enterprises by interweaving the perceptions, challenges, and opportunities of big data for businesses. While enterprises are aware of the value of big data to their businesses, there are challenges of exploiting big data in terms of data quality and usage. Executives might lack knowledge on how applications are related to one another in the big data ecosystem and the business benefits to reap. We summarize the results of a research study that explores emerging business perceptions of big data. We examine the current practice in 20 large enterprises, each having an annual revenue of more than USD 0.5 billion and present our findings related to executive perceptions. We provide insights on how firms can develop their big data expertise along various dimensions and identify critical ideas to be further investigated to better understand the issues that practitioners and researchers might be equally grappling with.},
keywords={Big Data;Data integrity;Data analysis;Software;Investment;Companies;Big data;business analytics;Hadoop;people;talent gap;implementation},
doi={10.1109/EMR.2019.2900208},
ISSN={1937-4178},
month={Firstquarter},}
@INPROCEEDINGS{8464790,
author={Hsseinoiun, Sara and Abdullah, Rusli and Jusoh, Yusmadi Yah and Jabar, Marzanah},
booktitle={2018 Fourth International Conference on Information Retrieval and Knowledge Management (CAMP)},
title={Information System Success and Knowledge Grid Integration in Facilitating Knowledge Sharing Among Big Data Community},
year={2018},
volume={},
number={},
pages={1-5},
abstract={Nowadays many domains interested to use big data to improve their decision making, strategic planning, and productivity while communication infrastructure supports various applications for users to share their resources, ideas or experiences. However, accessing, managing, analyzing, and using the rapidly expanding big data had raised challenges especially because of dispersed and heterogeneous nature of data. This review aimed to identify the factors to influence facilitating knowledge sharing among big data community from user satisfaction aspect and the way knowledge grid effect big data sharing by Delone and Maclean model for information system success. The research analysis and results revealed three features of knowledge grid which may influence information and system quality which defined as main factors affect knowledge sharing among the community from member's aspect. It also demonstrates seven factors to measure the facilitating knowledge sharing from community member's satisfaction view and quantity of knowledge sharing. Finally, the review established an initial conceptual model for facilitating knowledge sharing among big data which in follow up research will review by domain experts.},
keywords={Big Data;Data models;Knowledge management;Usability;Decision making;knowledge sharing;knowledge grid;big data community},
doi={10.1109/INFRKM.2018.8464790},
ISSN={},
month={March},}
@INPROCEEDINGS{8858666,
author={Yong, Yang},
booktitle={2019 11th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)},
title={Research on Intelligent Evaluation Method of Building Construction Quality Based on Big Data Under the Influence of Multiple Factors},
year={2019},
volume={},
number={},
pages={574-580},
abstract={In order to solve the problems of large errors and low consistency in traditional evaluation methods, the intelligent evaluation method of building construction quality is studied in a multi-factor environment combined with big data method. By calculating the index weight coefficient of building construction quality evaluation elements, the consistency index parameters, diversity index and randomness index of building construction quality are obtained. According to the index, the evaluation grade of building construction quality evaluation index is divided, the building construction quality evaluation index system is established, and the evaluation steps of the evaluation index system are optimized. Finally, the research requirement of accurately evaluating the construction quality under the influence of multiple factors is met. Experiments prove that the intelligent evaluation method of building construction quality based on big data under the influence of multiple factors has relatively higher consistency and smaller error compared with the evaluation results of traditional methods, which fully meets the research requirements.},
keywords={Conferences;Q measurement;Mechatronics;Automation;multi-factor influence;Big data;Construction;Intelligent assessment of quality},
doi={10.1109/ICMTMA.2019.00132},
ISSN={2157-1481},
month={April},}
@ARTICLE{9099505,
author={Gu, Lin and Hu, Jie and Zeng, Deze and Guo, Song and Jin, Hai},
journal={IEEE Transactions on Network Science and Engineering},
title={Service Function Chain Deployment and Network Flow Scheduling in Geo-Distributed Data Centers},
year={2020},
volume={7},
number={4},
pages={2587-2597},
abstract={Network Function Virtualization (NFV), as an emerging solution to virtualize network services traditionally running on proprietary, dedicated devices, can effectively reduce the cost of big data processing service providers and improve Quality of Service (QoS) by running a chain of ordered Virtual Network Functions (VNFs) on commodity hardware. One fundamental and critical problem of big data processing with NFV is how to deploy the chained VNFs and dispatch corresponding network flows to process the big data traffics so that the service cost can be minimized with guaranteed QoS. In this paper, we study the problem of VNF deployment and flow scheduling in distributed data centers with joint consideration of the service requirements and the resource capacity, and prove its NP-hardness through reduction from the k-level uncapacitated facility location problem. A two-phased algorithm is also devised by first balancing VNF resource requirements and then selecting VNF locations with a low complexity of O(M2log2M) and an approximation ratio of K + ρ. Finally, with extensive simulation experiments, the result shows that our algorithm can efficiently reduce the total cost of VNF deployment and flow communication in various scenarios.},
keywords={Data centers;Big Data;Noise measurement;Approximation algorithms;Quality of service;Servers;Virtualization;Network function virtualization;SFC deployment;flow scheduling},
doi={10.1109/TNSE.2020.2997376},
ISSN={2327-4697},
month={Oct},}
@INPROCEEDINGS{8034978,
author={Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua and Gudivada, Venkat},
booktitle={2017 IEEE International Conference on Services Computing (SCC)},
title={Building a Deep Learning Classifier for Enhancing a Biomedical Big Data Service},
year={2017},
volume={},
number={},
pages={140-147},
abstract={Providing an easily accessible data service with high quality data is important for building big data applications. In this paper, we introduce a big data service for managing and accessing massive-scale biomedical image data. The service includes three major components: a NoSQL database for storing images and data analytics results, a client consisting of a group of query scripts for data access and management, and a data quality enhancement component for improving the performance of data analytics. Low-quality data can result in incorrect analytics results and may lead to no value even harmful conclusions. Therefore, it is important to provide an effective mechanism for ensuring data quality improvement in a big data service. We describe the implement ion of a deep learning classifier to automatically filter low quality data in datasets. To improve the effectiveness of data separation, the classifier is rigorously validated with synthetic data generated by a collection of scientific tools. Design of big data services with data quality improvement as an integral component, along with the best practices collected from this experimental study, will help researchers and practitioners to develop strategies for improving the quality of big data services, building big data applications, and designing machine learning classifiers.},
keywords={Diffraction;Big Data;Machine learning;Morphology;Support vector machines;Three-dimensional displays;Noise measurement;machine learning;deep learning;big data;data quality;cross-validation},
doi={10.1109/SCC.2017.25},
ISSN={2474-2473},
month={June},}
@INPROCEEDINGS{8465422,
author={Abed, Sa'ed and Shubair, Duha S.},
booktitle={2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)},
title={Enhancement of Task Scheduling Technique of Big Data Cloud Computing},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Big Data refers to the large size chunks of data that traditional computing approaches can handle. Despite the truth of having huge cloud systems to manage this data nowadays, there are many challenges related to performing the tasks in the cloud within the expected timeframe using the minimum number of resources possible. The necessity to fulfil user requirements is the main reason of having studies for optimizing the cloud computing of big data in terms of latency, bandwidth, execution time and resource utilization. Therefore, we proposed an efficient task scheduling technique capable to manage big data processing and storage in the cloud in an efficient way that meets user expectations. We provide a solution that involves multiple number of metrics necessary to optimize the solution of big data cloud computing. Our designed model consists of multiple control nodes that control the work done on multiple compute nodes. We used a load balancing algorithm to manage task scheduling on the compute nodes so we make sure that all nodes have equal balance of loads at all times. We simulate different scenarios to prove the concept of the study including latency, task execution time, bandwidth and resource utilization. This study achieved 31.4% as an average decrease percentage in task execution time and this has led to 11.36% utilization of resources.},
keywords={Task analysis;Cloud computing;Processor scheduling;Big Data;Scheduling;Quality of service;Load modeling;Big data;latency;load balancing},
doi={10.1109/ICABCD.2018.8465422},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8920730,
author={Yu, Ping and Yan, Hui and Liu, Xinzheng and Liu, Li},
booktitle={2019 International Conference on Virtual Reality and Intelligent Systems (ICVRIS)},
title={Research on Construction of Evaluation Index System of Agricultural Professional Managers for Big Data},
year={2019},
volume={},
number={},
pages={400-403},
abstract={The application of big data technology in agriculture is the core development direction to realize the maximum utilization of agricultural data information. It is an important task to cultivate high-quality agricultural talents to promote the sustainable and rapid development of intelligent agriculture. Based on the demand side and supply side of intelligent agricultural education system, the big data platform of evaluation index system for professional agricultural managers is constructed. Firstly, this paper studies the application of big data technology in the evaluation index system. Then a rational and effective evaluation index system of agricultural professional managers is constructed by using the network analytic hierarchy process and the grey theory model.},
keywords={Indexes;Big Data;Agriculture;Data models;Analytical models;Education;evaluation index system, professional manager, big data, intelligent agriculture},
doi={10.1109/ICVRIS.2019.00103},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8560980,
author={Yang, Mary Qu and Yu, Shucheng and Cruz-Neira, Carolina and Yang, William and Tudoreanu, M. Eduard and Li, Dan and Zhang, Yifan and He, Qingfang and Guan, Renchu and Wang, Richard Y. and Zhao, Wenbing},
booktitle={2017 International Conference on Computational Science and Computational Intelligence (CSCI)},
title={Secure Privacy Preserving across Personal Health Data and Single Cell Genomics Research INSPIRE Academic Pedagogy — Merging Big Data Multiplatform with Deep Learning},
year={2017},
volume={},
number={},
pages={1244-1251},
abstract={Enhancing student academic performance and transdisciplinary ability is challenging, but the time and effort put into accomplishing this ambitious feat is priceless. We develop secure privacy preserving across Personal Health Data (PHD) repository and single-cell genomics research for building an Innovative Systematic Pedagogy for Integrated Research - Education (INSPIRE) (http://americancse.org/events/csce2017/csce17_awards). In this paper we further build a novel, eclectic, and insightful framework based on classical and popular machine learning approaches to help us meet the educational challenge. Our framework focuses on using integrative research technologies to help solve “Education's Performance Prediction Data Mining Crisis” (EPPDMC), by putting to rest issues associated with mining and making best use of big data for educational enhancement, such as multi-source education acquisition, data fusion, and unstructured data analysis. We exploit the uses of deep learning, text classification, and semi-supervised learning approaches to solve challenging problems that educators face when analyzing multiplatform big data involved in education, research and training students. Based on new machine learning approached we developed for genomic big-data research and in combination with machine learning methods (http://americancse.org/events/csce2017/keynotes_lectures/yang_talk) and the vast availability of education data available to us, not only can we utilize structured, unstructured, and even multi-media data, but while engaging in leaning intelligent thinking along the way, we can also maximize the utilization of big data by studying the motion and performance of these data. Hence we build the INSPIRE model that can further incorporate Student Face Expression in Class (SFEiC) to help educators and managers make further improvements as they become involved in the teaching-learning process. This research further facilitates the effectiveness of the INSPIRE model.},
keywords={Secure privacy preserving across Personal Health Data (PHD);Single-cell genomics;Innovative Systematic Pedagogy for Integrated Research and Education (INSPIRE);Deep Learning},
doi={10.1109/CSCI.2017.219},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9407963,
author={Xia, Qiufen and Ren, Wenhao and Li, Mingchu and Ren, Jiankang},
booktitle={2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
title={Age-Aware Query Evaluation for Big Data Analytics in Mobile Edge Clouds},
year={2020},
volume={},
number={},
pages={214-222},
abstract={The advancement of 5G technology and extensive usage of smart devices have led to a deluge of data. It is invaluable to analyze such big data by issuing queries to derive real-time business insights for better, smarter and fact-based decisions. Timely big data analytics are crucial in many service domains, thus the age of data (AoD) required by queries is emerging as a novel metric that measures the freshness of data and evaluates the quality of data analytics. Traditional big data analytics that are performed in remote clouds cannot satisfy the AoD requirements of queries, due to the congested core networks and long transmission latency between users and clouds. The technique of mobile edge computing (MEC) is expected to reduce the age and guarantee the timeliness of queries for big data analytics, by processing data at edge cloudlets close to users. In this paper, we investigate a problem of age-aware query evaluation for big data analytics in a mobile edge cloud network. We first formulate the problem with certain data processing latency as an Integer Linear Program (ILP). We then develop an online learning algorithm for the problem with uncertain data processing latency. We finally evaluate the performance of proposed algorithms against existing studies by simulations and testbed implementations. Evaluation results show that the proposed algorithms outperform existing works by achieving 20% lower AoD.},
keywords={Measurement;Data analysis;Query processing;High performance computing;Conferences;Big Data;Real-time systems;Age of data;Mobile edge cloud;Big data analytics;Query evaluation;Online learning},
doi={10.1109/HPCC-SmartCity-DSS50907.2020.00027},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8622624,
author={Bao, Shunxing and Parvarthaneni, Prasanna and Huo, Yuankai and Barve, Yogesh and Plassard, Andrew J. and Yao, Yuang and Sun, Hongyang and Lyu, Ilwoo and Zald, David H. and Landman, Bennett A. and Gokhale, Aniruddha},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Technology Enablers for Big Data, Multi-Stage Analysis in Medical Image Processing},
year={2018},
volume={},
number={},
pages={1337-1346},
abstract={Big data medical image processing applications involving multi-stage analysis often exhibit significant variability in processing times ranging from a few seconds to several days. Moreover, due to the sequential nature of executing the analysis stages enforced by traditional software technologies and platforms, any errors in the pipeline are only detected at the later stages despite the sources of errors predominantly being the highly compute-intensive first stage. This wastes precious computing resources and incurs prohibitively higher costs for re-executing the application. The medical image processing community to date remains largely unaware of these issues and continues to use traditional high-performance computing clusters, which incur a high operating cost due to the use of dedicated resources and expensive centralized file systems. To overcome these challenges, this paper proposes an alternative approach for multi-stage analysis in medical image processing by using the Apache Hadoop ecosystem and offering it as a service in the cloud. We make the following contributions. First, we propose a concurrent pipeline execution framework and an associated semi-automatic, real-time monitoring and checkpointing framework that can detect outliers and achieve quality assurance without having to completely execute the expensive first stage of processing thereby expediting the entire multi-stage analysis. Second, we present a simulator to rapidly estimate the execution time for a given multi-stage analysis, which can aid the users in deciding the appropriate approach for their use cases. We conduct empirical evaluation of our framework and show that it requires 76.75% lesser wall time and 29.22% lesser resource time compared to the traditional approach that lacks such a quality assurance mechanism.},
keywords={Biomedical image processing;Pipelines;Cloud computing;Biomedical imaging;Quality assurance;Big Data;Ecosystems;Hadoop;Medical image processing;Big data multistage analysis;Simulator},
doi={10.1109/BigData.2018.8622624},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9378094,
author={Chen, Zechuan and Yu, Haomin and Geng, Yangli-ao and Li, Qingyong and Zhang, Yingjun},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={EvaNet: An Extreme Value Attention Network for Long-Term Air Quality Prediction},
year={2020},
volume={},
number={},
pages={4545-4552},
abstract={Air quality affects social activities and human health. Air quality prediction, especially for extreme events such as severe haze pollution, plays an essential guiding role in government decision-making and outdoor activity scheduling. Established prediction models face the challenges of forecasting extreme values and long-term tendency. In this paper, we propose an extreme value attention network (EvaNet) based on encoder and decoder framework to achieve long-term air quality prediction. This model designs an extreme value attention mechanism to alleviate the impact of sudden changes on prediction. In addition, to capture long-term dependence relationships, EvaNet introduces a temporal attention mechanism. Integrating the dual attention mechanisms, the extracted features are fed into a decoder to yield the final prediction. The experiments evaluated on two real-world air quality datasets show the superiority of our method against other state-of-the-art baselines.},
keywords={Pollution;Atmospheric modeling;Predictive models;Big Data;Air quality;Feature extraction;Decoding;Air quality;Data mining;Forecasting;Deep learning;Attention mechanism},
doi={10.1109/BigData50022.2020.9378094},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8258090,
author={Stojanovic, Nenad and Dinic, Marko and Stojanovic, Ljiljana},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={A data-driven approach for multivariate contextualized anomaly detection: Industry use case},
year={2017},
volume={},
number={},
pages={1560-1569},
abstract={Anomaly detection is the process of discovering some anomalous behaviour in the real-time operation of a system. It is a difficult task, since in a general case (multivariate anomaly detection) an anomaly can be related to the behaviour of several parameters which are not necessarily behaving anomalously per se, but their (complex) relation is anomalous (not usual/normal). This implies the need for a very efficient modeling of the normal behaviour in order to know what should be treated as anomalous/outlier/unusual. Consequently, classical model-driven approaches, due to their focus on the selected parameters for creating models, are not able to model the behaviour of the whole system. This is why data-driven approaches for anomaly detection are getting even more important for the industry use cases where hundreds (thousands) of parameters should be taken into account. However, current approaches are usually focused on the univariate anomaly detection (or some variations of it), so without observing the entire space of relations since the computation is very complex. In this paper we present a novel approach for the multivariate anomaly detection that is based on modeling and managing the streams of variations in a multidimensional space. The main advantage of this approach is the possibility to observe the relations between variations in a large set of parameters and create clusters of “normal/usual” variations. In order to ensure scaling, which is one of the most challenging requirements, the approach is based on the usage of the big data technologies for realizing data analytics tasks/calculations. The approach is realized as a part of D2Lab (Data Diagnostics Laboratory) framework and has been applied in several industrial use cases. In this paper we present an interesting usage for the anomaly detection in the process of functional testing of home appliances (in particular case refrigerators) after manufacturing/assembling process. It has been done for a big vendor (Whirlpool), who expects huge saving in testing and improved customer satisfaction from this approach.},
keywords={Anomaly detection;Big Data;Clustering algorithms;Industries;Real-time systems;Testing;anomaly detetction;big data analytics;scalability quality control process},
doi={10.1109/BigData.2017.8258090},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9384654,
author={Linlin, Liu},
booktitle={2020 International Conference on Modern Education and Information Management (ICMEIM)},
title={To Comprehensively Improve Students' Professional Quality under the Guiding Role of the Communist Party in the Context of Big Data},
year={2020},
volume={},
number={},
pages={214-217},
abstract={Big data can summarize and discover massive information. Under the background of modernization, vocational colleges play the leading role of the communist party and strengthen the participation of big data in student management, which is conducive to improving the scientific and effective management of students, it can better help relevant personnel use big data for student learning and life.},
keywords={Education;Big Data;Information management;Personnel;big data;party leadership;professional literacy;student literacy},
doi={10.1109/ICMEIM51375.2020.00056},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7363827,
author={Xie, Sihong and Hu, Qingbo and Zhang, Jingyuan and Gao, Jing and Fan, Wei and Yu, Philip S.},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Robust crowd bias correction via dual knowledge transfer from multiple overlapping sources},
year={2015},
volume={},
number={},
pages={815-820},
abstract={One of the largest constituents of big data is the crowdsourced or user-generated data which contain a wide range of valuable information. However, they are inherently biased and possibly spammed, making trustworthy information extraction an imperative task. As a special case, we study reviewer-posted ratings for products. The biased ratings can lead to disappointed customers due to overrated products, and reduced revenues of business owners caused by undeserved negative ratings. To distill objective product quality measurements, most existing methods try to infer unbiased ratings from the raw ratings alone, and may not overcome the inherent bias to recover the underlying true ratings. Though improved bias corrections have been achieved with domain expert helps, the overhead of expert efforts can be rather expensive in practice. We exploit the variety of big data and adopt a multiple source mining approach, which finds trustworthy measurements without domain expert, but with knowledge crowdsourced and transferred from external domains. We address the challenges that the multiple data sources are 1) inherently heterogeneous, 2) at most only partially overlapping and 3) biased by themselves. We explore and analyze the strengths and weaknesses of various knowledge transfer strategies. We then propose Consensus Ranking Dual Transfer (CRDT) to handle the above challenges by identifying "anchor reviewers" as a bridge for robust "dual transfer", and removing bias in individual sources via consensus ranking aggregation. Experiments on real-world rating datasets demonstrate that the proposed approach can deliver more robust bias correcting effects than the baselines and can identify abnormal reviewers.},
keywords={Quality assessment;Product design;Big data;Robustness;Motion pictures},
doi={10.1109/BigData.2015.7363827},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8367196,
author={Kim, Cheol-Jin and Park, Sung-Hoon and Ha, Young-Guk},
booktitle={2018 IEEE International Conference on Big Data and Smart Computing (BigComp)},
title={Correlation Analysis Between Vehicular Traffic and PM Using Sensor Big Data},
year={2018},
volume={},
number={},
pages={644-648},
abstract={Air pollution has a negative impact on human life around the world. Particularly, researches on PM, which is a harmful air pollution source, are being actively carried out these days. To help these environmental problems, we propose the system through this paper. The proposed system is a useful system for analyzing environmental problems related to air pollution. It uses various information collectively to generate high quality data for analysis. In addition, we analyzed the correlation between PM and vehicular traffic in the Daejeon area. As a result, we found that the speed of the vehicle has a greater effect on the road PM value than the number of vehicles.},
keywords={Atmospheric measurements;Roads;Pollution measurement;Correlation;Urban areas;Air pollution;Area measurement;atmospheric analysis;big data processing;correlation anlysis;spatio temporal sampling;analysis of fine dust and vehicular traffic},
doi={10.1109/BigComp.2018.00116},
ISSN={2375-9356},
month={Jan},}
@INPROCEEDINGS{9443925,
author={Liang, SHEN and Baozhong, HAO and Yang, LI and Han, YU and Song, ZHANG and Hao, MEN and Jingang, FAN},
booktitle={2020 International Conference on Computer Science and Management Technology (ICCSMT)},
title={Blockchain-based Power Grid Data Asset Management Architecture},
year={2020},
volume={},
number={},
pages={207-211},
abstract={Although the continuous construction of smart grid and power Internet of Things has brought massive data to grid enterprises, the cost of data storage, operation and maintenance is gradually increasing, on the contrary, the value of data has not been effectively mined and utilized, the main reason is that the current data asset management of State Grid Corporation of China is still an infancy stage, and still faces the problems of data aggregation quality, safety and compliance control, shared application scope, and management mechanism efficiency and so on. In order to solve the above problems, Big Data Center of State Grid Corporation of China takes advantage of the blockchain with distributed consensus autonomy and data storage, non-tampering and traceability, and business intelligence contract script, explores the application of blockchain in power grid data asset management, and then, tries to construct the architecture of power grid data asset management based on blockchain, which is to promote security compliance and open sharing of power grid data operation, and realize the efficient management and operation of power grid data assets.},
keywords={Distributed databases;Collaboration;Memory;Blockchain;Computer architecture;Power grids;Asset management;blockchain;power grid data asset management;solution architecture;security compliance;open sharing},
doi={10.1109/ICCSMT51754.2020.00049},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8597313,
author={Sheshasaayee, Ananthi and Malathi, S.},
booktitle={2018 International Conference on Inventive Research in Computing Applications (ICIRCA)},
title={Challenges Associated with Quality and Big Data Tool Based Study in Blended Learning Models},
year={2018},
volume={},
number={},
pages={695-699},
abstract={The main aim of this study focus on blended learning and its ways of maintaining the quality studies based on various blended models. Most of the issues faced by students and teachers and the ways of resolving forms the discussion of the paper. As blended learning is based on traditional and face-to -face method, teaching and learning components such as assessment including prior knowledge of the topic, evaluation methods of assessment and appeals, identification of barriers such as language, Blended learning (BL) literacy & needs, fees structure of the course, balanced workload, recognition towards achievement, monitoring continuous improvement of individual student etc., forms the quality along with different challenges and the ways of reaching the solution. It is more evident that big data which is an innovative driving tool in education supports teaching quality with various impacts on individual student long term achievement.},
keywords={Education;Tools;Big Data;Computational modeling;Face;Schedules;Monitoring;BL (Blended Learning);MOODLE;QI;QA;BD(Big Data);Value Added (VA)},
doi={10.1109/ICIRCA.2018.8597313},
ISSN={},
month={July},}
@INPROCEEDINGS{9532187,
author={Di, Ding and Guang, Li and Zhu-shi, He and Xiao-qiong, Liu and Hao-hao, Zheng and Tian-ze, Li},
booktitle={2020 7th International Conference on Information Science and Control Engineering (ICISCE)},
title={Controlled Source Electromagnetic Data Denoising Based On CEEMD and Correlation Analysis},
year={2020},
volume={},
number={},
pages={2336-2338},
abstract={Controlled-source electromagnetic method (CSEM) signals are inevitably contaminated by man-made noises. For this reason, a new CSEM data processing method was proposed. First, the 50 Hz powerline interference was removed by FFT method, followed by the CEEMD method to remove baseline drift, and finally the correlation analysis method was used to select the high-quality signal. The method was applied to the processing of measured data in Huidong, Sichuan. As a conclusion, the presented method can effectively remove the strong cultural noise from raw CSEM data and preserve the useful signal completely; the apparent resistivity curves acquired by using the filtered data are improved significantly upon the previous.},
keywords={Correlation;Power measurement;Noise reduction;Process control;Interference;Signal processing;Data processing;CSEM data;De-noising;Signal-noise separation;CEEMD;Correlation Analysis},
doi={10.1109/ICISCE50968.2020.00458},
ISSN={},
month={Dec},}
@ARTICLE{9162126,
author={Expósito, Roberto R. and Galego-Torreiro, Roi and González-Domínguez, Jorge},
journal={IEEE Access},
title={SeQual: Big Data Tool to Perform Quality Control and Data Preprocessing of Large NGS Datasets},
year={2020},
volume={8},
number={},
pages={146075-146084},
abstract={This paper presents SeQual, a scalable tool to efficiently perform quality control of large genomic datasets. Our tool currently supports more than 30 different operations (e.g., filtering, trimming, formatting) that can be applied to DNA/RNA reads in FASTQ/FASTA formats to improve subsequent downstream analyses, while providing a simple and user-friendly graphical interface for non-expert users. Furthermore, SeQual takes full advantage of Big Data technologies to process massive datasets on distributed-memory systems such as clusters by relying on the open-source Apache Spark cluster computing framework. Our scalable Spark-based implementation allows to reduce the runtime from more than three hours to less than 20 minutes when processing a paired-end dataset with 251 million reads per input file on an 8-node multi-core cluster.},
keywords={Tools;Quality control;Big Data;Bioinformatics;Sparks;Sequential analysis;Acceleration;Big data;next-generation sequencing (NGS);bioinformatics;quality control;apache spark},
doi={10.1109/ACCESS.2020.3015016},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7809598,
author={Kläs, Michael and Putz, Wolfgang and Lutz, Tobias},
booktitle={2016 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement (IWSM-MENSURA)},
title={Quality Evaluation for Big Data: A Scalable Assessment Approach and First Evaluation Results},
year={2016},
volume={},
number={},
pages={115-124},
abstract={High-quality data is a prerequisite for most types of analysis provided by software systems. However, since data quality does not come for free, it has to be assessed and managed continuously. The increasing quantity, diversity, and velocity that characterize big data today make these tasks even more challenging. We identified challenges that are specific for big data quality assessments with particular emphasis on their usage in smart ecosystems and make a proposal for a scalable cross-organizational approach that addresses these challenges. We developed an initial prototype to investigate scalability in a multi-node test environment using big data technologies. Based on the observed horizontal scalability behavior, there is an indication that the proposed approach also allows dealing with increasing volumes of heterogeneous data.},
keywords={Big data;Quality assessment;Metadata;Instruments;Ecosystems;Companies;big data quality assessment;quality measurement;velocity;volume;variety;SQA4BD;QUAMOCO;smart ecosystems;SPARK;HADOOP},
doi={10.1109/IWSM-Mensura.2016.026},
ISSN={},
month={Oct},}
@ARTICLE{8536432,
author={Feng, Jun and Yang, Laurence T. and Zhang, Ronghao and Zhang, Shunli and Dai, Guohui and Qiang, Weizhong},
journal={IEEE Transactions on Sustainable Computing},
title={A Tensor-Based Optimization Model for Secure Sustainable Cyber-Physical-Social Big Data Computations},
year={2020},
volume={5},
number={2},
pages={223-234},
abstract={Secure cyber-physical-social big data computations are being increasingly used to protect the users' data security in cyber-physical-social systems (CPSS). Despite the increasing popularity, how to process the tasks of the secure cyber-physical-social big data computations, while taking care of the energy consumption and meeting the users' requirements, remains challenging. To address the problem, in this work, we propose a novel tensor-based optimization model for the secure sustainable cyber-physical-social big data computations. The proposed model is a general and fine-grained model, which can jointly optimize the execution time, energy consumption, reliability, and quality of experience, and can comprehensively take into account step, task, time slot, type, node, core, cryptosystem, and security level. To our knowledge, this is the first study to holistically optimize the tasks in the secure cyber-physical-social big data computations. To illustrate the proposed model, the case study of the secure high-order Lanczos in cloud-assisted CPSS is presented. Finally, the proposed model is empirically evaluated by using multi-objective optimization, and the extensive results demonstrate that from the users' perspective our proposed tensor-based optimization model is preferable for the secure sustainable cyber-physical-social big data computations.},
keywords={Big Data;Tensile stress;Optimization;Computational modeling;Quality of experience;Energy consumption;Cryptography;Data security;secure cyber-physical-social systems;energy consumption;big data;tensor computation;cloud computing},
doi={10.1109/TSUSC.2018.2881466},
ISSN={2377-3782},
month={April},}
@INPROCEEDINGS{9115188,
author={Wu, Hao and Liu, Qian and Zhang, Zhifang},
booktitle={2020 Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)},
title={Analysis of University Students Employment Recommendation System Based on Apriori Algorithm},
year={2020},
volume={},
number={},
pages={262-265},
abstract={“Employment Recommendation System for College Students Based on Apriori Algorithm”, which has taken the big data of the web recruitment platform in China as its technology and service foundation, has adopted Apriori recommendation algorithm to restructure relevant quality information on employment intelligently by data analyzing information on employment of all large recruitment platforms. The system has achieved the accurate employment recommendation for college students, which has improved not only the job satisfaction and employment rate of college students, but the work efficiency of employment service for college students.},
keywords={Computers;Itemsets;Databases;Image processing;Employment;Filtering algorithms;Big Data;apriori algorithm;data analysis;employment recommendation},
doi={10.1109/IPEC49694.2020.9115188},
ISSN={},
month={April},}
@INPROCEEDINGS{7840729,
author={Xu, Yanan and Zhu, Yanmin},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={When remote sensing data meet ubiquitous urban data: Fine-grained air quality inference},
year={2016},
volume={},
number={},
pages={1252-1261},
abstract={With the growth of the economy, the air quality is becoming a serious issue, especially for those developing countries, such as China. Therefore, it is very important for the public and the government to access real-time air quality information. Unfortunately, the limited number of air quality monitoring stations is unable to provide fine-grained air quality information in a huge city, such as Beijing. One cost-effective approach for obtaining fine-grained air quality information is to infer air quality with those measured data at the monitoring stations. However, existing inference techniques have poor performance because of the extreme data sparsity problem (e.g., only 0.2% data are known). We observe that remote sensing has been a high-quality data source about urban dynamics. In this paper, we propose to integrate remote sensing data and ubiquitous urban data for air quality inference. There are two main challenges, i.e., data heterogeneity and incomplete remote sensing data. In response to the challenges, we propose a two-stage inference approach. In the first stage, we use the AOT remote sensing data and the meteorological data to infer the air quality values with an Artificial Neural Network (ANN). After this stage, we significantly reduce the percentage of empty cells in the tensor representing the spatio-temporal air quality values. In the second stage, we propose a tensor decomposition method to infer the complete set of air quality values. We use the spatial features (i.e., road features and POI features) and the temporal features (i.e., meteorological features) as the constraints in the tensor decomposition process. Experiments with real data sets show that our approach has profound performance advantage over the state-of-the-art methods, such as U-Air.},
keywords={Air quality;Remote sensing;Monitoring;Tensile stress;Sensors;Urban areas;Pollution measurement;Remote sensing;AOT;air quality;inference;neural network;tensor decomposition},
doi={10.1109/BigData.2016.7840729},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8900190,
author={Han, Weiguo and Jochum, Matthew},
booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium},
title={Practices and Experiences in High Volumes of Satellite Data Management},
year={2019},
volume={},
number={},
pages={4364-4367},
abstract={High volumes of satellite data management within an organization is still challenging and daunting in the era of big data. The increasing information technology costs and limited budgets, growing satellite data needs, data availability across multiple teams and projects, strategic goals of organization, and expected project outcomes require better satellite data management mechanism and system to facilitate research and development activities. An organization level centralized satellite data repository is a practical solution to satisfy these requirements. This paper describes the best practices and experiences from building such a central satellite data repository within our organization, including data management strategy and policy, scalable and extensible system infrastructure, comprehensive data management system, and technical support and user assistance. These practices can be borrowed and applied in other organizations with similar requirements.},
keywords={Satellites;Organizations;Research and development;Databases;Monitoring;Buildings;Data integrity;Satellite Data Management;Big Data;Data Quality;Central Data Repository},
doi={10.1109/IGARSS.2019.8900190},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{7107424,
author={Sneed, Harry M. and Erdoes, Katalin},
booktitle={2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)},
title={Testing big data (Assuring the quality of large databases)},
year={2015},
volume={},
number={},
pages={1-6},
abstract={The volume and variety of modern day databases presents a particular challenge to the system testing community. The question is how to go about testing such large collections of various data types ranging from tables to texts and images. To test those applications which use them, these conglomerations of multiple data object types have to be automatically generated and validated. There is no other way but to automate the test process. This contribution outlines the challenge and presents an automated approach to setting up and testing big data bases. At the end a case study of a large data warehouse is discussed with lessons learned from that industrial test project.},
keywords={Testing;Data warehouses;Big data;Relational databases;Database systems;Generators;mixed databases;relational data;text data;images;data warehouses;validation rules;data assertions;data testing tools},
doi={10.1109/ICSTW.2015.7107424},
ISSN={},
month={April},}
@INPROCEEDINGS{9202267,
author={Wang, Chen and Feng, Shuangchang},
booktitle={2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS)},
title={Research on Collection and Preprocessing of Multisource Heterogeneous Elevator Data},
year={2020},
volume={},
number={},
pages={490-493},
abstract={With the rapid development of China's economy, the elevator has become a basic tool in our daily life, and the safety risks brought by the elevator are also increasingly serious. It is necessary to use the technology of big data to mine the existing multi-source Heterogeneous elevator data, so as to find out the various rules of fault. Data collection and preprocessing are the important parts of big data mining and analysis. In this paper, through the research of collection and preprocessing of multi-source heterogeneous elevator data, the quality of inputting data which is needed for the data modeling later on is improved, so as to provide better service for elevator fault prediction.},
keywords={Elevators;Inspection;Maintenance engineering;Databases;Big Data;Data mining;big data;data collection;data preprocessing;multi-source Heterogeneous;elevator fault prediction},
doi={10.1109/ICPICS50287.2020.9202267},
ISSN={},
month={July},}
@INPROCEEDINGS{9045835,
author={Wang, Yu},
booktitle={2019 6th International Conference on Dependable Systems and Their Applications (DSA)},
title={Controllable Correlation Big Data Dynamic Prediction Model for Mobile Communication},
year={2020},
volume={},
number={},
pages={143-150},
abstract={At present, there were many mobile communication deviees in China. In order to improve the prediction effect of mobile communication and realize the scalability and dynamic evolution of the prediction system, a dynamic prediction model of controllable correlation big data for mobile communication was proposed. The application experiment was carried out, and the experimental results of various models were compared and analyzed. The experimental results showed that the model fully integrated the openness, extensibility and big data dynamic prediction advantages of mobile communication. The communication quality, total duration and quantitative qualitative prediction of con trollable correlation big data based on mobile communication in big data environment were realizedÇ4bstract).},
keywords={Analytical models;Correlation;Fluctuations;Scalability;Heuristic algorithms;Big Data;Predictive models;mobile communication;controllable relevance;big data environment;dynamic prediction(key words)},
doi={10.1109/DSA.2019.00025},
ISSN={},
month={Jan},}
@ARTICLE{9126812,
author={Mehmood, Erum and Anees, Tayyaba},
journal={IEEE Access},
title={Challenges and Solutions for Processing Real-Time Big Data Stream: A Systematic Literature Review},
year={2020},
volume={8},
number={},
pages={119123-119143},
abstract={Contribution: Recently, real-time data warehousing (DWH) and big data streaming have become ubiquitous due to the fact that a number of business organizations are gearing up to gain competitive advantage. The capability of organizing big data in efficient manner to reach a business decision empowers data warehousing in terms of real-time stream processing. A systematic literature review for real-time stream processing systems is presented in this paper which rigorously look at the recent developments and challenges of real-time stream processing systems and can serve as a guide for the implementation of real-time stream processing framework for all shapes of data streams. Background: Published surveys and reviews either cover papers focusing on stream analysis in applications other than real-time DWH or focusing on extraction, transformation, loading (ETL) challenges for traditional DWH. This systematic review attempts to answer four specific research questions. Research Questions: 1)Which are the relevant publication channels for real-time stream processing research? 2) Which challenges have been faced during implementation of real-time stream processing? 3) Which approaches/tools have been reported to address challenges introduced at ETL stage while processing real-time stream for real-time DWH? 4) What evidence have been reported while addressing different challenges for processing real-time stream? Methodology: A systematic literature was conducted to compile studies related to publication channels targeting real-time stream processing/joins challenges and developments. Following a formal protocol, semi-automatic and manual searches were performed for work from 2011 to 2020 excluding research in traditional data warehousing. Of 679,547 papers selected for data extraction, 74 were retained after quality assessment. Findings: This systematic literature highlights implementation challenges along with developed approaches for real-time DWH and big data stream processing systems and provides their comparisons. This study found that there exists various algorithms for implementing real-time join processing at ETL stage for structured data whereas less work for un-structured data is found in this subject matter.},
keywords={Real-time systems;Big Data;Libraries;Systematics;Data mining;Bibliographies;Quality assessment;Real-time stream processing;big data streaming;structured/un-structured data;ETL;systematic literature review},
doi={10.1109/ACCESS.2020.3005268},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8367655,
author={Zhang, Wei and Qin, Shiming},
booktitle={2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA)},
title={A brief analysis of the key technologies and applications of educational data mining on online learning platform},
year={2018},
volume={},
number={},
pages={83-86},
abstract={With the rapid development of the Internet and communication technology, online education has drawn more and more attention, online learning platforms, on the other hand, store massive learner behavioral data and educational data. How to effectively analyze and utilize the data to improve the quality of online education has become a key issue urgently needed to be solved in the field of big data in education(BDE), educational data mining(EDM) is exactly an effective and practical method and means of applying BDE. Therefore, EDM is an important academic research hotspot in the field of EDM. Firstly, the paper introduces the basic concepts of BDE, EDM and online learning platform, and then elaborates on the process of how educational data mining transforms raw data into knowledge. Finally, the key technologies of data mining are classified according to their uses, and gives its application in online education scene. The paper can provide some guidance for the research and application of educational data mining based on online education.},
keywords={Education;Data mining;Big Data;Data models;Prediction algorithms;Classification algorithms;Predictive models;component;educational data mining;online education;big data in education;online learning platforms},
doi={10.1109/ICBDA.2018.8367655},
ISSN={},
month={March},}
@INPROCEEDINGS{8817168,
author={Kavakli, Evangelia and Sakellariou, Rizos and Stankovski, Vlado},
booktitle={2019 IEEE World Congress on Services (SERVICES)},
title={Towards a Methodology for Evaluating Big Data Platforms},
year={2019},
volume={2642-939X},
number={},
pages={380-381},
abstract={In recent years, several new multipurpose Big Data platforms have emerged. They are used in various application domains with diverse requirements. Evaluating complex Big Data solutions is not a trivial task, due to the need to assess their utility in both quantitative and qualitative terms based on existing use cases. In this short paper, we discuss the requirements and the methodology for such an evaluation. We also discuss how benchmarking could be part of such an evaluation methodology.},
keywords={Big Data;Benchmark testing;Stakeholders;Phase measurement;Industries;Quality of experience;Big Data;Benchmarking},
doi={10.1109/SERVICES.2019.00113},
ISSN={2642-939X},
month={July},}
@INPROCEEDINGS{8780724,
author={Zhu, Xin and Geng, Shuqin and Hou, Yupeng and Peng, Xiaohong and Hou, Ligang and Zhang, Mingming and Chu, Menghao},
booktitle={2018 IEEE 4th International Conference on Computer and Communications (ICCC)},
title={Design and Implementation of Air Quality Data Processing System Based on Big Data Technology},
year={2018},
volume={},
number={},
pages={1846-1850},
abstract={In this article, we design and implement an air quality data processing system based on big data technology, which leverages Flume to acquire data, Kafka to cache data, and Storm to process data, and finally save the results to the in-memory database called Redis and distributed database called HBase. In Redis, only the latest data of each monitoring point is saved, and the original data is deleted periodically to reduce the memory load. An application is implemented by which people can get real-time data from Redis and query historical data from HBase. Through experiments, it is proved that querying real-time data from Redis is faster than relational database. The system solves the issues of poor storage capacity and slow query speed when using relational database in the big data environment, and improves the efficiency of data processing significantly.},
keywords={Monitoring;Real-time systems;Air quality;Big Data;Storms;Relational databases;air pollution;big data;Kafka;storm;Redis;HBase},
doi={10.1109/CompComm.2018.8780724},
ISSN={},
month={Dec},}
@ARTICLE{8416666,
author={Choi, Hyunjin and Gim, Jangwon and Seo, Young-Duk and Baik, Doo-Kwon},
journal={IEEE Access},
title={VPL-Based Big Data Analysis System: UDAS},
year={2018},
volume={6},
number={},
pages={40883-40897},
abstract={Over the past five years, research on big data analysis has been actively conducted, and many services have been developed to find valuable data. However, low quality of raw data and data loss problem during data analysis make it difficult to perform accurate data analysis. With the enormous generation of both unstructured and structured data, refinement of data is becoming increasingly difficult. As a result, data refinement plays an important role in data analysis. In addition, as part of efforts to ensure research reproducibility, the importance of reuse of researcher data and research methods is increasing; however, the research on systems supporting such roles has not been conducted sufficiently. Therefore, in this paper, we propose a big data analysis system named the unified data analytics suite (UDAS) that focuses on data refinement. UDAS performs data refinement based on the big data platform and ensures the reusability and reproducibility of refinement and analysis through the visual programming language interface. It also recommends open source and visualization libraries to users for statistical analysis. The qualitative evaluation of UDAS using the functional evaluation factor of the big data analysis platform demonstrated that the average satisfaction of the users is significantly high.},
keywords={Data analysis;Tools;Data visualization;Big Data;Statistical analysis;Synthetic aperture sonar;Data analysis;data visualization;reproducibility of results;clouds;data refinement;R},
doi={10.1109/ACCESS.2018.2857845},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7745338,
author={Alshawish, Raja A. and Alfagih, Salma A. M. and Musbah, Mohamed S.},
booktitle={2016 International Conference on Engineering & MIS (ICEMIS)},
title={Big data applications in smart cities},
year={2016},
volume={},
number={},
pages={1-7},
abstract={A “Smart City” generally means a technologically advanced city, that is able to understand its environment through analyzing its data so that it immediately makes changes to solve issues and to improve the residents' quality of life. The huge volume, high velocity and wide variety of city's data require the utilization of “Big Data” technologies to gain valuable insights from it. This paper reviews the applications and, hence, the potentials where Big Data technology can drive a city to be smart. Starting from investigating the visibility of the city, which means collecting data from all networks, devices and sensors that embedded in its infrastructure. Continuing to explain how can this data become valuable by passing different processing stages, and by applying advanced analyzing Big Data platforms on data. The smartness of the data-driven city is achieved by visualizing the data in useful shape in order to improve any city's system application. The review has also included few real world examples that shows the practical applications of Big Data in a Smart City in the domains of smart energy, smart public safety and smart traffic systems.},
keywords={Big data;Smart cities;Data mining;Data visualization;Real-time systems;Indexes;Big Data;Smart City;Hadoop},
doi={10.1109/ICEMIS.2016.7745338},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8456103,
author={Liu, Kai-Cheng and Kuo, Chuan-Wei and Liao, Wen-Chiuan and Wang, Pang-Chieh},
booktitle={2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)},
title={Optimized Data de-Identification Using Multidimensional k-Anonymity},
year={2018},
volume={},
number={},
pages={1610-1614},
abstract={In the globalized knowledge economy, big data analytics have been widely applied in diverse areas. A critical issue in big data analysis on personal information is the possible leak of personal privacy. Therefore, it is necessary to have an anonymization-based de-identification method to avoid undesirable privacy leak. Such method can prevent published data form being traced back to personal privacy. Prior empirical researches have provided approaches to reduce privacy leak risk, e.g. Maximum Distance to Average Vector (MDAV), Condensation Approach and Differential Privacy. However, previous methods inevitably generate synthetic data of different sizes and is thus unsuitable for general use. To satisfy the need of general use, k-anonymity can be chosen as a privacy protection mechanism in the de-identification process to ensure the data not to be distorted, because k-anonymity is strong in both protecting privacy and preserving data authenticity. Accordingly, this study proposes an optimized multidimensional method for anonymizing data based on both the priority weight-adjusted method and the mean difference recommending tree method (MDR tree method). The results of this study reveal that this new method generate more reliable anonymous data and reduce the information loss rate.},
keywords={Data privacy;Privacy;Numerical models;Big Data;Measurement;Data models;Greedy algorithms;privacy preserving;k-anonymity;de-identification;data quality;information loss},
doi={10.1109/TrustCom/BigDataSE.2018.00235},
ISSN={2324-9013},
month={Aug},}
@INPROCEEDINGS{7363850,
author={Chen, Jiaoyan and Chen, Huajun and Hu, Daning and Pan, Jeff Z. and Zhou, Yalin},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Smog disaster forecasting using social web data and physical sensor data},
year={2015},
volume={},
number={},
pages={991-998},
abstract={Smog disaster is a type of air pollution event that negatively affects people's life and health. Forecasting smog disasters may largely reduce potential loss that they may cause. However, it is a great challenge since smog disasters are often caused by many complex factors. With the availability of huge amounts of data from the social web and physical sensors, covering information of air quality, meteorology, social event, human mobility, people's opinion, etc., it becomes possible to utilize such big data to forecast smog disasters. Especially, we can investigate the effect of social activities in smog disaster forecasting with the help of social web, which is ignored in traditional studies. In this paper, we propose a big data approach named B-Smog for smog disaster forecasting. It mainly has two components: 1) features extraction from multiple data sources to model the factors that indicate the appearance or disappearance of a smog disaster like traffic condition, human mobility, weather condition and air pollution transportation; 2) learning and predicting with heterogeneous features in multiple views. For the second component, we propose a prediction model based on an ensemble learning framework and artificial neural networks (ANNs), which achieves high accuracy in this application and can also be applied to other similar problems. We present the effectiveness of B-Smog through two cases studies in Beijing and Shanghai, and evaluate the accuracy of the prediction model through comparing it with some baselines. Moreover, the empirical findings of our study can also support decision making in smog disaster management.},
keywords={Forecasting;Air pollution;Feature extraction;Atmospheric modeling;Meteorology;Predictive models;Big data;Smog Disaster Forecasting;Big Data;Social Media;Ensemble Learning;Feature Extraction},
doi={10.1109/BigData.2015.7363850},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8970744,
author={Huang, Xiaoqing and Liu, Qi and Wang, Chao and Han, Haoyu and Ma, Jianhui and Chen, Enhong and Su, Yu and Wang, Shijin},
booktitle={2019 IEEE International Conference on Data Mining (ICDM)},
title={Constructing Educational Concept Maps with Multiple Relationships from Multi-Source Data},
year={2019},
volume={},
number={},
pages={1108-1113},
abstract={Concept map is an useful tool to help people organize and improve knowledge. Particularly in educational domain, it is beneficial for students and teachers to improve the learning and teaching quality. Traditionally, manual educational concept maps, provided by teachers, are quite time-consuming and limited to teachers' experience. Thus, it is meaningful to automatically construct high-quality concept maps. However, existing data-driven solutions only focus on either separate data source or single pedagogic relationship, which are not sufficient to satisfy actual demands. To this end, we propose a novel framework, named Extracting Multiple Relationships Concept Map (EMRCM), to construct multiple relations concept maps from Multi-source Data. Specifically, we design various targeted evidences to explore diverse information of multi-source data from different perspectives. Then, we employ three classic classifiers to bulid the predictive model for extracting key concepts and multiple concept relationships using the proposed evidences. We create a real dataset for empirically studying this problem. Extensive experiments on a real-world dataset show the effectiveness of our method.},
keywords={Educational concept map;Multi-source data;Multiple relationships},
doi={10.1109/ICDM.2019.00132},
ISSN={2374-8486},
month={Nov},}
@INPROCEEDINGS{9360999,
author={Yu, Shan and Song, Chunyan},
booktitle={2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)},
title={Research on Real Estate Marketing Innovation System in the Era of Big Data},
year={2020},
volume={},
number={},
pages={309-312},
abstract={This article analyzes the importance of real estate marketing in the era of big data. The content of this article includes how to enhance the value of data application, the important means of reconstructing marketing strategies, and the inevitable trend of changes in the marketing system. Combining the content of real estate marketing strategy, the author studies how to analyze customer needs, do a good job of customer segmentation and clustering, optimize marketing strategy distribution models, improve the real estate marketing system, strengthen marketing effect evaluation, and improve the overall quality of personnel. The purpose of this article is to improve the applicability of the content of the system and accelerate the development of the real estate economy.},
keywords={Economics;Technological innovation;Biological system modeling;Machine learning;Market research;Personnel;Optimization;Big data Era;Real Estate;Marketing Innovation System;Product Strategy},
doi={10.1109/MLBDBI51377.2020.00066},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7820454,
author={Zhao, Junfeng and Wang, Yongmei and Xia, Yuanyi},
booktitle={2016 12th International Conference on Computational Intelligence and Security (CIS)},
title={Analysis of Information Security of Electric Power Big Data and Its Countermeasures},
year={2016},
volume={},
number={},
pages={243-248},
abstract={In recent years, big data technology has become the representative of new information technology. It is also playing an increasing important role in the electric power industry. With big data technology, electric power company introduces new business, like accurate load forecasting, user behavior analysis, etc. It helps make the service more quality and the upper level decisions more reasonable. This paper expounds the concept of big data. The characteristics of electric power big data and data processing steps are introduced as well. On this basis, the opportunities and challenges faced by big data are analyzed. The problems and countermeasures of big data are focused in this paper. The future development of big data security is summarized in the end. With the guarantee of technology and management, big data technology will have a bright future in the big data industry. It will provide strong support for improving enterprise efficiency.},
keywords={Big data;Security;Data privacy;Power systems;Companies;Industries;Safety;electric power big data;safety protection;data leakage;energy internet},
doi={10.1109/CIS.2016.0063},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8817094,
author={Christensen, Jeppe and Pontoppidan, Niels and Anisetti, Marco and Bellandi, Valerio and Cremonini, Marco},
booktitle={2019 IEEE World Congress on Services (SERVICES)},
title={Improving Hearing Healthcare with Big Data Analytics of Real-Time Hearing Aid Data},
year={2019},
volume={2642-939X},
number={},
pages={307-313},
abstract={Modern hearing aids are not simple passive sound enhancers, but rather complex devices that can log (via smartphones) multivariate real-time data from the acoustic environment of a user. In the evotion project (http://h2020evotion.eu) such hearing aids are integrated with a Big Data analytics platform to bring about ecologically valid evidence to support the hearing healthcare sector. Here, we present the background of the Big Data analytics platform and demonstrate that modeling of longitudinally sampled data from hearing aids can support clinical investigations with hypotheses about hearing aid usage prognosis, and support public health decision-making within the hearing healthcare sector by simulation techniques. We found, that distinct characteristics of the acoustic environment significantly modulate how hearing impaired individuals use their hearing aids. Higher sound levels and an increased sound diversity but degraded signal quality all predicts more minutes of use per hour. By simulation, we show that a projected increase in the overall sound levels by 10dB followed by a 4dB increase in noise exposure will increase the need for hearing aid use by an additional 1 hour/day across a population of hearing impaired hearing aid users.},
keywords={Task analysis;Big Data;Hearing aids;Data models;Auditory system;Acoustics;Predictive models;hearing aids;Big Data analytics;mixed models;multilevel clustered data;evidence based public health policies},
doi={10.1109/SERVICES.2019.00086},
ISSN={2642-939X},
month={July},}
@INPROCEEDINGS{8029334,
author={El Kassabi, Hadeel T. and Serhani, Mohamed Adel},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={De-Centralized Reputation-Based Trust Model to Discriminate between Cloud Providers Capable of Processing Big Data},
year={2017},
volume={},
number={},
pages={266-273},
abstract={Trust and reputation systems represent a significant trend in decision support including selection of best match cloud providers to process Big Data. Reputation is often considered as a collective measure of trustworthiness based on the referrals or ratings from members in a community. Reputation systems have been applied in various applications such as online service provision. However, reputation models do not reflect user's quality of service (QoS) preferences and thus they might not be satisfied with the recommendations from others. In this paper, we propose a de-centralized reputation-based trust model that incorporates the user QoS preferences to select the best match Cloud Service Provider to process Big Data. Our trust model relies on three multi-attribute decision-making (MADM) methods including Simple Additive Weighting (SAW), Weighted Product Method (WPM), and Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). We conducted several experiments using simulated cloud environment to validate our trust model and assess the three MADM methods. The results show that the proposed model is pliable to users' requirements and efficiently evaluate trust of cloud providers.},
keywords={Cloud computing;Quality of service;Big Data;Computational modeling;Data models;Adaptation models;Big Data;Big Data processing;cloud computing;cloud selection;reputation;trust evaluation;trust model;Multi-criteria decision-making;SAW;WPM;TOPSIS},
doi={10.1109/BigDataCongress.2017.41},
ISSN={},
month={June},}
@INPROCEEDINGS{9215250,
author={Peethambaran, Geetha and Naikodi, Chandrakant and Suresh, L},
booktitle={2020 International Conference on Smart Electronics and Communication (ICOSEC)},
title={An Ensemble Learning Approach for Privacy–Quality–Efficiency Trade-Off in Data Analytics},
year={2020},
volume={},
number={},
pages={228-235},
abstract={Privacy is an issue of concern in the electronic era where data has become a primary source of investment for businesses and organizations. The value generated from data is put to use in a number of ways for economic benefit. Customer profiling is one such instance, where data collected is used for targeted marketing, personalized purchase recommendations and customized product deliveries. In such applications, the risk of individual sensitive information disclosure always prevails, affecting the privacy of individuals involved. Hence privacy preserving analysis demands suppressing or transforming data before it is published for analysis, thus curbing data leak. Subsequently, data quality degrades, and operative analytics is affected. With Big data, algorithms that offer a reasonable qualityprivacy trade off need enhancements in terms of efficiency and scalability. In this paper, the work proposed uses a privacy based composite classifier model to analyze the accuracy of classification. The diverse characteristics of algorithms in the composite classifier are found to balance the classification accuracy that is likely to get affected by privacy model. Further, the model's performance with respect to execution time is then evaluated using the parallel computing framework Spark.},
keywords={Data privacy;Privacy;Support vector machines;Big Data;Classification algorithms;Data models;Data integrity;Privacy;Scalability;Big Data;Spark;Analytics;Privacy Preserving;Performance;Utility;UCI;Composite;Efficiency;Anonymization},
doi={10.1109/ICOSEC49089.2020.9215250},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8258174,
author={Dugenie, Pascal and Freire, Nuno and Broeder, Daan},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Building new knowledge from distributed scientific corpus: HERBADROP & EUROPEANA: Two concrete case studies for exploring big archival data},
year={2017},
volume={},
number={},
pages={2231-2239},
abstract={This paper presents approaches for building new knowledge using emerging methods and big data technologies together with archival practices. Two cases studies have been considered. The first one called HERBADROP is concerned with preservation and analysis of herbarium images. The second one called EUROPEANA investigates how to facilitate the re-use of cultural heritage language resources for research purposes. The common point between these two case studies is that they are both concerned with the use of valuable heritage resources within the EUDAT (European Data) infrastructure. HERBADROP leverages on the data services provided by EUDAT for long-term preservation, while EUROPEANA leverages on EUDAT to achieve citability and persistent identification of cultural heritage datasets. EUDAT1 is an initiative of some of the main European data centers and together with community research infrastructure organisations, to build a common eInfrastructure for general research data management. In this paper, we show how technologcal trends may offer some new research potential in the domain of computational archival science in particular appraising the challenges of producing quality, meaning, knowledge and value from quantity, tracing data and analytic provenance across complex big data platforms and knowledge production ecosystems.},
keywords={Europe;Collaboration;Optical character recognition software;Big Data;Cultural differences;History},
doi={10.1109/BigData.2017.8258174},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9360942,
author={Dongen, Zhou},
booktitle={2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)},
title={Kindergarten Big Data System Solution Architecture},
year={2020},
volume={},
number={},
pages={313-318},
abstract={The kindergarten big data system is designed based on the practical problems of insufficient pre-school education resources, inadequate regulatory systems and mechanisms, and the need to improve the quality of childcare, and the management needs of the kindergarten urgently need to improve the science, flexibility, efficiency, and safety, developing. It includes three major aspects of network system architecture, environment system and management system. The kindergarten big data system is based on modern information technology and can eventually achieve "intelligent connection with government authorities" to facilitate supervision. The kindergarten big data system can "integrate intelligently the various affairs of the kindergarten" for effective management; "monitor children's health information at all times" to ensure safety. It also can "real-time interact with parents and share children's information" to achieve home-school co-education; "develop games, learning environment and resources" to provide personalized services and other functions for kindergarten teachers.},
keywords={Pediatrics;Systems architecture;Machine learning;Big Data;Real-time systems;Safety;Monitoring;Kindergarten;Big Data System;Scheme Architecture},
doi={10.1109/MLBDBI51377.2020.00067},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9421870,
author={Qu, Xueyu},
booktitle={2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)},
title={Management Innovation of Party Building Work in Higher Vocational Colleges under the Background of "Internet +" and Big Data},
year={2020},
volume={},
number={},
pages={1544-1547},
abstract={In the process of increasing the level of informatization technology, the development speed of Internet technology is also getting faster and faster. Big data technology and Internet technology have become important types of technology used in various industries. In the context of "Internet +" and big data, innovative management of the current party building work in higher vocational colleges can improve the efficiency and quality of party building work. In the management of party building in higher vocational colleges, we need to study the actual application of network technology and the actual requirements of party building work. The author of this article puts forward the key points of the application of Internet + and big data technology in campus party building innovation management.},
keywords={Industries;Technological innovation;Databases;Buildings;Big Data;Innovation management;Internet;big data technology;Internet;Party building;Management innovation},
doi={10.1109/ICMCCE51767.2020.00338},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9390151,
author={Rico-Bautista, Dewar and Medina-Cárdenas, Yurley and Areniz-Arévalo, Yesenia and Barrientos-Avendaño, Edwin and Maestre-Gongora, Gina and Guerrero, Cesar D.},
booktitle={2020 9th International Conference On Software Process Improvement (CIMPS)},
title={Smart University: Big Data adoption model},
year={2020},
volume={},
number={},
pages={52-60},
abstract={New technologies foster a variety of smart solutions in university settings to improve the quality of life and performance for both teachers and students. Research on information governance shows the importance of the alignment between information and communication technologies (ICTs) and strategic objectives. From this perspective, the adoption of smart technologies is the result of strategic management deliberations that address the application, the risk, the use of resources and the feasibility of technology. The main challenge is to predict how this technology is adopted through overcoming the barriers that affect, for example, your perception of usefulness or your intention to use it. This has led to the concept of an Intelligent University where Big Data has proven to be very important. This article reviews technology adoption models and proposes a specific model for Big Data based on three factors: Individual perception; security and risk; and organizational support.},
keywords={Big Data;Data models;Software;Information and communication technology;Security;Adopting smart technology;Big Data;Characterization;Process;Smart university},
doi={10.1109/CIMPS52057.2020.9390151},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6691571,
author={Nadungodage, Chandima Hewa and Xia, Yuni and Lee, John Jaehwan and Lee, Myungcheol and Park, Choon Seo},
booktitle={2013 IEEE International Conference on Big Data},
title={GPU accelerated item-based collaborative filtering for big-data applications},
year={2013},
volume={},
number={},
pages={175-180},
abstract={Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer's future preferences from the past behaviors of that customer and the other customers. Most of the popular online stores process millions of transactions per day; therefore, providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel, item-based recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data, we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the naïve GPU implementation which does not use compression.},
keywords={Graphics processing units;Runtime;Kernel;Instruction sets;Indexes;Memory management;Acceleration;recommendation systems GPU;CUDA;collaborative filtering;big-data},
doi={10.1109/BigData.2013.6691571},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8516121,
author={Yao, Le and Ge, Zhiqiang and Shao, Weiming and Song, Zhihuan},
booktitle={2018 IEEE 7th Data Driven Control and Learning Systems Conference (DDCLS)},
title={A Novel Scalable Semi-supervised GMM and Its Application for Multimode Process Quality Prediction with Big Data},
year={2018},
volume={},
number={},
pages={314-319},
abstract={In this paper, a novel variational inference semi-supervised GMM (VI-S2GMM) model is firstly proposed for multimode process predictive modeling with semi-supervised data. Since all the labeled and unlabeled data samples are involved in each iteration of parameter updating, an intractable computing problem occurs when facing a high-dimension and large-scale dataset. To tack this problem, a scalable Stochastic Variational Inference semi-supervised GMM (SVI-S2GMM) is further proposed for massive semi-supervised data. Through taking advantage of stochastic gradient optimization algorithm to maximize the Evidence of Lower Bound (ELBO), the VI-based algorithm becomes scalable. In SVI-S2GMM, only one or a mini-batch of samples is randomly selected to update parameters in each iteration, which is more efficient than VI-S2GMM. In this way, a large number of unlabeled process data can be useful in the modeling, which will benefit the parameter identification. The SVI-S2GMM is then exploited for the prediction of quality-related key performance index (KPI). Two modeling cases with large scale of semi-supervised datasets demonstrate the feasibility and effectiveness of the proposed algorithms.},
keywords={Data models;Inference algorithms;Big Data;Predictive models;Adaptation models;Computational modeling;Stochastic processes;Stochastic Variational Inference;semi-supervised modeling;Gaussian Mixture Model;big data;multimode process modeling;quality prediction},
doi={10.1109/DDCLS.2018.8516121},
ISSN={},
month={May},}
@INPROCEEDINGS{9260387,
author={Sun, Yanjun and Sun, Jianwei and Wu, Wenshuang and Du, Qiang and Zhao, Haiyan and Liu, Juanjuan},
booktitle={2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Research on Personalized Service Strategy of University Library Based on Big Data Mining System},
year={2020},
volume={},
number={},
pages={515-518},
abstract={In order to improve the level and efficiency of personalized service of university library, and to meet the personalized needs of college teachers and students for borrowing books, this paper puts forward a novel personalized service strategy of university library based on big data mining system. This strategy is based on big data mining technology, and comprehensively adopts high and new technologies such as association data analysis, clustering data analysis, classification data analysis and so on. The research shows that this strategy can not only really improve the level and efficiency of personalized service of university library, but also meet the personalized needs of college teachers and students to borrow books to a certain extent.},
keywords={Zirconium;Smart grids;Quality function deployment;Hafnium compounds;Conferences;Automation;Big data mining system;University library;Personalized service},
doi={10.1109/ICSGEA51094.2020.00117},
ISSN={},
month={June},}
@INPROCEEDINGS{8301496,
author={Sharif, Abida and Li, Jianping and Khalil, Mudassir and Kumar, Rajesh and Sharif, Muhammad Irfan and Sharif, Atiqa},
booktitle={2017 14th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)},
title={Internet of things — smart traffic management system for smart cities using big data analytics},
year={2017},
volume={},
number={},
pages={281-284},
abstract={Smart Traffic System (STS) is a one of the important aspect for future smart city. STS is more expensive and highly configurable to provide better quality of service for public traffic management. This paper proposes a low cost future STS to provide better service by deploying traffic update instantly. Low cost vehicle detecting sensors are fixed in the middle of road for every 500 meters. Internet of Things (IoT) is being used to attain public traffic data quickly and send it for data processing. The Real time streaming data is sent for Big Data analytics. There are several analytical scriptures to analyze the traffic density and provide solution through predictive analytics.},
keywords={Smart cities;Sensors;Big Data;Roads;Internet of Things;Big Data;Smart Cities;Smart Traffic Management System},
doi={10.1109/ICCWAMTIP.2017.8301496},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7031617,
author={Elagib, Sara B. and Najeeb, Atahur Rahman and Hashim, Aisha H. and Olanrewaju, Rashidah F.},
booktitle={2014 International Conference on Computer and Communication Engineering},
title={Big Data Analysis Solutions Using MapReduce Framework},
year={2014},
volume={},
number={},
pages={127-130},
abstract={Recently, data that generated from variety of sources with massive volumes, high rates, and different data structure, data with these characteristics is called Big Data. Big Data processing and analyzing is a challenge for the current systems because they were designed without Big Data requirements in mind and most of them were built on centralized architecture, which is not suitable for Big Data processing because it results on high processing cost and low processing performance and quality. MapReduce framework was built as a parallel distributed programming model to process such large-scale datasets effectively and efficiently. This paper presents six successful Big Data software analysis solutions implemented on MapReduce framework, describing their datasets structures and how they were implemented, so that it can guide and help other researchers in their own Big Data solutions.},
keywords={Big data;Data analysis;Data mining;Computers;Abstracts;Statistical analysis;Clustering algorithms;Big Data;data analysis;MapReduce;data mining},
doi={10.1109/ICCCE.2014.46},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7384080,
author={Jie-Feng, Wang},
booktitle={2015 International Conference on Intelligent Transportation, Big Data and Smart City},
title={Multidimensional Assessment Results Data Mining and Analysis for Courses of Science},
year={2015},
volume={},
number={},
pages={519-522},
abstract={This system adopts the decision tree ID3 algorithm and the teachers' classroom teaching quality evaluation attribute are classified and evaluation attribute characteristic of each category is obtained. The result of the rules of data mining are analyzed and the analysis results are applied to improve the level of teaching quality, which has very strong practical significance. By using data mining technique, multidimensional assessment results are deeply analyzed, so that we can more clearly find out correlation between factors influencing the quality of classroom teaching of science course. Then we can help the teaching management and teachers to solve teaching problem and improve the quality of classroom teaching of science course.},
keywords={Transportation;Big data;Smart cities;ID3 algorithm;attribute;data mining;quality of classroom teaching},
doi={10.1109/ICITBS.2015.134},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7573163,
author={Casalicchio, Emiliano and Lundberg, Lars and Shirinbad, Sogand},
booktitle={2016 IEEE International Conference on Autonomic Computing (ICAC)},
title={An Energy-Aware Adaptation Model for Big Data Platforms},
year={2016},
volume={},
number={},
pages={349-350},
abstract={Platforms for big data includes mechanisms and tools to model, organize, store and access big data (e.g. Apache Cassandra, Hbase, Amazon SimpleDB, Dynamo, Google BigTable). The resource management for those platforms is a complex task and must account also for multi-tenancy and infrastructure scalability. Human assisted control of Big data platform is unrealistic and there is a growing demand for autonomic solutions. In this paper we propose a QoS and energy-aware adaptation model designed to cope with the real case of a Cassandra-as-a-Service provider.},
keywords={Adaptation models;Big data;Throughput;Cloud computing;Optimization;Runtime;Scalability;Autonomic computing;Cloud computing;Green computing;Apache Cassandra;Big Data},
doi={10.1109/ICAC.2016.13},
ISSN={},
month={July},}
@INPROCEEDINGS{9253463,
author={Peng, Bei and Liu, Lizhi},
booktitle={2020 5th International Conference on Control, Robotics and Cybernetics (CRC)},
title={Query Optimization for Air Quality Big Data based on Hive-ORC},
year={2020},
volume={},
number={},
pages={19-23},
abstract={To improve the efficiency of analyzing the massive amount of monitoring data collected from an air quality monitoring system, a method based on Hive data warehouse to store data as ORC file format then create Row Group Index and Bloom Filter Index is proposed to optimize the query of air quality big data. The air quality monitoring data from the environmental monitoring center of Hubei province was taken as a research object. After the data was transferred to Hive, Spark was used to query and analyze data. Five queries were carried out on three datasets to conduct the comparison experiment between the method proposed in this paper and the method of storing data as TextFile by default on Hive. The results show that the optimization method based on Hive-ORC and its indexes reduces the storage space of big data by 90% and reduces its query time by 86%. What's more, better optimization effect can be achieved with the increase of data volume.},
keywords={Query processing;Optimization methods;Big Data;Air quality;Indexes;Sparks;Robots;hive;ORC file;big data;query optimization},
doi={10.1109/CRC51253.2020.9253463},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7544782,
author={Kulkarni, Amey and Jafari, Ali and Shea, Colin and Mohsenin, Tinoosh},
booktitle={2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
title={CS-Based Secured Big Data Processing on FPGA},
year={2016},
volume={},
number={},
pages={201-201},
abstract={The four V's in Big data sets, Volume, Velocity, Variety, and Veracity, provides challenges in many different aspects of real-time systems. Out of these areas securing big data sets, reduction in processing time and communication bandwidth are of utmost importance. In this paper we adopt Compressive Sensing (CS) based framework to address all three issues. We implement compressive Sensing using Deterministic Random Matrix (DRM) on Artix-7 FPGA, and CS reconstruction using Orthogonal Matching Pursuit (OMP) algorithm on Virtex-7 FPGA. The results show that our implementations for CS sampling and reconstruction are 183x and 2.7x respectively faster when compared to previously published work. We also perform case study of two different applications i.e. multi-channel Seizure Detection and Image processing to demonstrate the efficiency of our proposed CS-based framework. CS-based framework allows us to reduce communication transfers up to 75% while achieving satisfactory range of quality. The results show that our proposed framework is 290x faster and has 7.9x less resource utilization as compared to previously published AES based encryption.},
keywords={Image reconstruction;Big data;Field programmable gate arrays;Compressed sensing;Matching pursuit algorithms;Kernel;Encryption;Compressive Sensing;Big data;Encryption},
doi={10.1109/FCCM.2016.59},
ISSN={},
month={May},}
@INPROCEEDINGS{7930047,
author={Rong, Chuitian and Lin, Chunbin and Silva, Yasin N. and Wang, Jianguo and Lu, Wei and Du, Xiaoyong},
booktitle={2017 IEEE 33rd International Conference on Data Engineering (ICDE)},
title={Fast and Scalable Distributed Set Similarity Joins for Big Data Analytics},
year={2017},
volume={},
number={},
pages={1059-1070},
abstract={Set similarity join is an essential operation in big data analytics, e.g., data integration and data cleaning, that finds similar pairs from two collections of sets. To cope with the increasing scale of the data, distributed algorithms are called for to support large-scale set similarity joins. Multiple techniques have been proposed to perform similarity joins using MapReduce in recent years. These techniques, however, usually produce huge amounts of duplicates in order to perform parallel processing successfully as MapReduce is a shared-nothing framework. The large number of duplicates incurs on both large shuffle cost and unnecessary computation cost, which significantly decrease the performance. Moreover, these approaches do not provide a load balancing guarantee, which results in a skewness problem and negatively affects the scalability properties of these techniques. To address these problems, in this paper, we propose a duplicatefree framework, called FS-Join, to perform set similarity joins efficiently by utilizing an innovative vertical partitioning technique. FS-Join employs three powerful filtering methods to prune dissimilar string pairs without computing their similarity scores. To further improve the performance and scalability, FS-Join integrates horizontal partitioning. Experimental results on three real datasets show that FS-Join outperforms the state-of-theart methods by one order of magnitude on average, which demonstrates the good scalability and performance qualities of the proposed technique.},
keywords={Scalability;Big Data;Load management;Partitioning algorithms;Data engineering;Data integration;Cleaning},
doi={10.1109/ICDE.2017.151},
ISSN={2375-026X},
month={April},}
@INPROCEEDINGS{8271966,
author={Hassanein, Hossam S. and Oteafy, Sharief M. A.},
booktitle={2017 13th International Conference on Distributed Computing in Sensor Systems (DCOSS)},
title={Big Sensed Data Challenges in the Internet of Things},
year={2017},
volume={},
number={},
pages={207-208},
abstract={Internet of Things (IoT) systems are inherently built on data gathered from heterogeneous sources. In the quest to gather more data for better analytics, many IoT systems are instigating significant challenges. First, the sheer volume and velocity of data generated by IoT systems are burdening our networking infrastructure, especially at the edge. The mobility and intermittent connectivity of edge IoT nodes are further hampering real-time access and reporting of IoT data. As we attempt to synergize IoT systems to leverage resource discovery and remedy some of these challenges, the rising challenges of Quality of Information (QoI) and Quality of Resource (QoR) calibration, render many IoT interoperability attempts far-fetched. We survey a number of challenges in realizing IoT interoperability, and advocate for a uniform view of data management in IoT systems. We delve into three planes that encompass Big Sensed Data (BSD) research directions, presenting a building block for future research efforts in IoT data management.},
keywords={Sensors;Calibration;Internet of Things;Data integration;Conferences;Interoperability;Standards;Internet of Things;Big Sensed Data;Next Generation Networks;Quality of Data;Quality of Information},
doi={10.1109/DCOSS.2017.35},
ISSN={2325-2944},
month={June},}
@INPROCEEDINGS{7872993,
author={Pu, Dong-Mei and Gao, Da-Qi and Yuan, Yu-Bo},
booktitle={2016 International Conference on Machine Learning and Cybernetics (ICMLC)},
title={A dynamic data correction algorithm based on polynomial smooth support vector machine},
year={2016},
volume={2},
number={},
pages={820-824},
abstract={Data quality plays an important role in modern intelligent information system and is crucial to any data analysis task. Many imperfection-handling techniques avoid overfitting or simply remove offending portions of the data. Data correction can help to retain and recover as much information as possible from the original data resources. In this paper, we proposed a novel technique based on polynomial smooth support vector machine. The quadratic polynomial and the first degree of polynomial as the support vector machine smooth functions are investigated. At the same time, the function was used as smooth function to calculate compensation values. In order to show the procedures of our algorithm, some necessary steps need to be considered. Firstly, the original data are normalized, so as to eliminate experimental effects of dimensional problems. Secondly, the three different kinds of smooth functions need to be analysed mathematically. The difference measure are calculated to make sure the results of correction through different data correction models. The results of given noised data sets can show that the proposed the data correction method based on polynomial smooth support vector machine is effectiveness.},
keywords={Support vector machines;Heuristic algorithms;Data analysis;Aerodynamics;Cybernetics;Big data;Machine learning algorithms;Data analysis;Data correction;Support vector machine;Data Mining},
doi={10.1109/ICMLC.2016.7872993},
ISSN={2160-1348},
month={July},}
@INPROCEEDINGS{8875267,
author={Dai, Ranran and Cheng, Shi and Chen, Fengmei and Huang, Tao and Cheng, Xianyi},
booktitle={2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)},
title={Precise Subsidization Grants for College Students over Big Data Optimized Random Forest},
year={2019},
volume={},
number={},
pages={488-494},
abstract={It is difficult to accurate identify and ascertain college students' grants. Open "poorer than poor" funding mode lacks humanistic care. How to subsidize has always been a difficult problem in the education of University grants. In the past, due to the lack of data and the high cost of acquisition data, how to introduce big data processing technology and machine learning method into the accurate prediction of college student grants has been not achieved good results. We establish a prediction model in this paper, by optimizing the characteristic selection and running parameters for random forest model, also, we can precise subsidization grants for college students' based on balanced corpus strategy and big data analysis. The experiments on real data sets show that the accuracy can reach 91%, although not the most final determinative result, to improve the precision and quality of college student grant work, has the very vital significance.},
keywords={Data models;Handheld computers;Conferences;Internet of Things;Green computing;Social computing;Big Data;precise subsidize, big data, forest model, Grants, college students},
doi={10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00100},
ISSN={},
month={July},}
@INPROCEEDINGS{8104467,
author={Ying, Liu},
booktitle={2017 International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Strategy Analysis of Psychological Quality Education in the Environment of Big Data},
year={2017},
volume={},
number={},
pages={636-640},
abstract={College psychological health education is very important for college psychological discipline construction, lodging the important role of college psychology in the reform and the change, emphasizes two related themes of the psychological health education-the empirical application and the overall adjustment for prevention. With the development of modern network technology and the increasing information, society begins to gradually into the era of big data. Under the environment of Big data, the campuses how to implement the psychological quality education strategy with the help of big data, and how to strengthen the utilization of big data to make construction of informational psychological quality education strategy become the focus of current topic which people should think. In view of the foregoing, in this paper, starting from the definition of big data, the meaning of big data in psychological quality education strategy and the specific informational psychological quality education strategy application are analyzed, and psychological quality education weighting model based on AHP was built, so as to provide reference for the application of big data psychological quality education strategy in the future.},
keywords={Smart grids;Automation;Big Data;Psychological Quality Education;Analytic Hierarchy Process},
doi={10.1109/ICSGEA.2017.127},
ISSN={},
month={May},}
@INPROCEEDINGS{7300848,
author={Kanchi, Sravanthi and Sandilya, Shubhrika and Ramkrishna, Shashank and Manjrekar, Siddhesh and Vhadgar, Akshata},
booktitle={2015 3rd International Conference on Future Internet of Things and Cloud},
title={Challenges and Solutions in Big Data Management -- An Overview},
year={2015},
volume={},
number={},
pages={418-426},
abstract={Currently, a huge explosion of data is observed in many organizations in the world. Industry analysts and businesses are looking towards Big Data As the next big thing to provide opportunities, insights, solutions and a new way to increase profits in business. From social networking sites to records in a hospital, Big Data has played an important role to improve businesses and innovation. Businesses strive to get quality information and retrieve them for data analysis and business purposes. Though big data is obtained from numerous resources, there are many issues and challenges that companies face while storing and handling Big Data. Proper data management practices, techniques, technology and infrastructure can help overcome these challenges, problems and issues. This paper gives an overview about the issues and challenges of Big Data management, also their solutions and practices being used to handle them.},
keywords={Big data;Cloud computing;Databases;Organizations;Bismuth;Big data;issues;challenges;big data management},
doi={10.1109/FiCloud.2015.121},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8705430,
author={Ying, Dai},
booktitle={2018 3rd International Conference on Smart City and Systems Engineering (ICSCSE)},
title={Competition Decision for Bottleneck Traveling Salesman Problem Based on Big Data Mining Algorithm with Multi-Segment Support},
year={2018},
volume={},
number={},
pages={725-729},
abstract={Due to the existence of multiple constraints and multiple optimization objectives, the competition decision for bottleneck traveling salesman problem is very difficult. The paper proposes the competition decision for bottleneck traveling salesman based on big data mining algorithm with multi-segment support. The big data mining algorithm involving multi-segment support covers the initial division of the tour area of travelling salesman, competition decision on the tourism regional boundary, and creation of the initial solution. The competition decision for bottleneck traveling salesman problem defines the local search scope within nearest neighbor of network K, and only to search the most likely spatial neighborhood, and iteratively improves the quality of the solution. Test the performance of the algorithm through the competition decision for bottleneck traveling salesman problem. The test results show that the proposed method in this paper can solve the bottleneck travelling salesman competition decision problem of 6,400 customer points within 15 minutes. The quality of the solution is about 10.8% better than ArcGIS, and the calculation time is about 21.2% of ArcGIS.},
keywords={Companies;Traveling salesman problems;Big Data;Data mining;Transportation;Heuristic algorithms;Optimization;Bottleneck Travelling salesman;Multi-segment Support;Big Data Mining Algorithm;Problem Competition Decision},
doi={10.1109/ICSCSE.2018.00156},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9378296,
author={Shrivastava, Shrey and Patel, Dhaval and Zhou, Nianjun and Iyengar, Arun and Bhamidipaty, Anuradha},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={DQLearn : A Toolkit for Structured Data Quality Learning},
year={2020},
volume={},
number={},
pages={1644-1653},
abstract={Data Quality (DQ) has been one of the key focuses as Data Analytics and Artificial Intelligence (AI) fields continue to grow. Yet, data quality analysis has mostly been a disjointed, ad-hoc, and cumbersome process in the overall data analysis workflow. There have been ongoing attempts to formalize this process, but the solutions that have come out are not universally applicable. Most of the proposed solutions try to address the problem of data quality from a limited perspective and suc-cessfully address only a subset of all challenges. These solutions fail to translate to other domains due to a lack of structure. In this paper, we present DQLearn, a toolkit for structured data quality learning. We start by presenting the core principle on which we build our library and introduce the four components that provide a solid base to address the needs of the data quality problem. Then, we showcase our automation structure - "Workflows", and the two optimization techniques equipped with it, that help the users to structure their learning problem very easily. Next, we discuss four important scenarios of the DQ Workflows in the overall life-cycle. Finally, we demonstrate the utility of the proposed toolkit with public datasets and show benchmark results from optimization experiments.},
keywords={Data analysis;Automation;Data integrity;Optimization methods;Big Data;Solids;Task analysis},
doi={10.1109/BigData50022.2020.9378296},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8648791,
author={Hou, Aiqin and Wu, Chase Q. and Fang, Dingyi and Zuo, Liudong and Zhu, Michelle M. and Zhang, Xiaoyang and Qiao, Ruimin and Yin, Xiaoyan},
booktitle={2018 IEEE/ACM Innovating the Network for Data-Intensive Science (INDIS)},
title={Bandwidth Scheduling for Big Data Transfer with Deadline Constraint between Data Centers},
year={2018},
volume={},
number={},
pages={55-63},
abstract={An increasing number of applications in scientific and other domains have moved or are in active transition to clouds, and the demand for the movement of big data between geographically distributed cloud-based data centers is rapidly growing. Many modern backbone networks leverage logically centralized controllers based on software-defined networking (SDN) to provide advance bandwidth reservation for data transfer requests. How to fully utilize the bandwidth resources of the links connecting data centers with guaranteed QoS for each user request is an important problem for cloud service providers. Most existing work focuses on bandwidth scheduling for a single request for data transfer or multiple requests using the same service model. In this work, we construct rigorous cost models to quantify user satisfaction degree and formulate a generic problem of bandwidth scheduling for multiple deadline-constrained data transfer requests of different types to maximize the request scheduling success ratio while minimizing the data transfer completion time of each request. We prove this problem to be NP-complete and design a heuristic solution. Extensive simulation results show that our scheduling scheme significantly outperforms existing methods in terms of user satisfaction degree and scheduling success ratio.},
keywords={Bandwidth;Data transfer;Cloud computing;Data centers;Scheduling;Big Data;Computer science;Big-data;data-center;high-performance-networks;software-defined-networking;bandwidth-scheduling},
doi={10.1109/INDIS.2018.00009},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8121916,
author={Sattart, Farook and McQuay, Colter and Driessen, Peter F.},
booktitle={2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)},
title={Marine mammal sound anomaly and quality detection using multitaper spectrogram and hydrophone big data},
year={2017},
volume={},
number={},
pages={1-6},
abstract={This paper proposes a novel method for anomaly and quality detection of marine mammal sounds using multitaper spectrogram and hydrophone big data. The proposed method is aimed to automatically detect anomaly, such as high-frequency vessel noise, Doppler noise, in sperm whale (SPW) sound as well as the quality of the sound. A new signature function derived from a multi-taper spectrogram is able to detect the anomaly in the data and a new anomaly distortion measure can detect the sound quality into good/bad. The proposed method, is tested with 1905 minutes of data spanning a single year, and using a human operator's annotations. The experimental results reveal that the proposed multitaper spectrogram based approach is efficient in detecting anomaly as well as sperm whale sound quality for hydrophone big data and high detection accuracy (>85%) is achieved for raw input hydrophone data.},
keywords={Spectrogram;Whales;Sonar equipment;Big Data;Acoustic distortion;Anomaly detection;Anomaly detection;Hydrophone Big data;Quality detection;Multitaper spectrogram;Marine mammal sound;Sperm whale},
doi={10.1109/PACRIM.2017.8121916},
ISSN={},
month={Aug},}