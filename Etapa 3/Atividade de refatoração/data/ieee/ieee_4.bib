@INPROCEEDINGS{9148143,
author={Lu, Xinghua and Liu, Peihao and Nie, Weidong and Zhang, Hao},
booktitle={2020 International Conference on Computer Information and Big Data Applications (CIBDA)},
title={Whole Process Tracing Model of Pigeon Quality in Block Chain Environment},
year={2020},
volume={},
number={},
pages={435-439},
abstract={In order to improve the whole process traceability of meat pigeon quality, a whole process traceability model of meat pigeon quality based on block chain data fusion is proposed. The method comprises the following steps of: constructing a statistical information distribution model for tracing the whole process of meat pigeon quality; reorganizing the structure of information sources for tracing the whole process of meat pigeon quality by adopting a data structure reorganization method; establishing an information source characteristic distribution model for tracing the whole process of meat pigeon quality; carrying out tracking identification and large data mining of meat pigeon quality information by adopting an association rule mining method under a block chain mode; constructing a meat pigeon quality whole process tracing model; and combining information extraction and optimal scheduling of meat pigeon quality. The quantitative feature distribution set of meat pigeon quality is extracted, and the statistical feature analysis of the whole process traceability of meat pigeon quality is realized by combining the information detection and feature positioning methods. The dynamic evaluation of meat pigeon quality information is realized by using the meat pigeon quality statistical large data analysis method. The optimization design of the whole process traceability model of meat pigeon quality is realized by combining the block chain data fusion and the knowledge map feature analysis method. The simulation results show that the method has good real-time performance, strong dynamic tracing ability and good information positioning ability for the quality of meat pigeons.},
keywords={Data models;Data mining;Production;Data integration;Feature extraction;Process control;block chain;Pigeons;Quality;Tracing the whole process},
doi={10.1109/CIBDA50819.2020.00104},
ISSN={},
month={April},}
@INPROCEEDINGS{9378004,
author={Esteves, André and Ponciano, Vasco and Miguel Pires, Ivan},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Diseases identification with big data concept – The older people community},
year={2020},
volume={},
number={},
pages={3721-3726},
abstract={The use of the big data in conjunction with artificial intelligence methods used in health areas is increasingly being used. For data capture, smartphones and embedded sensors are an increasingly reliable, accurate and fast way to detect certain types of bio signals. In this study we propose a system for collecting data for the acquisition and study of associated diseases and symptoms to demonstrate how the use of sensors and their connection to a database and later application of methods to detect patterns to remove conclusions and contribute to advances in health and to a higher quality of life especially in the elderly.},
keywords={Senior citizens;Machine learning;Big Data;Tools;Time measurement;Smart phones;Diseases;Diseases;Data analysis;elderly people;prevention;Big data},
doi={10.1109/BigData50022.2020.9378004},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6691678,
author={Xu, Weijia and Esteva, Maria and Trelogan, Jessica and Swinson, Todd},
booktitle={2013 IEEE International Conference on Big Data},
title={A case study on entity Resolution for Distant Processing of big Humanities data},
year={2013},
volume={},
number={},
pages={113-120},
abstract={At the forefront of big data in the Humanities, collections management can directly impact collections access and reuse. However, curators using traditional data management methods for tasks such as identifying redundant from relevant and related records, a small increase in data volume can significantly increase their workload. In this paper, we present preliminary work aimed at assisting curators in making important data management decisions for organizing and improving the overall quality of large unstructured Humanities data collections. Using Entity Resolution as a conceptual framework, we created a similarity model that compares directories and files based on their implicit metadata, and clusters pairs of closely related directories. Useful relationships between data are identified and presented through a graphical user interface that allows qualitative evaluation of the clusters and provides a guide to decide on data management actions. To evaluate the model's performance, we experimented with a test collection and asked the curator to classify the clusters according to four model cluster configurations that consider the presence of related and duplicate information. Evaluation results suggest that the model is useful for making data management action decisions.},
keywords={Erbium;Organizations;Vectors;Data models;Information management;Data handling;Data storage systems;Collections Management;Entity Resolution;Distant Processing;Digital Humanities},
doi={10.1109/BigData.2013.6691678},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8862250,
author={Nair, Preeti and Kashyap, Indu},
booktitle={2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)},
title={Hybrid Pre-processing Technique for Handling Imbalanced Data and Detecting Outliers for KNN Classifier},
year={2019},
volume={},
number={},
pages={460-464},
abstract={Data mining is a technique of examining huge quanta of pre-existing data in order to discover new patterns and relationships among them, which will help to make better decisions. Classification is a data mining technique which organises data into categories. In this paper, in order to enhance the performance of k nearest neighbour (kNN) classifier-a kind of classification technique that is among the most widely used-a new data pre-processing technique has been proposed, which can handle some classification issues such as imbalanced data and outliers. In an imbalanced dataset, the classification categories are not equally distributed. Imbalanced dataset have an inherent issue when it comes to using classifiers on them that have been developed using machine learning algorithms. The basic nature of these algorithms is to reduce errors without relying on balance of classes. Another issue addressed in this paper is the matter of outliers or extreme values. Outlier or extreme values are those values that are outside the expected range of values. The quality of Classification modelling can be greatly enhanced by identifying and excision of these values. In this proposed technique two data pre-processing techniques have been combined to form a hybrid pre-processing technique. The two data pre-processing techniques are resample technique and inter quartile range technique (IQR). Some Imbalanced dataset with outliers that can be considered as yardsticks were taken for this study. It was observed that the classification results obtained were far superior to the classification done without the pre-processing technique.},
keywords={Surgery;Classification algorithms;Data mining;Training data;Sonar measurements;Machine learning algorithms;Data Pre-processing;kNN;Outliers;Imbalanced Dataset},
doi={10.1109/COMITCon.2019.8862250},
ISSN={},
month={Feb},}
@INPROCEEDINGS{9346938,
author={Li, Shuting and Gao, Shifa and Wu, Jianbin and Xie, Dongsheng and Xi, Guoping and Zhao, Yaqin and Zuo, Zhuowen and Huang, He and Qi, Li},
booktitle={2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2)},
title={Research on Topology Identification of Distribution Network Under the Background of Big Data},
year={2020},
volume={},
number={},
pages={4294-4297},
abstract={Traditional identification of distribution network topology needs to rely on manual identification and verification, which has the problems of low identification rate and poor accuracy. Under the current background of power big data, effective results can be obtained by fully mining the potential effective information of big data and applying it to the identification of distribution network topology structure. This paper presents a line-to-line relationship identification method based on the maximum correlation minimum redundancy method and a line-to-transformer relationship identification method based on the conversion of three-phase unbalanced outlet voltage. The voltage acquisition system is fully utilized, and the maximum correlation minimum redundancy method (mRMR) is used to eliminate the redundancy of feature variables in line-to-line relationship identification to obtain the most accurate correlation results. In the identification of line-transformer relationship, the conversion method of three-phase unbalanced outlet voltage is adopted to eliminate the influence of misjudgment caused by three-phase unbalanced voltage and improve the identification accuracy. The identification method proposed in this paper is applied to the topology identification of a regional distribution network. By comparison with the actual topology, the identification accuracy is up to 99.78% and the effect is remarkable.},
keywords={Correlation;Network topology;Redundancy;Distribution networks;Big Data;Topology;Velocity measurement;topology identification;distribution network;mRMR},
doi={10.1109/EI250167.2020.9346938},
ISSN={},
month={Oct},}
@ARTICLE{7582489,
author={Dong, Le and Lin, Zhiyu and Liang, Yan and He, Ling and Zhang, Ning and Chen, Qi and Cao, Xiaochun and Izquierdo, Ebroul},
journal={IEEE Transactions on Big Data},
title={A Hierarchical Distributed Processing Framework for Big Image Data},
year={2016},
volume={2},
number={4},
pages={297-309},
abstract={This paper introduces an effective processing framework nominated Image Cloud Processing (ICP) to powerfully cope with the data explosion in image processing field. While most previous researches focus on optimizing the image processing algorithms to gain higher efficiency, our work dedicates to providing a general framework for those image processing algorithms, which can be implemented in parallel so as to achieve a boost in time efficiency without compromising the results performance along with the increasing image scale. The proposed ICP framework consists of two mechanisms, i.e., Static ICP (SICP) and Dynamic ICP (DICP). Specifically, SICP is aimed at processing the big image data pre-stored in the distributed system, while DICP is proposed for dynamic input. To accomplish SICP, two novel data representations named P-Image and Big-Image are designed to cooperate with MapReduce to achieve more optimized configuration and higher efficiency. DICP is implemented through a parallel processing procedure working with the traditional processing mechanism of the distributed system. Representative results of comprehensive experiments on the challenging ImageNet dataset are selected to validate the capacity of our proposed ICP framework over the traditional state-of-the-art methods, both in time efficiency and quality of results.},
keywords={Image processing;Distributed databases;Iterative closest point algorithm;Cloud computing;Heuristic algorithms;Big data;Parallel processing;Big data;image processing;MapReduce;distributed system;cloud computing},
doi={10.1109/TBDATA.2016.2613992},
ISSN={2332-7790},
month={Dec},}
@INPROCEEDINGS{7946520,
author={Fonseka, W. R. A. and Nadeesha, D. G. M. and Thakshila, P. M. C. and Jeewandara, N. A. and Wijesinghe, D. M. and Sahabandu, R. V. De. S. and Asanka, P. P. G. D.},
booktitle={2016 IEEE International Conference on Information and Automation for Sustainability (ICIAfS)},
title={Use of data warehousing to analyze customer complaint data of Consumer Financial Protection Bureau of United States of America},
year={2016},
volume={},
number={},
pages={1-6},
abstract={The Consumer Financial Protection Bureau was established in USA for enabling the USA consumers to report customer support and complaint related information regarding their financial issues with the US government. The complaint data is freely available for analysis and tracking of how efficiently and effectively the financial institutes handle the complaints lodged against them. Each complaint consists of attributes that can uniquely describe and identify it. These features have been exploited for data mining, analysis and predictions. The data warehouse creation and data analysis was done using Microsoft SQL Server Technologies. The data mining techniques such as Microsoft Decision Tree, Microsoft Naïve Bayes, Microsoft Time Series and Microsoft Neural Network models were used in this study. Based on the results, it was observed that there is a correlation between the growth of complaints in certain financial domains with regards to changes in the economic, political and regulatory forces. Probability predictions also show, how each product can get a particular issue-related complaint, how a particular issue can get a timely response, how a particular issue can cause a consumer dispute and what type of issues are mostly lodged via a particular submission method, etc. This information can be used in prescriptive analysis to enhance financial consumer services and also improve the response quality of automated consumer support systems.},
keywords={Companies;Data warehouses;Databases;Data models;Data mining;Warehousing;Servers;financial;data mining;big data;data warehousing;time series;Naïve Bayes;decision trees;neural network},
doi={10.1109/ICIAFS.2016.7946520},
ISSN={2151-1810},
month={Dec},}
@ARTICLE{9261414,
author={Duan, Gui-Jiang and Yan, Xin},
journal={IEEE Access},
title={A Real-Time Quality Control System Based on Manufacturing Process Data},
year={2020},
volume={8},
number={},
pages={208506-208517},
abstract={Quality prediction is one of the key links of quality control. Benefitting from the development of digital manufacturing, manufacturing process data have grown rapidly, which allows product quality predictions to be made based on a real-time manufacturing process. A real-time quality control system (RTQCS) based on manufacturing process data is presented in this paper. In this study, the relationship between the product real-time quality status and processing task process was established by analyzing the relationship between the product manufacturing resources and the quality status. The key quality characteristics of the product were identified by analyzing the similarity of the product quality characteristic variations in the manufacturing process based on the big data technology, and a quality-resource matrix was constructed. Based on the quality-resource matrix, the RTQCS was established by introducing an association-rule incremental-update algorithm. Finally, the RTQCS was applied in actual production, and the performance of RTQCS was verified by experiments. The experiments showed that the RTQCS can effectively guarantee the quality of product manufacturing and improve the manufacturing efficiency during production.},
keywords={Manufacturing processes;Production;Quality control;Product design;Real-time systems;Manufacturing;Quality assessment;Quality management;production control;prediction methods},
doi={10.1109/ACCESS.2020.3038394},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8080525,
author={Fujita, Hamido},
booktitle={2017 IEEE 15th International Symposium on Intelligent Systems and Informatics (SISY)},
title={Data analytics for clouds health-care and risk predictions based on ensemble classifiers and subjective projection},
year={2017},
volume={},
number={},
pages={000011-000012},
abstract={Discovering patterns from big data attracts a lot of attention due to its importance in discovering accurate patterns and features that are used in predictions of decision making. The challenges in big data analytics are the high dimensionality and complexity in data representation. Granular computing and feature selection are among the challenge to deal with big data analytics that is used for Decision making. We will discuss these challenges in this talk and provide new projection on ensemble learning for health care risk prediction. In decision making most approaches are taking into account objective criteria, however the subjective correlation among different ensembles provided as preference utility is necessary to be presented to provide confidence preference additive among it reducing ambiguity and produce better utility preferences measurement for good quality predictions. Most models in Decision support systems are assuming criteria as independent. Different type of data (time series, linguistic values, interval data, etc.) imposes some difficulties to data analytics due to preprocessing and normalization processes which are expensive and difficult when data sets are raw and imbalanced. We will highlight these issues though project applied to health-care for elderly, by merging heterogeneous metrics for providing health care predictions for elderly at home. We have utilized ensemble learning as multi-classification techniques on multi-data streams that collected from multi-sensing devices. Subjectivity (i.e., service personalization) would be examined based on correlations between different contextual structures that are reflecting the framework of personal context, for example in nearest neighbor based correlation analysis fashion. Some of the attributes incompleteness also may lead to affect the approximation accuracy. Attributes with preference-ordered domain relations properties become one aspect in ordering properties in rough approximations. We outline issues on Virtual Doctor Systems, and highlights its innovation in interactions with elderly patients, also discuss these challenges in granular computing and decision support systems research domains. In this talk I will present the current state of art and focus it on health care risk analysis with examples from our experiments.},
keywords={Medical services;Data analysis;Big Data;Decision making;Correlation;Senior citizens;Decision support systems},
doi={10.1109/SISY.2017.8080525},
ISSN={1949-0488},
month={Sep.},}
@ARTICLE{8664564,
author={Oh, Hyeontaek and Park, Sangdon and Lee, Gyu Myoung and Heo, Hwanjo and Choi, Jun Kyun},
journal={IEEE Access},
title={Personal Data Trading Scheme for Data Brokers in IoT Data Marketplaces},
year={2019},
volume={7},
number={},
pages={40120-40132},
abstract={With the widespread use of the Internet of Things, data-driven services take the lead of both online and off-line businesses. Especially, personal data draw heavy attention of service providers because of the usefulness in value-added services. With the emerging big-data technology, a data broker appears, which exploits and sells personal data about individuals to other third parties. Due to little transparency between providers and brokers/consumers, people think that the current ecosystem is not trustworthy, and new regulations with strengthening the rights of individuals were introduced. Therefore, people have an interest in their privacy valuation. In this sense, the willingness-to-sell (WTS) of providers becomes one of the important aspects for data brokers; however, conventional studies have mainly focused on the willingness-to-buy (WTB) of consumers. Therefore, this paper proposes an optimized trading model for data brokers who buy personal data with proper incentives based on the WTS, and they sell valuable information from the refined dataset by considering the WTB and the dataset quality. This paper shows that the proposed model has a global optimal point by the convex optimization technique and proposes a gradient ascent-based algorithm. Consequently, it shows that the proposed model is feasible even if the data brokers spend costs to gather personal data.},
keywords={Data models;Pricing;Data privacy;Sensors;Economics;Data integrity;Data brokers;profit maximization;willingness-to-buy;willingness-to-sell},
doi={10.1109/ACCESS.2019.2904248},
ISSN={2169-3536},
month={},}
@ARTICLE{9126214,
author={Ren, Lei and Meng, Zihao and Wang, Xiaokang and Lu, Renquan and Yang, Laurence T.},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={A Wide-Deep-Sequence Model-Based Quality Prediction Method in Industrial Process Analysis},
year={2020},
volume={31},
number={9},
pages={3721-3731},
abstract={Product quality prediction, as an important issue of industrial intelligence, is a typical task of industrial process analysis, in which product quality will be evaluated and improved as feedback for industrial process adjustment. Data-driven methods, with predictive model to analyze various industrial data, have been received considerable attention in recent years. However, to get an accurate prediction, it is an essential issue to extract quality features from industrial data, including several variables generated from supply chain and time-variant machining process. In this article, a data-driven method based on wide-deep-sequence (WDS) model is proposed to provide a reliable quality prediction for industrial process with different types of industrial data. To process industrial data of high redundancy, in this article, data reduction is first conducted on different variables by different techniques. Also, an improved wide-deep (WD) model is proposed to extract quality features from key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based sequence model is presented for exploring quality information from time-domain features. Under the joint training strategy, these models will be combined and optimized by a designed penalty mechanism for unreliable predictions, especially on reduction of defective products. Finally, experiments on a real-world manufacturing process data set are carried out to present the effectiveness of the proposed method in product quality prediction.},
keywords={Feature extraction;Predictive models;Data models;Quality assessment;Product design;Data mining;Analytical models;Industrial artificial intelligence (AI);industrial big data;Industrial Internet of Things;product quality prediction;wide-deep-sequence (WDS) model},
doi={10.1109/TNNLS.2020.3001602},
ISSN={2162-2388},
month={Sep.},}
@INPROCEEDINGS{8284476,
author={Apurva, Aviral and Ranakoti, Pranshu and Yadav, Saurav and Tomer, Shashank and Roy, Nihar Ranjan},
booktitle={2017 International Conference on Computing and Communication Technologies for Smart Nation (IC3TSN)},
title={Redefining cyber security with big data analytics},
year={2017},
volume={},
number={},
pages={199-203},
abstract={The cyber world is expanding rapidly day by day and more and more people are getting connected to this world, resulting in generation of a large amount of data called Big Data. Along with the cyber world, the number of cyber criminals is also expanding rapidly. To fight against the cyber criminals and safeguard the interest of innocent civilians, we take the help of Big Data Analytics. Big data is large in both quantity and quality and can be efficiently used to analyze certain patterns and behavior anomaly which can help us prevent or be prepared for the thread or any upcoming attack. This proactive and analytical approach will help us greatly reduce the rate of Cyber Crimes and also get the knowledge out of that data which was not previously observable. This paper discusses some of the key characteristics of Big data, architecture, categories of cybercrimes and big data analytic techniques that can be used.},
keywords={Big Data;Computer crime;Electronic mail;Social network services;Cloud computing;Big Data;Cyber Security;Data Flow;Data Visualization;Social Media},
doi={10.1109/IC3TSN.2017.8284476},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9204277,
author={Tikito, Iman and Souissi, Nissrine},
booktitle={2020 International Conference on Intelligent Systems and Computer Vision (ISCV)},
title={Smart Data Collection in Mobile Edge Computing Environment},
year={2020},
volume={},
number={},
pages={1-7},
abstract={With the digital transformation, businesses and public administrations must change the place of data in the value chain to serve all areas of the business and open up information systems. The value of the knowledge extracted from this data is directly linked to the quality of data collection. Mobile devices are particularly suitable for reporting data. They are very widespread, very suitable and can be used at any time. These characteristics mean that the use of mobile support for data collection corresponds to a paradigm shift more than a simple new additional technology compared to the panoply of existing tools. The explosion of information sharing and data, which stems from our daily by these devices is stored mostly in the cloud servers. Thus, to reduce the number of data transferred and generated by mobile devices to the cloud servers, the edge computing allows to process data at the network edge where they are generated directly reducing certain characteristics of Big Data. Big data involves the collection of complex data on the “V” dimensions which describe the quantity and type of data collected, as well as their importance and relevance to the challenges of the requester. However, the smart data goes a step further and consist to extract from the data collected only the most relevant information for the client in order to make predictions. Our results show that using an intelligent data collection process in mobile computing could generate savings in terms of data storage and analysis at the cloud level.},
keywords={Edge computing;Big Data;Data mining;Cloud computing;Databases;Business;Smart Data;Data Collection;Data Management;Mobile Data;Edge Computing;BPMN;ODK-X},
doi={10.1109/ISCV49265.2020.9204277},
ISSN={},
month={June},}
@ARTICLE{7809119,
author={Hildebrandt, Kai and Panse, Fabian and Wilcke, Niklas and Ritter, Norbert},
journal={IEEE Transactions on Big Data},
title={Large-Scale Data Pollution with Apache Spark},
year={2020},
volume={6},
number={2},
pages={396-411},
abstract={Because of the increasing volume of autonomously collected data objects, duplicate detection is an important challenge in today's data management. To evaluate the efficiency of duplicate detection algorithms with respect to big data, large test data sets are required. Existing test data generation tools, however, are either not able to produce large test data sets or are domain-dependent which limits their usefulness to a few cases. In this paper, we describe a new framework that can be used to pollute a clean, homogeneous and large data set from an arbitrary domain with duplicates, errors and inhomogeneities. To prove its concept, we implemented a prototype which is built upon the cluster computing framework Apache Spark and evaluate its performance in several experiments.},
keywords={Big data;Pollution;Databases;Generators;Prototypes;Gold;Standards;Data quality;duplicate detection;data pollution;Apache Spark},
doi={10.1109/TBDATA.2016.2637378},
ISSN={2332-7790},
month={June},}
@INPROCEEDINGS{8090691,
author={Pelikant, Adam},
booktitle={2017 18th International Symposium on Electromagnetic Fields in Mechatronics, Electrical and Electronic Engineering (ISEF) Book of Abstracts},
title={Big data object-oriented representation based on genom data searching system},
year={2017},
volume={},
number={},
pages={1-2},
abstract={Article presents method of storing and processing Big Data using object-oriented datatype in commercial relational database server. Methodology was presented based on genomic data. Additionally, some methods of pattern matching implemented in the object were described and commented. Due to set of numerical experiments based on real data the efficiency of the system was testified. Obtain results shows good quality and high scalability of presented solutions.},
keywords={Bioinformatics;Genomics;Biological cells;Databases;Algorithm design and analysis;Servers;Conferences;Big Data;genomic data;patern maching;object-orinted datatypes;efficiency tests;relational databases},
doi={10.1109/ISEF.2017.8090691},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8397405,
author={Djenouri, Youcef and Bendjoudi, Ahcene and Djenouri, Djamel and Belhadi, Asma and Nouali-Taboudjemat, Nadia},
booktitle={2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={New GPU-based swarm intelligence approach for reducing big association rules space},
year={2017},
volume={},
number={},
pages={1-6},
abstract={This paper deals with exploration and mining of association rules in big data, with the big challenge of increasing computation time. We propose a new approach based on metarules discovery that gives to the user the summary of the rules' space through a meta-rules representation. This allows the user to decide about the rules to take and prune. We also adapt a pruning strategy of our previous work to keep only the representatives rules. As the meta-rules space is much larger than the rules space, two approaches are proposed for efficient exploitation. The first one uses a bees swarm optimization method in the meta-rules discovery process, which is extended using GPU-based parallel programming to form the second one. The sequential version has been first tested using medium rules set, and the results show clear improvement in terms of the number of returned meta-rules. The two versions have then been compared on large scale rules sets, and the results illustrate the acceleration on the summarization process by the parallel approach without reducing the quality of resulted meta-rules. Further experiments on Webdocs big data instances reveal that the proposed method of pruning rules by summarizing metarules considerably reduces the association rules space compared to state-of-the-art association rules mining-based approaches.},
keywords={Itemsets;Graphics processing units;Big Data;Data mining;Computer architecture;Particle swarm optimization;Instruction sets;Big Data;association rules;summarization;optimization methods;GPU Architecture;pruning strategy},
doi={10.1109/UIC-ATC.2017.8397405},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9006479,
author={Mainenti, David C.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Big data and traditional Chinese medicine (TCM): What’s state of the art?},
year={2019},
volume={},
number={},
pages={1417-1422},
abstract={Big data and traditional Chinese medicine (TCM) is a new interdisciplinary field quietly emerging in Chinese society. Internet of Things (IoT) sensor system technology is currently being developed to gather large volumes of personal data so that TCM, specifically herbal pharmaceutics, can be applied to treat acute and chronic diseases alike utilizing low cost, safe, and effective treatment protocols that have been prescribed in clinical practice for thousands of years. Through a survey of existing literature, this paper investigates what a future state of medicine may look like with the deployment of a big data TCM system as a means to enhance humankind's health and quality of life.},
keywords={Big Data;Medical diagnostic imaging;Diseases;Internet of Things;Protocols;clinical diagnosis;cultural medicine;herbal pharmaceutical technology;Internet of Things (IoT);sensor systems and applications},
doi={10.1109/BigData47090.2019.9006479},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8170630,
author={Ianculescu, Marilena and Alexandru, Adriana and Gheorghe-Moisii, Maria},
booktitle={2017 5th International Symposium on Electrical and Electronics Engineering (ISEEE)},
title={Harnessing the potential of big data in Romanian healthcare},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Big Data technologies provide new opportunities to enable improved methods and facilities for raising the quality and efficiency of healthcare services. The specific characteristics of health data and information, their huge amount and diversity of sources have imposed the intensive use of Big Data Analytics in the health domain. Health software applications have become sustainable and compulsory tools for providing better healthcare delivery. This paper brings forward the boost brought by the potential use of Big Data Analytics inside some Romanian health informatics systems.},
keywords={Medical services;Big Data;Informatics;Software;Interoperability;Insurance;health data and information;Big Data and Analytics;health software application;wireless sensors;real time processing;data transmission},
doi={10.1109/ISEEE.2017.8170630},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9051249,
author={Zhao, Zhichao and Wu, Tiefeng},
booktitle={2019 International Conference on Intelligent Computing, Automation and Systems (ICICAS)},
title={Study on Teaching Mode Selection Model Based on Big Data},
year={2019},
volume={},
number={},
pages={263-266},
abstract={the selection of teaching mode is related to the improvement of teaching quality. In order to improve the quality of teaching and promote the transformation of teaching results, combined with big data statistical analysis method, a choosing teaching method is proposed based on big data's analysis, which constructs the statistical analysis model of teaching mode selection by descriptive statistical analysis method, and extracts the quantity of characteristic information that reflects the optimization of teaching method selection. The big data analysis model based on fuzzy C-means clustering is used to evaluate the performance of teaching methods, and the method of segmental sample detection is used to carry out regression analysis to realize the sample fitting of teaching method selection. The mathematical model of teaching mode selection is designed through the sample fitting result, and the statistical mathematical model based on big data's teaching mode selection is constructed. The simulation results show that the method is used to select the teaching method; it can effectively extract the regular characteristic quantity, which reflects the teaching performance, realizes the association rule mining of the choice of teaching method. The teaching mode is selected according to the result of big data mining and statistical analysis, improving the teaching quality, and promoting the reform of teaching methods.},
keywords={big data, teaching mode selection, mathematical model, Statistical analysis, feature extraction, regression analysis},
doi={10.1109/ICICAS48597.2019.00063},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9226286,
author={Laigner, Rodrigo and Kalinowski, Marcos and Diniz, Pedro and Barros, Leonardo and Cassino, Carlos and Lemos, Melissa and Arruda, Darlan and Lifschitz, Sérgio and Zhou, Yongluan},
booktitle={2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)},
title={From a Monolithic Big Data System to a Microservices Event-Driven Architecture},
year={2020},
volume={},
number={},
pages={213-220},
abstract={Context: Data-intensive systems, a.k.a. big data systems (BDS), are software systems that handle a large volume of data in the presence of performance quality attributes, such as scalability and availability. Before the advent of big data management systems (e.g. Cassandra) and frameworks (e.g. Spark), organizations had to cope with large data volumes with custom-tailored solutions. In particular, a decade ago, Tecgraf/PUC-Rio developed a system to monitor truck fleet in real-time and proactively detect events from the positioning data received. Over the years, the system evolved into a complex and large obsolescent code base involving a costly maintenance process. Goal: We report our experience on replacing a legacy BDS with a microservice-based event-driven system. Method: We applied action research, investigating the reasons that motivate the adoption of a microservice-based event-driven architecture, intervening to define the new architecture, and documenting the challenges and lessons learned. Results: We perceived that the resulting architecture enabled easier maintenance and faultisolation. However, the myriad of technologies and the complex data flow were perceived as drawbacks. Based on the challenges faced, we highlight opportunities to improve the design of big data reactive systems. Conclusions: We believe that our experience provides helpful takeaways for practitioners modernizing systems with data-intensive requirements.},
keywords={Computer architecture;Big Data;Software;Stakeholders;Software systems;Maintenance engineering;Aging;big data system;microservices;event-driven},
doi={10.1109/SEAA51224.2020.00045},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9209382,
author={Guojian, Nie and Hongqi, Yang and Yong, Pan and Yujie, Liu and Zhe, Lai},
booktitle={2020 Asia-Pacific International Symposium on Advanced Reliability and Maintenance Modeling (APARM)},
title={Research on the Technology to Build Safety Integration Model of Complex System Based on Relevant Failure},
year={2020},
volume={},
number={},
pages={1-7},
abstract={In view of the problems that the existing safety modeling methods can not represent the functions, processes, behaviors and their relevant characteristics of complex system, combined with the requirements for safety work of complex systems, the research on the technology to build safety integrated model of complex system based on the relevant failure is carried out. Firstly, the analysis method for relevant failure characteristics of complex systems is given. Secondly, the method and process to build safety integrated model of complex system is put forward by using the technology of relevant failure analysis, function structure success tree, event sequence diagram and so on. Finally, an application case about a control system in aerospace is given, and the safety integrated model of the control system is built. The feasibility and effectiveness of the technical methods proposed in this paper are verified through comparative analysis.},
keywords={Safety;Complex systems;Analytical models;Maintenance engineering;Reliability engineering;Data models;complex system;relevant failure;safety;integrated model},
doi={10.1109/APARM49247.2020.9209382},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8285647,
author={Dissanayake, D. M. C. and Jayasena, K. P. N.},
booktitle={2017 National Information Technology Conference (NITC)},
title={A cloud platform for big IoT data analytics by combining batch and stream processing technologies},
year={2017},
volume={},
number={},
pages={40-45},
abstract={The Internet of things is a current major developing technology, which is a network of everyday physical objects that enhances the quality of lifestyle. Application of the internet of things encounters dealing with huge amount of data. One of the directions of big data is this huge amount of data with respect to the internet of things. As the name implies, big data refers to the data that cannot be analyzed by a traditional data processing software. The key challenge of this phenomenon is to use a proper way to analyse, which can provide useful features from the data absorbed by the perception layer of the internet of things in order to provide feedback to end users, which helps them in better decision making and improves the performance of the corresponding internet of things network. Analysis of big data in the internet of things is obviously a hard task. Data storages are distributed and there should be parallel data processing. Transmission of the data across the network can slow down because of the massive amount of data. In this regard, this paper focuses on how to analyze the massive and heterogeneous data of the internet of things in a proper way. At first, the internet of things and the big data are discussed separately with architectures, applications, challenges etc. Since these two technologies are interrelated, data analysis in the internet of things is discussed with various methodologies and challenges-Finally, the study discusses a proper framework that can analyze the big data in the internet of things in an efficient way.},
keywords={Big Data;Internet of Things;Computer architecture;Sensors;Real-time systems;Protocols;Perception layer;parallel data processing;heterogeneous data;framework;internet of things},
doi={10.1109/NITC.2017.8285647},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8257965,
author={Yu, Yangwen and Yu, James J.Q. and Li, Victor O. K. and Lam, Jacqueline C. K.},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Low-rank singular value thresholding for recovering missing air quality data},
year={2017},
volume={},
number={},
pages={508-513},
abstract={With the increasing awareness of the harmful impacts of urban air pollution, air quality monitoring stations have been deployed in many metropolitan areas. These stations provide air quality data to the public. However, due to sampling device failures and data processing errors, missing data in air quality measurements is common. Data integrity becomes a critical challenge when such data are employed for public services. In this paper, we investigate the mathematical property of air quality measurements, and attempt to recover the missing data. First, we empirically study the low rank property of these measurements. Second, we formulate the low rank matrix completion (LRMC) optimization problem to reconstruct the missing air quality data. The problem is transformed using duality theory, and singular value thresholding (SVT) is employed to develop sub-optimal solutions. Third, to evaluate the performance of our methodology, we conduct a series of case studies including different types of missing data patterns. The simulation results demonstrate that the proposed SVT methodology can effectively recover missing air quality data, and outperform the existing Interpolation. Finally, we investigate the parameter sensitivity of SVT. Our study can serve as a guideline for missing data recovery in the real world.},
keywords={Pollution measurement;Atmospheric measurements;Air pollution;Monitoring;Optimization;Atmospheric modeling;Missing data recovery;air quality measurements;low-rank matrix completion;singular value thresholding},
doi={10.1109/BigData.2017.8257965},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9524052,
author={Fangke, Chen},
booktitle={2020 International Conference on Robots & Intelligent System (ICRIS)},
title={Application Strategy of Armed Police Force Logistics Construction Based on Data System Engineering},
year={2020},
volume={},
number={},
pages={401-404},
abstract={In order to overcome the problems existing in the logistics construction of the armed police force, such as the data standard is not unified and the real-time monitoring is not available, this paper proposes a novel application strategy of the armed police force logistics construction Based on data system engineering. The application strategy starts from three dimensions: logistics command decision, logistics fine management and logistics precise support. In addition, the application strategy is introduced into data system engineering, which makes full use of the advantages of data system engineering, and analyzes the feasibility of data acquisition, data use and data processing. The research results show that the application strategy can improve the efficiency and quality of the armed police force logistics work scientifically and efficiently.},
keywords={Law enforcement;Local government;Force;Big Data;Data systems;Real-time systems;Personnel;Data system engineering;Armed police force;Logistics construction;Big data},
doi={10.1109/ICRIS52159.2020.00104},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7027578,
author={Greenlaw, Reynold and Muddiman, Andrew and Friberg, Therese and Moi, Matthias and Cristaldi, Massimo and Ludwig, Thomas and Reuter, Christian},
booktitle={2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
title={The Emergent Project: Emergency Management in Social Media Generation Dealing with Big Data from Social Media Data Streams},
year={2014},
volume={},
number={},
pages={687-689},
abstract={The Emergent project will use social media to support the management of large scale emergencies. The project includes the construction of a big online store of data which will be continuously mined to provide emergency information and alerts. The overall objective is a stronger connection between citizens and emergency management authorities through social media.},
keywords={Media;Semantics;Data mining;Ontologies;Databases;Emergency services;Semantic Web;emergencies;data mining;social media;information mining;information quality},
doi={10.1109/UCC.2014.111},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9322931,
author={Mandrakov, Egor S. and Vasiliev, Victor A. and Dudina, Diana A.},
booktitle={2020 International Conference Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)},
title={Non-conforming Products Management in a Digital Quality Management System},
year={2020},
volume={},
number={},
pages={266-268},
abstract={This article addresses the issue of changes in the quality management system, with the digitalization of the company. More specifically, changes regarding the process of managing non-conforming products. The article reflects how the use of digital tools and new technologies affects the process of detecting non-conformities and how the procedure for working with non-conforming products changes: how is the collection of data on the characteristics of the facility, what decisions need to be made regarding the inconsistencies, how the structure of work with non-conformances is changing and to what extent the use of information tools is beneficial.},
keywords={Quality management;Information technology;Neural networks;Tools;Information security;Big Data;Standards organizations;quality management system;digital transformation;digital environment;non-conforming products;big data;neural networks;internet of things},
doi={10.1109/ITQMIS51053.2020.9322931},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9391620,
author={Junaid, Muhammad and Wagan, Shiraz Ali and Qureshi, Nawab Muhammad Faseeh and Nam, Choon Sung and Shin, Dong Ryeol},
booktitle={2020 Global Conference on Wireless and Optical Technologies (GCWOT)},
title={Big data Predictive Analytics for Apache Spark using Machine Learning},
year={2020},
volume={},
number={},
pages={1-7},
abstract={In today's digital world data is producing at a rapid speed and handling this massive diverse data become more challenging. The environment of big data is capable of handling data efficiently from data warehouses and in real-time. In Big data environment, Apache Spark is cluster-based, open-source computing technology explicitly designed for bulky data handling. Apache spark services are to perform composite Analytics through in-memory processing. This plays an active role in making meaningful exploration through machine learning and processes a large amount of data. Machine learning API is known as Mllib. It is highly prominent and efficient for big data platforms also offers excellent functionalities. In this paper, we have performed an experiment to look at the analytical qualities of Mllib in the apache spark environment. Likewise, we have highlighted the modern tendencies of Machine learning in big data studies and provides an understanding of upcoming work.},
keywords={Wireless communication;Runtime;Cluster computing;Machine learning;Big Data;Real-time systems;Sparks;apache-spark;clusters;predictive analysis;Mllib;pandas;5Vs of big data},
doi={10.1109/GCWOT49901.2020.9391620},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8875279,
author={Zhang, Xi and Wang, Jingqing and Zhu, Qixuan},
booktitle={2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)},
title={Q-Learning Based Energy Harvesting for Heterogeneous Statistical QoS Provisioning over Multihop Big-Data Relay Networks},
year={2019},
volume={},
number={},
pages={807-814},
abstract={With the increasing demand for the data-intensive wireless multimedia services over the time-varying wireless channels, the big-data based wireless network problem demands the 5G candidate framework to process such massive amount of multimedia data without causing extra burden to the backhaul links in supporting the heterogeneous statistical delay-bounded quality-of-service (QoS) provisionings. Due to the benefits of energy harvesting (EH) technologies, wireless devices are able to support the data-intensive wireless multimedia services by harvesting energy from the environment. Energy harvesting has emerged as the promising technology to solve the energy supply problem while bringing new challenges due to the stochastic nature of the harvested energy in supporting the heterogeneous statistical quality-of-service (QoS) provisionings. However, due to the unknown dynamics of the harvested energy as well as the channel state information (CSI), it is challenging to design the efficient routing protocol for selecting the optimal routing and power allocation policies under the statistical delay-bounded QoS constraints. To overcome the aforementioned problems, in this paper we propose the Q-learning based optimal routing and power allocation policies through learning from the history of the energy harvesting process while satisfying the heterogeneous statistical delay-bounded QoS constraints over multihop big-data relay networks. In particular, under the heterogeneous statistical delay-bounded QoS requirements, we formulate the end-to-end effective-capacity optimization problem for the battery-free energy harvesting based big-data multihop relay networks. Then, we apply the Markov decision process as well as Q-learning methods for deriving the optimal multihop routing algorithms over big-data multihop relay networks. Also conducted is a set of simulations which evaluate the system performances and show that our proposed Q-learning based multihop routing scheme outperforms the other existing schemes under the heterogeneous statistical delay-bounded QoS constraints over multihop big-data relay networks.},
keywords={Quality of service;Spread spectrum communication;Relay networks (telecommunications);Wireless communication;Energy harvesting;Routing;Resource management;Q-learning;energy harvesting;heterogeneous statistical delay-bounded quality of service (QoS);effective capacity;multihop big-data relay networks},
doi={10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00147},
ISSN={},
month={July},}
@INPROCEEDINGS{8386529,
author={Huang, Weinan and Chen, Junyi and Meng, Lei and Lillis, David},
booktitle={2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
title={Towards an open science platform for the evaluation of data fusion},
year={2018},
volume={},
number={},
pages={290-294},
abstract={Combining the results of different search engines in order to improve upon their performance has been the subject of many research papers. This has become known as the “Data Fusion” task, and has great promise in dealing with the vast quantity of unstructured textual data that is a feature of many Big Data scenarios. However, no universally-accepted evaluation methodology has emerged in the community. This makes it difficult to make meaningful comparisons between the various proposed techniques from reading the literature alone. Variations in the datasets, metrics, and baseline results have all contributed to this difficulty. This paper argues that a more unified approach is required, and that a centralised software platform should be developed to aid researchers in making comparisons between their algorithms and others. The desirable qualities of such a system have been identified and proposed, and an early prototype has been developed. Re-implementing algorithms published by other researchers is a great burden on those proposing new techniques. The prototype system has the potential to greatly reduce this burden and thus encourage more comparable results being generated and published more easily.},
keywords={Data integration;Task analysis;Measurement;Standards;Prototypes;Training data;Big Data;information retrieval;data fusion;evaluation},
doi={10.1109/ICCCBDA.2018.8386529},
ISSN={},
month={April},}
@INPROCEEDINGS{8529703,
author={Das, Nivedita and Das, Leena and Rautaray, Siddharth Swarup and Pandey, Manjusha},
booktitle={2018 3rd International Conference for Convergence in Technology (I2CT)},
title={Detection and Prevention of HIV AIDS Using Big Data Tool},
year={2018},
volume={},
number={},
pages={1-5},
abstract={Big Data is an accumulation of data sets which are abundant and intricate in character. They comprise both structured and unstructured data that evolve abundant, so speedy they are not convenient by classical relational database systems or current analytical tools. Big Data Analytics is not linearly able to expand. It is a predefined schema. Big data analysis is the demanding one because it contains huge number of records. In today's world, the massive instruction about health care is to be prepared in order to recognize, diagnose, identify and prevent the various diseases. It is projected to develop a centralized patient monitoring system using big data. Health care is the conservation or advancement of health along the avoidance, interpretation and medical care of disorder, bad health, abuse, and other substantial and spiritual deterioration in mortal. One of the best areas where big data can be used to form an advance medical care. Medical care has the potential to reduce costs of treatment, predict outbreaks of epidemics, avoid preventable diseases and improve the quality of life in general. Average human lifespan is increasing along world population, which poses new challenges to today's treatment delivery methods. A disease is a particular abnormal condition that affects part or all of an organism not caused by external force and that consists of a disorder of a structure or function, usually serving as an evolutionary disadvantage. The study of disease is called pathology, which includes the study of cause. This paper mainly focuses on the prediction of disease like HIV/AIDS using R programming.},
keywords={Big Data;Viruses (medical);Human immunodeficiency virus;Relational databases;Predictive Analysis;Big Data Tools;Proposed framework;R-Programming},
doi={10.1109/I2CT.2018.8529703},
ISSN={},
month={April},}
@INPROCEEDINGS{9208288,
author={Rahman, Md. Saifur and Reza, Hassan},
booktitle={2020 IEEE International Conference on Electro Information Technology (EIT)},
title={Systematic Mapping Study of Non-Functional Requirements in Big Data System},
year={2020},
volume={},
number={},
pages={025-031},
abstract={Big data has become the most popular and influential to exist in this competitive digital world. In this regard, the selection of suitable quality attributes in big data software architecture can play a million-dollar solution. In this paper, we work on gathering and understanding key non-functional requirements in the domain of big data systems. Using Systematic Mapping Study (SMS) as a research methodology, we find more than 40 different quality attributes related to big data systems. Then, we implement the ISO/IEC 25010:2011 quality model to map all these NFRs into 8 characteristics of ISO/IEC 25010:2011 model. Finally, we show performance efficiency, functional suitability, reliability, security, usability, and scalability are the most important quality attributes for a data-intensive system. Outcomes from this survey will assist software developers to understand NFRs for data-intensive systems and identify them long before implementing them into practice.},
keywords={Big Data;Software architecture;Software;IEC Standards;ISO Standards;Databases;Systematics;Non-functional requirements;Quality attributes;Big data system;ISO/IEC 25010:2011},
doi={10.1109/EIT48999.2020.9208288},
ISSN={2154-0373},
month={July},}
@INPROCEEDINGS{9006251,
author={Oehmcke, Stefan and Thrysøe, Christoffer and Borgstad, Andreas and Salles, Marcos Antonio Vaz and Brandt, Martin and Gieseke, Fabian},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Detecting Hardly Visible Roads in Low-Resolution Satellite Time Series Data},
year={2019},
volume={},
number={},
pages={2403-2412},
abstract={Massive amounts of satellite data have been gathered over time, holding the potential to unveil a spatiotemporal chronicle of the surface of Earth. These data allow scientists to investigate various important issues, such as land use changes, on a global scale. However, not all land-use phenomena are equally visible on satellite imagery. In particular, the creation of an inventory of the planet's road infrastructure remains a challenge, despite being crucial to analyze urbanization patterns and their impact. Towards this end, this work advances data-driven approaches for the automatic identification of roads based on open satellite data. Given the typical resolutions of these historical satellite data, we observe that there is inherent variation in the visibility of different road types. Based on this observation, we propose two deep learning frameworks that extend state-of-the-art deep learning methods by formalizing road detection as an ordinal classification task. In contrast to related schemes, one of the two models also resorts to satellite time series data that are potentially affected by missing data and cloud occlusion. Taking these time series data into account eliminates the need to manually curate datasets of high-quality image tiles, substantially simplifying the application of such models on a global scale. We evaluate our approaches on a dataset that is based on Sentinel 2 satellite imagery and OpenStreetMap vector data. Our results indicate that the proposed models can successfully identify large and medium-sized roads. We also discuss opportunities and challenges related to the detection of roads and other infrastructure on a global scale.},
keywords={Roads;Satellites;Data mining;Machine learning;Spatial resolution;Data models;Time series analysis;Remote Sensing;Deep Learning;Segmentation;Satellite Data;Big Data;Road and Infrastructure Detection},
doi={10.1109/BigData47090.2019.9006251},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6602615,
author={Tien, James M.},
booktitle={2013 10th International Conference on Service Systems and Service Management},
title={Big Data: Unleashing information},
year={2013},
volume={},
number={},
pages={4-4},
abstract={Summary form only given. At present, it is projected that about 4 zettabytes (or 10**21 bytes) of electronic data are being generated per year by everything from underground physics experiments to retail transactions to security cameras to global positioning systems. In the U. S., major research programs are being funded to deal with big data in all five economic sectors (i.e., services, manufacturing, construction, agriculture and mining) of the economy. Big Data is a term applied to data sets whose size is beyond the ability of available tools to undertake their acquisition, access, analytics and/or application in a reasonable amount of time. Whereas Tien (2003) forewarned about the data rich, information poor (DRIP) problems that have been pervasive since the advent of large-scale data collections or warehouses, the DRIP conundrum has been somewhat mitigated by the Big Data approach which has unleashed information in a manner that can support informed - yet, not necessarily defensible or knowledgeable - decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications, appropriate actions can be undertaken with the obtained information. New acquisition, access, analytics and application technologies are being developed to further Big Data as it is being employed to help resolve the 14 grand challenges (identified by the National Academy of Engineering in 2008), underpin the 10 breakthrough technologies (compiled by the Massachusetts Institute of Technology in 2013) and support the Third Industrial Revolution of mass customization.},
keywords={Information management;Data handling;Data storage systems;Educational institutions;Physics;Security;Cameras},
doi={10.1109/ICSSSM.2013.6602615},
ISSN={2161-1904},
month={July},}
@INPROCEEDINGS{9539334,
author={Yao, Wang},
booktitle={2020 International Conference on Virtual Reality and Intelligent Systems (ICVRIS)},
title={Research on Static Software Defect Prediction Algorithm Based on Big Data Technology},
year={2020},
volume={},
number={},
pages={610-613},
abstract={The static page processing software is easily disturbed by code defects, which causes the static page processing software to be paralyzed, thus making the accuracy of the static page processing poor. In order to improve the automatic prediction capability of the static page processing software, a code defect prediction technology for the static page processing software based on big data fusion and defect feature location technology algorithm is proposed, and the syntax running state characteristics of the operation and maintenance control management layer and software source code of the static page processing software are analyzed and tested. Using polymorphic software to drive the control program to carry out fault feature monitoring and information fusion of the page static processing software, carrying out polymorphic factor fusion and state feature analysis on the large data of defect fault feature distribution in the pseudo code of the software control program, combining Boehm model and ISO/IEC 9126 model to realize fault feature point location and defect active prediction of the page static processing software, According to the logicality of functions, codes and state variables of the page static processing software, the method of software running program continuity and similarity feature detection is adopted to realize the self-adaptive defect prediction and positioning of the page static processing software, and the global convergence control in defect prediction of the page static processing software is carried out by combining the big data fusion and defect feature positioning algorithm. The simulation results show that the prediction accuracy of page static processing software defects using this method is higher, the localization of defect codes is better, and the reliable operation capability of page static processing software is improved.},
keywords={Solid modeling;Codes;Software algorithms;Process control;Big Data;Predictive models;Prediction algorithms;big data technology;Static software;Defects;Prediction algorithm},
doi={10.1109/ICVRIS51417.2020.00150},
ISSN={},
month={July},}
@INPROCEEDINGS{7752415,
author={Lanius, Candice L.},
booktitle={2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
title={Arguments and interpretation in big social data analysis: A survey of the ASONAM community},
year={2016},
volume={},
number={},
pages={1364-1367},
abstract={Big social data is becoming an important part of human decision making around the world. With the high stakes of decisions based on technical systems, it is important to evaluate the role of researchers in shaping that shared future. I present the results of a survey of 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining participants who performed big social data analyses. The goal was to understand how data scientists use interpretation to complete their projects and how they communicate results to their audience. By looking at research design as both a technical roadmap and an argument, results generated from social media data sets can be evaluated for their quality. Ultimately, these results will assist in the creation of field-dependent evaluation standards than can be used by big social data researchers.},
keywords={Social network services;Standards;Data analysis;Big data;Data mining;Data models;Decision making;survey;big social data;arguments;ground truth;communication;research design;ethics;policy},
doi={10.1109/ASONAM.2016.7752415},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6691561,
author={Kumar, Dheeraj and Palaniswami, Marimuthu and Rajasegarar, Sutharshan and Leckie, Christopher and Bezdek, James C. and Havens, Timothy C.},
booktitle={2013 IEEE International Conference on Big Data},
title={clusiVAT: A mixed visual/numerical clustering algorithm for big data},
year={2013},
volume={},
number={},
pages={112-117},
abstract={Recent algorithmic and computational improvements have reduced the time it takes to build a minimal spanning tree (MST) for big data sets. In this paper we compare single linkage clustering based on MSTs built with the Filter-Kruskal method to the proposed clusiVAT algorithm, which is based on sampling the data, imaging the sample to estimate the number of clusters, followed by non-iterative extension of the labels to the rest of the big data with the nearest prototype rule. Numerical experiments with both synthetic and real data confirm the theory that clusiVAT produces true single linkage clusters in compact, separated data. We also show that single linkage fails, while clusiVAT finds high quality partitions that match ground truth labels very well. And clusiVAT is fast: it recovers the preferred c = 3 Gaussian clusters in a mixture of 1 million two-dimensional data points with 100% accuracy in 3.1 seconds.},
keywords={Information management;Data handling;Data storage systems;Clustering algorithms;Partitioning algorithms;Couplings;Visualization;Cluster Analysis;Pattern Recognition;Single Linkage;Big Data;Filter-Kruskal MST},
doi={10.1109/BigData.2013.6691561},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9403807,
author={Hou, Xuemei and Gao, Fei and Yu, Lei and Chen, Yufei},
booktitle={2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)},
title={Test Data Generation Method Based on Siamese Network in Face Recognition System},
year={2020},
volume={},
number={},
pages={454-457},
abstract={With the development of AI and big data technology, machine learning system has been widely used in various fields and achieved satisfactory results. However, due to the increasing complexity of machine learning system, there are inevitably a variety of unpredictable problems, errors or failures in machine learning system, which cause fatal damage to the quality and reliability of machine learning system. In order to test machine learning system better, automatic test data generation is the key technology. An improved test data generation method based on GAN and siamese network in face recognition system is proposed. The method focuses on discriminator module composed of siamese network which can better measure the similarity between real and fake images. The results show that the proposed test data generation algorithm can generate images that are very close to the real samples. The test data of machine learning system is enhanced.},
keywords={Training;Machine learning algorithms;Image recognition;Face recognition;Machine learning;Generative adversarial networks;Feature extraction;Test data generation;Siamese network;Face recognition system;GAN},
doi={10.1109/ICBASE51474.2020.00102},
ISSN={},
month={Oct},}
@ARTICLE{8936349,
author={Zhaofeng, Ma and Lingyun, Wang and Xiaochang, Wang and Zhen, Wang and Weizhe, Zhao},
journal={IEEE Internet of Things Journal},
title={Blockchain-Enabled Decentralized Trust Management and Secure Usage Control of IoT Big Data},
year={2020},
volume={7},
number={5},
pages={4000-4015},
abstract={With the fast development of Internet-of-Things (IoT) technologies, IoT big data and its applications are getting more and more useful. However, traditional IoT data management is fragile and vulnerable. Once the gathered data are untrusted or the stored data are tampered with deliberately from the internal users or attacked by an external hacker, then the tampered data have a serious problem to be utilized. To solve the problems of trust and security of IoT big data management, in this article, we propose a permissioned blockchain-based decentralized trust management and secure usage control scheme of IoT big data (called BlockBDM), upon which all the data operations and management, such as data gathering, invoking, transfer, storage, and usage, are processed over the blockchain smart contract. To encourage the IoT client to supply high-quality content, in our scheme, we design public-blockchain-based tokens reward mechanism for the high-quality data supply contribution. All the data processing and usage procedure can be recorded in a cryptography-signed and Merkle tree-based transaction(s) and block(s) with high-level security in a global and distributed ledger with tamper resistance. For data utilization and consumption, we propose secure usage control for digital rights management and token-based data consumption approach of high-value data from being violated or spread without any limitation. We implemented the BlockBDM scheme based on public and permissioned blockchain for IoT big data management. Finally, a large amount of evaluation manifests that the proposed BlockBDM scheme is feasible, secure, and scalable for decentralized trust management of IoT big data.},
keywords={Blockchain;Big Data;Trust management;Cloud computing;Internet of Things;Cryptography;Big data;blockchain;decentralized trust management;Internet of Things (IoT);secure usage control},
doi={10.1109/JIOT.2019.2960526},
ISSN={2327-4662},
month={May},}
@INPROCEEDINGS{7403678,
author={Tekieh, Mohammad Hossein and Raahemi, Bijan},
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
title={Importance of data mining in healthcare: A survey},
year={2015},
volume={},
number={},
pages={1057-1062},
abstract={In this survey, we collect the related information that demonstrate the importance of data mining in healthcare. As the amount of collected health data is increasing significantly every day, it is believed that a strong analysis tool that is capable of handling and analyzing large health data is essential. Analyzing the health datasets gathered by electronic health record (EHR) systems, insurance claims, health surveys, and other sources, using data mining techniques is very complex and is faced with very specific challenges, including data quality and privacy issues. However, the applications of data mining in healthcare, advantages of data mining techniques over traditional methods, special characteristics of health data, and new health condition mysteries have made data mining very necessary for health data analysis.},
keywords={Data mining;Diseases;Insurance;Data analysis;Organizations;Sociology;data mining;health data analysis;data quality;predictive modelling;health big data;data mining applications},
doi={10.1145/2808797.2809367},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9526742,
author={Liu, Bo},
booktitle={2020 13th International Conference on Intelligent Computation Technology and Automation (ICICTA)},
title={Research on Data Search Optimization under Big Data Based on Association Rule Algorithm},
year={2020},
volume={},
number={},
pages={610-613},
abstract={The construction industry is an important pillar industry of our national economy. It has played an enormous role in promoting China's economic development and social progress. With the establishment of the socialist market economy and the continuous deepening of reforms, the competition among construction companies within the construction industry has become increasingly fierce, the construction period has shortened, the quality requirements have become higher, the cost of construction materials has continued to rise, and the labor costs have continued to increase. The survival and development of enterprises have brought great pressure, the competition in the construction industry has become increasingly fierce, and the profit-making space has continued to decrease, making construction companies have to develop towards refinement, information, and integration. And level, in order to improve the management level of construction companies, to achieve the “construction of internal quality, appearance of the tree” of construction companies, and ultimately improve the competitiveness of the construction industry in the world.},
keywords={Costs;Correlation;Databases;Companies;Medical services;Data mining;Task analysis;big data;association rules;data mining;algorithm;optimization},
doi={10.1109/ICICTA51737.2020.00135},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8648695,
author={Sukumar Gurugubelli, Venkata and Li, Zhouzhou and Wang, Honggang and Fang, Hua},
booktitle={2018 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)},
title={A Computational Efficient Fuzzy Clustering Algorithm for Big Incomplete Longitudinal Trial Data},
year={2018},
volume={},
number={},
pages={25-26},
abstract={Our previous research has shown that the enhanced Fuzzy C-Means algorithm (eFCM) can improve the clustering quality in the big cross-sectional "synthetic data" [1] where a substantive number of clusters are identified with different degrees of overlap, with enhanced computational efficiency. Here, we provide additional evidence to showcase that our novel initialization method built in MIFuzzy [2-9] [10-12] can improve the computational efficiency in big longitudinal intervention data with missing values. Our numerical analyses further show this improvement in identifying clusters from simulated big incomplete longitudinal data generated using the parameters of our real longitudinal intervention data with missing values. Our findings imply the applicability of this algorithm to similar studies.},
keywords={Computational efficiency;Clustering algorithms;Standards;Clustering methods;Runtime;Information science;Roads;MIFuzzy, eFCM, Longitudinal, RCT, Computational Efficiency, Big Temporal data},
doi={10.1145/3278576.3278588},
ISSN={},
month={Sep.},}
@ARTICLE{8825507,
author={Kaur, Kuljeet and Garg, Sahil and Kaddoum, Georges and Bou-Harb, Elias and Choo, Kim-Kwang Raymond},
journal={IEEE Transactions on Industrial Informatics},
title={A Big Data-Enabled Consolidated Framework for Energy Efficient Software Defined Data Centers in IoT Setups},
year={2020},
volume={16},
number={4},
pages={2687-2697},
abstract={The rapidly evolving industry standards and transformative advances in the field of Internet of Things are expected to create a tsunami of Big Data shortly. This, in turn, will demand real-time data analysis and processing from cloud computing platforms. A substantial part of the computing infrastructure is supported by large-scale and geographically distributed data centers (DCs). Nevertheless, these DCs impose a substantial cost in terms of rapidly growing energy consumption, which in turn adversely affects the environment. In this context, efficient resource utilization is seen as a potential candidate to enhance energy efficiency and minimize the load on the power sector. Nevertheless, in the majority of the public clouds, the resources are idle most of the time (i.e., under-utilized) as the load of the servers is unpredictable; thereby leading to a lofty increase in the energy utilization index and wastage of resources. Thus, it is highly essential to devise a precise and efficient resource management technique. Therefore, in this article, we leverage the advantages of software defined data centers (SDDCs) to minimize energy utilization levels. Precisely, SDDC refers to the process of programmatically abstracting the logical computing, network, and storage resources; and configuring them in real-time based on workload demands. In detail, we demonstrate the possibility of 1) designing a consolidated SDDC-based model to jointly optimize the process of virtual machine (VM) deployment and network bandwidth allocation for reduced energy consumption and guaranteed quality of service (QoS), particularly for heterogeneous computing infrastructures; 2) formulating a multiobjective optimization problem to deduce the optimal allocation of resources for both critical and noncritical applications; and 3) designing an efficient scheme based on heuristics to provide suboptimal results for the formulated multiobjective optimization problem. The proposed article presents a suboptimal approach based on first fit decreasing algorithm. Further, our empirical evaluations suggest that the proposed framework leads to almost 27.9% savings in terms of energy consumptions against the existing schemes with negligible QoS violations (approximately 0.33).},
keywords={Quality of service;Switches;Data centers;Cloud computing;Software;Virtual machining;Internet of Things;Big Data;cloud computing;energy minimization;Internet of Things;multiobjective optimization problem;prioritized scheduling;software defined data centers;and virtual machines},
doi={10.1109/TII.2019.2939573},
ISSN={1941-0050},
month={April},}
@INPROCEEDINGS{8817138,
author={Bagozi, Ada},
booktitle={2019 IEEE World Congress on Services (SERVICES)},
title={IDEAaS: Interactive Data Exploration As-a Service},
year={2019},
volume={2642-939X},
number={},
pages={345-348},
abstract={Recently servitization has been proposed as a strategic business innovation to enrich products offerings with the delivery of remote services (e.g. remote monitoring services), thus also improving the perception of the product quality. The increasing connections of systems that produce high volumes of real time data have raised the need for advanced Data Exploration techniques able to face the impact of Big Data, in order to make remote monitoring services sustainable. In this paper, the IDEAaS (Interactive Data Exploration As-a-Service) approach is presented, apt to support and ease exploration of real time data in a dynamic context of interconnected systems, where large amounts of data must be incrementally collected, organised and analysed on-the-fly. The proposed approach relies on three main pillars: (i) a multi-dimensional organisation of data, for data exploration according to different analysis dimensions; (ii) data summarisation, based on incremental clustering algorithm, to provide summarised representation of collected data streams; (iii) data relevance evaluation techniques, to attract the users attention on relevant data only during exploration. Finally, the approach has been tested in a Smart Factory context, applying the interactive data exploration techniques in order to assist anomaly detection in remote monitoring services.},
keywords={Data models;Big Data;Analytical models;Industries;Tools;Remote monitoring;data exploration, big data, Internet of Things, Internet of Services, service oriented architecture, Industry 4.0},
doi={10.1109/SERVICES.2019.00096},
ISSN={2642-939X},
month={July},}
@ARTICLE{8291120,
author={He, Xiaoming and Wang, Kun and Huang, Huawei and Liu, Bo},
journal={IEEE Communications Magazine},
title={QoE-Driven Big Data Architecture for Smart City},
year={2018},
volume={56},
number={2},
pages={88-93},
abstract={In the era of big data, the applications/services of the smart city are expected to offer end users better QoE than in a conventional smart city. Nevertheless, various types of sensors will produce an increasing volume of big data along with the implementation of a smart city, where we face redundant and diverse data. Therefore, providing satisfactory QoE will become the major challenge in the big-data-based smart city. In this article, to enhance the QoE, we propose a novel big data architecture consisting of three planes: the data storage plane, the data processing plane, and the data application plane. The data storage plane stores a wide variety of data collected by sensors and originating from different data sources. Then the data processing plane filters, analyzes, and processes the ocean of data to make decisions autonomously for extracting high-quality information. Finally, the application plane initiates the execution of the events corresponding to the decisions delivered from the data processing plane. Under this architecture, we particularly use machine learning techniques, trying to acquire accurate data and deliver precise information to end users. Simulation results indicate that our proposals could achieve high QoE performance for the smart city.},
keywords={Big Data;Smart cities;Quality of experience;Memory;Data mining;Sensors},
doi={10.1109/MCOM.2018.1700231},
ISSN={1558-1896},
month={Feb},}
@INPROCEEDINGS{8095957,
author={Mao, Meiqin and Wang, Yangyang and Yue, You and Chang, Liuchen},
booktitle={2017 IEEE Energy Conversion Congress and Exposition (ECCE)},
title={Multi-time scale forecast for schedulable capacity of EVs based on big data and machine learning},
year={2017},
volume={},
number={},
pages={1425-1431},
abstract={The application of large-scale electric vehicles (EVs) into the future smart grid may bring about serious power quality problems. But EVs can provide ancillary services for the power system as distributed energy resources through Vehicle-to-grid (V2G) technology. The fast and accurate prediction of schedulable capacity (SC) of EVs is the first step to enable this benefit. In this paper, two different time scales schedulable capacity forecasting (SCF) methods are proposed, including the real-time SCF method and a day head SCF method. Firstly, the real time of SCF of EVs is calculated by collecting the real-time charging or discharging data of individual EVs and a distributed data parallel calculation method to solve problems of the mass data process and storage problem. Secondly, the large amount of results of the real-time SCF, is used as historical data, to train the SCF models one day ahead by parallel decision tree regression algorithm (PDTR) and parallel random forest regression algorithm (PRFR). These proposed methods are evaluated by the real time operation data involving 521 EVs in the filed in China on Spark. The test results show the advantage of using big data technology in the ability and speed of dealing with large-scale data, and PDTR is faster and better than PRFR in one day ahead SCF.},
keywords={Real-time systems;Sparks;Big Data;Regression tree analysis;Predictive models;Data models;electric vehicle;schedulable capacity;big data;multi-time scale;spark},
doi={10.1109/ECCE.2017.8095957},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8672697,
author={Khader, Mariam and Awajan, Arafat and Al-Naymat, Ghazi},
booktitle={2018 International Arab Conference on Information Technology (ACIT)},
title={The Effects of Natural Language Processing on Big Data Analysis: Sentiment Analysis Case Study},
year={2018},
volume={},
number={},
pages={1-7},
abstract={The social networks are one of the main sources of big data. Continuously, it produce huge volume of variety types of data at high velocity rates. This huge volume of data contains valuable information that requires efficient and scalable analysis techniques to be extracted. Hadoop/MapReduce is considered the most suitable framework for handling big data because of its scalability, reliability and simplicity. One of the basic applications to extract valuable information from data is the sentiment analysis. The sentiment analysis studies peoples' opinion by classifying their written text into positive or negative polarity. In this work, a sentiment analysis method for analyzing a Twitter data set is analyzed. The method uses the Naive Bayes algorithm for classifying the text into positive and negative polarity. Several linguistic and NLP preprocessing techniques were applied on the data set. The aim of these preprocessing techniques is to study their effects on the quality of big data classification. The applied preprocessing techniques have achieved an enhancement in the classification accuracy of the Naive Bayes algorithm. The experiments prove that the performance of the sentiment analysis is enhanced by 5% using NLP and linguistic processing, yielding an accuracy of 73 % on the used data set.},
keywords={Sentiment analysis;Classification algorithms;Linguistics;Twitter;Big Data;Data mining;Big Data;Natural Language Processing;Mahout;MapReduce Framework;Naive Bayes;Sentiment Analysis},
doi={10.1109/ACIT.2018.8672697},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8466465,
author={Wai, Khaing Phyo and Nwe Aung, Than},
booktitle={2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)},
title={Distance-based Clustering of Moving Objects’ Trajectories from Spatiotemporal Big Data},
year={2018},
volume={},
number={},
pages={567-572},
abstract={With the rapid development of smart sensors, smartphones and social media, distributed sensors and tracking systems are generating overwhelming amounts of high velocity spatiotemporal big data. Clustering spatiotemporal data is an important way to mine hidden information behind moving object sampling data, such as understanding trends in movement patterns, gaining high popularity in geographic information and so on. However, the current approaches for clustering trajectory data generally do not apply for excessive costs in both scalability and computing performance for spatiotemporal big data. To find the clusters of moving objects’ trajectories, the issue is to find a distance measurement method that respects the geographic distance and the semantic similarity for each trajectory. Therefore, a distance measure to compute the spatial similarity between trajectories based on both geographical features and semantic features of motion is proposed in this research, then a framework for clustering moving objects trajectories is also designed to find out the groups of similar paths from big spatiotemporal data. The cluster quality of the proposed method is validated by means of external and internal validation criteria and is practically evaluated by TDrive datasets which is real trajectory dataset.},
keywords={Trajectory;Semantics;Spatiotemporal phenomena;Clustering algorithms;Big Data;Hidden Markov models;Global Positioning System;spatiotemporal big data;moving object’ trajectories;geographic distance;semantic similarity},
doi={10.1109/ICIS.2018.8466465},
ISSN={},
month={June},}
@INPROCEEDINGS{7004410,
author={Ruan, Guangchen and Zhang, Hui and Plale, Beth},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Parallel and quantitative sequential pattern mining for large-scale interval-based temporal data},
year={2014},
volume={},
number={},
pages={32-39},
abstract={Mining frequent subsequences of patterns, or sequential pattern mining, has wide application in customer shopping sequence analysis, web log stream analysis, multi-modal behavioral studies, to name a few. To detect unknown, anomalous, and unexpected patterns from large-scale interval-based temporal data without complete a priori knowledge is challenging. In this paper, we present a framework - PESMiner which allows parallel and quantitative mining of sequential patterns at scale. Whereas most existing sequential mining algorithms can only find sequential orders of temporal events, our work presents a novel interactive temporal data mining algorithm capable of extracting precise temporal properties of sequential patterns. Furthermore, our work provides a unified parallel solution that scales our algorithms to larger temporal data sets by exploiting iterative MapReduce tasks. Comprehensive performance evaluations demonstrate that PESMiner significantly outperforms existing interval-based mining algorithms in terms of both quality (i.e. accuracy, precision, and recall) and scalability.},
keywords={Pattern matching;Data mining;Clustering algorithms;Prototypes;Algorithm design and analysis;Web services;Educational institutions;quantitative sequential pattern mining;iterative MapReduce;interval-based temporal data},
doi={10.1109/BigData.2014.7004410},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8258381,
author={Martey, Emmanuel Nii and Ahmed, Lasisi and Attoh-Okine, Nii},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={Track geometry big data analysis: A machine learning approach},
year={2017},
volume={},
number={},
pages={3800-3809},
abstract={Track geometry has a considerable effect on rail travel comfort and safety and deteriorates with age and tonnage. In order to maintain the track geometry quality, maintenance activities such as tamping, stone blowing and ballast undercutting are usually employed. However, these activities are ineffective if the underlying cause of track deformation such as subgrade failure is not addressed. Geosyn-thetics such as geocells and geogrids can be placed in the subballast which strengthens the layer, lowers the stresses on the weak subgrade and invariably enhances track geometry quality. Machine learning techniques are becoming increasingly imperative in processing and analyzing of large volumes of track geometry data which exhibit the classical attributes of big data. Several unsupervised and supervised learning techniques were used to analyze the effect of geocell installation on track geometry quality. Cluster analysis was used to group the track geometry data with major clusters found to differ by surface and alignment features. Principal component analysis was employed as an effective dimension reduction tool to simplify the track geometry data based on the proportion of variance explained. Supervised learning techniques such as multiple linear regression, decision tree regression, random forest regression and support vector regression were subsequently used to estimate and predict the effect of geocell installation on the track geometry quality. Random forest regression was found to the best performing model for both the original and dimensionally-reduced data.},
keywords={Geometry;Maintenance engineering;Supervised learning;Safety;Decision trees;Support vector machines;Rails;machine learning;track geometry;railroad;supervised learning;unsupervised learning},
doi={10.1109/BigData.2017.8258381},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7004457,
author={Rupp, C.J. and Rayson, Paul and Gregory, Ian and Hardie, Andrew and Joulain, Amelia and Hartmann, Daniel},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Dealing with heterogeneous big data when geoparsing historical corpora},
year={2014},
volume={},
number={},
pages={80-83},
abstract={It has long been known that `variety' is one of the key challenges and opportunities of big data. This is especially true when we consider the variety of content in historical corpora resulting from large-scale digitisation activities. Collections such as Early English Books Online (EEBO) and the British Library 19th Century Newspapers are extremely large and heterogeneous data sources containing a variety of content in terms of time, location, topic, style and quality. The range of geographical locations referenced in these corpora poses a difficult challenge for state of the art geoparsing tools. In the context of our work on Spatial Humanities analyses, we present our solution for dealing with the variety and scale of these corpora.},
keywords={Libraries;Geography;Context;Big data;Geographic information systems;Diseases;Pipelines;Text mining;Toponym Resolution;Historical Corpora;NLP Pipelines and Workflows},
doi={10.1109/BigData.2014.7004457},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9188195,
author={Demchenko, Yuri and Cushing, Reggie and Los, Wouter and Grosso, Paola and de Laat, Cees and Gommans, Leon},
booktitle={2019 International Conference on High Performance Computing & Simulation (HPCS)},
title={Open Data Market Architecture and Functional Components},
year={2019},
volume={},
number={},
pages={1017-1021},
abstract={This paper discusses the principles of organisation and infrastructure components of Open Data Markets (ODM) that would facilitate secure and trusted data exchange between data market participants, and other cooperating organisations. The paper provides a definition of the data properties as economic goods and identifies the generic characteristics of ODM as a Service. This is followed by a detailed description of the generic data market infrastructure that can be provisioned on demand for a group of cooperating parties. The proposed data market infrastructure and its operation are employing blockchain technologies for securing data provenance and providing a basis for data monetisation. Suggestions for trust management and data quality assurance are discussed.},
keywords={Economics;Cloud computing;Data models;Contracts;Big Data;Computer architecture;Open Data Market;Data Marketplace;Trusted Data Market;Industrial Data Space;Data Economics;STREAM Data Properties},
doi={10.1109/HPCS48598.2019.9188195},
ISSN={},
month={July},}
@INPROCEEDINGS{9137477,
author={Zhang, Shuhao and Shao, Caixing and Xiao, Wei},
booktitle={2020 3rd International Conference on Artificial Intelligence and Big Data (ICAIBD)},
title={Research on Red Wine Quality Based on Data Visualization},
year={2020},
volume={},
number={},
pages={128-132},
abstract={Due to the rapid development of modern society, red wine has gradually become popular. Research on the quality of red wine has turn into an important topic. Red wine contains more than 600 kinds of ingredients, in terms of alcohol, minerals, tannic acid, citric acid, chloride and other substances. This paper analyzes 12 factors affecting red wine quality in the data set and studies the influence of each ingredient on the quality of red wine through data mining algorithm. Based on the data visualization of Python processing, classical visualization tools such as histogram, heat map, box-plot and Pearson correlation coefficient algorithm are used for data mining. The histogram is adopted for univariate analysis and the heat map composed of Pearson coefficient is used for multivariate analysis. Then, the box-plot is used for cross-verification. Finally, it is concluded that alcohol, sulfate, citric acid and volatile acidity are the decisive factors affecting the quality of red wine. This conclusion can not only be used as a reference for consumers to buy, but also provide suggestions for wine manufacturers to improve the quality of red wine.},
keywords={Heating systems;Histograms;Thermal factors;Data visualization;Raw materials;Thermal analysis;Minerals;data visualization;red wine quality;data mining},
doi={10.1109/ICAIBD49809.2020.9137477},
ISSN={},
month={May},}
@INPROCEEDINGS{7545062,
author={Tu, Shouzhong and Huang, Minlie},
booktitle={2016 IEEE Second International Conference on Multimedia Big Data (BigMM)},
title={Scalable Functional Dependencies Discovery from Big Data},
year={2016},
volume={},
number={},
pages={426-431},
abstract={Functional dependencies (FDs) represent potentially novel and interesting patterns existent in relational databases. The discovery of functional dependencies has a wide range of applications such as database design, knowledge discovery, data quality assessment, etc. There has been growing interest in the problem of functional dependencies discovery in the last ten years. However, existing functional dependencies discovery algorithms are mainly applied to centralized small data. It is far more challenging to discover functional dependencies from big data. In this paper, we propose an efficient functional dependencies discovery algorithm, for mining functional dependencies from distributed big data. We prune candidate FDs at each node by local fragmented data and batch verify candidate FDs in parallel. Load balance is taken into account when discovering functional dependencies. Experiments show that the proposed algorithm is effective on real dataset and synthetic dataset.},
keywords={Distributed databases;Big data;Lattices;Algorithm design and analysis;Partitioning algorithms;Knowledge discovery;Functional dependencies;Discovering functional dependencies;Knowledge discovery;Big data},
doi={10.1109/BigMM.2016.63},
ISSN={},
month={April},}
@INPROCEEDINGS{7336197,
author={Shi, Weiwei and Zhu, Yongxin and Zhang, Jinkui and Tao, Xiang and Sheng, Gehao and Lian, Yong and Wang, Guoxing and Chen, Yufeng},
booktitle={2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems},
title={Improving Power Grid Monitoring Data Quality: An Efficient Machine Learning Framework for Missing Data Prediction},
year={2015},
volume={},
number={},
pages={417-422},
abstract={Big data techniques has been applied to power grid for the evaluation and prediction of grid conditions. However, the raw data quality rarely can meet the requirement of precise data analytics since raw data set usually contains samples with missing data to which the common data mining models are sensitive. Though classic interpolation or neural network methods can been used to fill the gaps of missing data, their predicted data often fail to fit the rules of power grid conditions. This paper presents a machine learning framework (OR_MLF) to improve the prediction accuracy for datasets with missing data points, which mainly combines preprocessing, optimizing support vector machine (OSVM) and refining SVM (RSVM). On top of the OSVM engine, the scheme introduces dedicated data training strategies. First, the original data originating from data generation facilities is preprocessed through standardization. Traditional SVM is then trained to obtain a preliminary prediction model. Next, the optimized SVM predictors are achieved with new training data set, which is extracted based on the preliminary prediction model. Finally, the missing data prediction result depending on OSVM is selectively inputted into the traditional SVM and the refined SVM is lastly accomplished. We test the OR_MLF framework on missing data prediction of power transformers in power grid system. The experimental results show that the predictors based on the proposed framework achieve lower mean square error than traditional ones. Therefore, the framework OR_MLF would be a good candidate to predict the missing data in power grid system.},
keywords={Support vector machines;Data models;Predictive models;Training;Data mining;Feature extraction;Power grids;missing data prediction;machine learning;support vector machine (SVM);power transformer},
doi={10.1109/HPCC-CSS-ICESS.2015.16},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8703920,
author={Ullah, Faheem and Babar, Muhammad Ali},
booktitle={2019 IEEE International Conference on Software Architecture (ICSA)},
title={An Architecture-Driven Adaptation Approach for Big Data Cyber Security Analytics},
year={2019},
volume={},
number={},
pages={41-50},
abstract={Big Data Cyber Security Analytics (BDCA) systems leverage big data technologies (e.g., Hadoop and Spark) for collecting, storing, and analyzing large volume of security event data to detect cyber-attacks. Accuracy and response time are the two most important quality concerns for BDCA systems. However, the frequent changes in the operating environment of a BDCA system (such as quality and quantity of security event data) significantly impact these qualities. In this paper, we first study the impact of such environmental changes. We then present ADABTics, an architecture-driven adaptation approach that (re)composes the system at runtime with a set of components to ensure optimal accuracy and response time. We finally evaluate our approach both in a single node and multinode settings using a Hadoop-based BDCA system and different adaptation scenarios. Our evaluation shows that on average ADABTics improves BDCA's accuracy and response time by 6.06% and 23.7% respectively.},
keywords={Time factors;Big Data;Detectors;Data integrity;Software architecture;big data;cyber security;adaptivity;software architecture;accuracy;response time},
doi={10.1109/ICSA.2019.00013},
ISSN={},
month={March},}
@ARTICLE{8485466,
author={Qin, Xiaowei and Tang, Shuang and Chen, Xiaohui and Miao, Dandan and Wei, Guo},
journal={China Communications},
title={SQoE KQIs anomaly detection in cellular networks: Fast online detection framework with Hourglass clustering},
year={2018},
volume={15},
number={10},
pages={25-37},
abstract={The explosive growth of data volume in mobile networks makes fast online diagnose a pressing search problem. In this paper, an object-oriented detection framework with a two-step clustering, named as Hourglass Clustering, is given. Where three object parameters are chosen as Synthetical Quality of Experience (SQoE) Key Quality Indicators (KQIs) to reflect accessibility, integrality, and maintainability of networks. Then, we choose represented Key Performance Indicators (rKPIs) as cause parameters with correlation analysis. For these two kinds of parameters, a hybrid algorithm combining the self-organizing map (SOM) and k-medoids is used for clustering them into different types. We apply this framework to online anomaly detection in Cellular Networks, named SQoE-driven Anomaly Detection and Cause Location System (SQoE-ADCL). Our experiments with real 4G data show that besides fast online detection, SQoE-ADCL makes a better soft decision instead of a traditional hard decision. Furthermore, it is also a general way of being applied to other similar applications in big data.},
keywords={Quality of experience;Anomaly detection;Cellular networks;Self-organizing feature maps;Quality of service;Big Data;Monitoring;big data;SQoE;anomaly detection;hourglass clustering;codebook},
doi={10.1109/CC.2018.8485466},
ISSN={1673-5447},
month={Oct},}
@INPROCEEDINGS{7840737,
author={Huang, Zhichuan and Xie, Tiantian and Zhu, Ting and Wang, Jianwu and Zhang, Qingquan},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Application-driven sensing data reconstruction and selection based on correlation mining and dynamic feedback},
year={2016},
volume={},
number={},
pages={1322-1327},
abstract={As sensors spread across almost every industry, the Internet of Things (IoT) is going to trigger an era of big data. However, the abundance of available sensing data causes new challenges when building IoT applications. One main challenge is how to select proper data from large amount of sensing data for learning useful information efficiently. Existing approaches require developers to manage data for each specific application, which is very time consuming since the developers may not have enough knowledge about the dynamic changing data quality of different sensors. In this paper, we propose a data management middleware to learn the correlations between time series sensor data without prior knowledge. The learned correlation is then applied to select the useful sensor and reconstruct the incorrect data. To generalize the correlation models for each application, we utilize the dynamic feedback from the application to update the data selection and reconstruction. We evaluate our data management middleware in smart grids. The evaluation results show that our middleware can achieve better application performance with the help of dynamic feedback, data reconstruction and data selection.},
keywords={},
doi={10.1109/BigData.2016.7840737},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9137454,
author={Yang, Jining and Liu, Chenghua and Zheng, Jiewen and Xiao, Hanfang and Yuan, Jiulin and Qin, Yuping},
booktitle={2020 3rd International Conference on Artificial Intelligence and Big Data (ICAIBD)},
title={Identification of Influence Factors of Emergency Management Evaluation of Agro-Products Quality and Safety Based on Fuzzy DEMATEL Method},
year={2020},
volume={},
number={},
pages={275-278},
abstract={At present, the emergency management of agro-products, which is based on big data, Internet plus and AI technology, is promoting the orderly development. Meanwhile, the emergency management of agro-products will inevitably be affected by different kinds of interaction factors. In this paper, the indicators are sorted out and constructed, and the method combining direct fuzzy information with decision-making laboratory evaluation (DEMATEL) is proposed. The direct influence matrix based on intuitionistic fuzzy information expression is established; the comprehensive evaluation matrix is constructed by combining risk preference coefficients and the score function, and the evaluation indicators are analyzed according to influence degree, center degree and cause degree, so as to find out their interaction relationship. By constructing and analyzing the model, we can effectively quantify and evaluate the emergency management ability of agro-product quality and safety.},
keywords={Analytical models;Decision making;Big Data;Emergency services;Data models;Internet;Artificial intelligence;component;quality and safety of agro-products;emergency management;intuitionistic fuzzy;DEMATEL method},
doi={10.1109/ICAIBD49809.2020.9137454},
ISSN={},
month={May},}
@INPROCEEDINGS{7011535,
author={Liu, Wanting and Peng, Yonghong and Tobin, Desmond J},
booktitle={2014 IEEE Symposium on Computational Intelligence in Big Data (CIBD)},
title={Integrated analytics of microarray big data reveals robust gene signature},
year={2014},
volume={},
number={},
pages={1-7},
abstract={The advance of high throughput biotechnology enables the generation of large amount of biomedical data. The microarray is increasingly a popular approach for the detection of genome-wide gene expression. Microarray data have thus increased significantly in public accessible database repositories, which provide valuable big data for scientific research. To deal with the challenge of microarray big data collected in different research labs using different experimental set-ups and on different bio-samples, this paper presents a primary study to evaluate the impact of two important factors (the origin of bio-samples and the quality of microarray data) on the integrated analytics of multiple microarray data. The aim is to enable the extraction of reliable and robust gene biomarkers from microarray big data. Our work showed that in order to enhance biomarker discovery from microarray big data (i) it is necessary to treat the microarray data differently in terms of their quality, (ii) it is recommended to stratifying (i.e., sub-group) the data according to the origin of bio-samples in the analytics.},
keywords={Accuracy;Diseases;Big data;Robustness;Educational institutions;Bioinformatics;Malignant tumors;Microarray;integrated analytics;biomarkers},
doi={10.1109/CIBD.2014.7011535},
ISSN={},
month={Dec},}
@ARTICLE{8756123,
author={Zhang, Xi and Zhu, Qixuan},
journal={IEEE Journal on Selected Areas in Communications},
title={Information-Centric Virtualization for Software-Defined Statistical QoS Provisioning Over 5G Multimedia Big Data Wireless Networks},
year={2019},
volume={37},
number={8},
pages={1721-1738},
abstract={The multimedia transmission represents a typical big data application in the fifth-generation (5G) wireless networks. However, supporting multimedia big data transmission over 5G wireless networks imposes many new and open challenges because multimedia big data services are both time-sensitive and bandwidth-intensive over time-varying wireless channels with constrained wireless resources. To overcome these difficulties, in this paper we propose the information-centric virtualization architectures for software-defined statistical delay-bounded quality of service (QoS) provisioning over 5G multimedia big data wireless networks. In particular, our proposed schemes integrate the three 5G-promising candidate techniques to guarantee the statistical delay-bounded QoS for multimedia big data transmissions: 1) information-centric network (ICN), to derive the optimal in-network caching locations for multimedia big data; 2) network functions virtualization (NFV), to abstract the PHY-layer infrastructures into several virtualized networks to derive the optimal multimedia data contents delivery paths; and 3) software-defined networks (SDNs), to dynamically reconfigure wireless resources allocation architectures through the SDN-control plane. Under our proposed architectures, to jointly optimize the implementations of NFV and SDN techniques under ICN architectures, we develop the three virtual network selection and transmit-power allocation schemes to: 1) maximize single user's effective capacity; 2) jointly optimize the aggregate effective capacity and allocation fairness over all users; and 3) coordinate non-cooperative gaming among all users, respectively. By simulations and numerical analyses, we show that our proposed architectures and schemes significantly outperform the other existing schemes in supporting the statistical delay-bounded QoS provisioning over the 5G multimedia big data wireless networks.},
keywords={Big Data;Quality of service;Wireless networks;5G mobile communication;Resource management;Wireless sensor networks;5G multimedia big data wireless networks;ICN;NFV;SDN;optimal transmit power;statistical delay-bounded QoS;effective capacity;relay selection},
doi={10.1109/JSAC.2019.2927088},
ISSN={1558-0008},
month={Aug},}
@INPROCEEDINGS{7326686,
author={Marinoni, Andrea and Dagliati, Arianna and Bellazzi, Riccardo and Gamba, Paolo},
booktitle={2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},
title={Inferring air quality maps from remotely sensed data to exploit georeferenced clinical onsets: The Pavia 2013 case},
year={2015},
volume={},
number={},
pages={3937-3940},
abstract={Recent developments in data acquisition, storage, mining and maintenance have allowed the flourishing of several multi-disciplinary research fields, which can be stated, defined and carried out according to the so-called Big Data paradigm. In this environment, the investigation and analysis of interactions between human phenomena and natural events play a key-role, as they can be fundamental for several applications, from sustainable development to community policy design and short-, medium- and long-range resource allocation planning. In this paper, we provide a study of the interplay between air pollution (as estimated by remotely sensed data processing) and clinical records, so that inferences and correlations among black particulate concentration, micro- and macro-vascular disease onsets and hospitalization tracks can be efficiently drawn. We focused on the second order administrative area of the city of Pavia, Italy, on 2013. Experimental results show how effective connections between the estimated air quality and the hospitalizations behavior can be accurately drawn and derived.},
keywords={Air quality;Remote sensing;Earth;Satellites;Cities and towns;Medical services;Correlation;Air quality maps;aerosol pollution;human-environment interaction;Big Data},
doi={10.1109/IGARSS.2015.7326686},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{8456033,
author={Fadheel, Wesam and Salih, Raed and Lilien, Leszek},
booktitle={2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)},
title={PHeDHA: Protecting Healthcare Data in Health Information Exchanges with Active Data Bundles},
year={2018},
volume={},
number={},
pages={1187-1195},
abstract={Health Information Exchanges (HIEs) collect and disseminate electronic patient healthcare data (EHRs/EMRs) among different healthcare providers to improve the quality and reduce the cost of healthcare services. However, the dissemination of patient data raises privacy and security concerns due to ease of copying and unauthorized dissemination of electronic data. This paper proposes a HIE system called PHeDHA (Protecting Healthcare Data in HIEs with Active Data Bundles), which provides privacy and security protection for patient data during their transmission via an HIE among different healthcare providers. PHeDHA uses as its basis the scheme named Active Data Bundles with Trusted Third Party (ADB-TTP). As the name suggests, ADB-TTP is based on an integration of a trusted third party (TTP) with Active Data Bundles (ADBs). An ADB is a software object that keeps patient healthcare data as sensitive data; includes metadata describing these sensitive data and prescribing their use (via data access and privacy policies specified within metadata); and encompasses a policy enforcement engine (called a virtual machine or VM), which controls and manages how the ADB behaves. In particular, the VM assures ADB's data integrity and enforces its policies specified as a part of metadata. We describe and discuss the conceptual model for PHeDHA, based on ADB-TTP. We are currently evaluating PHeDHA via simulation experiments.},
keywords={Security;Data privacy;Privacy;Hospitals;Metadata;Information exchange;active data bundles;health information exchange;privacy;security;trusted third party},
doi={10.1109/TrustCom/BigDataSE.2018.00164},
ISSN={2324-9013},
month={Aug},}
@INPROCEEDINGS{7004209,
author={Greenberg, Jane and Ogletree, Adrian and Murillo, Angela P. and Caruso, Thomas P. and Huang, Herbie},
booktitle={2014 IEEE International Conference on Big Data (Big Data)},
title={Metadata capital: Simulating the predictive value of Self-Generated Health Information (SGHI)},
year={2014},
volume={},
number={},
pages={31-36},
abstract={Metadata is crucial for understanding data, and can be viewed as a form of capital in the context of Big data. This paper reports on research simulating the potential of SGHI (Self-Generated Health Information) for predicting asthma episodes. A data set of 2,000 cases was generated using the Monte Carlo simulation method, with secondary modifications on air quality and geo-location. The research is being pursued as part of a National Consortium for Data Science (NCDS) effort. The research conducted demonstrates that metadata has an inherent “predictive value” and confirms that metadata is crucial for data analytics. The work presented also provides insights into the best direction for future work in this area.},
keywords={Economics;Medical services;Big data;Context;Biomedical monitoring;Educational institutions;self-generated health information;metadata;capital;predictive models},
doi={10.1109/BigData.2014.7004209},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9351494,
author={Islam, Mohaiminul and Karim, Rezaul and Khatun, MST Asha and Reza, Shamim},
booktitle={2020 International Conference on Information Science and Communications Technologies (ICISCT)},
title={A Research on Big Data Analytics in Healthcare Industry},
year={2020},
volume={},
number={},
pages={1-5},
abstract={Big Data has changed the way we manage, analyze and leverage data in any industry. One of the most promising areas where it can be applied to make a change is healthcare. Healthcare analytics have the potential to reduce costs of treatment, predict outbreaks of epidemics, avoid preventable diseases and improve the quality of life in general. Average human lifespan is increasing along world population, which poses new challenges to today's treatment delivery methods. In this article, we would like to address the need of big data in health care industry, big data analytics advantages, big data applications and so on.},
keywords={Industries;Warehousing;Medical services;Big Data;Tools;Statistics;Investment;Data Analysis;Healthcare;Big Data;Treatment;Epidemics},
doi={10.1109/ICISCT50599.2020.9351494},
ISSN={},
month={Nov},}
@ARTICLE{8415743,
author={Yao, Le and Ge, Zhiqiang},
journal={IEEE Transactions on Industrial Electronics},
title={Scalable Semisupervised GMM for Big Data Quality Prediction in Multimode Processes},
year={2019},
volume={66},
number={5},
pages={3681-3692},
abstract={In this paper, a novel variational inference semisupervised Gaussian mixture model (VI-S2GMM) model is first proposed for semisupervised predictive modeling in multimode processes. Parameters of Gaussian components are identified more accurately with extra unlabeled samples, which improve the prediction performance of the regression model. Since all labeled and unlabeled data samples are involved in each iteration of parameter updating, intractable computing problems occur when facing high-dimension datasets. To tackle this problem, a scalable stochastic VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic gradient optimization algorithm to maximize the evidence of lower bound, the VI-based algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples is randomly selected to update parameters in each iteration, which is more efficient than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data modeling issue. In this way, a large number of unlabeled data can be useful in the modeling, which will further benefit the prediction performance. The SVI-S2GMM is then exploited for the prediction of a quality-related key performance index. Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.},
keywords={Data models;Big Data;Predictive models;Inference algorithms;Prediction algorithms;Semisupervised learning;Computational modeling;Big data;Gaussian mixture model (GMM);multimode process modeling;quality prediction;semisupervised modeling;stochastic variational inference (SVI)},
doi={10.1109/TIE.2018.2856200},
ISSN={1557-9948},
month={May},}
@INPROCEEDINGS{6597144,
author={Wang, Lijuan and Luo, Junzhou and Shen, Jun and Dong, Fang},
booktitle={2013 IEEE International Congress on Big Data},
title={Cost and Time Aware Ant Colony Algorithm for Data Replica in Alpha Magnetic Spectrometer Experiment},
year={2013},
volume={},
number={},
pages={247-254},
abstract={Huge collections of data have been created in recent years. Cloud computing provides a way to enable massive amounts of data to work together as data-intensive services. Considering Big Data and the cloud together, which is a practical and economical way to deal with Big Data, will accelerate the availability and acceptability of analysis of the data. Providing an efficient mechanism for optimized data-intensive services will become critical to meet the expected growth in demand. Because the competition is an extremely important factor in the marketplace, the cost model for data-intensive service provision is the key to provide a sustainable service market. As data play the dominant role in execution of data-intensive service composition, the cost and access response time of data sets influence the quality of the service that requires the data sets. In this paper, a data replica selection optimization algorithm based on an ant colony system is proposed. The performance of the data replica selection algorithm is evaluated by simulations. The background application of the work is the Alpha Magnetic Spectrometer experiment, which involves large amounts of data being transferred, organized and stored. It is critical and challenging to be cost and time aware to manage the data and services in this intensive research environment.},
keywords={Data models;Servers;Time factors;Information management;Data handling;Data storage systems;Pricing;service provision;data-intensive;ant colony optimization;cloud computing;Big Data;QoS},
doi={10.1109/BigData.Congress.2013.41},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{8215604,
author={Shao, Junming and Gao, Chongming and Zeng, Wei and Song, Jingkuan and Yang, Qinli},
booktitle={2017 IEEE International Conference on Data Mining (ICDM)},
title={Synchronization-Inspired Co-Clustering and Its Application to Gene Expression Data},
year={2017},
volume={},
number={},
pages={1075-1080},
abstract={In this paper, we propose a new synchronization-inspired co-clustering algorithm by dynamic simulation, called CoSync, which aims to discover biologically relevant subgroups embedding in a given gene expression data matrix. The basic idea is to view a gene expression data matrix as a dynamical system, and the weighted two-sided interactions are imposed on each element of the matrix from both aspects of genes and conditions, resulting in the values of all element in a co-cluster synchronizing together. Experiments show that our algorithm allows uncovering high-quality co-clusterings embedded in gene expression data sets and has its superiority over many state-of-the-art algorithms.},
keywords={Synchronization;Gene expression;Data models;Heuristic algorithms;Clustering algorithms;Artificial intelligence;Algorithm design and analysis;co-clustering;gene expression data;synchronization},
doi={10.1109/ICDM.2017.141},
ISSN={2374-8486},
month={Nov},}
@INPROCEEDINGS{9070346,
author={Jang, Sung Su and Hwang, Kyu Hong and Ha, Young Guk},
booktitle={2020 IEEE International Conference on Big Data and Smart Computing (BigComp)},
title={High Quality Training Set Collection using Generative Adversarial Network},
year={2020},
volume={},
number={},
pages={455-458},
abstract={Image classification and object detection using deep learning have evolved with continuous research. In particular, the development of big data and the improvement of computer hardware performance have contributed extremely to the development of deep learning. Deep learning technologies like Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) can train models through training data. In other words, the optimal value of the weight parameter is automatically obtained from the training data. In this way, training data is the most important in deep learning. The quality and amount of training data affects the learning performance of deep learning models. In order to obtain high quality training data, it is difficult for the user to collect it directly. And using a web crawler to collect images for search keywords is also in problem. The images collected by the crawler include many low-resolution images and irrelevant images. These image data are bad for training deep learning model. Therefore, this paper purposes to collect high quality learning data by automatically applying SRGAN to low resolution images after image collection through image crawler and converting high quality images.},
keywords={Image resolution;Machine learning;Training data;Training;Crawlers;Data models;Generative adversarial networks;Low resolution, Super-resolution, Training Data Set, ESRGAN, Crawler},
doi={10.1109/BigComp48618.2020.00-27},
ISSN={2375-9356},
month={Feb},}
@INPROCEEDINGS{7984409,
author={Lopes, Antonio M. and Abreu, Paulo and Restivo, Maria Teresa},
booktitle={2017 4th Experiment@International Conference (exp.at'17)},
title={Analysis and pattern identification on smart sensors data},
year={2017},
volume={},
number={},
pages={97-98},
abstract={This work exemplifies the use of a data analysis technique applied to indoor air quality data obtained in a laboratory. The environment data is acquired with a wireless sensor system, NSensor. The sensing system, developed at the Faculty of Engineering, University of Porto (FEUP), is used for indoor environment monitoring, with the capability to store, in a remotely accessed database, air quality parameters such as temperature, relative humidity, pressure, illuminance, carbon dioxide and volatile organic components. For the current study, it was selected the data from temperature and relative humidity, and a period of ten months was considered. The data analysis uses Fourier transforms to identify patterns on the acquired data. For the temperature data, five main patterns were possible to identify. This work explores the potential of using data analyses techniques for big data on the field of indoor air quality evaluation. To make use of this data, further developments must be carried out so that would be possible to go from the monitoring and identification to the phase of controlling the indoor environment.},
keywords={Temperature measurement;Air quality;Data analysis;Monitoring;Humidity;Temperature sensors;Wireless sensor networks;remote monitoring;sensors network;indoor air quality;data analysis;pattern identification},
doi={10.1109/EXPAT.2017.7984409},
ISSN={},
month={June},}
@INPROCEEDINGS{9373200,
author={Bachechi, Chiara and Desimoni, Federico and Po, Laura and Casas, David Martínez},
booktitle={2020 24th International Conference Information Visualisation (IV)},
title={Visual analytics for spatio-temporal air quality data},
year={2020},
volume={},
number={},
pages={460-466},
abstract={Air pollution is the second biggest environmental concern for Europeans after climate change and the major risk to public health. It is imperative to monitor the spatiotemporal patterns of urban air pollution. The TRAFAIR air quality dashboard is an effective web application to empower decision-makers to be aware of the urban air quality conditions, define new policies, and keep monitoring their effects. The architecture copes with the multidimensionality of data and the real-time visualization challenge of big data streams coming from a network of low-cost sensors. Moreover, it handles the visualization and management of predictive air quality maps series that is produced by an air pollution dispersion model. Air quality data are not only visualized at a limited set of locations at different times but in the continuous space-time domain, thanks to interpolated maps that estimate the pollution at un-sampled locations.},
keywords={Atmospheric modeling;Data visualization;Predictive models;Air pollution;Data models;Real-time systems;Monitoring;Climate change;air quality;temporal data;spatial data;visualization;interpolation maps;prediction},
doi={10.1109/IV51561.2020.00080},
ISSN={2375-0138},
month={Sep.},}
@INPROCEEDINGS{8005836,
author={Li, Dong and Wu, Jiechu and Deng, Zehang and Chen, Zelin and Xu, Yang},
booktitle={2017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)},
title={QoS-Based Service Selection Method for Big Data Service Composition},
year={2017},
volume={1},
number={},
pages={436-443},
abstract={Different from the traditional web services, the big data services' execution duration vary from the input data volume, so the traditional Quality of Service (QoS) analysis model for traditional web services cannot be directly applied to big data services. Additionally, since many big data or web services provide overlapping or identical functionality, albeit with the description of different QoS, some methods should be taken to determine which service are to participate in a given composite service. To address the problems above, this paper proposes an expanded edition of QoS-based analysis model: (1) Use the linear regression model to estimate the execution duration of the big data services to support the QoS model. (2)Set the weight of QoS using AHP analysis. (3) Improve the service selection algorithm based on backtracking method and validate its effectiveness, which is proved more high-performance than the original backtracking method and Integer Programming method.},
keywords={Quality of service;Big Data;Mathematical model;Data models;Analytical models;Algorithm design and analysis;Linear regression;Big Data Services;Quality of Service (QoS);Service Composition;Backtracking method},
doi={10.1109/CSE-EUC.2017.84},
ISSN={},
month={July},}
@INPROCEEDINGS{7169822,
author={Blat, Josep and Evans, Alun and Agenjo, Javi and Kim, Hansung and Imre, Evren and Hilton, Adrian and Tefas, Anastasios and Nikolaidis, Nikos and Pitas, Ioannis and Polok, Lukas and Smrz, Pavel and Zemcik, Pavel},
booktitle={2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)},
title={IMPART: Big media data processing and analysis for film production},
year={2015},
volume={},
number={},
pages={1-5},
abstract={A typical high-end film production generates several terabytes of data per day, either as footage from multiple cameras or as background information regarding the set (laser scans, spherical captures, etc). The EU project IMPART (impart.upf.edu) has been researching solutions that improve the integration and understanding of the quality of the multiple data sources to support creative decisions onset or near it, and an enhanced post-production as well. The main results covered in this paper are: a public multisource production dataset made available for research purposes, monitoring and quality assurance of multicamera set-ups, multisource registration, anthropocentric visual analysis for semantic content annotation, acceleration of 3D reconstruction, and integrated 2D-3D web visualization tools.},
keywords={Three-dimensional displays;Production;Cameras;Data visualization;Films;Semantics;Calibration;Multi-modal data processing;big media data analysis;web 3D visualization},
doi={10.1109/ICMEW.2015.7169822},
ISSN={},
month={June},}
@INPROCEEDINGS{8250692,
author={Shahare, Firoj Fattulal},
booktitle={2017 International Conference on Intelligent Computing and Control Systems (ICICCS)},
title={Sentiment analysis for the news data based on the social media},
year={2017},
volume={},
number={},
pages={1365-1370},
abstract={Now social Data are increases very fast, in every area social data play an important role in every angle, social media big data mining area welcomed by researchers from both government, academic and industry. A computing sentiment of news data is a significant component of the social media big data. The computing sentiment of news information may be a major factor of the social media massive information. In current web word range of user use social media and social network to browse and read news connected information. Everyday range of issue area unit occurring and social media influence the news associated with this news. Existing sentiment computing ways area unit primarily supported the feeling wordbook or supervised ways, that aren't climbable to the social media massive information. As a result of once bid information suggests that information size increases this methodology result on potency. Therefore we tend to propose a replacement methodology to try and do the sentiment analysis for news data a lot of specially, supported the social media information and social news (i.e.text and emotions text) of a happening, a Levenshtein algorithm is made to together categorical its linguistics and emotions, that lays the muse for the happening sentiment analysis. The word feeling computation algorithmic rule is planned to get the beginning word feeling that area unit more refined through the quality emotion wordbook. With the word emotions in hand, we are able to reason each sentence sentiments. The proposed method uses Naïve Bayes and Levenshtein algorithm to determine the emotion into different categories from given social media news data. This method provides the excellent performance for real time news data on social media and also provides the better result in terms of accuracy.},
keywords={Social network services;Sentiment analysis;Algorithm design and analysis;Classification algorithms;Machine learning algorithms;Big Data;Data mining;Text mining;sentiment computing;emotion classification;social media big dataset},
doi={10.1109/ICCONS.2017.8250692},
ISSN={},
month={June},}
@INPROCEEDINGS{9150155,
author={Su, Yuelai},
booktitle={2020 International Conference on Big Data and Informatization Education (ICBDIE)},
title={Prediction of air quality based on Gradient Boosting Machine Method},
year={2020},
volume={},
number={},
pages={395-397},
abstract={With the rapid development of China's economy, the degree of industrialization is gradually deepened, therefore leading to environmental pollution problems. Air is the material basis on which human beings live. Beijing is the capital of China and the national economic, political and cultural center, Beijing's air quality is an important indicator to measure whether the city is livable or not, while PM2.5 has also become an important standard to measure and monitor the air quality of Beijing. In today's era of big data, the use of efficient computing software to conduct data analysis and prediction has become a trend of future environmental detection and data analysis, which can effectively monitor the urban air environment. In this paper, two methods, Light Gradient Boosting Machine (Light GBM) and eXtreme Gradient Boosting (XGB) were used to extract and predict the characteristics of the air monitoring data in Beijing, and the prediction accuracy and operation time of the two methods were evaluated. Finally, conclusion was drawn that the accuracy and operation efficiency of Light GBM was much higher than that of XGB was reached.},
keywords={Predictive models;Air quality;Prediction algorithms;Monitoring;Atmospheric modeling;Boosting;Air quality;Beijing;Air quality monitoring;Light GBM;XGB;PM2.5},
doi={10.1109/ICBDIE50010.2020.00099},
ISSN={},
month={April},}
@INPROCEEDINGS{8356832,
author={Amjad, Mahwish and Iradat, Faisal},
booktitle={2018 International Conference on Information and Computer Technologies (ICICT)},
title={A brief review of methods and approaches proposed in existing literature to address issues in wireless big data},
year={2018},
volume={},
number={},
pages={13-17},
abstract={Big data generated from wireless devices bring distinct challenges for the researchers due to its inherent spatial temporal nature. Reviewing existing literature on wireless big data, two major challenges are identified, processing and storage. To address these challenges various methodologies/approaches have been proposed and presented in the recent literature. To present the existing state of research on challenges associated with wireless big data, we present a systematic literature review of selected publications that address these issues. We also present areas which have not been addressed in the existing literature.},
keywords={Big Data;Cloud computing;Wireless communication;Wireless sensor networks;Computer architecture;Bibliographies;Quality of service;component;formatting;style;styling;insert Introduction},
doi={10.1109/INFOCT.2018.8356832},
ISSN={},
month={March},}
@INPROCEEDINGS{8481834,
author={Dabab, Maoloud and Craven, Rebecca and Barham, Husam and Gibson, Elizabeth},
booktitle={2018 Portland International Conference on Management of Engineering and Technology (PICMET)},
title={Exploratory Strategic Roadmapping Framework for Big Data Privacy Issues},
year={2018},
volume={},
number={},
pages={1-9},
abstract={The applications of Big Data continue to expand, due to the many possibilities and unprecedented insights it offers to people, organizations, and communities. However, Big Data poses serious challenges as well, including challenges to the privacy and security of individuals and their data. This paper considers how to best address one concern related to Big Data: the social problems that the pervasiveness of data collection, analysis, and storage create with regard to individuals' ability to control their own data. The paper uses Quality Function Deployment (QFD) and Technology Roadmapping analysis methods to assess the social problems, technologies, resources, and industries that are most relevant to data privacy, and what should be done to address it. The findings indicated that the healthcare industry is one of the most important industries to consider concerning data privacy because of the nature of the data generated through medical processes and technologies. Furthermore, it was found that enforcement mechanisms, specifically in the form of federal enforcement agencies, are the most effective approach to ensure compliance by actors. It was also realized that there are extenuating political circumstances and increased costs that make the implementation of those policies challenging in the United States.},
keywords={Big Data;Data privacy;Industries;Privacy;Security;Quality function deployment;Medical services},
doi={10.23919/PICMET.2018.8481834},
ISSN={2159-5100},
month={Aug},}
@INPROCEEDINGS{8004386,
author={Kanstrén, Teemu},
booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
title={Experiences in Testing and Analysing Data Intensive Systems},
year={2017},
volume={},
number={},
pages={589-590},
abstract={Testing software-intensive systems, for us, has traditionally focused on verifying and validating compliance and conformance to specification, as well as some general non-functional requirements such as performance of different components. In recent years, we have seen a strong move towards more data intensive systems. We have found that these types of systems require a different approach for testing and analysis, moving more towards exploring the system, its elements, behaviour and properties from a big data and analytics perspective. This paper summarizes our experiences in building and developing test and analytics environments for evaluating the performance, reliability, and security of such data-intensive systems.},
keywords={Testing;Security;Software reliability;Data models;Big Data;Software;testing;analytics;reliability;security},
doi={10.1109/QRS-C.2017.107},
ISSN={},
month={July},}
@INPROCEEDINGS{8002536,
author={Adrian, Cecilia and Abdullah, Rusli and Atan, Rodziah and Jusoh, Yusmadi Yah},
booktitle={2017 International Conference on Research and Innovation in Information Systems (ICRIIS)},
title={Factors influencing to the implementation success of big data analytics: A systematic literature review},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Big data analytics (BDA) readiness factors have been widely researched; nevertheless, few have investigated the impact and success factors of BDA implementation in the organizational context. The relevance and quality of BDA outcomes have been a significant concern to the organizational leaders in supporting them for strategic decision-making. To that end, the objective of this study is twofold. Firstly, it investigates the factors that influencing the success of BDA implementation for effective decision-making. Secondly, this study adds to the body of knowledge in the information system (IS) domain, especially with its focus on BDA implementation packages. Based on 18 selected papers, this review has established 10 influencing factors that may influence the success of BDA implementation, therefore, contribute to the practice and research of BDA domain and its effectiveness towards the organizational performance enhancement.},
keywords={Big Data;Organizations;Medical services;Quality assessment;Manufacturing;Decision making;big data analytics;capabilities;quality;big data analytics implementation},
doi={10.1109/ICRIIS.2017.8002536},
ISSN={2324-8157},
month={July},}
@ARTICLE{8214278,
author={Evans, Rhett and Boreland, Matthew},
journal={IEEE Journal of Photovoltaics},
title={Multivariate Data Analytics in PV Manufacturing—Four Case Studies Using Manufacturing Datasets},
year={2018},
volume={8},
number={1},
pages={38-47},
abstract={Many industries are being revolutionized through the use of advanced analytical tools that generate insights from large sets of data. These tools are used as a part of a diversely described but analogous set of pursuits, such as “data science,” “data mining,” and “big data.” In manufacturing, they result in improved quality, improved cost of manufacturing, and more streamlined approaches. Many of these tools are applicable to PV manufacturing data and so present significant opportunities for the industry, but there are limited published studies and limited public domain knowledge of commercial activities in the area. This paper highlights these opportunities for PV manufacturing by describing four case studies applying different analytical approaches to sets of data from different manufacturing facilities. The analyses primarily provide insight into the source of variance in manufacturing, offering manufacturers a detailed and quantifiable way to measure and improve quality. These types of approaches will become more necessary to keep control of manufacturing facilities that continue to grow at high rates, and thus they offer a glimpse for the future operation and organization of large-scale PV manufacturing.},
keywords={Manufacturing;Mathematical model;Data models;Numerical analysis;Analytical models;Covariance matrices;Software;Big data;data mining;manufacturing;multivariate statistics;photovoltaic cells;process control},
doi={10.1109/JPHOTOV.2017.2778571},
ISSN={2156-3403},
month={Jan},}
@INPROCEEDINGS{8534028,
author={Behrisch, Michael and Krueger, Robert and Lekschas, Fritz and Schreck, Tobias and Gehlenborg, Nils and Pfister, Hanspeter},
booktitle={2018 International Symposium on Big Data Visual and Immersive Analytics (BDVA)},
title={Visual Pattern-Driven Exploration of Big Data},
year={2018},
volume={},
number={},
pages={1-11},
abstract={Pattern extraction algorithms are enabling insights into the ever-growing amount of today's datasets by translating reoccurring data properties into compact representations. Yet, a practical problem arises: With increasing data volumes and complexity also the number of patterns increases, leaving the analyst with a vast result space. Current algorithmic and especially visualization approaches often fail to answer central overview questions essential for a comprehensive understanding of pattern distributions and support, their quality, and relevance to the analysis task. To address these challenges, we contribute a visual analytics pipeline targeted on the pattern-driven exploration of result spaces in a semi- automatic fashion. Specifically, we combine image feature analysis and unsupervised learning to partition the pattern space into interpretable, coherent chunks, which should be given priority in a subsequent in-depth analysis. In our analysis scenarios, no ground-truth is given. Thus, we employ and evaluate novel quality metrics derived from the distance distributions of our image feature vectors and the derived cluster model to guide the feature selection process. We visualize our results interactively, allowing the user to drill down from overview to detail into the pattern space and demonstrate our techniques in two case studies on Earth observation and biomedical genomic data.},
keywords={Feature extraction;Visualization;Data mining;Pipelines;Measurement;Data visualization;Space exploration},
doi={10.1109/BDVA.2018.8534028},
ISSN={},
month={Oct},}
@ARTICLE{6848734,
author={Ranjan, Rajiv},
journal={IEEE Cloud Computing},
title={Streaming Big Data Processing in Datacenter Clouds},
year={2014},
volume={1},
number={1},
pages={78-83},
abstract={Today, we live in a digital universe in which information and technology are not only around us but also play important roles in dictating the quality of our lives. As we delve deeper into this digital universe, we're witnessing explosive growth in the variety, velocity, and volume of data1,2 being transmitted over the Internet. A zetabyte of data passed through the Internet in the past year; IDC predicts that this digital universe will explode to an unimaginable eight Zbytes by 2015. These data are and will be generated mainly from Internet search, social media, mobile devices, the Internet of Things, business transactions, next-generation radio astronomy telescopes, high-energy physics synchrotron, and content distribution. Government and business organizations are now overflowing with data, easily aggregating to terabytes or even petabytes of information.},
keywords={Big data;Data mining;Distributed databases;Computer architecture;Quality of service;Programming;Data centers;Streaming media;Cloud computing;datacenter clouds;streaming big data processing;data analysis;big data applications;data mining;cloud;big data},
doi={10.1109/MCC.2014.22},
ISSN={2325-6095},
month={May},}
@INPROCEEDINGS{7584934,
author={Yin, Jianwei and Tang, Yan and Lo, Wei and Wu, Zhaohui},
booktitle={2016 IEEE International Congress on Big Data (BigData Congress)},
title={From Big Data to Great Services},
year={2016},
volume={},
number={},
pages={165-172},
abstract={Big Data is increasingly adopted by a wide range of service industries to improve the quality and value of their services, e.g., inventory that matches well the supply and demand, and pricing that reflects well the market needs. Customers benefit from higher quality of service enabled by Big Data. Service providers get higher profits from more precise control of costs and accurate knowledge of customer needs. In this paper, we define the next generation high quality services as Great Services, characterized by 4P Quality-of-Service (QoS) dimensions: Panorama, Penetration, Prediction and Personalization, which go much further than current services. The transformation of Big Data into Great Services would be difficult and expensive without methodical techniques and software tools. We call the intermediate step Deep Knowledge, which is generated by Big Data (with the 4V challenges - Volume, Velocity, Variety, and Veracity) and used in the creation of Great Services. Deep Knowledge is distinguished from traditional Big Data by 4C properties (Complexity, Cross-domain, Customization, and Convergence). In order to achieve the 4P QoS dimensions of Great Services, we need Deep Knowledge with 4C properties. In this paper, we describe an informal characterization of Great Services with 4P QoS dimensions with examples, and outline the techniques and tools that facilitate the transformation of Big Data into Deep Knowledge with 4C properties, and then the use of Deep Knowledge in Great Services.},
keywords={Big data;Quality of service;Public transportation;Vehicles;Industries;Next generation networking;Complexity theory;Great Services;4P QoS},
doi={10.1109/BigDataCongress.2016.28},
ISSN={},
month={June},}
@INPROCEEDINGS{8770562,
author={Noonpakdee, Wasinee and Phothichai, Acharaphun and Khunkornsiri, Thitiporn},
booktitle={2019 28th Wireless and Optical Communications Conference (WOCC)},
title={Challenges of Big Data Implementation in a Public Hospital},
year={2019},
volume={},
number={},
pages={1-5},
abstract={The digitalization of the healthcare system has resulted in a huge amount of data in medicine. These data help healthcare organizations improve health-process efficiency, enhance healthcare quality, and reduce healthcare cost. In Thailand, several private hospitals obtain numerous benefits by utilizing big data analytics. However, public hospitals might encounter with some difficulties for implementing big data technology. This paper presents challenges of big data implementation in a case study public hospital. According to the analysis, challenges are discussed in aspects of technology, data, human, and organization. The recommendation for implementing big data is proposed as a guideline for big data implementation in the public hospital.},
keywords={Big Data;Hospitals;Organizations;Wireless communication;Big Data;Healthcare;Challenges},
doi={10.1109/WOCC.2019.8770562},
ISSN={2379-1276},
month={May},}
@INPROCEEDINGS{8421880,
author={Guo, Naiwang and Su, Yun and Yang, Hongshan},
booktitle={2018 5th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2018 4th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)},
title={Storage and Indexing of Big Data for Power Distribution Networks},
year={2018},
volume={},
number={},
pages={224-2243},
abstract={With the construction of power distribution networks, a large amount of various types of data has accumulated, including automation and information technology system data as well as customer power consumption data, including distribution transformer, distribution transformer station, distribution switch station, meter, and electrical energy quality. The storage and index association of a large amount of different data is the basis for big data analysis. As a massive type of time-series data, customer power consumption load includes large-scale customers and high-density data collection. To improve the efficiency of query and analysis, this study proposed time-series data indexing technology to reduce the time required for data query and retrieval, to improve the efficiency of time-series data analysis, and to enable power companies to deeply analyze and cluster the power consumption behaviors of their customers.},
keywords={Big Data;Power demand;Companies;Indexing;Power distribution;big-data;Hadoop;Hbase;power-distribution-networks},
doi={10.1109/CSCloud/EdgeCom.2018.00049},
ISSN={},
month={June},}
@INPROCEEDINGS{9060229,
author={Xiaohua, Feng and Feng, Yunzhong and Xu, Junke and Sun, Shu and Zhao, Yuping},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title={Computer Laws Consideration on Smart City Data Planning of Chongli 2022},
year={2019},
volume={},
number={},
pages={1695-1699},
abstract={Smart Cities make a good use of modern ICT and innovation idea to provide systematically creative, integrated of variety of services. In order to serve citizen's life a good quality, optimize management and services, enhance the efficiency of resource usage, a good smart city planning is in a big demand. Since 2014, professionals have started to consider Smart City planning and design for Chongli, in order to prepare the 2022 Winter Olympics there properly. Nevertheless, there is a shortage of cyber security consideration in current Smart City planning and designing. Although Yu et al (2018) discussed the Smart Cities development, but more works are still needed. Legal preparation actually is very important for Smart Chongli.},
keywords={Smart cities;Law;Computer security;Data protection;Big Data;Data Security, Digital Forensics, Big Data, Data Analysis, Digital Evidence, Smart Cities, DEA, Cyber Security, Data Mining, Computer Laws, AI},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00302},
ISSN={},
month={Aug},}
@ARTICLE{8820763,
author={Yan, Ming and Li, Wenwen and Chan, Chien Aun and Bian, Sen and I, Chih-Lin and Gygax, André F.},
journal={China Communications},
title={PECS: Towards personalized edge caching for future service-centric networks},
year={2019},
volume={16},
number={8},
pages={93-106},
abstract={Mobile operators face the challenge of how to best design a service-centric network that can effectively process the rapidly increasing number of bandwidth-intensive user requests while providing a higher quality of experience (QoE). Existing content distribution networks (CDN) and mobile content distribution networks (mCDN) have both latency and throughput limitations due to being multiple network hops away from end-users. Here, we first propose a new Personalized Edge Caching System (PECS) architecture that employs big data analytics and mobile edge caching to provide personalized service access at the edge of the mobile network. Based on the proposed system architecture, the edge caching strategy based on user behavior and trajectory is analyzed. Employing our proposed PECS strategies, we use data mining algorithms to analyze the personalized trajectory and service usage patterns. Our findings provide guidance on how key technologies of PECS can be employed for current and future networks. Finally, we highlight the challenges associated with realizing such a system in 5G and beyond.},
keywords={Big Data;Cloud computing;5G mobile communication;Content distribution networks;Trajectory;Servers;Computer architecture;big data;data mining;edge caching;content network;personalized edge caching},
doi={10.23919/JCC.2019.08.009},
ISSN={1673-5447},
month={Aug},}
@INPROCEEDINGS{9197808,
author={Al-Bahri, B and Noronha, Herald and Pandey, Jitendra and Singh, Ajay Vikram and Rana, Ajay},
booktitle={2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)},
title={Evaluate the Role of Big Data in Enhancing Strategic Decision Making for E-governance in E-Oman Portal},
year={2020},
volume={},
number={},
pages={914-917},
abstract={This research study is to analyse the role of big data for enhancing the decision making for E-Oman. E-Oman is one of the It's organisation in Oman which provides key to It solutions includes application, infrastructure in Oman. Big data has become a forthcoming part of all trades and business segment Oman in electronic portal is for the citizens which make easy use of a transactions. Enhancement of big data in e-Oman Company supports in providing perfect public services and also searching of big term in process. The range which is used in this topic is exemplified in a crucial panel conversation a recent big data conference. We present a cooperative big data analytics stage for big data as a service. It takes longer time to achieve wrinkle data, progress events and investigative services. The outdate technologies do not become an appropriate solution to process a big data platform has begun to appear. The quality of big data is of great significance is more significant because the quality of material is affected by size, speed and format in which the data is generated. The main benefit of the e-service is user-friendly. By implementing the e-services it makes easy communication between the government and the citizens. The quality of big data is great pertinent and it is more significant. Quality of information is affected by the size, speed, and the data in which the format is generated.},
keywords={Big Data;Electronic government;Tools;Portals;Decision making;Big Data;E-governance;Decision Making and E-Oman portal},
doi={10.1109/ICRITO48877.2020.9197808},
ISSN={},
month={June},}
@INPROCEEDINGS{9260293,
author={Yu, Liu},
booktitle={2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Construction and Application of Accounting Information Platform based on Big Data Environment},
year={2020},
volume={},
number={},
pages={475-479},
abstract={With the wide application of big data technology in accounting work and the continuous development of Internet technology, accounting informatization has become an inevitable trend in the development of accounting work. This paper mainly studies the service composition architecture of the application system based on the SOA idea, which not only completes the accounting integrated service management platform, but also provides the related web services related to accounting information for other internal independent systems. At the same time, the application service engine provided by cloud computing platform is used to realize all kinds of required financial applications. By using the SOA solution of Hessian, a distributed computing framework that provides users with more appropriate instant information services and related information query functions, and presents the problems encountered in the implementation process and solutions. It realizes the interface and service release between the accounting integrated service platform and other information management systems, as well as the information sharing and resource integration among the independent systems within the enterprise, which finally promotes the efficiency of the accounting staff.},
keywords={Zirconium;Smart grids;Quality function deployment;Conferences;Automation;accounting information system;SOA;business service;big data},
doi={10.1109/ICSGEA51094.2020.00109},
ISSN={},
month={June},}
@ARTICLE{7350093,
author={Blat, Josep and Evans, Alun and Kim, Hansung and Imre, Evren and Polok, Lukàš and Ila, Viorela and Nikolaidis, Nikos and Zemčík, Pavel and Tefas, Anastasios and Smrž, Pavel and Hilton, Adrian and Pitas, Ioannis},
journal={Proceedings of the IEEE},
title={Big Data Analysis for Media Production},
year={2016},
volume={104},
number={11},
pages={2085-2113},
abstract={A typical high-end film production generates several terabytes of data per day, either as footage from multiple cameras or as background information regarding the set (laser scans, spherical captures, etc). This paper presents solutions to improve the integration of the multiple data sources, and understand their quality and content, which are useful both to support creative decisions on-set (or near it) and enhance the postproduction process. The main cinema specific contributions, tested on a multisource production dataset made publicly available for research purposes, are the monitoring and quality assurance of multicamera set-ups, multisource registration and acceleration of 3-D reconstruction, anthropocentric visual analysis techniques for semantic content annotation, and integrated 2-D–3-D web visualization tools. We discuss as well improvements carried out in basic techniques for acceleration, clustering and visualization, which were necessary to deal with the very large multisource data, and can be applied to other big data problems in diverse application fields.},
keywords={Cameras;Computer applications;Big data;Semantics;Data visualization;Motion pictures;Three-dimensional displays;Data analysis;Computer applications;Anthropocentric semantic video analysis;big media data analysis and integration;graph processing acceleration;multimodal data processing;outdoor 3-D reconstruction;web 3-D visualization},
doi={10.1109/JPROC.2015.2496111},
ISSN={1558-2256},
month={Nov},}
@INPROCEEDINGS{7972391,
author={Hegde, Sandeep Kumar and Srinivasa K.G},
booktitle={2017 Third International Conference on Advances in Electrical, Electronics, Information, Communication and Bio-Informatics (AEEICB)},
title={A novel pattern classifier approach towards the performance optimization of Big Data analysis in distributed environment},
year={2017},
volume={},
number={},
pages={100-104},
abstract={Big data is a collection of massive amount of data whose characteristics exceeds the capabilities of conventional algorithm and techniques to derive the useful value. The real power lies not just in having colossal data but in what insights can be drawn from this data to facilitate better and faster decisions. Hadoop Map Reduce is emerged as powerful and cost effective open source framework for processing large data set distributed computing environment. The load imbalance, which occurs while processing the Big Data is common problem with this framework which reduces the efficiency of the processing, may impact the QOS of the application. In this paper a new pattern classifier algorithm has been proposed to tackle these problems. The proposed algorithm integrated with the traditional environment and experimentally it has been proved that response time of Map Reduce jobs are amplified by minimizing the load imbalance factor, thereby QOS requirement of the application is achieved.},
keywords={Classification algorithms;Engines;Clustering algorithms;Admission control;Prediction algorithms;Big Data;Admission;control job;congenial job;forecast engine;pattern classifier;QOS},
doi={10.1109/AEEICB.2017.7972391},
ISSN={},
month={Feb},}
@INPROCEEDINGS{8621867,
author={Gibson, J. Kade and Lee, Dongeun and Choi, Jaesik and Sim, Alex},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Dynamic Online Performance Optimization in Streaming Data Compression},
year={2018},
volume={},
number={},
pages={534-541},
abstract={Compression is essential to high bandwidth applications such as scientific simulations and sensing applications to reduce resource burden such as storage, network transmission, and more recently I/O. Existing lossy compression methods attempt to minimize the Euclidean distance between original data and reconstructed data, which significantly limits either compression performance or reconstruction quality since original and reconstructed data sequences should be aligned. Substituting the Euclidean distance for a statistical similarity maximizes the compression performance while retaining essential data features. By implementing this methodology, IDEALEM has recently demonstrated compression ratios far exceeding 100:1, better than best-known compression methods, while preserving reconstruction quality. This work proposes an online algorithm for streaming data compression which takes account of generally concave trend of compression ratio curve, and optimizes key operation parameters. We demonstrate that the proposed algorithm successfully adapts one of the key parameters in IDEALEM to the optimal value and yields near maximum compression ratios for time series data.},
keywords={Random variables;Data compression;Euclidean distance;Monitoring;Buffer storage;Optimization;Market research;compression performance optimization;multistage random sampling;lossy compression;Kolmogorov-Smirnov test;time series data},
doi={10.1109/BigData.2018.8621867},
ISSN={},
month={Dec},}
@ARTICLE{7914196,
author={Ding, Xiaoou and Wang, Hongzhi and Gao, Yitong and Li, Jianzhong and Gao, Hong},
journal={Tsinghua Science and Technology},
title={Efficient currency determination algorithms for dynamic data},
year={2017},
volume={22},
number={3},
pages={227-242},
abstract={Data quality is an important aspect in data application and management, and currency is one of the major dimensions influencing its quality. In real applications, datasets timestamps are often incomplete and unavailable, or even absent. With the increasing requirements to update real-time data, existing methods can fail to adequately determine the currency of entities. In consideration of the velocity of big data, we propose a series of efficient algorithms for determining the currency of dynamic datasets, which we divide into two steps. In the preprocessing step, to better determine data currency and accelerate dataset updating, we propose the use of a topological graph of the processing order of the entity attributes. Then, we construct an Entity Query B-Tree (EQB-Tree) structure and an Entity Storage Dynamic Linked List (ES-DLL) to improve the querying and updating processes of both the data currency graph and currency scores. In the currency determination step, we propose definitions of the currency score and currency information for tuples referring to the same entity and use examples to discuss methods and algorithms for their computation. Based on our experimental results with both real and synthetic data, we verify that our methods can efficiently update data in the correct order of currency.},
keywords={Heuristic algorithms;Remuneration;Real-time systems;Databases;Big Data;data quality management; data currency; dynamic determining},
doi={10.23919/TST.2017.7914196},
ISSN={1007-0214},
month={June},}
@INPROCEEDINGS{9150257,
author={Gong, Bing and Jing, Fan and Liu, Lili},
booktitle={2020 International Conference on Big Data and Informatization Education (ICBDIE)},
title={Research on the relationship between teachers' and students' behavior and the measures to improve the teaching quality},
year={2020},
volume={},
number={},
pages={334-337},
abstract={With the approach of artificial intelligence to education, big data analysis has played a very important role in higher education and higher education management. However, due to the complex and changeable characteristics of teachers and students' classroom behavior, especially the interpretation of classroom behavior has obvious subjective color, so the use of big data behavior data analysis and AI assisted teaching behavior interpretation will become the only way for the development of education. This paper use a variety of technical means to collect the behavior data of teachers and students in classroom teaching, and cross judge the multi-source data, introduce complete random number algorithm and transformation regression model to regularize the classroom data and build a prediction model. The positive and negative feedback of teachers' teaching behavior in classroom teaching is carried out, and the mapping relationship between students' behavior data and teachers' behavior data in classroom teaching is used to further judge teachers' teaching behavior, form evaluation mechanism in real time, and establish quantitative evaluation basis for teachers' teaching ability improvement.},
keywords={Education;Big Data;Data models;Predictive models;Psychology;Analytical models;component: classroom teaching;behavior data of teachers and students;prediction model;real-time evaluation mechanism},
doi={10.1109/ICBDIE50010.2020.00084},
ISSN={},
month={April},}
@ARTICLE{7217788,
author={Lee, Dongeun and Choi, Jaesik and Shin, Heonshik},
journal={IEEE Sensors Journal},
title={A Scalable and Flexible Repository for Big Sensor Data},
year={2015},
volume={15},
number={12},
pages={7284-7294},
abstract={Data generation rates of sensors are rapidly increasing, reaching a limit such that storage expansion cannot keep up with the data growth. We propose a new big data archiving scheme that handles the huge volume of sensor data with an optimized lossy coding. Our scheme leverages spatial and temporal correlations inherent in typical sensor data. The spatio-temporal correlations, observed in quality adjustable sensor data, enable us to compress a massive amount of sensor data without compromising distinctive attributes in sensor signals. Sensor data fidelity can also be decreased gradually. In order to maximize storage efficiency, we derive an optimal storage configuration for this data aging scenario. Experiments show outstanding compression ratios of our scheme and the optimality of storage configuration that minimizes system-wide distortion of sensor data under a given storage space.},
keywords={Data models;Distortion;Quantization (signal);Sensor phenomena and characterization;Encoding;Analytical models;Quality-adjustable sensor data;storage management;big data archiving;data compression;wireless sensor network;Big data archiving;data compression;quality-adjustable sensor data;storage management;wireless sensor network},
doi={10.1109/JSEN.2015.2471802},
ISSN={1558-1748},
month={Dec},}
@INPROCEEDINGS{9251155,
author={Aalijah, Kanwal and Irfan, Rabia},
booktitle={2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
title={Scalable Taxonomy Generation and Evolution on Apache Spark},
year={2020},
volume={},
number={},
pages={634-639},
abstract={Big data mainly refers to a huge volume of rapidly growing data over size exabytes (1018). A major chunk of this data is unstructured text data produced from several sources. In order to use such data effectively, they need to be processed and organized. Taxonomy, a hierarchical structure, is considered an effective way of organizing the data. In the past, many techniques have been proposed to generate taxonomy automatically. Recently some attempts have also been made to evolve the static structure of taxonomy to deal with the rapidly changing nature of data. However, the voluminous nature of today's data currently exceeds the processing capabilities of conventional techniques. In this regard, there is a need for a scalable technique that potentially speeds up the process of taxonomy generation and evolution and caters to a large amount of unstructured big data. This paper presents a technique for both the generation and the evolution of taxonomy on the Apache Spark framework. The technique is tested on a text dataset belonging to a computing domain. The test results show that the scalable taxonomy generation and evolution technique proposed in this paper is not only time- efficient but also produces a good quality taxonomy as compared to state-of-the-art techniques.},
keywords={Cloud computing;Tree graphs;Scalability;Taxonomy;Cluster computing;Big Data;Testing;Big Data;Apache Spark;Scalable Taxonomy Generation;Scalable Taxonomy Evolution;Unstructured Data},
doi={10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00110},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8334494,
author={Alves, Gabriel M. and Cruvinel, Paulo E.},
booktitle={2018 IEEE 12th International Conference on Semantic Computing (ICSC)},
title={Big Data Infrastructure for Agricultural Tomographic Images Reconstruction},
year={2018},
volume={},
number={},
pages={346-351},
abstract={A single agricultural soil sample obtained by a tomograph is composed of too many projections. In addition, considering that a sample is scanned at different angles a big set of projections is formed. Therefore, when using micro-resolution tomographic instruments, for a single soil sample it is necessary to deal with amounts of data in the order of gigabytes. On the other hand, the quantity of samples contributes to quality of information, for example, in the construction of maps to study agricultural soils. In general, in order to get improvements in the quality of soil analyzes it is required to increase exponentially the amount of the soil samples, i. e., increasing the amount of data to be reconstructed, which exceed the order of terabytes. This massive amount of data suggests new emerging methods and technologies. In this sense, Big Data has shown the great potential in optimizing data, making decisions, spotting business trends in various fields such agriculture. In this work, the infrastructure for the process of tomographic reconstruction of agricultural soil samples based on Big Data is presented. We introduced the Big Data architecture which uses the Hadoop framework. Additionally, we present the Filtered Back-Projection (FBP) algorithm adapted to the MapReduce model. The use of Big Data environment allows reconstructing a greatest number of agricultural soil tomographic images in the same time-frame and, consequently, it allows increasing the number of analysis contributing to improvement of quality of information about agricultural soils. Furthermore, the developed application has required both interpretation and language generation to allow the organization of knowledge, as well as the establishment of an adequate computational semantics for its operation.},
keywords={Image reconstruction;Soil;Big Data;Attenuation;Computed tomography;Mathematical model;big data;infrastructure;semantics in big data;agricultural tomograph;apache hadoop},
doi={10.1109/ICSC.2018.00071},
ISSN={},
month={Jan},}
@INPROCEEDINGS{5556606,
author={Lucas, Ana},
booktitle={5th Iberian Conference on Information Systems and Technologies},
title={Corporate data quality management: From theory to practice},
year={2010},
volume={},
number={},
pages={1-7},
abstract={It is now assumed that poor quality data is costing large amounts of money to corporations all over the world. Although research on methods and techniques for data quality assessment and improvement have begun in the early nineties of the past century and being currently abundant and innovative, it is noted that the academic and professional communities virtually have no dialogue, which turns out to be harmful to both of them. The challenge of promoting the relevance in information systems research, without compromising the necessary rigor, is still present in the various disciplines of information systems scientific area, including the data quality one. In this paper we present “data as a corporate asset” as a business philosophy, and a framework for the concepts related to that philosophy, derived from the academic and professional literature. According to this framework, we present, analyze and discuss a single explanatory case study, developed in a fixed and mobile telecommunications company, operating in one of the European Union Countries. The results show that, in the absence of data stewardship roles, data quality problems become more of an "IT problem" than typically is considered in the literature, owing to Requirements Analysis Teams of the IS Development Units, to become a “quality negotiator” between the various stakeholders. Other findings are their bottom-up approach to data quality management, their biggest focus on motivating employees through innovative forms of communication, which appears to be a critical success factor (CSF) for data quality management, as well as the importance of a data quality champion leadership.},
keywords={Quality management;Data models;Companies;Context;Process control;data quality management;framework;data quality initiative;case study},
doi={},
ISSN={2166-0735},
month={June},}
@ARTICLE{9142149,
author={Yang, Shuhui and Yuan, Zimu and Li, Wei},
journal={Big Data Mining and Analytics},
title={Error data analytics on RSS range-based localization},
year={2020},
volume={3},
number={3},
pages={155-170},
abstract={The quality of measurement data is critical to the accuracy of both outdoor and indoor localization methods. Due to the inevitable measurement error, the analytics on the error data is critical to evaluate localization methods and to find the effective ones. For indoor localization, Received Signal Strength (RSS) is a convenient and low-cost measurement that has been adopted in many localization approaches. However, using RSS data for localization needs to solve a fundamental problem, that is, how accurate are these methods? The reason of the low accuracy of the current RSS-based localization methods is the oversimplified analysis on RSS measurement data. In this proposed work, we adopt a generalized measurement model to find optimal estimators whose estimated error is equal to the Cramér-Rao Lower Bound (CRLB). Through mathematical techniques, the key factors that affect the accuracy of RSS-based localization methods are revealed, and the analytics expression that discloses the proportional relationship between the localization accuracy and these factors is derived. The significance of our discovery has two folds: First, we present a general expression for localization error data analytics, which can explain and predict the accuracy of range-based localization algorithms; second, the further study on the general analytics expression and its minimum can be used to optimize current localization algorithms.},
keywords={Measurement errors;Measurement uncertainty;Data analysis;Loss measurement;Newton method;Covariance matrices;Current measurement;Cramér-Rao Lower Bound (CRLB);error data analytics;generalized least squares;Received Signal Strength (RSS)},
doi={10.26599/BDMA.2020.9020001},
ISSN={2096-0654},
month={Sep.},}
@INPROCEEDINGS{8539474,
author={Ahn, Hoo Young and Kim, Hyunjae and You, WoongShik},
booktitle={2018 International Conference on Information and Communication Technology Convergence (ICTC)},
title={Performance Study of Distributed Big Data Analysis in YARN Cluster},
year={2018},
volume={},
number={},
pages={1261-1266},
abstract={In the 4-th Industrial Revolution era, various intelligent solutions and services have been emerging recently. To provide high quality service in those intelligent applications, the big data should be collected without any loss and comprehensively analyzed. Especially, when using machine and deep learning techniques, the big data processing delays should be minimized in order to guarantee the freshness of models. In this paper, we evaluate the performance of Apache Spark which is one of the most popular big data processing and analysis frameworks. Beyond the performance analysis of Spark in distributed cluster environment, we evaluate the performance of TensorFlowOnSpark which is the promising distributed deep learning framework designed to handle big data efficiently. From the experimental results, we can conclude that Spark on YARN is a solid underlying framework that guarantees the performance and scalability of distributed machine and deep learning by efficiently processing its data and algorithms in a parallel and distributed manner.},
keywords={Big Data;Sparks;Yarn;Training;Data models;Computational modeling;Distributed Machine Learning;Distributed Deep Learning;Big Data;Parallel Computing;Spark;YARN;TensorFlow;Benchmark},
doi={10.1109/ICTC.2018.8539474},
ISSN={2162-1233},
month={Oct},}
@INPROCEEDINGS{9155928,
author={Tavakoli, Mohammadreza and Elias, Mirette and Kismihók, Gábor and Auer, Sören},
booktitle={2020 IEEE 20th International Conference on Advanced Learning Technologies (ICALT)},
title={Quality Prediction of Open Educational Resources A Metadata-based Approach},
year={2020},
volume={},
number={},
pages={29-31},
abstract={In the recent decade, online learning environments have accumulated millions of Open Educational Resources (OERs). However, for learners, finding relevant and high quality OERs is a complicated and time-consuming activity. Furthermore, metadata play a key role in offering high quality services such as recommendation and search. Metadata can also be used for automatic OER quality control as, in the light of the continuously increasing number of OERs, manual quality control is getting more and more difficult. In this work, we collected the metadata of 8,887 OERs to perform an exploratory data analysis to observe the effect of quality control on metadata quality. Subsequently, we propose an OER metadata scoring model, and build a metadata-based prediction model to anticipate the quality of OERs. Based on our data and model, we were able to detect high-quality OERs with the F1 score of 94.6%.},
keywords={Metadata;Quality control;Predictive models;Open Educational Resources;Measurement;Data analysis;OER;open educational resources;metadata quality;OER quality;Big data;data analysis;quality prediction},
doi={10.1109/ICALT49669.2020.00007},
ISSN={2161-377X},
month={July},}