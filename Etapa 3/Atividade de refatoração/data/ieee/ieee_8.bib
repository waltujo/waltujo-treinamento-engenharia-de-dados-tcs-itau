@INPROCEEDINGS{8701948,
author={Ida, Masaaki},
booktitle={2019 21st International Conference on Advanced Communication Technology (ICACT)},
title={Consideration on the variation of financial data of institutions for canonical correlation analysis},
year={2019},
volume={},
number={},
pages={569-572},
abstract={In these days, progress of e-government and spread of electrical data lead to the prevail of public open databases, and also lead to the large amount of data analysis applications applying to these official open data. With regard to data analysis method, Canonical Correlation Analysis, which is one of the basic data analysis method and also data visualization method, is becoming the requisite skill for data scientists in this Big Data era. This paper examines the open data of financial data of education institutions. Especially, we focus on the higher education institutions and their financial data. In addition, we examine the variation of data and its problem to the data analysis. We aim to apply this analysis method and the result of consideration for supporting the improvement of quality assurance of higher education institutions.},
keywords={Correlation;Education;Data analysis;Databases;Correlation coefficient;Data visualization;Quality assurance;data analysis;canonical correlation analysis;data variation;financial data;higher education institution},
doi={10.23919/ICACT.2019.8701948},
ISSN={1738-9445},
month={Feb},}
@INPROCEEDINGS{8078801,
author={Song, Huaming and Cao, Zhexiu},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
title={Research on product quality evaluation based on big data analysis},
year={2017},
volume={},
number={},
pages={173-177},
abstract={In order to evaluate product quality from nonnumerical data, we propose the product quality evaluation model based on big data analysis including data collecting, data preprocessing, quality feature extraction, vector quantization and quality classification. Quality feature word extension algorithm, reviews quantization algorithm and machine learning algorithm are applied. We finally obtain the qualified rate(88.94%) and 7 features that most concerned by consumers through the analysis of 184,967 effective product reviews of wooden toys. In the end, we compare the SVM machine learning algorithm with decision tree and naive bayes, and discuss the credibility of the results. Our research on product quality evaluation extends the application of big data analysis, and also presents a new method to evaluate product quality in the field of manufacture.},
keywords={Quality assessment;Product design;Feature extraction;Dictionaries;Toy manufacturing industry;Data mining;Sentiment analysis;quality evaluation;online reviews;big data analysis;machine learning},
doi={10.1109/ICBDA.2017.8078801},
ISSN={},
month={March},}
@INPROCEEDINGS{7840715,
author={Zhang, Quan and Qiao, Mu and Routray, Ramani R. and Shi, Weisong},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={H2O: A hybrid and hierarchical outlier detection method for large scale data protection},
year={2016},
volume={},
number={},
pages={1120-1129},
abstract={Data protection is the process of backing up data in case of a data loss event. It is one of the most critical routine activities for every organization. Detecting abnormal backup jobs is important to prevent data protection failures and ensure the service quality. Given the large scale backup endpoints and the variety of backup jobs, from a backup-as-a-service provider viewpoint, we need a scalable and flexible outlier detection method that can model a huge number of objects and well capture their diverse patterns. In this paper, we introduce H2O, a novel hybrid and hierarchical method to detect outliers from millions of backup jobs for large scale data protection. Our method automatically selects an ensemble of outlier detection models for each multivariate time series composed by the backup metrics collected for each backup endpoint by learning their exhibited characteristics. Interactions among multiple variables are considered to better detect true outliers and reduce false positives. In particular, a new seasonal-trend decomposition based outlier detection method is developed, considering the interactions among variables in the form of common trends, which is robust to the presence of outliers in the training data. The model selection process is hierarchical, following a global to local fashion. The final outlier is determined through an ensemble learning by multiple models. Built on top of Apache Spark, H2O has been deployed to detect outliers in a large and complex data protection environment with more than 600,000 backup endpoints and 3 million daily backup jobs. To the best of our knowledge, this is the first work that selects and constructs large scale outlier detection models for multivariate time series on Big Data platforms.},
keywords={Time series analysis;Data protection;Market research;Water;Big data;Computational modeling;Data models;anomaly detection;multivariate time series;hybrid;hierarchical;big data;model selection},
doi={10.1109/BigData.2016.7840715},
ISSN={},
month={Dec},}
@ARTICLE{8053816,
author={Wang, Xiaokang and Yang, Laurence T. and Liu, Huazhong and Deen, M. Jamal},
journal={IEEE Transactions on Big Data},
title={A Big Data-as-a-Service Framework: State-of-the-Art and Perspectives},
year={2018},
volume={4},
number={3},
pages={325-340},
abstract={Due to the rapid advances of information technologies, Big Data, recognized with 4Vs characteristics (volume, variety, veracity, and velocity), bring significant benefits as well as many challenges. A major benefit of Big Data is to provide timely information and proactive services for humans. The primary purpose of this paper is to review the current state-of-the-art of Big Data from the aspects of organization and representation, cleaning and reduction, integration and processing, security and privacy, analytics and applications, then present a novel framework to provide high-quality so called Big Data-as-a-Service. The framework consists of three planes, namely sensing plane, cloud plane and application plane, to systemically address all challenges of the above aspects. Also, to clearly demonstrate the working process of the proposed framework, a tensor-based multiple clustering on bicycle renting and returning data is illustrated, which can provide several suggestions for rebalancing of the bicycle-sharing system. Finally, some challenges about the proposed framework are discussed.},
keywords={Big Data;Tensile stress;Cleaning;Organizations;Data privacy;Security;Big Data representation;Big Data integration;Big Data analytics;tensor;tensor decompositions;tensor networks},
doi={10.1109/TBDATA.2017.2757942},
ISSN={2332-7790},
month={Sep.},}
@INPROCEEDINGS{9006283,
author={Dilmaghani, Saharnaz and Brust, Matthias R. and Danoy, Grégoire and Cassagnes, Natalia and Pecero, Johnatan and Bouvry, Pascal},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Privacy and Security of Big Data in AI Systems: A Research and Standards Perspective},
year={2019},
volume={},
number={},
pages={5737-5743},
abstract={The huge volume, variety, and velocity of big data have empowered Machine Learning (ML) techniques and Artificial Intelligence (AI) systems. However, the vast portion of data used to train AI systems is sensitive information. Hence, any vulnerability has a potentially disastrous impact on privacy aspects and security issues. Nevertheless, the increased demands for high-quality AI from governments and companies require the utilization of big data in the systems. Several studies have highlighted the threats of big data on different platforms and the countermeasures to reduce the risks caused by attacks. In this paper, we provide an overview of the existing threats which violate privacy aspects and security issues inflicted by big data as a primary driving force within the AI/ML workflow. We define an adversarial model to investigate the attacks. Additionally, we analyze and summarize the defense strategies and countermeasures of these attacks. Furthermore, due to the impact of AI systems in the market and the vast majority of business sectors, we also investigate Standards Developing Organizations (SDOs) that are actively involved in providing guidelines to protect the privacy and ensure the security of big data and AI systems. Our far-reaching goal is to bridge the research and standardization frame to increase the consistency and efficiency of AI systems developments guaranteeing customer satisfaction while transferring a high degree of trustworthiness.},
keywords={Artificial intelligence;Big Data;Data privacy;Security;Data models;IEC Standards},
doi={10.1109/BigData47090.2019.9006283},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9378401,
author={Makkar, Himanshu and Toshniwal, Durga and Jangra, Shalini},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Closed Itemset based Sensitive Pattern Hiding for Improved Data Utility and Scalability},
year={2020},
volume={},
number={},
pages={4026-4035},
abstract={Frequent itemset mining is used to extract interesting associations and correlations between the itemsets present in transactional datasets. The frequently appearing patterns are used for various business decision making policies, for instance to increase co-purchase of products, price optimization, cross promotion etc. However, there are some sensitive patterns present in datasets that can reveal individual or organisation's specific confidential information that they would not prefer to be known since it can cause them huge social and monetary loss. Privacy Preserving Data Mining (PPDM) approaches are used to hide these sensitive patterns with maintaining the utility of the data. Heuristics-based PPDM approaches are widely adopted sensitive pattern hiding approaches due to their simplicity and lesser computational time as compared to the border-based and exact approaches. However, these approaches causes high side effects concerning the quality of datasets. In this paper, two heuristics-based algorithms, Removal of Closed Sensitive Itemsets with Maximum Support (MaxRCSI) and Removal of Closed Sensitive Itemsets with Minimum Support (MinRCSI), are proposed. In these algorithms, data sanitization is performed over closed sensitive itemsets to improve the utility of sanitized data. The proposed algorithms are parallelized on Spark parallel computing framework to deal with the massive amount of data i.e. big data. Experiments performed on real and synthetic datasets show that the proposed algorithms preserve the privacy of datasets with substantially better utility as compared to the traditional algorithms with less execution time.},
keywords={Data privacy;Itemsets;Heuristic algorithms;Scalability;Big Data;Parallel processing;Sparks;Privacy Preserving Data Mining;Data Sanitisation;Reduced Sensitive itemset;Data Quality},
doi={10.1109/BigData50022.2020.9378401},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7751629,
author={Li, Mingxin and Wei, Heng and Liao, Hongxi},
booktitle={2016 16th International Symposium on Communications and Information Technologies (ISCIT)},
title={Mobile terminal quality of experience analysis based on big data},
year={2016},
volume={},
number={},
pages={241-245},
abstract={In this paper, we proposes a method to analyze and evaluate the quality of experience (QoE) in mobile terminals using “big data”. The feature parameters of key quality indicator (KQI) are obtained from operators and quantized through the use of a scoring system. Then the scores of customer experience indicator (CEI) and QoE are calculated based on our proposed analytical model. In combination with the data of market operation, the terminal QoE evaluation scores contribute to offer effective suggestions on the promotion of mobile terminal.},
keywords={Indexes;Mobile communication;Mobile computing;Big data;Internet;Delays;big data;terminal;QoE;variable coefficient method},
doi={10.1109/ISCIT.2016.7751629},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8315370,
author={Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal},
booktitle={2017 IEEE 7th International Symposium on Cloud and Service Computing (SC2)},
title={Quality Profile-Based Cloud Service Selection for Fulfilling Big Data Processing Requirements},
year={2017},
volume={},
number={},
pages={149-156},
abstract={Big data has emerged as promising technology to handle huge and special data. Processing Big data involves selecting the appropriate services and resources thanks to the variety of services offered by different Cloud providers. Such selection is difficult, especially if a set of Big data requirements should be met. In this paper, we propose a dynamic cloud service selection scheme that assess Big data requirements, dynamically map these to the most available cloud services, and then recommend the best match services that fulfill different Big data processing requests. Our selection is conducted in two stages: 1) relies on a Big data task profile that efficiently capture Big data task's requirements and map them to QoS parameters, and then classify cloud providers that best satisfy these requirements, 2) uses the list of selected providers from stage 1 to further select the appropriate Cloud services to fulfill the overall Big Data task requirements. We extend the Analytic Hierarchy Process (AHP) based ranking mechanism to cope with the problem of multi-criteria selection. We conduct a set of experiments using simulated cloud setup to evaluate our selection scheme as well as the extended AHP against other selection techniques. The results show that our selection approach outperforms the others and select efficiently the appropriate cloud services that guarantee Big data task's QoS requirements.},
keywords={Big Data;Quality of service;Task analysis;Cloud computing;Data models;Analytic hierarchy process;Mathematical model;Big Data Task profile;Cloud service selection;QoS},
doi={10.1109/SC2.2017.30},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9524021,
author={Lili, Zhao and Jianing, Yang and Weiwei, Liu},
booktitle={2020 International Conference on Robots & Intelligent System (ICRIS)},
title={Prevention and Nursing method of Vascular Crisis Based on Big Data},
year={2020},
volume={},
number={},
pages={438-441},
abstract={In order to address issues related to the prevention and care of postoperative vascular crisis in patients with multifinger detachment reimplantation, and to improve the success rate and postoperative nursing efficiency of replantation operation, the paper comes up with a novel prevention and nursing method of vascular crisis based on big data. In this paper, 248 patients with multifinger detachment injuries admitted to the Department of Hand and Foot Surgery from January 2017 to December 2019 were used and divided into a control group and an observation group. The control group adopts the traditional conventional nursing care, while the observation group adopts the total quality control concept of nursing care, which takes patients' needs and satisfaction as the ultimate goal, respects patients' personal habits, and satisfies patients' psychological and physiological needs through management and quality control from beginning to end. The experimental results show that at the p less than 0.05 level for the incidence of vascular crisis in the observation group was lower than in the control group, the success rate of finger amputation and reimplantation in the observation group was higher than in the control group, the quality of nursing care was completed better in the observation group than in the control group, and the satisfaction of patients with nurses was higher in the observation group than in the control group.},
keywords={Surgery;Psychology;Medical services;Quality control;Big Data;Physiology;Intelligent systems;Total quality control;Patients after replantation of multiple severed fingers;Prevention and nursing of vascular crisis;Big data technology},
doi={10.1109/ICRIS52159.2020.00113},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8622412,
author={Stojanovic, Nenad and Milenovic, Dejan},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={Data-driven Digital Twin approach for process optimization: an industry use case},
year={2018},
volume={},
number={},
pages={4202-4211},
abstract={In this paper we present a novel approach for the process improvement based on the data-driven modelling. The idea is that by performing Big data analytics on the past process data we can model what is (statistically analyzed) usual/normal for a selected period and check the variations from that model in the real-time (as Six Sigma requires). Additionally, these data-driven models can support the root- cause analysis that should provide insights what can be eliminated as a waste in the process (as Lean requires). However, due to the above mentioned variety and volume of data, the analytics must be a) robust - dealing with differences efficiently and b) scalable - realized in an extremely parallel way. We propose a novel method for process control that uses big data analytics approaches to deal with the multidimensionality and the large size of the process space. In order to realize this idea we develop a new concept of self- aware digital twins which are able to reason about own behaviour and react if needed. Indeed, we revolutionize the concept of digital twins by extending their "virtual replica" (of physical objects) nature into "digital self-awareness" of physical objects (assets, systems), leading to the new generation of digital twins, so called self-aware DTs, which can "reasons" about the behaviour of an object (and not only mimic it) and actively participate in its improvement. We present the outcomes from the case study related to 3D laser cutting process.},
keywords={Process control;Big Data;Data models;Laser beam cutting;Monitoring;Production;Manufacturing;industry data analytics;data twin;process optimization},
doi={10.1109/BigData.2018.8622412},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9378181,
author={Srivastava, Divesh},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Towards High-Quality Big Data: Lessons from FIT},
year={2020},
volume={},
number={},
pages={4-4},
abstract={Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Data are being generated, collected, and analyzed today at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. As the use of big data has grown, so too have concerns that poor-quality data, prevalent in large data sets, can have serious adverse consequences on data-driven decision making. Responsible data science thus requires a recognition of the importance of veracity, the fourth "V" of big data. In this talk, we first present a vision of high-quality big data and highlight the substantial challenges that the first three V’s, volume, velocity, and variety, bring to dealing with veracity in big data. We then present the FIT Family of adaptive, data-driven statistical tools that we have designed, developed, and deployed at AT&T for continuous data quality monitoring of a large and diverse collection of continuously evolving data. These tools monitor data movement to discover missing, partial, duplicated, and delayed data; identify changes in the content of spatiotemporal streams; and pinpoint anomaly hotspots based on persistence, pervasiveness, and priority. We conclude with lessons from FIT relevant to big data quality that are cause for optimism.},
keywords={Big Data;Monitoring;Decision making;Data science;Conferences;Spatiotemporal phenomena;Magnetic heads},
doi={10.1109/BigData50022.2020.9378181},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8406658,
author={Palacio, Ana León and López, Óscar Pastor},
booktitle={2018 12th International Conference on Research Challenges in Information Science (RCIS)},
title={From big data to smart data: A genomic information systems perspective},
year={2018},
volume={},
number={},
pages={1-11},
abstract={During the last two decades, data generated by Next Generation Sequencing Technologies have revolutionized our understanding of human biology and improved the study on how changes (variations) in the DNA are involved in the risk of suffering a certain disease. A huge amount of genomic data is publicly available and frequently used by the research community in order to extract meaningful and reliable gene-disease relationships. However, management of this exponential growth of data has become a challenge for biologists; under such a big data problem perspective, they are forced to delve into a lake of complex data spread in over thousand heterogeneous repositories, represented in multiple formats and with different levels of quality; but when data are used to solve a concrete problem only a small part of that “data lake” is really significant; this is what we call the “smart” data perspective. Using conceptual models and the principles of data quality management, adapted to the genomic domain, we propose a systematic approach to move from a big data to a smart data perspective. The aim of this approach is to populate an Information System with genomic data which must be accessible, informative and actionable enough to extract valuable knowledge.},
keywords={Genomics;Bioinformatics;Big Data;Data integrity;Diseases;Data models;Conceptual Modelling;Data Quality;Big Data;Smart Data;Genomics},
doi={10.1109/RCIS.2018.8406658},
ISSN={2151-1357},
month={May},}
@ARTICLE{7817820,
author={Zhu, Julie Yixuan and Sun, Chenxi and Li, Victor O.K.},
journal={IEEE Transactions on Big Data},
title={An Extended Spatio-Temporal Granger Causality Model for Air Quality Estimation with Heterogeneous Urban Big Data},
year={2017},
volume={3},
number={3},
pages={307-319},
abstract={This paper deals with city-wide air quality estimation with limited air quality monitoring stations which are geographically sparse. Since air pollution is influenced by urban dynamics (e.g., meteorology and traffic) which are available throughout the city, we can infer the air quality in regions without monitoring stations based on such spatial-temporal (ST) heterogeneous urban big data. However, big data-enabled estimation poses three challenges. The first challenge is data diversity, i.e., there are many different categories of urban data, some of which may be useless for the estimation. To overcome this, we extend Granger causality to the ST space to analyze all the causality relations in a consistent manner. The second challenge is the computational complexity due to processing the massive volume of data. To overcome this, we introduce the non-causality test to rule out urban dynamics that do not “Granger” cause air pollution, and the region of influence (ROI), which enables us to only analyze data with the highest causality levels. The third challenge is to adapt our grid-based algorithm to non-grid-based applications. By developing a flexible grid-based estimation algorithm, we can decrease the inaccuracies due to grid-based algorithm while maintaining computation efficiency.},
keywords={Estimation;Vehicle dynamics;Monitoring;Atmospheric modeling;Big data;Air pollution;Granger causality;spatio-temporal (ST);heterogeneous;big data;air quality estimation},
doi={10.1109/TBDATA.2017.2651898},
ISSN={2332-7790},
month={Sep.},}
@INPROCEEDINGS{8531368,
author={Liu, Qing and Zeng, Ming},
booktitle={2018 International Conference on Virtual Reality and Intelligent Systems (ICVRIS)},
title={Selection Model of Optimal Mixed Teaching Mode in Higher Vocational Colleges Based on Big Data},
year={2018},
volume={},
number={},
pages={139-142},
abstract={Under the background of big data, we should select the best hybrid teaching mode in higher vocational colleges, improve the ability of big data analysis of the mixed teaching mode in higher vocational colleges, and improve the quality of hybrid teaching mode in higher vocational colleges. A model for selecting hybrid teaching mode in the optimal higher vocational colleges is proposed based on big data. The big data analysis model of hybrid teaching in the optimal higher vocational colleges is constructed, and the information fusion of the mixed teaching mode in the optimal higher vocational colleges is carried out by using the structured big data information recombination method. The characteristic quantity of the associated information describing the optimal hybrid teaching mode in higher vocational colleges is extracted, and the big data fusion scheduling and optimization selection of the mixed teaching mode based on the piecewise information fusion is adopted. According to the characteristic clustering results, the self-regression analysis of the evaluation ability of hybrid teaching in the optimal higher vocational colleges is carried out, and the test statistic model is constructed to optimize the selection of the hybrid teaching model in higher vocational colleges. The simulation results show that this method is used to select the mixed teaching mode in higher vocational colleges, the information fusion ability of outputting big data is better, and the accuracy of model selection is high.},
keywords={Education;Big Data;Data models;Analytical models;Data mining;Mathematical model;Computational modeling;big data;higher vocational colleges;mixed teaching model;information fusion},
doi={10.1109/ICVRIS.2018.00041},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8713218,
author={Mylavarapu, Goutam and Thomas, Johnson P and Viswanathan, K Ashwin},
booktitle={2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)},
title={An Automated Big Data Accuracy Assessment Tool},
year={2019},
volume={},
number={},
pages={193-197},
abstract={Data analysis is the most important aspect of any business as it is critical to decision-making. Data quality assessment is a necessary function to be performed before data analysis, as the quality of data has high impact on the outcome of the analysis. Data quality is a multi-dimensional factor that affects the analysis in numerous ways. Among all the dimensions, accuracy is the most important and hardest dimension to assess. With the advent of big data, this problem becomes more complicated. There are only few studies that focus on data accuracy with minimal domain expert dependency. In this paper, we propose an extensive data accuracy assessment tool that uses machine learning to determine the accuracy of data. In addition, our model also addresses the intrinsic and contextual categories of data accuracy. Our model was developed on Apache Spark which serves as the big data environment for handling large datasets.},
keywords={Data integrity;Big Data;Tools;Data models;Standards;Machine learning;Couplings;Data accuracy;contextual accuracy;data quality;word embeddings;record linkage;k-nearest neighbors;logistic regression;decision trees},
doi={10.1109/ICBDA.2019.8713218},
ISSN={},
month={March},}
@INPROCEEDINGS{9377820,
author={Serajian, Reza and Ehsani, Reza},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Application of big data for improving air quality during almond harvesting process},
year={2020},
volume={},
number={},
pages={5819-5821},
abstract={What was once a seasonal disturbance is becoming a more significant problem with orchard dust moving into towns and across main traffic thoroughfares. It's the act of sweeping the fruit into windrows that creates the problem by simultaneously agitating and blowing the soil along with the fruit. The soil billows up in clouds of dust, creating zero visibility. This ultimately creates a windrow that is equally filled with fruit and soil and is further exacerbated by picking up the combination of fruit and soil (within the windrow) and billowing a second round of dense dust into the surrounding air. Solutions to the dust problem focus on removing the sweepers and potentially the `pick-ups'. In this study, we are designing a new mechanism for almond sweeping and pick up with minimum dust generated by adapted sweepers in order to have minimum contact with ground using distance sensors and actively measure particulate matters (PM2.5 and PM10) using dust sensors and adjust system to reach acceptable level of generated dust according to National Ambient Air Quality Standards in almond harvesting process.},
keywords={Soil;Big Data;Air quality;Particle measurements;Sensor systems;Sensors;Standards;Dust;Sensor;Almond;Harvest;Particulate matter},
doi={10.1109/BigData50022.2020.9377820},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7840998,
author={Du, Fang and Li, Ting and Shi, Yingjie and Song, Lijuan and Gu, Xiaojun},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Drug target path discovery on semantic biomedical big data},
year={2016},
volume={},
number={},
pages={3381-3386},
abstract={Systems chemical biology integrate chemistry, biology and computation tools as a whole system, which can help researchers to deeply study the interaction and relationship among small molecules, such as genes, proteins, targets, compounds and so on. With systems chemical biology, researchers can concentrate on new way of drug discovery, including drug target path discovery, which can not only help biomedical researchers to find evidences for existing disease associate genes, but also to design new effect medicine based on targets. Network based approaches are the state-of-art solutions for drug target path discovery, however, there are still some challenges: 1) The quality of the network dominate the efficiency and accuracy of the results, therefore a well designed network is quite important on drug target path discovery mission; 2) the existing network based approaches only work on small graph, it can not handle massive data well. In the paper, we designed a novel framework of systems chemical biology based on semantic big data. In the paper, we proposed a novel drug target path discovery approach. It can identify targets associated with specific medicines (disease) and the path of relationship based on a RDF semantic D-T network. The ranking of candidate targets is performed through an improved parallel random walk with restart algorithm. The experimental studies show that the proposed approaches can efficiently discover drug target relationship path, meanwhile, the approaches have good scalability which are suitable for big data analysis.},
keywords={Decision support systems;Big data;Semantics;Drugs;Conferences;Buildings;Distributed algorithms;Drug target path discovery;Semantic D-T network;Biomedical big data;Random walk with restart},
doi={10.1109/BigData.2016.7840998},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9378231,
author={Zhao, Shangping and Liu, Pan and Tang, Guanxiu and Guo, Yanming and Li, Guohui},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Development and Application of an Intensive Care Medical Data Set for Deep Learning},
year={2020},
volume={},
number={},
pages={3369-3373},
abstract={A large number of patient healthcare data have been collected in the process of diagnosis and treatment of intensive care medicine, which provides major benefits for patient safety and quality. Unfortunately, the application of medical data is greatly limited. Key barriers to the use of the data include difficulties in data extraction and cleaning, and the construction of high-quality data sets promotes the research of medical big data analysis. In China, there is few intensive care data set built by clinicians has been used for clinical outcome prediction. This study developed and evaluated an Intensive Care Medical (ICM) data set for critically care patients that can be used for deep learning. The ICM data set contained four types of data collected routinely in Chinese hospitals, including all-cause characteristics of administrative information, vital signs, laboratory tests, and intravenous medication records. A total of 17,291 ICU admissions involving 12,815 patients aged 14 years and older were extracted from the data set. Deep learning model achieved high accuracy for tasks in hospital mortality predicting (AUROC[area under the receiver operator curve] reach 0.8941). We believe that the ICM data set can be used to create accurate predictions for a variety of clinical scenarios.},
keywords={Deep learning;Hospitals;Big Data;Safety;Data mining;Task analysis;Medical diagnostic imaging;intensive care;critical care;data set;deep learning;clinical outcome},
doi={10.1109/BigData50022.2020.9378231},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8750951,
author={Mehmood, Hassan and Gilman, Ekaterina and Cortes, Marta and Kostakos, Panos and Byrne, Andrew and Valta, Katerina and Tekes, Stavros and Riekki, Jukka},
booktitle={2019 IEEE 35th International Conference on Data Engineering Workshops (ICDEW)},
title={Implementing Big Data Lake for Heterogeneous Data Sources},
year={2019},
volume={},
number={},
pages={37-44},
abstract={Modern connected cities are more and more leveraging advances in ICT to improve their services and the quality of life of their inhabitants. The data generated from different sources, such as environmental sensors, social networking platforms, traffic counters, are harnessed to achieve these end goals. However, collecting, integrating, and analyzing all the heterogeneous data sources available from the cities is a challenge. This article suggests a data lake approach built on Big Data technologies, to gather all the data together for further analysis. The platform, described here, enables data collection, storage, integration, and further analysis and visualization of the results. This solution is the first attempt to integrate a diverse set of data sources from four pilot cities as part of the CUTLER project (Coastal urban development through the lenses of resiliency). The design and implementation details, as well as usage scenarios are presented in this paper.},
keywords={Lakes;Smart cities;Big Data;Sensors;Economics;Monitoring;data lake, Big Data, Smart City, data analysis},
doi={10.1109/ICDEW.2019.00-37},
ISSN={2473-3490},
month={April},}
@ARTICLE{7970191,
author={Zhu, Julie Yixuan and Zhang, Chao and Zhang, Huichu and Zhi, Shi and Li, Victor O.K. and Han, Jiawei and Zheng, Yu},
journal={IEEE Transactions on Big Data},
title={pg-Causality: Identifying Spatiotemporal Causal Pathways for Air Pollutants with Urban Big Data},
year={2018},
volume={4},
number={4},
pages={571-585},
abstract={Many countries are suffering from severe air pollution. Understanding how different air pollutants accumulate and propagate is critical to making relevant public policies. In this paper, we use urban big data (air quality data and meteorological data) to identify the spatiotemporal (ST) causal pathways for air pollutants. This problem is challenging because: (1) there are numerous noisy and low-pollution periods in the raw air quality data, which may lead to unreliable causality analysis; (2) for large-scale data in the ST space, the computational complexity of constructing a causal structure is very high; and (3) the ST causal pathways are complex due to the interactions of multiple pollutants and the influence of environmental factors. Therefore, we present pg-Causality, a novel pattern-aided graphical causality analysis approach that combines the strengths of pattern mining and Bayesian learning to efficiently identify the ST causal pathways. First, pattern mining helps suppress the noise by capturing frequent evolving patterns (FEPs) of each monitoring sensor, and greatly reduce the complexity by selecting the pattern-matched sensors as “causers”. Then, Bayesian learning carefully encodes the local and ST causal relations with a Gaussian Bayesian Network (GBN)-based graphical model, which also integrates environmental influences to minimize biases in the final results. We evaluate our approach with three real-world data sets containing 982 air quality sensors in 128 cities, in three regions of China from 01-Jun-2013 to 31-Dec-2016. Results show that our approach outperforms the traditional causal structure learning methods in time efficiency, inference accuracy and interpretability.},
keywords={Air pollution;Bayes methods;Data mining;Atmospheric modeling;Time series analysis;Big Data;Urban areas;Causality;pattern mining;Bayesian learning;spatiotemporal (ST) big data;urban computing},
doi={10.1109/TBDATA.2017.2723899},
ISSN={2332-7790},
month={Dec},}
@INPROCEEDINGS{7840925,
author={Mohan, Aravind and Ebrahimi, Mahdi and Lu, Shiyong and Kotov, Alexander},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Scheduling big data workflows in the cloud under budget constraints},
year={2016},
volume={},
number={},
pages={2775-2784},
abstract={Big data is fast becoming a ubiquitous term in both academia and industry and there is a strong need for new data-centric workflow tools and techniques to process and analyze large-scale complex datasets that are growing exponentially. On the other hand, the unbound resource leasing capability foreseen in the cloud facilitates data scientists to wring actionable insights from the data in a time and cost efficient manner. In the data-centric workflow environment, scheduling data processing tasks onto appropriate resources are often driven by the constraints provided by the users. Enforcing a constraint while executing the workflow in the cloud adds a new optimization challenge on how to meet the objective while satisfying the given constraint. In this paper, we propose a new Big dAta woRkflow schEduler uNder budgeT constraint known as BARENTS that supports high-performance workflow scheduling in a heterogeneous cloud computing environment with a single objective to minimize the workflow makespan under a provided budget constraint. Our case study and experiments show the competitive advantages of our proposed scheduler. The proposed BARENTS scheduler is implemented in a new release of DATA VIEW, one of the most usable big data workflow systems in the community.},
keywords={Big data;Cloud computing;Data communication;Cost function;Computational modeling;Quality of service;Job shop scheduling;Scheduler;BARENTS;Big Data},
doi={10.1109/BigData.2016.7840925},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9298381,
author={J, Bastin Robins.},
booktitle={2020 IEEE International Conference for Innovation in Technology (INOCON)},
title={Cognito - Intuitive Auto Data Exploratory Toolkit},
year={2020},
volume={},
number={},
pages={1-6},
abstract={In any machine learning model, the eminence of discovered knowledge is directly associated with the quality of data used for analysis. In the technological world, massive growth is observed in the scale of data generation. Big data resolve many challenges of the organization by predicting the future or finding valuable insights from the data. Despite this, Big data itself is a major challenge in front of researchers because the data doesn't come in cleaned format. Data preprocessing becomes a crucial phase that handles anomalies and noise in the data. Though various data pre-processing algorithms are available to clean the dataset, data analysts spend a large amount of time to achieve it. Cognito being an open-source python library offers various features to reduce the time and efforts of the data scientist. Cognito facilitates the automatic pre-processing and fundamental data analysis by providing the cleaned dataset, summarized report about the dataset, features of each column of the dataset, and possible questions that can be asked to the dataset in an attractive format. It provides output in CSV format and a summary of the dataset in PDF format. Developers do not need to wrap their heads around or waste their time analyzing the data. It also reduces the user's effort by a considerable margin. Moreover, the cleaned data generated by Cognito is crunched, thus reducing the space required. Cognito automatically manages various data pre-processing operations such as feature selection, missing value imputation, normalization, outlier treatment, data reduction with additional features of auto-generated description insights.},
keywords={Feature extraction;Data mining;Data analysis;Big Data;Tools;Data models;Transforms;Data analysis;Data pre-processing;Data reduction;Python;Imputation},
doi={10.1109/INOCON50539.2020.9298381},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9526772,
author={Hongling, Liu and Di, Wan},
booktitle={2020 13th International Conference on Intelligent Computation Technology and Automation (ICICTA)},
title={Research on Data Preprocessing and 3D Matrix Model},
year={2020},
volume={},
number={},
pages={606-609},
abstract={In order to overcome the problems existing in the current big data mining platform, this paper proposes a novel construction method of three-dimensional matrix model based on big data mining technology. This construction method uses the massive data mining function and massive data storage function provided by big data cloud platform, starting from the needs of users, in order to solve the problems existing in big data mining platform, for example, lack of scientific and efficient data model, lack of scientific and reasonable high-dimensional data association algorithm, dealing with the thorny problems of big data link, tedious and complex association rules, and so on. The construction method takes big data extended data mining technology as the center, combining data analysis technology, data extraction technology and so on. On this basis, based on the three-dimensional matrix model, the construction method completes the scientific and efficient preprocessing of data information from different sources. The research results show that this construction method can improve the efficiency and quality of association principle mining on the basis of massive high-dimensional data, and effectively alleviate the tedious and complex problem of association rules in the process of dealing with big data.},
keywords={Solid modeling;Cloud computing;Analytical models;Data analysis;Computational modeling;Memory;Big Data;Data preprocessing;Data mining;Three-dimensional matrix;High and new technology},
doi={10.1109/ICICTA51737.2020.00134},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9260297,
author={LYU, Bu and CHEN, Jianhui and WANG, Na},
booktitle={2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA)},
title={Teaching Quality Evaluation Model for Human Resource Development and Management Major Under the Background of Big Data},
year={2020},
volume={},
number={},
pages={529-533},
abstract={The evaluation of teaching quality in colleges relies on reliable and comprehensive evaluation data, which is also an important basis for educational decision-making. The application of big data technology can realize scientific analysis and in-depth mining of a large number of data, analyze the value and connection implied in the data on a multidimensional level, and enable the evaluation of teaching quality to transform from the previous analysis of segment information and small sample data to the whole process of all-round data decision-making. This paper proposes a learning quality evaluation model based on big data in cloud computing environment, using the structure method, AHP, fuzzy comprehensive evaluation and multi-objective optimization method. It is also combined with a variety of theories such as user perception theory and user experience theory, and the development of this model is studied. Finally, we develop and design the learning quality evaluation system under cloud computing, to test the system and prove the credibility of our scheme.},
keywords={Zirconium;Xenon;Smart grids;Quality function deployment;Hafnium compounds;Conferences;Automation;big data;cloud computing;teaching quality;human resource},
doi={10.1109/ICSGEA51094.2020.00120},
ISSN={},
month={June},}
@INPROCEEDINGS{9403739,
author={Wei, Li and Dawei, Wang and Lixia, Wang},
booktitle={2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)},
title={Research on data Traceability Method Based on blockchain Technology},
year={2020},
volume={},
number={},
pages={45-49},
abstract={Energy Internet is a major innovation to deal with the environmental crisis and efficient energy management and use in the current society. The important condition to achieve this goal is to summarize, integrate, process and apply the data of various industries in the energy field, and then support the relevant management and decision-making. In this process, how to ensure the authenticity and credibility of data is one of the keys in the construction of energy Internet. Therefore, this paper will study the application scenarios of blockchain technology in data traceability. With the help of the natural characteristics of blockchain, such as decentralized, distributed storage, tamper proof, open and transparent, combined with relevant national standards and international theoretical models, based on the needs of energy Internet data integration and management, this paper will develop a data traceability method suitable for the energy industry, and build a covering energy Data life cycle model of Internet. Through the research of this paper, we can help all localities to establish data traceability mechanism in the energy Internet, to help users to accurately grasp where the data is created, what systems have been transferred, which users have carried out query and modification, and so on, so as to realize the monitoring and control of the whole process of data flow, which helps to improve the credibility of data, and also helps to ensure the safety and quality of data and promote the construction of energy Internet huge data application.},
keywords={Industries;Technological innovation;Distributed databases;Blockchain;Data models;Internet;Safety;blockchain;data traceability;data quality;data security;data governance;energy Internet;huge data},
doi={10.1109/ICBASE51474.2020.00017},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6603739,
author={Park, Hyoung Woo and Yeo, Il Yeon and Lee, Jongsuk Ruth and Jang, Haengjin},
booktitle={2013 Seventh International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing},
title={Study on Big Data Center Traffic Management Based on the Separation of Large-Scale Data Stream},
year={2013},
volume={},
number={},
pages={591-594},
abstract={The network of traditional data center has been usually designed and constructed for the provision of user's equal access of data centre's resource or data. Therefore, network administrators have a strong tendency to manage user traffic from the viewpoint that the traffic has a similar size and characteristics. But, the emersion of big data begins to make data centers have to deal with 1015 byte-data transfer at once. Such a big data transfer can cause problems in network traffic management in the existed data center. And, the tiered network architecture of the legacy data center magnifies the magnitude of the problems. One of the well-known big data in science is from large hadron collider such as LHC in Swiss CERN. CERN LHC generates multi-peta byte data per year. From our experience of CERN data service, this paper showed the impact of network traffic affected by large-scale data stream using NS2 simulation, and then, suggested the evolution direction based on separating of large-scale data stream for the big data center's network architecture.},
keywords={Information management;Data handling;Data storage systems;Computer architecture;Large Hadron Collider;Data models;Buildings;Big data traffic;data-centered network;subnetwork evolution;low-end traffic seperation;future traffic management},
doi={10.1109/IMIS.2013.104},
ISSN={},
month={July},}
@INPROCEEDINGS{8258138,
author={Furtado, Vasco and Furtado, Elizabeth and Caminha, Carlos and Lopes, André and Dantas, Victor and Ponte, Caio and Cavalcante, Sofia},
booktitle={2017 IEEE International Conference on Big Data (Big Data)},
title={A data-driven approach to help understanding the preferences of public transport users},
year={2017},
volume={},
number={},
pages={1926-1935},
abstract={The maintenance of the quality of the public transport service in big cities requires constant monitoring, which may become an expensive and time-consuming practice. The perception of quality, from the users point of view is an important aspect of quality monitoring. In this sense, we proposed a methodology based on big data analysis and visualization, which allows for the structuring of estimates and assumptions of where and who seems to be having unsatisfactory experiences while making use of the public transportation in metropolitan areas. Moreover, it provides support in setting up a plan for on-site quality surveys. The proposed methodology increases the likelihood that, with the on-site visits, the interviewer finds users who suffer inconveniences, which influence their behavior. Simulation comparison and a small-scale pilot survey helped validate the proposed method.},
keywords={Tools;Urban areas;Monitoring;Data visualization;Big Data;Quality survey;Public transport;Smart City;Intelligent Data mining},
doi={10.1109/BigData.2017.8258138},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8636154,
author={Mahmoud, Nesma and Elbeh, Heba and Abdlkader, Hatem M.},
booktitle={2018 14th International Computer Engineering Conference (ICENCO)},
title={Ontology Learning Based on Word Embeddings for Text Big Data Extraction},
year={2018},
volume={},
number={},
pages={183-188},
abstract={Big Data term describes data that exists everywhere in humongous volumes, raw forms, and heterogenous types. Unstructured and uncategorized data forms 95% of big data. Text big data lacks to efficiently extract domain-relevant data in a suitable time. Thus, text big data stills a barrier for big data integration and subsequently big data analytics. Because big data integration can't consider text big data in its process of preparing data for big data analytics. On the other side, ontology represents information and knowledge in a graph schema that provides a shareable, reusing and domain-specific data. Thus, ontology fits text big data needs of extracting domain relevant data. So, this paper proposes an ontology learning (OL) methodology for text big data extraction. OL aims to provides algorithms, techniques, and tools for automatic ontology construction from the text. The proposed OL method exploits a deep learning approach i.e., word embeddings, and advanced hierarchical clustering i.e., BIRCH. The utilization of the word embeddings and the advanced hierarchical clustering improve OL quality in text big data extraction and reduce the processing time. Also, deep learning unsupervisory learns from a massive amount of unlabeled and uncategorized raw data. This great big benefit solves analytical challenge of the text big data. In evaluation, precision, recall, and f - value for the work quality and the running time for performance are measured. The quality of work is evaluated by comparing its results with gold standard datasets results. Experimental results and evaluation demonstrate that the proposed OL methodology efficiently suitable for text big data extraction.},
keywords={Big Data;Ontologies;Data mining;Clustering algorithms;Task analysis;Buildings;Deep learning;Text Big data;Big Data Integration;Deep learning;Word embeddings;Data mining;Hierarchical clustering},
doi={10.1109/ICENCO.2018.8636154},
ISSN={2475-2320},
month={Dec},}
@INPROCEEDINGS{7840769,
author={Ganapathi, Archana and Chen, Yanpei},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={Data quality: Experiences and lessons from operationalizing big data},
year={2016},
volume={},
number={},
pages={1595-1602},
abstract={Data quality issues pose a significant barrier to operationalizing big data. They pertain to the meaning of the data, the consistency of that meaning, the human interpretation of results, and the contexts in which the results are used. Data quality issues arise after organizations have moved past clear-cut technical solutions to early bottlenecks in using data. Left unaddressed, such issues can and have led to high profile missteps, and raise doubts about the data-driven world view altogether. In this paper, we share real-world case studies of tackling data quality challenges across industry verticals. We present initial ideas on how to systematically address data quality issues via technology. The success of operationalizing big data will depend on the quality of data involved, and whether such data causes uncertainty and disruptions, or delivers genuine knowledge and value.},
keywords={Cleaning;Big data;Measurement;Business;Software;Instruments;Industries},
doi={10.1109/BigData.2016.7840769},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7473058,
author={Gao, Jerry and Xie, Chunli and Tao, Chuanqi},
booktitle={2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)},
title={Big Data Validation and Quality Assurance -- Issuses, Challenges, and Needs},
year={2016},
volume={},
number={},
pages={433-441},
abstract={With the fast advance of big data technology and analytics solutions, big data computing and service is becoming a very hot research and application subject in academic research, industry community, and government services. Nevertheless, there are increasing data quality problems resulting in erroneous data costs in enterprises and businesses. Current research seldom discusses how to effectively validate big data to ensure data quality. This paper provides informative discussions for big data validation and quality assurance, including the essential concepts, focuses, and validation process. Moreover, the paper presents a comparison among big data validation tools and several major players in industry are discussed. Furthermore, the primary issues, challenges, and needs are discussed.},
keywords={Big data;Quality assurance;Organizations;Q-factor;Standards organizations;Quality assurance;big data quality assurance;big data validation;data validation},
doi={10.1109/SOSE.2016.63},
ISSN={},
month={March},}
@INPROCEEDINGS{8036724,
author={Miller, John A. and Peng, Hao and Cotterell, Michael E.},
booktitle={2017 IEEE World Congress on Services (SERVICES)},
title={Adding Support for Theory in Open Science Big Data},
year={2017},
volume={},
number={},
pages={71-75},
abstract={Open Science Big Data is emerging as an important area of research and software development. Although there are several high quality frameworks for Big Data, additional capabilities are needed for Open Science Big Data. These include data provenance, citable reusable data, data sources providing links to research literature, relationships to other data and theories, transparent analysis/reproducibility, data privacy, new optimizations/advanced algorithms, data curation, data storage and transfer. An important part of science is explanation of results, ideally leading to theory formation. In this paper, we examine means for supporting the use of theory in big data analytics as well as using big data to assist in theory formation. One approach is to fit data in a way that is compatible with some theory, existing or new. Functional Data Analysis allows precise fitting of data as well as penalties for lack of smoothness or even departure from theoretical expectations. This paper discusses principal differential analysis and related techniques for fitting data where, for example, a time-based process is governed by an ordinary differential equation. Automation in theory formation is also considered. Case studies in the fields of computational economics and finance are considered.},
keywords={Big Data;Biological system modeling;Economic indicators;Analytical models;Data models;Mathematical model;Data analysis;Big data;Predictive analytics;Theory;Frameworks;Functional data analysis;Principal differential analysis},
doi={10.1109/SERVICES.2017.20},
ISSN={},
month={June},}
@ARTICLE{7452617,
author={Lin, Bing and Guo, Wenzhong and Xiong, Naixue and Chen, Guolong and Vasilakos, Athanasios V. and Zhang, Hong},
journal={IEEE Transactions on Network and Service Management},
title={A Pretreatment Workflow Scheduling Approach for Big Data Applications in Multicloud Environments},
year={2016},
volume={13},
number={3},
pages={581-594},
abstract={The rapid development of the latest distributed computing paradigm, i.e., cloud computing, generates a highly fragmented cloud market composed of numerous cloud providers and offers tremendous parallel computing ability to handle big data problems. One of the biggest challenges in multiclouds is efficient workflow scheduling. Although the workflow scheduling problem has been studied extensively, there are still very few primal works tailored for multicloud environments. Moreover, the existing research works either fail to satisfy the quality of service (QoS) requirements, or do not consider some fundamental features of cloud computing such as heterogeneity and elasticity of computing resources. In this paper, a scheduling algorithm, which is called multiclouds partial critical paths with pretreatment (MCPCPP), for big data workflows in multiclouds is presented. This algorithm incorporates the concept of partial critical paths, and aims to minimize the execution cost of workflow while satisfying the defined deadline constraint. Our approach takes into consideration the essential characteristics of multiclouds such as the charge per time interval, various instance types from different cloud providers, as well as homogeneous intrabandwidth vs. heterogeneous interbandwidth. Various types of workflows are used for evaluation purpose and our experimental results show that the MCPCPP is promising.},
keywords={Cloud computing;Big data;Scheduling;Quality of service;Scheduling algorithms;Optimization;Big data;cloud computing;multiclouds;scheduling;scientific workflow},
doi={10.1109/TNSM.2016.2554143},
ISSN={1932-4537},
month={Sep.},}
@INPROCEEDINGS{9251206,
author={Jiang, Yushan and Liu, Yongxin and Liu, Dahai and Song, Houbing},
booktitle={2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)},
title={Applying Machine Learning to Aviation Big Data for Flight Delay Prediction},
year={2020},
volume={},
number={},
pages={665-672},
abstract={Flight delay has been a serious and widespread problem that needs to be solved. One promising solution is the flight delay prediction. Although big data analytics and machine learning have been applied successfully in many domains, their applications in aviation are limited. This paper presents a comprehensive study of flight delay spanning data pre-processing, data visualization and data mining, in which we develop several machine learning models to predict flight arrival delays. Two data sets were used, namely Airline On-Time Performance (AOTP) Data and Quality Controlled Local Climatological Data (QCLCD). This paper aims to recognize useful patterns of the flight delay from aviation data and perform accurate delay prediction. The best result for flight delay prediction (five classes) using machine learning models is 89.07% (Multilayer Perceptron). A Convolution neural network model is also built which is enlightened by the idea of pattern recognition and success of neural network method, showing a slightly better result with 89.32% prediction accuracy.},
keywords={Atmospheric modeling;Neural networks;Machine learning;Predictive models;Big Data;Delays;Quantum cascade lasers;Flight Delay;Machine Learning;Aviation Data Analytics},
doi={10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00114},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9148508,
author={Wen, Yan and Li, Min and Ye, Yu},
booktitle={2020 International Conference on Computer Information and Big Data Applications (CIBDA)},
title={MapReduce-Based BP Neural Network Classification of Aquaculture Water Quality},
year={2020},
volume={},
number={},
pages={132-135},
abstract={As information technology develops and prevails across the globe, the informatization and data processing efficiency in the aquaculture field are exerting increasing impact on the intelligent conversion of this field. The aquaculture water quality indicators were analyzed by a feedforward error back propagation algorithm (BP neural network) with strong nonlinear mapping capacity, and the complicated nonlinear relations among the parameters of the aquaculture environment were solved. An aquaculture element analysis model was proposed, the Johnson attribute reduction algorithm based on the discernibility matrix was used to optimize the traditional algorithm, and the network convergence speed was increased under a given accuracy. The MapReduece distributed programming model was then used to perform parallel design of the BP neural network algorithm to meet the needs of massive data processing in aquaculture platforms. Also, case studies were performed to analyze the aquaculture element model and the parallel learning algorithm, and the big data framework design and data analysis method are integrated to develop an efficient, fault-tolerant aquaculture data management, mining, visualization big data system.},
keywords={Neural networks;Aquaculture;Data analysis;BP neural network algorithm;MapReduce Framework;Intelligent Aquaculture;Data analysis},
doi={10.1109/CIBDA50819.2020.00038},
ISSN={},
month={April},}
@INPROCEEDINGS{8078702,
author={Wang, Dongshan and Song, Yanbin and Zhao, Chong},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},
title={Bayesian classification based service-awareness in software defined optical network for big data services},
year={2017},
volume={},
number={},
pages={589-592},
abstract={With emerging of multiple services brought by Big Data, the rapid evolution of Software Defined Optical Network (SDON) provides great support for big data services with high dynamic. However, newly emerging big data services have diversified characteristics and demands, which presents great problems for current SDON to match these services with high quality and matching degree. This paper proposes a Bayesian Classification based Service Awareness (BC-SA) mechanism of SDON for big data services. By using Bayesian classification, the BC-SA mechanism is able to be aware of the type of service and to cooperate with the OpenFlow protocol, with the aim to achieve high matching degree between services and SDON. Based on this BC-SA, bandwidth scheduling can be improved in SDON. Simulation results show that the BC-SA can match requirements by big data services with better performances.},
keywords={Bayes methods;Big Data;Delays;Optical fiber networks;Protocols;Packet loss;Big Date Services;Software Defined Optical Network;Service Awareness;Bayesian Classification},
doi={10.1109/ICBDA.2017.8078702},
ISSN={},
month={March},}
@ARTICLE{8809689,
author={Fiore, Sandro and Elia, Donatello and Pires, Carlos Eduardo and Mestre, Demetrio Gomes and Cappiello, Cinzia and Vitali, Monica and Andrade, Nazareno and Braz, Tarciso and Lezzi, Daniele and Moraes, Regina and Basso, Tania and Kozievitch, Nádia P. and Fonseca, Keiko Verônica Ono and Antunes, Nuno and Vieira, Marco and Palazzo, Cosimo and Blanquer, Ignacio and Meira, Wagner and Aloisio, Giovanni},
journal={IEEE Access},
title={An Integrated Big and Fast Data Analytics Platform for Smart Urban Transportation Management},
year={2019},
volume={7},
number={},
pages={117652-117677},
abstract={Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.},
keywords={Urban areas;Big Data;Data analysis;Transportation;Cloud computing;Data mining;Europe;Big data;cloud computing;data analytics;data privacy;data quality;distributed environment;public transport management;smart city},
doi={10.1109/ACCESS.2019.2936941},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7818902,
author={Lakshen, Guma Abdulkhader and Vraneš, Sanja and Janev, Valentina},
booktitle={2016 24th Telecommunications Forum (TELFOR)},
title={Big data and quality: A literature review},
year={2016},
volume={},
number={},
pages={1-4},
abstract={Big Data refers to data volumes in the range of Exabyte (1018) and beyond. Such volumes exceed the capacity of current on-line storage and processing systems. With characteristics like volume, velocity and variety big data throws challenges to the traditional IT establishments. Computer assisted innovation, real time data analytics, customer-centric business intelligence, industry wide decision making and transparency are possible advantages, to mention few, of Big Data. There are many issues with Big Data that warrant quality assessment methods. The issues are pertaining to storage and transport, management, and processing. This paper throws light into the present state of quality issues related to Big Data. It provides valuable insights that can be used to leverage Big Data science activities.},
keywords={Big data;Benchmark testing;Quality assessment;Databases;Software;Sparks;Engines;Big Data;Quality assessment;stream processing;survey;Big Data frameworks},
doi={10.1109/TELFOR.2016.7818902},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8622249,
author={Huang, Yu and Milani, Mostafa and Chiang, Fei},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={PACAS: Privacy-Aware, Data Cleaning-as-a-Service},
year={2018},
volume={},
number={},
pages={1023-1030},
abstract={Data cleaning consumes up to 80% of the data analysis pipeline. This is a significant overhead for organizations where data cleaning is still a manually driven process requiring domain expertise. Recent advances have fueled a new computing paradigm called Database-as-a-Service, where data management tasks are outsourced to large service providers. We propose a new Data Cleaning-as-a-Service model that allows a client to interact with a data cleaning provider who hosts curated, and sensitive data. We present PACAS: a Privacy-Aware data Cleaning-As-a-Service framework that facilitates communication between the client and the service provider via a data pricing scheme where clients issue queries, and the service provider returns clean answers for a price while protecting her data. We propose a practical privacy model in such interactive settings called (X,Y,L)-anonymity that extends existing data publishing techniques to consider the data semantics while protecting sensitive values. Our evaluation over real data shows that PACAS effectively safeguards semantically related sensitive values, and provides improved accuracy over existing privacy-aware cleaning techniques.},
keywords={Cleaning;Data privacy;Pricing;Data models;Semantics;Osteoarthritis;Maintenance engineering;data quality;data cleaning;data privacy},
doi={10.1109/BigData.2018.8622249},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8085914,
author={Nazarenko, Maxim A. and Khronusova, Tatiana V.},
booktitle={2017 International Conference "Quality Management,Transport and Information Security, Information Technologies" (IT&QM&IS)},
title={Big data in modern higher education. Benefits and criticism},
year={2017},
volume={},
number={},
pages={676-679},
abstract={Big Data technology is very efficient in some cases, however, it has some disadvantages. It should be applied to modern universities which current quality of education is high enough. Thus the universities have to satisfy several conditions, for example, providing broad sets of different type tasks, including group discussions, oral speeches, essays with more than one possible correct opinion, developing complex skills of their students; collecting information about courses, student's activities and progress, alumni skills. Undoubtedly, Big Data are not the only opportunity to develop quality of education. Many small universities provide private educational programs for small groups. Moreover, they offer their students more direct conversations with lecturers in words of mouth format. This educational strategy definitely has its own advantages. Usage of Big Data in Russia is possible, however, it claims increasing of higher education quality.},
keywords={Big Data;Mouth;Educational programs;Medical services;Big Data;education;universities;types of tasks;complex skills},
doi={10.1109/ITMQIS.2017.8085914},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8631726,
author={Silva, Bhagya Nathali and Khan, Murad and Seo, Jihun and Muhammad, Diyan and Yoon, Yongtak and Han, Jihun and Han, Kiiun},
booktitle={2018 12th International Conference on Signal Processing and Communication Systems (ICSPCS)},
title={Exploiting Big Data Analytics for Urban Planning and Smart City Performance Improvement},
year={2018},
volume={},
number={},
pages={1-4},
abstract={The smart city notion facilitate interoperation among multiple disciplines to improve the Quality of Life (QoL) of urban citizens. Unceasingly growing urban networks has significantly increased the data processing complexity. In consequence, real-time data processing and analysis has become a major concern in modern smart city designing and implementation. Considering the challenges of existing smart cities, in this work we propose a smart city architecture embedded with Big Data Analytics (BDA). The utmost goal of the proposed scheme is to enhance the quality of real-time decision-making through efficient Big Data (BD) processing. The proposed architecture is in three folds to manage data collection, data processing, and data application. We evaluate the proposed BDA embedded smart city using authentic datasets on water consumption, traffic congestion, parking management, and air pollution measurements. The analysis offer useful insights for the community development, while ensuring the performance improvement of the proposed framework in terms of processing time and throughput.},
keywords={Smart cities;Big Data;Computer architecture;Computer science;Throughput;smart city;Big Data smart city;WoT smart city;urban planning},
doi={10.1109/ICSPCS.2018.8631726},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8284019,
author={Bellatreche, Ladjel},
booktitle={2017 First International Conference on Embedded & Distributed Systems (EDiS)},
title={Take the best of big data: Just focus on some of its V's},
year={2017},
volume={},
number={},
pages={3-3},
abstract={Big data represents a new technology for managing data with high velocity, volume, variety and contributes to creating value for companies. As quoted in1, capturing all queries made on the company website or from customer support calls, emails or chat lines, regardless of their outcome, may have significant value in identifying emerging trends. The Big Data Era has largely contributed in accelerating the development strategic plans issued from governments and research organisms, coving the management, exploitation and analysis of these data by taking into account the different V's of Big Data. Among these plans, we can cite for instance the development of: (a) large-scale platforms (ex. data-clusters, distributed data clusters), (b) Software Defined Environments (SDE) (ex. IBM SDE), (c) advanced programming paradigms (ex. map-reduce, Spark, etc.), Data Analytics Tools (Rapid Miner, Google Fusion Tables, Solver), (d) Visualization tools (Google Chart, Tableau, Oracle Visual Analyzer), and (h) high quality and valuable Knowledge Bases (KB), constructed either by academicians (e.g., Cvc, DBpedia, Freebase, and YAGO) and industrials (e.g., Google Knowledge Graph, Facebook Knowledge Graph, Amazon Knowledge Graph, Credit Rating Agencies, Enterprise Knowledge Base, etc.). In this talk, we would like to foster the creation of a think tank dedicated to getting the best from Big Data V's and the efforts related to it to revisit our research activities without compromising them. In this talk, we would like to share the experience conducted with our Model and Data Engineering Team of the LIAS Laboratory at ISAE-ENSMA, which aims at the design of data warehousing applications. Based on the literature, this design is based on two main approaches: (i) a supply-driven approach (also called data-driven) that starts with an analysis of operational data sources in order to identify all the available data and (ii) a user-driven approach (also known as requirement-driven or goal-orientated) which stems from the determination of the information requirements of different business users. Several studies and experiments show that resorting to these two approaches entails a high risk for companies, since some functional requirements cannot be satisfied. This is due to the lack of relevant data in sources. In parallel, reference studies have identified the crucial role of knowledge bases (KB) for analytical tasks, by offering analysts more entities (people, places, products, etc.). The availability of a huge, high quality valuable KB is an asset for data warehousing designers and decision-makers to construct/exploit a valuable data warehouse. So, faced with this situation, we here present a value-driven approach that revisits the traditional life cycle of the design of data warehouses, by considering KB as an external resource. These different phases are illustrated via the YAGO KB.},
keywords={Big Data;Knowledge based systems;Companies;Warehousing;Distributed databases;Data warehouses;Conferences},
doi={10.1109/EDIS.2017.8284019},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8367700,
author={Yu, Bin and Zhang, Chen and Tang, ZhouHua and Sun, JiangYan},
booktitle={2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA)},
title={Verification method of data quality in science and technology cloud in Shaanxi province},
year={2018},
volume={},
number={},
pages={319-323},
abstract={This paper analyzes and summarizes the data quality problems in the Shaanxi Science and Technology Resource Coordination Center “Science and Technology Cloud” project. These two major problems about scientific and technological information data quality are verified. One is data redundancy caused by organizations' name abbreviation and the other is partial scientific and technological information data missing. This paper designs and implements solutions to the problems in “Science and Technology Cloud” project. This paper extracts 15643 data from scientific and technical talent pool and scientific literature library. The experimental results verify the effectiveness and feasibility of the solution of data redundancy and data missing.},
keywords={Data integrity;Redundancy;Databases;Dynamic programming;Education;Remuneration;Organizations;Science and Technology Cloud;data quality;data redundancy;missing value processing},
doi={10.1109/ICBDA.2018.8367700},
ISSN={},
month={March},}
@INPROCEEDINGS{9005627,
author={Benbernou, Salima and Ouziri, Mourad},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={QualiFood: An Intelligent Quality Food Evaluation Using Logical Satisfiability Reasoning On Spark},
year={2019},
volume={},
number={},
pages={5165-5171},
abstract={There is an urgent need to address unhealthy dietary patterns for people. Therefore, having a high food quality is essential to health by adopting good eating habits. Furthermore, the evaluation of food quality is complex because it needs huge numbers of information related to food, for example its ingredients, the environmental conditions of food productions, the consumer state of health. Such information are scattered on multiple and non communicating systems including food producer systems, open data, medical data, etc. Applying the semantic to a very large collection of information from different data sources can highly contribute to enhance the food quality score. Hence, it will improve the individual diet/health by giving adequate nutritional recommendation and avoiding inconsistent food mixing by following set of rules.In this paper, we propose an intelligent and scalable approach to ensuring the food quality for meal healthiness query answering over big data related to food in a distributed way on a Spark ecosystem. For that, the cleaning inconsistent and contradictory big data approach is built by following the steps (1)modeling the consistency rules including inference and inconsistent rules (2) detecting inconsistency through rule evaluation on Apache Spark framework to discover the minimally subset of inconsistent data (3) cleaning the inconsistency through finding the cleaned meals for consistent query answering.},
keywords={Ontologies;Diseases;Semantics;Fats;Resource description framework;Heart;Big Data;Food quality;Logical satisfiability reasoning;data cleaning;big data;RDF(S);OWL},
doi={10.1109/BigData47090.2019.9005627},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9373666,
author={Tan, Yongchun},
booktitle={2020 International Conference on Computers, Information Processing and Advanced Education (CIPAE)},
title={How to Effectively Infiltrate Emotional Education in Primary School Chinese Teaching from Perspective of Big Data},
year={2020},
volume={},
number={},
pages={138-141},
abstract={Cultural inheritance and education is an important part of social development, primary school is an important period for children to receive cultural education, we should not only pay attention to the study of cultural courses but also pay attention to the infiltration of emotional education. With the development of the Internet, cloud technology and big data information in the field of education, the establishment of various education platforms and learning platforms has an increasing impact on primary school Chinese teaching. Based on this background, the purpose of this study is to combine the advanced technology of big data information with the efficient Chinese teaching in primary schools to improve the teaching quality. The research idea of this paper is to use big data information analysis and calculation in the emotional infiltration education of primary school Chinese teaching, analyze and demonstrate the data through the form of questionnaire and interview, and design the research scheme of how to effectively infiltrate emotional education in primary school Chinese teaching from the perspective of big data from the perspective of theory and practice. The research results of this paper show that in the emotional penetration education of Chinese teaching in primary schools, the proportion that the learning quality is affected by big data technology accounts for 67%, and the proportion that the emotional education has a positive impact on students accounts for 63%. The research results indicate that the active application of big data technology can effectively improve the quality of education and benefit the physical and mental health development of students. However, most teachers have little understanding and application of big data, and there is also the irrational phenomenon of emphasizing cognition and ignoring emotion.},
keywords={Systematics;Education;Mental health;Big Data;Permeability;Cultural differences;Interviews;big data vision;Chinese teaching in primary schools;emotional education;quality of education},
doi={10.1109/CIPAE51077.2020.00044},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8805946,
author={Yang, Peng and Yang, Ming},
booktitle={2019 International Conference on Communications, Information System and Computer Engineering (CISCE)},
title={Research on the Management Model of University Students Academic Early Warning Based on Big Data Analysis},
year={2019},
volume={},
number={},
pages={639-642},
abstract={The advancement of technology has greatly promoted education. And with the popularization of information technology construction in higher education institutions, the concept of "Big Data" has been paid more and more attention in the management of higher education. Data mining is used to analyze students learning situation. The design of the academic early warning management system can effectively prevent some students from being at risk of dropping out of school due to academic difficulties, improve the quality of higher education operations, and provide higher education managers with the idea of carrying out targeted risk prevention.},
keywords={Education;Big Data;Distributed databases;Data mining;Data visualization;Alarm systems;big data;academic warning;model},
doi={10.1109/CISCE.2019.00148},
ISSN={},
month={July},}
@INPROCEEDINGS{7363743,
author={Libes, Don and Shin, Seungjun and Woo, Jungyub},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Considerations and recommendations for data availability for data analytics for manufacturing},
year={2015},
volume={},
number={},
pages={68-75},
abstract={Data analytics is increasingly becoming recognized as a valuable set of tools and techniques for improving performance in the manufacturing enterprise. However, data analytics requires data and a lack of useful and usable data has become an impediment to research in data analytics. In this paper, we describe issues that would help aid data availability including data quality, reliability, efficiency, and formats specific to data analytics in manufacturing. To encourage data availability, we present recommendations and requirements to guide future data contributions. We also describe the need for data for challenge problems in data analytics. A better understanding of these needs, recommendations, and requirements may improve the ability of researchers and other practitioners to improve research and more rapidly deploy data analytics in manufacturing.},
keywords={Data analysis;Manufacturing;Sensors;Encryption;NIST;Synchronization;big data;challenge problems;data analytics;data quality;requirements;smart manufacturing},
doi={10.1109/BigData.2015.7363743},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8622349,
author={Sinha, Shweta and Seys, Marcia},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={HL7 Data Acquisition & Integration: Challenges and Best Practices},
year={2018},
volume={},
number={},
pages={2453-2457},
abstract={Lack of interoperability between health data systems is a leading challenge for healthcare in the United States. This paper describes the challenges and lessons learned in the process of incorporating HL7 data and integration with Electronic Health Records and Health Information Exchanges from the perspective of a midsized Health Plan (Payer). As a Health Care Payer, Premera has a unique perspective regarding how health plans can provide the necessary data to complete the picture of care. This paper shares some of the best practices and focus areas for successful implementation of healthcare data integrations. This paper also focuses on integrating claims and clinical data using a master patient index as well as challenges faced in that process.Note that going forward `Health Plan' and `Payer' will be used interchangeably. Also, `Provider(s)', `Hospitals', `Healthcare Providers', `Clinics', `Provider Organizations' will be used interchangeably and in the context of this paper may mean the same.},
keywords={Medical services;Organizations;Standards organizations;Engines;Monitoring;Best practices;Interoperability;HL7;big data;integration;acquisition;master patient index;payer;interoperability;data quality},
doi={10.1109/BigData.2018.8622349},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8029332,
author={Miller, John A. and Peng, Hao and Cotterell, Michael E.},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={Adding Support for Theory in Open Science Big Data},
year={2017},
volume={},
number={},
pages={251-255},
abstract={Open Science Big Data is emerging as an important area of research and software development. Although there are several high quality frameworks for Big Data, additional capabilities are needed for Open Science Big Data. These include data provenance, citable reusable data, data sources providing links to research literature, relationships to other data and theories, transparent analysis/reproducibility, data privacy, new optimizations/advanced algorithms, data curation, data storage and transfer. An important part of science is explanation of results, ideally leading to theory formation. In this paper, we examine means for supporting the use of theory in big data analytics as well as using big data to assist in theory formation. One approach is to fit data in a way that is compatible with some theory, existing or new. Functional Data Analysis allows precise fitting of data as well as penalties for lack of smoothness or even departure from theoretical expectations. This paper discusses principal differential analysis and related techniques for fitting data where, for example, a time-based process is governed by an ordinary differential equation. Automation in theory formation is also considered. Case studies in the fields of computational economics and finance are considered.},
keywords={Big Data;Biological system modeling;Predictive models;Mathematical model;Economic indicators;Analytical models;Data models;Big data;Predictive analytics;Theory;Frameworks;Functional data analysis;Principal differential analysis},
doi={10.1109/BigDataCongress.2017.40},
ISSN={},
month={June},}
@INPROCEEDINGS{9418960,
author={Wang, Jianguo},
booktitle={2020 International Conference on Information Science and Education (ICISE-IE)},
title={Optimization of Teaching Content and Reform of Teaching Methods on the Course of Coal-Geology Based on Big Data Analysis},
year={2020},
volume={},
number={},
pages={54-57},
abstract={Coal-geology course is one of the core courses of resource exploration engineering major in most universities of geology, mining and petroleum. In the process of teaching, we should constantly improve the optimization design of course content, optimize course content and reform teaching methods through big data technology analysis. On the basis of the training goal, requirement, the study purpose and the emphasis difference of resource exploration engineering major, the course has relatively fewer class schedule, obviously, fewer theoretical and experimental classes, it increases the difficulty of teachers to master the teaching content of the course. Since the content of the course is the basis of ensuring the teaching goal, talent training and implementation process, therefore, the content of the course should be targeted and selected, we should grasp flexibly the big data features of systematicness, scientificalness and progressiveness of courses content, improving students' learning motivation and active participation, making full use of information technology, constantly innovating teaching methods, improving teaching quality, promoting teaching level and students' comprehensive quality, in this way, we can ensure the unity of teaching process, optimization design and teaching efficiency.},
keywords={Training;Schedules;Information science;Geology;Education;Big Data;Data mining;coal-geology;course content;big data analysis;evaluating indicator;reform of teaching methods},
doi={10.1109/ICISE51755.2020.00019},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7363818,
author={Bonner, Stephen and McGough, Andrew Stephen and Kureshi, Ibad and Brennan, John and Theodoropoulos, Georgios and Moss, Laura and Corsar, David and Antoniou, Grigoris},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={Data quality assessment and anomaly detection via map/reduce and linked data: A case study in the medical domain},
year={2015},
volume={},
number={},
pages={737-746},
abstract={Recent technological advances in modern healthcare have lead to the ability to collect a vast wealth of patient monitoring data. This data can be utilised for patient diagnosis but it also holds the potential for use within medical research. However, these datasets often contain errors which limit their value to medical research, with one study finding error rates ranging from 2.3%-26.9% in a selection of medical databases. Previous methods for automatically assessing data quality normally rely on threshold rules, which are often unable to correctly identify errors, as further complex domain knowledge is required. To combat this, a semantic web based framework has previously been developed to assess the quality of medical data. However, early work, based solely on traditional semantic web technologies, revealed they are either unable or inefficient at scaling to the vast volumes of medical data. In this paper we present a new method for storing and querying medical RDF datasets using Hadoop Map / Reduce. This approach exploits the inherent parallelism found within RDF datasets and queries, allowing us to scale with both dataset and system size. Unlike previous solutions, this framework uses highly optimised (SPARQL) joining strategies, intelligent data caching and the use of a super-query to enable the completion of eight distinct SPARQL lookups, comprising over eighty distinct joins, in only two Map / Reduce iterations. Results are presented comparing both the Jena and a previous Hadoop implementation demonstrating the superior performance of the new methodology. The new method is shown to be five times faster than Jena and twice as fast as the previous approach.},
keywords={Resource description framework;Medical diagnostic imaging;Medical services;Big data;Sensors;Biomedical monitoring;RDF;Medical Data;Map / Reduce;Joins},
doi={10.1109/BigData.2015.7363818},
ISSN={},
month={Oct},}
@ARTICLE{9139506,
author={Zhou, Shengyao and He, Jie and Yang, Hui and Chen, Donghua and Zhang, Runtong},
journal={IEEE Access},
title={Big Data-Driven Abnormal Behavior Detection in Healthcare Based on Association Rules},
year={2020},
volume={8},
number={},
pages={129002-129011},
abstract={Healthcare insurance frauds are causing millions of dollars of public healthcare fund losses around the world in various ways, which makes it very important to strengthen the management of medical insurance in order to guarantee the steady operation of medical insurance funds. Healthcare fraud detection methods can reduce the losses of healthcare insurance funds and improve medical quality. Existing fraud detection studies mostly focus on finding normal behavior patterns and treat those violating normal behavior patterns as fraudsters. However, fraudsters can often disguise themselves with some normal behaviors, such as some consistent behaviors when they seek medical treatments. To address these issues, we combined a MapReduce distributed computing model and association rule mining to propose a medical cluster behavior detection algorithm based on frequent pattern mining. It can detect certain consistent behaviors of patients in medical treatment activities. By analyzing 1.5 million medical claim records, we have verified the effectiveness of the method. Experiments show that this method has better performance than several benchmark methods.},
keywords={Insurance;Medical diagnostic imaging;Big Data;Data mining;Data models;Medical treatment;Big data;abnormal behavior;healthcare insurance;association rules},
doi={10.1109/ACCESS.2020.3009006},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7723680,
author={Tian, Ling and Wang, Hongyu and Tang, Qinyu and Zhou, Yimin},
booktitle={2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)},
title={Surveillance Source Compression with Background Modeling for Video Big Data},
year={2016},
volume={},
number={},
pages={105-110},
abstract={Video source is a kind of critical component in big data. As a typical video source, the cost of storage and transmission is extremely high for surveillance source. Thus, it is the coding target to get a tradeoff between bit-rate and visual quality for surveillance video. Devoted to this subject, this work proposes a new background modeling scheme for surveillance source, which adopts the residual gradient and the block edge differences to construct background picture. The constructed background picture preserves the spatial characteristics of the source contents and then it is chosen as the long-term reference picture. This work proposes a novel background-based coding optimization algorithm (BCOA) for both picture level and the largest coding unit (LCU) level in video compression. According to the effective adjustment of quantization parameter (QP) and lagrange multiplier (λ), the proposed BCOA improves the visual quality. Compared with the background-modeling-based hierarchical prediction structure optimization, experimental results show that BCOA achieves better visual quality and BD-Rate gain up to 46.68%.},
keywords={video big data;video compression;surveillance video;background modeling},
doi={10.1109/BDCloud-SocialCom-SustainCom.2016.26},
ISSN={},
month={Oct},}
@ARTICLE{8732398,
author={Xia, Qiufen and Xu, Zichuan and Liang, Weifa and Yu, Shui and Guo, Song and Zomaya, Albert Y.},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={Efficient Data Placement and Replication for QoS-Aware Approximate Query Evaluation of Big Data Analytics},
year={2019},
volume={30},
number={12},
pages={2677-2691},
abstract={Enterprise users at different geographic locations generate large-volume data that is stored at different geographic datacenters. These users may also perform big data analytics on the stored data to identify valuable information in order to make strategic decisions. However, it is well known that performing big data analytics on data in geographical-located datacenters usually is time-consuming and costly. In some delay-sensitive applications, the query result may become useless if answering a query takes too long time. Instead, sometimes users may only be interested in timely approximate rather than exact query results. When such approximate query evaluation is the case, applications must sacrifice timeliness to get more accurate evaluation results or tolerate evaluation result with a guaranteed error bound obtained from analyzing the samples of the data to meet their stringent timeline. In this paper, we study quality-of-service (QoS)-aware data replication and placement for approximate query evaluation of big data analytics in a distributed cloud, where the original (source) data of a query is distributed at different geo-distributed datacenters. We focus on the problems of placing data samples of the source data at some strategic datacenters to meet stringent query delay requirements of users, by exploring a non-trivial trade-off between the cost of query evaluation and the error bound of the evaluation result. We first propose an approximation algorithm with a provable approximation ratio for a single approximate query. We then develop an efficient heuristic algorithm for evaluating a set of approximate queries with the aim to minimize the evaluation cost while meeting the delay requirements of these queries. We finally demonstrate the effectiveness and efficiency of the proposed algorithms through both experimental simulations and implementations in a real test-bed, real datasets are employed. Experimental results show that the proposed algorithms are promising.},
keywords={Big Data;Query processing;Delays;Approximation algorithms;Quality of service;Distributed databases;Data analysis;Data replication and placement;big data analytics;approximate query evaluation;approximation algorithms;algorithm analysis},
doi={10.1109/TPDS.2019.2921337},
ISSN={1558-2183},
month={Dec},}
@INPROCEEDINGS{9109990,
author={Wang, Lijun},
booktitle={2020 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)},
title={Research on Tax Collection and Administration Based on Big Data Analysis},
year={2020},
volume={},
number={},
pages={679-682},
abstract={Utilizing big data is the main trend in tax collection and administration. In the era of big data, to enhance application effect of big data in tax collection and administration, we must try our best to excavate big data. First, the work mode of tax collection and administration needs to be changed, and management force of taxation work process should be strengthened. Then, according to the current status of enterprise development, a comprehensive big data application system should be formulated, to allow enterprise personnel to grasp various tax-related information fully. Constructing a set of tax management systems that are compatible with the development of the enterprise, and paying attention to the use and research, quality and efficiency of tax administration can be steadily improved.},
keywords={Economics;Law;Smart cities;Local government;Force;Finance;Legislation;Big data;Tax collection and administration;Internet technology;Information},
doi={10.1109/ICITBS49701.2020.00149},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8823437,
author={Zeng, Yu-Ren and Chang, Yue Shan and Fang, You Hao},
booktitle={2019 International Conference on System Science and Engineering (ICSSE)},
title={Data Visualization for Air Quality Analysis on Bigdata Platform},
year={2019},
volume={},
number={},
pages={313-317},
abstract={With the advances of industry, air pollution is increasingly becoming serious, and most of governments in the world has deployed many devices to monitor daily air quality. Monitoring and forecasting of air quality has also become an important issue to improve the quality of people's lives. As far as we know, bad air quality does not only affect the health of the respiratory tract, it may but also even cause mental illness. Many researchers have investigated different approaches to work on air quality forecast, and the visualization of forecasting becomes important. In this paper, we present an architecture for visualizing forecasted air quality on a big data platform. We implemented an ETL (Extract-Transform-Load) based framework in the platform, which includes computing nodes and storage nodes. Computational nodes are used for data collection and for air quality forecasting over the next 1 to 8 hours through machine learning and deep learning. Storage nodes are used to retrieve, analyze, and preprocess of collected data. We use the RESTful Web Service as an API, and finally we use the browser to get the data by predefined API and to present the forecasted and monitored results with Google Map API and D3 JavaScript library. It reveals that the visualization on big data framework can work well for air quality analysis.},
keywords={Data visualization;Air quality;Big Data;Forecasting;Monitoring;Web servers;Air Quality;Big Data;Forecasting;Cloud Environment;Data Visualization},
doi={10.1109/ICSSE.2019.8823437},
ISSN={2325-0925},
month={July},}
@INPROCEEDINGS{9182539,
author={Liu, Zhenhua and Su, Liwei},
booktitle={2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)},
title={Big Data Analysis Model of Customer Appeal Based on Power Enterprise Service Platform},
year={2020},
volume={},
number={},
pages={1328-1331},
abstract={Today is the era of Internet information. Affected by economic development and living standards, power users have higher and higher expectations for power supply services. Although customer complaints are inevitable, as an important part of customer feedback in high-quality services, how to use customer complaint information efficiently, reasonably, and scientifically has become a problem that every power grid company must face in the Internet + era. The purpose of this article is to study the big data analysis model of customer demands based on the electric power enterprise service platform. This article first introduces the meaning of customer satisfaction and customer demands, and then analyzes the development status of the power enterprise service platform, and proposes the need to improve customer demand management. Based on this, this article establishes a big data analysis model of customer demands. The experimental results prove that the analysis model designed in this paper can not only solve the needs of power enterprises, but also improve the economic benefits of enterprises. In this paper, the economic benefits obtained from the four indicators of failure repair service, power outage information management norms, business process norms and customer service representatives' work efficiency improvement, and the statistical results of a power company using the model one year later are 5.25 million yuan.},
keywords={Companies;Power supplies;Analytical models;Customer services;Big Data;Data models;Electric Power Enterprise;Customer Satisfaction;Customer Demand;Big Data;Analysis Model},
doi={10.1109/ICAICA50127.2020.9182539},
ISSN={},
month={June},}
@INPROCEEDINGS{7364058,
author={Feng, Tao and Zhuang, Zhenyun and Pan, Yi and Ramachandra, Haricharan},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={A memory capacity model for high performing data-filtering applications in Samza framework},
year={2015},
volume={},
number={},
pages={2600-2605},
abstract={Data quality is essential in big data paradigm as poor data can have serious consequences when dealing with large volumes of data. While it is trivial to spot poor data for small-scale and offline use cases, it is challenging to detect and fix data inconsistency in large-scale and online (real-time or near-real time) big data context. An example of such scenario is spotting and fixing poor data using Apache Samza, a stream processing framework that has been increasingly adopted to process near-real-time data at LinkedIn. To optimize the deployment of Samza processing and reduce business cost, in this work we propose a memory capacity model for Apache Samza to allow denser deployments of high performing data-filtering applications built on Samza. The model can be used to provision just-enough memory resource to applications by tightening the bounds on the memory allocations. We apply our memory capacity model on Linkedln's real use cases in production, which significantly increases the deployment density and saves business costs. We will share key learning in this paper.},
keywords={Containers;LinkedIn;Data models;Big data;Java;Measurement;Real-time systems;Apache Samza;capacity model;data filtering;performance},
doi={10.1109/BigData.2015.7364058},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8748855,
author={Sehgal, Shallu and Agarwal, Manisha},
booktitle={2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)},
title={Analogous Examination of Various Machine Learning Algorithm Applied to Big Data},
year={2018},
volume={},
number={},
pages={121-123},
abstract={All domains from criminal justice to real estate to health care have adopted Big Data analytics forreaping multifold benefits. Prime need of business is actionable data. Big Data analysis business results and suggests future plans. It's application ranges from tracking everything from crime to weather to shopping to brands. Distinguishing factor here is Big Data's capacity for dealing with vast quantities of real-time unstructured data. Big Data Analytics along with machine learning helps in making big impact on service quality and customer satisfaction. In this work we explore the machine learning models for estimating the absolute quality of a model. On the basis of these properties, a candid evaluation of these models brings out the relative merits of all the models.},
keywords={Big Data;Machine learning;Support vector machines;Business;Machine learning algorithms;Meteorology;Training;Big data;machine learning;classification;support vector machine;K nearest neighbor;Random forest},
doi={10.1109/ICACCCN.2018.8748855},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7991657,
author={Alguliyev, Rasim and Aliguliyev, Ramiz and Bagirov, Adil and Karimov, Rafael},
booktitle={2016 IEEE 10th International Conference on Application of Information and Communication Technologies (AICT)},
title={Batch clustering algorithm for big data sets},
year={2016},
volume={},
number={},
pages={1-4},
abstract={Vast spread of computing technologies has led to abundance of large data sets. Today tech companies like, Google, Facebook, Twitter and Amazon handle big data sets and log terabytes, if not petabytes, of data per day. Thus, there is a need to find similarities and define groupings among the elements of these big data sets. One of the ways to find these similarities is data clustering. Currently, there exist several data clustering algorithms which differ by their application area and efficiency. Increase in computational power and algorithmic improvements have reduced the time for clustering of big data sets. But it usually happens that big data sets can't be processed whole due to hardware and computational restrictions. In this paper, the classic k-means clustering algorithm is compared to the proposed batch clustering (BC) algorithm for the required computation time and objective function. The BC algorithm is designed to cluster large data sets in batches but maintain the efficiency and quality. Several experiments confirm that batch clustering algorithm for big data sets is more efficient in using computational power, data storage and results in better clustering compared to k-means algorithm. The experiments are conducted with the data set of 2 (two) million two-dimensional data points.},
keywords={Clustering algorithms;Algorithm design and analysis;Big Data;Classification algorithms;Partitioning algorithms;Linear programming;Machine learning algorithms;Big Data;Big Data Clustering;Clustering Algorithms;k-means;Batch Clustering},
doi={10.1109/ICAICT.2016.7991657},
ISSN={2472-8586},
month={Oct},}
@INPROCEEDINGS{9006604,
author={Zhao, Peijiang and Zettsu, Koji},
booktitle={2019 IEEE International Conference on Big Data (Big Data)},
title={Decoder Transfer Learning for Predicting Personal Exposure to Air Pollution},
year={2019},
volume={},
number={},
pages={5620-5629},
abstract={Personal air quality is an important indicator when assessing the impact of air pollution on personal health. Because personal air quality data are collected manually, it difficult to collect such data in large quantities. The main challenge facing personal air quality predictions is building an effective prediction model with a small amount of training data. Moreover, public atmospheric monitoring stations in urban areas have collected large quantities of air quality data. Therefore, we focus on using atmospheric monitoring data with a transfer-learning method to predict personal air quality. In this paper, we design a transferlearning framework based on an encoder-decoder structure. This transfer-learning framework uses the Wasserstein distance to match the heterogeneous distribution of the source domain (the data from the atmospheric monitoring stations) and the target domain (the personal air quality); we refer to this as decoder transfer learning (DTL). We use data from public atmospheric monitoring stations, collected by the Atmospheric Environmental Regional Observation System (AEROS) of Japan, as the source domain dataset and private datasets collected in Fujisawa, Japan, and Tokyo, Japan, as the target domain datasets to evaluate this approach. The experimental results demonstrate that compared with the inverse distance weighting (IDW), IDW with linear regression, and typical transfer-learning models, the proposed DTL framework demonstrates a significant improvement in prediction performance.},
keywords={Atmospheric modeling;Air quality;Monitoring;Predictive models;Data models;Decoding;Gallium nitride;Transfer Learning;Personal Air Quality;Private Dataset;Heterogeneous Data;Wasserstein Distance},
doi={10.1109/BigData47090.2019.9006604},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9524042,
author={Lu, Runbang},
booktitle={2020 International Conference on Robots & Intelligent System (ICRIS)},
title={Computer E-commerce Security System Under the Background of Big Data},
year={2020},
volume={},
number={},
pages={409-412},
abstract={In order to solve various problems of the current computer e-commerce platform, such as low security, poor interaction, low quality of service personnel, low level of information technology, this paper proposes a new computer e-commerce security system under the background of big data. The system combines big data technology, gives full play to the function of mining information and collecting information of big data technology, strengthens the combination of big data technology and Internet technology, realizes the interaction between buyers and sellers, and improves the satisfaction of users. Based on this, the system can also make full use of big data technology to solve the security problem of e-commerce platform, ensure the security of users' personal information, and reduce the risk of transaction activities. The experimental results show that the system can solve most of the current computer e-commerce platform problems, such as low security, poor interaction, and so on, so as to provide a good trading environment for buyers and sellers.},
keywords={Quality of service;Big Data;Network security;Regulation;Internet;Security;Personnel;Big data;Computer;E-commerce;Information security},
doi={10.1109/ICRIS52159.2020.00106},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8472999,
author={Desai, Palak V.},
booktitle={2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT)},
title={A survey on big data applications and challenges},
year={2018},
volume={},
number={},
pages={737-740},
abstract={Big data defines huge, diverse and fast growing data which requires new technologies to handle. With the rapid growth of data, big data has brought attention of researchers to use it in most prominent way for decision making in various emerging applications. These huge data is extremely useful and valuable for scientific exploration, increase productivity in business and improvement in mankind. It helps from public sector to business activities, healthcare to better navigation, smart cities to national security. Though, with large opportunities to work, the challenges are to handle these data is also increased. In this paper basic of big data with its application and challenges have been discussed. These challenges are also inherent from verity, volume and velocity of data. However if we can manage this issues related to big data then there will be potential improvement in quality of our lives.},
keywords={Big Data;Data visualization;Social network services;Medical services;Decision making;Machine learning algorithms;Big data;Challenges;Big data applications},
doi={10.1109/ICICCT.2018.8472999},
ISSN={},
month={April},}
@INPROCEEDINGS{8836982,
author={Islam Sarker, Md Nazirul and Wu, Min and Chanthamith, Bouasone and Yusufzada, Shaheen and Li, Dan and Zhang, Jie},
booktitle={2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD)},
title={Big Data Driven Smart Agriculture: Pathway for Sustainable Development},
year={2019},
volume={},
number={},
pages={60-65},
abstract={Increasing agricultural production is top most solution in the face of rapid population growth through digitalization of agriculture by using most developed technology like big data. There is a long debate on the application of big data in agriculture. This study is an attempt to explore the suitability of the big data technologies for increasing production and improving quality in agriculture. The study uses an extensive review of current research works and studies in agriculture for exploring the best and compatible practices which can help farmers at field level for increasing production and improving quality. This study reveals a number of available big data technologies and practices in agriculture for solving the current problems and challenges at field level. A conceptual model is developed for proper implementation of available big data technologies at farmer's field level. The study highlights data generation procedure, availability of technology, availability of hardware, software, data collection techniques, method of analysis and suitability of application of big data technologies for smart agriculture. The article explores that there are still some challenges exists in this field as a new domain in agriculture like privacy of data, data quality, availability, initial investment, infrastructure and related expertise. The study suggests that government initiatives, public-private partnership, openness of data, financial investment and regional basis research work are necessary for implementing the big data technologies in agriculture at large scale.},
keywords={Agriculture;Big Data;Production;Sensors;Systematics;Sociology;Statistics;big data;smart agriculture;data driven;precision agriculture;smart farming},
doi={10.1109/ICAIBD.2019.8836982},
ISSN={},
month={May},}
@INPROCEEDINGS{8785536,
author={Auer, Florian and Felderer, Michael},
booktitle={2019 IEEE/ACM 4th International Workshop on Metamorphic Testing (MET)},
title={Addressing Data Quality Problems with Metamorphic Data Relations},
year={2019},
volume={},
number={},
pages={76-83},
abstract={In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.},
keywords={Big Data;Data integrity;Testing;Encyclopedias;Internet;Electronic publishing;metamorphic testing, data quality, big data, quality assessment, metamorphic data relations},
doi={10.1109/MET.2019.00019},
ISSN={},
month={May},}
@INPROCEEDINGS{9352886,
author={Qi, Cui and Mingyue, Sun and Na, Mi and Honggang, Wang and Yanhong, Jian and Jing, Zhu},
booktitle={2020 IEEE 3rd International Conference on Electronics and Communication Engineering (ICECE)},
title={Regional Electricity Sales Forecasting Research Based on Big Data Application Service Platform},
year={2020},
volume={},
number={},
pages={229-233},
abstract={Regional monthly electricity sales forecast is an important basis for regional power grid planning and construction, evaluation of regional economic development and operation, and protection of residents' lives. It is also an important work of regional power regulation and management, decision-making of power generation and purchase, improvement of power supply equipment utilization rate and deepening of power system reform. Based on the current situation of power supply enterprise information development, distribution network business status and characteristics, this paper analyzes the factors affecting electricity sales. According to the characteristics of annual changes in electricity sales and data quality factors, the recurrent neural network model is selected based on the big data application service platform. The long short term memory neural network model performs multi-step multivariate prediction on time series, and uses the attention mechanism to combine two independent models for prediction. Experiments conducted on the historical electricity sales data set of a power supply company show that compared with traditional machine learning methods, this method has advantages in accuracy and efficiency.},
keywords={Recurrent neural networks;Power supplies;Time series analysis;Predictive models;Big Data applications;Prediction algorithms;Data models;Recurrent neural network;Long Short-Term Memory neural network;regional monthly electricity sales;electricity sales forecast;big data application service platform},
doi={10.1109/ICECE51594.2020.9352886},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7321725,
author={Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay},
booktitle={2014 IEEE/ACM International Symposium on Big Data Computing},
title={A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning},
year={2014},
volume={},
number={},
pages={16-25},
abstract={In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.},
keywords={Big data;Engines;Bayes methods;Partitioning algorithms;Accuracy;Algorithm design and analysis;Distributed databases;Big Data;Bayesian network;Distributed computing;Ensemble learning;Scientific workflow;Kepler;Hadoop},
doi={10.1109/BDC.2014.10},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8046998,
author={Yao, Xiaochuang and Yang, Jianyu and Li, Lin and Yun, Wenju and Zhao, Zuliang and Ye, Sijing and Zhu, Dehai},
booktitle={2017 6th International Conference on Agro-Geoinformatics},
title={LandQv1: A GIS cluster-based management information system for arable land quality big data},
year={2017},
volume={},
number={},
pages={1-6},
abstract={In the era of spatial big data, geographic information system (GIS) faces many opportunities and challenges. The first challenge for future GIS is how to store and manage the spatial big data efficiently. For example, in 2013, the volume of Chinese arable land quality (ALQ) dataset is up to 2.51TB with ESRI Shapefile format, and traditional GIS development pattern with standalone version is not meeting the needs including storage, query, analysis and visualization. To solve above problems, in this paper, we present a system framework, LandQv1, based on the GIS cluster to support arable land quality big data management and analysis in geospatial domain. Firstly, it describes the design of the system architecture with three layers in details, and implemented by different technologies accordingly. Secondly, three models, data storage model, service release model, and data calling model, are developed to solve the key problems of each layer in the system framework. And then, LandQv1 is developed with the WPF, GIS cluster, Oracle database and C# language. Finally, through application and system test, the results show that LandQv1 with GIS map tools, data query and other functions can be meted the needs in high performance, which will lay the foundation for arable land big data analyzing in the future.},
keywords={Spatial databases;Big Data;Servers;Data models;Geographic information systems;Memory;Government;Big data;GIS cluster;Arable land quality (ALQ);MIS},
doi={10.1109/Agro-Geoinformatics.2017.8046998},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6816764,
author={Saha, Barna and Srivastava, Divesh},
booktitle={2014 IEEE 30th International Conference on Data Engineering},
title={Data quality: The other face of Big Data},
year={2014},
volume={},
number={},
pages={1294-1297},
abstract={In our Big Data era, data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Recent studies have shown that poor quality data is prevalent in large databases and on the Web. Since poor quality data can have serious consequences on the results of data analyses, the importance of veracity, the fourth `V' of big data is increasingly being recognized. In this tutorial, we highlight the substantial challenges that the first three `V's, volume, velocity and variety, bring to dealing with veracity in big data. Due to the sheer volume and velocity of data, one needs to understand and (possibly) repair erroneous data in a scalable and timely manner. With the variety of data, often from a diversity of sources, data quality rules cannot be specified a priori; one needs to let the “data to speak for itself” in order to discover the semantics of the data. This tutorial presents recent results that are relevant to big data quality management, focusing on the two major dimensions of (i) discovering quality issues from the data itself, and (ii) trading-off accuracy vs efficiency, and identifies a range of open problems for the community.},
keywords={Information management;Data handling;Data storage systems;Databases;Maintenance engineering;Quality management;Cleaning},
doi={10.1109/ICDE.2014.6816764},
ISSN={2375-026X},
month={March},}
@INPROCEEDINGS{9378313,
author={Mahoney, Christian J. and Jensen, Katie and Wei, Fusheng and Zhao, Haozhen and Qin, Han and Ye, Shi},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Application of Deep Learning in Recognizing Bates Numbers and Confidentiality Stamping from Images},
year={2020},
volume={},
number={},
pages={2127-2130},
abstract={In eDiscovery, it is critical to ensure that each page produced in legal proceedings conforms with the requirements of court or government agency production requests. Errors in productions could have severe consequences in a case, putting a party in an adverse position. The volume of pages produced continues to increase, and tremendous time and effort has been taken to ensure quality control of document productions. This has historically been a manual and laborious process. This paper demonstrates a novel automated production quality control application which leverages deep learning-based image recognition technology to extract Bates Number and Confidentiality Stamping from legal case production images and validate their correctness. Effectiveness of the method is verified with an experiment using a real-world production data.},
keywords={Image recognition;Law;Government;Production;Quality control;Manuals;Big Data;eDiscovery;Bates Number;OCR;Confidentiality;legal document production;deep learning;information extraction;image},
doi={10.1109/BigData50022.2020.9378313},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7207252,
author={Abusharekh, Ashraf and Stewart, Samuel A. and Hashemian, Nima and Abidi, Syed Sibte Raza},
booktitle={2015 IEEE International Congress on Big Data},
title={H-DRIVE: A Big Health Data Analytics Platform for Evidence-Informed Decision Making},
year={2015},
volume={},
number={},
pages={416-423},
abstract={Healthcare operations generates large volumes of data. Big data analytics methods are needed to derive actionable and decision-quality 'intelligence' from 'big' healthcare data in order to improve patient care. Given the technical challenges to big health data analytics, in this paper we present a specialized health analytics platform -- H-DRIVE (Health Data Reconciliation Inferencing and Visualization Environment). H-DRIVE is an integrated, end-to-end health data analytics service-oriented workbench designed to empower data analysts and researchers to design analytical experiments and then perform complex analytics on their health data. We present the high-level functional and technical architecture of H-DRIVE. As a case study, we demonstrate the application of H-DRIVE in the context of optimizing the operations of a provincial pathology lab, where we analyze province-wide lab orders to prepare scorecards outlining physician lab testing performance and offer an operational dashboard to provide an overview of lab utilization.},
keywords={Data visualization;Data analysis;Medical services;Semantics;Terminology;Big data;Standards;Health data analytics;Situation awareness},
doi={10.1109/BigDataCongress.2015.68},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{8814579,
author={Haja, Dávid and Vass, Balázs and Toka, László},
booktitle={2019 IEEE 12th International Conference on Cloud Computing (CLOUD)},
title={Improving Big Data Application Performance in Edge-Cloud Systems},
year={2019},
volume={},
number={},
pages={187-189},
abstract={Data analysis is widely used in all domains of the economy. While the amount of data to process grows, the time criteria and the resource consumption constraints get stricter. These phenomena call for advanced resource orchestration for the big data applications. The challenge is actually even greater at the advent of edge computing: orchestration of big data resources in a hybrid edge-cloud infrastructure is challenging. The difficulty stems from the fact that wide-area networking and all its well-known issues come into play and affect the performance of the application. In this paper we present the steps we made towards network-aware big data application design over such distributed systems. We propose a HDFS block placement algorithm for the network reliability problem we identify in geographically distributed topologies. The heuristic algorithm we propose provides better big data application performance compared to the default block placement method. We implement our solution in our simulation environment and show the improved quality of big data applications.},
keywords={Reliability;Servers;Data centers;Topology;Big Data applications;Cloud computing;Network topology;big data;edge;reliability;HDFS},
doi={10.1109/CLOUD.2019.00039},
ISSN={2159-6190},
month={July},}
@INPROCEEDINGS{8367644,
author={Xiaorong, Feng and Shizhun, Jia and Songtao, Mai},
booktitle={2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA)},
title={The research on industrial big data information security risks},
year={2018},
volume={},
number={},
pages={19-23},
abstract={With the industrial data digitized and across the enterprise boundaries and even across borders, industrial big data information security issues become a heated discussion. Due to the lack of mature laws and regulations, criterion and technical achievements, the industrial big data faces an increasingly severe security situation in the initial stage. Therefore it has important strategic significance for industrial big data information security research. In the view of characteristic analysis on security risks, the paper focus on relative security factors existing on life cycle of data acquisition, storage, transmission, decision making and control phases; it illustrates assessment and identification methods on industrial big data information security risk and put forward corresponding measurements and suggestions on information security risk aversion, so as to against potential security threats hidden in industrial systems and make better use of the value of industrial big data.},
keywords={Big Data;Industrial control;Industries;Data models;information security risk;industrial big data;life cycle;characteristic;risk assessment},
doi={10.1109/ICBDA.2018.8367644},
ISSN={},
month={March},}
@INPROCEEDINGS{7723688,
author={Yan, Ke and You, Xiaoming and Ji, Xiaobo and Yin, Guangqiang and Yang, Fan},
booktitle={2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)},
title={A Hybrid Outlier Detection Method for Health Care Big Data},
year={2016},
volume={},
number={},
pages={157-162},
abstract={Technology advancements in health care informatics, digitalizing health records, and telemedicine has resulted in rapid growth of health care data. One challenge is how to effectively discover useful and important information out of such massive amount of data through techniques such as data mining. Outlier detection is a typical technique used in many fields to analyze big data. However, for the large scale and high-dimensional heath care data, the conventional outlier detection methods are not efficient. This paper proposes a novel hybrid outlier detection method, namely, Pruning-based K-Nearest Neighbor (PB-KNN), which integrates the density-based, cluster-based methods and KNN algorithm to conduct effective outlier detection. The proposed PB-KNN adopts the case classification quality character (CCQC) as the medical quality evaluation model and uses the attribute overlapping rate (AOR) algorithm for data classification and dimensionality reduction. To evaluate the performance of the pruning operations in PB-KNN, we conduct extensive experiments. The experiment results show that the PB-KNN method outperforms the k-nearest neighbor (KNN) and local outlier factor (LOF) in terms of the accuracy and efficiency.},
keywords={Medical services;Medical diagnostic imaging;Classification algorithms;Big data;Clustering algorithms;Algorithm design and analysis;Detection algorithms;K-Nearest Neighbor;pruning;health care;outlier detection;attribute overlapping rate;case classification quality character;big data},
doi={10.1109/BDCloud-SocialCom-SustainCom.2016.34},
ISSN={},
month={Oct},}
@ARTICLE{8302840,
author={Kaur, Devinder and Aujla, Gagangeet Singh and Kumar, Neeraj and Zomaya, Albert Y. and Perera, Charith and Ranjan, Rajiv},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={Tensor-Based Big Data Management Scheme for Dimensionality Reduction Problem in Smart Grid Systems: SDN Perspective},
year={2018},
volume={30},
number={10},
pages={1985-1998},
abstract={Smart grid (SG) is an integration of traditional power grid with advanced information and communication infrastructure for bidirectional energy flow between grid and end users. A huge amount of data is being generated by various smart devices deployed in SG systems. Such a massive data generation from various smart devices in SG systems may lead to various challenges for the networking infrastructure deployed between users and the grid. Hence, an efficient data transmission technique is required for providing desired QoS to the end users in this environment. Generally, the data generated by smart devices in SG has high dimensions in the form of multiple heterogeneous attributes, values of which are changed with time. The high dimensions of data may affect the performance of most of the designed solutions in this environment. Most of the existing schemes reported in the literature have complex operations for the data dimensionality reduction problem which may deteriorate the performance of any implemented solution for this problem. To address these challenges, in this paper, a tensor-based big data management scheme is proposed for dimensionality reduction problem of big data generated from various smart devices. In the proposed scheme, first the Frobenius norm is applied on high-order-tensors (used for data representation) to minimize the reconstruction error of the reduced tensors. Then, an empirical probability-based control algorithm is designed to estimate an optimal path to forward the reduced data using software-defined networks for minimization of the network load and effective bandwidth utilization. The proposed scheme minimizes the transmission delay incurred during the movement of the dimensionally reduced data between different nodes. The efficacy of the proposed scheme has been evaluated using extensive simulations carried out on the data traces using `R' programming and Matlab. The big data traces considered for evaluation consist of more than two million entries (2,075,259) collected at one minute sampling rate having hetrogenous features such as-voltage, energy, frequency, electric signals, etc. Moreover, a comparative study for different data traces and a real SG testbed is also presented to prove the efficacy of the proposed scheme. The results obtained depict the effectiveness of the proposed scheme with respect to the parameters such asnetwork delay, accuracy, and throughput.},
keywords={Big Data;Smart devices;Tensile stress;Data models;Throughput;Electronic mail;Proposals;Big data;dimensionality reduction;flow table management;smart grid;software-defined networks;tensors},
doi={10.1109/TKDE.2018.2809747},
ISSN={1558-2191},
month={Oct},}
@INPROCEEDINGS{8531354,
author={Liu, Weishang and Liu, Jinde},
booktitle={2018 International Conference on Virtual Reality and Intelligent Systems (ICVRIS)},
title={A Novel Artwork Design Method Based on Big Data Technology},
year={2018},
volume={},
number={},
pages={79-82},
abstract={How to fully the big data technology to improve the performance of artwork design is of great importance in computer vision. In this paper, we propose a novel artwork design method based on big data technology. Firstly, we provide an overview of the big data technology, which is made up of data acquisition and cloud application technology, data processing and distributed processing technology, data storage and storage technology, and data formation results and perception technology. Secondly, we discuss how to design the artwork product using the big data technology, and the proposed method is implemented by surface deformation, which is solved by an optimization problem. Finally, some examples are given to show that the proposed method can achieve realistic effects and can fully exploit the big data technology and computer vision technology to generate high quality artwork design.},
keywords={Big Data;Cloud computing;Art;Strain;Distributed databases;Mathematical model;artwork design, big data, surface modeling, surface deformation},
doi={10.1109/ICVRIS.2018.00027},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8243651,
author={Li, Xiaolei and Tu, Zhenyu and Jia, Quanchao and Man, Xinjiang and Wang, Hui and Zhang, Xiuli},
booktitle={2017 Chinese Automation Congress (CAC)},
title={Deep-level quality management based on big data analytics with case study},
year={2017},
volume={},
number={},
pages={4921-4926},
abstract={The Big data analytics gives new chances to the enterprises to enhance their management and manufacturing levels. A solution with case study is proposed to accomplish deep-level quality management based on big data analytics. First, the implementation of big data analytics based on industrial process data is illustrated with case study illustration. Through the analysis and feature extraction of off-line data, the corresponding reference model library is constructed, which can be used for real-time processing of unlabeled data in the industrial field. The cluster, outlier and other data indicators we can get from the calculation and analysis would have a certain guiding significance for the enterprise's deep-level quality management. Then, the distributed memory computing engine based on Spark, and the implementation of web application platform based on Spring MVC framework is also described. This can help to get higher performance and unrestricted abilities for data analytics. Meanwhile, good visibility and human-data interface can be achieved.},
keywords={Big Data;Production;Quality management;Clustering algorithms;Sparks;Data models;Engines;Big data analytics;feature extraction;Spark;Spring MVC},
doi={10.1109/CAC.2017.8243651},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7840878,
author={Vakali, Athena and Korosoglou, Paschalis and Daoglou, Pavlos},
booktitle={2016 IEEE International Conference on Big Data (Big Data)},
title={A multi-layer software architecture framework for adaptive real-time analytics},
year={2016},
volume={},
number={},
pages={2425-2430},
abstract={Highly distributed applications dominate today's software industry posing new challenges for novel software architectures capable of supporting real time processing and analytics. The proposed framework, so called REAXICS, is motivated by the fact that the demand for aggregating current and past big data streams requires new software methodologies, platforms and services. The proposed framework is designed to tackle with data intensive problems in real time environments, via services built dynamically under a fully scalable and elastic Lambda based architecture. REAXICS proposes a multi-layer software platform, based on the lambda architecture paradigm, for aggregating and synchronizing real time and batch processing. The proposed software layers and adaptive components support quality of experience, along with community driven software development. Flexibility and elasticity are targeted by hiding the complexity of bootstrapping and maintaining a multi level architecture, upon which the end user can drive queries over input data streams. REAXICS proposes a flexible and extensible software architecture that can capture users preference at the front-end and adapt the appropriate distributed technologies and processes at the back-end. Such a model enables real time analytics in large-scale data driven cloud-based systems.},
keywords={Real-time systems;Big data;Computer architecture;Software;Software architecture;Data models;Optimization;software architectures;real time data management;big data analytics;cloud based services},
doi={10.1109/BigData.2016.7840878},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9377900,
author={Tawakuli, Amal and Kaiser, Daniel and Engel, Thomas},
booktitle={2020 IEEE International Conference on Big Data (Big Data)},
title={Synchronized Preprocessing of Sensor Data},
year={2020},
volume={},
number={},
pages={3522-3531},
abstract={Sensor data whether collected for machine learning, deep learning or other applications must be preprocessed to fit input requirements or improve performance and accuracy. Data preparation is an expensive, resource consuming and complex phase often performed centrally on raw data for a specific application. The dataflow between the edge and the cloud can be enhanced in terms of efficiency, reliability and lineage by preprocessing the datasets closer to their data sources. We propose a dedicated data preprocessing framework that distributes preprocessing tasks between a cloud stage and two edge stages to create a dataflow with progressively improving quality. The framework handles heterogenous data and dynamic preprocessing plans simultaneously targeting diverse applications and use cases from different domains. Each stage autonomously executes sensor specific preprocessing plans in parallel while synchronizing the progressive execution and dynamic updates of the preprocessing plans with the other stages. Our approach minimizes the workload on central infrastructures and reduces the resources used for transferring raw data from the edge. We also demonstrate that preprocessing data can be sensor specific rather than application specific and thus can be performed prior to knowing a specific application.},
keywords={Deep learning;Cloud computing;Data preprocessing;Big Data;Synchronization;Reliability;Task analysis;Data Quality;Data Preprocessing;Sensor Data;Edge Computing;Data Management},
doi={10.1109/BigData50022.2020.9377900},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7347061,
author={Wenlu Yang and Da Silva, Alzennyr and Picard, Marie-Luce},
booktitle={2015 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM)},
title={Computing data quality indicators on Big Data streams using a CEP},
year={2015},
volume={},
number={},
pages={1-5},
abstract={Big Data is often referred to as the 3Vs: Volume, Velocity and Variety. A 4th V (validity) was introduced to address the quality dimension. Poor data quality can be costly, lead to breaks in processes and invalidate the company's efforts on regulatory compliance. In order to process data streams in real time, a new technology called CEP (complex event processing) was developed. In France, the current deployment of smart meters will generate massive electricity consumption data. In this work, we developed a diagnostic approach to compute generic quality indicators of smart meter data streams on the fly. This solution is based on Tibco StreamBase CEP. Visualization tools were also developed in order to give a better understanding of the inter-relation between quality issues and geographical/temporal dimensions. According to the application purpose, two visualization methods can be loaded: (1) StreamBase LiveView is used to visualize quality indicators in real time; and (2) a Web application provides a posteriori and geographical analysis of the quality indicators which are plotted on a map within a color scale (lighter colors indicate good quality and darker colors indicate poor quality). In future works, new quality indicators could be added to the solution which can be applied in an operational context in order to monitor data quality from smart meters.},
keywords={Smart meters;Data visualization;Smart grids;Real-time systems;Image color analysis;Indexes;Data quality;Big Data;data stream;CEP;smart grids},
doi={10.1109/IWCIM.2015.7347061},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7363879,
author={Deolalikar, Vinay},
booktitle={2015 IEEE International Conference on Big Data (Big Data)},
title={How valuable is your data? A quantitative approach using data mining},
year={2015},
volume={},
number={},
pages={1248-1253},
abstract={Unstructured textual data has grown rapidly in the past two decades in various domains like enterprises, web, scientific, etc. A question that arises naturally when there is such a surfeit of data is: how valuable is a certain piece of data as compared to another? In an enterprise, the answer to this question would determine how valuable said data is to the enterprise. In this paper, we build a framework using data mining that quantifies the value of data. We first identify a specific notion of "value" that is motivated by applications in Enterprise unstructured Information Management (EIM). Namely, we posit that for several applications in EIM, the value of unstructured data is determined by the associations it captures between concepts. The more such associations in data, the more valuable it is. Next, we build a framework using data mining that "counts" the number of associations in data. Our framework uses clustering and frequent itemsets. It also normalizes for data size. We demonstrate our approach on two of the most widely used text benchmark datasets: Reuters and 20 Newsgroups. Our general intuition is that a corpus of professionally written news articles are more valuable (in the sense of capturing more associations between concepts) than newsgroup postings of variable quality written by non-experts. Our quantitative approach indeed reaches the same inference.},
keywords={Itemsets;Data mining;Big data;Benchmark testing;Customer relationship management;Semantics;Context},
doi={10.1109/BigData.2015.7363879},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7870183,
author={Clarke, Roger},
booktitle={2016 European Intelligence and Security Informatics Conference (EISIC)},
title={Quality Assurance for Security Applications of Big Data},
year={2016},
volume={},
number={},
pages={1-8},
abstract={The quality of inferences drawn from data, big or small, is heavily dependent on the quality of the data and the quality of the processes applied to it. Big data analytics is emerging from laboratories and being applied to intelligence and security needs. To achieve confidence in the outcomes of these applications, a quality assurance framework is needed. This paper outlines the challenges, and draws attention to the consequences of misconceived and misapplied projects. It presents key aspects of the necessary risk assessment and risk management approaches, and suggests opportunities for research.},
keywords={Big data;Q-factor;Sociology;Statistics;Security;Reliability;Quality assurance;risk assessment;risk management;information quality;data semantics;data scrubbing;decision quality;transparency},
doi={10.1109/EISIC.2016.010},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8231848,
author={Ramos Rojas, Julian A. and Beth Kery, Mary and Rosenthal, Stephanie and Dey, Anind},
booktitle={2017 IEEE 7th Symposium on Large Data Analysis and Visualization (LDAV)},
title={Sampling techniques to improve big data exploration},
year={2017},
volume={},
number={},
pages={26-35},
abstract={The success of Big Data relies fundamentally on the ability of a person (the data scientist) to make sense and generate insights from this wealth of data. The process of generating actionable insights, called data exploration, is a difficult and time-consuming task. Data exploration of a big dataset usually requires first generating a small and representative data sample that can be easily plotted and viewed, managed and interpreted to generate insights. However, the literature on the topic hints at data scientists only using random sampling with regular sized datasets and it is unclear what they do with Big Data. In this work, we first show evidence from a survey that random sampling is the only technique commonly used by data scientists to quickly gain insights from a big dataset despite theoretical and empirical evidence from the active learning community that suggests benefits of using other sampling techniques. Second, to evaluate and demonstrate the benefits of other sampling techniques, we conducted an online study with 34 data scientists. These scientists performed a data exploration task to support a classification goal using data samples from more than 2 million records of editing data from Wikipedia articles, generated using different sampling techniques. The study results demonstrate that sampling techniques other than random sampling can generate insights that help to focus on different characteristics of the data, without compromising quality in a data exploration.},
keywords={Big Data;Data visualization;Data models;Human computer interaction;Data mining;Uncertainty;Visual Knowledge Discovery;Data Filtering;Human-Computer Interaction},
doi={10.1109/LDAV.2017.8231848},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8622989,
author={Micic, Natasha and Neagu, Daniel and Torgunov, Denis and Campean, Felician},
booktitle={2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
title={Exploring Methods for Comparing Similarity of Dimensionally Inconsistent Multivariate Numerical Data},
year={2018},
volume={},
number={},
pages={1528-1535},
abstract={When developing multivariate data classification and clustering methodologies for data mining, it is clear that most literature contributions only really consider data that contain consistently the same attributes. There are however many cases in current big data analytics applications where for same topic and even same source data sets there are differing attributes being measured, for a multitude of reasons (whether the specific design of an experiment or poor data quality and consistency). We define this class of data a dimensionally inconsistent multivariate data, a topic that can be considered a subclass of the Big Data Variety research. This paper explores some classification methodologies commonly used in multivariate classification and clustering tasks and considers how these traditional methodologies could be adapted to compare dimensionally inconsistent data sets. The study focuses on adapting two similarity measures: Robinson-Foulds tree distance metrics and Variation of Information; for comparing clustering of hierarchical cluster algorithms (such clusters are derived from the raw multivariate data). The results from experiments on engineering data highlight that adapting pairwise measures to exclude non-common attributes from the traditional distance metrics may not be the best method of classification. We suggest that more specialised metrics of similarity are required to address challenges presented by dimensionally inconsistent multivariate data, with specific applications for big engineering data analytics.},
keywords={Measurement;Time series analysis;Feature extraction;Phylogeny;Mutual information;Big Data;Data mining;Similarity measures;Robinson Foulds;Variation of Information;Engineering data;Multivariate numerical data;Dimensional inconsistency},
doi={10.1109/HPCC/SmartCity/DSS.2018.00251},
ISSN={},
month={June},}
@INPROCEEDINGS{8070869,
author={Angra, Sheena and Ahuja, Sachin},
booktitle={2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC)},
title={Implementation of data mining algorithms on student's data using rapid miner},
year={2017},
volume={},
number={},
pages={387-391},
abstract={Data mining offers a new advance to data analysis using techniques based on machine learning, together with the conventional methods collectively known as educational data mining (EDM). Educational Data Mining has turned up as an interesting and useful research area for finding methods to improve quality of education and to identify various patterns in educational settings. It is useful in extracting information of students, teachers, courses, administrators from educational institutes such as schools/colleges/universities and helps to suggest interesting learning experiences to various stakeholders. This paper focuses on the applications of data mining in the field of education and implementation of three widely used data mining techniques using Rapid Miner on the data collected through a survey.},
keywords={Handheld computers;Big Data;Computational intelligence;DH-HEMTs;Educational Data Mining;Data Mining;EDM Objectives;Rapid Miner;EDM data and Stakeholders},
doi={10.1109/ICBDACI.2017.8070869},
ISSN={},
month={March},}
@INPROCEEDINGS{8316294,
author={Grulich, Philipp M. and Zukunft, Olaf},
booktitle={2017 International Conference on Big Data Innovations and Applications (Innovate-Data)},
title={Bringing Big Data into the Car: Does it Scale?},
year={2017},
volume={},
number={},
pages={9-16},
abstract={The increasing velocity of big data captured by various sensors and processed in real-time offers support for a range of new application domains. For car information systems (CIS), data from different sources including IoT needs to be combined to offer an adequate service to the user. In this paper, we introduce a novel CIS big data-centric architecture based on a smart streaming infrastructure integrating data source in and outside of the car. We have created a prototype implementation of this architecture and run several experiments to validate the quality of our solution. Especially, we have examined the fault tolerance of the architecture by systematically introducing failures and evaluating their effects on the car information system. The experimental results show that our solution for a smart data based car information system is both scalable and fault tolerant.},
keywords={Automobiles;Sparks;Information systems;Computer architecture;Real-time systems;Fault tolerance},
doi={10.1109/Innovate-Data.2017.14},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9332332,
author={Wang, Xin and Yu, Liling and Zhao, Xinbin},
booktitle={2020 IEEE 3rd International Conference of Safe Production and Informatization (IICSPI)},
title={Discover the Tail Strike Risk during Take-off of an Airline Based on QAR Big Data},
year={2020},
volume={},
number={},
pages={553-556},
abstract={In order to help airlines discover the hidden risks of tail strike during the take-off phase, this article uses the quick access record (QAR) big data of the flight operational quality assurance (FOQA) Station of CAAC, and uses the industry-wide QAR data to compare with the QAR data of individual airline to find out whether an airline has outstanding problems, and uses mathematics Statistical t test to verify whether there is a significant difference. This article analyzes the data from July to December 2019 as an example, and finds that the A321 model of a certain airline has the risk of tail strike, that is, the take-off pitch angle is too big. The t-test of mathematical statistics is used to verify that there is a significant difference between the airline's take-off pitch angle and the industry's take-off pitch angle. In addition, the related speed at rotation, the speed at liftoff, and the average pitch rate are also analyzed, and it is found that they also have significant differences. This method can be further extended to other flight quality analysis to find potential safety hazards.},
keywords={Industries;Atmospheric modeling;Big Data;Hazards;Data models;Mathematical model;Risk management;risk management;tail strike;pitch;QAR data;normal distribution;t test},
doi={10.1109/IICSPI51290.2020.9332332},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7129549,
author={Tang, Nan},
booktitle={2015 31st IEEE International Conference on Data Engineering Workshops},
title={Big RDF data cleaning},
year={2015},
volume={},
number={},
pages={77-79},
abstract={Without a shadow of a doubt, data cleaning has played an important part in the history of data management and data analytics. Possessing high quality data has been proven to be crucial for businesses to do data driven decision making, especially within the information age and the era of big data. Resource Description Framework (RDF) is a standard model for data interchange on the semantic web. However, it is known that RDF data is dirty, since many of them are automatically extracted from the web. In this paper, we will first revisit data quality problems appeared in RDF data. Although many efforts have been put to clean RDF data, unfortunately, most of them are based on laborious manual evaluation. We will also describe possible solutions that shed lights on (semi-)automatically cleaning (big) RDF data.},
keywords={Resource description framework;Cleaning;Ontologies;Data mining;Knowledge based systems;Conferences;Databases},
doi={10.1109/ICDEW.2015.7129549},
ISSN={},
month={April},}
@ARTICLE{8361574,
author={Meng, Qianyu and Wang, Kun and He, Xiaoming and Guo, Minyi},
journal={Big Data Mining and Analytics},
title={QoE-driven big data management in pervasive edge computing environment},
year={2018},
volume={1},
number={3},
pages={222-233},
abstract={In the age of big data, services in the pervasive edge environment are expected to offer end-users better Quality-of-Experience (QoE) than that in a normal edge environment. However, the combined impact of the storage, delivery, and sensors used in various types of edge devices in this environment is producing volumes of high-dimensional big data that are increasingly pervasive and redundant. Therefore, enhancing the QoE has become a major challenge in high-dimensional big data in the pervasive edge computing environment. In this paper, to achieve high QoE, we propose a QoE model for evaluating the qualities of services in the pervasive edge computing environment. The QoE is related to the accuracy of high-dimensional big data and the transmission rate of this accurate data. To realize high accuracy of high-dimensional big data and the transmission of accurate data through out the pervasive edge computing environment, in this study we focused on the following two aspects. First, we formulate the issue as a high-dimensional big data management problem and test different transmission rates to acquire the best QoE. Then, with respect to accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on deep learning, which is suitable for high-dimensional big data analysis in the pervasive edge computing environment. Our simulation results reveal that our proposed algorithm can achieve high QoE performance.},
keywords={Big Data;Quality of experience;Edge computing;Quality of service;Computational modeling;Training;Streaming media;Quality-of-Experience (QoE); high-dimensional big data management; deep learning; pervasive edge},
doi={10.26599/BDMA.2018.9020020},
ISSN={2096-0654},
month={Sep.},}
@INPROCEEDINGS{6984224,
author={Munar, Antoni and Chiner, Esteban and Sales, Ignacio},
booktitle={2014 International Conference on Future Internet of Things and Cloud},
title={A Big Data Financial Information Management Architecture for Global Banking},
year={2014},
volume={},
number={},
pages={385-388},
abstract={Global investment banks and financial institutions are facing growing data processing demands. These originate not only from increasing regulatory requirements and an expanding variety and disparity of data sources, but also from ongoing pressures in cost reduction without compromising system scalability and flexibility. In this context, the ability to apply promising state-of-the-art big data technologies to extract the maximum value from the vast amounts of the data generated is generating a lot of interest in the financial services industry. In this paper we present a Big Data architecture system design, based in open distributed computing paradigms like Hadoop map-reduce, offering horizontal scalability and no-SQL flexibility while at the same time meeting the stringent quality and resilience requirements of the banking software standards. The proposed architecture is able to consolidate, validate, enrich and process with different Big Data analytics techniques the data gathered from the different source systems as encountered in the banking practice, while at the same time supporting the different data integration, transmission and process orchestration requirements traditionally encountered in a global financial institution.},
keywords={Data models;Banking;Reliability;Big data;Distributed databases;financial information;big data analytics;big data architectures;map-reduce;hadoop},
doi={10.1109/FiCloud.2014.68},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6972054,
author={Ludwig, Heiko},
booktitle={2014 IEEE 18th International Enterprise Distributed Object Computing Conference},
title={Managing Big Data Effectively - A Cloud Provider and a Cloud Consumer Perspective},
year={2014},
volume={},
number={},
pages={91-91},
abstract={Summary form only given. Instrumentation of processes and an organization's environment provides vast amounts of data that can be used to drivedecisions. Next to setting up data collection, supervising data quality, and applying proper methods of analysis, organizationsface the challenge to set up an infrastructure and architecture to do so efficiently and cost-effectively. Virtualized platformssuch as private or public clouds are the method of choice for deployment, in particular for data analyses not occurringconstantly. A cloud provider, either a commercial Cloud company or an IT organization within an enterprise, will like to set upa cloud platform such that clients can run big data workloads effectively on. Cloud customers would like to set up big dataapplications in a cost-effective and performant way on their platform.This keynote will walk through a few real life big data analysis scenarios from different industries and discuss thechallenges Cloud providers face making trade-offs. Understanding those challenges and solutions help cloud users choose theright match between their algorithm, big data system and cloud platform.},
keywords={Big data;Cloud computing;Organizations;Face;Conferences;Abstracts;Instruments},
doi={10.1109/EDOC.2014.21},
ISSN={1541-7719},
month={Sep.},}
@INPROCEEDINGS{9255497,
author={Hasibuan, Zainal A.},
booktitle={2020 International Workshop on Big Data and Information Security (IWBIS)},
title={Towards Using Universal Big Data in Artificial Intelligence Research and Development to Gain Meaningful Insights and Automation Systems},
year={2020},
volume={},
number={},
pages={9-18},
abstract={The increasing number of human activities using information and communication technology (ICT) generates a tremendous amount of data. It brings the opportunity to use universal big data for further computation that may deliver new insights and automation system. The technology that enables this work is artificial intelligence (AI). As known, the utilization of AI technology is becoming more and more pervasive in our daily life. Furthermore, discussion related to AI has played an essential role in an organization's decision-making process. Thus, this paper discusses the utilization of AI technology that penetrates end-to-end various aspects of human activities, such as in education, health, business, social life and so forth. The end-to-end process begins with data collection covering various types of data (text, picture, audio, video, animation) and various methods (survey, observation, interview, experiment). Then, it continues with the pre-process data cleansing and process to determine data features, and seeks the relationship within them, using machine learning process with appropriate algorithms. The results may then be used for applications portfolio in order to improve organization strategies and programs. The organization's performance can be visualized from time to time for continuous quality improvement. The end-to-end cycles of processes continue, from data collection, data pre-processing and processing, performance computation monitoring (identification, classification, prediction, and prescription), improving strategy and program, and implementing in the real-life activities until generating more behavioral data, finding pattern and design the systems. As a whole, this end-to-end cycle leads us to an AI automation system, with the power to generate meaningful insights suited to solve current problems, predicting trending issues, and understanding phenomena.},
keywords={Feature extraction;Artificial intelligence;Classification algorithms;Correlation;Automation;Data visualization;Big Data;ICT;artificial intelligence;AI;automation system;big data;finding pattern;machine learning;deep learning;end-to-end cycle processes},
doi={10.1109/IWBIS50925.2020.9255497},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8622613,
author={Rinaldi, Antonio M. and Russo, Cristiano},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={User-centered Information Retrieval using Semantic Multimedia Big Data},
year={2018},
volume={},
number={},
pages={2304-2313},
abstract={The user is a basic component in the whole information management process and for this reason s/he has to be taken into account in the design and implementation of frameworks and systems for information retrieval. For this reason, the problem of defining efficient techniques for knowledge representation in a user's prospective is becoming a challenging topic in both academic and industrial community. Moreover, the large amount of available data induces several problems from different points of view and it is stressed in the bigdata vision. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of user-based information retrieval applications. In this paper we propose the use of a semantic approach to design the structure of a multimedia BigData. In addition, the recognition of multimodal features to represent concepts and its attributes together with linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. Information about users (i.e. position) is also used to recognize his/her behaviour and improve the quality of the information retrieval process. Our framework has been implemented using a NoSQL graphdb populated from very large knowledge sources and mobile technologies. Extended experiments are presented to show the effectiveness of our approach.},
keywords={Semantics;Big Data;Ontologies;Information retrieval;Data models;OWL;Visualization;Semantic BigData;Multimedia Ontologies;Spatial Data},
doi={10.1109/BigData.2018.8622613},
ISSN={},
month={Dec},}
@ARTICLE{9096305,
author={Nazir, Shah and Khan, Sulaiman and Khan, Habib Ullah and Ali, Shaukat and García-Magariño, Iván and Atan, Rodziah Binti and Nawaz, Muhammad},
journal={IEEE Access},
title={A Comprehensive Analysis of Healthcare Big Data Management, Analytics and Scientific Programming},
year={2020},
volume={8},
number={},
pages={95714-95733},
abstract={Healthcare systems are transformed digitally with the help of medical technology, information systems, electronic medical records, wearable and smart devices, and handheld devices. The advancement in the medical big data, along with the availability of new computational models in the field of healthcare, has enabled the caretakers and researchers to extract relevant information and visualize the healthcare big data in a new spectrum. The role of medical big data becomes a challenging task in the form of storage, required information retrieval within a limited time, cost efficient solutions in terms care, and many others. Early decision making based healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Scientific programming play a significant role to overcome the existing issues and future problems involved in the management of large scale data in healthcare, such as by assisting in the processing of huge data volumes, complex system modelling, and sourcing derivations from healthcare data and simulations. Therefore, to address this problem efficiently a detailed study and analysis of the available literature work is required to facilitate the doctors and practitioners for making the decisions in identifying the disease and suggest treatment accordingly. The peer reviewed reputed journals are selected for the accumulated of published research work during the period ranges from 2015 - 2019 (a portion of 2020 is also included). A total of 127 relevant articles (conference papers, journal papers, book section, and survey papers) are selected for the assessment and analysis purposes. The proposed research work organizes and summarizes the existing published research work based on the research questions defined and keywords identified for the search process. This analysis on the existence research work will help the doctors and practitioners to make more authentic decisions, which ultimately will help to use the study as evidence for treating patients and suggest medicines accordingly.},
keywords={Big Data;Diseases;Data mining;Medical diagnostic imaging;Data models;Healthcare;big data;big data management;big data analytics},
doi={10.1109/ACCESS.2020.2995572},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7507493,
author={Rathore, Purva and Shukla, Deepak},
booktitle={2015 International Conference on Communication Networks (ICCN)},
title={Analysis and performance improvement of K-means clustering in big data environment},
year={2015},
volume={},
number={},
pages={43-46},
abstract={The big data environment is used to support the huge amount of data processing. In this environment tons (i.e. Giga bytes, Tera bytes) of data is processed. Therefore the various online applications where the huge data request are generated are treated using the big data i.e. facebook, google. In this presented work the big data environment is studied and investigated how the data is consumed using the big data and how the supporting tools are working with the Hadoop storage. Furthermore, for keen understanding and investigation, a cluster analysis technique more specifically the K-mean clustering algorithm is implemented through the Hadoop and MapReduce. The clustering is a part of big data analytics where the unlabelled data is processed and utilized to make groups of the data. In addition of that it is observed the traditional k-mean algorithm is not much suitably works with the Hadoop and MapReduce thus small amount of modification is performed on the data processing technique. In addition of that during cluster analysis various issues are found in traditional k-means i.e. fluctuating accuracy, outliers and empty cluster. Therefore a new clustering algorithm with modification on traditional approach of k-means clustering is proposed and implemented. That approach first enhances the data quality by removing the outlier points in datasets and then the bi-part method is used to perform the clustering. The proposed clustering technique implemented using the JAVA, Hadoop and MapReduce finally the performance of the proposed clustering approach is evaluated and compared with the traditional k-means clustering algorithm. The obtained performance shows the effective results and enhanced accuracy of cluster formation with the removal of the de-efficiency. Thus the proposed work is adoptable for the big data environment with improving the performance of clustering.},
keywords={Image recognition;Data visualization;Breast;Image segmentation;data mining;clustering;big data;performance improvement;implementation},
doi={10.1109/ICCN.2015.9},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8648789,
author={Lu, Qiming and Zhang, Liang and Sasidharan, Sajith and Wu, Wenji and DeMar, Phil and Guok, Chin and Macauley, John and Monga, Inder and Yu, Se-young and Chen, Jim Hao and Mambretti, Joe and Kim, Jin and Noh, Seo-Young and Yang, Xi and Lehman, Tom and Liu, Gary},
booktitle={2018 IEEE/ACM Innovating the Network for Data-Intensive Science (INDIS)},
title={BigData Express: Toward Schedulable, Predictable, and High-Performance Data Transfer},
year={2018},
volume={},
number={},
pages={75-84},
abstract={Big Data has emerged as a driving force for scientific discoveries. Large scientific instruments (e.g., colliders, and telescopes) generate exponentially increasing volumes of data. To enable scientific discovery, science data must be collected, indexed, archived, shared, and analyzed, typically in a widely distributed, highly collaborative manner. Data transfer is now an essential function for science discoveries, particularly within big data environments. Although significant improvements have been made in the area of bulk data transfer, the currently available data transfer tools and services can not successfully address the high-performance and time-constraint challenges of data transfer required by extreme-scale science applications for the following reasons: disjoint end-to-end data transfer loops, cross-interference between data transfers, and existing data transfer tools and services are oblivious to user requirements (deadline and QoS requirements). Fermilab has been working on the BigData Express project to address these problems. BigData Express seeks to provide a schedulable, predictable, and high-performance data transfer service for big data science. The BigData Express software is being deployed and evaluated at multiple research institutions, which include UMD, StarLight, FNAL, KISTI, KSTAR, SURFnet, Ciena, and other sites. Meanwhile, the BigData Express research team is collaborating with the StarLight International/National Communications Exchange Facility to deploy BigData Express at various research platforms, including Pacific Research Platform, National Research Platform, and Global Research Platform. It is envisioned that we are working toward building a high-performance data transfer federation for big data science.},
keywords={Data transfer;Tools;Task analysis;Big Data;Time factors;Real-time systems;Wide area networks;big-data;high-performance-data-transfer;DTN;SDN;co-scheduling;high-speed-networking;performance},
doi={10.1109/INDIS.2018.00011},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8539254,
author={Hossen, J. and Jesmeen H, M. Z. and Sayeed, Shohel},
booktitle={2018 7th International Conference on Computer and Communication Engineering (ICCCE)},
title={Modifying Cleaning Method in Big Data Analytics Process using Random Forest Classifier},
year={2018},
volume={},
number={},
pages={208-213},
abstract={Accurate data is a key success factor influencing the performance of data analytics results, especially for the detection and prediction purpose. Nowadays, Big Data analytics (BDA) is used to analyze the sheer volume of data available in an organization. These data quality must be maintained in order to obtain correct alert and valuable insights from the rapidly changing data of high volume, velocity, variety, veracity, and value. This paper aim is to modify existing framework of big data analytics by improving an important step in pre-processing (i.e. Data Cleaning). Initially, feature selection based on Random Forest is used to extract effective features. Then, two classifier algorithms (i.e. Random Forest classifier and Linear SVM classifier) are applied to train using the dataset to classify data quality and to develop an intelligent model. In evaluation, our experimental results show a consistent accuracy of Random Forest and Linear Regression around 90%. Using this approach, we expect to provide a set of cleaned data for further processing. Besides, analysts can benefit from this system in data analytical process in cleaning stage and conclude that the data is cleaned. Finally, a comparison is presented between available functions which are used to handle missing values with the developed system.},
keywords={Forestry;Support vector machines;Data models;Cleaning;Feature extraction;Training;Big Data;Big data;Data Analytics;missing data;data cleaning;Machine learning;Random Forest;Gini Index},
doi={10.1109/ICCCE.2018.8539254},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8113071,
author={Yu, Weiqing and Zhu, Wendong and Liu, Guangyi and Kan, Bowen and Zhao, Ting and Liu, He},
booktitle={2017 3rd International Conference on Big Data Computing and Communications (BIGCOM)},
title={Cluster-Based Best Match Scanning for Large-Scale Missing Data Imputation},
year={2017},
volume={},
number={},
pages={232-238},
abstract={High-quality data are the prerequisite for analyzing and using big data to guarantee the value of the data. Missing values in data is a common yet challenging problem in data analytics and data mining, especially in the era of big data. Amount of missing values directly affects the data quality. Therefore, it is critical to properly recover missing values in the dataset. This paper presents a new imputation algorithm called Cluster-based Best Match Scanning (CBMS) designed for Big Data. It is a modification of k-NN imputation. CBMS focuses on recovering continuous numeric missing values, and aims at balancing computational complexity and accuracy. As an imputation algorithm, it can potentially reduce the time complexity of k-NN from O(n^2*d) to O(n^1.5*d), and also reduce the space/memory usage, while perform no worse than k-NN imputation. On top of that CBMS is highly parallelizable.Simulation of CBMS is conducted on smart meter reading data. Data is manually divided into training set and testing set, and testing accuracy is evaluated by computing the mean absolute deviation. Comparison with linear interpolation and k-NN imputation is made to demonstrate the power and effectiveness of our proposed CBMS algorithm.},
keywords={Time complexity;Clustering algorithms;Algorithm design and analysis;Estimation;Big Data;Data mining;big data;cluster-based best match scanning;data imputation;k-NN},
doi={10.1109/BIGCOM.2017.48},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8622576,
author={Portugal, Ivens and Alencar, Paulo and Cowan, Donald},
booktitle={2018 IEEE International Conference on Big Data (Big Data)},
title={A Software Framework for Cluster Lifecycle Analysis in Transportation},
year={2018},
volume={},
number={},
pages={4534-4539},
abstract={Novel forms of data analysis methods have emerged as a significant research direction in the transportation domain. These methods can potentially help to improve our understanding of the dynamic flows of vehicles, people, and goods. Understanding these dynamics has economic and social consequences, which can improve the quality of life locally or worldwide. Aiming at this objective, a significant amount of research has focused on clustering moving objects to address problems in many domains, including the transportation, health and environment. However, previous research has not investigated the lifecycle of a cluster, including cluster genesis, existence, and disappearance. The representation and analysis of cluster lifecycles can create novel avenues for research, result in new insights for analyses, and allow unique forms of prediction. This paper focuses on studying the lifecycle of clusters by investigating the relations that a cluster has with moving elements and other clusters. This paper also proposes a big data framework that manages the identification and processing of a cluster lifecycle. The ongoing research approach will lead to new ways to perform cluster analysis and advance the state of the art by leading to new insights related to cluster lifecycle. These results can have a significant impact on transport industry data science applications in a wide variety of areas, including congestion management, resource optimization, and hotspot management.},
keywords={Trajectory;Big Data;Data analysis;Optimization;Public transportation;Computer science;transportation data;cluster analysis;cluster lifecycle;big data analysis},
doi={10.1109/BigData.2018.8622576},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8859426,
author={He, Tieke and Chen, Shenghao and Hao, Lian and Liu, Jia},
booktitle={2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
title={Quality Driven Judicial Data Governance},
year={2019},
volume={},
number={},
pages={66-70},
abstract={With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.},
keywords={Data integrity;Big Data;Decision making;Organizations;Standards organizations;data quality;judicial data governance;quality measurement},
doi={10.1109/QRS-C.2019.00026},
ISSN={},
month={July},}
@INPROCEEDINGS{8524938,
author={Bataev, Alexey V.},
booktitle={2018 IEEE International Conference "Quality Management, Transport and Information Security, Information Technologies" (IT&QM&IS)},
title={Evaluation of Using Big Data Technologies in Russian Financial Institutions},
year={2018},
volume={},
number={},
pages={573-577},
abstract={Today's development of the world economy cannot be imagined without the implementation of digital technologies that transform entire economic sectors. Digital modernization of the world economy has led to the emergence of a new direction in economic development - the digital economy. It bases on the introduction of the most advanced information and communication technologies. In 2017, the state program "Digital Economy of the Russian Federation" was adopted in Russia. It provides the goals and tasks for implementing the state policy to develop the digital economy in Russia. The introduction and application of technologies for processing and analysis of big data are on the first place in this program. One of the leaders in this area in the Russian Federation is financial institutions. They have accumulated huge amounts of data for many years of their existence that require careful analysis and processing for further use. The paper analyzes modern technologies in the field of big data, reveals the most promising technologies that spread in the Russian market. In addition, the financial evaluation of the Russian big data market was carried out, the dynamics and growth rates were analyzed, future development was determined, the leaders in the application and implementation of these technologies were identified. The introduction and use of big data in Russian financial institutions were analyzed, the financial evaluation of the Russian big data market in the financial sphere was made, and the main directions of application of these technologies in credit institutions are defined. During the research, the reasons that hinder the more mass and rapid introduction of big data in Russian financial institutions were identified and considered.},
keywords={Big Data;Economics;Companies;Globalization;Information and communication technology;Task analysis;information technology management;digitalization of the economy;big data;Russian financial institutions},
doi={10.1109/ITMQIS.2018.8524938},
ISSN={},
month={Sep.},}