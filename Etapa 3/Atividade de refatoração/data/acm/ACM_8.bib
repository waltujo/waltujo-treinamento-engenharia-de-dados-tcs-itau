@inproceedings{10.1145/2983323.2983345,title = {City-Scale Localization with Telco Big Data}, author = {Zhu Fangzhou , Luo Chen , Yuan Mingxuan , Zhu Yijian , Zhang Zhengqing , Gu Tao , Deng Ke , Rao Weixiong , Zeng Jia },year = {2016}, isbn = {9781450340731}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2983323.2983345}, doi = {10.1145/2983323.2983345}, abstract = {It is still challenging in telecommunication (telco) industry to accurately locate mobile devices (MDs) at city-scale using the measurement report (MR) data, which measure parameters of radio signal strengths when MDs connect with base stations (BSs) in telco networks for making/receiving calls or mobile broadband (MBB) services. In this paper, we find that the widely-used location based services (LBSs) have accumulated lots of over-the-top (OTT) global positioning system (GPS) data in telco networks, which can be automatically used as training labels for learning accurate MR-based positioning systems. Benefiting from these telco big data, we deploy a context-aware coarse-to-fine regression (CCR) model in Spark/Hadoop-based telco big data platform for city-scale localization of MDs with two novel contributions. First, we design map-matching and interpolation algorithms to encode contextual information of road networks. Second, we build a two-layer regression model to capture coarse-to-fine contextual features in a short time window for improved localization performance. In our experiments, we collect 108 GPS-associated MR records in the centroid of Shanghai city with 12 x 11 square kilometers for 30 days, and measure four important properties of real-world MR data related to localization errors: stability, sensitivity, uncertainty and missing values. The proposed CCR works well under different properties of MR data and achieves a mean error of 110m and a median error of $80m$, outperforming the state-of-art range-based and fingerprinting localization methods.}, location = {Indianapolis, Indiana, USA}, series = {CIKM '16}, pages = {439\u2013448}, numpages = {10}, keywords = {telco big data, localization, regression models}}
@inproceedings{10.1145/3538950.3538956,title = {Visual Presentation and Trend Derivation of the Medical Big Data Research based on a Multi-level Bibliometric Analysis}, author = {Zhou Wei , Wu Zhaoxia , Xu Yizhen },year = {2022}, isbn = {9781450395632}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3538950.3538956}, doi = {10.1145/3538950.3538956}, abstract = {In the context of the rapid development of Internet technology, medical big data (MBD) research has been broadly noticed for its own advantages. Among the vast amount of MBD research results, despite the bibliometric analysis, it does not provide readers with a comprehensive understanding of the cutting-edge trends and developments in MBD research due to the small amount of data and poor timeliness. Therefore, based on the 3353 publications retrieved from Web of Science between 2002 and 2022, this paper performed quantitative, various kinds of cooperative networks, clustering networks, keywords, frontier trends and main path analysis. The conclusions are as follows. (1) MBD research findings are increasing at an even higher rate. (2) Research contribution differs widely between countries and institutions, and is relatively average between authors and journals. (3) MBD research frontiers show a tendency to span a wide range of disciplines and to be strongly associated with Internet technologies. (4) According to the results of the main path analysis, health IoT and medical big data are the hot spots for MBD research.}, location = {Beijing, China}, series = {BDE '22}, pages = {32\u201341}, numpages = {10}, keywords = {Co-citation analysis, Medical big data, Bibliometric analysis}}
@inproceedings{10.1145/2612733.2612779,title = {Building government's capacity for big data analysis}, author = {Harrison Teresa M. },year = {2014}, isbn = {9781450329019}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2612733.2612779}, doi = {10.1145/2612733.2612779}, abstract = {\"Big data\" has emerged as a compelling topic for e-government practitioners who have the opportunity to develop strategies for exploiting the data streams they currently possess or can access in an effort to make government organizations smarter and more responsive to their constituencies. This panel brings together researchers who are exploring big data analytic capabilities and opportunities in government organizations as well as the technical, organizational and ethical constraints that government practitioners are likely to face.}, location = {Aguascalientes, Mexico}, series = {dg.o '14}, pages = {306\u2013308}, numpages = {3}, keywords = {policy informatics, communication, census data, auditors, social networks, big data, text analytics, Mexico, political science, natural language processing}}
@inproceedings{10.1145/3472163.3472195,title = {Customized Eager-Lazy Data Cleansing for Satisfactory Big Data Veracity}, author = {Sahri Soror , Moussa Rim },year = {2021}, isbn = {9781450389914}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472163.3472195}, doi = {10.1145/3472163.3472195}, abstract = {Big data systems are becoming mainstream for big data management either for batch processing or real-time processing. In order to extract insights from data, quality issues are very important to address, particularly. A veracity assessment model is consequently needed. In this paper, we propose a model which ties quality of datasets and quality of query resultsets. We particularly examine quality issues raised by a given dataset, order attributes along their fitness for use and correlate veracity metrics to business queries. We validate our work using the open dataset NYC taxi\u2019 trips.}, location = {Montreal, QC, Canada}, series = {IDEAS '21}, pages = {157\u2013165}, numpages = {9}, keywords = {Veracity, Big data}}
@inproceedings{10.1145/2351316.2351322,title = {Incrementally optimized decision tree for noisy big data}, author = {Yang Hang , Fong Simon },year = {2012}, isbn = {9781450315470}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2351316.2351322}, doi = {10.1145/2351316.2351322}, abstract = {How to extract meaningful information from big data has been a popular open problem. Decision tree, which has a high degree of knowledge interpretation, has been favored in many real world applications. However noisy values commonly exist in high-speed data streams, e.g. real-time online data feeds that are prone to interference. When processing big data, it is hard to implement pre-processing and sampling in full batches. To solve this tradeoff, this paper proposes a new incremental decision tree algorithm so called incrementally optimized very fast decision tree (iOVFDT). The experiment evaluates the proposed algorithm in comparison to existing methods under noisy data streams environment. Result shows iOVFDT has outperformance on the aspects of higher accuracy and smaller model size.}, location = {Beijing, China}, series = {BigMine '12}, pages = {36\u201344}, numpages = {9}, keywords = {decision tree classification, incremental optimization, data stream mining, optimized very fast decision tree}}
@inproceedings{10.1145/3386415.3386964,title = {Study on Clustering Computing Methods of Big Data}, author = {Chen Lijun , Pan Zhengjun , Yuan Lina },year = {2019}, isbn = {9781450372930}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3386415.3386964}, doi = {10.1145/3386415.3386964}, abstract = {In the past few years, the rapidly developing technology in the field of information technology is \"big data\". Clustering is one of the key tasks in a wide range of areas dealing with large amounts of data. This survey introduces various clustering methods used for effective big data clustering. Therefore, this review paper reviewed 15 research papers, which proposed various methods for effective big data clustering, such as k-means clustering, k-means variant clustering, fuzzy c-means clustering, possibility c-means clustering, collaborative filtering and optimization based clustering. In addition, detailed analysis is carried out by referring to the implementation tools used, the data sets used and the big data clustering framework adopted. Then, an effective solution must be developed to go beyond the existing technology to the special management of big data. Finally, the research problems and gaps of various big data clustering technologies are proposed to enable researchers to start with better big data clustering.}, location = {Zhuzhou, Hunan, China}, series = {ICITEE-2019}, pages = {1\u20135}, numpages = {5}, keywords = {Cluster, C-mean, Big data, Map, K-mean, Reduce}}
@inproceedings{10.1145/3003733.3003767,title = {Signal Processing Techniques Restructure The Big Data Era}, author = {Petrou Charilaos , Paraskevas Michael },year = {2016}, isbn = {9781450347891}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3003733.3003767}, doi = {10.1145/3003733.3003767}, abstract = {Big data science has been developed into a topic that attracts attention from industry, academia and governments. The main objective in Big Data science is to recognize and extract meaningful information from huge amounts of heterogeneous data and unstructured data (which constitute 95% of big data). Signal Processing (SP) techniques and related statistical learning (SL) tools such as Principal Component Analysis (PCA), R-PCA (Robust PCA), Compressive Sampling (CS), convex optimization (CO), stochastic approximation (SA), kernel based learning (KBL) tasks are used for robustness, compression and dimensionality reduction in Big Data arising challenges. This review paper introduces Big Data related SP techniques and presents applications of this emerging field.}, location = {Patras, Greece}, series = {PCI '16}, pages = {1\u20136}, numpages = {6}, keywords = {big data, convex optimization, signal processing techniques, statistical learning tools, stochastic approximation}}
@inproceedings{10.1145/3150919.3150923,title = {Geospatial Big Data Analytics Engine for Spark}, author = {Wang Shaohua , Zhong Yang , Lu Hao , Wang Erqi , Yun Weiying , Cai Wenwen },year = {2017}, isbn = {9781450354943}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3150919.3150923}, doi = {10.1145/3150919.3150923}, abstract = {With the rapid development of geospatial data acquisition and processing technology, the scale of spatial data is expanding. Mass production applications put forward higher requirements for the performance of geospatial data analysis. In this study, we developed a geospatial big data analytics engine based on SuperMap iObject for Java and Apache Spark. The geospatial big data analytics engine can increase the RDD representation ability of spatial data. The spatial indexing can make the spatial calculation on the nodes of the Spark cluster distributed and efficient. The experimental results show that compared with the traditional algorithm, the geospatial big data analytics engine for Spark has better execution efficiency.}, location = {Redondo Beach, CA, USA}, series = {BigSpatial'17}, pages = {42\u201345}, numpages = {4}, keywords = {SuperMap GIS, Cross-platform GIS, Spark, distributed computing, Geospatial big data}}
@inproceedings{10.1145/3512731.3534211,title = {Towards Intellectual Property Rights Protection in Big Data}, author = {Hamza Rafik , Dao Minh-Son , Ito Sadanori , Koji Zettsu },year = {2022}, isbn = {9781450392419}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3512731.3534211}, doi = {10.1145/3512731.3534211}, abstract = {Big Data applications can revolutionize any platform by facilitating the analysis of large amounts of information. However, the biggest challenge associated with Big Data is overcoming the intellectual property barriers associated with the use of this data, especially in cross-database applications. Although intellectual property provisions have been formulated to limit inappropriate use and manage access to Big Data, it is difficult to make this trade-off and overcome the challenges of Big Data. This paper explores the limits of intellectual property rights in Big Data applications. The advent of Big Data requires an alternative conceptual framework along with security policies and regulations. The profound issues of copyright on cross-database platforms are highlighted in this paper, as well as the paradigm shift from ownership to control of access to and use of Big Data, especially on cross-database platforms. We also present a real-world case study of the underlying technologies of cross-data analytics (xdata.nict.jp). The xData platform aims to coordinate competing social, personal, and industrial interests in data to ensure fair access while minimizing legal and ethical threats. Finally, we discuss the idea of using blockchain-enabled smart contracts to protect intellectual property rights on cross-data platforms and highlight some important aspects of copyright issues, highlighting key issues and current open challenges.}, location = {Newark, NJ, USA}, series = {ICDAR '22}, pages = {50\u201357}, numpages = {8}, keywords = {industrial application, big data, security policy, intellectual property}}
@inproceedings{10.1145/3168390.3168425,title = {Measurement Metric Proposed For Big Data Analytics System}, author = {Samosir Ridha Sefina , Hendric Harco Leslie , Gaol Ford Lumban , Abdurachman Edi , Soewito Benfano },year = {2017}, isbn = {9781450353922}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3168390.3168425}, doi = {10.1145/3168390.3168425}, abstract = {Big data is defined as a very large data set (volume), velocity and variety. Big data analytics systems must be supports for parallel processing and large storage. The problem of this research is how to identify measurement metric based on big data analytics system characteristic. One device that support big data platform is Hadoop. Measurement is a process for assigning values or symbols to the attributes of an entity. The purpose of measurement is to distinguish between entities one to another. Indicator for software measurement represented with a metric. The aim of this research is to proposes some measurement metric for big data analytics system. This research using UML exactly a class diagram in system modelling to identify the measurement metric. Both of dynamic and static metric is proposed as solution to measure big data analytics system. Result for this researh are some measurement ndicator both of dynamic and static metric based on class diagram for big data analytics.}, location = {Jakarta, Indonesia}, series = {CSAI 2017}, pages = {265\u2013269}, numpages = {5}, keywords = {Measurement, Software, Metric, Big Data Analytics}}
@inproceedings{10.5555/3192424.3192613,title = {A privacy weaving pipeline for open big data}, author = {Yu Yuan-Chih , Tsai Dwen-Ren },year = {2016}, isbn = {9781509028467}, publisher = {IEEE Press}, abstract = {The power of big data gives us an unprecedented chance to understand, analyze, and recreate the world, while open data ensures that power be shared and widely exploited. Open and big data has become the emerging topics for researchers and governments. Thus, the related privacy issues also become an emerging urgent problem. In this work, we propose a conceptual framework of privacy weaving pipeline dedicated for producing open and big data while preserving privacy. Within the processing pipeline, each step of the process flow considers the privacy assurance to manipulate datasets. However, the complexity of process flow is the same as normal data pipeline. The experimental prototype confirms the feasibility of framework design. We hope this work will facilitate the development of open and big data industry.}, location = {Davis, California}, series = {ASONAM '16}, pages = {997\u2013998}, numpages = {2}, keywords = {privacy breach, open data, data pipeline, big data}}
@inproceedings{10.1145/3239283.3239322,title = {Study on big data framework for product design}, author = {Liu Zhuang , Gong Lin },year = {2018}, isbn = {9781450365215}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3239283.3239322}, doi = {10.1145/3239283.3239322}, abstract = {With the arrival of the intelligent era, Big Data will play an increasingly important role in all aspects of product design. In the product design stage, comprehensive evaluations such as product quality assessment and social demand exploration are directly associated with various data and information. The advantages of big data in data mining and model prediction can be used to provide new ideas for process optimization and problem improvement of design innovation. Based on this situation, this paper proposes a big data framework based on Hadoop for product design. The framework contains the complete flow of big data analysis, and these processes are efficiently connected with good efficiency and practicality. This paper selects the data sets of product quality design and product demand design respectively, mining the design factors in the product design process through data mining technology, thus providing new ideas for product design.}, location = {Singapore, Singapore}, series = {DSIT '18}, pages = {105\u2013110}, numpages = {6}, keywords = {product design, Hadoop, data mining, big data}}
@inproceedings{10.1145/2935753,title = {Veracity of Big Data: Challenges of Cross-Modal Truth Discovery}, author = {Berti-Equille Laure , Ba Mouhamadou Lamine },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2935753}, doi = {10.1145/2935753}, pages = {1\u20133}, numpages = {3}, keywords = {data fusion, data quality, information extraction, Truth discovery, fact checking}}
@inproceedings{10.1145/3322431.3326330,title = {Securing Big Data: New Access Control Challenges and Approaches}, author = {Kantarcioglu Murat },year = {2019}, isbn = {9781450367530}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3322431.3326330}, doi = {10.1145/3322431.3326330}, abstract = {Recent cyber attacks have shown that the leakage/stealing of big data may result in enormous monetary loss and damage to organizational reputation, and increased identity theft risks for individuals. Furthermore, in the age of big data, protecting the security and privacy of stored data is paramount for maintaining public trust, and getting the full value from the collected data. In this talk, we first discuss the unique security and privacy challenges arise due to big data and the NoSQL systems designed to analyze big data. Also we discuss our proposed SecureDL system that is built on top of existing NoSQL databases such as Hadoop and Spark and designed as a data access broker where each request submitted by a user app is automatically captured. These captured requests are logged, analyzed and then modified (if needed) to conform with security and privacy policies (e.g.,[5]), and submitted to underlying NoSQL database. Furthermore, SecureDL can allow organizations to audit their big data usage to prevent data misuse and comply with various privacy regulations[2]. SecureDL is totally transparent from the user point of view and does not require any change to the user's code and/or the underlying NoSQL database systems. Therefore, it can be deployed on existing NoSQL databases.Later on, we discuss how to add additional security layer for protecting big data using encryption techniques (e.g., [1, 3, 4]). Especially, we discuss our work on leveraging the modern hardware based trusted execution environments (TEEs) such as Intel SGX for secure encrypted data processing. We also discuss how to provide a simple, secure and high level language based framework that is suitable for enabling generic data analytics for non-security experts who do not have security concepts such as \"oblivious execution''. Our proposed framework allows data scientists to perform the data analytic tasks with TEEs using a Python/Matlab like high level language; and automatically compiles programs written in our language to optimal execution code by managing issues such as optimal data block sizes for I/O, vectorized computations to simplify much of the data processing, and optimal ordering of operations for certain tasks. Using these design choices, we show how to provide guarantees for efficient and secure big data analytics over encrypted data.}, location = {Toronto ON, Canada}, series = {SACMAT '19}, pages = {1\u20132}, numpages = {2}, keywords = {privacy, nosql databases, access control, intrusion detection, security, encrypted data processing}}
@inproceedings{10.1145/3524383.3533248,title = {Digital Management Capability and Innovation of Manufacturing Enterprises in the Era of Big Data}, author = {Liu Sheng , Hou Liutong , Chen Xiuying , Tang Xuanhe , He Yuzi },year = {2022}, isbn = {9781450395793}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3524383.3533248}, doi = {10.1145/3524383.3533248}, abstract = {With the deep integration of the new generation of information technology and the real economy, what impact will digital management capabilities have on manufacturing companies? Using data from the China Employer-Employee Survey (CEES), this article examines the impact of digital management capabilities on the innovation performance of manufacturing companies. The results show that the enhancement of digital management capabilities can significantly promote the innovation performance of manufacturing enterprises, and this effect is robust. Dimensional regression results show that the availability of data in the decision-making process, the degree of data dependence in the decision-making process, the diversity of corporate data collection entities, and the frequency of use of work process data can significantly promote corporate innovation performance, while the influence of the frequency of data using in the decision-making process and the frequency of data use in the decision-making process and the frequency of using statistical methods to predict on the innovation performance of manufacturing enterprises is not obvious. Sub-sample regression results show that the effect of digital management capabilities on the innovation performance of manufacturing enterprises is heterogeneous. Specifically, when the company is characterized as with a long life span, with a large scale, with the type of family business, with a high proportion of CEOs who owned a college degree or above, with a high number of years of education for middle and senior managers, with a board of directors or a party organization, with a location of the eastern region or in the areas of a high concentration of financial services and R&D services, the effect of digital management capabilities on innovation performance is more obvious. Therefore, this article provides evidence for revealing the microeconomic effects of industrial digital development and promoting the reform of innovation management in manufacturing enterprises.}, location = {Shanghai, China}, series = {ICBDE '22}, pages = {455\u2013460}, numpages = {6}, keywords = {big data, digital management capabilities, innovation performance, manufacturing enterprises, artificial intelligence}}
@inproceedings{10.1145/3301551.3301566,title = {The Implications of Big Data Analytics Orientation upon Deployment}, author = {Hyun Youyung , Hosoya Ryuichi , Kamioka Taro },year = {2018}, isbn = {9781450366298}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3301551.3301566}, doi = {10.1145/3301551.3301566}, abstract = {Big data analytics (BDA) is becoming a key way for leading companies to outperform their peers by utilizing big data and better understanding its business environment. However, there is evidence that many firms are facing difficulties in deploying BDA effectively into their business processes because of data silos, the rigid separation of data across divisions. In order to overcome data silos and capture the full potential of big data, it is considered important to create a corporate culture that encourages communication and sharing of data across departments. Therefore, to identify an appropriate corporate culture that supports BDA deployment in the context of big data, our research introduces BDA orientation which facilitates data flow across separate divisions and values business decisions based on insights gained from BDA. Drawing on resource-based view (RBV) and upper echelon theory, our research model examines the impact of BDA orientation on the deployment of BDA. Also, this study investigates the role of BDA orientation that mediates the effects of top management team initiative and BDA infrastructure on BDA deployment. To test our proposed model, an online survey was administered for a quantitative analysis and data from 166 Japanese upper-level managers were collected. Our findings confirm the significant impact of BDA orientation on BDA deployment, and the mediating role of BDA orientation in the suggested relationships above.}, location = {Hong Kong, Hong Kong}, series = {ICIT 2018}, pages = {42\u201348}, numpages = {7}, keywords = {Big data, big data analytics orientation, BDA deployment}}
@inproceedings{10.1145/3451400.3451414,title = {A Study on College Graduates' Employment Problem in The Context of Big Data Based on the Event of COVID-19}, author = {Ren Weixin },year = {2021}, isbn = {9781450389389}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3451400.3451414}, doi = {10.1145/3451400.3451414}, abstract = {Affected by the outbreak of COVID-19, the employment pressure of college graduates is increasing. The employment problem of graduates is a livelihood issue which is highly valued by social community. The arrival of big data era has a profound impact on the employment situation analysis, policy formulation, employment service and guidance, and college graduates\u2019 career development. Therefore, to explore the impact of the epidemic on the employment situation of graduates, this paper takes college graduates of the University of Electronic Science and Technology of China as the survey object, and uses questionnaires to investigate the employment status of college graduates under the background of the COVID-19. Starting from the three dimensions of the government, universities, and individual college graduates, to analyze the employment status and problems of graduates, and to explore ways to use big data to promote employment of college graduates under the influence of the COVID-19. This is of great significance for alleviating employment pressure and improving the quality of employment.}, location = {London, United Kingdom}, series = {ICBDE 2021}, pages = {88\u201391}, numpages = {4}, keywords = {graduate employment, Big data, COVID-19}}
@inproceedings{10.1145/2938503.2938540,title = {Provenance Support for Biomedical Big Data Analytics}, author = {McClatchey Richard , Branson Andrew , Shamdasani Jetendr },year = {2016}, isbn = {9781450341189}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2938503.2938540}, doi = {10.1145/2938503.2938540}, abstract = {One essential requirement for supporting analytics for Big Medical Data systems is the provision of a suitable level of traceability to data or processes ('Items') in large volumes of data. Systems should be designed from the outset to support usage of such Items across the spectrum of medical use and over time in order to promote traceability, to simplify maintenance and to assist analytics. The philosophy proposed in this paper is to design medical data systems using a 'description-driven' approach in which meta-data and the description of medical items are saved alongside the data, simplifying item re-use over time and thereby enabling the traceability of these items over time and their use in analytics. Details are given of a big data system in neuroimaging to demonstrate aspects of provenance data capture, collaborative analysis and longitudinal information traceability. Evidence is presented that the description-driven approach leads to simplicity of design and ease of maintenance following the adoption of a unified approach to Item management.}, location = {Montreal, QC, Canada}, series = {IDEAS '16}, pages = {386\u2013391}, numpages = {6}, keywords = {Big Data, traceability, medical analytics, provenance data, Description-driven systems}}
@inproceedings{10.1145/2479724.2479764,title = {The complexities of \"big data\": the opportunities and challenges for e-government}, author = {Harrison Teresa M. , Hrdinova Jana },year = {2013}, isbn = {9781450320573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2479724.2479764}, doi = {10.1145/2479724.2479764}, abstract = {In this plenary panel, speakers from academia and government consider the technical, scientific, and organizational implications of Big Data and offer observations about what it means for research and practice related to e-government.}, location = {Quebec, Canada}, series = {dg.o '13}, pages = {263\u2013264}, numpages = {2}, keywords = {social media, data stewardship, data integration, data analytics, security, big data, scientific data management, performance and benchmarking of big data systems, research data alliance, large-scale data systems, privacy}}
@inproceedings{10.1145/3378904,title = {Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology},year = {2020}, isbn = {9781450376839}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {Big data is an emerging paradigm applied to datasets whose size is beyond the ability of commonly used software tools to capture, manage, and process the data within a tolerable elapsed time. Such datasets are often from various sources (Variety) yet unstructured such as social media, sensors, scientific applications, surveillance, video and image archives, Internet texts and documents, Internet search indexing, medical records, business transactions and web logs; and are of large size (Volume) with fast data in/out (Velocity). 2020 2nd International Conference on Big Data Engineering and Technology is one of the premier events to network and learn from colleagues and other leading international scientific voices from across the world, who is actively engaged in advancing research and raising awareness of the many challenges in the diverse field of Big Data Engineering and Technology.}, location = {Singapore, China}}
@inproceedings{10.1145/3503928.3503929,title = {Big Data: Finding Frequencies of Faulty Multimedia Data}, author = {Barzan Abdalla Hemn , Mustafa Nasser , Ihnaini Baha },year = {2021}, isbn = {9781450385220}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3503928.3503929}, doi = {10.1145/3503928.3503929}, abstract = {In many health care domains, big data has arrived. How to manage and use big data better has become the focus of all walks of life. Many data sources provide the repeated fault data\u2014the repeated fault data forming the delay of processing time and storage capacity. Big data includes properties like volume, velocity, variety, variability, value, complexity, and performance put forward more challenges. Most healthcare domains face the problem of testing for structured and unstructured data validation in big data. It provides low-quality data and delays in response. In testing process is delay and not provide the correct response. In Proposed, pre-testing and post-testing are used for big data testing. In pre-testing, classify fault data from different data sources. After Classification to group big data using SVM algorithms such as Text, Image, Audio, and Video file. In post-testing, to implement the pre-processing, remove the zero file size, unrelated file extension, and de-duplication after pre-processing to implement the Map-reduce algorithm to find out the big data efficiently. This process reduces the pre-processing time, reduces the server energy, and increases the processing time. To remove the fault data before pre-processing means to increase the processing time and data storage.}, location = {Shanghai, China}, series = {ICISE 2021}, pages = {1\u20136}, numpages = {6}, keywords = {Map-reduce, Big Data, Classification using SVM, Pre-Processing, Fault data detection}}
@inproceedings{10.1145/3206505.3206556,title = {Big data landscapes: improving the visualization of machine learning-based clustering algorithms}, author = {Kammer Dietrich , Keck Mandy , Gr\u00fcnder Thomas , Groh Rainer },year = {2018}, isbn = {9781450356169}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3206505.3206556}, doi = {10.1145/3206505.3206556}, abstract = {With the internet, massively heterogeneous data sources need to be understood and classified to provide suitable services to users such as content observation, data exploration, e-commerce, or adaptive learning environments. The key to providing these services is applying machine learning (ML) in order to generate structures via clustering and classification. Due to the intricate processes involved in ML, visual tools are needed to support designing and evaluating the ML pipelines. In this contribution, we propose a comprehensive tool that facilitates the analysis and design of ML-based clustering algorithms using multiple visualization features such as semantic zoom, glyphs, and histograms.}, location = {Castiglione della Pescaia, Grosseto, Italy}, series = {AVI '18}, pages = {1\u20133}, numpages = {3}, keywords = {glyphs, clustering, big data landscapes, visualization, machine learning}}
@inproceedings{10.1145/3396452.3396458,title = {Automatic Construction of Subject Knowledge Graph based on Educational Big Data}, author = {Su Ying , Zhang Yong },year = {2020}, isbn = {9781450374989}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3396452.3396458}, doi = {10.1145/3396452.3396458}, abstract = {In this paper, we propose an automatic construction method of subject knowledge graph for educational applications. The subject knowledge graph is constructed based on educational big data by using a bootstrapping strategy to gradually expand knowledge points and connections between them. In this paper two different datasets are used. One is the subject teaching resources such as syllabuses, teaching plans, textbooks and etc., which is used to automatically construct the core of subject knowledge graph so as to reduce the dependence on the manual annotation. Meanwhile the high-quality of subject teaching resources is the guarantee of accuracy of the knowledge graph core. The other dataset is the massive Internet encyclopedia texts, which is used to expand and complete the subject knowledge graph. As to algorithm, this paper utilizes the BERT-BiLSTM-CRF model to automatically identify the subject knowledge points, and then evaluates the relationship between the knowledge points by calculating their semantic similarity, PMI and Normalized Google Distance between them. The experimental results show that BERT-BiLSTM-CRF outperforms the baselines significantly, and the three kinds of relationship evaluation models have achieved good results. Finally, computer science and physics science are taken as examples to construct the subject knowledge graphs successfully, which show the effectiveness of our method.}, location = {London, United Kingdom}, series = {ICBDE '20}, pages = {30\u201336}, numpages = {7}, keywords = {knowledge graph, normalized google distance, intelligent education, BERT-BILSTM-CRF, point mutual information}}
@inproceedings{10.1145/2783258.2789989,title = {Big Data Analytics: Optimization and Randomization}, author = {Yang Tianbao , Lin Qihang , Jin Rong },year = {2015}, isbn = {9781450336642}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2783258.2789989}, doi = {10.1145/2783258.2789989}, abstract = {As the scale and dimensionality of data continue to grow in many applications of data analytics (e.g., bioinformatics, finance, computer vision, medical informatics), it becomes critical to develop efficient and effective algorithms to solve numerous machine learning and data mining problems. This tutorial will focus on simple yet practically effective techniques and algorithms for big data analytics. In the first part, we plan to present the state-of-the-art large-scale optimization algorithms, including various stochastic gradient descent methods, stochastic coordinate descent methods and distributed optimization algorithms, for solving various machine learning problems. In the second part, we will focus on randomized approximation algorithms for learning from large-scale data. We will discuss i) randomized algorithms for low-rank matrix approximation; ii) approximation techniques for solving kernel learning problems; iii) randomized reduction methods for addressing the high-dimensional challenge. Along with the description of algorithms, we will also present some empirical results to facilitate understanding of different algorithms and comparison between them.}, location = {Sydney, NSW, Australia}, series = {KDD '15}, pages = {2327}, numpages = {1}, keywords = {randomized reduction, optimization, machine learning, randomized approximation}}
@inproceedings{10.1145/2676723.2693616,title = {Situating Computational Thinking with Big Data: Pedagogy and Technology (Abstract Only)}, author = {Bart Austin Cory },year = {2015}, isbn = {9781450329668}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2676723.2693616}, doi = {10.1145/2676723.2693616}, abstract = {As Computational Thinking becomes pervasive in undergraduate programs, new students must be educated in meaningful, authentic contexts that they find both motivating and relatable. I propose working with big data as a novel context for introductory programming, authentic given its importance in diverse fields such as agriculture, history, and more. Big data is considered difficult to use because of its inherent technical obstacles. To overcome these difficulties, I introduce a new project: CORGIS - a \"Collection of Real-time, Giant, Interesting, Situated Datasets\". The CORGIS project comprises a collection of libraries that provide an interface to big data for students, architectures for rapidly enabling new datasets, and a web-based textbook platform for disseminating relevant course materials. This textbook features an online block-based programming environment, real-time collaborative text editing, and continuous server-side storage. In this poster, I describe the educational theory guiding this work, the novel technolgy created and deployed, and the initial, promising results.}, location = {Kansas City, Missouri, USA}, series = {SIGCSE '15}, pages = {719}, numpages = {1}, keywords = {Big data, Computational Thinking, CORGIS, Motivation}}
@inproceedings{10.1145/2320765.2320830,title = {Towards measuring test data quality}, author = {Held Johannes , Lenz Richard },year = {2012}, isbn = {9781450311434}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2320765.2320830}, doi = {10.1145/2320765.2320830}, abstract = {In order to enable proper system and integration testing, it is often necessary to have huge test data inventories, reflecting the heterogeneous live system. Although the maintenance of large data stores can be guided by advice obtained from data quality evaluations, this technique can be only partly applied to test data inventories. Assessing test data quality is difficult, as the well-known data quality dimensions are not applicable in an easy fashion. For example, an otherwise good value of 100% for correctness would not allow to store erroneous test data items. The need for data quality dimensions dedicated to assessing test data quality can't be satisfied by well-known data quality dimensions. In this paper, we present our thesis approach to identify and validate new quality dimensions applicable for test data quality and develop quantification methods. We propose proximity to reality and degree of coverage as two new test data quality dimension and sketch quantification approach to measures, specifically suited for test data.}, location = {Berlin, Germany}, series = {EDBT-ICDT '12}, pages = {233\u2013238}, numpages = {6}, keywords = {testing, data quality dimensions, test data quality}}
@inproceedings{10.1109/CCGrid.2015.85,title = {Big data provenance analysis and visualization}, author = {Chen Peng , Plale Beth },year = {2015}, isbn = {9781479980062}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2015.85}, doi = {10.1109/CCGrid.2015.85}, abstract = {Provenance captured from E-Science experimentation is often large and complex, for instance, from agent-based simulations that have tens of thousands of heterogeneous components interacting over extended time periods. The subject of study of my dissertation is the use of E-Science provenance at scale. My initial research studied the visualization of large provenance graphs and proposed an abstract representation of provenance that supports useful data mining. Recent work involves analyzing large provenance data generated from agent-based simulations on a single machine. In continuation, I propose stream processing techniques to support the continuous and realtime analysis of data provenance, which is captured from agent based simulations on HPC and thus has unprecedented volume and complexity.}, location = {Shenzhen, China}, series = {CCGRID '15}, pages = {797\u2013800}, numpages = {4}, keywords = {data provenance, mining, stream processing, visualization, big data}}
@inproceedings{10.1145/2632320.2632325,title = {Big data and data management: a topic for secondary computing education}, author = {Grillenberger Andreas },year = {2014}, isbn = {9781450327558}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2632320.2632325}, doi = {10.1145/2632320.2632325}, abstract = {The topics data management and data analysis are currently discussed in various contexts, e.g. in Computer Science but also in daily life and society. The recent developments in this field, which are often summarized under the term Big Data, did not only lead to the emergence of new database models, but also comprise new threats, e.g. for data privacy. These topics include many aspects that are important for everyone, but they only gain in relevance slowly in higher education and hardly in secondary education. Hence, I will evaluate data management as a topic for secondary education, with a view on the long-lasting concepts and aspects in this field.}, location = {Glasgow, Scotland, United Kingdom}, series = {ICER '14}, pages = {147\u2013148}, numpages = {2}, keywords = {databases, daily life, data management, big data, nosql, data analysis, secondary schools, data privacy}}
@inproceedings{10.1145/3358505.3358512,title = {Big Data Platforms and Tools for Data Analytics in the Data Science Engineering Curriculum}, author = {Demchenko Yuri },year = {2019}, isbn = {9781450371650}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3358505.3358512}, doi = {10.1145/3358505.3358512}, abstract = {This paper presents experiences of development and teaching courses on Big Data Infrastructure Technologies for Data Analytics (BDIT4DA) as a part of the general Data Science curricula. The authors built the discussed course based on the EDISON Data Science Framework (EDSF), in particular, Data Science Body of Knowledge (DS-BoK) related to Data Science Engineering knowledge area group (KAG-DSENG). The paper provides overview of the cloud based platforms and tools for Big Data Analytics and stresses importance of including into curriculum the practical work with clouds for future graduates or specialists workplace adaptability. The paper discusses a relationship between the DSENG BoK and Big Data technologies and platforms, in particular Hadoop based applications and tools for data analytics that should be promoted through all course activities: lectures, practical activities and self-study.}, location = {Oxford, United Kingdom}, series = {ICCBDC 2019}, pages = {60\u201364}, numpages = {5}, keywords = {Big Data Infrastructure Technologies, EDISON Data Science Framework (EDSF), Cloud Computing, Data Science Body of Knowledge (DS-BoK), Hadoop ecosystem, Data Science Engineering}}
@inproceedings{10.1145/2699414,title = {Exascale computing and big data}, author = {Reed Daniel A. , Dongarra Jack },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2699414}, doi = {10.1145/2699414}, abstract = {Scientific discovery and engineering innovation requires unifying traditionally separated high-performance computing and big data analytics.}, pages = {56\u201368}, numpages = {13}}
@inproceedings{10.1145/2612733.2619954,title = {Informetric mapping of \"big data\" in FI-WARE}, author = {Villase\u00f1or Elio , Estrada Hugo },year = {2014}, isbn = {9781450329019}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2612733.2619954}, doi = {10.1145/2612733.2619954}, abstract = {Today, governmental entities are embracing new trends in information technology. One of the technological developments that generate more excitement is Big Data; because this technology let us analyze the huge amount of information produced by the government and is useful for decisions making. On the other hand, the Future Internet platform of the European Community (FI- WARE) is one of the most powerful trends around the world and has aroused more interest in governments. This technology is based on a set of Generic Enablers (GE) for various applications, including Big Data. The FI-WARE is a platform under construction and knowing how this process performed is essential to join in this monumental effort and take advantages of its benefits. This document presents the results of the application of text and data mining techniques as well as informetric mapping to gain understanding regarding the development of Big Data technology present in the FI- WARE.}, location = {Aguascalientes, Mexico}, series = {dg.o '14}, pages = {348\u2013349}, numpages = {2}, keywords = {generic enablers, FI-ware, informetric analysis, big data}}
@inproceedings{10.1145/3148055.3148079,title = {Understanding Behavior Trends of Big Data Frameworks in Ongoing Software-Defined Cyber-Infrastructure}, author = {Chen Shouwei , Rodero Ivan },year = {2017}, isbn = {9781450355490}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3148055.3148079}, doi = {10.1145/3148055.3148079}, abstract = {As data analytics applications become increasingly important in a wide range of domains, the ability to develop large-scale and sustainable platforms and software infrastructure to support these applications has significant potential to drive research and innovation in both science and business domains. This paper characterizes performance and power-related behavior trends and tradeoffs of the two predominant frameworks for Big Data analytics (i.e., Apache Hadoop and Spark) for a range of representative applications. It also evaluates system design knobs, such as storage and network technologies and power capping techniques. Experimental results from empirical executions provide meaningful data points for exploring the potential of software-defined infrastructure for Big Data processing systems through simulation. The results provide better understanding of the design space to build multi-criteria application-centric models as well as show significant advantages of software-defined infrastructure in terms of execution time, energy and cost. It motivates further research focused on in-memory processing formulations regarding systems with deeper memory hierarchies and software-defined infrastructure.}, location = {Austin, Texas, USA}, series = {BDCAT '17}, pages = {199\u2013208}, numpages = {10}, keywords = {characterization and tradeoffs, big data processing frameworks, software-defined infrastructure}}
@inproceedings{10.5555/2735522.2735573,title = {Big data analytics using hadoop}, author = {Mohammadi Mohammad Mahdi , Raahemi Bijan , Cheraghchi Fatemeh , Obidallah Wael , Bigdeli Elnaz },year = {2014}, publisher = {IBM Corp.}, address = {USA}, abstract = {The exponential growth of data, especially over the internet; leads to the dramatic rise of unstructured and semi-structured data, in addition to the traditional (structured) data. Since relational databases and associated tools were designed to interact with structured data, companies such as Google and Yahoo were facing challenges dealing with the unstructured and semi-structured data. When the volume of data goes beyond the processing capacity of the existing algorithms, it is considered as Big Data. Hadoop is a popular technology for analyzing Big data. There are tools available on Hadoop platform to assist analysts create complex queries and run machine learning algorithms in a parallel and distributed fashion. The goal of this workshop is to provide the participants with hands-on experiences on analyzing Big data, installing Hadoop on Linux-based machines (PCs equipped with Ubuntu OS), and running examples on Hadoop framework.}, location = {Markham, Ontario, Canada}, series = {CASCON '14}, pages = {323\u2013325}, numpages = {3}}
@inproceedings{10.1145/3134302.3134335,title = {Modelling of Educational Data Following Big Data Value Chain}, author = {Petrova-Antonova Dessislava , Georgieva Olga , Ilieva Sylvia },year = {2017}, isbn = {9781450352345}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3134302.3134335}, doi = {10.1145/3134302.3134335}, abstract = {Big Data is attracting increasing amount of attention among academy, industry and citizens. It poses both opportunities and challenges for society as a whole. In order to gain value from Big Data, it needs to be processed and analysed in an appropriate way, and the results have to be presented in a visual manner as to be able to effectively support decision making. Following the current trends in Big Data, this paper aims to prove the value-creation of Big Data by proposing a new data model in the field of the primary and secondary education in Bulgaria. It follows the Big Data Value Chain concept in order to group the schools depending on the concentration of students with learning deficits and risk of premature leaving the education system. The primary purpose of the proposed Data Model is to drive decisions by turning information into intelligence.}, location = {Ruse, Bulgaria}, series = {CompSysTech'17}, pages = {88\u201395}, numpages = {8}, keywords = {Education, Big Data, Big Data Value Chain, Decision support, Data model}}
@inproceedings{10.1145/3006299.3006317,title = {A big data analytics based approach to anomaly detection}, author = {Razaq Abdul , Tianfield Huaglory , Barrie Peter },year = {2016}, isbn = {9781450346177}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3006299.3006317}, doi = {10.1145/3006299.3006317}, abstract = {We present a novel Cyber Security analytics framework. We demonstrate a comprehensive cyber security monitoring system to construct cyber security correlated events with feature selection to anticipate behaviour based on various sensors.}, location = {Shanghai, China}, series = {BDCAT '16}, pages = {187\u2013193}, numpages = {7}, keywords = {event correlation, SIEM, advanced persistent threats, security analytics, IDS/IPS, process auditing}}
@inproceedings{10.5555/2740769.2740789,title = {Towards building a scholarly big data platform: challenges, lessons and opportunities}, author = {Wu Zhaohui , Wu Jian , Khabsa Madian , Williams Kyle , Chen Hung-Hsuan , Huang Wenyi , Tuarob Suppawong , Choudhury Sagnik Ray , Ororbia Alexander , Mitra Prasenjit , Giles C. Lee },year = {2014}, isbn = {9781479955695}, publisher = {IEEE Press}, abstract = {We introduce a big data platform that provides various services for harvesting scholarly information and enabling efficient scholarly applications. The core architecture of the platform is built on a secured private cloud, crawls data using a scholarly focused crawler that leverages a dynamic scheduler, processes by utilizing a map reduce based crawl-extraction-ingestion (CEI) workflow, and is stored in distributed repositories and databases. Services such as scholarly data harvesting, information extraction, and user information and log data analytics are integrated into the platform and provided by an OAI and RESTful API. We also introduce a set of scholarly applications built on top of this platform including citation recommendation and collaborator discovery.}, location = {London, United Kingdom}, series = {JCDL '14}, pages = {117\u2013126}, numpages = {10}, keywords = {big data, information extraction, scholarly big data}}
@inproceedings{10.1145/2612733.2612780,title = {Using big data for digital government research}, author = {Harrison Teresa M. },year = {2014}, isbn = {9781450329019}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2612733.2612780}, doi = {10.1145/2612733.2612780}, abstract = {\"Big data\" has captured the imagination of e-government researchers as the source of potential advances in government innovation, strategy, and policy, and as the basis for entirely new approaches to research investigations across the disciplines. Digital data is everywhere, and, it is thought, considerable value may be obtained in analyzing the digital traces generated by internet users as they traverse social, political, economic and material spaces. However, once past the seminal anecdotes (e.g, estimates of flu outbreaks based on Google searches), generating knowledge from big data presents complexities along with its potentials. In this panel, we bring together researchers to share their experiences with big data projects in e-government, considering the research questions they are asking, the opportunities and complications they are encountering, and the new strategies or responses they are creating in response.}, location = {Aguascalientes, Mexico}, series = {dg.o '14}, pages = {309\u2013310}, numpages = {2}, keywords = {big data, political science, social media, policy informatics, social networks, automated linguistic analysis, 2008 financial crisis, project X haren, communication, Facebook}}
@inproceedings{10.1145/3184407.3184420,title = {Performance Prediction of Cloud-Based Big Data Applications}, author = {Ardagna Danilo , Barbierato Enrico , Evangelinou Athanasia , Gianniti Eugenio , Gribaudo Marco , Pinto T\u00falio B. M. , Guimar\u00e3es Anna , Couto da Silva Ana Paula , Almeida Jussara M. },year = {2018}, isbn = {9781450350952}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3184407.3184420}, doi = {10.1145/3184407.3184420}, abstract = {Data heterogeneity and irregularity are key characteristics of big data applications that often overwhelm the existing software and hardware infrastructures. In such context, the exibility and elasticity provided by the cloud computing paradigm over a natural approach to cost-effectively adapting the allocated resources to the application's current needs. Yet, the same characteristics impose extra challenges to predicting the performance of cloud-based big data applications, a central step in proper management and planning. This paper explores two modeling approaches for performance prediction of cloud-based big data applications. We evaluate a queuing-based analytical model and a novel fast ad-hoc simulator in various scenarios based on different applications and infrastructure setups. Our results show that our approaches can predict average application execution times with 26% relative error in the very worst case and about 12% on average. Moreover, our simulator provides performance estimates 70 times faster than state of the art simulation tools.}, location = {Berlin, Germany}, series = {ICPE '18}, pages = {192\u2013199}, numpages = {8}, keywords = {big data, performance modeling, approximate methods, spark, simulation}}
@inproceedings{10.1145/3419634,title = {A Survey on IoT Big Data: Current Status, 13 V\u2019s Challenges, and Future Directions}, author = {Bansal Maggi , Chana Inderveer , Clarke Siobh\u00e1n },year = {2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3419634}, doi = {10.1145/3419634}, abstract = {Driven by the core technologies, i.e., sensor-based autonomous data acquisition and the cloud-based big data analysis, IoT automates the actuation of data-driven intelligent actions on the connected objects. This automation enables numerous useful real-life use-cases, such as smart transport, smart living, smart cities, and so on. However, recent industry surveys reflect that data-related challenges are responsible for slower growth of IoT in recent years. For this reason, this article presents a systematic and comprehensive survey on IoT Big Data (IoTBD) with the aim to identify the uncharted challenges for IoTBD. This article analyzes the state-of-the-art academic works in IoT and big data management across various domains and proposes a taxonomy for IoTBD management. Then, the survey explores the IoT portfolio of major cloud vendors and provides a classification of vendor services for the integration of IoT and IoTBD on their cloud platforms. After that, the survey identifies the IoTBD challenges in terms of 13 V\u2019s challenges and envisions IoTBD as \u201cBig Data 2.0.\u201d Then the survey provides comprehensive analysis of recent works that address IoTBD challenges by highlighting their strengths and weaknesses to assess the recent trends and future research directions. Finally, the survey concludes with discussion on open research issues for IoTBD.}, pages = {1\u201359}, numpages = {59}, keywords = {cloud IoT services, cloud computing in IoT, IoT big data survey, IoT big data, big data 2.0, V\u2019s challenges for IoT big data}}
@inproceedings{10.1145/3400903.3409117,title = {Multidimensional Clustering over Big Data: Models, Issues, Analysis, Emerging Trends}, author = {Cuzzocrea Alfredo },year = {2020}, isbn = {9781450388146}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3400903.3409117}, doi = {10.1145/3400903.3409117}, abstract = {Clustering is an essential task of the whole pattern recognition process, and it can serve under several roles, for instance in terms of data pre-processing tool for better (i.e., more accurate) pattern recognition analysis and mining. In this vest, a critical applicative setting is represented by applying pattern recognition tools over emerging big data. Here, clustering specially plays a challenging role within the context of this conceptual mining framework, and, under a larger vision, it can act as pre-processing task for general big data clustering problems. In this paper, we first focus on state-of-the-art solutions for big data clustering in the specific pattern recognition context, by highlighting benefits and limitations. Then, we focus the attention on the problem of effectively and efficiently clustering big data via innovative multidimensional metaphors, thus achieving the definition of so-called multidimensional clustering over big data. In this so-delineated research setting, based on the well-known challenges of big data management (e.g., volume, velocity, variety, and veracity), we provide critical review and discussion, complemented by a rich set of research directions, development perspectives and emerging trends of the investigated topics, as contextualized in the reference big-data-analytics scientific area.}, location = {Vienna, Austria}, series = {SSDBM 2020}, pages = {1\u20136}, numpages = {6}, keywords = {Big Data Clustering, Multidimensional Clustering over Big Data, Big Data Analytics, Big Data}}
@inproceedings{10.1145/2859889.2883586,title = {Big Data Applications Performance Assurance}, author = {Zibitsker Boris },year = {2016}, isbn = {9781450341479}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2859889.2883586}, doi = {10.1145/2859889.2883586}, abstract = {Today's fast-paced businesses have to make business decisions in real-time. That creates pressure on IT leaders to develop near real-time Big Data and Data Warehouse applications that apply advance analytics against large volumes of data to deliver recommendations fast. Hardware and software used to build Big Data infrastructure is cheap, but management of complex environments is not easy In this presentation we will review role of Performance Assurance incorporating Descriptive, Diagnostic, Predictive, Prescriptive and Control Analytics during each phase of the Application and Data life cycle. We will review challenges and Performance Assurance solutions for Big Data Batch and Real Time applications based on YARN, Map/Reduce, Kafka, Spark/Storm and Cassandra Apache projects}, location = {Delft, The Netherlands}, series = {ICPE '16 Companion}, pages = {31}, numpages = {1}}
@inproceedings{10.1145/3372454.3372479,title = {A Multi-sensor Big Data fusion Method in Quality Prediction of the Plasma Enhanced Chemical Vapor Deposition Process}, author = {Guo Yiming , Xia Zhijie , Zhang Zhisheng , Sun Mengze },year = {2019}, isbn = {9781450372015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3372454.3372479}, doi = {10.1145/3372454.3372479}, abstract = {Plasma Enhanced Chemical Vapor Deposition (PECVD) is a critical process in the processing of solar cells. Large quantities of the process data are collected from different sensors during the PECVD process, which are high-dimensional and highly correlated. Most existing research only focus on the analysis of single sensor data instead of multi-sensor data. However, the information contained in single sensor data is incomplete. In this paper, the method of Convolutional Neural Networks (CNN) is adopted to analysis multi-sensor big data form PECVD process. The regression model between the multi-sensor data and the quality of solar cells is established for quality prediction. The impact of various types of hyper-parameters on the performance of the model is analyzed, and the predictive performance of the model is optimized by adjusting the hyper-parameters. The performance of the proposed method is compared with existing methods in a real-world case study.}, location = {Cergy-Pontoise, France}, series = {ICBDR 2019}, pages = {24\u201329}, numpages = {6}, keywords = {Quality prediction, Multi-sensor data, Big data, CNN, PECVD}}
@inproceedings{10.1145/2433396.2433459,title = {Big data, lifelong machine learning and transfer learning}, author = {Yang Qiang },year = {2013}, isbn = {9781450318693}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2433396.2433459}, doi = {10.1145/2433396.2433459}, abstract = {A major challenge in today's world is the Big Data problem, which manifests itself in Web and Mobile domains as rapidly changing and heterogeneous data streams. A data-mining system must be able to cope with the influx of changing data in a continual manner. This calls for Lifelong Machine Learning, which in contrast to the traditional one-shot learning, should be able to identify the learning tasks at hand and adapt to the learning problems in a sustainable manner. A foundation for lifelong machine learning is transfer learning, whereby knowledge gained in a related but different domain may be transferred to benefit learning for a current task. To make effective transfer learning, it is important to maintain a continual and sustainable channel in the life time of a user in which the data are annotated. In this talk, I outline the lifelong machine learning situations, give several examples of transfer learning and applications for lifelong machine learning, and discuss cases of successful extraction of data annotations to meet the Big Data challenge.}, location = {Rome, Italy}, series = {WSDM '13}, pages = {505\u2013506}, numpages = {2}, keywords = {big data, transfer learning, lifelong machine learning}}
@inproceedings{10.1145/2447481.2447491,title = {Predictive analytics with surveillance big data}, author = {Ayhan Samet , Pesce Johnathan , Comitz Paul , Gerberick Gary , Bliesner Steve },year = {2012}, isbn = {9781450316927}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2447481.2447491}, doi = {10.1145/2447481.2447491}, abstract = {In this paper, we describe a novel analytics system that enables query processing and predictive analytics over streams of aviation data. As part of an Internal Research and Development project, Boeing Research and Technology (BR&T) Advanced Air Traffic Management (AATM) built a system that makes predictions based upon descriptive patterns of archived aviation data. Boeing AATM has been receiving live Aircraft Situation Display to Industry (ASDI) data and archiving it for over two years. At the present time, there is not an easy mechanism to perform analytics on the data. The incoming ASDI data is large, compressed, and requires correlation with other flight data before it can be analyzed.The service exposes this data once it has been uncompressed, correlated, and stored in a data warehouse for further analysis using a variety of descriptive, predictive, and possibly prescriptive analytics tools. The service is being built partially in response to requests from Boeing Commercial Aviation (BCA) for analysis of capacity and flow in the US National Airspace System (NAS). The service utilizes a custom tool for correlating the raw ASDI feed, IBM Warehouse with DB2 for data management, WebSphere Message Broker for real-time message brokering, SPSS Modeler for statistical analysis, and Cognos BI for front-end business intelligence (BI) visualization. This paper describes a scalable service architecture, implementation and the value it adds to the aviation domain.}, location = {Redondo Beach, California}, series = {BigSpatial '12}, pages = {81\u201390}, numpages = {10}, keywords = {data analytics, data stream management, big data, data warehouse}}
@inproceedings{10.1145/2295136.2295148,title = {Emerging trends around big data analytics and security: panel}, author = {Bhatti Rafae , LaSalle Ryan , Bird Rob , Grance Tim , Bertino Elisa },year = {2012}, isbn = {9781450312950}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2295136.2295148}, doi = {10.1145/2295136.2295148}, abstract = {This panel will discuss the interplay between key emerging security trends centered around big data analytics and security. With the explosion of big data and advent of cloud computing, data analytics has not only become prevalent but also a critical business need. Internet applications today consume vast amounts of data collected from heterogeneous big data repositories and provide meaningful insights from it. These include applications for business forecasting, investment and finance, healthcare and well-being, science and hi-tech, to name a few. Security and operational intelligence is one of the critical areas where big data analytics is expected to play a crucial role. Security analytics in a big data environment presents a unique set of challenges, not properly addressed by the existing security incident and event monitoring (or SIEM) systems that typically work with a limited set of traditional data sources (firewall, IDS, etc.) in an enterprise network. A big data environment presents both a great opportunity and a challenge due to the explosion and heterogeneity of the potential data sources that extend the boundary of analytics to social networks, real time streams and other forms of highly contextual data that is characterized by high volume and speed. In addition to meeting infrastructure challenges, there remain additional unaddressed issues, including but not limited to development of self-evolving threat ontologies, integrated network and application layer analytics, and detection of \"low and slow\" attacks. At the same time, security analytics requires a high degree of data assurance, where assurance implies that the data be trustworthy as well as managed in a privacy preserving manner. Our panelists represent individuals from industry, academia, and government who are at the forefront of big data security analytics. They will provide insights into these unique challenges, survey the emerging trends, and lay out a vision for future.}, location = {Newark, New Jersey, USA}, series = {SACMAT '12}, pages = {67\u201368}, numpages = {2}, keywords = {big data, analytics, security, privacy}}
@inproceedings{10.1145/3396452,title = {Proceedings of the 2020 The 3rd International Conference on Big Data and Education},year = {2020}, isbn = {9781450374989}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {Due to the outbreak of COVID-19, and considering the participants' healthy, the 2020 3rd International Conference on Big Data and Education (ICBDE 2020) was held successfully online during April 01-03, 2020. Online conference is a good scientific platform for both local and international scientists, managers, business leaders, educators, scholars, engineers and technologists who work in all aspects of Big Data and Education to exchange and share their experiences, new ideas, and discuss the practical challenges encountered and the solutions adopted.}, location = {London, United Kingdom}}
@inproceedings{10.1109/CCGRID.2017.107,title = {A Big Data Architecture for Automotive Applications: PSA Group Deployment Experience}, author = {Haroun Amir , Mostefaoui Ahmed , Dessables Fran\u00e7ois },year = {2017}, isbn = {9781509066100}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGRID.2017.107}, doi = {10.1109/CCGRID.2017.107}, abstract = {Vehicles have become moving sensor platforms collecting huge volumes of data from their various embedded sensors. This data has a great value for automotive manufacturers and vehicles owners. Indeed, connected vehicles data can be used in a large broad of automotive services ranging from safety services to well-being services (e.g. fatigue detection). However, vehicle fleets send big volumes of data that traditional computing and storage approaches are not able to manage efficiently. In this paper, we present the experience of the PSA Group1 on leveraging big data in automotive context. We describe in depth the big data architecture deployed within the PSA Group and the underlaying technologies/products used in each component.}, location = {Madrid, Spain}, series = {CCGrid '17}, pages = {921\u2013928}, numpages = {8}, keywords = {Connected Vehicles, Big Data, Reference Architecture}}
@inproceedings{10.5555/2819289.2819293,title = {Safely managing data variety in big data software development}, author = {Cerqueus Thomas , de Almeida Eduardo Cunha , Scherzinger Stefanie },year = {2015}, publisher = {IEEE Press}, abstract = {We consider the task of building Big Data software systems, offered as software-as-a-service. These applications are commonly backed by NoSQL data stores that address the proverbial Vs of Big Data processing: NoSQL data stores can handle large volumes of data and many systems do not enforce a global schema, to account for structural variety in data. Thus, software engineers can design the data model on the go, a flexibility that is particularly crucial in agile software development. However, NoSQL data stores commonly do not yet account for the veracity of changes when it comes to changes in the structure of persisted data. Yet this is an inevitable consequence of agile software development. In most NoSQL-based application stacks, schema evolution is completely handled within the application code, usually involving object mapper libraries. Yet simple code refactorings, such as renaming a class attribute at the source code level, can cause data loss or runtime errors once the application has been deployed to production. We address this pain point by contributing type checking rules that we have implemented within an IDE plugin. Our plugin ControVol statically type checks the object mapper class declarations against the code release history. ControVol is thus capable of detecting common yet risky cases of mismatched data and schema, and can even suggest automatic fixes.}, location = {Florence, Italy}, series = {BIGDSE '15}, pages = {4\u201310}, numpages = {7}}
@inproceedings{10.1145/3335484.3335509,title = {Developing a Minimum Viable Product for Big Data and AI Education: Action Research Based on a Two-Year Reform of an Undergraduate Program of Internet and New Media}, author = {Liao Han-Teng , Wang Zijia , Wu Xue },year = {2019}, isbn = {9781450362788}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3335484.3335509}, doi = {10.1145/3335484.3335509}, abstract = {The advancement in Big Data and Artificial Intelligence has posed challenges and opportunities for undergraduate education. It raises several questions regarding what is desirable and viable for preparing undergraduate students for future success. This paper presents the rationales and outcomes of a two-year reform of an undergraduate program of Internet and New Media in China, summarizing the ways in which the curriculum design can incorporate Big Data and Artificial Intelligence education for future Internet product managers and HCI professionals. Using the notion of \"minimum viable products\" to frame the action research of education reform, it first describes the ways in which we identify the job market need for Internet product managers and HCI professionals, and explores the learning pathways, departing from the conventional Internet and New Media programs in China. It then documents the results of the minimum required changes to deliver such a viable learning product and the initial evidence from students that validates the designed learning product. The overall findings demonstrate the usefulness and challenges in preparing students for careers in a data-intensive or data-driven world.}, location = {Guangzhou, China}, series = {ICBDC '19}, pages = {42\u201347}, numpages = {6}, keywords = {Knowledge Brokerage, Sustainable Design, Big Data Education, Sustainable HCI, Ecological Design, ICT4D}}
@inproceedings{10.14778/2824032.2824140,title = {Big data research: will industry solve all the problems?}, author = {Balazinska Magdalena },year = {2015}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2824032.2824140}, doi = {10.14778/2824032.2824140}, abstract = {The need for effective tools for big data data management and analytics continues to grow. While the ecosystem of tools is expanding many research problems remain open: they include challenges around efficient processing, flexible analytics, ease of use, and operation as a service. Many new systems and much innovation, however, come from industry (or from academic projects that quickly became big players in industry). An important question for our community is whether industry will solve all the problems or whether there is a place for academic research in big data and what is that place. In this paper, we address this question by looking back at our research on the Nuage, CQMS, Myria, and Data Pricing projects, and the SciDB collaboration.}, pages = {2053\u20132056}, numpages = {4}}