@inproceedings{10.1145/3193063.3193069,
author = {Cheng, Susu and Zhao, Haijun},
title = {An Overview of Techniques for Confirming Big Data Property Rights},
year = {2018},
isbn = {9781450363785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193063.3193069},
doi = {10.1145/3193063.3193069},
abstract = {The major premise of big data circulation is to identify the ownership of data resource. This paper summed some feasible techniques and methods for confirming big data property which are data citation technology, data provenance technology, data reversible hiding technology, computer forensic technology and block chain technology. The ownership of information property which from different sizes, different formats and different storage condition on distributed heterogeneous platforms can be confirmed by comprehensive application of these techniques and methods based on the coupling interface between them in the practice of big data.},
booktitle = {Proceedings of the 2018 International Conference on Intelligent Information Technology},
pages = {59–64},
numpages = {6},
keywords = {Information property index, Method for confirming information property rights, Big Data, Confirmation of Information Property},
location = {Ha Noi, Viet Nam},
series = {ICIIT 2018}
}

@inproceedings{10.1145/3352740.3352756,
author = {Haikun, Teng and Shiying, Wang and Yue, Xiao-Guang},
title = {Application of Big Data Technology in Emergency Decision System},
year = {2019},
isbn = {9781450372053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352740.3352756},
doi = {10.1145/3352740.3352756},
abstract = {Due to the randomness and ambiguity of decision information, when emergencies occur, multiple scenarios may occur, and the probability and consequences of each scenario are different after using different emergency solutions. Decision makers often have psychological characteristics such as reference dependence, loss avoidance, and probability judgment distortion when making decisions. Therefore, this paper proposes a multi-attribute group decision making method based on foreground theory and language decision information based on big data technology. The method makes full use of the qualitative and quantitative conversion characteristics of the cloud model, and solves the problems of ambiguity and randomness of emergency decision information. At the same time, based on the foreground theory, the influence of the psychological factors of the decision makers on the decision results is fully considered, and the emergency response is emergency. Management and decision making provide supplementary reference.},
booktitle = {Proceedings of the 2019 3rd International Workshop on Education, Big Data and Information Technology},
pages = {93–97},
numpages = {5},
keywords = {Emergencies, Big data technology, Decision making methods, Emergency management, Decision information},
location = {Guilin, China},
series = {EBDIT 2019}
}

@article{10.1145/1541880.1541883,
author = {Batini, Carlo and Cappiello, Cinzia and Francalanci, Chiara and Maurino, Andrea},
title = {Methodologies for Data Quality Assessment and Improvement},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/1541880.1541883},
doi = {10.1145/1541880.1541883},
abstract = {The literature provides a wide range of techniques to assess and improve the quality of data. Due to the diversity and complexity of these techniques, research has recently focused on defining methodologies that help the selection, customization, and application of data quality assessment and improvement techniques. The goal of this article is to provide a systematic and comparative description of such methodologies. Methodologies are compared along several dimensions, including the methodological phases and steps, the strategies and techniques, the data quality dimensions, the types of data, and, finally, the types of information systems addressed by each methodology. The article concludes with a summary description of each methodology.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {16},
numpages = {52},
keywords = {Data quality, quality dimension, methodology, information system, data quality measurement, data quality assessment, data quality improvement}
}

@inproceedings{10.5555/1812530.1812588,
author = {Chen, Kuang and Chen, Harr and Conway, Neil and Dolan, Heather and Hellerstein, Joseph M. and Parikh, Tapan S.},
title = {Improving Data Quality with Dynamic Forms},
year = {2009},
isbn = {9781424446629},
publisher = {IEEE Press},
abstract = {Organizations in developing regions want to efficiently collect digital data, but standard data gathering practices from the developed world are often inappropriate. Traditional techniques for form design and data quality are expensive and labour-intensive. We propose a new data-driven approach to form design, execution (filling) and quality assurance. We demonstrate USHER, an end-to-end system that automatically generates data entry forms that enforce and maintain data quality constraints during execution. The system features a probabilistic engine that drives form-user interactions to encourage correct answers.},
booktitle = {Proceedings of the 3rd International Conference on Information and Communication Technologies and Development},
pages = {487},
numpages = {1},
location = {Doha, Qatar},
series = {ICTD'09}
}

@inproceedings{10.1109/WI-IAT.2014.15,
author = {Hansmann, Thomas and Niemeyer, Peter},
title = {Big Data - Characterizing an Emerging Research Field Using Topic Models},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.15},
doi = {10.1109/WI-IAT.2014.15},
abstract = {Big Data is one of the latest emerging topics in the field of business information systems, and is marketed as being the key for companies' future success. Many analytic solutions are offered by IT companies to help other businesses with the flood of data that is generated within and outside of a company. Despite the extensive use of the notion Big Data for marketing purposes, there is no common understanding of how to characterize the elements of the Big Data concept. The authors contribute to the clarification of this concept with a methodologically enriched literature review by deriving characteristic dimensions from existing definitions of Big Data. These dimensions are validated and enriched with a two-step approach by applying topic models on 248 publications relevant to Big Data. The authors propose that the concept of Big Data can be described by the dimensions of data, IT infrastructure, applied methods, and an applications perspective. The assignment of the results to a generic data analysis process reveals that recent publications focus on data analysis and processing, and less attention is given to the initial data selection or the visualization and utilization of the analysis results.},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 01},
pages = {43–51},
numpages = {9},
keywords = {topic models, Big Data, literature review},
series = {WI-IAT '14}
}

@inproceedings{10.1145/2851613.2851881,
author = {Jlassi, Aymen and Martineau, Patrick},
title = {Virtualization Technologies for the Big Data Environment},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851881},
doi = {10.1145/2851613.2851881},
abstract = {Today, consumers request virtual resources like CPU, RAM, disk (etc.) supplied by the service providers (like Amazon) and they pay on a "pay-as-you-go" basis. Generally, the supervisors adopt virtualization technologies, which optimize resources usage and limit the operating cost. The virtualization technologies are classified in two categories. The first one concerns the heavy virtualization. Each virtual machines (VM) emulates hardware and embeds its own operating system (OS) that is completely isolated from the host OS. The second one concerns the light virtualization, which is based on the management of containers. The containers share the host OS kernel [5] while ensuring isolation. In this paper, we benchmark the performance and the energy consumption of an infrastructure that is based on the software Hadoop regarding the two technologies of virtualization. At first, we will identify the points to be improved concerning Hadoop performances and then we will reduce the deployment cost on the cloud. Second, the Hadoop community finds an in-depth study of the resources consumption depending on the environment of deployment. Our experiments are based on the comparison of the Docker technology (light virtualization) and VMware technology® (heavy virtualization). We come to the point that in most experiments the light technology offers better performances in completion time of workloads and it is more adapted to be used with the Hadoop software.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {542–545},
numpages = {4},
keywords = {Hadoop, resources consumption, benchmarks, virtualization},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3377170.3377180,
author = {Liu, Qinxian and Hyun, Youyung and Hosoya, Ryuichi and Kamioka, Taro},
title = {How Big Data Analytics Impacts Agility: The Moderation Effect of Orientation of Interactive Team Cognition},
year = {2020},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377170.3377180},
doi = {10.1145/3377170.3377180},
abstract = {Nowadays, organizations have been faced with the rapidly changing environment; thus, in order to survive in such volatile business environment, agility which is the ability to sense opportunities and defend threats has become a critical issue. Big data analytics (BDA) has been known to positively influence the agility; however, to our knowledge, the impact of BDA use on agility has not been studied abundantly in relation to the perspective of team cognition. To fill this research gap, we developed a new construct called orientation of interactive team cognition (OITC) based on the interactive team cognition (ITC) theory. After analyzing the survey data from 173 respondents, our paper found that OITC plays a positive role in moderating the relationship between the use of BDA and agility.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {34–39},
numpages = {6},
keywords = {orientation of interactive team cognition, agility, performance, Big data analytics},
location = {Shanghai, China},
series = {ICIT 2019}
}

@inproceedings{10.1145/3141128.3141149,
author = {Cuzzocrea, Alfredo},
title = {Scalable OLAP-Based Big Data Analytics over Cloud Infrastructures: Models, Issues, Algorithms},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141149},
doi = {10.1145/3141128.3141149},
abstract = {Starting from the combination of two emerging research areas, namely OLAP-based big data analytics tools and Cloud infrastructures, this paper focuses the attention on so-called scalable OLAP-based big data analytics tools, by providing literature overview and two state-of-the-art research contributions of recent years. Acting as fundamental components, these solutions are likely to be integrated in larger OLAP-based big data analytics tools of the future.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {17–21},
numpages = {5},
keywords = {Big Data Analytics, OLAP-Based Big Data Analytics, Cloud and Big Data Computing, Scalable Big Data Analytics},
location = {London, United Kingdom},
series = {ICCBDC 2017}
}

@inproceedings{10.5555/645919.672811,
author = {Patterson, Blake},
title = {The Need for Data Quality},
year = {1993},
isbn = {155860152X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 19th International Conference on Very Large Data Bases},
pages = {709},
series = {VLDB '93}
}

@inproceedings{10.1145/3234664.3234677,
author = {Fei, Xian-hong and Zhai, Cheng-gong and Qiao, Han},
title = {Research on Clothing and Accouterment Security Based on Big Data},
year = {2018},
isbn = {9781450364850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234664.3234677},
doi = {10.1145/3234664.3234677},
abstract = {The research on the Clothing and Accouterment security based on big data plays an important role in deepening the army's reform and strengthening the scientific management of the army. This paper introduces the main influence of big data on the installation security, analyzes the application status of data in the installation security, and puts forward the application assumption of the big data in the installation security.},
booktitle = {Proceedings of the 2018 2nd High Performance Computing and Cluster Technologies Conference},
pages = {19–23},
numpages = {5},
keywords = {Big data, data analysis, clothing and accouterment security},
location = {Beijing, China},
series = {HPCCT '18}
}

@inproceedings{10.1145/3206157.3206167,
author = {Pirouz, Matin and Zhan, Justin},
title = {Optimized Label Propagation Community Detection on Big Data Networks},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206167},
doi = {10.1145/3206157.3206167},
abstract = {Identifying community structures and subnetwork patterns for complex networks provide us with great knowledge about network. Community detection has been getting lots of attention and interest in recent years. The application for such knowledge goes from target marketing to biology, social studies, and physics. The existing algorithms either lack accuracy or are too slow for Big Data graphs. Due to the rise of Big Data graphs, such solutions prove impractical for real-world datasets. In this study, we change the feed system for the Label Propagation algorithm from a random method to a degree-based system. In addition, we introduce a new convergence method that checks the membership for every node and flags them as converged when they meet the requirement. The main contributions of this work are twofold: (i) we optimize the Label Propagation algorithm, improving the accuracy by a factor of two. The results depend on the complexity of the graph; i.e. the denser a graph structure is, the better result the algorithm will achieve. (ii) We solved the inconsistency of identified communities of Label Propagation algorithm. The results are depicted using two well-known metrics known as the Normalized Mutual Information and the Adjusted Rand Index. We present that Optimized Label Propagation has better results in various real-world dataset and artificial datasets.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {57–62},
numpages = {6},
keywords = {Adjusted Rand Index, Clustering, Normalized Mutual Information, Hidden Communities, Node Degree},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/2566486.2568002,
author = {Kontokostas, Dimitris and Westphal, Patrick and Auer, S\"{o}ren and Hellmann, Sebastian and Lehmann, Jens and Cornelissen, Roland and Zaveri, Amrapali},
title = {Test-Driven Evaluation of Linked Data Quality},
year = {2014},
isbn = {9781450327442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2566486.2568002},
doi = {10.1145/2566486.2568002},
abstract = {Linked Open Data (LOD) comprises an unprecedented volume of structured data on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowdsourced or extracted data of often relatively low quality. We present a methodology for test-driven quality assessment of Linked Data, which is inspired by test-driven software development. We argue that vocabularies, ontologies and knowledge bases should be accompanied by a number of test cases, which help to ensure a basic level of quality. We present a methodology for assessing the quality of linked data resources, based on a formalization of bad smells and data quality problems. Our formalization employs SPARQL query templates, which are instantiated into concrete quality test case queries. Based on an extensive survey, we compile a comprehensive library of data quality test case patterns. We perform automatic test case instantiation based on schema constraints or semi-automatically enriched schemata and allow the user to generate specific test case instantiations that are applicable to a schema or dataset. We provide an extensive evaluation of five LOD datasets, manual test case instantiation for five schemas and automatic test case instantiations for all available schemata registered with Linked Open Vocabularies (LOV). One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {747–758},
numpages = {12},
keywords = {linked data, data quality, dbpedia},
location = {Seoul, Korea},
series = {WWW '14}
}

@inproceedings{10.1145/3239438.3239485,
author = {Chien, Ting-Ying and Chen, Chong-Yi and Jin, Guo-Lun and Ting, Hsien-Wei},
title = {Disease Trajectory Visualization System Based on Big Data Analytics},
year = {2018},
isbn = {9781450363891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239438.3239485},
doi = {10.1145/3239438.3239485},
abstract = {Background: For health promotion, recognition of risk factors that may modify health needs is necessary, and people then have the opportunity to change their behavior if they are aware of the risk factors. Big data analysis of health data is emerging as a new trend in research that may lead to discovery of unknown facts, and visualization of data is a convenient way in which to understand the characteristics of the data. This study used the National Health Insurance Research Database (NHIRD) as the basis to construct a visualization model for disease trajectory analysis.Methods: The NHIRD, which includes inpatient expenditure by admission (DD) and ambulatory care expenditure by visit (CD) data, was used in this study. We analyzed the medical care of patients, such as medications, surgery, medical expenditure and total length of stay, and calculated the number of patients who suffered other diseases within the next 6 months, 12 months and 24 months. A Sankey diagram was employed to show the disease trajectory. Based on data analytics, we constructed a system that helps the user to easily understand the disease trajectory.System implementation: The system includes two panels, a user input panel and an output panel. Using the input panel, the user can input basic information, such as gender, age, date and the queried disease. Based on user input and analytical models, the system will show the detailed trajectory for each queried disease, such as medical expenditure, medications and total length of stay in hospital, and uses Sankey diagrams to show the disease trajectory.Conclusions: This study constructed a visualization system based on analysis of the NHIRD. The Taiwan National Health Insurance Administration, Ministry of Health and Welfare, constructed the Health Bank, which contains health data of all individuals. This method may enable prediction of the risks of diseases in the future.},
booktitle = {Proceedings of the 2nd International Conference on Medical and Health Informatics},
pages = {73–75},
numpages = {3},
keywords = {National Health Insurance Research Database, Decision-making system, Big Data},
location = {Tsukuba, Japan},
series = {ICMHI '18}
}

@inproceedings{10.1145/3386723.3387826,
author = {Es-Sabery, Fatima and Hair, Abdellatif},
title = {Big Data Solutions Proposed for Cluster Computing Systems Challenges: A Survey},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387826},
doi = {10.1145/3386723.3387826},
abstract = {CCS (Cluster Computing System) is coming to solve the problems of standard technology. Whose, objective is to improve the performance/power efficiency of a single processor for storing and mining the large data sets, using the parallel programming to read and process the massive data sets on multiple disks and CPUs. The thing which makes these systems somewhat performant than the standard technology is the physical organization of computing nodes in the cluster. Currently, this kind of cluster does not entirely solve the problem because it comes with its challenges, which are Node failures, Computations, Network Bottleneck, and Distributed programming. All these problems are coming when we are mining and storing the massive volume of data using cluster computing. To solve these challenges, Google invented a new Big Data framework of data processing called MapReduce, to manage large scale data processing across large clusters of commodity servers. The paper outlines the running of CCS and presents its challenges in this era of Big Data. Moreover, it introduces the most popular Big Data solutions proposed to overcome the CCS challenges. Also, it shows how Big Data technologies solve CCS issues. Generally, the main goal of this work is to provide a better understanding of the challenges of CCS and identify the essential big data solutions in this increasingly important area.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {7},
numpages = {7},
keywords = {MapReduce, Challenges, Big Data, Distributed File System, CCS},
location = {Marrakech, Morocco},
series = {NISS2020}
}

@article{10.1145/2331042.2331062,
author = {Yang, Edward Z. and Simmons, Robert J.},
title = {Profile Jeff Dean Big Data at Google},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331062},
doi = {10.1145/2331042.2331062},
journal = {XRDS},
month = {sep},
pages = {69},
numpages = {1}
}

@inproceedings{10.1145/3305275.3305330,
author = {Ou, Xiu-ying},
title = {Research on University Education Management System Based on Big Data},
year = {2018},
isbn = {9781450365703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3305275.3305330},
doi = {10.1145/3305275.3305330},
abstract = {In recent years, information technology has achieved rapid development and has been widely used in many fields. With the continuous improvement of the level of higher education and the scale of education in China, colleges and universities have paid more and more attention to the use of information technology in education management. Many universities have carried out research work on education management systems based on big data. It summarizes the related theories of big data, analyzes the importance of big data, and puts forward effective solutions according to the application status of big data in college education management system. In the overall system architecture, database security, system requirements, After detailed requirements analysis of system functions and other aspects, a system solution combining flexibility, openness and applicability was developed. For the domestic educational management systems, most of them are based on C/S or B/S single mode, they are difficult to meet the system solution requirements proposed in this paper. The system developed in this paper proposes a combination of C/S and B/S modes. Oracle and PL/SQL are used as the back-end database. The front-end development tools use Delphi2009, ASP.net, PL/SQL Developer and auxiliary software Dreamweaver5.0.},
booktitle = {Proceedings of the International Symposium on Big Data and Artificial Intelligence},
pages = {275–280},
numpages = {6},
keywords = {college education, C/S and B/S, Big data, management system, impact countermeasures},
location = {Hong Kong, Hong Kong},
series = {ISBDAI '18}
}

@inproceedings{10.1145/2896825.2896835,
author = {Guerriero, Michele and Tajfar, Saeed and Tamburri, Damian A. and Di Nitto, Elisabetta},
title = {Towards a Model-Driven Design Tool for Big Data Architectures},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896835},
doi = {10.1145/2896825.2896835},
abstract = {Big Data technologies are rapidly becoming a key enabler for modern industries. However, the entry costs inherent to "going Big" are considerable, ranging from learning curve, renting/buying infrastructure, etc. A key component of these costs is the time spent on learning about and designing with the many big data frameworks (e.g., Spark, Storm, HadoopMR, etc.) on the market. To reduce said costs while decreasing time-to-market we advocate the usage of Model-Driven Engineering (MDE), i.e., software engineering by means of models and their automated manipulation. This paper outlines a tool architecture to support MDE for big data applications, illustrating with a case-study.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {37–43},
numpages = {7},
keywords = {model transformation, MDE, design tool, meta-models, big data applications design, architecture framework},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/3402569.3409041,
author = {Han, Ping},
title = {Research on Foreign Exchange Management Model Based on Big Data},
year = {2020},
isbn = {9781450377546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402569.3409041},
doi = {10.1145/3402569.3409041},
abstract = {The rapid development of Internet technology has promoted the process of economic globalization, and the application of big data technology has injected new vitality into the innovation of foreign exchange management models. Businesses such as foreign exchange deposits and loans and foreign currency exchange have encountered new development opportunities. Under the effect of big data technology, it can quickly process massive amounts of information, respond to various exchange rate changes, and achieve the improvement of foreign exchange business management. Especially under the circumstances of the current RMB marketization, exchange rate reform and the diversification of the international situation, the difficulty of foreign exchange management is gradually increasing. How to better improve the efficiency of foreign exchange management has become a problem that must be solved at present. Therefore, it is of great significance to explore the foreign exchange management model based on the background of big data, build a big data computing mechanism, give full play to its advantages in foreign exchange management, and promote the improvement of foreign exchange management.},
booktitle = {Proceedings of the 5th International Conference on Distance Education and Learning},
pages = {162–165},
numpages = {4},
keywords = {mode, Big data background, foreign exchange management},
location = {Beijing, China},
series = {ICDEL 2020}
}

@inproceedings{10.1145/2818869.2818881,
author = {Yongmei, Wei and Fengmin, Chen and Cher, Lim Khai},
title = {Large LDPC Codes for Big Data Storage},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818881},
doi = {10.1145/2818869.2818881},
abstract = {Current distributed storage systems mainly rely on data replication to ensure certain level of data availability and reliability. But in scenarios, like data archiving, replication is not cost effective and does not provides a robust solution to prevent data loss. A recent trend is to introduce erasure codes into the distributed storage. Inspired by the RAID system, early attempts have been focused on designing Reed-Solomon (R-S) based solutions with small block sizes. This paper investigates in details about repair traffic to apply Low Density Parity Check (LDPC) codes with relatively large block sizes. It has been demonstrated that the LDPC codes have unique advantages over R-S based solutions including low repair traffic for multiple erasures and parity erasures. The LDPC-based method is integrated with the Hadoop system with various configurations. Both theoretical analysis and simulations show that significant improvement in reliability can be achieved through using large LDPC codes without increasing the repair latency and network traffic especially for multiple erasures. Simulations also show great improvement in terms repairing latencies compared with Reed-Solomon codes. The latency is further improved through parallelism by engaging map-reduce processes from Hadoop.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {1},
numpages = {6},
keywords = {Redundancy, Reliability, Distributed storage, LDPC codes, Reed-Solomon codes, Erasure codes},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@proceedings{10.1145/2757384,
title = {Mobidata '15: Proceedings of the 2015 Workshop on Mobile Big Data},
year = {2015},
isbn = {9781450335249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2015 ACM Workshop on Mobile Big Data -- Mobidata'15. This workshop aims to foster the exchange of new ideas in the synergy of mobile computing and big data research. We have selected 13 papers to be presented at the workshop. We hope the workshop will facilitate the discussion of the future research directions in mobile big data.},
location = {Hangzhou, China}
}

@inproceedings{10.1145/3017680.3017705,
author = {Eickholt, Jesse and Shrestha, Sharad},
title = {Teaching Big Data and Cloud Computing with a Physical Cluster},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3017705},
doi = {10.1145/3017680.3017705},
abstract = {Cloud Computing and Big Data continue to be disruptive forces in computing and have made inroads in the Computer Science curriculum, with courses in Cloud Computing and Big Data being routinely offered at the graduate and undergraduate level. One major challenge in offering courses in Big Data and Cloud Computing is resources. The question is how to provide students with authentic experiences making use of current Cloud and Big Data resources and tools and do so in a cost effective manner. Historically, three options, namely physical clusters, virtual clusters and cloud-based clusters, have been used to support Big Data and Cloud Computing courses. Virtual clusters and cloud-based options are those that institutions have typically adopted and many arguments in favor of these options exist in the literature, citing cost and performance. Here we argue that teaching Big Data and Cloud Computing courses can be done making use of a physical cluster and that many of the existing arguments fail to take into account many important factors in their calculations. These factors include the flexibility and control of a physical cluster in responding to changes in industry, the ability to work with much larger datasets, and the synergy and broad applicability of an appropriately equipped physical cluster for courses such as Cloud Computing, Big Data and Data Mining. We present three possible configurations of a physical cluster which span the spectrum in terms of cost and provide cost comparisons of these configurations against virtual and cloud-based options, taking into account the unique requirements of an academic setting. While limitations do exist with a physical cluster and it is not an option for all situations, our analysis and experience indicates that there is great value in using a physical cluster to support teaching Cloud Computing and Big Data courses and it should not be dismissed.},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {177–181},
numpages = {5},
keywords = {cloud computing, big data, computing cluster},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@inproceedings{10.1145/3093338.3093372,
author = {Jimenez-Ruiz, Ivan and Gonzalez-Mendez, Ricardo and Ropelewski, Alexander},
title = {Optimizing High Performance Big Data Cancer Workflows},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3093372},
doi = {10.1145/3093338.3093372},
abstract = {Appropriate optimization of bioinformatics workflows is vital to improve the timely discovery of variants implicated in cancer genomics. Sequenced human brain tumor data was assembled to optimize tool implementations and run various components of RNA sequence (RNA-seq) workflows. The measurable information produced by these tools account for the success rate and overall efficiency of a standardized and simultaneous analysis. We used the National Center for Biotechnology Information) Sequence Read Archive (NCBI-SRA) database to retrieve two transcriptomic datasets containing over 104 million reads as input data. We used these datasets to benchmark various file systems on the Bridges supercomputer to improve overall workflow throughput. Based on program and job timings, we report critical recommendations on selections of appropriate file systems and node types to efficiently execute these workflows.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {45},
numpages = {4},
keywords = {Supercomputing, ACM proceedings, Memory, Workflows, Transcriptome, File Systems, Bioinformatics, Performance, Genome, Timings},
location = {New Orleans, LA, USA},
series = {PEARC17}
}

@article{10.1145/3156818,
author = {Bruno, Rodrigo and Ferreira, Paulo},
title = {A Study on Garbage Collection Algorithms for Big Data Environments},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3156818},
doi = {10.1145/3156818},
abstract = {The need to process and store massive amounts of data—Big Data—is a reality. In areas such as scientific experiments, social networks management, credit card fraud detection, targeted advertisement, and financial analysis, massive amounts of information are generated and processed daily to extract valuable, summarized information. Due to its fast development cycle (i.e., less expensive to develop), mainly because of automatic memory management, and rich community resources, managed object-oriented programming languages (e.g., Java) are the first choice to develop Big Data platforms (e.g., Cassandra, Spark) on which such Big Data applications are executed.However, automatic memory management comes at a cost. This cost is introduced by the garbage collector, which is responsible for collecting objects that are no longer being used. Although current (classic) garbage collection algorithms may be applicable to small-scale applications, these algorithms are not appropriate for large-scale Big Data environments, as they do not scale in terms of throughput and pause times.In this work, current Big Data platforms and their memory profiles are studied to understand why classic algorithms (which are still the most commonly used) are not appropriate, and also to analyze recently proposed and relevant memory management algorithms, targeted to Big Data environments. The scalability of recent memory management algorithms is characterized in terms of throughput (improves the throughput of the application) and pause time (reduces the latency of the application) when compared to classic algorithms. The study is concluded by presenting a taxonomy of the described works and some open problems, with regard to Big Data memory management, that could be addressed in future works.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {20},
numpages = {35},
keywords = {memory managed runtime, Java, Garbage collection, storage platform, scalability, Big Data environment, Big Data, processing platforms}
}

@inproceedings{10.5555/1564131.1564137,
author = {Hsueh, Pei-Yun and Melville, Prem and Sindhwani, Vikas},
title = {Data Quality from Crowdsourcing: A Study of Annotation Selection Criteria},
year = {2009},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Annotation acquisition is an essential step in training supervised classifiers. However, manual annotation is often time-consuming and expensive. The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates. In this paper, we consider the difficult problem of classifying sentiment in political blog snippets. Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined. Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty. Analysis confirm the utility of these criteria on improving data quality. We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency.},
booktitle = {Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing},
pages = {27–35},
numpages = {9},
location = {Boulder, Colorado},
series = {HLT '09}
}

@inproceedings{10.1145/3377571.3377600,
author = {Zihaoran, Wang},
title = {Research on Motivation and Regulation of Big Data "Slaughter" Behavior},
year = {2020},
isbn = {9781450372947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377571.3377600},
doi = {10.1145/3377571.3377600},
abstract = {Big data "killing" as a new manifestation of price discrimination in the era of big data refers to the service provider as a party with information superiority, using the large amount of customer information flow to distinguish the pricing of each individual consumer. Consumer surplus value may be captured, which seriously reduces the level of consumer welfare. Internet companies with monopoly status such as Amazon, Didi, and Orbitz have all been expelled to adopt a pricing strategy of big data. However, the existing literature mainly discusses how to regulate big data at the legal level, and there is less discussion about big data killing itself as a kind of primary price discrimination phenomenon. This article aims to address the following two questions: 1) What is the motivation behind the vendor's big data killing strategy? 2) How to regulate from the perspective of network economics for big data killing? In the discussion of the first problem, using the mathematical modeling method to start the profit maximization by the monopolist, the local aggregation coefficient in the network is used to describe the risk of the manufacturer adopting the big data killing strategy, so that it is not adopted. The profit of the killing strategy and the expected profit of adopting the killing strategy are the motivation of the manufacturer to adopt the big data killing strategy. In the discussion of the second question, taking the comparison between the expected profit of the killing strategy and the profit under normal operation as the starting point, explore how to use the degree of communication between consumers and rationally introduce the competitive market to smash the big data. Conduct regulation. Finally, this paper studies the interdisciplinary issue and provides the relevant government departments with the idea of regulating big data killing strategies under the perspective of network economics.},
booktitle = {Proceedings of the 2020 11th International Conference on E-Education, E-Business, E-Management, and E-Learning},
pages = {407–411},
numpages = {5},
keywords = {Big data killing, full price discrimination, consumer surplus, clustering coefficient},
location = {Osaka, Japan},
series = {IC4E 2020}
}

@article{10.1145/3158344,
author = {Kowolenko, Michael and Vouk, Mladen A.},
title = {Developing an Open Source 'Big Data' Cognitive Computing Platform: Big Data (Ubiquity Symposium)},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2018},
number = {March},
url = {https://doi.org/10.1145/3158344},
doi = {10.1145/3158344},
abstract = {The ability to leverage diverse data types requires a robust and dynamic approach to systems design. The needs of a data scientist are as varied as the questions being explored. Compute systems have focused on the management and analysis of structured data as the driving force of analytics in business. As open source platforms have evolved, the ability to apply compute to unstructured information has exposed an array of platforms and tools available to the business and technical community. We have developed a platform that meets the needs of the analytics user requirements of both structured and unstructured data. This analytics workbench is based on acquisition, transformation, and analysis using open source tools such as Nutch, Tika, Elastic, Python, PostgreSQL, and Django to implement a cognitive compute environment that can handle widely diverse data, and can leverage the ever-expanding capabilities of infrastructure in order to provide intelligence augmentation.},
journal = {Ubiquity},
month = {mar},
articleno = {2},
numpages = {15}
}

@proceedings{10.1145/2928294,
title = {SBD '16: Proceedings of the International Workshop on Semantic Big Data},
year = {2016},
isbn = {9781450342995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The current World-Wide Web enables an easy, instant access to a vast amount of online information. However, the content in the Web is typically for human consumption, and is not tailored for machine processing. The Semantic Web is hence intended to establish a machine-understandable Web, and is currently also used in many other domains and not only in the Web. The World Wide Web Consortium (W3C) has developed a number of standards around this vision. Among them is the Resource Description Framework (RDF), which is used as the data model of the Semantic Web. The W3C has also defined SPARQL as the RDF query language, RIF as the rule language, and the ontology languages RDFS and OWL to describe schemas of RDF. The usage of common ontologies increases interoperability between heterogeneous data sets, and the proprietary ontologies with the additional abstraction layer facilitate the integration of these data sets. Therefore, we can argue that the Semantic Web is ideally designed to work in heterogeneous Big Data environments.We define Semantic Big Data as the intersection of Semantic Web data and Big Data. There are masses of Semantic Web data freely available to the public - thanks to the efforts of the linked data initiative. According to http://stats.lod2.eu/ the current freely available Semantic Web data is approximately 90 billion triples in over 3,300 datasets, many of which are accessible via SPARQL query servers called SPARQL endpoints. Everyone can submit SPARQL queries to SPARQL endpoints via a standardized protocol, where the queries are processed on the datasets of the SPARQL endpoints and the query results are sent back in a standardized format. Hence, not only Semantic Big Data is freely available, but also distributed execution environments for Semantic Big Data are freely accessible. This makes the Semantic Web an ideal playground for Big Data research.The goal of this workshop is to bring together academic researchers and industry practitioners to address the challenges and report and exchange the research findings in Semantic Big Data, including new approaches, techniques and applications, make substantial theoretical and empirical contributions to, and significantly advance the state of the art of Semantic Big Data.},
location = {San Francisco, California}
}

@inproceedings{10.1145/3234825.3234843,
author = {Li, Xin},
title = {Research of Wisdom and Lifelong Education Based on Big Data},
year = {2018},
isbn = {9781450364409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234825.3234843},
doi = {10.1145/3234825.3234843},
abstract = {In recent years, big data has been infiltrating into all sectors of our society. Big data technology has brought revolutionary impact to every field, and is becoming the driving force and booster of disruptive innovation in all walks of life. Based on the application of big data in wisdom- lifelong education, this paper studies the impact of big data era on the development of lifelong education.First, the literature analysis software "CiteSpace" was used to explore the status of life education research at home and abroad. Through the study, it was found that the era of big data has brought a profound impact on the development of lifelong education. The main manifestation is that online education in the era of big data brings opportunities to reduce education costs and achieve educational equity. The conclusion is that we can make full use of the opportunity of big data education development.},
booktitle = {Proceedings of the 3rd International Conference on Information and Education Innovations},
pages = {35–39},
numpages = {5},
keywords = {cloud computing, teaching students in accordance with their aptitude, education revolution, wisdom education, big data, life-long education},
location = {London, United Kingdom},
series = {ICIEI '18}
}

@inproceedings{10.5555/2819009.2819232,
author = {Baresi, Luciano and Menzies, Tim and Metzger, Andreas and Zimmermann, Thomas},
title = {1st International Workshop on Big Data Software Engineering (BIGDSE 2015)},
year = {2015},
publisher = {IEEE Press},
abstract = {Big Data is about extracting valuable information from data in order to use it in intelligent ways such as to revolutionize decision-making in businesses, science and society. BIGDSE 2015 discusses the link between Big Data and software engineering and critically looks into issues such as cost-benefit of big data.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {965–966},
numpages = {2},
keywords = {software engineering, big data, software analytics},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.14778/2733004.2733071,
author = {Li, Yunyao and Liu, Ziyang and Zhu, Huaiyu},
title = {Enterprise Search in the Big Data Era: Recent Developments and Open Challenges},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733071},
doi = {10.14778/2733004.2733071},
abstract = {Enterprise search allows users in an enterprise to retrieve desired information through a simple search interface. It is widely viewed as an important productivity tool within an enterprise. While Internet search engines have been highly successful, enterprise search remains notoriously challenging due to a variety of unique challenges, and is being made more so by the increasing heterogeneity and volume of enterprise data. On the other hand, enterprise search also presents opportunities to succeed in ways beyond current Internet search capabilities. This tutorial presents an organized overview of these challenges and opportunities, and reviews the state-of-the-art techniques for building a reliable and high quality enterprise search engine, in the context of the rise of big data.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1717–1718},
numpages = {2}
}

@inproceedings{10.1145/2258056.2258058,
author = {Shekhar, Shashi and Gunturi, Viswanath and Evans, Michael R. and Yang, KwangSoo},
title = {Spatial Big-Data Challenges Intersecting Mobility and Cloud Computing},
year = {2012},
isbn = {9781450314428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2258056.2258058},
doi = {10.1145/2258056.2258058},
abstract = {Increasingly, location-aware datasets are of a size, variety, and update rate that exceeds the capability of spatial computing technologies. This paper addresses the emerging challenges posed by such datasets, which we call Spatial Big Data (SBD). SBD examples include trajectories of cellphones and GPS devices, vehicle engine measurements, temporally detailed road maps, etc. SBD has the potential to transform society via next-generation routing services such as eco-routing. However, the envisaged SBD-based next-generation routing services pose several significant challenges for current routing techniques. SBD magnifies the impact of partial information and ambiguity of traditional routing queries specified by a start location and an end location. In addition, SBD challenges the assumption that a single algorithm utilizing a specific dataset is appropriate for all situations. The tremendous diversity of SBD sources substantially increases the diversity of solution methods. Newer algorithms may emerge as new SBD becomes available, creating the need for a flexible architecture to rapidly integrate new datasets and associated algorithms.},
booktitle = {Proceedings of the Eleventh ACM International Workshop on Data Engineering for Wireless and Mobile Access},
pages = {1–6},
numpages = {6},
keywords = {mobility services, spatial big data, data mining},
location = {Scottsdale, Arizona},
series = {MobiDE '12}
}

@proceedings{10.1145/3323878,
title = {SBD '19: Proceedings of the International Workshop on Semantic Big Data},
year = {2019},
isbn = {9781450367660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goal of this workshop is to bring together academic researchers and industry practitioners to address the challenges and report and exchange the research findings in Semantic Big Data, including new approaches, techniques and applications, make substantial theoretical and empirical contributions to, and significantly advance the state of the art of Semantic Big Data.},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/3236461.3241967,
author = {Barham, Husam and Daim, Tugrul},
title = {Identifying Critical Issues in Smart City Big Data Project Implementation},
year = {2018},
isbn = {9781450357869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236461.3241967},
doi = {10.1145/3236461.3241967},
abstract = {Many cities across the globe are adopting smart city initiatives, as smart city holds the promise of better quality of life and equity for city's residents, more efficient use of city's infrastructure, and more effective city planning. Big data analytics is the backbone of smart city and the drive engine to achieve smart city's promises. However, statistics indicate that more than 50% of big data projects fail; they either never finish or do not offer the expected value. Resulting in severe consequences as such projects tends to be expensive and require allocating the organization's best resources while doing the project. This is even more crucial in the case of smart city, as cities usually have limited budget and resources.This paper conducted literature review and perspectives analysis to identify challenges, which can cause big data projects to fail, with focus on smart city related big data projects. The goal is to offer a list of challenges, that a project manager can consider as an initial list of risks for the upcoming project, and evaluate the city's readiness against each of them.},
booktitle = {Proceedings of the 1st ACM/EIGSCC Symposium on Smart Cities and Communities},
articleno = {1},
numpages = {9},
keywords = {risk and challenges, smart cities, Big data, project management},
location = {Portland, OR, USA},
series = {SCC '18}
}

@inproceedings{10.1145/3178442.3178447,
author = {Ertel, Sebastian and Adam, Justus and Castrillon, Jeronimo},
title = {Supporting Fine-Grained Dataflow Parallelism in Big Data Systems},
year = {2018},
isbn = {9781450356459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178442.3178447},
doi = {10.1145/3178442.3178447},
abstract = {Big data systems scale with the number of cores in a cluster for the parts of an application that can be executed in data parallel fashion. It has been recently reported, however, that these systems fail to translate hardware improvements, such as increased network bandwidth, into a higher throughput. This is particularly the case for applications that have inherent sequential, computationally intensive phases. In this paper, we analyze the data processing cores of state-of-the-art big data systems to find the cause for these scalability problems. We identify design patterns in the code that are suitable for pipeline and task-level parallelism, potentially increasing application performance. As a proof of concept, we rewrite parts of the Hadoop MapReduce framework in an implicit parallel language that exploits this parallelism without adding code complexity. Our experiments on a data analytics workload show throughput speedups of up to 3.5x.},
booktitle = {Proceedings of the 9th International Workshop on Programming Models and Applications for Multicores and Manycores},
pages = {41–50},
numpages = {10},
location = {Vienna, Austria},
series = {PMAM'18}
}

@inproceedings{10.1145/3284179.3284206,
author = {Mart\'{\i}nez-Abad, Fernando and Gamazo, Adriana and Rodr\'{\i}guez-Conde, Mar\'{\i}a Jos\'{e}},
title = {Big Data in Education: Detection of ICT Factors Associated with School Effectiveness with Data Mining Techniques},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284206},
doi = {10.1145/3284179.3284206},
abstract = {The search for non-contextual process factors associated with school effectiveness has become a line of research with very broad dissemination, primarily from the ubiquity of research that includes analysis of secondary data from large-scale evaluations. With the aim of detecting ICT factors related to school effectiveness from the Spanish sample of the PISA 2015 evaluation, this work applies statistical techniques of data mining, specifically decision trees, for the optimization of the process. The results demonstrate that, while the availability and excessive use of ICTs, both in the educational environment and in the home, are factors associated with low effectiveness, other more personal characteristics of the pupils, such as their perception of self-efficacy in the management of ICT, or their own interest in the use of technologies, can be considered as factors in the protection of school effectiveness. We conclude by discussing the relationship between school effectiveness and academic performance through the analysis of previous studies, focusing on the common elements detected within both perspectives.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {145–150},
numpages = {6},
keywords = {Data Mining, School Effectiveness, Information and Communication Technologies, Educational Evaluation, Secondary Education},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1145/3348400.3348414,
author = {Karim, Shakir and Gide, Ergun and Sandu, Raj},
title = {The Impact of Big Data on Health Care Services in Australia: Using Big Data Analytics to Categorise and Deal with Patients},
year = {2019},
isbn = {9781450371674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3348400.3348414},
doi = {10.1145/3348400.3348414},
abstract = {Big Data is the biggest emerging trend and promise in today's technology-driven world. It is continuing to create a lot of buzz in not only the field of technology, but across the world. It promises substantial involvements, vast changes, modernizations, and integration with and within people's ongoing life. It makes the world more demanding and helps with making prompt and appropriate decisions in real time. This paper aims to provide a comprehensive analysis of the health industry and health care system in Australia that are relevant to the consequences formed by Big Data. This paper primarily uses a secondary research analysis method to provide a wide-ranging investigation into the positive and negative consequences of health issues relevant to Big Data, the architects of those consequences, and those overstated by the consequences. The secondary resources are subject to journal articles, reports, conference proceedings, media articles, corporation-based documents, blogs and other appropriate information. In the future, the investigation will continue by employing Mixed Methodology (Qualitative and Quantitative) in relation to Big Data usage in the Australian Health industry. The paper initially finds that Big Data is an evidence source in health care and provides useful insight into the Australian healthcare system. It is steadily reducing the cost of the Australian healthcare system and improving patients' outcomes in Australia. Big data can not only improve the affairs between public and health enterprises, but can also make life better by increasing efficiency and modernization.},
booktitle = {Proceedings of the 2019 International Conference on Mathematics, Science and Technology Teaching and Learning},
pages = {34–38},
numpages = {5},
keywords = {High Risk and High Cost Patients, Health Care System, Australian Health Care Services, Big Data (BD)},
location = {Sydney, NSW, Australia},
series = {ICMSTTL 2019}
}

@inproceedings{10.5555/3172795.3172829,
author = {Huang, Yu and Chiang, Fei and Maier, Albert and Petitclerc, Martin and Saillet, Yannick and Spisic, Damir and Zuzarte, Calisto},
title = {Quantifying Duplication to Improve Data Quality},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Deduplication is a costly and tedious task that involves identifying duplicate records in a dataset. High duplication rates lead to poor data quality, where data ambiguity occurs as to whether two records refer to the same entity. Existing deduplication techniques compare a set of attribute values, and verify whether given similarity thresholds are satisfied. While potential duplicate records are identified, these techniques do not provide users with any information about the degree of duplication, i.e., the varying levels of closeness among the attribute values and between records that define the duplicates.In this paper, we present a duplication metric that quantifies the level of duplication for an attribute value, and within an attribute. This metric can be used by analysts to understand the distribution and similarity of values during the data cleaning process. We present a deduplication framework that differentiates terms during similarity matching step, and is agnostic to the ordering of values within a record. We compare our framework against two existing approaches, and show that we achieve improved accuracy and performance over real data collections.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {272–278},
numpages = {7},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/3349341.3349371,
author = {Li, Jiale and Liao, Shunbao},
title = {Quality Control Framework of Big Data for Early Warning of Agricultural Meteorological Disasters},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349371},
doi = {10.1145/3349341.3349371},
abstract = {Agricultural meteorological disasters, including floods, droughts, dry hot winds, low temperature chills, typhoons, hail and continuous rain, can lead to significant reduction in agricultural output. Big data platform for early warning of agricultural meteorological disaster is the basis of business operation system for early warning of agricultural meteorological disasters, and the data quality is an important guarantee for success of the early warning. Quality control of big data for early warning of agricultural meteorological disaster involves names of data sets, metadata, data documents and content of data sets. The quality control for contents of data sets is divided into quality control of attribute data and that of spatial data, and quality control of spatial data is divided into quality control of vector data and that of raster data. Methods for data quality control are divided into fully automatic, semi-automatic and full manual control methods.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {74–78},
numpages = {5},
keywords = {agro-meteorological disasters, big data, framework, early warning, quality control},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3424978.3425001,
author = {Wang, Guotian and Liu, Xu},
title = {Research on University Security Big Data and Emergency Collaborative Disposal},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425001},
doi = {10.1145/3424978.3425001},
abstract = {With the frequent occurrence of university security accidents in recent years, university security issues have become a hot topic of concern to society and research. Currently for university security, the most serious problems are the inability to response to emergencies timely due to the short of real-time data acquisition and processing devices. For these existing issues on the aspects of university security mentioned above, this paper focuses research on the university security big data and emergency collaborative disposal, the university big data integration system is established firstly for university security big data real-time collection, processing, analysis and storage, then the UML based multi-department emergency collaborative disposal organization structure is proposed to realize the efficient and coordinated emergency response operations of multi-department when emergencies occur, meanwhile, the emergency disposal and supplies dispatch scheme is designed to optimize relief supplies distribution and emergency response results. Lastly, the university security big data and emergency coordination system is implemented, and accuracy experiment of the system is conducted, the results of system experiment and implementation are not only indicate the effectiveness and feasibility of the methods proposed by this paper, but also proved to be capable of general applicability to university security management.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {23},
numpages = {8},
keywords = {Collaborative Disposal, Big Data, University Security},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.1145/2818302.2818308,
author = {Nguyen, Khanh and Fang, Lu and Xu, Guoqing and Demsky, Brian},
title = {Speculative Region-Based Memory Management for Big Data Systems},
year = {2015},
isbn = {9781450339421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818302.2818308},
doi = {10.1145/2818302.2818308},
abstract = {Most real-world Big Data systems are written in managed languages. These systems suffer from severe memory problems due to the massive volumes of objects created to process input data. Allocating and deallocating a sea of objects puts a severe strain on the garbage collector, leading to excessive GC efforts and/or out-of-memory crashes. Region-based memory management has been recently shown to be effective to reduce GC costs for Big Data systems. However, all existing region-based techniques require significant user annotations, resulting in limited usefulness and practicality. This paper reports an ongoing project, aiming to design and implement a novel speculative region-based technique that requires only minimum user involvement. In our system, objects are allocated speculatively into their respective regions and promoted into the heap if needed. We develop an object promotion algorithm that scans regions for only a small number of times, which will hopefully lead to significantly improved memory management efficiency. We also present an OpenJDK-based implementation plan and an evaluation plan.},
booktitle = {Proceedings of the 8th Workshop on Programming Languages and Operating Systems},
pages = {27–32},
numpages = {6},
keywords = {big data systems, performance optimization, language, region-based memory management, managed languages},
location = {Monterey, California},
series = {PLOS '15}
}

@inproceedings{10.1145/3194452.3194458,
author = {Aboagye, Emelia Opoku and James, Gee C. and Jianbin, Gao and Kumar, Rajesh and Khan, Riaz Ullah},
title = {Probabilistic Time Context Framework for Big Data Collaborative Recommendation},
year = {2018},
isbn = {9781450364195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194452.3194458},
doi = {10.1145/3194452.3194458},
abstract = {A parallel scheme based on Probabilistic Tensor Factorization which addresses the scalability problem of Collaborative Filtering (CF) is proposed for big data processing. Parallel algorithms for large scale recommendation problems have witnessed advancements in the big data era in recent times. Matrix Factorization models have been enormously used to tackle such constraints, which we see as not scalable and does not converge easily unless numerous iterations making it computationally expensive. This study proposes a novel coordinate descent based probabilistic Tensor factorization method; Scalable Probabilistic Time Context Tensor Factorization (SPTTF) for collaborative recommendation. Our experiments with natural datasets show its efficiency.},
booktitle = {Proceedings of the 2018 International Conference on Computing and Artificial Intelligence},
pages = {118–121},
numpages = {4},
keywords = {SPTTF, tensor, algorithm integration, Time contest},
location = {Chengdu, China},
series = {ICCAI 2018}
}

@inproceedings{10.1145/3281375.3281382,
author = {Bieh-Zimmert, Oliver and Felden, Carsten},
title = {A Knowledge Discovery in Community Contributions of Big Data Technologies},
year = {2018},
isbn = {9781450356220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281375.3281382},
doi = {10.1145/3281375.3281382},
abstract = {The increasing variety of big data technologies in open source communities is challenging organizations to generate value from those advancements. The technology landscape is missing an overall perspective that clarifies the fragmented understanding of technologies, unpredictable lifecycles, and the unknown adoption for organizations to enable their business with useful technologies. More than one million contributions of features, bugs, and changes were pushed on public available code repositories to develop big data technologies with hidden understanding of the underlying data basis. Using this source could help to identify insights about technological domains as well as their adoption process of contributors to new uprising big data technologies. A knowledge discovery process provided the potential to analyze 269 big data technologies regarding their contribution behavior of over 21,000 contributors. As a result, investigations show an ecosystem of structuring big data technologies based on dynamic contributor networks that have implications on organizations adoption.},
booktitle = {Proceedings of the 10th International Conference on Management of Digital EcoSystems},
pages = {55–58},
numpages = {4},
keywords = {knowledge discovery, open source, big data, adoption},
location = {Tokyo, Japan},
series = {MEDES '18}
}

@inproceedings{10.1109/CCGRID.2018.00052,
author = {Al-Jaroodi, Jameela and Mohamed, Nader},
title = {Service-Oriented Architecture for Big Data Analytics in Smart Cities},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00052},
doi = {10.1109/CCGRID.2018.00052},
abstract = {A smart city has recently become an aspiration for many cities around the world. These cities are looking to apply the smart city concept to improve sustainability, quality of life for residents, and economic development. The smart city concept depends on employing a wide range of advanced technologies to improve the performance of various services and activities such as transportation, energy, healthcare, and education, while at the same time improve the city's resources utilization and initiate new business opportunities. One of the promising technologies to support such efforts is the big data technology. Effective and intelligent use of big data accumulated over time in various sectors can offer many advantages to enhance decision making in smart cities. In this paper we identify the different types of decision making processes involved in smart cities. Then we propose a service-oriented architecture to support big data analytics for decision making in smart cities. This architecture allows for integrating different technologies such as fog and cloud computing to support different types of analytics and decision-making operations needed to effectively utilize available big data. It provides different functions and capabilities to use big data and provide smart capabilities as services that the architecture supports. As a result, different big data applications will be able to access and use these services for varying proposes within the smart city.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {633–640},
numpages = {8},
keywords = {smart city, cloud computing, service-oriented architecture, big data, middleware, fog computing},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1109/SEAMS.2017.20,
author = {Schmid, Sanny and Gerostathopoulos, Ilias and Prehofer, Christian and Bures, Tomas},
title = {Self-Adaptation Based on Big Data Analytics: A Model Problem and Tool},
year = {2017},
isbn = {9781538615508},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2017.20},
doi = {10.1109/SEAMS.2017.20},
abstract = {In this paper, we focus on self-adaptation in large-scale software-intensive distributed systems. The main problem in making such systems self-adaptive is that their adaptation needs to consider the current situation in the whole system. However, developing a complete and accurate model of such systems at design time is very challenging. To address this, we present a novel approach where the system model consists only of the essential input and output parameters. Furthermore, Big Data analytics is used to guide self-adaptation based on a continuous stream of operational data. We provide a concrete model problem and a reference implementation of it that can be used as a case study for evaluating different self-adaptation techniques pertinent to complex large-scale distributed systems. We also provide an extensible tool for endorsing an arbitrary system with self-adaptation based on analysis of operational data coming from the system. To illustrate the tool, we apply it on the model problem.},
booktitle = {Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {102–108},
numpages = {7},
keywords = {self-adaptation, model problem, big data analytics},
location = {Buenos Aires, Argentina},
series = {SEAMS '17}
}

@inproceedings{10.1145/2912152.2912159,
author = {Matsuoka, Satoshi},
title = {Towards Convergence of Extreme Computing and Big Data Centers},
year = {2016},
isbn = {9781450343527},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912152.2912159},
doi = {10.1145/2912152.2912159},
abstract = {Rapid growth in the use cases and demands for extreme computing and huge data processing is leading to convergence of the two infrastructures. Tokyo Tech.'s TSUBAME3.0, a 2017 addition to the highly successful TSUBAME2.5, will aim to deploy a series of innovative technologies, including ultra-efficient liquid cooling and power control, petabytes of non-volatile memory, as well as low cost Petabit-class interconnect. To address the challenges of such technology adoption, proper system architecture, software stack, and algorithm must be desgined and developed; these are being addressed by several of our ongoing research projects as well as prototypes, such as the TSUBAME-KFC/DL prototype which became #1 in the world in power efficiency on the Green500 twice in a row, the Billion-way Resiliency project that is investigating effective methods for future resilient supercomputers, as well as the Extreme Big Data (EBD) project which is looking at co-design development of convergent system stack given future extreme data and computing workloads. We are already successful in developing various algorithms and sottware substrates to manipulate big data elements directly on extreme supercomputers, such as graphs, tables (sort), trees, files, etc. and in fact became #1 in the world on the Graph 500 twice including the latest Nov. 2015 version. Our recent focus is also how to ssupport new workloads in categorizing big data represented by deep learning, and there we are collaborating with several partners such as DENSO to improve the scalability and predictability of such workloads; recent trial allowed scalablity to utilize 1146 GPUs for the entire week for a CNN workload. For TSUBAME3 and 2.5 combined we espect to increase such capabilities to over 80 Petaflops in early 2017, or 7 times faster than the K computer.},
booktitle = {Proceedings of the ACM International Workshop on Data-Intensive Distributed Computing},
pages = {1},
numpages = {1},
keywords = {algorithms, scalability, rapid growth},
location = {Kyoto, Japan},
series = {DIDC '16}
}

@inproceedings{10.1145/3175684.3175687,
author = {Kuschicke, Felix and Thiele, Thomas and Meisen, Tobias and Jeschke, Sabina},
title = {A Data-Based Method for Industrial Big Data Project Prioritization},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175687},
doi = {10.1145/3175684.3175687},
abstract = {The application of Big Data Techniques (BDT) in discrete manufacturing appears to be very promising, considering lighthouse projects in this area. In general, the goal is to collect all data from manufacturing systems comprehensively, in order to enable new findings and decision support by means of appropriate Industrial Big Data (IBD) analysis procedures. However, due to limited human and economic resources, potential IBD projects need to get prioritized -- in the best case according to their cost-benefit ratio. Available methods for this purpose are insufficient, due to their limited ability to be operationalized, error-proneness, and lack of scientific evidence. In this paper, we discuss how cost-benefit-analysis frameworks can be applied to the preliminary selection of production use cases for the implementation of BDT in larger production systems. It supports the use case selection process from information about production needs, available BDT, and given condition(s) per use case. This concept paper attempts to consolidate the hitherto fragmented discourse on how to prioritize IBD projects, evaluates the challenges of prioritization in this field, and presents a prioritization concept to overcome these challenges.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {6–10},
numpages = {5},
keywords = {Manufacturing, Project Prioritization, Industrial Big Data, Framework, Project Selection},
location = {London, United Kingdom},
series = {BDIOT2017}
}

@article{10.1145/2786983,
author = {Bartoli, Alberto and Lorenzo, Andrea De and Medvet, Eric and Tarlao, Fabiano},
title = {Data Quality Challenge: Toward a Tool for String Processing by Examples},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2786983},
doi = {10.1145/2786983},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {13},
numpages = {4},
keywords = {Programming by example, String processing}
}

@inproceedings{10.1145/3372454.3372478,
author = {Puangpontip, Supadchaya and Hewett, Rattikorn},
title = {Assessing Reliability of Big Data Stream for Smart City},
year = {2020},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372478},
doi = {10.1145/3372454.3372478},
abstract = {Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.},
booktitle = {Proceedings of the 2019 3rd International Conference on Big Data Research},
pages = {18–23},
numpages = {6},
keywords = {IoT, Data Reliability, Smart City, Theory of evidence},
location = {Cergy-Pontoise, France},
series = {ICBDR 2019}
}

@inproceedings{10.1145/3322134.3322149,
author = {Yang, Qiang and Li, Jian and Zou, Xiaohui},
title = {Big Data and Higher Vocational and Technical Education: Green Tourism Curriculum},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322149},
doi = {10.1145/3322134.3322149},
abstract = {This paper aims to explore the road to innovation in big data and higher vocational and technical education with the green tourism curriculum as an example. The method is as follows: first, introducing a statistical-based machine learning method to deal with large probability events, secondly, introducing a human-computer interaction interface technology to deal with small probability events, and third, introducing an expert knowledge acquisition technique to deal with special exceptions. It is characterized by a combination of three approaches, focusing on the interdisciplinary, cross-domain and cross-industry smart system construction, and converging to the knowledge module of the green tourism curriculum. The result is: not only highlights the comprehensive innovative concept of the green tourism curriculum, but also forms a smart guide system that combines personalization and standardization, through conceptual maps, knowledge graphs and methodological tools that express scientific principles, combined with typical examples and representative figures and featured scenic spots, and a new paradigm for computer-assisted instruction. The significance lies in: not only is it conducive to the creation of quality courses, but it is also beneficial to the teachers and students to theoretically and practically carry out the characteristics of the green tourism curriculum, namely: a series of problems, difficulties and pain points for international and domestic tourism, forming a reasonable division of labor is necessary to further develop the green tourism curriculum and its supporting smart systems of interpersonal, human-machine, inter-machine, and machine-to-person.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {108–112},
numpages = {5},
keywords = {E-education, Data Management, Big Data Applications},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@inproceedings{10.1145/2675743.2771878,
author = {Zhang, Tianning},
title = {Reliable Event Messaging in Big Data Enterprises: Looking for the Balance between Producers and Consumers},
year = {2015},
isbn = {9781450332866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675743.2771878},
doi = {10.1145/2675743.2771878},
abstract = {Event-driven enterprise IT architectures are typically based on message broker systems. Most broker systems focus on functional decoupling, and on the fast and reliable delivery of events towards event consumers. Such consumer-centricity eases the development and deployment of consumers applications. However, especially within a web-based big data enterprise we typically have relatively small number of stable producers that is based on the core enterprise business model, and a higher, dynamic number of highly heterogeneous consumers. With the consumer-centric view of event messaging, problems with some consumers can easily lead to the congestion or service degradation of the whole messaging system, and can affect the operations of the producers or other critical consumers. To avoid such non-functional tight coupling, we propose in this paper a producer-centric architecture for messaging backbone in a big data enterprise. Within this architecture the broker infrastructure concentrates on the event production activities and pushes the delivery functionalities towards the consumer clients and applications. In this way we can achieve a higher level of SOA loose coupling, higher reliability and higher maintainability of the enterprise infrastructure.},
booktitle = {Proceedings of the 9th ACM International Conference on Distributed Event-Based Systems},
pages = {226–233},
numpages = {8},
keywords = {big data, producer-centricity, event messaging, reliability, NoSQL, event backbone, redis, kafka},
location = {Oslo, Norway},
series = {DEBS '15}
}

