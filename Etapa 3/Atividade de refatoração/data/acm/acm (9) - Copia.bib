@article{10.1145/2378016.2378019,
author = {Cur\'{e}, Olivier},
title = {Improving the Data Quality of Drug Databases Using Conditional Dependencies and Ontologies},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2378016.2378019},
doi = {10.1145/2378016.2378019},
abstract = {Many health care systems and services exploit drug related information stored in databases. The poor data quality of these databases, e.g. inaccuracy of drug contraindications, can lead to catastrophic consequences for the health condition of patients. Hence it is important to ensure their quality in terms of data completeness and soundness.In the database domain, standard Functional Dependencies (FDs) and INclusion Dependencies (INDs), have been proposed to prevent the insertion of incorrect data. But they are generally not expressive enough to represent a domain-specific set of constraints. To this end, conditional dependencies, i.e. standard dependencies extended with tableau patterns containing constant values, have been introduced and several methods have been proposed for their discovery and representation. The quality of drug databases can be considerably improved by their usage.Moreover, pharmacology information is inherently hierarchical and many standards propose graph structures to represent them, e.g. the Anatomical Therapeutic Chemical classification (ATC) or OpenGalen’s terminology. In this article, we emphasize that the technologies of the Semantic Web are adapted to represent these hierarchical structures, i.e. in RDFS and OWL. We also present a solution for representing conditional dependencies using a query language defined for these graph oriented structures, namely SPARQL. The benefits of this approach are interoperability with applications and ontologies of the Semantic Web as well as a reasoning-based query execution solution to clean underlying databases.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {3},
numpages = {21},
keywords = {description logics, Data quality, conditional dependencies}
}

@inproceedings{10.1145/1839379.1839396,
author = {Pham Thi, Thanh Thoa and Helfert, Markus},
title = {Discovering Dynamic Integrity Rules with a Rules-Based Tool for Data Quality Analyzing},
year = {2010},
isbn = {9781450302432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1839379.1839396},
doi = {10.1145/1839379.1839396},
abstract = {Rules based approaches for data quality solutions often use business rules or integrity rules for data monitoring purpose. Integrity rules are constraints on data derived from business rules into a formal form in order to allow computerization. One of challenges of these approaches is rules discovering, which is usually manually made by business experts or system analysts based on experiences. In this paper, we present our rule-based approach for data quality analyzing, in which we discuss a comprehensive method for discovering dynamic integrity rules.},
booktitle = {Proceedings of the 11th International Conference on Computer Systems and Technologies and Workshop for PhD Students in Computing on International Conference on Computer Systems and Technologies},
pages = {89–94},
numpages = {6},
keywords = {data quality analyzing, business rules, data quality, integrity rules},
location = {Sofia, Bulgaria},
series = {CompSysTech '10}
}

@proceedings{10.5555/2757761,
title = {BDC '14: Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing},
year = {2014},
isbn = {9781479918973},
publisher = {IEEE Computer Society},
address = {USA}
}

@inproceedings{10.1145/3220199.3220211,
author = {Song, Mingoo and Choi, Jungin},
title = {Demand-Oriented Energy Big Data Services Using Hadoop-Based Large-Scale Distributed System Platform for District Heating},
year = {2018},
isbn = {9781450364263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220199.3220211},
doi = {10.1145/3220199.3220211},
abstract = {In the energy industry, a variety of Big Data services have been developed and provided for both producers and consumers. These services bring more convenience, reliability stability and a higher operational efficiency in energy control and management. For demand-oriented energy services, the primary objectives of are set to reduce the cost or improve the energy efficiency on the consumption side. The association of a Hadoop-based large-scale distributed system with SCADA has engendered various applications using energy Big Data. Especially, the integration of Spark with Hadoop has promoted the development of demand-oriented energy services for Big Data with growing necessities in the industry. In this paper, we investigate demand-oriented energy Big Data services using a Hadoop-based large-scale distributed system platform for district heating as a case. Moreover, we introduce a social energy Big Data service which can ameliorate energy control and management with the demonstration in district heating.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Computing},
pages = {10–13},
numpages = {4},
keywords = {energy control and management, social energy service, district heating, Big Data analytics, energy Big Data services},
location = {Shenzhen, China},
series = {ICBDC '18}
}

@proceedings{10.1145/2837060,
title = {BigDAS '15: Proceedings of the 2015 International Conference on Big Data Applications and Services},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jeju Island, Republic of Korea}
}

@inproceedings{10.1145/3209281.3209358,
author = {Jun, Daesung and Hagen, Loni and Lee, Eunmi and Lim, Hyewon and Kim, Dongwook},
title = {How Big Data Contributes to the Building of Citizen-Centric Smart Cities: The Case of Namyangju City in Korea},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209358},
doi = {10.1145/3209281.3209358},
abstract = {The recent increase of interest in the 'smart city' has led to many studies on this topic. However, there have been a lack of studies on improving the satisfaction of civil services that address the needs and demands of newly migrated inhabitants. In this study, we conducted a big data analysis of the city of Namyangju, an exemplary 'smart city' in Korea, from 2009 to 2016, regarding the change in population composition by the influx of new migrants and its effect on the civil service complaints. According to the results of this study, there was a statistical significance between the increase of civil service complaints and the change of migrants by age group. This suggests that the preference for civil services is different for each life-cycle stage. Moreover, the increase in the demands of civil services was different between the migrants within and from outside of Namyangju. This means that there is an expectancy disconfirmation depending on how many civil services are previously experienced. The results of this analysis also suggest the policy implications on the role of local government, to implement smart city policy for the enhancement of quality of life of a city.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {80},
numpages = {6},
keywords = {smart city, civil service complaints, life-cycle stage, quality of life, expectancy disconfirmation},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3366174.3366177,
author = {Kaneko, Itaru and Yuda, Emi},
title = {On the Privacy of Genomic Big Data and EHR Standardization and Regulation},
year = {2020},
isbn = {9781450372954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366174.3366177},
doi = {10.1145/3366174.3366177},
abstract = {In this paper, we will first summarize recent situations of Genomic Information and Electronic Health Data (EHR). Firstly, we look at the standardization of Genomic information representation. Then summarize regulations in various countries on the privacy of medical and health information. And at the end, we will also discuss the possible technologies and social practices to empower the privacy of genomic information.},
booktitle = {Proceedings of the 2019 4th International Conference on Biomedical Imaging, Signal Processing},
pages = {12–17},
numpages = {6},
keywords = {regulation, MPEG-G, standard, privacy, HER},
location = {Nagoya, Japan},
series = {ICBSP '19}
}

@inproceedings{10.1145/3358331.3358336,
author = {Jia, Dong-Ming and Yuan, Cun-Feng and Guo, Song and Jiang, Zu-Zhen and Xu, Ding and Wang, Da-An},
title = {Application of "Artificial Intelligence and Big Data" in Sports Rehabilitation for Chinese Judicial Administrative Drug Addicts},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358336},
doi = {10.1145/3358331.3358336},
abstract = {Under the background of "Wisdom Drug Rehabilitation", we introduced "Artificial Intelligence and Big Data" into "exercise rehabilitation" work of drug addicts in judicial administrative system. It is a practical innovation of drug treatment in China. This article will elaborate this innovation of the construction and application of "Exercise Rehabilitation" intelligence platform system. This system will improve mental status, alleviate physical and psychological symptoms, ensure safety in places, lighten the burden of professional police officers, make rapid analysis, make accurate decisions and improve the integrity rate of the addict.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {5},
numpages = {5},
keywords = {judicial administrative, big data, artificial intelligence, rehabilitation training, Sports rehabilitation},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@inproceedings{10.1145/3006299.3006304,
author = {Hassaan, Mohamed and Elghandour, Iman},
title = {A Real-Time Big Data Analysis Framework on a CPU/GPU Heterogeneous Cluster: A Meteorological Application Case Study},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006304},
doi = {10.1145/3006299.3006304},
abstract = {It is important to analyze and predict meteorological phenomena in real-time. Parallel programming by exploiting thousands of threads in GPUs can be efficiently used to speed up the execution of many applications. However, GPUs have limitations when used for processing big data, which can be better analyzed using distributed computing platforms such as Hadoop and Spark. In this paper, we propose DAMB a system that processes streamed data on a heterogeneous cluster of CPUs and GPUs in real-time. The core of DAMB is SparkGPU, a platform that extends Apache Spark to allow it to manage a heterogeneous cluster that has both CPUs and GPUs and to execute tasks on GPUs. DAMB also provides data visualization tools that present the analyzed data in an interactive way in real-time. As a case study, we focus on a meteorological application that analyzes lightening discharges. We show that DAMB can successfully process and analyze the meteorological data streamed to it and visualize the results in real-time on a cluster of size 12 nodes, each is equipped with one or more GPU cards. This is a speedup of two orders of magnitude as compared to a sequential program implementation for the same application.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {168–177},
numpages = {10},
keywords = {heterogeneous clusters, GPU programming, in-memory cluster computing},
location = {Shanghai, China},
series = {BDCAT '16}
}

@inproceedings{10.1145/3377672.3378052,
author = {Quanli, Wang and Chu-jian, Guo},
title = {Research on Evaluation of Innovation and Entrepreneurship Education in Local Universities under the Background of Big Data},
year = {2020},
isbn = {9781450362481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377672.3378052},
doi = {10.1145/3377672.3378052},
abstract = {The arrival of the era of big data has had a profound impact on the education system. As far as local university entrepreneurship education is concerned, in the context of big data, the evaluation model can be better applied to analyze the basic features of entrepreneurship education. Based on the profound influence of big data on entrepreneurship education, this paper analyzes the combination of entrepreneurship education and big data with the sample of entrepreneurship education in local colleges and universities, and analyzes the construction of scientific and reasonable evaluation system of entrepreneurship education.},
booktitle = {Proceedings of the 2019 Annual Meeting on Management Engineering},
pages = {169–173},
numpages = {5},
keywords = {Big data, Local universities, Evaluation of entrepreneurship education},
location = {Kuala Lumpur, Malaysia},
series = {AMME 2019}
}

@inproceedings{10.1145/3018896.3018950,
author = {Cheraghchi, Fatemeh and Iranzad, Arash and Raahemi, Bijan},
title = {Subspace Selection in High-Dimensional Big Data Using Genetic Algorithm in Apache Spark},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018950},
doi = {10.1145/3018896.3018950},
abstract = {In high-dimensional space with large amounts of data, distances between data points tend to become relatively uniform. The notion of the nearest neighbours of a data point thus becomes meaningless, a phenomenon known as "curse of dimensionality." Identifying outliers (data points with statistical characteristics significantly different than the majority of the data) in such a high-dimensional space can be a significant challenge. Mining for outliers in subspaces with relevant attributes is one of approaches for this problem, and identifying these attributes is the main objective of this work. In this paper, we scale a grid-based solution to search for subspaces that are candidates for outlier detection with regard to the subset of features in the subspace. We specify a population and a fitness function for a distributed genetic algorithm to heuristically search the subspaces within the high dimensional data, and find the subspace with maximal sparsity. We designed and implemented our proposed subspace selection algorithm in Apache Spark, a fast in-memory engine for large-scale data processing. The initial experimental results on a large dataset (77,000 records and 1,379 attributes) confirm that our proposed method can identify the most relevant subspaces for outlier detection.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {54},
numpages = {7},
keywords = {big data, apache spark, subspace selection, outlier detection, genetic algorithm, high-dimensional},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.5555/359640.359930,
author = {Bowen, Paul L. and Funk, James D. and Jarke, Matthias and Lee, Yang W. and Wand, Yair and Lee, Yang W.},
title = {Data Quality in Internet Time, Space, and Communities (Panel Session)},
year = {2000},
publisher = {Association for Information Systems},
address = {USA},
booktitle = {Proceedings of the Twenty First International Conference on Information Systems},
pages = {713–716},
numpages = {4},
location = {Brisbane, Queensland, Australia},
series = {ICIS '00}
}

@inproceedings{10.1145/1815695.1815706,
author = {Biller, Koby},
title = {A New Approach to "Storage Management" Restrictions Using the "Data Quality" Concept},
year = {2010},
isbn = {9781605589084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1815695.1815706},
doi = {10.1145/1815695.1815706},
abstract = {Fragmentation of data in storage devices, a phenomenon that has attracted considerable research attention, is a major problem underlying computer slowdown and other unpredictable storage-related symptoms. This poster presents the "Data Quality" approach, which includes an effective new method for measuring the quality of data affected by continuous fragmentation. It also incorporates a novel way to handle the affected data, with clear advantages over the current alternatives of replacing the device or upgrading the computer. As an added benefit, use of this approach allows a general picture to be obtained of the data quality throughout an organization.},
booktitle = {Proceedings of the 3rd Annual Haifa Experimental Systems Conference},
articleno = {8},
numpages = {1},
location = {Haifa, Israel},
series = {SYSTOR '10}
}

@inproceedings{10.1145/3331453.3360976,
author = {Zhou, Guixian and Chen, Kaijian and Tu, Jingmei},
title = {Research on Big Data Open Intelligent Platform of Guizhou Province E-Government Service},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3360976},
doi = {10.1145/3331453.3360976},
abstract = {Government affairs open service platform to integrate distributed and heterogeneous system of information resources, eliminate the "information island" phenomenon, through the open service platform, service background, e-government service management platform, monitoring platform four subsystems to achieve between different institutions, different application system and database based on the different transmission protocol of data exchange, information sharing and business collaboration, thus to provide good data to support the government and the enterprise information construction environment.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {25},
numpages = {5},
keywords = {Big data, E-government service, Open intelligent platform},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/3332186.3332205,
author = {DeFever, Ryan S. and Hanger, Walter and Sarupria, Sapna and Kilgannon, Jon and Apon, Amy W. and Ngo, Linh B.},
title = {Building A Scalable Forward Flux Sampling Framework Using Big Data and HPC},
year = {2019},
isbn = {9781450372275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332186.3332205},
doi = {10.1145/3332186.3332205},
abstract = {Forward flux sampling (FFS) is an established scientific method for sampling rare events in molecular simulations. However, as the difficulty of the scientific problem increases, the amount of data and the number of tasks required for FFS is challenging to manage with traditional scripting tools and languages for high performance computing. The SAFFIRE software framework has been developed to address these challenges. SAFFIRE utilizes Hadoop to manage a large number of tasks and data for large scale FFS simulations. The framework is shown to be highly scalable and able to support large scale FFS simulations. This enables studies of rare events in complex molecular systems on commodity cluster computing systems.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (Learning)},
articleno = {3},
numpages = {8},
keywords = {Forward Flux Sampling, molecular simulations, rare events, data-intensive computing, Hadoop},
location = {Chicago, IL, USA},
series = {PEARC '19}
}

@inproceedings{10.1145/3429889.3429920,
author = {Wu, Ning and Cao, Yanping and Chen, Zhuo and Zhu, Yifan},
title = {Limitation of on Big Data or Nature Language Processing Based Algorithm for Clinical Decision Artificial Intelligence},
year = {2020},
isbn = {9781450388603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429889.3429920},
doi = {10.1145/3429889.3429920},
abstract = {Intelligent clinical decision is an important utility of artificial intelligence. At present, most of its algorithm is based on big data or nature language processing. The limitation of such algorithm is discussed and summarized. That clinical decision artificial intelligence should meet the requirements of clinical medicine and artificial intelligence technique is proposed.},
booktitle = {Proceedings of the 2020 International Symposium on Artificial Intelligence in Medical Sciences},
pages = {161–165},
numpages = {5},
keywords = {Big data, Algorithm t, Clinical decision, Artificial intelligence, Nature language processing},
location = {Beijing, China},
series = {ISAIMS 2020}
}

@inproceedings{10.1145/3289430.3289431,
author = {Yang, Xin and Lu, Xinsheng and Geng, Chao},
title = {Study on Urban Microclimate Based on API System on the Background of Big Data: A Case Study of Beijing},
year = {2018},
isbn = {9781450365192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289430.3289431},
doi = {10.1145/3289430.3289431},
abstract = {With the rapid development of data and information, API system provided massive data downloading platform for various industries, which provided opportunities and challenges for urban microclimate research. This paper briefly introduced the research and application status of related data of API system and utilization of various data. Taking Beijing as an example, the paper extracted and analyzed meteorological station data in the city area, including temperature, relative humidity, solar radiation, wind speed and pressure. Interpolation analysis was carried out on the ArcGIS platform to get the climate distribution characteristics in a certain day in summer. Then the paper analyzed influence of transportation infrastructure distribution density on temperature, relative humidity, wind speed and pressure, and proposed the correlation between transportation infrastructure and temperature and relative humidity and no correlation with pressure. The climate environment in the city were complex and the big data platform provided new ways and new ideas for the related research.},
booktitle = {Proceedings of the 2018 2nd International Conference on Big Data and Internet of Things},
pages = {3–6},
numpages = {4},
keywords = {API data system, Beijing, city microclimate environment},
location = {Beijing, China},
series = {BDIOT 2018}
}

@inproceedings{10.1145/3372454.3372479,
author = {Guo, Yiming and Xia, Zhijie and Zhang, Zhisheng and Sun, Mengze},
title = {A Multi-Sensor Big Data Fusion Method in Quality Prediction of the Plasma Enhanced Chemical Vapor Deposition Process},
year = {2020},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372479},
doi = {10.1145/3372454.3372479},
abstract = {Plasma Enhanced Chemical Vapor Deposition (PECVD) is a critical process in the processing of solar cells. Large quantities of the process data are collected from different sensors during the PECVD process, which are high-dimensional and highly correlated. Most existing research only focus on the analysis of single sensor data instead of multi-sensor data. However, the information contained in single sensor data is incomplete. In this paper, the method of Convolutional Neural Networks (CNN) is adopted to analysis multi-sensor big data form PECVD process. The regression model between the multi-sensor data and the quality of solar cells is established for quality prediction. The impact of various types of hyper-parameters on the performance of the model is analyzed, and the predictive performance of the model is optimized by adjusting the hyper-parameters. The performance of the proposed method is compared with existing methods in a real-world case study.},
booktitle = {Proceedings of the 2019 3rd International Conference on Big Data Research},
pages = {24–29},
numpages = {6},
keywords = {PECVD, Big data, Multi-sensor data, CNN, Quality prediction},
location = {Cergy-Pontoise, France},
series = {ICBDR 2019}
}

@proceedings{10.1145/3148055,
title = {BDCAT '17: Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is with great pleasure, on behalf of the program committee, that we welcome you to the fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT2017), to be held in Austin, Texas, USA.BDCAT, as an international conference series, has established itself as the forum for researchers and practitioners in the varied spectrum of human endeavors where data is produced and consumed; from health and personalized medicine, to social services, to industrial processes, to security, to retail business and to high energy physics to identify elementary particle to unlock the secrets of the universe, among many other fields. Big data is an all-encompassing term combining the various characteristics of data that includes their volume, the velocity of data generation and consumption, the variety of data sources and formats, and the variability in their characteristics. The Big Data ecosystem encompasses theoretical and computational frameworks, the applications that deal with such data, and the emerging technologies that ultimately benefit the masses.Since its birth in 2014 in London, UK, BDCAT has become one of the premier forums for sharing of new advances in the methodology, the applications and technologies for big data. Today, BDCAT continues its success. This year we have received 93 submissions from 22 countries. Of these submissions, 27 were accepted for publication, leading to an acceptance rate of 29%.A monumental effort such as BDCAT2017 would not come to fruition without the vision and cooperative and dedicated work of many individuals across the globe. In particular we would like to thank the experts comprising the BDCAT Technical Program Committee for preserving the tradition of rigorous, high-quality peer reviews through their dedication, hard work, and discussions leading up to the selection of the papers. We acknowledge the relentless support that we received from our honorary leadership, Professors Rajkumar Buyya at the University of Melbourne, Australia, Geoffrey Fox at Indian University, USA and Beng Chin OOI of the National University of Singapore, Singapore. We also kindly acknowledge the dedicated support of the local organizing committee chairs: Professors Tim Cockerill of Texas Advanced Computing Center, Jerry Perez, Texas Tech University, Ravi Vadapalli, Texas Tech University and Zhangxi Lin, Texas Tech University, all of the USA. With efforts that spanned almost a year, we also acknowledge the efforts of the publicity chairs, professors David Chiu, University of Puget Sound, USA, Ningfang Mi, Northeastern University, USA, Gleb Radchenko, South Ural State University, Russia, Andrei Tchernykh, CICESE Research Center, Mexico, Yan Tang, Hohai University, China and Iman Elghandour, Alexandria University, Egypt.},
location = {Austin, Texas, USA}
}

@inproceedings{10.1145/2938503.2938517,
author = {Cuzzocrea, Alfredo and Psaila, Giuseppe and Toccu, Maurizio},
title = {An Innovative Framework for Effectively and Efficiently Supporting Big Data Analytics over Geo-Located Mobile Social Media},
year = {2016},
isbn = {9781450341189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938503.2938517},
doi = {10.1145/2938503.2938517},
abstract = {Mobile Social Media are gaining momentum in the broader context of Big Data Analytics, where the main issue is represented by the problem of extracting interesting and actionable knowledge from big data repositories. Mobile social media sources like Twitter and Instagram are indeed producing massive amounts of data (namely, posts) that represent a very rich source of knowledge for predictive analytics. In line with this emerging trend, this paper proposes an innovative approach for effectively and efficiently supporting big data analytics over geo-localized mobile social media, with particular emphasis with the context of modern tourist information systems. In this context, the innovative FollowMe suite, which implements the proposed methodology, is also described in details. We complement our analytical contribution with a real-life case study focusing on the EXPO 2015 event in Milan, Italy which clearly shows benefits and potentialities of our proposed big data analytics framework.},
booktitle = {Proceedings of the 20th International Database Engineering &amp; Applications Symposium},
pages = {62–69},
numpages = {8},
keywords = {Big Data Analytics, Mobile Social Media, Big Data Frameworks},
location = {Montreal, QC, Canada},
series = {IDEAS '16}
}

@inproceedings{10.1145/3318299.3318388,
author = {Zhuo, Zhiyi and Zhang, Shanhu},
title = {Research on the Application of Big Data Management in Enterprise Management Decision-Making and Execution Literature Review},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318388},
doi = {10.1145/3318299.3318388},
abstract = {This article reviews relevant theories and literature on big data management, management decision-making, execution, and other aspects, discusses the two significant factors of decision-making force and executive power that are the realization of corporate strategic goals, and puts forward the corporate data in the context of big data. The operating model (mainly for the enterprise's decision-making and implementation) faces new opportunities and challenges, that is, through in-depth analysis and exploration of big data management can effectively improve the company's decision-making ability and execution efficiency, and promote the realization of corporate strategic goals.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {268–273},
numpages = {6},
keywords = {decision-making, Big data management, execution, literature review},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1145/3278312.3278316,
author = {Win, Thee Zin and Kham, Nang Saing Moon},
title = {Mutual Information-Based Feature Selection Approach to Reduce High Dimension of Big Data},
year = {2018},
isbn = {9781450365567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278312.3278316},
doi = {10.1145/3278312.3278316},
abstract = {As increasing the massive amount of data demands effective and efficient mining strategies, practitioners and researchers are trying to develop scalable mining algorithms, machine learning algorithms and strategies to be successful data mining in turning mountains of data into nuggets. Data of high dimension significantly increases the memory storage requirements and computational costs for data analytics. Therefore, reducing dimension can mainly improve three data mining performance: speed of learning, predictive accuracy and simplicity and comprehensibility of mined result. Feature selection, data preprocessing technique, is effective and efficient in data mining, data analytics and machine learning problems particularly in high dimension reduction. Most feature selection algorithms can eliminate only irrelevant features but redundant features. Not only irrelevant features but also redundant features can degrade learning performance. Mutual information measured feature selection is proposed in this work to remove both irrelevant and redundant features.},
booktitle = {Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence},
pages = {3–7},
numpages = {5},
keywords = {Feature Selection, Mutual Information, High Dimensional Data, Redundant Features},
location = {Ha Noi, Viet Nam},
series = {MLMI2018}
}

@inproceedings{10.1145/3147213.3155012,
author = {Fox, Geoffrey},
title = {Components and Rationale of a Big Data Toolkit Spanning HPC, Grid, Edge and Cloud Computing},
year = {2017},
isbn = {9781450351492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147213.3155012},
doi = {10.1145/3147213.3155012},
abstract = {We look again at Big Data Programming environments such as Hadoop, Spark, Flink, Heron, Pregel; HPC concepts such as MPI and Asynchronous Many-Task runtimes and Cloud/Grid/Edge ideas such as event-driven computing, serverless computing, workflow, and Services. These cross many research communities including distributed systems, databases, cyberphysical systems and parallel computing which sometimes have inconsistent worldviews. There are many common capabilities across these systems which are often implemented differently in each packaged environment. For example, communication can be bulk synchronous processing or data flow; scheduling can be dynamic or static; state and fault-tolerance can have different models; execution and data can be streaming or batch, distributed or local. We suggest that one can usefully build a toolkit (called Twister2 by us) that supports these different choices and allows fruitful customization for each application area. We illustrate the design of Twister2 by several point studies. We stress the many open questions in very traditional areas including scheduling, messaging and checkpointing.},
booktitle = {Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {1},
numpages = {1},
keywords = {global machine learning, hpc, cloud computing, mapreduce, dataflow, mpi, edge computing},
location = {Austin, Texas, USA},
series = {UCC '17}
}

@article{10.1145/2927299.2933408,
author = {Date, Sachin},
title = {Should You Upload or Ship Big Data to the Cloud? The Accepted Wisdom Does Not Always Hold True.},
year = {2016},
issue_date = {March-April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1542-7730},
url = {https://doi.org/10.1145/2927299.2933408},
doi = {10.1145/2927299.2933408},
abstract = {It is accepted wisdom that when the data you wish to move into the cloud is at terabyte scale and beyond, you are better off shipping it to the cloud provider, rather than uploading it. This article takes an analytical look at how shipping and uploading strategies compare, the various factors on which they depend, and under what circumstances you are better off shipping rather than uploading data, and vice versa. Such an analytical determination is important to make, given the increasing availability of gigabit-speed Internet connections, along with the explosive growth in data-transfer speeds supported by newer editions of drive interfaces such as SAS and PCI Express. As this article reveals, the aforementioned "accepted wisdom" does not always hold true, and there are well-reasoned, practical recommendations for uploading versus shipping data to the cloud.},
journal = {Queue},
month = {mar},
pages = {111–135},
numpages = {25}
}

@inproceedings{10.1145/3329364,
author = {Haraty, Ramzi A. and Papadopoulos, Apostolos N. and Sun, Junping},
title = {Session Details: Theme: Information Systems: DBDM - Databases and Big Data Management Track},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329364},
doi = {10.1145/3329364},
abstract = {The world nowadays revolves around dealing with extreme large amount of data presented in various formats. So it is inevitable that researchers focus on advancing the state of managing information. From here, the importance of database technology ranks amongst the hottest areas of research, taking into account the consistent need for faster query processing as well as for managing huge amounts of data. This year the track has received many papers covering different areas of databases.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1145/2850420,
author = {Millar, Jeremy R. and Hodson, Douglas D. and Peterson, Gilbert L. and Ahner, Darryl K.},
title = {Data Quality Challenges in Distributed Live-Virtual-Constructive Test Environments},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2850420},
doi = {10.1145/2850420},
journal = {J. Data and Information Quality},
month = {apr},
articleno = {2},
numpages = {3},
keywords = {performance estimation, Distributed simulation}
}

@inproceedings{10.1145/2695664.2695753,
author = {Nascimento, Dimas C. and Pires, Carlos Eduardo and Mestre, Demetrio Gomes},
title = {A Data Quality-Aware Cloud Service Based on Metaheuristic and Machine Learning Provisioning Algorithms},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695753},
doi = {10.1145/2695664.2695753},
abstract = {Cloud Computing as a service has become a topic of increasing interest. The outsourcing of duties and infrastructure to external parties became a crucial concept for many business models. In this paper we discuss the design and experimental evaluation of provisioning algorithms, in a Data Quality-aware Service (DQaS) context, that enables dynamic Data Quality Service Level Agreements (DQSLA) management and optimization of cloud resources. The DQaS has been designed to respond effectively to the DQSLA requirements of the service customers, by minimizing SLA penalties and provisioning the cloud infrastructure for the execution of data quality algorithms. An experimental evaluation of the proposed provisioning algorithms, carried out through simulation, has provided very encouraging results that confirm the adequacy of these algorithms in the DQaS context.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1696–1703},
numpages = {8},
keywords = {data quality, cloud computing, provisioning, metaheuristic, machine learning},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3389649,
author = {Haraty, Ramzi A. and Papadopoulos, Apostolos N. and Sun, Junping},
title = {Session Details: Theme: Information Systems: DBDM - Databases and Big Data Management Track},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3389649},
doi = {10.1145/3389649},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/2979779.2979844,
author = {Agarwal, Prerna and Ahmed, Rafeeq and Ahmad, Tanvir},
title = {Identification and Ranking of Key Persons in a Social Networking Website Using Hadoop &amp; Big Data Analytics},
year = {2016},
isbn = {9781450342131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2979779.2979844},
doi = {10.1145/2979779.2979844},
abstract = {Big Data is a term which defines a vast amount of structured and unstructured data which is challenging to process because of its large size, using traditional algorithms and lack of high speed processing techniques. Now a days, vast amount of digital data is being gathered from many important areas, including social networking websites like Facebook and Twitter. It is important for us to mine this big data for analysis purpose. One important analysis in this domain is to find key nodes in a social graph which can be the major information spreader. Node centrality measures can be used in many graph applications such as searching and ranking of nodes. Traditional centrality algorithms fail on such huge graphs therefore it is difficult to use these algorithms on big graphs. Traditional centrality algorithms such as degree centrality, betweenness centrality and closeness centrality were not designed for such large data. In this paper, we calculate centrality measures for big graphs having huge number of edges and nodes by parallelizing traditional centrality algorithms so that they can be used in an efficient way when the size of graph grows. We use MapReduce and Hadoop to implement these algorithms for parallel and distributed data processing. We present results and anomalies of these algorithms and also show the comparative processing time taken on normal systems and on Hadoop systems.},
booktitle = {Proceedings of the International Conference on Advances in Information Communication Technology &amp; Computing},
articleno = {65},
numpages = {6},
keywords = {MapReduce, Betweenness Centrality, key persons, Closeness Centrality, Degree Centrality Big Data, ranking},
location = {Bikaner, India},
series = {AICTC '16}
}

@inproceedings{10.1145/2516775.2516782,
author = {Petkov, Plamen and Helfert, Markus},
title = {A Methodology for Analyzing and Measuring Semantic Data Quality in Service Oriented Architectures},
year = {2013},
isbn = {9781450320214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2516775.2516782},
doi = {10.1145/2516775.2516782},
abstract = {Nowadays, Service Oriented Architecture (SOA) has become a preferable way of building information systems because they enable enterprises to rapidly response to the business' changes. However, the more complex SOA develops, the more likely are data quality (DQ) issues to be encountered. Despite the huge number of studies that have been done on SOA, very little has been investigated about the DQ aspect. In this paper we address issues concerning the detection of data quality problems. Hence, we propose a DQ methodology in the SOA context which will assess semantic (business) data.},
booktitle = {Proceedings of the 14th International Conference on Computer Systems and Technologies},
pages = {201–208},
numpages = {8},
keywords = {data issues in SOA semantic data quality, data quality methodology in SOA, semantic inaccuracy, service oriented architectures},
location = {Ruse, Bulgaria},
series = {CompSysTech '13}
}

@inproceedings{10.1145/2837060.2837103,
author = {Kim, Ji-hye and Cho, Sang-woo and Park, Da-jeong and Lee, Kyung-hee and Choi, Chi-hwan and Cho, Wan-sup},
title = {Local Festival Marketing and Application Plan for Agricultural Products by Utilizing Big Data from Online Shopping Mall},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837103},
doi = {10.1145/2837060.2837103},
abstract = {The purpose of this research is to propose an establishment method of plans and marketing strategies for local festivals by utilizing issue categories and associated keywords obtained through big data analysis on online shopping mall. For this research, analysis was performed on organic product registration status and product titles which were posted on the biggest online shopping malls in Korea -- G Market, Auction, 11st -- on November 6, 2014 (1st round) and November 13th, 2014 (2nd round). As a result of the analysis, level of buzz was highest in the order of 'coffee/drink', 'infant/child clothing', 'pet product', and 'organic cotton' was obtained the most as the keyword related to organic product. The analysis has its significance in establishing marketing strategy by utilizing structured and unstructured web data of online shopping malls which have been rapidly growing recently.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {233–236},
numpages = {4},
keywords = {Analysis, Shopping, text},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/3257762,
author = {Haddadi, Hamed},
title = {Session Details: Big Data and Social Media Studies on Weightloss and Obesity},
year = {2016},
isbn = {9781450342247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257762},
doi = {10.1145/3257762},
booktitle = {Proceedings of the 6th International Conference on Digital Health Conference},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {DH '16}
}

@inproceedings{10.1145/2815782.2815791,
author = {Botha, Marna and Botha, Adele and Herselman, Marlien},
title = {On the Prioritization of Data Quality Challenges in E-Health Systems in South Africa},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815791},
doi = {10.1145/2815782.2815791},
abstract = {Data quality is one of many challenges experienced in e-health. The collection of data with substandard data quality leads to inappropriate information for health and management purposes. Given evidence of challenges with regards to data quality in electronic health systems, the purpose of the study is to prioritise data quality challenges as experienced by data users of electronic healthcare systems in South Africa. The study adopted a sequential QUAL-quan mixed method research design towards the realisation of the research purpose. After carrying out a literature review on the background of e-health and the current status of research on data quality challenges, a qualitative study was conducted to verify and extend the theoretical list of data quality challenges. A quantitative study followed to prioritise data quality challenges as experienced by data users of electronic healthcare systems. Data users of electronic healthcare systems in South Africa served as the unit of analysis in the study. The data collection process included the conducting of interviews with four data quality experts to verify and extend the theoretical list of data quality challenges. This was followed by a survey targeting 100 data users of electronic healthcare systems in South Africa for which 82 responses were received.From the results of the study, a prioritised list of data quality challenges has been developed which can be applied to assist data users of electronic health care systems in South Africa to improve the quality of data in electronic healthcare systems. The most important data challenge is training. The prioritised list of data quality challenges allowed for evidence-based recommendations which can assist health institutions in South Africa to ensure future data quality.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {5},
numpages = {10},
keywords = {data quality challenges, E-health, data quality, IT},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3287324.3287494,
author = {Deb, Debzani and Fuad, Muztaba and Irwin, Keith},
title = {A Module-Based Approach to Teaching Big Data and Cloud Computing Topics at CS Undergraduate Level},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287494},
doi = {10.1145/3287324.3287494},
abstract = {Big data and cloud computing collectively offer a paradigm shift in the way businesses are now acquiring, using and managing information technology. This creates the need for every CS student to be equipped with foundational knowledge in this collective paradigm and to possess some hands-on experience in deploying and managing big data applications in the cloud. We argue that, for substantial coverage of big data and cloud computing concepts and skills, the relevant topics need to be integrated into multiple core courses across the undergraduate CS curriculum rather than creating additional standalone core or elective courses and performing a major overhaul of the curriculum. Our approach to including these topics is to develop autonomous learning modules for specific core courses in which their coverage might find an appropriate context. In this paper, three such modules are discussed and our classroom experiences during these interventions are documented. So far, we have achieved reasonable success in attaining student learning outcomes, enhanced engagement, and interests. Our objective is to share our experience with the academics who aim at incorporating similar pedagogy and to receive feedback about our approach.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {2–8},
numpages = {7},
keywords = {mapreduce, apache spark, cloud computing, big data, curriculum},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1145/2837060.2837062,
author = {Cho, Wonhee and Choi, Eunmi},
title = {A GPS Trajectory Map-Matching Mechanism with DTG Big Data on the HBase System},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837062},
doi = {10.1145/2837060.2837062},
abstract = {Since smartphones equipped with GPS have been produced, the need to conduct an analysis by matching the mass of GPS trajectory data on a digital map has increased. However, the study of the existing map-matching algorithm technique is mainly for navigation. In order to analyze large amounts of GPS trajectories on a server, issues of the speed and performance of the system exist. The purpose of this study is to utilize a map-matching system using HBase, which is a distributed NoSQL DB in a Hadoop ecosystem. We defined the table specification of HBase for mounting the digital map and proposed and implemented the method for analysis with a map-matching algorithm. In this paper, we present the map-matching methodology using the NoSQL DB of Hadoop ecosystem for analyzing GPS trajectory.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {22–29},
numpages = {8},
keywords = {spatial analysis, HBase, Big data, map matching, Hadoop},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/3372931,
author = {April, Alain},
title = {Session Details: Big Data in Digital Public Health: Management and Technical Approaches},
year = {2019},
isbn = {9781450372084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372931},
doi = {10.1145/3372931},
booktitle = {Proceedings of the 9th International Conference on Digital Public Health},
location = {Marseille, France},
series = {DPH2019}
}

@inproceedings{10.1145/3167132.3167376,
author = {Bellaaj, Hatem and Mdhaffar, Afef and Jmaiel, Mohamed and Mseddi, Sondes Hdiji and Freisleben, Bernd},
title = {An Adaptive Neuro-Fuzzy Inference System for Improving Data Quality in Disease Registries},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167376},
doi = {10.1145/3167132.3167376},
abstract = {The purpose of disease registries is to collect and analyze data related to specific diseases in terms of incidence and prevalence. Since the data is typically entered by wearable sensors and/or human caregivers, errors in the data fields are often inevitable. In this paper, we propose a new approach to improve data quality in disease registries based on (a) a semi-random combination of parameters and (b) a learning algorithm for detecting and signaling the loss of quality of the entered data. To implement the approach, we have developed a novel adaptive neuro-fuzzy inference system. It is applied to specific sections of the Tunisian Fanconi Anemia Registry with the aims of reducing false alarms and automatically adjusting the parameters of coefficients of the disease. Our experimental results indicate that both aims can be achieved and effectively lead to improved data quality in disease registries.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {30–33},
numpages = {4},
keywords = {ANFIS, disease registry, data quality, fuzzy logic},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3208159.3208178,
author = {Zhang, Yanci and Liang, Zi and Li, Xiaoyao and Ren, Wenjie and Liu, Yanli},
title = {Automatic Identification of Performance Bottleneck for A Complex Rendering System through Big Data},
year = {2018},
isbn = {9781450364010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208159.3208178},
doi = {10.1145/3208159.3208178},
abstract = {In this paper, we present a data mining based algorithm to automatically locate performance bottlenecks at algorithm level for a complex rendering system. The basic idea is to treat the bottleneck identification problem as a variable importance analysis problem from a large volume of performance data which is generated by collecting the time costs under different combinations of algorithm level parameters. Based on the performance data set, random forest is adopted to conduct the variable importance ranking task. We also note an important fact that there might no performance bottleneck exists in the scope of the whole rendering system, but it is likely that bottlenecks could be found under some specific conditions. Thus we propose a bottleneck analysis tree to split the parameter space into many subspaces in which performance bottlenecks can be identified.},
booktitle = {Proceedings of Computer Graphics International 2018},
pages = {33–40},
numpages = {8},
keywords = {Data mining, Performance bottleneck analysis, Performance analysis tree, Variable importance},
location = {Bintan, Island, Indonesia},
series = {CGI 2018}
}

@inproceedings{10.1145/3297662.3365807,
author = {Shahoud, Shadi and Gunnarsdottir, Sonja and Khalloof, Hatem and Duepmeier, Clemens and Hagenmeyer, Veit},
title = {Facilitating and Managing Machine Learning and Data Analysis Tasks in Big Data Environments Using Web and Microservice Technologies},
year = {2020},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365807},
doi = {10.1145/3297662.3365807},
abstract = {Driven by the great advance of machine learning in a wide range of application areas, the need for developing machine learning frameworks effectively as well as easily usable by novices increased dramatically. Furthermore, building machine learning models in the context of big data environments still represents a great challenge. In the present paper, we tackle these challenges by introducing a new generic framework for efficiently facilitating the training, testing, managing, storing, and retrieving of machine learning models in the context of big data. The framework makes use of a powerful big data software stack and a microservice architecture for a fully manageable and highly scalable solution. A highly configurable user interface is introduced giving the user the ability to easily train, test, and manage machine learning models. Moreover, it automatically indexes models and allows flexible exploration of them in the visual interface. The performance of the new framework is evaluated on state-of-the-arts machine learning algorithms: it is shown that storing and retrieving machine learning models as well as a respective acceptable low overhead demonstrate an efficient approach to facilitate machine learning in big data environments.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {80–87},
numpages = {8},
keywords = {Web-based Applications, Data Analytic, Big Data, Machine Learning, Microservice},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1145/2847263.2847294,
author = {Ghasemi, Ehsan and Chow, Paul},
title = {A Scalable Heterogeneous Dataflow Architecture For Big Data Analytics Using FPGAs (Abstract Only)},
year = {2016},
isbn = {9781450338561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2847263.2847294},
doi = {10.1145/2847263.2847294},
abstract = {Due to rapidly expanding data size, there is increasing need for scalable, high-performance, and low-energy frameworks for large- scale data computation. We build a dataflow architecture that harnesses FPGA resources within a distributed analytics platform creating a heterogeneous data analytics framework. This approach leverages the scalability of existing distributed processing environments and provides easy access to custom hardware accelerators for large-scale data analysis. We prototype our framework within the Apache Spark analytics tool running on a CPU-FPGA heterogeneous cluster. As a specific application case study, we have chosen the MapReduce paradigm to implement a multi-purpose, scalable, and customizable RTL accelerator inside the FPGA, capable of incorporating custom High-Level Synthesis (HLS) MapReduce kernels. We demonstrate how a typical MapReduce application can be simply adapted to our distributed framework while retaining the scalability of the Spark platform.},
booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {274},
numpages = {1},
keywords = {fpga, mapreduce, big data, apache spark},
location = {Monterey, California, USA},
series = {FPGA '16}
}

@inproceedings{10.1145/1012453.1012461,
title = {A General Framework for Query Answering in Data Quality-Based Cooperative Information Systems},
year = {2004},
isbn = {1581139020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1012453.1012461},
doi = {10.1145/1012453.1012461},
abstract = {Data quality in Cooperative Information Systems (CISs) which integrate a number of local heterogeneous database systems is an increasingly important issue. Current in-practice developed CISs are based on different software and architectural paradigms, and are specified a number of ad-hoc algorithms for quality query-answering, without an unifying logic framework (differently from well defined semantics in data integration systems). The consequence is that each software system responds in some way to user queries with record for which is not often clear if it is really a logic answer from CISs database. Sometimes it is not also clear if the records obtained are complete answers w.r.t. the logically derivable answers from the system. Because of such considerations, in this paper we present a general framework for a query-answering in a logic theory in Data Quality Cooperative Information Systems (DaQuinCIS), by an epistemic extension of the standard data integration systems, and we introduce also a 4-valued logic in order to deal with incomplete and inconsistent information.},
booktitle = {Proceedings of the 2004 International Workshop on Information Quality in Information Systems},
pages = {44–50},
numpages = {7},
location = {Paris, France},
series = {IQIS '04}
}

@inproceedings{10.1145/3428502.3428515,
author = {Li, Lue},
title = {Macao Government Fights the COVID-19 Epidemics with the Help of e-Government and Big Data},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428515},
doi = {10.1145/3428502.3428515},
abstract = {Macao is a tourist city. When the COVID-19 epidemic occurred, Macao's risks and pressures were enormous. However, the response policies of the Government are timely and effective, making Macao one of the lightest epidemic regions in the world. This article reviewed the SAR government's anti-epidemic policies, and found four most important policies all based on the E-government technology and the implementation of big data: 1. Quickly screen, restrict and isolate tourists from the affected areas; 2. Supply masks; 3. Accurate collection, in-depth analysis and rapid release of information; 4. Electronic consumer card.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {112–115},
numpages = {4},
keywords = {Big data, Special Webpage against Epidemics, COVID-19 Epidemic, E-government, Public Health Crisis},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/3077286.3077294,
author = {Sezer, Omer Berat and Ozbayoglu, A. Murat and Dogdu, Erdogan},
title = {An Artificial Neural Network-Based Stock Trading System Using Technical Analysis and Big Data Framework},
year = {2017},
isbn = {9781450350242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077286.3077294},
doi = {10.1145/3077286.3077294},
abstract = {In this paper, a neural network-based stock price prediction and trading system using technical analysis indicators is presented. The model developed first converts the financial time series data into a series of buy-sell-hold trigger signals using the most commonly preferred technical analysis indicators. Then, a Multilayer Perceptron (MLP) artificial neural network (ANN) model is trained in the learning stage on the daily stock prices between 1997 and 2007 for all of the Dow30 stocks. Apache Spark big data framework is used in the training stage. The trained model is then tested with data from 2007 to 2017. The results indicate that by choosing the most appropriate technical indicators, the neural network model can achieve comparable results against the Buy and Hold strategy in most of the cases. Furthermore, fine tuning the technical indicators and/or optimization strategy can enhance the overall trading performance.},
booktitle = {Proceedings of the SouthEast Conference},
pages = {223–226},
numpages = {4},
keywords = {algorithmic trading, multi layer perceptron, Artificial neural network, technical analysis, Stock market},
location = {Kennesaw, GA, USA},
series = {ACM SE '17}
}

@inproceedings{10.1145/3167486.3167506,
author = {Dahdouh, K. and Dakkak, Ahmed and Oughdir, Lahcen},
title = {Integrating Big Data Technologies in a Dynamic Environment EIAH Dedicated to E-Learning Systems Based on Cloud Infrastructure},
year = {2017},
isbn = {9781450353069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167486.3167506},
doi = {10.1145/3167486.3167506},
abstract = {Online learning has experienced a lot of change, largely as a result of a number of technological Innovations. Newer e-learning (online learning) authoring tools have been developed, and more sophisticated Learning Management Systems (LMSs) have been deployed. Nowadays, e-learning systems have evolved exponentially. The emergence of new information and communication technologies, also with the development of new models of learning and new pedagogical concepts, which causes many problems such as the gigantic volume of data generated by e-learning platform, the large number of learners and the diversity of educational content. In this context, Big Data is a promising paradigm because of its permanent scalability and opportunities that offer to e-learning professionals in terms of data collection, storage, analysis, processing, optimization and representation of data. This article presents the Big Data concept, its characteristics, and focuses in particular on the integration of it in a dynamic environment dedicated to e-learning systems, and how Big Data impacts the future of e-learning. Moreover, it proposes a new approach for E-learning systems based on big data technologies in a cloud infrastructure. Furthermore this work explores the benefits and advantages of Big Data for e-learning professionals and how Big Data can improve the online learning experience.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems},
articleno = {20},
numpages = {7},
keywords = {Learning Management Systems (LMS), Cloud computing, Online learning, Big Data, MapReduce, E-learning, Spark, NoSQL databases, Learner, Hadoop, Cassandra, mongoDB},
location = {Larache, Morocco},
series = {ICCWCS'17}
}

@inproceedings{10.1145/3419635.3419652,
author = {Wu, Xiaogang},
title = {Research on the Innovation of Ideological and Political Education in Universities in the Era of Big Data},
year = {2020},
isbn = {9781450387729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419635.3419652},
doi = {10.1145/3419635.3419652},
abstract = {Along with the development of the era, information technology and computer technology, cloud computing technology are in rapid progress. Today, the modern teaching reform is deepening, the original paper of teaching resources has been unable to meet the needs of modern education. Especially in recent years, increasing amount and types of teaching resources, for teachers and students, teaching resources have been more and more important. In the future, the digital teaching resource management mode based on cloud computing technology, Web network technology and information technology will be the main direction of progress. Therefore, the project of constructing digital teaching resource management system is of great significance. This paper introduces the research status of big data and ideological and political education in colleges and universities at home and abroad, USES big data to promote the ideological and political education in colleges and universities, advances the application of big data to college education, and finally summarizes the results and prospects.},
booktitle = {Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education},
pages = {336–340},
numpages = {5},
keywords = {Opportunities and challenges, Ideological and political education in colleges and universities, Innovative exploration, Big data era},
location = {Ottawa, ON, Canada},
series = {CIPAE 2020}
}

@article{10.1145/2378016.2378018,
author = {Collins, Claire and Janssens, Kelly},
title = {Creating a General (Family) Practice Epidemiological Database in Ireland - Data Quality Issue Management},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2378016.2378018},
doi = {10.1145/2378016.2378018},
abstract = {In Ireland, while detailed information is available regarding hospital attendance, little is known regarding general (family) practice attendance. However, it is conservatively estimated that there are almost nine times as many general practice encounters than there are hospital encounters each year in Ireland. This represents a very significant gap in health information. Indeed, general practice has been shown in other countries to be an important and rich source of information about the health of the population, their behaviors and their utilization of health services. Funded by the Health Information and Quality Authority (HIQA), the Irish College of General Practitioners (ICGP) undertook a feasibility study of diagnostic coding of routinely entered patient data and the creation of a national general practice morbidity and epidemiological database (GPMED project). This article outlines the process of data quality issue management undertaken.The study’s findings suggest that the quality of data collection and reporting structures available in general practice throughout Ireland at the outset of this project were not adequate to permit the creation of a database of sufficient quality for service planning and policy or epidemiological research. Challenges include the dearth of a minimum standard of data recorded in consultations by GPs and the absence of the digital data recording and exporting infrastructure within Irish patient management software systems. In addition, there is at present a lack of recognition regarding the value of such data for patient management and service planning---including importantly, data collectors who do not fully accept the merit of maintaining data, which has a direct consequence for data quality. The work of this project has substantial implications for the data available to the health sector in Ireland and contributes to the knowledge base internationally regarding general practice morbidity data.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {2},
numpages = {9},
keywords = {epidemiology, family practice, Data quality}
}

@inproceedings{10.1145/3341620.3341624,
author = {Jia, Fengsheng and Gao, Yang and Wang, Yuming},
title = {Study on Standard System of Aerospace Quality Data Resources Integration under the Background of Big Data},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341624},
doi = {10.1145/3341620.3341624},
abstract = {The integration and application of aerospace product quality data resources is an important way to carry out quality improvement, quality evaluation and precise management. Standardization is the basis for promoting quality data resources integration. The unified and normative standard system is the guarantee for efficient development of integration standards. Firstly, we analyzed the features of quality data resources according to the status quo of integration. Integration structure of quality data resources in terms of vertical and horizontal integration was proposed by adopting the methods of "decomposition-integration" and "classification-association". Secondly, we constructed a three-dimensions architecture of quality data resource integration using the method of system engineering methodology, from the layer dimension (basis, common, special), technical dimension (description, collection, storage, transmission, processing, comprehensive management) and category dimension (rocket, spacecraft). Thirdly, we worked out 20 lists about basis and common standard by adopting the top-down approach. Some standard development suggestions are proposed based on the characteristics of quality data resources and standard research strategies. Finally, we applied the quality problem data resource standard construction and application to verify the proposed method.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {16–22},
numpages = {7},
keywords = {standard system, quality data resource integration, system planning, aerospace products},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/2535800.2535921,
author = {Goodacre, John},
title = {The Evolution of the ARM Architecture towards Big Data and the Data-Centre (Abstract Only)},
year = {2013},
isbn = {9781450325097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2535800.2535921},
doi = {10.1145/2535800.2535921},
abstract = {In recent years the rapid evolution of the ARM Architecture has lead to a potential inflection point where the architectural and compute capabilities the ARM partners deliver into the low cost, low power, consumer focused products has direct applicability to the requirement of compute in the data centre and the processing of big data. This talk will review these capabilities and the discuss the potential impact to the data centre. Finally, we'll take a glance into a project that aims to use a holistic approach to compute, ranging from 3D fabrication of an application specific, scalable compute element with virtualized access to system level resources so to balance to cost and requirement between compute, IO and memory.},
booktitle = {Proceedings of the 8th Workshop on Virtualization in High-Performance Cloud Computing},
articleno = {4},
numpages = {1},
location = {Denver, Colorado},
series = {VHPC '13}
}

@inproceedings{10.1145/3297662.3365822,
author = {Ishikawa, Hiroshi and Kato, Daiju and Endo, Masaki and Hirota, Masaharu},
title = {Applications of Generalized Difference Method for Hypothesis Generation to Social Big Data in Concept and Real Spaces},
year = {2020},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365822},
doi = {10.1145/3297662.3365822},
abstract = {Analytic methodology as to generation of integrated hypotheses is necessary for applications involving different sources of social big data. In this paper, first, we introduce an abstract data model for integrating data management and data mining by using mathematical concepts of families, collections of sets to facilitate reproducibility and accountability required for social big data applications. Next, we describe generalized difference methods as a methodology for generating integrated hypotheses. Finally, we validate our proposal by applying them to three use cases involving data in concept and real spaces by using our data model as their description guided by generalized difference methods.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {44–55},
numpages = {12},
keywords = {difference method, hypothesis generation, integrated analysis, Social big data, data management, data mining, data model},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1145/2751957.2751960,
author = {Sillaber, Christian and Breu, Ruth},
title = {Using Stakeholder Knowledge for Data Quality Assessment in IS Security Risk Management Processes},
year = {2015},
isbn = {9781450335577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2751957.2751960},
doi = {10.1145/2751957.2751960},
abstract = {The availability of high quality documentation of the IS as well as knowledgeable stakeholders are an important prerequisite for successful IS security risk management processes. However, little is known about the relationship between stakeholders, their knowledge about the IS, security documentation and how quality aspects influence the security and risk properties of the IS under investigation. We developed a structured data quality assessment process to identify quality issues in the security documentation of an information system. For this, organizational stakeholders were interviewed about the IS under investigation and models were created from their description in the context of an ongoing security risk management process process. Then, the research model was evaluated in a case study. We found that contradictions between the models created from stakeholder interviews and those created from documentation were a good indicator for potential security risks. The findings indicate that the proposed data quality assessment process provides valuable inputs for the ongoing security and risk management process. While current research considers users as the most important resource in security and risk management processes, little is known about the hidden value of various entities of documentation available at the organizational level. This study highlights the importance of utilizing existing IS security documentation in the security and risk management process and provides risk managers with a toolset for the prioritization of security documentation driven improvement activities.},
booktitle = {Proceedings of the 2015 ACM SIGMIS Conference on Computers and People Research},
pages = {153–159},
numpages = {7},
keywords = {information system security documentation quality, data quality of information system, information systems security risk management},
location = {Newport Beach, California, USA},
series = {SIGMIS-CPR '15}
}

