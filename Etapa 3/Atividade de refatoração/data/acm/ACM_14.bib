@inproceedings{10.1145/3230744.3230751,title = {Deep motion transfer without big data}, author = {Kwon Byungjun , Yu Moonwon , Jang Hanyoung , Cho KyuHyun , Lee Hyundong , Hahn Taesung },year = {2018}, isbn = {9781450358170}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3230744.3230751}, doi = {10.1145/3230744.3230751}, abstract = {This paper presents a novel motion transfer algorithm that copies content motion into a specific style character. The input consists of two motions. One is a content motion such as walking or running, and the other is movement style such as zombie or Krall. The algorithm automatically generates the synthesized motion such as walking zombie, walking Krall, running zombie, or running Krall. In order to obtain natural results, the method adopts the generative power of deep neural networks. Compared to previous neural approaches, the proposed algorithm shows better quality, runs extremely fast, does not require big data, and supports user-controllable style weights.}, location = {Vancouver, British Columbia, Canada}, series = {SIGGRAPH '18}, pages = {1\u20132}, numpages = {2}, keywords = {deep neural networks, character animation synthesis}}
@inproceedings{10.1145/2791347.2791380,title = {Privacy-preserving big data publishing}, author = {Zakerzadeh Hessam , Aggarwal Charu C. , Barker Ken },year = {2015}, isbn = {9781450337090}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2791347.2791380}, doi = {10.1145/2791347.2791380}, abstract = {The problem of privacy-preserving data mining has been studied extensively in recent years because of its importance as a key enabler in the sharing of massive data sets. Most of the work in privacy has focussed on issues involving the quality of privacy preservation and utility, though there has been little focus on the issue of scalability in privacy preservation. The reason for this is that anonymization has generally been seen as a batch and one-time process in the context of data sharing. However, in recent years, the sizes of data sets have grown tremendously to a point where the effective application of the current algorithms is becoming increasingly difficult. Furthermore, the transient nature of recent data sets has resulted in an increased need for the repeated application of such methods on the newer data sets which have been collected. Repeated application demands even greater computational efficiency in order to be practical. For example, an algorithm with quadratic complexity is unlikely to be implementable in reasonable time over terabyte scale data sets. A bigger issue is that larger data sets are likely to be addressed by distributed frameworks such as MapReduce. In such frameworks, one has to address the additional issue of minimizing data transfer across different nodes, which is the bottleneck. In this paper, we discuss the first approach towards privacy-preserving data mining of very massive data sets using MapReduce. We study two most widely-used privacy models k-anonymity and l-diversity for anonymization, and present experimental results illustrating the effectiveness of the approach.}, location = {La Jolla, California}, series = {SSDBM '15}, pages = {1\u201311}, numpages = {11}}
@inproceedings{10.1145/3481646.3481652,title = {Lyapunov Central Limit Theorem: Theoretical Properties and Applications in Big-Data-Populated Smart City Settings}, author = {Cuzzocrea Alfredo , Fadda Edoardo , Baldo Alessandro },year = {2021}, isbn = {9781450390408}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3481646.3481652}, doi = {10.1145/3481646.3481652}, abstract = {Central Limit Theorems have a fundamental role in statistics and in a wide range of practical applications. The most famous formulation was proposed by Lindeberg\u2013L\u00e9vy and it requires the variables to be independent and identically distributed. In the real setting these conditions are rarely matched, though. The Lyapunov Central Limit Theorem overcomes this limitation, since it does not require the same distribution of the random variables. However, the cost of this generalization is an increased complexity, moderately limiting its effective applicability. In this paper, we resume the main results on the Lyapunov Central Limit Theorem, providing an easy-to-prove condition to put in practice, and demonstrating its uniform convergence. These theoretical results are supported by some relevant applications in the field of big data in smart city settings.}, location = {Liverpool, United Kingdom}, series = {ICCBDC '21}, pages = {34\u201338}, numpages = {5}, keywords = {Uniform Convergence Condition, Smart City, Lyapunov Central Limit Theorem}}
@inproceedings{10.1145/3224207.3224220,title = {Blockchain and Big Data to Transform the Healthcare}, author = {Bhuiyan Md Zakirul Alam , Zaman Aliuz , Wang Tian , Wang Guojun , Tao Hai , Hassan Mohammad Mehedi },year = {2018}, isbn = {9781450364188}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3224207.3224220}, doi = {10.1145/3224207.3224220}, abstract = {The increase in reported incidents of security breaches that compromise privacy of individuals requires us to question the current model used to collect patient information. What we have learned from bitcoin and the underlying blockchain technology is that there are ways for us to protect this information by using a distributed ledger. In this paper, we review and propose a solution that can be used to manage individual health data as well as cross-institutional sharing of this information. The solution will increase clinical effectiveness and an increase in research when the data is shared with researchers. The proposed system solution based on blockchain technology that includes providers, hospitals and clinic, insurance companies, and patients. All along the ownership of the data would belong to the individual or the patient. In the solution, we suggest to adopt a private blockchain solution where all participants are known and trusted, which allows for privacy and security of the data.}, location = {Guangdong, China}, series = {ICDPA 2018}, pages = {62\u201368}, numpages = {7}, keywords = {Blockchain, healthcare, big data, patent data, privacy}}
@inproceedings{10.1145/3175684.3175717,title = {Research on Real Time Processing and Intelligent Analysis Technology of Power Big Data}, author = {Xue Jiarui , Chen Xiangzhou , Ding Huixia , He Xiao },year = {2017}, isbn = {9781450354301}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3175684.3175717}, doi = {10.1145/3175684.3175717}, abstract = {This paper focuses on the research of all kinds of data in the power grid business system, and carries on the research of intelligent analysis technology. It focuses on breaking the technical difficulties of intelligent and efficient analysis and mining, distributed multi-stream real-time processing, developing large-scale stock data for power and high-frequency incremental data efficient analysis system prototype, for wide-area distributed multi-stream real-time computing, from the data quick access to valuable information to solve the problems in the grid business system to improve the overall business performance, to achieve support power Data real-time processing and other technology applications.}, location = {London, United Kingdom}, series = {BDIOT2017}, pages = {43\u201347}, numpages = {5}, keywords = {distributed, data information, intelligence, data flow}}
@inproceedings{10.1145/2786451.2786482,title = {Big Data? Big Issues Degradation in Longitudinal Data and Implications for Social Sciences}, author = {Weber Matthew S. , Nguyen Hai },year = {2015}, isbn = {9781450336727}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2786451.2786482}, doi = {10.1145/2786451.2786482}, abstract = {This article analyzes the issue of degradation of data accuracy in large-scale longitudinal data sets. Recent research points to a number of issues with large-scale data, including problems of reliability, accuracy and quality over time. Simultaneously, large-scale data is increasingly being utilized in the social sciences. As scholars work to produce theoretically grounded research utilized \"small-scale\" methods, it is important for researchers to better understand the critical issues associated with the analysis of large-scale data. In order to illustrate the issues associated with this type of research, a case study analysis of archival Internet data is presented focusing on the issues of degradation of data accuracy over time. Suggestions for future studies are given.}, location = {Oxford, United Kingdom}, series = {WebSci '15}, pages = {1\u20135}, numpages = {5}, keywords = {Keywords are your own designated keywords}}
@inproceedings{10.1145/3012286,title = {Big Data Meets Digital Cultural Heritage: Design and Implementation of SCRABS, A Smart Context-awaRe Browsing Assistant for Cultural EnvironmentS}, author = {Amato Flora , Moscato Vincenzo , Picariello Antonio , Colace Francesco , Santo Massimo De , Schreiber Fabio A. , Tanca Letizia },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3012286}, doi = {10.1145/3012286}, abstract = {Information and Communication Technologies have radically changed the modern Cultural Heritage scenery: Simple traditional Information Systems supporting the management of cultural artifacts have left the place to complex systems that expose rich information extracted from heterogeneous data sources\u2014like Sensor Networks, Social Networks, Digital Libraries, Multimedia Collections, Web Data Service, and so on\u2014by means of sophisticated applications that enhance the users\u2019 experience. In this article, we describe SCRABS, a Smart Context-awaRe Browsing assistant for cultural EnvironmentS. SCRABS has been developed during the Cultural Heritage Information Systems national project and promoted by DATABENC, the Cultural Heritage Technological District of the Campania Region, in Italy. SCRABS has been designed on top of a Big Data technological stack as the result of a multidisciplinary project carried out by a heterogeneous team of computer scientists, archeologists, architects, and experts in humanities. We describe the main ideas that support the system, showing its use in some real application scenarios located in the Paestum Archeologica Sites.}, pages = {1\u201323}, numpages = {23}, keywords = {Big data, cultural heritage, multimedia}}
@inproceedings{10.1145/3265689.3265721,title = {Big Data Management and Analytics for Disability Datasets}, author = {Pan Zhiwen , Ji Wen , Chen Yiqiang , Dai Lianjun , Zhang Jun },year = {2018}, isbn = {9781450365871}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3265689.3265721}, doi = {10.1145/3265689.3265721}, abstract = {The disability datasets is the datasets which contains the information of disabled populations. By analyzing these datasets, professionals who work with disabled populations can have a better understanding of how to make working plans and policies, so that they support the populations in a better way. In this paper, we proposed a big data management and mining approach for disability datasets. The contributions of this paper are follows: 1) our proposed approach can improve the quality of disability data by estimating miss attribute values and detecting anomaly and low-quality data instances. 2) Our proposed approach can explore useful patterns which reflect the correlation, association and interactional between the disability data attributes. Experiments are conducted at the end to evaluate the performance of our approach.}, location = {Singapore, Singapore}, series = {ICCSE'18}, pages = {1\u20136}, numpages = {6}, keywords = {Decision support systems, Big data analytics, Data Mining, Disability Population, Disability Dataset, Data Management}}
@inproceedings{10.1145/3307681.3325410,title = {Perspectives on High-Performance Computing in a Big Data World}, author = {Fox Geoffrey C. },year = {2019}, isbn = {9781450366700}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3307681.3325410}, doi = {10.1145/3307681.3325410}, abstract = {High-Performance Computing (HPC) and Cyberinfrastructure have played a leadership role in computational science even since the start of the NSF computing centers program. Thirty years ago parallel computing was a centerpiece of computer science research. Naively Big Data surely requires HPC to be processed, and transformational Big Data technology such as Hadoop and Spark exploit parallelism to success. Nevertheless, the HPC community does not appear to be thriving as a leader in Data Science while parallel computing is no longer a centerpiece. Some reasons for this are the dominant presence of Industry in technology futures and the universal fascination with Artificial Intelligence and Machine Learning. Maybe the pendulum will swing back a bit, but I expect the \"AI first\" philosophy to dominate in the foreseeable future. Thus I describe a future where HPC thrives in collaboration with Industry and AI. In particular, I discuss the promise of MLforHPC (AI for systems) and HPCforML (systems for AI).}, location = {Phoenix, AZ, USA}, series = {HPDC '19}, pages = {145}, numpages = {1}, keywords = {HPC, computational science, data science, big data}}
@inproceedings{10.1109/SEAMS.2017.20,title = {Self-adaptation based on big data analytics: a model problem and tool}, author = {Schmid Sanny , Gerostathopoulos Ilias , Prehofer Christian , Bures Tomas },year = {2017}, isbn = {9781538615508}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/SEAMS.2017.20}, doi = {10.1109/SEAMS.2017.20}, abstract = {In this paper, we focus on self-adaptation in large-scale software-intensive distributed systems. The main problem in making such systems self-adaptive is that their adaptation needs to consider the current situation in the whole system. However, developing a complete and accurate model of such systems at design time is very challenging. To address this, we present a novel approach where the system model consists only of the essential input and output parameters. Furthermore, Big Data analytics is used to guide self-adaptation based on a continuous stream of operational data. We provide a concrete model problem and a reference implementation of it that can be used as a case study for evaluating different self-adaptation techniques pertinent to complex large-scale distributed systems. We also provide an extensible tool for endorsing an arbitrary system with self-adaptation based on analysis of operational data coming from the system. To illustrate the tool, we apply it on the model problem.}, location = {Buenos Aires, Argentina}, series = {SEAMS '17}, pages = {102\u2013108}, numpages = {7}, keywords = {big data analytics, self-adaptation, model problem}}
@inproceedings{10.1145/2699026.2699116,title = {Distributed Search over Encrypted Big Data}, author = {Kuzu Mehmet , Islam Mohammad Saiful , Kantarcioglu Murat },year = {2015}, isbn = {9781450331913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2699026.2699116}, doi = {10.1145/2699026.2699116}, abstract = {Nowadays, huge amount of documents are increasingly transferred to the remote servers due to the appealing features of cloud computing. On the other hand, privacy and security of the sensitive information in untrusted cloud environment is a big concern. To alleviate such concerns, encryption of sensitive data before its transfer to the cloud has become an important risk mitigation option. Encrypted storage provides protection at the expense of a significant increase in the data management complexity. For effective management, it is critical to provide efficient selective document retrieval capability on the encrypted collection. In fact, considerable amount of searchable symmetric encryption schemes have been designed in the literature to achieve this task. However, with the emergence of big data everywhere, available approaches are insufficient to address some crucial real-world problems such as scalability.In this study, we focus on practical aspects of a secure keyword search mechanism over encrypted data. First, we propose a provably secure distributed index along with a parallelizable retrieval technique that can easily scale to big data. Second, we integrate authorization into the search scheme to limit the information leakage in multi-user setting where users are allowed to access only particular documents. Third, we offer efficient updates on the distributed secure index. In addition, we conduct extensive empirical analysis on a real dataset to illustrate the efficiency of the proposed practical techniques.}, location = {San Antonio, Texas, USA}, series = {CODASPY '15}, pages = {271\u2013278}, numpages = {8}, keywords = {privacy, security, searchable encryption}}
@inproceedings{10.1145/3019612.3019700,title = {From IoT big data to IoT big services}, author = {Taherkordi Amir , Eliassen Frank , Horn Geir },year = {2017}, isbn = {9781450344869}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3019612.3019700}, doi = {10.1145/3019612.3019700}, abstract = {The large-scale deployments of Internet of Things (IoT) systems have introduced several new challenges in terms of processing their data. The massive amount of IoT-generated data requires design solutions to speed up data processing, scale up with the data volume and improve data adaptability and extensibility. Beyond existing techniques for IoT data collection, filtering, and analytics, innovative service computing technologies are required for provisioning data-centric and scalable IoT services. This paper presents a service-oriented design model and framework for realizing scalable and efficient acquisition, processing and integration of data-centric IoT services. In this approach, data-centric IoT services are organized in a service integrating tree structure, adhering to the architecture of many large-scale IoT systems, including recent fog-based IoT computing models. A service node in the tree is called a Big Service and acts as an integrator, collecting data from lower level Big Services, processing them, and delivering the result to higher level IoT Big Services. The service tree thereby encapsulates required data processing functions in a hierarchical manner in order to achieve scalable and real-time data collection and processing. We have implemented the IoT Big Services framework leveraging a popular cloud-based service and data platform called Firebase, and evaluated its performance in terms of real-time requirements.}, location = {Marrakech, Morocco}, series = {SAC '17}, pages = {485\u2013491}, numpages = {7}, keywords = {big data, big services, internet of things}}
@inproceedings{10.1145/2815782.2815793,title = {Challenges to the Organisational Adoption of Big Data Analytics: A Case Study in the South African Telecommunications Industry}, author = {Malaka Iman , Brown Irwin },year = {2015}, isbn = {9781450336833}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2815782.2815793}, doi = {10.1145/2815782.2815793}, abstract = {The purpose of this interpretive study was to explore the challenges to the adoption of Big Data Analytics (BDA) in organisations. The Technology-Organisation-Environment (TOE) model was used to guide the study. Data was collected from a large telecommunication organization in South Africa. Seven participants, from both Information Technology (IT) and business were interviewed to gain a holistic overview of challenges towards the adoption of BDA. An inductive approach was used for analysis. Findings revealed technological challenges to the adoption of BDA as being Data Integration; Data Privacy; Return on Investment; Data Quality; Cost; Data Integrity; and Performance and Scalability. From the organizational perspective, the major challenges were Ownership and Control; Skills Shortages; Business Focus and Prioritisation; Training and Exposure; Silos; and Unclear Processes. From the environmental context there were no major challenges highlighted. Organisational challenges were deemed to be the major inhibitors to adoption of BDA}, location = {Stellenbosch, South Africa}, series = {SAICSIT '15}, pages = {1\u20139}, numpages = {9}, keywords = {South Africa, Big Data Analytics, Big Data, Technology Adoption}}
@inproceedings{10.1145/3374587.3374650,title = {Construction and Application of Big Data Analysis Platform for Enterprise}, author = {Shen Shaoyi , Li Bin , Li Situo },year = {2019}, isbn = {9781450376273}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3374587.3374650}, doi = {10.1145/3374587.3374650}, abstract = {A data revolution has been leading by big data, which have the extremely profound influence on the economic, social development and public life. This paper introduces the meaning of big data, and discusses the innovation and opportunity of enterprise under the perspective of big data. According to the information architecture, this paper supplies the basic construction of enterprise big data analysis platform, and suggests the strategy of application, which have certain realistic directive significance.}, location = {Normal, IL, USA}, series = {CSAI2019}, pages = {54\u201358}, numpages = {5}, keywords = {Shared, Data asset, Distributed, Construction, Analysis of big data}}
@inproceedings{10.1145/3383923.3383964,title = {Challenges and Countermeasures of Education in the Era of Big Data}, author = {Xu Qingzheng , Wang Na , Tian Balin , Xing Lipeng , Bai Wenhua },year = {2020}, isbn = {9781450375085}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3383923.3383964}, doi = {10.1145/3383923.3383964}, abstract = {The advent of the era of big data provides some new ideas for the reform and development of higher education. In this paper, the importance of educational big data is analyzed from six aspects: improving the effectiveness of teaching and learning, promoting scientific decision-making in education, completing the quality monitoring system, facilitating comprehensive evaluation of education quality, promoting the popularization and personalization of education, and improving personalized teaching. Then, some challenges faced in the application process of education big data are analyzed, including thinking mode, data sharing, data technology, talent support, and data security and privacy. Based on them, several feasible countermeasures are proposed from the aspects of enhancing data awareness, realizing open sharing, accelerating professional talent training and strengthening privacy protection. At last, some research directions of educational big data are put forward.}, location = {Oxford, United Kingdom}, series = {ICEIT 2020}, pages = {215\u2013218}, numpages = {4}, keywords = {Big data, higher education, challenges, countermeasures}}
@inproceedings{10.1145/3141248,title = {Validating Data Quality Actions in Scoring Processes}, author = {Cappiello C. , Cerletti C. , Fratto C. , Pernici B. },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3141248}, doi = {10.1145/3141248}, abstract = {Data quality has gained momentum among organizations upon the realization that poor data quality might cause failures and/or inefficiencies, thus compromising business processes and application results. However, enterprises often adopt data quality assessment and improvement methods based on practical and empirical approaches without conducting a rigorous analysis of the data quality issues and outcome of the enacted data quality improvement practices. In particular, data quality management, especially the identification of the data quality dimensions to be monitored and improved, is performed by knowledge workers on the basis of their skills and experience. Control methods are therefore designed on the basis of expected and evident quality problems; thus, these methods may not be effective in dealing with unknown and/or unexpected problems. This article aims to provide a methodology, based on fault injection, for validating the data quality actions used by organizations. We show how it is possible to check whether the adopted techniques properly monitor the real issues that may damage business processes. At this stage, we focus on scoring processes, i.e., those in which the output represents the evaluation or ranking of a specific object. We show the effectiveness of our proposal by means of a case study in the financial risk management area.}, pages = {1\u201327}, numpages = {27}, keywords = {Data quality, assessment, decision processes, decision support}}
@inproceedings{10.1145/3041021.3054141,title = {When Fashion Meets Big Data: Discriminative Mining of Best Selling Clothing Features}, author = {Chen Kuan-Ting , Luo Jiebo },year = {2017}, isbn = {9781450349147}, publisher = {International World Wide Web Conferences Steering Committee}, address = {Republic and Canton of Geneva, CHE}, url = {https://doi.org/10.1145/3041021.3054141}, doi = {10.1145/3041021.3054141}, abstract = {With the prevalence of e-commence websites and the ease of online shopping, consumers are embracing huge amounts of various options in products. Undeniably, shopping is one of the most essential activities in our society and studying consumer's shopping behavior is important for the industry as well as sociology and psychology. Indisputable, one of the most popular e-commerce categories is clothing business. There arises the needs for analysis of popular and attractive clothing features which could further boost many emerging applications, such as clothing recommendation and advertising. In this work, we design a novel system that consists of three major components: 1) exploring and organizing a large-scale clothing dataset from a online shopping website, 2) pruning and extracting images of best-selling products in clothing item data and user transaction history, and 3) utilizing a machine learning based approach to discovering fine-grained clothing attributes as the representative and discriminative characteristics of popular clothing style elements. Through the experiments over a large-scale online clothing shopping dataset, we demonstrate the effectiveness of our proposed system, and obtain useful insights on clothing consumption trends and profitable clothing features.}, location = {Perth, Australia}, series = {WWW '17 Companion}, pages = {15\u201322}, numpages = {8}, keywords = {data mining, image analysis, online shopping, big data, clothing features}}
@inproceedings{10.1145/2331042.2331047,title = {Six tips for students interested in big data analytics}, author = {Bhambhri Anjul },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2331042.2331047}, doi = {10.1145/2331042.2331047}, pages = {9}, numpages = {1}}
@inproceedings{10.1145/3505745.3505749,title = {Risk Monitoring analysis of dangerous chemical transportation on highway based on big Data}, author = {An Qing , Chen Xijiang , Tang Suixin , Deng Qian , Liu Fenggang },year = {2021}, isbn = {9781450384339}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3505745.3505749}, doi = {10.1145/3505745.3505749}, abstract = {Based on the in-depth discussion of the uncertain influencing factors of dangerous chemicals in the process of road transportation, the historical data of dangerous chemicals road transportation accidents are sorted out, and the variable consistency dominance rough set theory (VC-DRSA) is proposed to quantitatively analyze the importance of uncertain factors of environmental risk in dangerous chemicals transportation, Furthermore, it reveals the causal relationship between risk factors and risks and its irreducible rules, which provides a certain theoretical basis for alleviating and preventing the risk of road transportation of dangerous chemicals}, location = {Tokyo, Japan}, series = {ICBDR 2021}, pages = {23\u201327}, numpages = {5}, keywords = {risk analysis, variable consistency dominance relation, rough set theory (VC-DRSA), uncertain factors, Hazardous chemicals}}
@inproceedings{10.1109/CCGRID.2017.55,title = {Data-Aware Support for Hybrid HPC and Big Data Applications}, author = {Ca\u00edno-Lores Silvina , Isaila Florin , Carretero Jes\u00fas },year = {2017}, isbn = {9781509066100}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGRID.2017.55}, doi = {10.1109/CCGRID.2017.55}, abstract = {Nowadays there is a raising interest in bridging the gap between Big Data application models and data-intensive HPC. This work explores the effects that Big Data-inspired paradigms could have in current scientific applications through the evaluation of a real-world application from the hydrology domain. This evaluation led to experience that portrayed the key aspects of the HPC and Big Data paradigms that made them successful in their respective worlds. With this information, we established a research roadmap to build a platform suitable for HPC hybrid applications, with a focus on efficient data management and fault-tolerance.}, location = {Madrid, Spain}, series = {CCGrid '17}, pages = {719\u2013722}, numpages = {4}, keywords = {data-intensive computing, data management, scientific computing, big data, high-performance computing}}
@inproceedings{10.1145/2168931.2168932,title = {WELCOME Interacting with big data}, author = {Wakkary Ron , Stolterman Erik },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2168931.2168932}, doi = {10.1145/2168931.2168932}, pages = {5}, numpages = {1}}
@inproceedings{10.1109/CCGRID.2018.00052,title = {Service-oriented architecture for big data analytics in smart cities}, author = {Al-Jaroodi Jameela , Mohamed Nader },year = {2018}, isbn = {9781538658154}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGRID.2018.00052}, doi = {10.1109/CCGRID.2018.00052}, abstract = {A smart city has recently become an aspiration for many cities around the world. These cities are looking to apply the smart city concept to improve sustainability, quality of life for residents, and economic development. The smart city concept depends on employing a wide range of advanced technologies to improve the performance of various services and activities such as transportation, energy, healthcare, and education, while at the same time improve the city's resources utilization and initiate new business opportunities. One of the promising technologies to support such efforts is the big data technology. Effective and intelligent use of big data accumulated over time in various sectors can offer many advantages to enhance decision making in smart cities. In this paper we identify the different types of decision making processes involved in smart cities. Then we propose a service-oriented architecture to support big data analytics for decision making in smart cities. This architecture allows for integrating different technologies such as fog and cloud computing to support different types of analytics and decision-making operations needed to effectively utilize available big data. It provides different functions and capabilities to use big data and provide smart capabilities as services that the architecture supports. As a result, different big data applications will be able to access and use these services for varying proposes within the smart city.}, location = {Washington, District of Columbia}, series = {CCGrid '18}, pages = {633\u2013640}, numpages = {8}, keywords = {fog computing, cloud computing, smart city, big data, service-oriented architecture, middleware}}
@inproceedings{10.1145/3478432.3499100,title = {A Course on Data Quality in Analytics}, author = {Zhu Hongwei },year = {2022}, isbn = {9781450390712}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3478432.3499100}, doi = {10.1145/3478432.3499100}, abstract = {Data quality is important to analytics; data preparation usually involves data cleaning and is often the most time-consuming part of analytics projects. When the topic is left to the discretion of individual courses in an analytics program, students often end up with light exposure to the topic. Instead, a course on data quality in analytics has been designed and implemented. Organized in eight modules, the first part of the course covers data preparation and preprocessing. This prepares students with the ability to tackle real datasets in other analytics courses. The second part covers analytics for data quality where algorithms for detecting and resolving data quality issues are covered. The third part addresses large scale and engineering issues of analytics practice where data collection needs to be managed and data quality tasks must be part of the pipeline.}, location = {Providence, RI, USA}, series = {SIGCSE 2022}, pages = {1106}, numpages = {1}, keywords = {data preparation, data analytics, curriculum, data quality}}
@inproceedings{10.1145/3193063.3193069,title = {An Overview of Techniques for Confirming Big Data Property Rights}, author = {Cheng Susu , Zhao Haijun },year = {2018}, isbn = {9781450363785}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3193063.3193069}, doi = {10.1145/3193063.3193069}, abstract = {The major premise of big data circulation is to identify the ownership of data resource. This paper summed some feasible techniques and methods for confirming big data property which are data citation technology, data provenance technology, data reversible hiding technology, computer forensic technology and block chain technology. The ownership of information property which from different sizes, different formats and different storage condition on distributed heterogeneous platforms can be confirmed by comprehensive application of these techniques and methods based on the coupling interface between them in the practice of big data.}, location = {Ha Noi, Viet Nam}, series = {ICIIT 2018}, pages = {59\u201364}, numpages = {6}, keywords = {Information property index, Method for confirming information property rights, Big Data, Confirmation of Information Property}}
@inproceedings{10.1145/3134271.3134300,title = {Research on Evolution and Visualization Analysis of Application of Big Data}, author = {Zhang Yong'an , Zhang Yuxiaodan },year = {2017}, isbn = {9781450352765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3134271.3134300}, doi = {10.1145/3134271.3134300}, abstract = {Based on literatures relevant to application of big data included in the Web of Science database as the data sources, this paper used CiteSpace as the research tool to visualize the distribution of keywords, evolution of hot spots and evolution rules. After that, it analyzed the hot spots, contexts, tool technologies and application fields of application of big data, so as to reveal the current situation of researches. The study indicates that existing researches involve a wide range of disciplines and technology is still the center of current researches. At present, the majority of the studies all focused on the field of management, network, information, medicine, health, environment, energy and etc. Overall, technology is still the important part in current research on application of big data. Technical tools, such as data mining, algorithms, cloud computing, mapreduce, hadoop, machine learning all provide strong supports for the practical application of big data.}, location = {Bei Jing, China}, series = {ICBIM 2017}, pages = {126\u2013130}, numpages = {5}, keywords = {Knowledge map, CiteSpace, Application, Big data}}
@inproceedings{10.1145/3501409.3501593,title = {Tourism Prediction based on Multi-source Big Data Fusion Technology}, author = {Diao Yanhua },year = {2021}, isbn = {9781450384322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3501409.3501593}, doi = {10.1145/3501409.3501593}, abstract = {In the practical application of existing big data tourism prediction, there are some practical problems, such as complicated data sources and difficult fusion, low prediction accuracy and poor guiding practice effect. In view of this situation, this paper intends to build a tourism big data index prediction model suitable for the characteristics of tourism development through core data extraction, multi-source data fusion, complex data modeling and other key technologies. With the help of the improved tourism prediction model based on multi-source big data fusion technology, the tourist flow and consumption characteristics of Shandong province are more accurately identified and predicted. It can provide help for optimizing public service of tourism, strengthening early warning of tourist flow and improving marketing strategy of tourist destination. This study innovatively supplements the effective integration theory of multi-source tourism big data and the organic integration theory of big data and traditional sampling survey data. At the same time, the relevant methods of tourism big data forecasting model are extended.}, location = {Xiamen, China}, series = {EITCE 2021}, pages = {1030\u20131036}, numpages = {7}, keywords = {Multi-source big data, Data fusion technology, Tourism prediction}}
@inproceedings{10.1145/2345316.2345322,title = {Big data and advanced spatial analytics}, author = {Lopez Xavier },year = {2012}, isbn = {9781450311137}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2345316.2345322}, doi = {10.1145/2345316.2345322}, abstract = {Today's business and government organizations are challenged when trying to manage and analyze information from enterprise databases, streaming servers, social media and open source. This is compounded by the complexity of integrating diverse data types (relational, text, spatial, images, spreadsheets) and their representations (customers, products, suppliers, events, and locations) - all of which need to be understood and re-purposed in different contexts. Identifying meaningful patterns across these different information sources is non-trivial. Moreover, conventional IT tools, such as conventional data warehousing and business intelligence alone, are insufficient at handling the volumes, velocity and variety of content at hand. A new framework and associated tools are needed. Dr. Lopez outlines how data scientists and analysts are applying Spatial and Semantic Web concepts to make sense of this Big Data stream. He will describe new approaches oriented toward search, discovery, linking, and analyzing information on the Web, and throughout the enterprise. The role of Map Reduce is described, as is importance of engineered systems to simplify the creation and configuration of Big Data environments. The key take away is use of spatial and linked open data concepts to enhance content alignment, interoperability, discovery and analytics in the Big Data stream.}, location = {Washington, D.C., USA}, series = {COM.Geo '12}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/2617660,title = {Big data meets big science}, author = {Wright Alex },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2617660}, doi = {10.1145/2617660}, abstract = {Next-generation scientific instruments are forcing researchers to question the limits of massively parallel computing.}, pages = {13\u201315}, numpages = {3}}
@inproceedings{10.1145/1536616.1536632,title = {The pathologies of big data}, author = {Jacobs Adam },year = {2009}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1536616.1536632}, doi = {10.1145/1536616.1536632}, abstract = {Scale up your datasets enough and your apps come undone. What are the typical problems and where do the bottlenecks surface?}, pages = {36\u201344}, numpages = {9}}
@inproceedings{10.1145/2641398,title = {In big data we trust?}, author = {Lehikoinen Juha , Koistinen Ville },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2641398}, doi = {10.1145/2641398}, pages = {38\u201341}, numpages = {4}}
@inproceedings{10.1145/2345316.2345329,title = {Cloud/big data computing for defense}, author = {Pino Robinson E. },year = {2012}, isbn = {9781450311137}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2345316.2345329}, doi = {10.1145/2345316.2345329}, abstract = {The ever growing necessity for Big Data processing within the industry, government, and specially within defense applications causes the need and requirement for the fast development of new technologies. In addition, the protection of Big Data can be a serious problem because security is commonly an afterthought during technology development, and the exponentially increasing rate at which new data is generated presents many challenges. Although conventional Turing computation has been remarkably successful, it does not scale well and is failing to adapt to novel application domains in cyberspace. Fortunately, Turing formalism for computation represents only a subset of all possible computational possibilities. Unconventional computing - the quest for new algorithms and physical implementations of novel computing paradigms based on and inspired by principles of information processing in physical and biological systems - may help to solve some of the information overflow problems facing the Defense community. These and other topics will be covered by our diverse panel of experts.}, location = {Washington, D.C., USA}, series = {COM.Geo '12}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/3415958.3433077,title = {Big-Data Driven Digital Ecosystem Framework for Online Predictive Control}, author = {Suleykin Alexander , Bakhtadze Natalya , Panfilov Peter },year = {2020}, isbn = {9781450381154}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3415958.3433077}, doi = {10.1145/3415958.3433077}, abstract = {In this paper, Big-Data Driven Digital Ecosystem Framework (BDDDEF) for Online Predictive Control Systems is created. The proposed framework consists of different Agents, where each Agent is a distributed and virtual service. In our work, we provide solutions to the Big Data challenges in building Digital Ecosystems for Online Control including high volumes, velocity and variety of data, and the need for low data latency. We propose to use BDDDEF for building robust, reliable, fault-tolerant, scalable and high-loaded data pipelines for Online Predictive Control Systems. We review Big Data Main Systems for Online Predictive Control Architecture, review the literature for Digital Ecosystems design for Control Systems Online, design and describe main features, main architectural components and functional architecture of the framework, and finally, propose new Predictive Control methodology for Online Predictions.}, location = {Virtual Event, United Arab Emirates}, series = {MEDES '20}, pages = {92\u201395}, numpages = {4}, keywords = {Digital ecosystem framework, Big Data, Predictive control, Message-oriented middleware, Inmemory computing}}
@inproceedings{10.1145/3205977.3205998,title = {Access Control in the Era of Big Data: State of the Art and Research Directions}, author = {Colombo Pietro , Ferrari Elena },year = {2018}, isbn = {9781450356664}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3205977.3205998}, doi = {10.1145/3205977.3205998}, abstract = {Data security and privacy issues are magnified by the volume, the variety, and the velocity of Big Data and by the lack, up to now, of a standard data model and related data manipulation language. In this paper, we focus on one of the key data security services, that is, access control, by highlighting the differences with traditional data management systems and describing a set of requirements that any access control solution for Big Data platforms may fulfill. We then describe the state of the art and discuss open research issues.}, location = {Indianapolis, Indiana, USA}, series = {SACMAT '18}, pages = {185\u2013192}, numpages = {8}, keywords = {big data, privacy, NOSQL data management systems, access control}}
@inproceedings{10.1145/3357292.3357302,title = {Big Data Informatization Applied to Optimization of Human Resource Performance Management}, author = {Kun-fa Li , Jing-chun Chen , Yan-xi Wang },year = {2019}, isbn = {9781450371445}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3357292.3357302}, doi = {10.1145/3357292.3357302}, abstract = {With the development of technology in the era of digital big data in the network and the promotion of network technology, big data is simultaneously integrated into different industry sectors to achieve Internet performance management, and enhance the new perspective of enterprise human resources performance management activities. Today's Internet, cloud computing, Internet of Things and other industrial technologies have undergone repeated changes, showing an unprecedented picture. At present, the subjective awareness of enterprise human resources performance management is too strong, lack of objective data understanding, and the theoretical framework of big data human resource management is not fully applied. This paper reconstructs the data system from four aspects: data source, collection, integration and analysis. Innovate the human resources performance management method from the system to provide more scientific and specific ideas for human resource performance management.}, location = {Chengdu, China}, series = {IMMS 2019}, pages = {12\u201317}, numpages = {6}, keywords = {performance management, human resources, Big data}}
@inproceedings{10.1145/2675743.2771878,title = {Reliable event messaging in big data enterprises: looking for the balance between producers and consumers}, author = {Zhang Tianning },year = {2015}, isbn = {9781450332866}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2675743.2771878}, doi = {10.1145/2675743.2771878}, abstract = {Event-driven enterprise IT architectures are typically based on message broker systems. Most broker systems focus on functional decoupling, and on the fast and reliable delivery of events towards event consumers. Such consumer-centricity eases the development and deployment of consumers applications. However, especially within a web-based big data enterprise we typically have relatively small number of stable producers that is based on the core enterprise business model, and a higher, dynamic number of highly heterogeneous consumers. With the consumer-centric view of event messaging, problems with some consumers can easily lead to the congestion or service degradation of the whole messaging system, and can affect the operations of the producers or other critical consumers. To avoid such non-functional tight coupling, we propose in this paper a producer-centric architecture for messaging backbone in a big data enterprise. Within this architecture the broker infrastructure concentrates on the event production activities and pushes the delivery functionalities towards the consumer clients and applications. In this way we can achieve a higher level of SOA loose coupling, higher reliability and higher maintainability of the enterprise infrastructure.}, location = {Oslo, Norway}, series = {DEBS '15}, pages = {226\u2013233}, numpages = {8}, keywords = {event messaging, kafka, reliability, redis, NoSQL, producer-centricity, event backbone, big data}}
@inproceedings{10.1145/3332301,title = {Orchestrating Big Data Analysis Workflows in the Cloud: Research Challenges, Survey, and Future Directions}, author = {Barika Mutaz , Garg Saurabh , Zomaya Albert Y. , Wang Lizhe , Moorsel Aad Van , Ranjan Rajiv },year = {2019}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3332301}, doi = {10.1145/3332301}, abstract = {Interest in processing big data has increased rapidly to gain insights that can transform businesses, government policies, and research outcomes. This has led to advancement in communication, programming, and processing technologies, including cloud computing services and technologies such as Hadoop, Spark, and Storm. This trend also affects the needs of analytical applications, which are no longer monolithic but composed of several individual analytical steps running in the form of a workflow. These big data workflows are vastly different in nature from traditional workflows. Researchers are currently facing the challenge of how to orchestrate and manage the execution of such workflows. In this article, we discuss in detail orchestration requirements of these workflows as well as the challenges in achieving these requirements. We also survey current trends and research that supports orchestration of big data workflows and identify open research challenges to guide future developments in this area.}, pages = {1\u201341}, numpages = {41}, keywords = {research taxonomy, and techniques, Big data, cloud computing, approaches, workflow orchestration}}
@inproceedings{10.1145/3377170.3377180,title = {How Big Data Analytics Impacts Agility: The Moderation Effect of Orientation of Interactive Team Cognition}, author = {Liu Qinxian , Hyun Youyung , Hosoya Ryuichi , Kamioka Taro },year = {2019}, isbn = {9781450376631}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377170.3377180}, doi = {10.1145/3377170.3377180}, abstract = {Nowadays, organizations have been faced with the rapidly changing environment; thus, in order to survive in such volatile business environment, agility which is the ability to sense opportunities and defend threats has become a critical issue. Big data analytics (BDA) has been known to positively influence the agility; however, to our knowledge, the impact of BDA use on agility has not been studied abundantly in relation to the perspective of team cognition. To fill this research gap, we developed a new construct called orientation of interactive team cognition (OITC) based on the interactive team cognition (ITC) theory. After analyzing the survey data from 173 respondents, our paper found that OITC plays a positive role in moderating the relationship between the use of BDA and agility.}, location = {Shanghai, China}, series = {ICIT 2019}, pages = {34\u201339}, numpages = {6}, keywords = {performance, agility, orientation of interactive team cognition, Big data analytics}}
@inproceedings{10.1145/3314074.3314097,title = {Challenges and Benefits of Deploying Big Data Storage Solution}, author = {Kachaoui Jabrane , Belangour Abdessamad },year = {2019}, isbn = {9781450361293}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3314074.3314097}, doi = {10.1145/3314074.3314097}, abstract = {Since data is at the heart of information systems, new technologies and approaches dealing with storing, processing and analyzing data have proliferated. Data Warehouses are among the most known approaches that tackle data storing and processing. However, they reached their limits in dealing with large quantities of data as those of Big Data. Consequently, a new concept which is an evolution of Data Warehouse known as \"Data Lake\" is emerging. This paper presents a detailed analysis that compares Data Lake and Data Warehouse key concepts. It sheds lights on the aspects and characteristics for the sake of revealing similarities and differences. It also emphasizes the complementary of the two technologies by showing the most appropriate use case of each of them.}, location = {Kenitra, Morocco}, series = {SMC '19}, pages = {1\u20135}, numpages = {5}, keywords = {Data Mart, Repository, Big Data, Hadoop, Data Warehouse, Data Lake, NoSQL, Ad Hoc, Distributed databases}}
@inproceedings{10.1145/2258056.2258058,title = {Spatial big-data challenges intersecting mobility and cloud computing}, author = {Shekhar Shashi , Gunturi Viswanath , Evans Michael R. , Yang KwangSoo },year = {2012}, isbn = {9781450314428}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2258056.2258058}, doi = {10.1145/2258056.2258058}, abstract = {Increasingly, location-aware datasets are of a size, variety, and update rate that exceeds the capability of spatial computing technologies. This paper addresses the emerging challenges posed by such datasets, which we call Spatial Big Data (SBD). SBD examples include trajectories of cellphones and GPS devices, vehicle engine measurements, temporally detailed road maps, etc. SBD has the potential to transform society via next-generation routing services such as eco-routing. However, the envisaged SBD-based next-generation routing services pose several significant challenges for current routing techniques. SBD magnifies the impact of partial information and ambiguity of traditional routing queries specified by a start location and an end location. In addition, SBD challenges the assumption that a single algorithm utilizing a specific dataset is appropriate for all situations. The tremendous diversity of SBD sources substantially increases the diversity of solution methods. Newer algorithms may emerge as new SBD becomes available, creating the need for a flexible architecture to rapidly integrate new datasets and associated algorithms.}, location = {Scottsdale, Arizona}, series = {MobiDE '12}, pages = {1\u20136}, numpages = {6}, keywords = {data mining, spatial big data, mobility services}}
@inproceedings{10.1145/3330431.3330447,title = {Lemmatization of big data in the Kazakh language}, author = {Rakhimova Diana , Turganbayeva Aliya },year = {2019}, isbn = {9781450372121}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3330431.3330447}, doi = {10.1145/3330431.3330447}, abstract = {In paper are considered existing algorithms for automatically isolating the bases for a number of natural languages and possible ways of synthesizing a normal form of a word for the Kazakh language. In paper are described the complete system of endings of the Kazakh language. The article presents the classification of affixes Kazakh language. The paper proposes a new approach to constructing a lemmatization algorithm for the Kazakh language on the basis of a complete set of endings of the Kazakh language. The lemmatization algorithm will be used for Kazakh language information retrieval to finding specific words in the documents by stemming base of word.}, location = {Astana, Kazakhstan}, series = {ICEMIS '19}, pages = {1\u20134}, numpages = {4}, keywords = {algorithm, language, retrieval, information, big data, lemmatization, Kazakh}}
@inproceedings{10.1145/3372938,title = {Proceedings of the 4th International Conference on Big Data and Internet of Things},year = {2019}, isbn = {9781450372404}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Rabat, Morocco}}
@inproceedings{10.1145/3423603.3424049,title = {Decisional architectures from business intelligence to big data: challenges and opportunities}, author = {Aissa Mohamed Mehdi Ben , Sfaxi Lilia , Robbana Riadh },year = {2020}, isbn = {9781405377539}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3423603.3424049}, doi = {10.1145/3423603.3424049}, abstract = {Information is one of the most important factors in business success, hence the importance of the Business Intelligence (BI) domain in order to simplify the decision making and make it more relevant. Decisional systems have been used for several years to help decision-makers access, analyze and extract value from the data that their organisation accumulated through the years. The success gained by these types of systems caused the establishment of a well-known architecture and development chain, and the proliferation of tools and methodologies that have proven their value. Nonetheless, in some use cases, the classical decisional architecture shows some shortcomings. In fact, the traditional storage and processing models in Business Intelligence systems are not sufficient anymore when confronted with data that becomes more and more massive, varied and with a high velocity. This is where Big Data solutions can be of great use. In fact, these solutions have proven their efficiency when dealing with enormous constantly increasing amounts of data with a changing schema. Our goal in this article is to show the various manners to integrate big data solutions into the decisional world, and to help architects choose which architecture corresponds better to their needs, by taking into consideration the environmental, technical and functional constraints they are faced with.}, location = {Virtual Event, Tunisia}, series = {DTUC '20}, pages = {1\u20139}, numpages = {9}, keywords = {business intelligence, decision tree for decisional architectures, data lake, big data, data warehouse, decisional systems, OLAP, software architecture}}
@inproceedings{10.1145/3535735.3535742,title = {Explore and practice of online and offline mixed teaching based on SPOC \u2014 uses the big data major as an example}, author = {Li Xin , Li Yongkui },year = {2022}, isbn = {9781450396196}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3535735.3535742}, doi = {10.1145/3535735.3535742}, abstract = {One of the core connotations of smart classroom is personalization, which abandons the neat and uniform knowledge transfer mode of traditional classroom. After analyzing the advantages and disadvantages of online and offline teaching mode, we focus on the principle and application of big data technology . The lecturer has made innovative designs in teaching mode, teaching content and curriculum evaluation system. It is true to change standardized teaching into personalized teaching that respects learning individuals and promotes students' knowledge construction and literacy improvement . We are teaching students according to their aptitude. At the same time, adhere to the people-oriented teaching concept and use data to guide teaching, but not too much rely on data, change the way of thinking, and transform teaching from industrialized production to agricultural deep-rooted and meticulous work to create good teaching ecological environment.}, location = {Belgrade, Serbia}, series = {ICIEI '22}, pages = {41\u201346}, numpages = {6}, keywords = {multidimensional evaluation, mixture teaching, Big data technology}}
@inproceedings{10.1145/3167918.3167924,title = {A survey on big data stream processing in SDN supported cloud environment}, author = {Al-Mansoori Ahmed , Yu Shui , Xiang Yong , Sood Keshav },year = {2018}, isbn = {9781450354363}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3167918.3167924}, doi = {10.1145/3167918.3167924}, abstract = {Big data is the term which denotes data with features such as voluminous data, a variety of data and streaming data as well. Processing big data became essential for enterprises to garner general intelligence and avoid biased conclusions. Due to these features, big data processing is considered to be a challenging task. Big data Processing should rely on a robust network. Cloud computing offers a suitable environment for these processes. However, it is more challenging when we move big data to the cloud, as managing the cloud resources is the main issue. Software Defined Network (SDN) has a potential solution to this issue. In this paper, first, we survey the present state of the art of SDN, cloud computing, and Big data Stream processing (BDSP). Then, we discuss SDN in the context of Big Data Stream Processing in Cloud environment. Finally, critical issues and research opportunity are discussed.}, location = {Brisband, Queensland, Australia}, series = {ACSW '18}, pages = {1\u201311}, numpages = {11}, keywords = {SDN, resource optimization, cloud computing, cost optimization, big data, big data stream processing}}
@inproceedings{10.1145/2684822.2685326,title = {Making Sense of Big Data with the Berkeley Data Analytics Stack}, author = {Franklin Michael },year = {2015}, isbn = {9781450333177}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2684822.2685326}, doi = {10.1145/2684822.2685326}, abstract = {The Berkeley AMPLab is creating a new approach to data analytics. Launching in early 2011, the lab aims to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and in crowds). The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the four years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark in-memory computation framework, and the Shark query processing system. BDAS features prominently in many industry discussions of the future of the Big Data analytics ecosystem -- a rare degree of impact for an ongoing academic project. Given this initial success, the lab is continuing on its research path, moving \"up the stack\" to better integrate and support advanced analytics and to make people a full-fledged resource for making sense of data. In this talk, I'll first outline the motivation and insights behind our research approach and describe how we have organized to address the cross-disciplinary nature of Big Data challenges. I will then describe the current state of BDAS with an emphasis on our newest efforts, including some or all of: the GraphX graph processing system, the Velox and MLBase machine learning platforms, and the SampleClean framework for hybrid human/computer data cleaning. Finally I will present our current views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.}, location = {Shanghai, China}, series = {WSDM '15}, pages = {1\u20132}, numpages = {2}, keywords = {big data}}
@inproceedings{10.1145/2818869.2818904,title = {Rethinking the Destination Marketing Organization Management in the Big Data Era}, author = {Yuan Yu-Lan , Ho Chaang-Iuan },year = {2015}, isbn = {9781450337359}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2818869.2818904}, doi = {10.1145/2818869.2818904}, abstract = {Big data is an important part of modern information management, providing new strategies for Destination Marketing Organizations (DMOs) for DMOs to gather large amount of data from tourists and stakeholders. DMOs that have the domain knowledge to analyze these data can take the opportunities presented by Big Data. This work describes the opportunities and challenges presented to DMOs in use of Big Data.}, location = {Kaohsiung, Taiwan}, series = {ASE BD&amp;SI '15}, pages = {1\u20135}, numpages = {5}, keywords = {Challenges, Big Data, Opportunities, Destination Management Organization}}
@inproceedings{10.1145/2623330.2630819,title = {Big data for social good}, author = {Eagle Nathan },year = {2014}, isbn = {9781450329569}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2623330.2630819}, doi = {10.1145/2623330.2630819}, abstract = {Petabytes of data about human movements, transactions and communication patterns are being generated by everyday technologies such as mobile phones & credit cards. This unprecedented volume of information facilitates a novel set of research questions applicable to a wide range of development issues. In collaboration involving 237 mobile phone operators across 102 countries, Jana's mobile technology platform can instantly poll and compensate 3.48 billion active mobile subscriptions. This talk will discuss how insights gained from living in Kenya became the genesis of a technology company currently working with global clients in over 50 countries, including P&G, Google, Unilever, Danone, General Mills, Nestle, Johnson & Johnson, Microsoft, the World Bank, and the United Nations. After providing an overview of the mobile and social media landscapes in emerging markets, we discuss a system that implements polls & mobile subscription compensation. The presentation will conclude by emphasizing the value of consumer data in underserved and understudied regions of the world.}, location = {New York, New York, USA}, series = {KDD '14}, pages = {1522}, numpages = {1}, keywords = {polling systems, emerging markets, mobile technology, social media}}
@inproceedings{10.5555/2693848.2694145,title = {Big data in daily manufacturing operations}, author = {Wilschut Tim , Adan Ivo J. B. F. , Stokkermans Joep },year = {2014}, publisher = {IEEE Press}, abstract = {Big data analytics is at the brink of changing the landscape in NXP Semiconductors Back End manufacturing operations. Numerous IT tools, implemented over the last decade, collect gigabytes of data daily, though the potential value of this data still remains to be explored. In this paper, the software tool called Heads Up is presented. Heads Up intelligently scans, filters, and explores the data with use of simulation. The software provides real-time relevant information, which is of high value in daily, as well as long term, production management. The software tool has been introduced at the NXP high volume manufacturing plant GuangDong China, where it is about to shift the paradigm on manufacturing operations.}, location = {Savannah, Georgia}, series = {WSC '14}, pages = {2364\u20132375}, numpages = {12}}
@inproceedings{10.1145/2436256.2436263,title = {Looking back at big data}, author = {Hoffmann Leah },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2436256.2436263}, doi = {10.1145/2436256.2436263}, abstract = {As computational tools open up new ways of understanding history, historians and computer scientists are working together to explore the possibilities.}, pages = {21\u201323}, numpages = {3}}
@inproceedings{10.1145/3361758,title = {Proceedings of the 3rd International Conference on Big Data and Internet of Things},year = {2019}, isbn = {9781450372466}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {We like to start by first acknowledging the traditional custodians of the lands, where La Trobe University campuses are located in Victoria. We recognize their continuing connection to land, waters and culture and we pay our respects to their Elders past, present and emerging.}, location = {Melbourn, VIC, Australia}}