@inproceedings{10.1145/3361758,title = {Proceedings of the 3rd International Conference on Big Data and Internet of Things},year = {2019}, isbn = {9781450372466}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {We like to start by first acknowledging the traditional custodians of the lands, where La Trobe University campuses are located in Victoria. We recognize their continuing connection to land, waters and culture and we pay our respects to their Elders past, present and emerging.}, location = {Melbourn, VIC, Australia}}
@inproceedings{10.14778/2735461.2735465,title = {In-memory performance for big data}, author = {Graefe Goetz , Volos Haris , Kimura Hideaki , Kuno Harumi , Tucek Joseph , Lillibridge Mark , Veitch Alistair },year = {2014}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2735461.2735465}, doi = {10.14778/2735461.2735465}, abstract = {When a working set fits into memory, the overhead imposed by the buffer pool renders traditional databases non-competitive with in-memory designs that sacrifice the benefits of a buffer pool. However, despite the large memory available with modern hardware, data skew, shifting workloads, and complex mixed workloads make it difficult to guarantee that a working set will fit in memory. Hence, some recent work has focused on enabling in-memory databases to protect performance when the working data set almost fits in memory. Contrary to those prior efforts, we enable buffer pool designs to match in-memory performance while supporting the \"big data\" workloads that continue to require secondary storage, thus providing the best of both worlds. We introduce here a novel buffer pool design that adapts pointer swizzling for references between system objects (as opposed to application objects), and uses it to practically eliminate buffer pool overheads for memoryresident data. Our implementation and experimental evaluation demonstrate that we achieve graceful performance degradation when the working set grows to exceed the buffer pool size, and graceful improvement when the working set shrinks towards and below the memory and buffer pool sizes.}, pages = {37\u201348}, numpages = {12}}
@inproceedings{10.1145/269012.269021,title = {Examining data quality}, author = {Tayi Giri Kumar , Ballou Donald P. },year = {1998}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/269012.269021}, doi = {10.1145/269012.269021}, pages = {54\u201357}, numpages = {4}}
@inproceedings{10.1145/3377571.3377600,title = {Research on Motivation and Regulation of Big Data \"Slaughter\" Behavior}, author = {Zihaoran Wang },year = {2020}, isbn = {9781450372947}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377571.3377600}, doi = {10.1145/3377571.3377600}, abstract = {Big data \"killing\" as a new manifestation of price discrimination in the era of big data refers to the service provider as a party with information superiority, using the large amount of customer information flow to distinguish the pricing of each individual consumer. Consumer surplus value may be captured, which seriously reduces the level of consumer welfare. Internet companies with monopoly status such as Amazon, Didi, and Orbitz have all been expelled to adopt a pricing strategy of big data. However, the existing literature mainly discusses how to regulate big data at the legal level, and there is less discussion about big data killing itself as a kind of primary price discrimination phenomenon. This article aims to address the following two questions: 1) What is the motivation behind the vendor's big data killing strategy? 2) How to regulate from the perspective of network economics for big data killing? In the discussion of the first problem, using the mathematical modeling method to start the profit maximization by the monopolist, the local aggregation coefficient in the network is used to describe the risk of the manufacturer adopting the big data killing strategy, so that it is not adopted. The profit of the killing strategy and the expected profit of adopting the killing strategy are the motivation of the manufacturer to adopt the big data killing strategy. In the discussion of the second question, taking the comparison between the expected profit of the killing strategy and the profit under normal operation as the starting point, explore how to use the degree of communication between consumers and rationally introduce the competitive market to smash the big data. Conduct regulation. Finally, this paper studies the interdisciplinary issue and provides the relevant government departments with the idea of regulating big data killing strategies under the perspective of network economics.}, location = {Osaka, Japan}, series = {IC4E 2020}, pages = {407\u2013411}, numpages = {5}, keywords = {Big data killing, consumer surplus, clustering coefficient, full price discrimination}}
@inproceedings{10.5555/3233397.3233441,title = {An efficient and privacy-preserving similarity evaluation for big data analytics}, author = {Gheid Zakaria , Challal Yacine },year = {2015}, isbn = {9780769556970}, publisher = {IEEE Press}, abstract = {Big data systems are gathering more and more information in order to discover new values through data analytics and depth insights. However, mining sensitive personal information breaches privacy and degrades services' reputation. Accordingly, many research works have been proposed to address the privacy issues of data analytics, but almost seem to be not suitable in big data context either in data types they support or in computation time efficiency. In this paper we propose a novel privacy-preserving cosine similarity computation protocol that will support both binary and numerical data types within an efficient computation time, and we prove its adequacy for big data high volume, high variety and high velocity.}, location = {Limassol, Cyprus}, series = {UCC '15}, pages = {281\u2013289}, numpages = {9}, keywords = {data analytics, cosine similarity, big data, privacy}}
@inproceedings{10.1145/3151759.3151780,title = {A new knowledge capitalization framework in big data context}, author = {Hirchoua Badr , Ouhbi Brahim , Frikh Bouchra },year = {2017}, isbn = {9781450352994}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3151759.3151780}, doi = {10.1145/3151759.3151780}, abstract = {In many companies data is used as a source for creating knowledge in their sphere of business. Therefore this paper presents a knowledge capitalization framework in big data context. Based on the technology derived from distributed systems, our research concerns the design and development of a knowledge engineering framework in big data context. It can be integrated in any knowledge management system. The proposed framework is based on four layers: we start by extracting hidden topics, using the LDA approach in batch processing to handle the complexity of multi knowledge domains and to keep the semantic relations between knowledge entities. Then we use clustering mechanisms to pick the best combination between topics from different sources. As a result, we get, in every distributed site, the related topics (knowledge), in order to facilitate the research and access, to get the useful knowledge in real time processing.}, location = {Salzburg, Austria}, series = {iiWAS '17}, pages = {40\u201348}, numpages = {9}, keywords = {machine learning, data intelligence, knowledge capitalization, topic modeling, big data computing}}
@inproceedings{10.1145/3491396.3506516,title = {Research of Big Data Storage System Based on Underground Space Information}, author = {Wang Chunxiao , Zhao Zhigang , Zhang Jian , Huo Jidong },year = {2022}, isbn = {9781450391603}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491396.3506516}, doi = {10.1145/3491396.3506516}, abstract = {In order to support the dynamic perception neural network of underground space and the intelligent brain of the city, according to the characteristics of multi-source, multi-category, multidimensional and multi-quantity of geological data, this paper studies the large data storage system which integrates multi-source acquisition and converged storage and intelligent processing to solve the problems of wide range, long time, multidimensional source and diverse processing of underground space information. This system promotes the combination of sensor network, big data and other technologies with the urban underground space perception industry, realizes the digitalization and intellectualization of various underground space information, improves the planning, risk assessment and disaster prediction of underground space, and provides support for the comprehensive development and utilization of underground space.}, location = {Jinan, China}, series = {ACM ICEA '21}, pages = {234\u2013239}, numpages = {6}, keywords = {Big data storage system, Big data, Underground space information}}
@inproceedings{10.1145/3555962,title = {Proceedings of the 2022 6th International Conference on Cloud and Big Data Computing},year = {2022}, isbn = {9781450396578}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Birmingham, United Kingdom}}
@inproceedings{10.1145/2967938.2967957,title = {Auto-tuning Spark Big Data Workloads on POWER8: Prediction-Based Dynamic SMT Threading}, author = {Jia Zhen , Xue Chao , Chen Guancheng , Zhan Jianfeng , Zhang Lixin , Lin Yonghua , Hofstee Peter },year = {2016}, isbn = {9781450341219}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2967938.2967957}, doi = {10.1145/2967938.2967957}, abstract = {Much research work devotes to tuning big data analytics in modern data centers, since %the truth that even a small percentage of performance improvement immediately translates to huge cost savings because of the large scale. Simultaneous multithreading (SMT) receives great interest from data center communities, as it has the potential to boost performance of big data analytics by increasing the processor resources utilization. For example, the emerging processor architectures like POWER8 support up to 8-way multithreading. However, as different big data workloads have disparate architectural characteristics, how to identify the most efficient SMT configuration to achieve the best performance is challenging in terms of both complex application behaviors and processor architectures. In this paper, we specifically focus on auto-tuning SMT configuration for Spark-based big data workloads on POWE-R8. However, our methodology could be generalized and extended to other programming software stacks and other architectures.We propose a prediction-based dynamic SMT threading (PBDST) framework to adjust the thread count in SMT cores on POWER8 processors by using versatile machine learning algorithms.Its innovation lies in adopting online SMT configuration predictions derived from micro-architecture level profiling, to regulate the thread counts that could achieve nearly optimal performance. Moreover it is implemented at Spark software stack layer and transparent to user applications. After evaluating a large set of machine learning algorithms, we choose the most efficient ones to perform online predictions. The experimental results demonstrate that our approach can achieve up to 56.3% performance improvement and an average performance gain of 16.2% in comparison with the default configuration---the maximum SMT configuration---SMT8 on our system.}, location = {Haifa, Israel}, series = {PACT '16}, pages = {387\u2013400}, numpages = {14}, keywords = {dynamic smt tuning, spark big data, power8}}
@inproceedings{10.1145/1012453.1012465,title = {Data quality assessment from the user's perspective}, author = {Cappiello Cinzia , Francalanci Chiara , Pernici Barbara },year = {2004}, isbn = {1581139020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1012453.1012465}, doi = {10.1145/1012453.1012465}, abstract = {The quality of data is often defined as \"fitness for use\", i.e., the ability of a data collection to meet user requirements. The assessment of data quality dimensions should consider the degree to which data satisfy users' needs. User expectations are clearly related to the selected services and at the same time a service can have different characteristics depending on the type of user that accesses it. The data quality assessment process has to consider both aspects and, consequently, select a suitable evaluation function to obtain a correct interpretation of results. This paper proposes a model that ties the assessment phase to user requirements. Multichannel information systems are considered as an example to show the applicability of the proposed model.}, location = {Paris, France}, series = {IQIS '04}, pages = {68\u201373}, numpages = {6}, keywords = {data quality, user requirements, quality assessment}}
@inproceedings{10.1145/2168931.2168943,title = {Interactions with big data analytics}, author = {Fisher Danyel , DeLine Rob , Czerwinski Mary , Drucker Steven },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2168931.2168943}, doi = {10.1145/2168931.2168943}, pages = {50\u201359}, numpages = {10}}
@inproceedings{10.1145/2897010.2897014,title = {Extending search-based software testing techniques to big data applications}, author = {Fredericks Erik M. , Hariri Reihaneh H. },year = {2016}, isbn = {9781450341660}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2897010.2897014}, doi = {10.1145/2897010.2897014}, abstract = {Massive datasets are quickly becoming a concern for many industries. For example, many web-based applications must be able to handle petabytes worth of transactions on a daily basis, and moreover, be able to quickly and efficiently act upon data that exists in each transaction. As a result, providing testing capabilities for such applications becomes a challenge of scale. We argue that existing approaches, such as automated test suite generation, may not necessarily scale without assistance. To this end, we discuss open issues and possible solutions specific to testing big data applications.}, location = {Austin, Texas}, series = {SBST '16}, pages = {41\u201342}, numpages = {2}, keywords = {search-based software testing, big data, test suite generation}}
@inproceedings{10.1145/2460999.2461024,title = {Data quality in empirical software engineering: a targeted review}, author = {Bosu Michael Franklin , MacDonell Stephen G. },year = {2013}, isbn = {9781450318488}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2460999.2461024}, doi = {10.1145/2460999.2461024}, abstract = {Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to September 2012. A total of 221 relevant studies met our inclusion criteria and were characterized in terms of their consideration and treatment of data quality. Results: We obtained useful insights as to how the ESE community considers these three elements of data quality. Only 23 of these 221 studies reported on all three elements of data quality considered in this paper. Conclusion: The reporting of data collection procedures is not documented consistently in ESE studies. It will be useful if data collection challenges are reported in order to improve our understanding of why there are problems with software engineering data sets and the models developed from them. More generally, data quality should be given far greater attention by the community. The improvement of data sets through enhanced data collection, pre-processing and quality assessment should lead to more reliable prediction models, thus improving the practice of software engineering.}, location = {Porto de Galinhas, Brazil}, series = {EASE '13}, pages = {171\u2013176}, numpages = {6}, keywords = {data sets, empirical software engineering, data quality, literature review}}
@inproceedings{10.1145/3331453.3362052,title = {Analysis of Behavioral Characteristics Based on Student's Personal Big Data}, author = {Shu Jiangbo , Peng Liyuan , Hu Qianqian , Tan Fengxia , Ge Xiong },year = {2019}, isbn = {9781450362948}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3331453.3362052}, doi = {10.1145/3331453.3362052}, abstract = {With the continuous improvement of the information construction of colleges and universities, the daily life and learning behaviors of college students are recorded and stored by major business systems, and they are accumulated, which has initially formed a large-scale and multi-type student personal big data environment.This paper mainly classifies and summarizes the students' data from the three aspects of student basic information, campus learning and campus life. It focuses on the feature extraction and index mining of students' campus consumption, curriculum and performance data, and constructs the student's personal big data behavior analysis model. In-depth analysis and mining of student consumption behavior data to explore students' dietary rules and consumption level. Through data analysis, the following rules were found: 1)The total number of students eating at school decreases year by year, and the breakfast rate decreases year by year; 2) Freshmen are one hour ahead of the \"peak period\" of breakfast meals for the whole group;3) The students' academic scores are highly correlated with the meal rate, breakfast meal rate and eating consumption level, and are less correlated with variables such as window selection stability, etc. 4) The more regular the student's diet, the more stable the level of consumption, and the higher the level of learning effort, the better the student's academic performance.}, location = {Sanya, China}, series = {CSAE 2019}, pages = {1\u20137}, numpages = {7}, keywords = {Correlation analysis, Behavior analysis, Education big data, Student personal big data}}
@inproceedings{10.5555/3375069.3375113,title = {Applying Big Data Technologies to Manage QoS in an SDN}, author = {Jain Shashwat , Khandelwal Manish , Katkar Ashutosh , Nygate Joseph },year = {2016}, isbn = {9783901882852}, publisher = {International Federation for Information Processing}, address = {Laxenburg, AUT}, abstract = {Managing QoS in a telecommunications network is a complex process. Effective network design and sizing in conjunction with load balancing, access control and traffic prioritization need to be orchestrated to optimize CAPEX investment, maximize network utilization and ensure that performance metrics and SLAs are met. This work shows how big data analytics were used to improve the management of QoS in an SDN by performing multi-dimensional analysis of Key Performance Indicators (KPIs) and applying machine learning algorithms to discover new correlations, perform root cause analysis and predict traffic congestion.}, location = {Montreal, Quebec, Canada}, series = {CNSM 2016}, pages = {302\u2013306}, numpages = {5}, keywords = {Performance Management, tSDN, data movement, QoS, Big Data}}
@inproceedings{10.1145/3338906.3338939,title = {Going big: a large-scale study on what big data developers ask}, author = {Bagherzadeh Mehdi , Khatchadourian Raffi },year = {2019}, isbn = {9781450355728}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3338906.3338939}, doi = {10.1145/3338906.3338939}, abstract = {Software developers are increasingly required to write big data code. However, they find big data software development challenging. To help these developers it is necessary to understand big data topics that they are interested in and the difficulty of finding answers for questions in these topics. In this work, we conduct a large-scale study on Stackoverflow to understand the interest and difficulties of big data developers. To conduct the study, we develop a set of big data tags to extract big data posts from Stackoverflow; use topic modeling to group these posts into big data topics; group similar topics into categories to construct a topic hierarchy; analyze popularity and difficulty of topics and their correlations; and discuss implications of our findings for practice, research and education of big data software development and investigate their coincidence with the findings of previous work.}, location = {Tallinn, Estonia}, series = {ESEC/FSE 2019}, pages = {432\u2013442}, numpages = {11}, keywords = {Big data topic hierarchy, Big data topic popularity, Stackoverflow, Big data topics, Big data topic difficulty}}
@inproceedings{10.1145/2484028.2494492,title = {Riding the multimedia big data wave}, author = {Smith John R. },year = {2013}, isbn = {9781450320344}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2484028.2494492}, doi = {10.1145/2484028.2494492}, abstract = {In this talk we present a perspective across multiple industry problems, including safety and security, medical, Web, social and mobile media, and motivate the need for large-scale analysis and retrieval of multimedia data. We describe a multi-layer architecture that incorporates capabilities for audio-visual feature extraction, machine learning and semantic modeling and provides a powerful framework for learning and classifying contents of multimedia data. We discuss the role semantic ontologies for representing audio-visual concepts and relationships, which are essential for training semantic classifiers. We discuss the importance of using faceted classification schemes in particular for organizing multimedia semantic concepts in order to achieve effective learning and retrieval. We also show how training and scoring of multimedia semantics can be implemented on big data distributed computing platforms to address both massive-scale analysis and low-latency processing. We describe multiple efforts at IBM on image and video analysis and retrieval, including IBM Multimedia Analysis and Retrieval System (IMARS), and show recent results for semantic-based classification and retrieval. We conclude with future directions for improving analysis of multimedia through interactive and curriculum-based techniques for multimedia semantics-based learning and retrieval.}, location = {Dublin, Ireland}, series = {SIGIR '13}, pages = {1\u20132}, numpages = {2}, keywords = {machine learning, multimedia information retrieval, semantic modeling, video analysis, content-based search}}
@inproceedings{10.1145/3406601.3406628,title = {Health Data Analytics with an Opportunistic Big Data Algorithm}, author = {Chalumporn Gantaphon , Hewett Rattikorn },year = {2020}, isbn = {9781450377591}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3406601.3406628}, doi = {10.1145/3406601.3406628}, abstract = {In data-driven society, health data can lead to profound impacts on public safety policies, epidemic modeling, and advancement of health science and medicine. This paper presents an approach to automatically elucidating useful information from \"Big\" health data. In particular, we analyze manufactured cosmetic products containing chemicals that are known or suspected to cause cancer, birth defects, or developmental and reproductive harm. Our analysis is based on the Apriori algorithm, the heart of the popular Association Rule Mining to discover associations among sets of influencing factors. However, with rapid growth of huge amount of data, including ours, existing data analytics algorithms designed for in-memory data are not adequate. Most Big data analytics algorithms are implemented on MapReduce framework for execution in parallel and distributed environments. Unlike traditional implementation, our approach employs an opportunistic MapReduce-based Apriori algorithm to fully exploit parallelism. The paper describes the algorithm and presents our findings, from 113, 179 data instances, both in terms of the execution times and the discovered associations among product profiles. For a support threshold of 10% (5%,), 20 (53) association rules are obtained with an improved execution time over that of the traditional MapReduce-based algorithm by 14.6% (40.3%) on the average over three machines.}, location = {Bangkok, Thailand}, series = {IAIT2020}, pages = {1\u20139}, numpages = {9}, keywords = {Association rules mining, Big Data Algorithms, MapReduce}}
@inproceedings{10.1145/3481646,title = {Proceedings of the 2021 5th International Conference on Cloud and Big Data Computing},year = {2021}, isbn = {9781450390408}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Liverpool, United Kingdom}}
@inproceedings{10.1145/2601074,title = {Visualizations make big data meaningful}, author = {CACM Staff },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2601074}, doi = {10.1145/2601074}, abstract = {New techniques are designed to translate \"invisible numbers\" into visible images.}, pages = {19\u201321}, numpages = {3}}
@inproceedings{10.5555/2460396.2460415,title = {20 years of data quality research: themes, trends and synergies}, author = {Sadiq Shazia , Yeganeh Naiem Khodabandehloo , Indulska Marta },year = {2011}, isbn = {9781920682958}, publisher = {Australian Computer Society, Inc.}, address = {AUS}, abstract = {Data Quality is a cross-disciplinary and often domain specific problem due to the importance of fitness for use in the definition of data quality metrics. It has been the target of research and development for over 4 decades by business analysts, solution architects, database experts and statisticians to name a few. However, the changing landscape of data quality challenges indicate the need for holistic solutions. As a first step towards bridging any gaps between the various research communities, we undertook a comprehensive literature study of data quality research published in the last two decades. In this study we considered a broad range of Information System (IS) and Computer Science (CS) publication (conference and journal) outlets. The main aims of the study were to understand the current landscape of data quality research, to create better awareness of (lack of) synergies between various research communities, and, subsequently, to direct attention towards holistic solutions. In this paper, we present a summary of the findings from the study, that include a taxonomy of data quality problems, identification of the top themes, outlets and main trends in data quality research, as well as a detailed thematic analysis that outlines the overlaps and distinctions between the focus of IS and CS publications.}, location = {Perth, Australia}, series = {ADC '11}, pages = {153\u2013162}, numpages = {10}, keywords = {research framework, data quality, literature survey}}
@inproceedings{10.1145/2663715,title = {Proceedings of the First International Workshop on Privacy and Secuirty of Big Data},year = {2014}, isbn = {9781450315838}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The 1st International Workshop on Privacy and Security of Big Data (PSBD 2014) focuses the attention on privacy and security research issues in the context of Big Data, a vibrant and challenging research context which is playing a leading role in the Database research community. Indeed, while Big Data is gaining the attention from the research community, also driven by some relevant technological innovations (like Clouds) as well as novel paradigms (like social networks), the issues of privacy and security of Big Data represent a fundamental problem in this research context, due to the fact Big Data are typically published online for supporting knowledge management and fruition processes and, in addition to this, such data are usually handled by multiple owners, with possible secure multi-part computation issues. Some of the hot topics in the context privacy and security of Big Data include: (i) privacy and security of Big Data integration and exchange; (ii) privacy and security of Big Data in data-intensive Cloud computing; (iii) system architectures in support of privacy and security of Big Data, e.g., GPUs: (iv) privacy and security issues of Big Data querying and analysis. These topics are first-class aspects to be addressed and investigated by PSBD 2014.These proceedings contain the papers selected for presentation at the workshop. We received 12 submissions from countries in North America, Europe and Asia. After careful review, the program committee selected 5 papers for presentation at the workshop. The accepted papers were presented in 2 sessions: scalable privacy-preserving and security-control methods for Big Data processing, user-oriented and data-oriented privacy methods for Big Data processing. A panel discussed advanced aspects of privacy and security of Big Data. We hope that these proceedings will serve as a valuable reference for researchers and practitioners focusing on privacy and security of Big Data.}, location = {Shanghai, China}}
@inproceedings{10.1145/3234825.3234843,title = {Research of Wisdom and Lifelong Education Based on Big Data}, author = {Li Xin },year = {2018}, isbn = {9781450364409}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3234825.3234843}, doi = {10.1145/3234825.3234843}, abstract = {In recent years, big data has been infiltrating into all sectors of our society. Big data technology has brought revolutionary impact to every field, and is becoming the driving force and booster of disruptive innovation in all walks of life. Based on the application of big data in wisdom- lifelong education, this paper studies the impact of big data era on the development of lifelong education.First, the literature analysis software \"CiteSpace\" was used to explore the status of life education research at home and abroad. Through the study, it was found that the era of big data has brought a profound impact on the development of lifelong education. The main manifestation is that online education in the era of big data brings opportunities to reduce education costs and achieve educational equity. The conclusion is that we can make full use of the opportunity of big data education development.}, location = {London, United Kingdom}, series = {ICIEI '18}, pages = {35\u201339}, numpages = {5}, keywords = {wisdom education, cloud computing, life-long education, big data, education revolution, teaching students in accordance with their aptitude}}
@inproceedings{10.1145/3302424.3303988,title = {Runtime Object Lifetime Profiler for Latency Sensitive Big Data Applications}, author = {Bruno Rodrigo , Patricio Duarte , Sim\u00e3o Jos\u00e9 , Veiga Luis , Ferreira Paulo },year = {2019}, isbn = {9781450362818}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3302424.3303988}, doi = {10.1145/3302424.3303988}, abstract = {Latency sensitive services such as credit-card fraud detection and website targeted advertisement rely on Big Data platforms which run on top of memory managed runtimes, such as the Java Virtual Machine (JVM). These platforms, however, suffer from unpredictable and unacceptably high pause times due to inadequate memory management decisions (e.g., allocating objects with very different lifetimes next to each other, resulting in severe memory fragmentation). This leads to frequent and long application pause times, breaking Service Level Agreements (SLAs). This problem has been previously identified, and results show that current memory management techniques are ill-suited for applications that hold in memory massive amounts of long-lived objects (which is the case for a wide spectrum of Big Data applications).Previous works reduce such application pauses by allocating objects in off-heap, in special allocation regions/generations, or by using ultra-low latency Garbage Collectors (GC). However, all these solutions either require a combination of programmer effort and knowledge, source code access, offline profiling (with clear negative impacts on programmer's productivity), or impose a significant impact on application throughput and/or memory to reduce application pauses.We propose ROLP, a Runtime Object Lifetime Profiler that profiles application code at runtime and helps pretenuring GC algorithms allocating objects with similar lifetimes close to each other so that the overall fragmentation, GC effort, and application pauses are reduced. ROLP is implemented for the OpenJDK 8 and was evaluated with a recently proposed open-source pretenuring collector (NG2C). Results show long tail latencies reductions of up to 51% for Lucene, 85% for GraphChi, and 69% for Cassandra. This is achieved with negligible throughput (< 6%) and memory overhead, with no programmer effort, and no source code access.}, location = {Dresden, Germany}, series = {EuroSys '19}, pages = {1\u201316}, numpages = {16}, keywords = {Profiling, Tail Latency, Garbage Collection, Big Data, Pretenuring}}
@inproceedings{10.1145/2487575.2506178,title = {Big data analytics for healthcare}, author = {Sun Jimeng , Reddy Chandan K. },year = {2013}, isbn = {9781450321747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2487575.2506178}, doi = {10.1145/2487575.2506178}, abstract = {Large amounts of heterogeneous medical data have become available in various healthcare organizations (payers, providers, pharmaceuticals). Those data could be an enabling resource for deriving insights for improving care delivery and reducing waste. The enormity and complexity of these datasets present great challenges in analyses and subsequent applications to a practical clinical environment. In this tutorial, we introduce the characteristics and related mining challenges on dealing with big medical data. Many of those insights come from medical informatics community, which is highly related to data mining but focuses on biomedical specifics. We survey various related papers from data mining venues as well as medical informatics venues to share with the audiences key problems and trends in healthcare analytics research, with different applications ranging from clinical text mining, predictive modeling, survival analysis, patient similarity, genetic data analysis, and public health. The tutorial will include several case studies dealing with some of the important healthcare applications.}, location = {Chicago, Illinois, USA}, series = {KDD '13}, pages = {1525}, numpages = {1}}
@inproceedings{10.1145/3444757.3485107,title = {Dynamic and Scalable Enforcement of Access Control Policies for Big Data}, author = {Anisetti Marco , Ardagna Claudio A. , Braghin Chiara , Damiani Ernesto , Polimeno Antongiacomo , Balestrucci Alessandro },year = {2021}, isbn = {9781450383141}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3444757.3485107}, doi = {10.1145/3444757.3485107}, abstract = {The conflict between the need of protecting and sharing data is hampering the spread of big data applications. Security and privacy assurance is required to protect data owners, while data access and sharing are fundamental to implement smart big data solutions. In this context, access control systems can assume a central role in balancing data protection and data sharing. However, existing access control solutions are not general and scalable enough to address the software and technological complexity of big data ecosystems, being unable to support such a dynamic and collaborative environment. In this paper, we propose an access control system that enforces access to data in a distributed, multi-party big data environment. It is based on data annotations and secure data transformations performed at ingestion time. We show the feasibility of our approach in the smart city domain using an Apache-based big data engine.}, location = {Virtual Event, Tunisia}, series = {MEDES '21}, pages = {71\u201378}, numpages = {8}, keywords = {Data Ingestion, Big Data, Access Control, Data Transformation}}
@inproceedings{10.1145/3368756.3369096,title = {Big data analysis from the smart-logistics for smart-cities}, author = {Samir Tetouani , Abdelsamad Chouar , ElAlami Jamila },year = {2019}, isbn = {9781450362894}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3368756.3369096}, doi = {10.1145/3368756.3369096}, abstract = {Recent advances in information and communication technologies (ICT) have contributed to the evolution of the supply chain and logistics sector. Indeed, the analysis of massive data (Big Data) coming from smart-products makes it possible to extract enormous values for the decision-making of strategic choice: commercial or technical. But this also causes research problems because of the speed of data transmission, the huge volume, and the non-homogeneous types of data. This work provides an overview of the analysis of Big-Data (BD) from the Internet of Things (IoT) in new Logistics. This article begins with a discussion of the needs and challenges of the Internet of Things (IoT) and Big Data (BD) analysis in logistics. Then, major data analysis technologies are examined and discussed. In addition, this article also describes future directions in this promising area.}, location = {Casablanca, Morocco}, series = {SCA '19}, pages = {1\u20134}, numpages = {4}, keywords = {big data analytics, Logistics 4.0, internet of things}}
@inproceedings{10.1145/2818869.2818884,title = {Cost-Effective and Reliable Cloud Storage for Big Data}, author = {Wu Chun-Hsin , Hsu Pu },year = {2015}, isbn = {9781450337359}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2818869.2818884}, doi = {10.1145/2818869.2818884}, abstract = {Efficiency and reliability are critical to the performance of big data computing. Hadoop is one of the most popular cloud platforms to support big data analysis. To improve data availability, the Hadoop distributed file system usually stores blocks of files in triplicate, but it incurs high storage costs and high information-leakage risks. In this paper we propose a new cost-effective approach in xHDFS: paired blocks from two machines are erasure-coded to generate redundant blocks that are intelligently managed. We evaluate that xHDFS can effectively improve the reliability of cloud storage and tolerate network or site failures with lower storage costs.}, location = {Kaohsiung, Taiwan}, series = {ASE BD&amp;SI '15}, pages = {1\u20136}, numpages = {6}, keywords = {Cloud File Systems, Cloud Storage, Reliability, Remote Backup, Erasure Codes, Big Data Computing, Hadoop}}
@inproceedings{10.1145/2808797.2808841,title = {Big Data and the Regulation of Financial Markets}, author = {O'Halloran Sharyn , Maskey Sameer , McAllister Geraldine , Park David K. , Chen Kaiping },year = {2015}, isbn = {9781450338547}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2808797.2808841}, doi = {10.1145/2808797.2808841}, abstract = {The development of computational data science techniques in natural language processing (NLP) and machine learning (ML) algorithms to analyze large and complex textual information opens new avenues to study intricate processes, such as government regulation of financial markets, at a scale unimaginable even a few years ago. This paper develops scalable NLP and ML algorithms (classification, clustering and ranking methods) that automatically classify laws into various codes/labels, rank feature sets based on use case, and induce best structured representation of sentences for various types of computational analysis. The results provide standardized coding labels of policies to assist regulators to better understand how key policy features impact financial markets.}, location = {Paris, France}, series = {ASONAM '15}, pages = {1118\u20131124}, numpages = {7}, keywords = {financial regulation, natural language processing, big data, political economics, machine learning}}
@inproceedings{10.1145/2649387.2660821,title = {Big data challenges for estimating genome assembler quality}, author = {Biswas Abhishek , Gauthier David , Ranjan Desh , Zubair Mohammad },year = {2014}, isbn = {9781450328944}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2649387.2660821}, doi = {10.1145/2649387.2660821}, abstract = {The selection of an appropriate assembler is important to obtain best assembly of a fragment dataset, avoid misassembles and minimize further finishing effort. It is known that the assembly quality of assemblers is dependent on the input data parameters such as DNA fragmentation parameters and genome sequence structure. To the best of our knowledge no large scale systematic effort has been made in quantifying the quality of the assembly generated by various assemblers over a range of input parameters. The correlation between input parameters and assembler quality can be used to define the characteristics of an assembler and design an optimal assembler selection algorithm. The critical barrier is the computational challenge of assembling simulated high-throughput sequence libraries of thousands of genomes with input parameters varied to cover the spectrum of values obtained from major sequencers available to biologists today. We present a study to show that a quantifiable correlation can be drawn between their input and output characteristics for four major open-source assemblers. Based on our result we propose a simple model to estimate the quality of assemblies generated by these assemblers for given input parameters.}, location = {Newport Beach, California}, series = {BCB '14}, pages = {653\u2013660}, numpages = {8}, keywords = {genome fragmentation parameters, assembler characteristics, big data, assembly quality model}}
@inproceedings{10.1145/1077501.1077519,title = {Data quality inference}, author = {Pon Raymond K. , C\u00e1rdenas Alfonso F. },year = {2005}, isbn = {1595931600}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1077501.1077519}, doi = {10.1145/1077501.1077519}, abstract = {In the field of sensor networks, data integration and collaboration, and intelligence gathering efforts, information on the quality of data sources are important but are often not available. We describe a technique to rank data sources by observing and comparing their behavior (i.e., the data produced by data sources) to rank. Intuitively, our measure characterizes data sources that agree with accurate or high-quality data sources as likely accurate. Furthermore, our measure includes a temporal component that takes into account a data source's past accuracy in evaluating its current accuracy. Initial experimental results based on simulation data to support our hypothesis demonstrate high precision and recall on identifying the most accurate data sources.}, location = {Baltimore, Maryland}, series = {IQIS '05}, pages = {105\u2013111}, numpages = {7}}
@inproceedings{10.1145/3297662.3365797,title = {Integrating data quality requirements to citizen science application design}, author = {Musto Jiri , Dahanayake Ajantha },year = {2019}, isbn = {9781450362382}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297662.3365797}, doi = {10.1145/3297662.3365797}, abstract = {Data quality is an important aspect in many fields. In citizen science application databases, data quality is often found lacking, which is why there needs to be a method of integrating data quality into the design. This paper tackles the problem by dividing data quality into separate characteristics according to the ISO / IEC 25012 standard. These characteristics are integrated into a conceptual model of the system and data model for citizen science applications. Furthermore, the paper describes a way to measure data quality using the data quality characteristics. The models and measuring methods are theoretical and can be adapted into case specific designs.}, location = {Limassol, Cyprus}, series = {MEDES '19}, pages = {166\u2013173}, numpages = {8}, keywords = {Data quality, Citizen science, Data Quality Characteristics, Conceptual model, Data Quality requirements}}
@inproceedings{10.5555/1274453.1274459,title = {A data quality metamodel extension to CWM}, author = {Gomes Pedro , Farinha Jos\u00e9 , Trigueiros Maria Jos\u00e9 },year = {2007}, isbn = {192068285X}, publisher = {Australian Computer Society, Inc.}, address = {AUS}, abstract = {The importance of metadata has been broadly referred in the last years, mainly in the field of data warehousing and decision support systems. Contemporarily, in the adjacent field of data quality, several approaches and tools have been set out for the purpose of data profiling and cleaning. However, little effort has been made in order to formally specify metrics and techniques for data quality in a structured way. As a matter of fact, little relevance has been assigned to metadata regarding data quality and data cleaning issues. This paper aims at filling this gap, proposing a conceptual metamodel for data quality and cleaning, both applicable to operational and data warehousing contexts. The presented metadata model is integrated with OMG's CWM, offering a possible extension of this standard toward data quality.}, location = {Ballarat, Australia}, series = {APCCM '07}, pages = {17\u201326}, numpages = {10}, keywords = {data warehouses, data quality, metamodel, metadata, standards, CWM, data cleaning}}
@inproceedings{10.1145/2331042.2331050,title = {Mastering real-time big data with stream processing chains}, author = {Bonino Dario , De Russis Luigi },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2331042.2331050}, doi = {10.1145/2331042.2331050}, abstract = {To conciliate application logic concerns with event handling performance, we introduce the spChains processing framework.}, pages = {83\u201386}, numpages = {4}}
@inproceedings{10.1145/3220228.3220236,title = {A new data science framework for analysing and mining geospatial big data}, author = {Saraee Mo , Silva Charith },year = {2018}, isbn = {9781450364454}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3220228.3220236}, doi = {10.1145/3220228.3220236}, abstract = {Geospatial Big Data analytics are changing the way that businesses operate in many industries. Although a good number of research works have reported in the literature on geospatial data analytics and real-time data processing of large spatial data streams, only a few have addressed the full geospatial big data analytics project lifecycle and geospatial data science project lifecycle. Big data analysis differs from traditional data analysis primarily due to the volume, velocity and variety characteristics of the data being processed. One of a motivation of introducing new framework is to address these big data analysis challenges. Geospatial data science projects differ from most traditional data analysis projects because they could be complex and in need of advanced technologies in comparison to the traditional data analysis projects. For this reason, it is essential to have a process to govern the project and ensure that the project participants are competent enough to carry on the process. To this end, this paper presents, new geospatial big data mining and machine learning framework for geospatial data acquisition, data fusion, data storing, managing, processing, analysing, visualising and modelling and evaluation. Having a good process for data analysis and clear guidelines for comprehensive analysis is always a plus point for any data science project. It also helps to predict required time and resources early in the process to get a clear idea of the business problem to be solved.}, location = {Prague, Czech Republic}, series = {ICGDA '18}, pages = {98\u2013102}, numpages = {5}, keywords = {big data, data science, data mining, machine learning, geospatial big data}}
@inproceedings{10.1145/2505515.2514697,title = {From big data to big knowledge}, author = {Murphy Kevin },year = {2013}, isbn = {9781450322638}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2505515.2514697}, doi = {10.1145/2505515.2514697}, abstract = {We are drowning in big data, but a lot of it is hard to interpret. For example, Google indexes about 40B webpages, but these are just represented as bags of words, which don't mean much to a computer. To get from \"strings to things\", Google introduced the Knowledge Graph (KG), which is a database of facts about entities (people, places, movies, etc.) and their relations (nationality, geo-containment, actor roles, etc). KG is based on Freebase, but supplements it with various other structured data sources. Although KG is very large (about 500M nodes/ entities, and 30B edges/ relations), it is still very incomplete. For example, 94% of the people are missing their place of birth, and 78\\% have no known nationality - these are examples of missing links in the graph. In addition, we are missing many nodes (corresponding to new entities), as well as new types of nodes and edges (corresponding to extensions to the schema). In this talk, I will survey some of the efforts we are engaged in to try to \"grow\" KG automatically using machine learning methods. In particular, I will summarize our work on the problems of entity linkage, relation extraction, and link prediction, using data extracted from natural language text as well as tabular data found on the web.}, location = {San Francisco, California, USA}, series = {CIKM '13}, pages = {1917\u20131918}, numpages = {2}, keywords = {machine learning, knowledge bases, information extraction}}
@inproceedings{10.1145/3277139.3277155,title = {Promoting information resource management for e-government through big data approach}, author = {Sarker Md Nazirul Islam , Hossin Md Altab , Frimpong Adasa Nkrumah Kofi , Xiaohua Yin },year = {2018}, isbn = {9781450364867}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3277139.3277155}, doi = {10.1145/3277139.3277155}, abstract = {Big data has a potential to transform traditional government system to data-driven e-government system by utilizing modern analytical techniques. The aim of this article is to explore the applicability of big data for ensuring e-government. An extensive literature review has been administered using various levels of scales and indicators. Literature survey shows that a number of models have been developed to explain e-governance but systematic research on the suitability of big data for e-government is still lacking. This article argues that big data can help the information resource management system of the government for improving transparency and reducing corruption, fastest public service delivery, reducing public hassle, providing easy access to public services, reducing error and reducing poverty through e-services, e-management, e-democracy, and e-commerce. This article further argues that big data has a significant role in cost-effective service delivery to citizens, policy coherence, access to public services, participation and engagement, representation, access to information, open government and corruption control. The finding suggests that big data technologies should be implemented in every public-sector organization by minimizing technological challenges and threats, ensuring the privacy of citizen's information, maximizing utilization of data and promoting information management capacity.}, location = {Chengdu, China}, series = {IMMS '18}, pages = {99\u2013104}, numpages = {6}, keywords = {public agency, e-government, administration, governance, big data, IRM}}
@inproceedings{10.1145/3436286,title = {Proceedings of the 2020 2nd International Conference on Big Data and Artificial Intelligence},year = {2020}, isbn = {9781450376457}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Johannesburg, South Africa}}
@inproceedings{10.1145/3150919.3150925,title = {Improved Locally Linear Embedding for Big-data Classification}, author = {Ramirez Andres , Rahnemoonfar Maryam },year = {2017}, isbn = {9781450354943}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3150919.3150925}, doi = {10.1145/3150919.3150925}, abstract = {A hyperspectral image provides a multidimensional data consisting of hundreds of spectral dimensions. Even though having an abundance of spectral might seem favorable, classification of hyperspectral data tends to collide with the curse of dimensionality. Therefore, reducing the number of dimensions before classification is always favorable. For this research, the feature extraction method will consist of a nonlinear manifold learning technique named locally linear embedding (LLE). Additionally, another problem that we attempt to overcome is the high computational time required to run manifold learning methods. In order to help overcome this problem, this research compares one implementation of LLE against an improved version that runs much quicker than the original version.}, location = {Redondo Beach, CA, USA}, series = {BigSpatial'17}, pages = {37\u201341}, numpages = {5}, keywords = {Big-data, Hyperspectral, Locally Linear Embedding}}
@inproceedings{10.1145/2559206.2580093,title = {Big data for social good}, author = {Eagle Nathan },year = {2014}, isbn = {9781450324748}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2559206.2580093}, doi = {10.1145/2559206.2580093}, abstract = {Petabytes of data about human movements, transactions, and communication patterns are continuously being generated by everyday technologies such as mobile phones and credit cards. This unprecedented volume of information facilitates a novel set of research questions applicable to a wide range of development issues.In a collaboration involving 237 mobile operators across 102 countries, Jana's mobile technology platform can instantly poll and compensate 3.48 billion active mobile subscriptions. This talk will discuss how insights gained from living in Kenya became the genesis of a technology company currently working with global clients in over 50 countries, including P&G, Google, Unilever, Danone, General Mills, Nestle, Johnson & Johnson, Microsoft, the World Bank, and the United Nations. After providing an overview of the mobile and social media landscapes in emerging markets, it will conclude by emphasizing the value of consumer data in underserved and understudied regions of the world.}, location = {Toronto, Ontario, Canada}, series = {CHI EA '14}, pages = {11\u201312}, numpages = {2}, keywords = {keynote/invited talk}}
@inproceedings{10.1145/3481056.3481108,title = {Application and Development of Educational Big Data in University Course Learning}, author = {Yanzheng Wang , Xiaohong Wang },year = {2021}, isbn = {9781450390224}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3481056.3481108}, doi = {10.1145/3481056.3481108}, abstract = {Big data in education plays a pivotal role in university course learning at present, and it is also the trend of future education development.This paper introduces the application functions of big data in the teaching and learning of university subjects: the effective data analysis of big data in education, the resource-sharing function of big data in education, and the the evaluation of a variety of students' schoolwork.At the same time, this paper studies the development and application of intelligent platform based on big data, discusses the construction and application of automatic writing evaluation system based on \"big data\" and \"artificial intelligence\" technology.}, location = {Kyoto, Japan}, series = {ICEMT 2021}, pages = {1\u20134}, numpages = {4}, keywords = {education, application and development, curriculum learning, data analysis, big data}}
@inproceedings{10.1145/2694730,title = {Proceedings of the 1st Workshop on Performance Analysis of Big Data Systems},year = {2015}, isbn = {9781450333382}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {It is our great pleasure to welcome you to the 2015 ACM Workshop on Performance Analysis of Big Data Systems -- PABS'15 in conjunction with ICPE2015.The main objective of the workshop is to discuss the performance challenges imposed by big data systems and the different state-of-the-art solutions proposed to overcome these challenges. The workshop aims at providing a platform for scientific researchers, academicians and practitioners to discuss techniques, models, benchmarks, tools and experiences while dealing with performance issues in big data systems.The program committee reviewed 4 and accepted 2 full technical papers with acceptance rate as 50%.We welcome attendees to attend the keynote, invited talk and paper presentations. These valuable and insightful talks can and will guide us to a better understanding of the future: Accelerating Big Data Processing on Modern Clusters, Prof. D.K. Panda (Ohio State University, USA)Experimentation as a Tool for the Performance Evaluation of Big Data Systems, Prof. Amy W. Apon (Clemson University, USA)}, location = {Austin, Texas, USA}}
@inproceedings{10.1109/CCGRID.2018.00097,title = {Main-memory requirements of big data applications on commodity server platform}, author = {Makrani Hosein Mohammadi , Rafatirad Setareh , Houmansadr Amir , Homayoun Houman },year = {2018}, isbn = {9781538658154}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGRID.2018.00097}, doi = {10.1109/CCGRID.2018.00097}, abstract = {The emergence of big data frameworks requires computational and memory resources that can naturally scale to manage massive amounts of diverse data. It is currently unclear whether big data frameworks such as Hadoop, Spark, and MPI will require high bandwidth and large capacity memory to cope with this change. The primary purpose of this study is to answer this question through empirical analysis of different memory configurations available for commodity server and to assess the impact of these configurations on the performance Hadoop and Spark frameworks, and MPI based applications. Our results show that neither DRAM capacity, frequency, nor the number of channels play a critical role on the performance of all studied Hadoop as well as most studied Spark applications. However, our results reveal that iterative tasks (e.g. machine learning) in Spark and MPI are benefiting from a high bandwidth and large capacity memory.}, location = {Washington, District of Columbia}, series = {CCGrid '18}, pages = {653\u2013660}, numpages = {8}, keywords = {big data, memory, Spark, hadoop, performance}}
@inproceedings{10.1145/3194206.3194229,title = {Based on Hadoop's tech big data combination and mining technology framework}, author = {Zhichao Xu , Jiandong Zhao , Huan Huang },year = {2018}, isbn = {9781450363457}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3194206.3194229}, doi = {10.1145/3194206.3194229}, abstract = {With the advent of the Internet + era in the field of Tech big data, the big data of Tech big data has a large amount of data and various characteristics. It is an important means to carry out research on the big data of Tech big data to realize the combination and mining of efficient multi-source foreign technology data. However, at present, the big data of Tech big data are divided into disciplines and different formats, which are difficult to realize the intersection of effective scientific and technological information and realize data sharing. This paper puts forward a kind of big data combined with Tech big data and mining technology based on the Hadoop framework.It includes a unified collection and preprocessing method of big data of Tech big data and the design of storage and management platform for data sources. It is based on Map/Reduce Tech big data parallelization computing model and system.Its correlation with important scientific data mining services.The framework has good practicability and expansibility.}, location = {Shanghai, China}, series = {ICIAI '18}, pages = {59\u201363}, numpages = {5}, keywords = {Hadoop, combination, tech big data, mining}}
@inproceedings{10.1145/2616498.2616525,title = {Applying Lessons from e-Discovery to Process Big Data using HPC}, author = {Sondhi Sukrit , Arora Ritu },year = {2014}, isbn = {9781450328937}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2616498.2616525}, doi = {10.1145/2616498.2616525}, abstract = {The term 'Big Data' defines large datasets that are difficult to use and manage through conventional software tools. Legal Electronic Discovery (e-Discovery) is a business domain which has massive consumption of Big Data, where electronic records such as e-mail, documents, databases and social media postings are processed in order to discover evidence that may be pertinent to legal/compliance needs, litigation or other investigations. Numerous vendors exist in the market to provide organizations with services such as data collection, digital forensics and electronic discovery. High-end instrumentation and modern information technologies are creating data at an ever increasing rate. The challenges associated with managing the large datasets are related to the capture, storage, search, sharing, analytics, and visualization of the data. Big Data also offers unprecedented opportunities in other fields, ranging from astronomy and biology to marketing and e-commerce. This paper presents lessons learnt from the legal e-Discovery domain that can be adapted to process Big Data effectively on HPC resources, thereby benefitting the various disciplines of science, engineering and business that are grappling with a deluge of Big Data challenges and opportunities.}, location = {Atlanta, GA, USA}, series = {XSEDE '14}, pages = {1\u20132}, numpages = {2}, keywords = {parallel programming, e-Discovery, Big Data, predictive analytics}}
@inproceedings{10.5555/3018100.3018105,title = {Model driven advanced hybrid cloud services for big data: paradigm and practice}, author = {Yang Xi , Lehman Tom },year = {2016}, isbn = {9781509061587}, publisher = {IEEE Press}, abstract = {Advanced hybrid cloud services aim to serve big data applications by bridging multi-provider high performance cloud resources including direct connects, hypervisor bypassing VM interfaces, on premise clusters, parallel storage and high speed inter-cloud networks. We present a new \"full-stack model driven orchestration\" paradigm to integrate these diverse resources through semantic modeling and provide complex high-end services through dynamic orchestrated workflows. We also present architectural design of a real-world orchestration system, VersaStack, that implements the paradigm as well as a case study for providing full-scale advanced hybrid cloud services in practice.}, location = {Salt Lake City, Utah}, series = {DataCloud '16}, pages = {32\u201336}, numpages = {5}, keywords = {big data, advanced hybrid cloud, semantic modeling, service orchestration}}
@inproceedings{10.1145/2500873,title = {Big-data applications in the government sector}, author = {Kim Gang-Hoon , Trimi Silvana , Chung Ji-Hyong },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2500873}, doi = {10.1145/2500873}, abstract = {In the same way businesses use big data to pursue profits, governments use it to promote the public good.}, pages = {78\u201385}, numpages = {8}}
@inproceedings{10.1145/2970276.2970325,title = {Applying combinatorial test data generation to big data applications}, author = {Li Nan , Lei Yu , Khan Haider Riaz , Liu Jingshu , Guo Yun },year = {2016}, isbn = {9781450338455}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2970276.2970325}, doi = {10.1145/2970276.2970325}, abstract = {Big data applications (e.g., Extract, Transform, and Load (ETL) applications) are designed to handle great volumes of data. However, processing such great volumes of data is time-consuming. There is a need to construct small yet effective test data sets during agile development of big data applications. In this paper, we apply a combinatorial test data generation approach to two real-world ETL applications at Medidata. In our approach, we first create Input Domain Models (IDMs) automatically by analyzing the original data source and incorporating constraints manually derived from requirements. Next, the IDMs are used to create test data sets that achieve t-way coverage, which has shown to be very effective in detecting software faults. The generated test data sets also satisfy all the constraints identified in the first step. To avoid creating IDMs from scratch when there is a change to the original data source or constraints, our approach extends the original IDMs with additional information. The new IDMs, which we refer to as Adaptive IDMs (AIDMs), are updated by comparing the changes against the additional information, and are then used to generate new test data sets. We implement our approach in a tool, called comBinatorial bIg daTa Test dAta Generator (BIT-TAG). Our experience shows that combinatorial testing can be effectively applied to big data applications. In particular, the test data sets created using our approach for the two ETL applications are only a small fraction of the original data source, but we were able to detect all the faults found with the original data source.}, location = {Singapore, Singapore}, series = {ASE 2016}, pages = {637\u2013647}, numpages = {11}, keywords = {Big Data Testing, Test Data Generation, Adaptive Input Domain Model, Input Domain Model, Combinatorial Testing}}
@inproceedings{10.1145/3006299,title = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},year = {2016}, isbn = {9781450346177}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {Rapid advances in digital sensors, networks, storage, and computation along with their availability at low cost is leading to the creation of huge collections of data---dubbed \"Big Data.\" As a result, a Big Data computing paradigm has emerged, enabling new insights that can change the way business, science, and governments deliver services to their consumers, and can impact society as a whole. BDCAT provides an international forum for researchers and practitioners to present and discuss new discoveries, developments, and results, as well as the latest trends in big data computing, technologies, and applications.}, location = {Shanghai, China}}
@inproceedings{10.1145/3129292.3129296,title = {Towards Real-Time Road Traffic Analytics using Telco Big Data}, author = {Costa Constantinos , Chatzimilioudis Georgios , Zeinalipour-Yazti Demetrios , Mokbel Mohamed F. },year = {2017}, isbn = {9781450354257}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3129292.3129296}, doi = {10.1145/3129292.3129296}, abstract = {A telecommunication company (telco) is traditionally only perceived as the entity that provides telecommunication services, such as telephony and data communication access to users. However, the IP backbone infrastructure of such entities spanning densely urban spaces and widely rural areas, provides nowadays a unique opportunity to collect immense amounts of mobility data that can provide valuable insights for road traffic management and avoidance. In this paper we outline the components of the Traffic-TBD (Traffic Telco Big Data) architecture, which aims to become an innovative road traffic analytic and prediction system with the following desiderata: i) provide micro-level traffic modeling and prediction that goes beyond the current state provided by Internet-based navigation enterprises utilizing crowdsourcing; ii) retain the location privacy boundaries of users inside their mobile network operator, to avoid the risks of exposing location data to third-party mobile applications; and iii) be available with minimal costs and using existing infrastructure (i.e., cell towers and TBD data streams are readily available inside a telco). Road traffic understanding, management and analytics can minimize the number of road accidents, optimize fuel and energy consumption, avoid unexpected delays, contribute to a macroscopic spatio-temporal understanding of traffic in cities but also to \"smart\" societies through applications in city planning, public transportation, logistics and fleet management for enterprises, startups and governmental bodies.}, location = {Munich, Germany}, series = {BIRTE '17}, pages = {1\u20135}, numpages = {5}, keywords = {Telco, Road Traic, Big Data, Data Analytics}}