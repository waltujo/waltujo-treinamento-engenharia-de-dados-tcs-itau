@inproceedings{10.1145/3429630.3429642,title = {Research on the Management and Education System of Students in Clinical Practice in Medical Colleges in the Age of Big Data}, author = {Wang Shaozhuo },year = {2020}, isbn = {9781450388528}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3429630.3429642}, doi = {10.1145/3429630.3429642}, abstract = {Big data technology plays an important role in the management of college students. In this context, there are some problems existing in the clinical learning stage of medical college students, such as the imperfect hospital-college linkage education system, insufficient information management in affiliated hospitals, weak consciousness of big data, as well as single data collection and analysis, etc. This article analyzes the current situation of student management in the clinical internship stage of medical colleges in the age of big data, proposes to establish a sound school-university linkage system, optimize the information management system of affiliated hospitals, and use artificial neural networks to perform data information digging and analyzing the students\u2019 school conditions, providing decision-making basis for managers.}, location = {Busan, Republic of Korea}, series = {ICDTE 2020}, pages = {83\u201387}, numpages = {5}, keywords = {medical colleges, management, education, big data}}
@inproceedings{10.1145/3482632.3483127,title = {Construction of Educational Economy and Management Content System Based on Big Data}, author = {Meng Lingyan },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483127}, doi = {10.1145/3482632.3483127}, abstract = {The influence of BDT(big data technology) today is very huge and profound. Many work processes in all walks of life can not do without this technology, which has penetrated into all aspects of life and work. In this context, the field of education is gradually trying to use BDT technology to reform and innovate the system construction of teaching content. In order to explore the impact of applying BDT technology to educational economy and management on the construction of the content system of this major, this paper specially selects two universities a and B as the experimental research objects. University a applies BDT technology in the construction of educational economy and management, while university B still constructs the teaching content according to the conventional method Building. Then our experimental results show that the teaching performance and class efficiency of university a are much higher than that of University B. The highest teaching performance and class efficiency of university a are 97% and 97% respectively, and the highest teaching performance and class efficiency of University B are 82% and 85% respectively.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1259\u20131262}, numpages = {4}}
@inproceedings{10.1145/3029806.3029841,title = {Differentially-Private Big Data Analytics for High-Speed Research Network Traffic Measurement}, author = {Niculaescu Oana-Georgiana , Maruseac Mihai , Ghinita Gabriel },year = {2017}, isbn = {9781450345231}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3029806.3029841}, doi = {10.1145/3029806.3029841}, abstract = {High-speed research networks (e.g., Internet2, Geant) represent the backbone of large-scale research projects that bring together stakeholders from academia, industry and government. Such projects have increasing demands on throughput (e.g., 100Gbps line rates), and require a high amount of configurability. Collecting and sharing traffic data for such networks can help in detecting hotspots, troubleshooting, and designing novel routing protocols. However, sharing network data directly introduces serious privacy breaches, as an adversary may be able to derive private details about individual users (e.g., personal preferences or activity patterns). Our objective is to sanitize high-speed research network data according to the de-facto standard of differential privacy (DP), thus supporting benefic applications of traffic measurement without compromising individuals' privacy. In this paper, we present an initial framework for computing DP-compliant big data analytics for high-speed research network data. Specifically, we focus on sharing data at flow-level granularity, and we describe our initial steps towards an environment that relies on Hadoop and HBase to support privacy-preserving NetFlow analytics.}, location = {Scottsdale, Arizona, USA}, series = {CODASPY '17}, pages = {151\u2013153}, numpages = {3}, keywords = {network measurement, differential privacy}}
@inproceedings{10.1145/3482632.3484013,title = {Research on Big Data Encryption Algorithm Based on Vector Evaluation Genetic Algorithm}, author = {Yun Dawei , Yu Jingxian , Jia Zhengao , Zheng Xiuyan , Wang Jiabin , Qi Yintao },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3484013}, doi = {10.1145/3482632.3484013}, abstract = {In recent years, with the rapid development of cloud computing, Internet of Things and social networks, the amount of data carried by the network has increased dramatically. Traditional encrypted storage technology and management methods have been difficult to meet the requirements of big data in terms of speed, capacity, storage efficiency and security. Under the data environment, users' data security and privacy protection are facing great challenges. A vector evaluation genetic algorithm is proposed. The algorithm uses population diversity information to adjust inertia weight nonlinearly, and introduces velocity mutation operator and position crossover operator in the later stage of the algorithm, so that the algorithm can get rid of the constraint that it is easy to fall into local optimum in the later stage, so as to solve the multi-objective reactive power optimization problem and solve the Pareto optimal solution set of the problem. Simulation results show that the proposed method can improve the privacy protection performance of users' information, and the encryption and hiding overhead of the algorithm is small, and the real-time performance of information coding and hiding is high.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1663\u20131667}, numpages = {5}}
@inproceedings{10.1145/3229607,title = {Proceedings of the 2018 Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},year = {2018}, isbn = {9781450359047}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Budapest, Hungary}}
@inproceedings{10.1145/2695664.2696078,title = {A fast support vector data description system for anomaly detection using big data}, author = {Rekha A. G. },year = {2015}, isbn = {9781450331968}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2695664.2696078}, doi = {10.1145/2695664.2696078}, abstract = {Anomaly (or outlier) detection is one of the most studied data mining techniques due to its importance and inherent challenges. Recently, Support Vector Data Description (SVDD) driven approaches are shown as having good predictive accuracy. 'Big Data' as input enhances the inferential power of algorithms, but it challenges even state-of-the-art computation methods. The runtime complexity of SVDD is linear in the number of support vectors. Hence speeding up the decision function becomes important for applications requiring fast real time performance. This work aims to provide solutions that reduce the computational complexity of SVDD so as to make one-class classification practical on Big Data. The proposed work first reduces the complexity of SVDD by computing a point known as the agent of the SVDD sphere center in the input space during the training phase. It retains the benefit of the kernel trick also. We then propose to hadoopize this lightly trained SVDD named as LT-SVDD so that a faster classification can be achieved in a big-data setting.}, location = {Salamanca, Spain}, series = {SAC '15}, pages = {931\u2013932}, numpages = {2}}
@inproceedings{10.1145/3297662.3365822,title = {Applications of Generalized Difference Method for Hypothesis Generation to Social Big Data in Concept and Real Spaces}, author = {Ishikawa Hiroshi , Kato Daiju , Endo Masaki , Hirota Masaharu },year = {2019}, isbn = {9781450362382}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297662.3365822}, doi = {10.1145/3297662.3365822}, abstract = {Analytic methodology as to generation of integrated hypotheses is necessary for applications involving different sources of social big data. In this paper, first, we introduce an abstract data model for integrating data management and data mining by using mathematical concepts of families, collections of sets to facilitate reproducibility and accountability required for social big data applications. Next, we describe generalized difference methods as a methodology for generating integrated hypotheses. Finally, we validate our proposal by applying them to three use cases involving data in concept and real spaces by using our data model as their description guided by generalized difference methods.}, location = {Limassol, Cyprus}, series = {MEDES '19}, pages = {44\u201355}, numpages = {12}, keywords = {data mining, difference method, integrated analysis, hypothesis generation, data model, data management, Social big data}}
@inproceedings{10.1145/3210604.3210644,title = {Potential for participatory big data ethics and algorithm design: a scoping mapping review}, author = {Whitman Madisson , Hsiang Chien-yi , Roark Kendall },year = {2018}, isbn = {9781450355742}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3210604.3210644}, doi = {10.1145/3210604.3210644}, abstract = {Ubiquitous networked data collection and algorithm-based information systems have the potential to disparately impact lives around the planet and pose a host of emerging ethical challenges. One response has been a call for more transparency and democratic control over the design and implementation of such systems. This scoping mapping review focuses on participatory approaches to the design, governance, and future of these systems across a wide variety of contexts and domains.1}, location = {Hasselt and Genk, Belgium}, series = {PDC '18}, pages = {1\u20136}, numpages = {6}, keywords = {speculative design, algorithm design, research through design}}
@inproceedings{10.1145/3419635.3419652,title = {Research on the Innovation of Ideological and Political Education in Universities in the Era of Big Data}, author = {Wu Xiaogang },year = {2020}, isbn = {9781450387729}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3419635.3419652}, doi = {10.1145/3419635.3419652}, abstract = {Along with the development of the era, information technology and computer technology, cloud computing technology are in rapid progress. Today, the modern teaching reform is deepening, the original paper of teaching resources has been unable to meet the needs of modern education. Especially in recent years, increasing amount and types of teaching resources, for teachers and students, teaching resources have been more and more important. In the future, the digital teaching resource management mode based on cloud computing technology, Web network technology and information technology will be the main direction of progress. Therefore, the project of constructing digital teaching resource management system is of great significance. This paper introduces the research status of big data and ideological and political education in colleges and universities at home and abroad, USES big data to promote the ideological and political education in colleges and universities, advances the application of big data to college education, and finally summarizes the results and prospects.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2020}, pages = {336\u2013340}, numpages = {5}, keywords = {Innovative exploration, Ideological and political education in colleges and universities, Big data era, Opportunities and challenges}}
@inproceedings{10.5555/2382887.2382903,title = {Building a big data research program at a small university}, author = {Carter Thomas , Hauselt Peggy , Martin Melanie , Thomas Megan },year = {2012}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {In the 2010-2011 school year we received an Army High Performance Computing Research Center grant whose goal was increasing the number of Hispanic engineers with expertise in complex systems, simulations and large data sets. Our university is a medium-sized, public, Hispanic-serving institution; our department is small. Our goals were to improve the social support available to our Hispanic students, encourage them to complete their degrees, and give them a valid research experience to provide a basis for informed decisions about whether or not they want to go to graduate school. This paper will cover how we structured our program to accomplish our goals, including how we factored in results from prior research on minority student experiences. In the second year, we expanded our program to include geography and give a new cohort of students a multi-disciplinary experience. We will discuss how a small computer science department successfully built an undergraduate research program organized around the theme of large data sets, what we have accomplished so far and how we hope to continue.}, pages = {95\u2013102}, numpages = {8}}
@inproceedings{10.1145/3423603.3424059,title = {Depth insight for data scientist with RapidMiner \u00ab an innovative tool for AI and big data towards medical applications\u00bb}, author = {Bjaoui Mohamed , Sakly Houneida , Said Mourad , Kraiem Naoufel , Bouhlel Mohamed Salim },year = {2020}, isbn = {9781405377539}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3423603.3424059}, doi = {10.1145/3423603.3424059}, abstract = {RapidMiner tool is considered among the advanced analytics and powerful platform services in the field of artificial intelligence besides the Big Data storage. This solution has emerged towards several industries such as Financial Services, Energy, Logistics, Life Science and Healthcare and has shown a crucial impact for predictive decisions in this area. This work seeks to describe the solution strategy of this tool for data scientist by depicting a depth insight of this concept which contains more 1500 native algorithms, data preparation and data science functions. This features allows professionals to support any machine learning libraries and integrate python and R codes. RapidMiner offers three different modalities to access to their products which are the main Platform, the automated data science and the AI cloud.}, location = {Virtual Event, Tunisia}, series = {DTUC '20}, pages = {1\u20136}, numpages = {6}, keywords = {RapidMiner tool, data science, advanced analytics, security policies, artificial intelligence, big data}}
@inproceedings{10.1145/2818950.2818983,title = {Energy Efficient Scale-In Clusters with In-Storage Processing for Big-Data Analytics}, author = {Choi I. Stephen , Kee Yang-Suk },year = {2015}, isbn = {9781450336048}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2818950.2818983}, doi = {10.1145/2818950.2818983}, abstract = {Big data drives a computing paradigm shift. Due to enormous data volumes, data-intensive programming frameworks are pervasive and scale-out clusters are widespread. As a result, data-movement energy dominates overall energy consumption and this will get worse with a technology scaling. We propose scale-in clusters with In-Storage Processing (ISP) devices that would enable energy efficient computing for big-data analytics. ISP devices eliminate/reduce data movements towards CPUs and execute tasks more energy-efficiently. Thus, with energy efficient computing near data and higher throughput enabled, clusters with ISP can achieve more than quadruple energy efficiency with fewer number of nodes as compared to the energy efficiency of similarly performing its counter-part scale-out clusters.}, location = {Washington DC, DC, USA}, series = {MEMSYS '15}, pages = {265\u2013273}, numpages = {9}}
@inproceedings{10.5555/3320516.3320569,title = {Recent trends in stochastic gradient descent for machine learning and big data}, author = {Newton David , Pasupathy Raghu , Yousefian Farzad },year = {2018}, isbn = {978153866570}, publisher = {IEEE Press}, abstract = {Stochastic Gradient Descent (SGD), also known as stochastic approximation, refers to certain simple iterative structures used for solving stochastic optimization and root finding problems. The identifying feature of SGD is that, much like in gradient descent for deterministic optimization, each successive iterate in the recursion is determined by adding an appropriately scaled gradient estimate to the prior iterate. Owing to several factors, SGD has become the leading method to solve optimization problems arising within large-scale machine learning and \"big data\" contexts such as classification and regression. This tutorial covers the basics of SGD with an emphasis on modern developments. The tutorial starts with examples where SGD is applicable, and then details important flavors of SGD and reported complexity calculations.}, location = {Gothenburg, Sweden}, series = {WSC '18}, pages = {366\u2013380}, numpages = {15}}
@inproceedings{10.1145/3548785.3548806,title = {The adaptation of the OODA loop to the decision-making systems processing Big Data in the area of morality}, author = {W\u0119grzyn Damian Tadeusz },year = {2022}, isbn = {9781450397094}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3548785.3548806}, doi = {10.1145/3548785.3548806}, abstract = {The rapid development of autonomous systems and their presence in human life force them to make quick decisions based on Big Data. Many of these decisions involve moral judgments that are then transformed into specific actions. Side effects of the choices made by the decision systems can be dangerous, so we have to be very careful when increasing the capacity of these systems. The decisions the autonomous systems make should be as ethical as possible. This paper adapts the observe-orient-decide-act (OODA) loop to the decision-making process in the moral area. It combines the parameterization of cognitive aspects of autonomous systems with ethical standards and moral inference. Problems related to the implementation of moral inference to autonomous systems, including artificial intelligence (AI) systems, are presented. Thanks to the adaptation of the OODA loop, it is possible to make morally correct decisions and actions based on a set of ethical principles adjusted to a specific situation. The presented proposal allows for moral inference, which extends the possibilities of autonomous systems that use the inference loop, especially those processing Big Data. The decision-making system still has the possibility of a choice aimed at doing more good or less evil.}, location = {Budapest, Hungary}, series = {IDEAS '22}, pages = {144\u2013149}, numpages = {6}, keywords = {Autonomous systems, Decision-making process, OODA loop, Big Data}}
@inproceedings{10.1145/3482632.3482698,title = {Construction of University Network Ideology Management Platform Based on Big Data Education Platform}, author = {Zhou Guanwen },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3482698}, doi = {10.1145/3482632.3482698}, abstract = {Big data, like an information storm, has swept through all aspects of university development and played an important role in improving the effectiveness of university network ideology education. Facing the brand-new development situation, in order to better complete the ideology education in colleges and universities and establish a safety barrier between students and bad network information, colleges and universities should actively establish an internet ideology education system, construct a perfect network ideology management structure, guide students to establish a correct outlook on life and values, and improve practical work efficiency. Under the background of big data, the Internet ideology education in colleges and universities has faced many difficulties and opportunities. Educators must correctly understand the positive significance of ideology education, effectively combine the characteristics of network development, and better carry out ideology education. This paper analyzes the factors affecting the effectiveness of university network ideology education, summarizes the main problems existing in the process of education and teaching, and puts forward the construction method of university network ideology management platform based on big data.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {315\u2013318}, numpages = {4}}
@inproceedings{10.1145/2533888.2533939,title = {Semantic extraction of geographic data from web tables for big data integration}, author = {Cruz Isabel F. , Ganesh Venkat R. , Mirrezaei Seyed Iman },year = {2013}, isbn = {9781450322416}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2533888.2533939}, doi = {10.1145/2533888.2533939}, abstract = {There are millions of web tables with geographic data that are pertinent for big data integration in a variety of domain applications, such as urban sustainability, transportation networks, policy studies, and public health. These tables, however, are heterogeneous in structure, concepts, and metadata. One of the challenges in semantically extracting geographic data is the need to resolve these heterogeneities so as to uncover a conceptual hierarchy, metadata associated with instances, and geographic information---corresponding respectively to ontologies, elements that we call features, and cell values that can be used to identify geographic coordinates. In this paper, we present an architecture with methods to: (1) extract feature-rich web tables; (2) identify features; (3) construct a schema and instances using RDF; (4) perform geocoding. Preliminary experiments led to high accuracy in table identification and feature naming even when compared to manual evaluation.}, location = {Orlando, Florida}, series = {GIR '13}, pages = {19\u201326}, numpages = {8}, keywords = {information extraction, GIS, web tables, geocoding, semantic data integration, spatial databases, geographic data}}
@inproceedings{10.1145/3420038,title = {Deep Hash-based Relevance-aware Data Quality Assessment for Image Dark Data}, author = {Liu Yu , Wang Yangtao , Gao Lianli , Guo Chan , Xie Yanzhao , Xiao Zhili },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3420038}, doi = {10.1145/3420038}, abstract = {Data mining can hardly solve but always faces a problem that there is little meaningful information within the dataset serving a given requirement. Faced with multiple unknown datasets, to allocate data mining resources to acquire more desired data, it is necessary to establish a data quality assessment framework based on the relevance between the dataset and requirements. This framework can help the user to judge the potential benefits in advance, so as to optimize the resource allocation to those candidates. However, the unstructured data (e.g., image data) often presents dark data states, which makes it tricky for the user to understand the relevance based on content of the dataset in real time. Even if all data have label descriptions, how to measure the relevance between data efficiently under semantic propagation remains an urgent problem. Based on this, we propose a Deep Hash-based Relevance-aware Data Quality Assessment framework, which contains off-line learning and relevance mining parts as well as an on-line assessing part. In the off-line part, we first design a Graph Convolution Network (GCN)-AutoEncoder hash (GAH) algorithm to recognize the data (i.e., lighten the dark data), then construct a graph with restricted Hamming distance, and finally design a Cluster PageRank (CPR) algorithm to calculate the importance score for each node (image) so as to obtain the relevance representation based on semantic propagation. In the on-line part, we first retrieve the importance score by hash codes and then quickly get the assessment conclusion in the importance list. On the one hand, the introduction of GCN and co-occurrence probability in the GAH promotes the perception ability for dark data. On the other hand, the design of CPR utilizes hash collision to reduce the scale of graph and iteration matrix, which greatly decreases the consumption of space and computing resources. We conduct extensive experiments on both single-label and multi-label datasets to assess the relevance between data and requirements as well as test the resources allocation. Experimental results show our framework can gain the most desired data with the same mining resources. Besides, the test results on Tencent1M dataset demonstrate the framework can complete the assessment with a stability for given different requirements.}, pages = {1\u201326}, numpages = {26}, keywords = {relevance, data quality assessment, Resource allocation, CPR, GAH, data mining}}
@inproceedings{10.1145/2484838.2484884,title = {Making sense of big data with the Berkeley data analytics stack}, author = {Franklin Michael J. },year = {2013}, isbn = {9781450319218}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2484838.2484884}, doi = {10.1145/2484838.2484884}, abstract = {The Berkeley AMPLab was founded on the idea that the challenges of emerging Big Data applications require a new approach to analytics systems. Launching in early 2011, the project set out to rethink the traditional analytics stack, breaking down technical and intellectual barriers that had arisen during decades of evolutionary development. The vision of the lab is to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (such as machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and en masse, as with crowd-sourced human computation). To pursue this goal, we assembled a research team with diverse interests across computer science, forged relationships with domain experts on campus and elsewhere, and obtained the support of leading industry partners and major government sponsors. The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the nearly three years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark in-memory computation framework, and the Shark query processing system. In this talk I'll describe the current state of BDAS with an emphasis on the key components that have been released to date. I'll then discuss ongoing efforts on machine learning scalability and ease of use, including the MLbase system, as our focus moves higher up the stack. Finally I will present our longer-term views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.}, location = {Baltimore, Maryland, USA}, series = {SSDBM}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/3468945.3468964,title = {An Empirical Study on the Classification, Grading, Sharing and Opening of Healthcare Big Data Based on Current Policies and Standards}, author = {Hou Hanfang , Fu Qiang , Zhang Yang },year = {2021}, isbn = {9781450390057}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3468945.3468964}, doi = {10.1145/3468945.3468964}, abstract = {This paper expounds the connotations and features of healthcare big data, as well as the concepts and logical relations for its classification, grading, sharing and opening. It also summarizes overseas policies related thereto and their status quo, and emphatically introduces main policies, laws, regulations and national standards regarding the classification, grading, sharing and opening of healthcare big data in China, as well as practices in three places represented by Shandong Province, Guangdong Province and Fuzhou. Finally, principles and practical suggestions for the classification, grading, sharing and opening of healthcare big data were proposed based on current polices, regulations and standards}, location = {Tianjin, China}, series = {IMIP '21}, pages = {116\u2013121}, numpages = {6}, keywords = {Opening, Classification, Healthcare big data, Sharing, Grading}}
@inproceedings{10.1145/3482632.3483007,title = {English Teaching Ability Assessment Algorithm Based on Big Data Fuzzy k-Means Clustering}, author = {Zhang Hongyu },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483007}, doi = {10.1145/3482632.3483007}, abstract = {At present, our country is stepping up efforts to promote the development of big data technology. In this context, English teaching is also facing major reforms. When discussing the inaccuracy of data estimation of traditional English teaching ability, a targeted algorithm for estimation of English teaching ability based on big data fuzzy K-means clustering is proposed. Integrate and classify English teaching data, formulate appropriate courses for the allocation of teaching resources, and evaluate English teaching ability; after using the comparative analysis method, the following conclusions are reached: the period of formation of English teaching ability is after employment, especially the use of teaching methods, educational wit, and teaching and research abilities are all higher than 80%. In addition, the two abilities of teaching organization and management and teaching content processing are also more than 75% after the job.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {740\u2013744}, numpages = {5}}
@inproceedings{10.1145/3318464.3384705,title = {Big Data Series Analytics Using TARDIS and its Exploitation in Geospatial Applications}, author = {Zhang Liang , Alghamdi Noura , Eltabakh Mohamed Y. , Rundensteiner Elke A. },year = {2020}, isbn = {9781450367356}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3318464.3384705}, doi = {10.1145/3318464.3384705}, abstract = {The massive amounts of data series data continuously generated and collected by applications require new indices to speed up data series similarity queries on which various data mining techniques rely. However, the state-of-the-art iSAX-based indexing techniques do not scale well due to the binary fanout that leads to a highly deep index tree and suffer from accuracy degradation due to the character-level cardinality that leads to poor maintenance of the proximity. To address this problem, we recently proposed TARDIS to supports indexing and querying billion-scale data series datasets. It introduces a new iSAX-T signatures to reduce the cardinality conversion cost and corresponding sigTree to construct a compact index structure to preserve better similarity. The framework consists of one centralized index and local distributed indices to efficiently re-partition and index dimensional datasets. Besides, effective query strategies based on sigTree structure are proposed to greatly improve the accuracy. In this demonstration, we present GENET, a new interactive exploration demonstration that allows users to support Big Data Series Approximate Retrieval and Recursive Interactive Clustering in large-scale geospatial datasets using TARDIS index techniques.}, location = {Portland, OR, USA}, series = {SIGMOD '20}, pages = {2785\u20132788}, numpages = {4}, keywords = {KNN approximate query, TARDIS, sigtree, GENET, iSAX-T, geospatial, data series, distributed indexing, approximate query processing, clustering, word-level cardinality}}
@inproceedings{10.1145/3469213.3470315,title = {Research on Industry and Commerce Management System Based on Computer Big Data algorithm}, author = {Zhang Yubing },year = {2021}, isbn = {9781450390200}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3469213.3470315}, doi = {10.1145/3469213.3470315}, abstract = {The rapid development and popularization of computer technology and big data has promoted the progress of various industries and has advantageously promoted the development of e-government. The industry and commerce management system is an important participant, manager, and service provider of national economic construction and development and plays a pivotal role in the construction of informatization. This article studies the critical technologies needed to realize the industry and commerce management system: ASP.NET technology, object-oriented technology, and system development tools ASP.NET and back-end database management system SQL Server 2010. Design the system architecture and functional modules Structure, according to the basic theory of database, the process and results of big data modeling are given. The detailed design of essential business modules such as business registration, daily management, and case handling are discussed.}, location = {Chongqing, China}, series = {ICAIIS 2021}, pages = {1\u20136}, numpages = {6}}
@inproceedings{10.1145/3411174.3411195,title = {A Framework of Utilizing Big Data of Social Media to Find Out the Habits of Users Using Keyword}, author = {Lubis Arif Ridho , Nasution Mahyuddin K. M. , Sitompul Opim Salim , Zamzami Elviawaty Muisa },year = {2020}, isbn = {9781450387668}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411174.3411195}, doi = {10.1145/3411174.3411195}, abstract = {Optimal search in search engines is very urgent, especially search performs on social media search engine. Very big data in social media have not been not used as many as expected, making the existed data are only limited to the data themselves. However, using keywords on social media search engines could only produces incomplete and inaccurate data thereby the search results only have limited usage. This paper contains a framework using keywords in social media search engines that aims to gain users' habits by utilizing social media data. The framework offered is in the form of steps to optimize the search engines, so that the optimal social media search engine could be used as an entry point to find the desired data on specific social media user. The step is initialized by analyzing existing posts to get more specific and accurate data from social media users. The social media system then could use those specific keywords to identify the identity of a specific user in the information space that can be accessed by the search engines.}, location = {Singapore, Singapore}, series = {ICCCM '20}, pages = {140\u2013144}, numpages = {5}, keywords = {social media keywords, Big data, user habit, social media search engine}}
@inproceedings{10.1145/3291801.3291829,title = {Lifelong Machine Learning: Outlook and Direction}, author = {Hong Xianbin , Wong Prudence , Liu Dawei , Guan Sheng-Uei , Man Ka Lok , Huang Xin },year = {2018}, isbn = {9781450364768}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3291801.3291829}, doi = {10.1145/3291801.3291829}, abstract = {The Lifelong machine learning is an advanced machine learning paradigm and also is the key to the stronger AI. In this paper, we review the development history of the lifelong machine learning and evaluate the current stage. The aim, definition and main components of it is introduced. In addition, the bottleneck and possible solution also is discussed and the further development waypoint is proposed.}, location = {Weihai, China}, series = {ICBDR 2018}, pages = {76\u201379}, numpages = {4}, keywords = {Classification, Lifelong Machine Learning, Big Data, Natural Language Processing}}
@inproceedings{10.1145/3293663.3293680,title = {Evaluation and Analysis of Capacity Scheduler and Fair Scheduler in Hadoop Framework on Big Data Technology}, author = {Salman Muhammad , Husna Diyanatul , Wicaksono Adhitya , Ratna Anak Agung Putri },year = {2018}, isbn = {9781450366410}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3293663.3293680}, doi = {10.1145/3293663.3293680}, abstract = {Apache Hadoop is an open source framework that implements MapReduce. It is scalable, reliable, and fault tolerant. Scheduling is an important process in Hadoop MapReduce. It is because scheduling has responsibility to allocate resources for running applications based on resource capacity, queues, running tasks, and the number of users. Changing single node to multi node Hadoop cluster can optimize HDFS, but quite costly. Scheduler performs the function of scheduling based on resource requirements, such as memory, CPU, disk, and network. The most general purpose of scheduling algorithm is minimizing the time of completing a task. Hadoop Scheduling is an independent module where users are able to design their own scheduler based on the application's actual need. So it can fulfill the specific need of the business in accordance with the desired result. This research will analyze the characteristic of Capacity Scheduler and Fair Scheduler.}, location = {Nagoya, Japan}, series = {AIVR 2018}, pages = {1\u20135}, numpages = {5}, keywords = {Capacity Scheduler, Big Data, YARN, Hadoop, Fair Scheduler}}
@inproceedings{10.1145/3485730.3493447,title = {Exploring Co-dependency of IoT Data Quality and Model Robustness in Precision Cattle Farming}, author = {Papst Franz , Schodl Katharina , Saukh Olga },year = {2021}, isbn = {9781450390972}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3485730.3493447}, doi = {10.1145/3485730.3493447}, abstract = {Low-cost sensors are extensively used in numerous Internet of Things (IoT) applications to measure relevant physical processes. Today, processing context data is increasingly done by proprietary algorithms tuned to a specific use-case, e.g., a sensor measuring activity intensity of a cow. Readings from these sensors may be subject to data distribution shifts, which challenge robustness of models using these sensor readings. In this paper, we propose a new sensor data processing framework, which leverages a co-dependency between data quality and model robustness to detect performance issues of data-driven predictive models in the field. We show how distribution shifts in the input data impact the quality of the model, which relies on application-specific sensors, and present indicators capable of detecting such shifts in the wild. The proposed framework used in the context of precision cattle farming allows improving the quality of cow lameness predictive models on the field data by up to 62%.}, location = {Coimbra, Portugal}, series = {SenSys '21}, pages = {433\u2013438}, numpages = {6}, keywords = {data quality, predictive modeling, outlier detection, robustness}}
@inproceedings{10.1145/3507485.3507496,title = {Research on the Implementation Path of Big Data Technology to Promote the Integration of Production and Marketing of Agricultural Products}, author = {Du Rongliang , Cai Shuo },year = {2021}, isbn = {9781450385831}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3507485.3507496}, doi = {10.1145/3507485.3507496}, abstract = {The disconnection between the production and sales of agricultural products leads to the imbalance between supply and demand, which hinders the development of the industry. Therefore, it is urgent to integrate the production and sales of agricultural products. From the perspective of big data technology, this study discusses the implementation path of using big data technology to realize the integration of production and marketing of agricultural products. It mainly includes improving the basic database, building an integrated production, marketing model of agricultural products based on big data technology, building Omni-channel marketing combination scheme of agricultural products, which will improve the integration of production and marketing of agricultural products in China.}, location = {Osaka, Japan}, series = {ICSEB 2021}, pages = {61\u201366}, numpages = {6}, keywords = {Rural E-commerce, Agricultural product, Integration of production and marketing, Big data}}
@inproceedings{10.1145/3139243.3139254,title = {Impact of Crowdsourced Data Quality on Travel Pattern Estimation}, author = {Rodrigues Jo\u00e3o G. P. , Pereira Jo\u00e3o P. , Aguiar Ana },year = {2017}, isbn = {9781450355551}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3139243.3139254}, doi = {10.1145/3139243.3139254}, abstract = {Mobile crowdsensing can provide mobility researchers with fine grained spatio-temporal location data. But crowdsourcing impacts data quality both due to device and OS heterogeneity, and to annotation errors. Additionally, it is often necessary to deal with multimodality, i.e. participants using different travel modes often in the same trip. In this paper, we address how to draw value from a crowdsensed dataset for characterising mobility demand through origin-destination (OD) matrices, highlighting challenges and providing some solutions.First, we identify typical errors in heterogeneous location data, propose and compare methods to automatically improve data quality. Then, we devise a method to detect among 5 transport modes (walk, car, bus, metro, bike) offline a posteriori. We segment trips on stopped periods and propose a random forest model to detect transportation mode per segment using only location data. Our results show that with adequate pre-processing and robust features, an RF classifier is able to achieve accuracy and precision of 85% in trip segments. This is similar to the literature, but our work uses a very heterogeneous crowdsourced trajectory dataset when compared to the others. Finally, we quantify the impact of the model on mulit-modal OD matrices and whole trip characterisation. We can correctly identify used transportation modes accurately, but the precision is impaired by the high likelihood of at least one false positive in the whole trip.}, location = {Delft, Netherlands}, series = {CrowdSenSys '17}, pages = {38\u201343}, numpages = {6}, keywords = {Transportation Mode Estimation, Crowdsensing, Data Mining}}
@inproceedings{10.1145/2501221,title = {Proceedings of the 2nd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},year = {2013}, isbn = {9781450323246}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The aim of this workshop is to bring together people from both academia and industry to present their most recent work related to big-data issues, and exchange ideas and thoughts in order to advance this big-data challenge, which has been considered as one of the most exciting opportunities in the past 10 years.Recent years have witnessed a dramatic increase in our ability to collect data from various sensors, devices, in different formats, from independent or connected applications. This data flood has outpaced our capability to process, analyze, store and understand these datasets. Consider the Internet data. The web pages indexed by Google were around one million in 1998, but quickly reached 1 billion in 2000 and have already exceeded 1 trillion in 2008. This rapid expansion is accelerated by the dramatic increase in acceptance of social networking applications, such as Facebook, Twitter, Weibo, etc., that allow users to create contents freely and amplify the already huge Web volume. Furthermore, with mobile phones becoming the sensory gateway to get real-time data on people from different aspects, the vast amount of data that mobile carrier can potentially process to improve our daily life has significantly outpaced our past CDR (call data record)- based processing for billing purposes only. It can be foreseen that Internet of things (IoT) applications will raise the scale of data to an unprecedented level. People and devices (from home coffee machines to cars, to buses, railway stations and airports) are all loosely connected. Trillions of such connected components will generate a huge data ocean, and valuable information must be discovered from the data to help improve quality of life and make our world a better place. For example, after we get up every morning, in order to optimize our commute time to work and complete the optimization before we arrive at office, the system needs to process information from traffic, weather, construction, police activities to our calendar schedules, and perform deep optimization under the tight time constraints. In all these applications, we are facing significant challenges in leveraging the vast amount of data, including challenges in (1) system capabilities (2) algorithmic design (3) business models.}, location = {Chicago, Illinois}}
@inproceedings{10.1145/3338906.3338953,title = {White-box testing of big data analytics with complex user-defined functions}, author = {Gulzar Muhammad Ali , Mardani Shaghayegh , Musuvathi Madanlal , Kim Miryung },year = {2019}, isbn = {9781450355728}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3338906.3338953}, doi = {10.1145/3338906.3338953}, abstract = {Data-intensive scalable computing (DISC) systems such as Google\u2019s MapReduce, Apache Hadoop, and Apache Spark are being leveraged to process massive quantities of data in the cloud. Modern DISC applications pose new challenges in exhaustive, automatic testing because they consist of dataflow operators, and complex user-defined functions (UDF) are prevalent unlike SQL queries. We design a new white-box testing approach, called BigTest to reason about the internal semantics of UDFs in tandem with the equivalence classes created by each dataflow and relational operator. Our evaluation shows that, despite ultra-large scale input data size, real world DISC applications are often significantly skewed and inadequate in terms of test coverage, leaving 34% of Joint Dataflow and UDF (JDU) paths untested. BigTest shows the potential to minimize data size for local testing by 10^5 to 10^8 orders of magnitude while revealing 2X more manually-injected faults than the previous approach. Our experiment shows that only few of the data records (order of tens) are actually required to achieve the same JDU coverage as the entire production data. The reduction in test data also provides CPU time saving of 194X on average, demonstrating that interactive and fast local testing is feasible for big data analytics, obviating the need to test applications on huge production data.}, location = {Tallinn, Estonia}, series = {ESEC/FSE 2019}, pages = {290\u2013301}, numpages = {12}, keywords = {test generation, map reduce, symbolic execution, data intensive scalable computing, dataflow programs}}
@inproceedings{10.1145/3246871,title = {Session details: Special Session in Big Data, Metadata and Knowledge}, author = {Papatheodorou Christos , Gergatsoulis Manolis },year = {2014}, isbn = {9781450328975}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3246871}, doi = {10.1145/3246871}, location = {Athens, Greece}, series = {PCI '14}, pages = {}}
@inproceedings{10.1145/3437120.3437352,title = {Towards the design of a Conceptual Framework for the operation of Intensive Care Units based on Big Data Analysis}, author = {Markopoulos Dimitris , Tsolakidis Anastasios , N. Karanikolas Nikitas , Skourlas Christos },year = {2020}, isbn = {9781450388979}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3437120.3437352}, doi = {10.1145/3437120.3437352}, abstract = {The development of Big Data Analytics (BDA) technology and the maturity of the Machine Learning (ML) sector offer great opportunities for applications in Intensive Care Units (ICUs). This paper describes a Conceptual Framework and proposes its use in designing architectures and big data applications in ICUs. The Conceptual Framework is based on BDA,MLNatural Language Processing (NLP) and consists of the following subsystems: The \"Big Data Integration and ICUs\" module, the \"ICUs and critical care services\" module, the \"Use of standards and ICUs\" module, the \"Machine Learning and ICUs\" module, and the \u201cNLP and ICUs\u201d module. The framework is developed using Soft System Methodology (SSM) and Design Science Research Methodology (DSRM).}, location = {Athens, Greece}, series = {PCI 2020}, pages = {411\u2013415}, numpages = {5}, keywords = {Machine Learning, Conceptual Framework, Intensive Care Unit, Big Data Analysis}}
@inproceedings{10.1145/2884781.2896829,title = {Predicting and fixing vulnerabilities before they occur: a big data approach}, author = {Chen Hong-Mei , Kazman Rick , Monarch Ira , Wang Ping },year = {2016}, isbn = {9781450341523}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2884781.2896829}, doi = {10.1145/2884781.2896829}, abstract = {The number and variety of cyber-attacks is rapidly increasing, and the rate of new software vulnerabilities is also rising dramatically. The cybersecurity community typically reacts to attacks after they occur. Being reactive is costly and can be fatal, where attacks threaten lives, important data, or mission success. Taking a proactive approach, we are: (I) identifying potential attacks before they come to fruition, and based on this identification; (II) developing preventive counter-measures. We describe a Proactive Cybersecurity System (PCS), a layered, modular service platform that applies big data collection and processing tools a wide variety of unstructured data sources to identify potential attacks and develop countermeasures. The PCS provides security analysts a holistic, proactive, and systematic approach to cybersecurity. Here we describe our research vision and progress towards that vision.}, location = {Austin, Texas}, series = {BIGDSE '16}, pages = {72\u201375}, numpages = {4}, keywords = {concept clustering, architecture analysis, software security}}
@inproceedings{10.1145/2896825.2896829,title = {Predicting and fixing vulnerabilities before they occur: a big data approach}, author = {Chen Hong-Mei , Kazman Rick , Monarch Ira , Wang Ping },year = {2016}, isbn = {9781450341523}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2896825.2896829}, doi = {10.1145/2896825.2896829}, abstract = {The number and variety of cyber-attacks is rapidly increasing, and the rate of new software vulnerabilities is also rising dramatically. The cybersecurity community typically reacts to attacks after they occur. Being reactive is costly and can be fatal, where attacks threaten lives, important data, or mission success. Taking a proactive approach, we are: (I) identifying potential attacks before they come to fruition, and based on this identification; (II) developing preventive counter-measures. We describe a Proactive Cybersecurity System (PCS), a layered, modular service platform that applies big data collection and processing tools a wide variety of unstructured data sources to identify potential attacks and develop countermeasures. The PCS provides security analysts a holistic, proactive, and systematic approach to cybersecurity. Here we describe our research vision and progress towards that vision.}, location = {Austin, Texas}, series = {BIGDSE '16}, pages = {72\u201375}, numpages = {4}, keywords = {software security, architecture analysis, concept clustering}}
@inproceedings{10.5555/3512489.3512520,title = {Advantages of Python programming language in the world of big data: poster poster abstract}, author = {Arora Meharban },year = {2021}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {This paper introduces the readers to industry and society's adoption of technology, where the involvement of computer programs, coded with Python, have become a necessity. This paper elaborates on the increased usage of data and the rise of the \"Big Data\" industry. Upon that, there are details about Data science and Machine learning as a byproduct of the Big Data industry. Furthermore, the paper shifts the focus from \"Big picture\" data towards Python programming language. It specifies why this coding language is important and how it simplifies the data analysis process through the use of open-source Python libraries. The advantages of Python compared to the other similar languages such as C, and MATLAB are also included. Lastly, this project will also review several Python packages that can be downloaded from open-source online resources. There are explanations of the ways the Python packages can assist a Python programmer, novice or expert, analyze data functions in numerous different professional fields. Specifically, this paper will go over the Matplotlib package and one specific dataset example that can be used with it. Matplotlib example is included to show what functions a programmer can access when using Python based libraries. It showcases the ease of use of Python when visualizing data in a dataset.}, pages = {170}, numpages = {1}}
@inproceedings{10.1145/3512576.3512661,title = {Research on City Area Traffic Generation Analysis Based on Multi-source Big Data}, author = {Luo Jing , Li Qingqing , Li Baopeng , Wang Shuai },year = {2021}, isbn = {9781450384971}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3512576.3512661}, doi = {10.1145/3512576.3512661}, abstract = {City area traffic demand analysis is an important foundation for comprehensive traffic planning. In the context of the rapid development of big data technology, how to organically combine multi-source data, improve model accuracy, improve work efficiency and reduce work cost is a key issue in the construction of traffic generation analysis model. This paper integrates relevant data such as resident travel OD data, population migration data, and poi data. On the basis of the traditional traffic generation model, add the geographical advantage correction factor to establish a city area traffic generation analysis model based on multi-source big data. And take Heyuan City, Guangdong Province as an example for case analysis to demonstrate the validity and practicability of the model. The results show that the average absolute errors of traffic production and attraction in all traffic communities are 16.29% and 14.98% respectively. This paper can provide a new method to improve the accuracy of city area traffic generation analysis modeling.CCS CONCEPTS \u2022 Applied computing \u2022 Operations research \u2022 Transportation}, location = {Guangzhou, China}, series = {ICIT 2021}, pages = {493\u2013498}, numpages = {6}, keywords = {City area, Multi-source data, Geographical advantage, Traffic Generation Analysis}}
@inproceedings{10.1145/3318299.3318349,title = {Abnormal Phone Analysis Based on Learning to Rank and Ensemble Learning in Environment of Telecom Big Data}, author = {Liu Jian , Ji Ke , Sun Runyuan , Ma Kun , Chen Zhenxiang , Wang Lin },year = {2019}, isbn = {9781450366007}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3318299.3318349}, doi = {10.1145/3318299.3318349}, abstract = {With the rapid development of Telecom era, the number of telecom users has increased dramatically. User phone have been widely recognized as user identities and are registered in a large number of Internet applications. Due to the leakage of user information, a series of social problems have arisen. Abnormal telephone has become a social problem to be solved. Current methods are mostly passive detection methods, and some of them are extremely expensive and do not meet the requirements of practical application. Our current situation of lack of effective control measures and active detection methods for abnormal phones. Based on the existing telecommunication big data, an abnormal phone active detection method is designed based on learning to rank and ensemble learning algorithm. The experimental results on the real dataset show that the proposed method has higher accuracy than the experimental results obtained by a single learning algorithm.}, location = {Zhuhai, China}, series = {ICMLC '19}, pages = {301\u2013305}, numpages = {5}, keywords = {learning to rank, ensemble learning, abnormal phone, Telecom big data}}
@inproceedings{10.1145/3018009.3018040,title = {Integration and exchange method of multi-source heterogeneous big data for intelligent power distribution and utilization}, author = {Xu Gang , Wu Shunyu , Xie Pengfei },year = {2016}, isbn = {9781450348195}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3018009.3018040}, doi = {10.1145/3018009.3018040}, abstract = {With the development of smart grid and big data technologies, the stability and economy of distribution network operation are enhanced effectively. Intelligent power distribution and utilization (IPDU) big data platform, which exchanges operation data with other related distribution network management systems, makes decisions for demand side management, power system and distributed energy operation strategies by analyzing the big data. In order to solve the data fusion and exchange problems among all information systems, we proposed a kind of general information model for multi-source heterogeneous big data. In addition, a data fusion and exchange mechanism is established based on circle buffer to ensure the data quality. Finally, this paper demonstrates the effective of the method of IPDU big data fusion method by the example of distribution network reconfiguration. The method proposed in this paper can satisfy the data exchanging demands of future smart grid and demand side management, and it also has good confluent and extensible feature.}, location = {Singapore, Singapore}, series = {ICCIP '16}, pages = {38\u201342}, numpages = {5}, keywords = {data fusion and exchange, information model, multi-source and heterogeneous, intelligent power distribution and utilization}}
@inproceedings{10.1145/1651415,title = {Proceedings of the first international workshop on Model driven service engineering and data quality and security},year = {2009}, isbn = {9781605588162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {These proceedings include the papers accepted for the First International Workshop on Model Driven Service Engineering and Data Quality and Security (MoSE+DQS 2009), which was held in Hong Kong, on November 6th 2009.This workshop included two different tracks focusing on Model Driven Service Engineering (MoSE track) and Data Quality and Security (DQS track).Regarding the first issue we can see that Model-Driven Engineering (MDE) deals with the provision of models, transformations between them and code generators to address software development. One of the main advantages of model-driven approaches is the provision of a conceptual structure where the models used by business managers and analysts can be traced towards more detailed models used by software developers. This kind of alignment between high level business specifications and the lower level Service Oriented Architectures (SOA) is a crucial aspect in the field of Service-Oriented Development (SOD) where meaningful business services and business process specifications are those that can give support to real business environment usually changing with increasing speed. SOD has become currently in one of the major research topics in the field of software engineering, leading the appearance of a novel and emerging discipline called Service Engineering (SE), which aim to bring together benefits of SOA and Business Process Management (BPM). SE focuses on the identification of service (a client-provider interaction that creates value for the client) as first class elements for the software construction. The convergence of SE with MDE holds out the promise of rapid and accurate development of software that serves software users' goals.On the other hand, Information technologies are becoming one of the most important aspects for organizations. The business value of the data stored in the company databases has been growing to become one of the most important assets of the company. These data represent one crucial asset for tactic, strategic and operational decisions. Due to this important role of the data, companies should assure the access to the data to several users guaranteeing the right levels of quality they need to accomplish the task they have to do.Data Quality is a crucial issue in assessing the quality of business decisions support systems. Many aspects are related with the quality of the data, such as integrity, completeness, actuality and several other factors that make this kind of quality a multidimensional issue and a difficult issue. Data Security is another crucial aspect on information systems, not only because it affects Data Quality, but also because current information systems store sensitive and private data that should be treated rightly. Also, as Data Quality and Data Security are not independent concepts, the relationship between both concepts is worth being analyzed in order to give organizations some tools that can help in assuring both data dimensions.The Workshop on Model Driven Service Engineering and Data Quality and Security intends to provide a forum for researchers and practitioners working on different issues related to SE in conjunction with MDE, boarding open research problems in this area as well as practical experiences. The workshop is also focused on auditing, measuring, predicting, evaluating, controlling, assuring and improving the quality and security of data. Particular interests include methods, modelling languages, development methodologies and techniques in these fields.The six full papers (an acceptance rate of 54.5%) and four short papers were selected very carefully by the Program Committee in order to ensure a high quality workshop.}, location = {Hong Kong, China}}
@inproceedings{10.1145/2627534.2627558,title = {Modeling and analytics for cyber-physical systems in the age of big data}, author = {Sharma Abhishek B. , Ivan\u010di\u0107 Franjo , Niculescu-Mizil Alexandru , Chen Haifeng , Jiang Guofei },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2627534.2627558}, doi = {10.1145/2627534.2627558}, abstract = {In this position paper we argue that the availability of \"big\" monitoring data on Cyber-Physical Systems (CPS) is challenging the traditional CPS modeling approaches by violating their fundamental assumptions. However, big data alsobrings unique opportunities in its wake by enabling new modeling and analytics approaches as well as facilitating novel applications. We highlight a few key challenges andopportunities, and outline research directions for addressing them. To provide a proper context, we also summarize CPS modeling approaches, and discuss how modeling and analytics for CPS differs from general purpose IT systems.}, pages = {74\u201377}, numpages = {4}}
@inproceedings{10.1145/3482632.3482728,title = {Discussion on New Marketing Mode Based on Collaborative Recommendation Algorithm in Big Data Era}, author = {Chen Huimin },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3482728}, doi = {10.1145/3482632.3482728}, abstract = {In recent years, information technology has penetrated into all fields of production and consumption, and big data is bringing more economic benefits to enterprises. Marketing is the front-end operation of enterprises, which is used to collect accumulated sales behavior data, while sales are directly affected by big data. Big data analysis technology can be used to promote the development of the whole marketing operation, and many enterprises can hire experienced marketing experts without high salary. Enterprises can directly use big data analysis to achieve accurate market positioning and increase sales to target groups. It can not only save the marketing cost of enterprises, but also bring more benefits to enterprises and help enterprises achieve the goal of maximizing profits. In order to help enterprises develop faster and occupy more market resources in the fierce market competition, this paper analyzes the new marketing model based on collaborative recommendation algorithm in the context of big data era, and helps managers make scientific and reasonable decisions quickly and accurately.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {454\u2013457}, numpages = {4}}
@inproceedings{10.14778/3551793.3551855,title = {Fine-grained modeling and optimization for intelligent resource management in big data processing}, author = {Lyu Chenghao , Fan Qi , Song Fei , Sinha Arnab , Diao Yanlei , Chen Wei , Ma Li , Feng Yihui , Li Yaliang , Zeng Kai , Zhou Jingren },year = {2022}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3551793.3551855}, doi = {10.14778/3551793.3551855}, abstract = {Big data processing at the production scale presents a highly complex environment for resource optimization (RO), a problem crucial for meeting performance goals and budgetary constraints of analytical users. The RO problem is challenging because it involves a set of decisions (the partition count, placement of parallel instances on machines, and resource allocation to each instance), requires multi-objective optimization (MOO), and is compounded by the scale and complexity of big data systems while having to meet stringent time constraints for scheduling. This paper presents a MaxCompute based integrated system to support multi-objective resource optimization via fine-grained instance-level modeling and optimization. We propose a new architecture that breaks RO into a series of simpler problems, new fine-grained predictive models, and novel optimization methods that exploit these models to make effective instance-level RO decisions well under a second. Evaluation using production workloads shows that our new RO system could reduce 37--72% latency and 43--78% cost at the same time, compared to the current optimizer and scheduler, while running in 0.02-0.23s.}, pages = {3098\u20133111}, numpages = {14}}
@inproceedings{10.1145/2623330.2630806,title = {Management and analytic of biomedical big data with cloud-based in-memory database and dynamic querying: a hands-on experience with real-world data}, author = {Feng Mengling , Ghassemi Mohammad , Brennan Thomas , Ellenberger John , Hussain Ishrar , Mark Roger },year = {2014}, isbn = {9781450329569}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2623330.2630806}, doi = {10.1145/2623330.2630806}, abstract = {Analyzing Biomedical Big Data (BBD) is computationally expensive due to high dimensionality and large data volume. Performance and scalability issues of traditional database management systems (DBMS) often limit the usage of more sophisticated and complex data queries and analytic models. Moreover, in the conventional setting, data management and analysis use separate software platforms. Exporting and importing large amounts of data across platforms require a significant amount of computational and I/O resources, as well as potentially putting sensitive data at a security risk. In this tutorial, the participants will learn the difference between in-memory DBMS and traditional DBMS through hands-on exercises using SAP's cloud-based HANA in-memory DBMS in conjunction with the Multi-parameter Intelligent Monitoring in Intensive Care (MIMIC) dataset. MIMIC is an open-access critical care EHR archive (over 4TB in size) and consists of structured, unstructured and waveform data. Furthermore, this tutorial will seek to educate the participants on how a combination of dynamic querying, and in-memory DBMS may enhance the management and analysis of complex clinical data.}, location = {New York, New York, USA}, series = {KDD '14}, pages = {1970}, numpages = {1}, keywords = {biomedical data, dynamic querying, in-memory dbms, big data}}
@inproceedings{10.1145/3456887.3456894,title = {Asset Management Mode of Higher Vocational Colleges Based on Big Data Technology}, author = {Liao Xiaoling },year = {2021}, isbn = {9781450389969}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456887.3456894}, doi = {10.1145/3456887.3456894}, abstract = {The integration research and development of asset information in higher vocational colleges (HVC) is the core component of the education informatization project, and an important form and method of promoting modern education with educational informatization in china. It fully reflects the development direction and measures of the current social education system. The purpose of this study is to study the financial management (FM) mode and strengthen the current FM work in colleges and universities (CAU), so that CAU can get considerable development in FM, and build an efficient FM mode to ensure the financial stability of HVC, and to ensure adequate education funds. Based on the reform of china's higher education system, through the use of big data technology for reference, this paper analyzes the shortcomings of the asset management mode of a higher vocational college in Jiangxi province, and finds corresponding solutions. The results show that the expenditure of teachers and teaching assistants accounts for 26.3% of the total education cost. However, books and grants accounted for 22.4% and 4.8% respectively. Housing construction accounts for the largest proportion of fixed assets in HVC, with an increase rate of 33.8% from 53.4 million yuan in 2016 to 71.45 million yuan in 2019.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2021}, pages = {29\u201333}, numpages = {5}}
@inproceedings{10.1145/3482632.3482722,title = {Financial Supervision Innovation Based on Decision Tree Classification Algorithm in Big Data Era}, author = {Li Yunhong , Zhang Lingling },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3482722}, doi = {10.1145/3482632.3482722}, abstract = {Nowadays, with the development of network communication technology and digital technology, it is widely used at all levels of society. The convenience and rapidity of the Internet make finance continuously integrate into people's daily life. Internet finance, as one of the innovative forms of financial information, has developed rapidly in just a few years under the support of information technology. While seeing that Internet finance has brought great support to economic development, we can't ignore its various problems. The potential risks accumulated in the early stage of the Internet finance industry are constantly erupting, exposing various problems in financial supervision, which not only damages the rights and interests of the long-tail group, but also increases the social cost, which is not conducive to the long-term development of the Internet finance industry. This paper discusses the inheritance and innovation relationship between big data analysis and traditional financial statistics. Based on the decision tree classification algorithm, aiming at strengthening financial supervision and improving macro decision-making level, this paper puts forward the idea of making overall use of big data analysis and statistical analysis.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {423\u2013427}, numpages = {5}}
@inproceedings{10.1145/3537693.3537719,title = {Establishment and Optimization of Enterprise Financial Shared Center in the Era of Big Data\u2013taking Haier as an Example}, author = {Guo Yong },year = {2022}, isbn = {9781450396523}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3537693.3537719}, doi = {10.1145/3537693.3537719}, abstract = {With the rapid development of Internet technology, the technological innovation based on \"Internet+\" has been applied in many industries and fields, which has played a vital role in promoting economic development. The development of big data has intensified economic globalization, the business scale of the group company continuously expands to all regions and the trend of extending from China to abroad is more obvious, increasing the difficulty in financial control of each branch (subsidiary) by the group. To solve the problem, the group enterprises are required to accelerate the promotion of financial transformation and upgrading, focus on enhancing financial value creation and business support capabilities and effectively control financial and operational risks. The research and practice show that the effect achieved by implementing financial sharing service is relatively remarkable. This paper takes Haier enterprise as a case for study and analysis, studies the problems in the application of enterprise Financial Shared Service Center under the big data environment, deeply analyzes the cause of the problem, and puts forward detailed solutions and suggestions for specific problems. With Haier's Financial Shared Service Center as the study object, this paper studies the advantage and application of financial sharing, the specific operation of each business process, the deficiencies of each financial management module and the process modules of the Financial Shared Service Center to be improved and optimized under the big data technology environment, and explores the theoretical methods and practical application value of enterprises optimizing the Financial Shared Center under the background of big data by deep analysis of Haier case.}, location = {Plymouth, United Kingdom}, series = {ICEEG '22}, pages = {170\u2013175}, numpages = {6}, keywords = {Financial Shared Center, Big Data,Financial Management Mode}}
@inproceedings{10.1145/3378936.3378981,title = {Big Data Analyses of ZeroNet Sites for Exploring the New Generation DarkWeb}, author = {Ding Jianwei , Guo Xiaoyu , Chen Zhouguo },year = {2020}, isbn = {9781450376907}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3378936.3378981}, doi = {10.1145/3378936.3378981}, abstract = {ZeroNet is a new generation typical dark web, which uses the Bitcoin encryption algorithm and BitTorrent technology to build a distributed and censored-resistant communication network. Based on our cumulative studies on the onion router, we present a big data analyses framework for automated multi-categorization of ZeroNet websites to facilitate analyst situational awareness of new content that emerges from this dynamic landscape. Over the last two years, our team has developed a distributed crawling infrastructure called ZeroCrawler that automatically crawls and updates ZeroNet websites in realtime. It stores data into a research repository designed to help better understand ZeroNet's hidden service ecosystem. The analysis component of our framework is called Automated Multi-Categorization Labeling (AMCL), which introduces a three-stage thematic labeling strategy: (1) it learns descriptive and discriminative keywords for different categories, and (2) get a probability distribution of the keywords for different categories, and then (3) uses these terms to map ZeroNet website content to several labels. We also present empirical results of AMCL and our ongoing experimentation with it, as we have gained experience applying it to the entirety of our ZeroNet repository, now over 3000 indexed websites. The experimental results show that AMCL can discover categories on previously unlabeled websites, and we discuss applications of AMCL in supporting various analyses and investigations of the ZeroNet websites.}, location = {Sydney, NSW, Australia}, series = {ICSIM '20}, pages = {46\u201352}, numpages = {7}, keywords = {Multi-categarization labels, ZeroNet, Dark web conent analysis}}
@inproceedings{10.1145/3482632.3483025,title = {Online Mental Health Education Model in Colleges and Universities of the Big Data Era}, author = {Liu Xiaomin },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483025}, doi = {10.1145/3482632.3483025}, abstract = {With the rapid development of the era of big data, applying big data to online mental health education (MHE) in colleges and universities (CUs) can effectively ameliorate the advanced and scientific nature. Various problems will arise during the development of online MHE in CUs. The application of big data has brought unprecedented opportunities and challenges to online MHE in CUs. Psychological education in CUs must seize opportunities, meet challenges, innovate online MHE models in CUs, and effectively use big data to facilitate the scientific development of online MHE in CUs.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {811\u2013814}, numpages = {4}}
@inproceedings{10.1145/3544109.3544404,title = {Research on Web Big Data Migration Algorithm based on Association Rule Mining Model}, author = {Luo Jinman , Li Xiaoxia , Feng Youjun , Wang Lina },year = {2022}, isbn = {9781450395786}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544109.3544404}, doi = {10.1145/3544109.3544404}, abstract = {With the popularization of the Internet and the rapid development of information technology, big data on the Web has spread across all fields of life and has become an important part of people's lives in the cloud computing era. However, the scale of Web big data is so huge that users cannot directly obtain the information they need from it. Data migration is a key part of data system integration to ensure smooth system upgrades and updates, and it also plays an important role in cloud computing. The quality of data migration is not only an important prerequisite for the new system to be put into use, but also a strong guarantee for stable operation in the future. When cloud computing lays out data sets, because it mainly considers the load in the data, it will generally give priority to distributing the data sets in the data with lower load. Such a layout method makes it possible to perform tasks of these two types of applications. When the number of network visits increases, problems such as increasing the number of network visits will be prevented. For these two special applications, this paper designs an efficient and dynamic data migration algorithm based on the association rule mining model to optimize the migration performance.}, location = {Dalian, China}, series = {IPEC '22}, pages = {1030\u20131034}, numpages = {5}}
@inproceedings{10.1145/3482632.3483160,title = {Management of Physical Education Classroom Teaching in the Context of Big Data}, author = {Liang Fangmei },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483160}, doi = {10.1145/3482632.3483160}, abstract = {In the era of big data (BD), education management is a key area facing reform in China, and the value of physical education management data is more important in the era of BD. The current problems faced in the development of physical education classroom management have emerged as new solution ideas in the context of BD, and the application of BD thinking to solve classroom management problems is a proven means. The purpose of this paper is to study the management of physical education classroom teaching in the context of BD. From the perspective of education policy and education management, this paper explores the problems that arise in China's physical education classroom and the corresponding solutions based on three aspects: cognition, mechanism and management mode in the context of BD. This paper adopts various analytical methods such as SWOT analysis, field interview method and questionnaire survey method to explore the problems in the development of physical education classroom teaching management, and proposes the use of BD technology to promote the construction of physical education informatization, so as to improve the teaching mode and learning mode of physical education classroom and enhance the quality of physical education teaching. The experimental results show that only 35% of physical education teachers feel that school sports facilities and necessary sports equipment can be fully satisfied in teaching in the era of BD, but more than 70% of students like physical education classes, so school administrators and physical education teachers need to improve the importance of physical education classes.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1400\u20131404}, numpages = {5}}