@inproceedings{10.1145/3297730.3297731,title = {Target Data Optimization based on Big Data-streaming for Two-stage Fuzzy Extraction System}, author = {Chen Rui-Yang },year = {2018}, isbn = {9781450365826}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297730.3297731}, doi = {10.1145/3297730.3297731}, abstract = {How to extract target data effectively and intelligently is key point in the big data-streaming operation. Data extractions need focus on priority of data selection to reduce impact of 4Vs because of requirement of real-time computation. Corresponding big data-streaming for three-stage extraction system is presented in terms of hierarchal base and fuzzy representations. The proposed approach is based on a combination of clustering, classification and relationships method with fuzzy weighted similarity under hierarchical feature-based model. Moreover, heuristic fuzzy CBR-FDT- algorithms are provided to explore the target data optimization. Successful case study and experiment with simulations demonstrated the performance of the proposed approach.}, location = {Chengdu, China}, series = {BDET 2018}, pages = {26\u201330}, numpages = {5}, keywords = {big data-streaming, Target data optimization, fuzzy decision tree, fuzzy case-based reasoning}}
@inproceedings{10.1109/MET.2019.00019,title = {Addressing data quality problems with metamorphic data relations}, author = {Auer Florian , Felderer Michael },year = {2019}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/MET.2019.00019}, doi = {10.1109/MET.2019.00019}, abstract = {In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.}, location = {Montreal, Quebec, Canada}, series = {MET '19}, pages = {76\u201383}, numpages = {8}, keywords = {metamorphic data relations, quality assessment, data quality, big data, metamorphic testing}}
@inproceedings{10.5555/2694476.2694483,title = {Big data: from querying to transaction processing}, author = {Ramachandra Karthik , Sudarshan S. },year = {2013}, publisher = {Computer Society of India}, address = {Mumbai, Maharashtra, IND}, abstract = {The term Big Data has been used and abused extensively in the past few years, and means different things to different people. A commonly used notion says Big Data is about \"volume\" (of data), \"velocity\" (rate at which data is inserted/updated) and \"variety\" (of data types). In this tutorial, we use the term Big Data to refer to any data processing need that requires a high degree of parallelism. In other words, we focus primarily on the \"volume\" and \"velocity\" aspects.As part of this tutorial, we will cover some aspects of Big Data management, in particular scalable storage, scalable query processing, and scalable transaction processing.This is an introductory tutorial for those who are not familiar with the areas that we will be covering. The focus will be conceptual; it is not meant as a tutorial on how to use any specific system.}, location = {Ahmedabad, India}, series = {COMAD '13}, pages = {10}, numpages = {1}}
@inproceedings{10.1145/3372454.3372478,title = {Assessing Reliability of Big Data Stream for Smart City}, author = {Puangpontip Supadchaya , Hewett Rattikorn },year = {2019}, isbn = {9781450372015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3372454.3372478}, doi = {10.1145/3372454.3372478}, abstract = {Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.}, location = {Cergy-Pontoise, France}, series = {ICBDR 2019}, pages = {18\u201323}, numpages = {6}, keywords = {IoT, Data Reliability, Theory of evidence, Smart City}}
@inproceedings{10.1145/2463676.2465338,title = {Machine learning for big data}, author = {Condie Tyson , Mineiro Paul , Polyzotis Neoklis , Weimer Markus },year = {2013}, isbn = {9781450320375}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2463676.2465338}, doi = {10.1145/2463676.2465338}, abstract = {Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities.The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.}, location = {New York, New York, USA}, series = {SIGMOD '13}, pages = {939\u2013942}, numpages = {4}, keywords = {big data, databases, machine learning}}
@inproceedings{10.1145/3437075.3437085,title = {Big Data Analyses and New Technology Applications in Sport Management, an Overview}, author = {Mataruna-Dos-Santos Leonardo Jose , Faccia Alessio , Hel\u00fa Hussein Mu\u00f1oz , Khan Mohammed Sayeed },year = {2020}, isbn = {9781450375061}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3437075.3437085}, doi = {10.1145/3437075.3437085}, abstract = {Technology has profoundly changed our lives, especially in the past two decades. The introduction of the internet and PCs, first of all, cell phones and later smartphones, has changed our daily habits, leading us to be always connected for many hours of the day. Changes that have affected all fields, not least sporting activity, always focused on performance maximization. Technology in sport has made great strides, allowing both amateurs and even more professionals to use innovative technical solutions that can improve performance: first of all during training and then, consequently, in official competitions. Innovations both in the field of materials, but above all in terms of tools for verifying correct training through the collection of a large number of data, turned into carefully analysed useful information. There are sports that have benefited most from these new technologies, based on their particular characteristics. This research focused on a systematic analysis of the most important technologies that are currently allowing great progress in sports performance and in the impartiality of competitions through the analysis of the collected data. In particular, the research highlighted three particular areas of interest: a) video assistant data collectors; b) Wearable technologies; c) Scouting tech-based techniques.}, location = {Manchester, United Kingdom}, series = {ICBDM 2020}, pages = {17\u201322}, numpages = {6}, keywords = {Big Data, Sport Management, Sport performances, Technologies in Sport}}
@inproceedings{10.1145/3404687.3404692,title = {A Near Metal Platform for Intensive Big Data Processing Using A Novel Approach: Persistent Distributed Channels}, author = {Fikri Noussair , Rida Mohamed , Abghour Noureddine , Moussaid Khalid , El Omri Amina },year = {2020}, isbn = {9781450375474}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3404687.3404692}, doi = {10.1145/3404687.3404692}, abstract = {In this paper we present a new Golang based framework for distributed intensive data processing and also micro batching. It uses a novel approach, the persistent distributed channels, based on the concept of Share memory by communicating, and inspired from Resilient distributed datasets of Apache Spark. The architecture of our proposed system is considered as near-metal platform for Big Data operations in order to enhance the speed of massive data processing.}, location = {Chengdu, China}, series = {ICBDC '20}, pages = {1\u20135}, numpages = {5}, keywords = {batching, Resilient, near-metal, Golang, massive, Big data, distributed, datasets, persistent, channels}}
@inproceedings{10.1145/3404687.3404705,title = {Understanding Consumer Behavior by Big Data Visualization in the Smart Space Laboratory}, author = {Yau Peter ChunYu , Wong Dennis , Luen Woo Hok , Leung Joseph },year = {2020}, isbn = {9781450375474}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3404687.3404705}, doi = {10.1145/3404687.3404705}, abstract = {In this paper, we describe a proof-of-concept (PoC) methodology to understand consumer behavior and spending pattern via visualization analysis in a custom-made smart space laboratory. This laboratory simulates the real-world shopping environment, allows big data generation and collection from various kinds of shopping activities. Data were captured from the service users who are having their technical and business training in a controlled setting environment. Consumer behavior modeling will be described, technical detail such as environment construction, theory, logic, framework, infrastructure, and architecture will also be discussed in this paper. Preliminary results showed that both \"holding time\" and the \"frequency on the spots\" have a certain relationship to the purchase decision which made by the consumer (i.e. service user in the laboratory): the longer stay time where the service user is located, the higher chances that the product(s) will be purchased.}, location = {Chengdu, China}, series = {ICBDC '20}, pages = {13\u201317}, numpages = {5}, keywords = {Heat map, Consumer behavior, Smart Space, Big data, Visualization, Internet of Things (IoT), Spending pattern}}
@inproceedings{10.1145/3474944.3474954,title = {The Regulatory Path of Big-Data Price Discrimination-Based on Economic Characteristics and Legal Accountability}, author = {Shen Zhuoyi },year = {2021}, isbn = {9781450389280}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3474944.3474954}, doi = {10.1145/3474944.3474954}, abstract = {Big data has not been effectively governed after it has received social attention since 2018. This paper aims to explore a more appropriate and feasible regulatory path by exploring the reasons why big-data price discrimination has failed to obtain legal regulation, as well as the economic, technological, and market structure causes of big-data price discrimination. In the argument, it is found that there are legal problems such as doubtful illegality, regulatory gaps that belong to the intersection of multiple laws, and difficulty in obtaining evidence. And because of the profit-seeking nature of the platform economy, the unconsciousness of algorithms and the uniqueness of the platform market structure, the boundaries between big-data price discrimination and reasonable marketing strategies are difficult to decide. It is not possible to totally prohibit big-data price discrimination and hinder the development of the new business model of the platform economy. However, when the platform's profit-seeking behavior infringes on the market order and consumer rights, big-data price discrimination is legally accountable. On one side, big-data price discrimination may infringe consumers\u2019 fair trading rights, and the judgment criteria should be determined based on cost and profit rate. On the other side, in the anti-monopoly law, the boundary between reasonable transaction habits and legally accountable differential treatment should be distinguished for big-data price discrimination.}, location = {Singapore, Singapore}, series = {BDET 2021}, pages = {58\u201362}, numpages = {5}, keywords = {anti-monopoly, differential treatment, platform economy, big-data price discrimination, fair trading}}
@inproceedings{10.1145/3372454.3372480,title = {Anomaly detection for machinery by using Big Data Real-Time processing and clustering technique}, author = {Wang Zhuo , Zhou Yanghui , Li Gangmin },year = {2019}, isbn = {9781450372015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3372454.3372480}, doi = {10.1145/3372454.3372480}, abstract = {This paper aims to apply techniques of Big Data Analytics including K-Means Clustering to diagnose potential problems for offshore rotating machinery. The innovative methods are attempted in both Batch K-Means and Streaming K-Means. Their performances are compared with the conventional signal analysis method. Both K-Means models have a better performance on detecting significant mechanical faults as anomalies for offshore rotating machinery which can be considered as appropriate method for machine operational maintenance.}, location = {Cergy-Pontoise, France}, series = {ICBDR 2019}, pages = {30\u201336}, numpages = {7}, keywords = {K-Means Clustering, Trouble Diagnose, Streaming K-Means Clustering, Big Data, Real-Time processing, Anomaly detection}}
@inproceedings{10.1145/3436286.3436304,title = {The Researches on Public Service Information Security in the Context of Big Data}, author = {Wang Chao , Jin Xiangyu },year = {2020}, isbn = {9781450376457}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3436286.3436304}, doi = {10.1145/3436286.3436304}, abstract = {In recent years, with the continuous development of Internet and big data technology, public service informatization has been paid more and more attention. At the same time, information technology provides convenient public services for the public, it also brings great challenges to the information security of public services, and it also brings great threats to the public security. In this paper, the author puts forward some problems, such as \"weak awareness of information security\", \"lack of professional talents\", \"security risks of cloud storage and local storage\" and \"improving the awareness of information security management\"and so on. The author offer proposals one by one, like\" improve the understanding of information security management\", \"train high-quality technology team\", use \"combination of VMware and Ghost\" and \"dual machine hot standby\" technology to solve the security assurance scheme of information storage of public service institutions, provide more convenient, economic, safer and more efficient technical and ideological support for the information security construction of public service institutions, and better maintain public security.}, location = {Johannesburg, South Africa}, series = {ISBDAI '20}, pages = {86\u201392}, numpages = {7}, keywords = {VMware virtual machine, Information security, Big data, Public service, Ghost, Public security}}
@inproceedings{10.1145/1985374.1985376,title = {Data quality: cinderella at the software metrics ball?}, author = {Shepperd Martin },year = {2011}, isbn = {9781450305938}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1985374.1985376}, doi = {10.1145/1985374.1985376}, abstract = {In this keynote I explore what exactly do we mean by data quality, techniques to assess data quality and the very significant challenges that poor data quality can pose. I believe we neglect data quality at our peril since - whether we like it or not - our research results are founded upon data and our assumptions that data quality issues do not confound our results. A systematic review of the literature suggests that it is a minority practice to even explicitly discuss data quality. I therefore suggest that this topic should become a higher priority amongst empirical software engineering researchers.}, location = {Waikiki, Honolulu, HI, USA}, series = {WETSoM '11}, pages = {1\u20134}, numpages = {4}, keywords = {empirical research, data quality, software metrics}}
@inproceedings{10.1145/2513190.2517827,title = {Revisiting aggregation techniques for big data}, author = {Tsotras Vassilis J. },year = {2013}, isbn = {9781450324120}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2513190.2517827}, doi = {10.1145/2513190.2517827}, abstract = {In this talk we first present an introduction to AsterixDB [1], a parallel, semistructured platform to ingest, store, index, query, analyze, and publish \"big data\" (http://asterixdb.ics.uci.edu) and the various challenges we addressed while building it. AsterixDB combines ideas from semistructured data management, parallel database systems, and first-generation data-intensive computing platforms (MapReduce and Hadoop). The full AsterixDB software stack provides support for big data applications from the storage and processing engine (Hyracks [2] available at: http://hyracks.googlecode.com), to the exible query optimization layer (Algebricks), to the interfaces for user-level interaction (AQL, HiveQL, Pregelix, etc.) Hyracks is a partitioned-parallel engine for data intensive computing jobs in the form of DAGs. Algebricks is a model-agnostic, algebraic layer for compiling and optimizing parallel queries to be processed by Hyracks. Queries for AsterixDB can be expressed by either popular higher-level data analysis languages like Pig, Hive or Jaql, or by its native query language (AQL) and data model (ADM) with support for semi-structured information and fuzzy data.Fundamental data processing operations, like joins and aggregations, are natively supported in AsterixDB. The second part of the talk focuses on our experiences while designing efficient local (per node) aggregation algorithms for AsterixDB. In particular, there are two challenges for local aggregations in a big data system: first, if the aggregation is group-based (like the \"group-by\" in SQL), the aggregation result may not fit in main memory; second, in order to allow multiple operations being processed simultaneously, an aggregation operation should work within a strict memory budget provided by the platform. Despite its importance and challenges, the design and evaluation of local aggregation algorithms has not received the same level of attention that other basic operators, such as joins, have received in the literature. Facing a lack of \"off the shelf\" local aggregation algorithms for big data, we present low-level implementation details for engineering the aggregation operator, utilizing (i) sort-based, (ii) hash-based, and (iii) sort-hash-hybrid approaches. We present six algorithms all of which work within a strictly bounded memory budget, and can easily adapt between in-memory and external processing. Among them, two are novel and four are based on extending existing join algorithms.We deployed all algorithms as operators in the Hyracks platform and evaluated their performance through extensive experimentation. Our experiments cover many different performance factors, including input cardinality, memory, data distribution, and hash table structure. Our study guided our selection of the local aggregation algorithms supported in the recent release of AsterixDB, namely: the hybrid-hash. Pre-Partitioning algorithm for its tolerance on the estimation of the input grouping key cardinality, the Hash-Sort algorithm for its good performance when aggregating skewed data, and the Sort-Based algorithm when the input data is already sorted. This local aggregation work is the first part of a two-part big data aggregation study, as it addresses the \"map\" phase. Our findings provide the foundation for the global aggregation strategy we are currently investigating for the \"reduce\" phase. We hope our experience can help developers of other Big Data platforms to build a solid local aggregation operator.}, location = {San Francisco, California, USA}, series = {DOLAP '13}, pages = {1\u20132}, numpages = {2}, keywords = {aggregation, big data management system}}
@inproceedings{10.1145/3220199.3220221,title = {Network Traffic Monitoring System Based on Big Data Technology}, author = {Lv Bin , Yu Xuemin , Xu Guokun , Yin Qilei , Shi Zhixin },year = {2018}, isbn = {9781450364263}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3220199.3220221}, doi = {10.1145/3220199.3220221}, abstract = {With the rapid growth of network traffic and the increasing rich methods of network attacks, traditional network traffic monitoring system cannot meet the requirements of data storage and query in real time. Therefore, how to monitor the large scale network traffic effectively has become an important challenge for network security management. Aiming at it, we propose a new network monitoring system where Netflow as the monitoring object based on big data technology, which has four main functions: it can use Filebeat to collect Netflow in real time; it transfers the data reliably based on Logstash; it stores the data in ElasticSearch, it analyzes and displays the data in real time through Kabana. The experimental results show that our system is capable of meeting millisecond responses to 100 million of Netflows. It can meet the requirements of real-time monitoring for large-scale network traffic, and provide the basis for network security control.}, location = {Shenzhen, China}, series = {ICBDC '18}, pages = {27\u201332}, numpages = {6}, keywords = {ElasticSearch, Logstash, Filebeat, Netflow, Kibana, Network traffic monitoring}}
@inproceedings{10.1145/2905055.2905215,title = {A Major Threat to Big Data: Data Security}, author = {Dubey Arunima , Srivastava Satyajee },year = {2016}, isbn = {9781450339629}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2905055.2905215}, doi = {10.1145/2905055.2905215}, abstract = {Big Data has nowadays become the most talked latest IT trends. The fact that it can handle all the forms of data which includes unstructured data, big data has now become the preferred choice for analysis of huge amount of data over the Relational Database Management System(RDBMS). Big Data is helpful for the analysis of petabytes of data which is not possible in case of normal database system. But as everything comes with pros and cons so does big data. There are certain challenges which big data analytics is facing. The various challenges include validating the end point input, handling enormous amount of data on a very large scale, ensuring the security of transactional data, ensuring the safe and secure storage of data, sharing of variants of data with third party and analyzing those data without skipping even a single piece of information in order to generate reports and draw a conclusion. In this paper, we will be considering the major challenge to the big data and that is data security. Even with enormous advantages, the industry is taking a backseat for the shift from normal database to big data because of the data privacy concern. Even many big organizations don't consider big data to be a safe option as the data can be accessed by anyone. Certain different methods like the encryption-decryption technique, anonymization based, etc have been suggested by the researchers who are working to overcome the major threat of Big Data i.e. Data security. But unfortunately because of the 3 V's of the Big Data as stated by Gartner i.e. Velocity, Volume and Variety these methods didn't prove to be advantageous. Basically in this paper we will be discussing the various concern issues of Big data, out of which our main focus will be on data security.}, location = {Udaipur, India}, series = {ICTCS '16}, pages = {1\u20136}, numpages = {6}, keywords = {encryption, Big data, transactional data, anonymization, unstructured data}}
@inproceedings{10.1145/3524383.3524423,title = {The Current Situation and Future Trend of Big Data: Visualization Analysis of literature Based on Citespace}, author = {Zeng Xianfeng , Chang Jing , Lai Yiqiang , Huang Changjiang },year = {2022}, isbn = {9781450395793}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3524383.3524423}, doi = {10.1145/3524383.3524423}, abstract = {More and more attentions have been paid to the value of big data, and the amount of research literature on big data is also increasing exponentially. In this paper, CiteSpace citation analysis software is used as an analysis tool and big data related literature in the Web of Science core database during 2011- 2021 is used as an analysis object. The major task is to create the knowledge map of big data with multi-dimension, build knowledge graphs of big data from several perspectives and develop visual analyses of the literature. From the sight of time and space. Differences between the numbers and the research contents of literature on big data published by different countries, regions, and institutions have been analyzed and compared; By co-cited literature, the classical papers and core literature in the field of big data have been detected, and the development context of big data domain have been clarified; Cluster analysis has been conducted on hot keywords to find the research hotspots of big data domain in recent years. Burst words detection algorithm has been used to track the research frontiers and development trends in the field of big data. The results show that China has surpassed the developed countries in Europe and the United States in the number of literature on big data. However, there is still a lack of core technology literature. The development of big data and AI will be in the same direction and depend on each other; The improvement of deep learning algorithms based of big data analysis is going to become a frontier research topic in a long-term; The integration of big data and all walks of life should to be deepened, and researchers need to find the breakthrough points for the application of big data based on the characteristics of the industry.}, location = {Shanghai, China}, series = {ICBDE '22}, pages = {368\u2013378}, numpages = {11}}
@inproceedings{10.1145/3468920,title = {The 2021 3rd International Conference on Big Data Engineering},year = {2021}, isbn = {9781450389426}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Shanghai, China}}
@inproceedings{10.1145/3047273.3047377,title = {Exploiting Big Data for Evaluation Studies}, author = {Netten Niels , Bargh Mortaza S. , Choenni Sunil , Meijer Ronald },year = {2017}, isbn = {9781450348256}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3047273.3047377}, doi = {10.1145/3047273.3047377}, abstract = {The collection and analysis of relevant data for evaluating public policies is not a straightforward task. An important type of such studies is the so-called ex-post evaluation. The main objective of ex-post evaluations is to determine to what extent a realized intervention is successful in tackling a societal challenge, e.g., youth unemployment. At a first glance an obvious method is to collect some baseline measurements for a set of relevant variables, apply the intervention for a while and collect the new measurement values for the same set of variables. Then, comparing the measurement values of the variables before and after the intervention provides an insight into the extent of successfulness of the intervention. This, however, is only true if the \"ceteris paribus\" condition holds. In practice it is infeasible to enforce this condition for societal challenges. Often, after having the baseline measurements, several phenomena emerge that may impact the new measurements without being taken into account. This makes it difficult to determine how much of the measured differences between the values of the variables before and after the intervention should be attributed to the emerging phenomena (or the so-called counterfactuals) and how much of the differences can be attributed to the applied intervention.This paper discusses how exploiting big data may contribute to the task of elucidating the influences of counterfactuals (and interventions) in ex-post evaluation studies. The paper proposes a framework to utilize big data for accounting for the impact of emerging phenomena in ex-post evaluation studies.}, location = {New Delhi AA, India}, series = {ICEGOV '17}, pages = {228\u2013231}, numpages = {4}, keywords = {ex-post policy evaluation, data linkage, counterfactuals, Big data}}
@inproceedings{10.1145/2723372.2742794,title = {Telco Churn Prediction with Big Data}, author = {Huang Yiqing , Zhu Fangzhou , Yuan Mingxuan , Deng Ke , Li Yanhua , Ni Bing , Dai Wenyuan , Yang Qiang , Zeng Jia },year = {2015}, isbn = {9781450327589}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2723372.2742794}, doi = {10.1145/2723372.2742794}, abstract = {We show that telco big data can make churn prediction much more easier from the $3$V's perspectives: Volume, Variety, Velocity. Experimental results confirm that the prediction performance has been significantly improved by using a large volume of training data, a large variety of features from both business support systems (BSS) and operations support systems (OSS), and a high velocity of processing new coming data. We have deployed this churn prediction system in one of the biggest mobile operators in China. From millions of active customers, this system can provide a list of prepaid customers who are most likely to churn in the next month, having $0.96$ precision for the top $50000$ predicted churners in the list. Automatic matching retention campaigns with the targeted potential churners significantly boost their recharge rates, leading to a big business value.}, location = {Melbourne, Victoria, Australia}, series = {SIGMOD '15}, pages = {607\u2013618}, numpages = {12}, keywords = {big data, telco churn prediction, customer retention}}
@inproceedings{10.1145/2896825.2896834,title = {A reference architecture for big data systems in the national security domain}, author = {Klein John , Buglak Ross , Blockow David , Wuttke Troy , Cooper Brenton },year = {2016}, isbn = {9781450341523}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2896825.2896834}, doi = {10.1145/2896825.2896834}, abstract = {Acquirers, system builders, and other stakeholders of big data systems need to define requirements, develop and evaluate solutions, and integrate systems together. A reference architecture enables these software engineering activities by standardizing nomenclature, defining key solution elements and their relationships, collecting relevant solution patterns, and classifying existing technologies. Within the national security domain, existing reference architectures for big data systems have not been useful because they are too general or are not vendor-neutral. We present a reference architecture for big data systems that is focused on addressing typical national defence requirements and that is vendor-neutral, and we demonstrate how to use this reference architecture to define solutions in one mission area.}, location = {Austin, Texas}, series = {BIGDSE '16}, pages = {51\u201357}, numpages = {7}, keywords = {big data, reference architecture}}
@inproceedings{10.1145/2463676.2486082,title = {Big data in capital markets}, author = {Nazaruk Alex , Rauchman Michael },year = {2013}, isbn = {9781450320375}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2463676.2486082}, doi = {10.1145/2463676.2486082}, abstract = {Over the past decade global securities markets have dramatically changed. Evolution of market structure in combination with advances in computer technologies led to emergence of electronic securities trading. Securities transactions that used to be conducted in person and over the phone are now predominantly executed by automated trading systems. This resulted in significant fragmentation of the markets, vast increase in the exchange volumes and even greater increase in the number of orders.In this talk we present and analyze forces behind the wide proliferation of electronic securities trading in US stocks and options markets. We also make a high-level introduction into electronic securities market structure. We discuss trading objectives of different classes of market participants and analyze how their activity affects data volumes. We also present typical securities trading firm data flow and analyze various types of data it uses in its trading operations.We close with the implications this \"sea change\" has on DBMS requirements in capital markets.}, location = {New York, New York, USA}, series = {SIGMOD '13}, pages = {917\u2013918}, numpages = {2}, keywords = {capital markets, securities, big data, transactions}}
@inproceedings{10.1145/2939672.2945369,title = {Big Data Needs Big Dreamers: Lessons from Successful Big Data Investors}, author = {Simoudis Evangelos , Gorenberg Mark , Guleri Tim , Ocko Matt , Sands Greg },year = {2016}, isbn = {9781450342322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2939672.2945369}, doi = {10.1145/2939672.2945369}, location = {San Francisco, California, USA}, series = {KDD '16}, pages = {11\u201312}, numpages = {2}, keywords = {panel}}
@inproceedings{10.1145/3193077.3193079,title = {Interaction Between Big Data and Cognitive Science}, author = {Han Xiao , Qingdong-Du },year = {2018}, isbn = {9781450363594}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3193077.3193079}, doi = {10.1145/3193077.3193079}, abstract = {Big data is sweeping in a storm way, the world was rushed into the age of big data.With the advent of the era of big data, people's way of thinking will also have a huge change. Therefore, we must from the previous thinking of small data quickly converted into the thinking of big data,to adapt to this innovative change. This revolution not only brings new challenges but also provides new opportunities for traditional cognitive science. Big data makes science from the pursuit of causality to pay attention to relevance;Through \"let the data sound\" put forward \"the science starts data\",added a new logical path for scientific discovery;The law of cause and effect was supplemented by the law of data, and widened the scope of scientific law.Big data brings new development to traditional scientific epistemology,thus formed the understanding science of the large data.}, location = {DeKalb, IL, USA}, series = {ICCDA 2018}, pages = {1\u20135}, numpages = {5}, keywords = {correlation, cognitive science, Big data}}
@inproceedings{10.1145/2812428.2812441,title = {Big data in electricity-visualization aspect}, author = {Keka Ilir , \u00c7i\u00e7o Betim },year = {2015}, isbn = {9781450333573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2812428.2812441}, doi = {10.1145/2812428.2812441}, abstract = {Nowadays, the rapid development of technology has made it possible to store the data in memory with high capacity. There are various sources from where the data are collected or generated Moreover, the speed of updating and processing the data is very important to many organizations. Big Data is a concept the covers capacity, speed and different formats of data in many organizations.The aim of this paper is to analyze the Big Data in electricity using visualization aspect. Also, it gives an overview of features and sources of Big Data. The visualization on electricity data is made based on Revolution R Enterprise (RRE) tool. Writing scripts in RRE is found the relationship between electricity load and time. This relationship is illustrated using the line of best fit and the smoothing line. Also, some statistical parameters are computed and the relationship of data is evaluated using visualization.}, location = {Dublin, Ireland}, series = {CompSysTech '15}, pages = {236\u2013243}, numpages = {8}, keywords = {electric power, visualization, revolution r enterprise, time, regression, load, big data}}
@inproceedings{10.1145/3358505.3358519,title = {Big data execution time based on Spark Machine Learning Libraries}, author = {G\u00e1rate-Escamilla Anna Karen , El Hassani Amir Hajjam , Andres Emmanuel },year = {2019}, isbn = {9781450371650}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3358505.3358519}, doi = {10.1145/3358505.3358519}, abstract = {The paper focuses on exploring the time consumption of supervised and unsupervised models of Apache Spark framework in massive datasets. Big Data analytics has been relevant in the industry due to the need to convert information into knowledge. Among the challenge of big data is the creation of strategies to improve the execution costs of running machine learning models to make a prediction. Apache Spark is a powerful in-memory platform that offers an extensive machine learning library for regression, classification, clustering, and rule extraction. This investigation, from a computation cost perspective, performs different experiments using real datasets. The main contribution of the paper is to compare the execution time of different machine learning models, such as random forests, decision tree, logistic regression, linear support vector machine, and kNN. The present work expects to combine the areas of big data and machine learning, comparing the results with different configurations and the use of the optimization methods, cache and persist. The evaluation experiments show that logistic regression performed the shortest execution time of the Spark MLlib models.}, location = {Oxford, United Kingdom}, series = {ICCBDC 2019}, pages = {78\u201383}, numpages = {6}, keywords = {Execution time prediction, Machine Learning, Performance prediction model, Apache Spark}}
@inproceedings{10.1145/3404687,title = {Proceedings of the 5th International Conference on Big Data and Computing},year = {2020}, isbn = {9781450375474}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {This volume contains the proceedings of the 2020 5th International Conference on Big Data and Computing (ICBDC 2020), held from May 28 to 30, 2020. The keynote speakers who also further explored this topic that is so significant for this field include: Prof. Rajkumar Buyya, University of Melbourne, Australia, on speech title \"New Frontiers in Cloud and Edge/Fog Computing for Big Data & Internet-of- Things Applications\", Prof. Changsheng Xu, Chinese Academy of Sciences, China, on speech title \"Connecting Isolated Social Multimedia Big Data\", Prof. Hong Shen, Sun Yat-sen University, China, on speech title \"Job Scheduling in Cloud Data Centers\", and Prof. Ruidong Li, National Institute of Information and Communications Technology, Japan, on speech Title \"Blockchain Meets In-Network Big Data Sharing.}, location = {Chengdu, China}}
@inproceedings{10.1145/2890602.2890619,title = {Big Data and Analytics Leaders: the Changing Role of CIO}, author = {Morabito Vincenzo , Viscusi Gianluigi , Themistocleus Marinos },year = {2016}, isbn = {9781450342032}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2890602.2890619}, doi = {10.1145/2890602.2890619}, abstract = {This article investigates the changing role of the Chief Information Officer (CIO) at organizational level with regard to the rise of Big Data and Big Data analytics as a potential source of innovation and competitive advantage. The paper aims to provide a theoretical contribution to the research stream on the topic, by further exploring the emergent properties and understandings related to the role of CIO. As a consequence of the need to adopt advanced technologies, the CIO has been named to master the current unheard information growth for business innovation. To this end we present the results of a qualitative research based on grounded theory carried out on data concerning CIOs of medium and large companies from different industries in the Italian market. Finally, a substantive theory and categories are discussed, showing the role of generation gap and power of new entrants as well as of project and execution excellence on the making of identity and recognition of the CIO as relevant at the time of Big Data analytics.}, location = {Alexandria, Virginia, USA}, series = {SIGMIS-CPR '16}, pages = {39\u201346}, numpages = {8}, keywords = {grounded theory, CIO, analytics, big data}}
@inproceedings{10.1145/2351316.2351320,title = {Parallel rough set based knowledge acquisition using MapReduce from big data}, author = {Zhang Junbo , Li Tianrui , Pan Yi },year = {2012}, isbn = {9781450315470}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2351316.2351320}, doi = {10.1145/2351316.2351320}, abstract = {Nowadays, with the volume of data growing at an unprecedented rate, big data mining and knowledge discovery have become a new challenge. Rough set theory for knowledge acquisition has been successfully applied in data mining. The recently introduced MapReduce technique has received much attention from both scientific community and industry for its applicability in big data analysis. To mine knowledge from big data, we present parallel rough set based methods for knowledge acquisition using MapReduce in this paper. Comprehensive experimental evaluation on large data sets shows that the proposed parallel methods can effectively process big data.}, location = {Beijing, China}, series = {BigMine '12}, pages = {20\u201327}, numpages = {8}, keywords = {knowledge acquisition, big data, rough sets, MapReduce}}
@inproceedings{10.1145/3396452.3396463,title = {Research on Methods of Psychological Health Education of University Students in the context of Big Data}, author = {Panqiu Jiang , Li Lin },year = {2020}, isbn = {9781450374989}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3396452.3396463}, doi = {10.1145/3396452.3396463}, abstract = {With the further development of big data, the psychological health education of university students confronts with new challenges and opportunities. However, so far, no research has been conducted to construct a new methods of psychological health education of University Students through combining big data from the perspective of Internet. This paper takes the psychological health education of university students as research object and the idea of positive psychology is referenced as guidance in order to explore the methods of psychological health education of university students in the context of big data. This work builds the mixed curriculum system of psychological health education combining online and offline in university to stimulate students' potential self-help capabilities and improve the system of psychological health education of university students.}, location = {London, United Kingdom}, series = {ICBDE '20}, pages = {10\u201314}, numpages = {5}, keywords = {positive psychology, university students, big data, psychological health education}}
@inproceedings{10.1145/3277104,title = {Proceedings of the 2018 International Conference on Computing and Big Data},year = {2018}, isbn = {9781450365406}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {2018 International Conference on Computing and Big Data (ICCBD 2018) has been successfully held in College of Charleston, Charleston, South Carolina, USA during September 08-10, 2018. The objective of the conference is to provide a forum for the discussion of new developments, recent progress, and innovations in the Computing and Big Data.}, location = {Charleston, SC, USA}}
@inproceedings{10.1145/2351316.2351323,title = {Compression-aware I/O performance analysis for big data clustering}, author = {Xue Zhenghua , Shen Geng , Li Jianhui , Xu Qian , Zhang Yang , Shao Jing },year = {2012}, isbn = {9781450315470}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2351316.2351323}, doi = {10.1145/2351316.2351323}, abstract = {As the data volume increases, I/O bottleneck has become a great challenge for data analysis. Data compression can alleviate the bottleneck effectively. Taking K-means algorithm as an example, this paper proposes a compression-aware performance improvement model for big-data clustering. The model quantitatively analyzes the effect of a variety of factors related to compression during the entire computational process. We perform clustering experiments on 10 dimensional data with up to 1.114 TB in size on a cluster computer with hundreds of computing cores. The measurement validates that using compression contributes significantly to improving the I/O performance, and confirms our theoretical analysis empirically. Furthermore, the proposed model can effectively determine when and how to use compression to improve I/O performance for big-data analysis.}, location = {Beijing, China}, series = {BigMine '12}, pages = {45\u201352}, numpages = {8}, keywords = {big data clustering, compression contribution model, I/O bottleneck}}
@inproceedings{10.1145/3328833.3328841,title = {Big Data in Healthcare: Are we getting useful insights from this avalanche of data?}, author = {Adenuga Kayode I. , Muniru Idris O. , Sadiq Fatai I. , Adenuga Rahmat O. , Solihudeen Muhammad J. },year = {2019}, isbn = {9781450361057}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3328833.3328841}, doi = {10.1145/3328833.3328841}, abstract = {The benefits of deriving useful insights from avalanche of data available everywhere cannot be overemphasized. Big Data analytics can revolutionize the healthcare industry. It can also ensure functional productivity, help forecast and suggest feedbacks to disease outbreaks, enhance clinical practice, and optimize healthcare expenditure which cuts across all stakeholders in healthcare sectors. Notwithstanding these immense capabilities available in the general application of big data; studies on derivation of useful insights from healthcare data that can enhance medical practice have received little academic attention. Therefore, this study highlighted the possibility of making very insightful healthcare outcomes with big data through a simple classification problem which classifies the tendency of individuals towards specific drugs based on personality measures. Our model though trained with less than 2000 samples and with a simple neural network architecture achieved mean accuracies of 76.87% (sd=0.0097) and 75.86% (sd=0.0123) for the 0.15 and 0.05 validation sets respectively. The relatively acceptable performance recorded by our model despite the small dataset could largely be attributed to number of attributes in our dataset. It is essential to uncover some of the many complexities in our societies in relations to healthcare; and through many machine learning architectures like the neural networks these complex relationships can be discovered}, location = {Cairo, Egypt}, series = {ICSIE '19}, pages = {196\u2013199}, numpages = {4}, keywords = {Analytics, Challenges, Benefits, Big Data}}
@inproceedings{10.1145/3345252.3345282,title = {Conceptual Architecture of GATE Big Data Platform}, author = {Petrova-Antonova Dessislava , Krasteva Iva , Ilieva Sylvia , Pavlova Irena },year = {2019}, isbn = {9781450371490}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3345252.3345282}, doi = {10.1145/3345252.3345282}, abstract = {Today we experience a data-driven society. All human activities, industrial processes and research lead to data generation of unprecedented scale, spurring new products, services and businesses. Big Data and its application have been a target for European Commission -- with more than 100 FP7 and about 50 H2020 funded projects under Big Data domain. GATE project aims to establish and sustain in the long run a Centre of Excellence as collaborative environment for conducting Big Data research and innovation, facilitated by GATE platform and Innovation Labs. This paper proposes a conceptual architecture of GATE platform, that is holistic, symbiotic, open, evolving and data-integrated. It is also modular and with component-based design that allows to position a mix of products and tools from different providers. GATE platform will enable start-ups, SMEs and large enterprises, as well as other organizations in a wide range of sectors, to build advanced Data driven services and applications. The usability of the proposed architecture is proven through a development of a sample time series data visualization application. Its architecture follows the proposed one through implementation of required components using open technology stack.}, location = {Ruse, Bulgaria}, series = {CompSysTech '19}, pages = {261\u2013268}, numpages = {8}, keywords = {GATE Platform, Smart City, Big Data Value Chain, Emerging Architectures, Big Data}}
@inproceedings{10.1145/3291801.3291817,title = {Application of big data collection based on self-powered technology in intelligent transportation system}, author = {Li Dezhi , Wang Xiaohui , Cai Wei , Zhu Linsen , Guo Chunsheng },year = {2018}, isbn = {9781450364768}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3291801.3291817}, doi = {10.1145/3291801.3291817}, abstract = {Intelligent transportation system is an important part of smart city. The wireless sensor network (WSN) system can provide real-time and reliable big data for the intelligent transportation system, but the power supply of the wireless sensor node is a difficult point that restricts its wide application. This study will use the piezoceramic to converts vibration energy in the surroundings into voltage output. The power management module can meet the energy requirements of MEMS devices such as wireless sensor nodes, wireless monitoring modules, accelerometers and so on. Using piezoceramic as energy supply, the total mass, speed and wheelbase of vehicles can be potentially measured through systematic layout, which is to achieve self-powered demand. The acceleration and temperature sensors can be used to record acceleration information and surface temperature data of the vehicle respectively. A wireless sensor node is established in unit of a wireless transmission module, and a wireless sensor network is further formed. The data obtained by the wireless sensor network is processed in the server to obtain traffic information such as traffic volume, vehicle type and overspeed in different sections in real time, which provides powerful and accurate data support for the planning and decision-making of the intelligent transportation system.}, location = {Weihai, China}, series = {ICBDR 2018}, pages = {47\u201351}, numpages = {5}, keywords = {intelligent transportation system, sensor node, wireless sensor network (WSN), self-powered, big data}}
@inproceedings{10.1145/3148238,title = {Requirements for Data Quality Metrics}, author = {Heinrich Bernd , Hristova Diana , Klier Mathias , Schiller Alexander , Szubartowicz Michael },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3148238}, doi = {10.1145/3148238}, abstract = {Data quality and especially the assessment of data quality have been intensively discussed in research and practice alike. To support an economically oriented management of data quality and decision making under uncertainty, it is essential to assess the data quality level by means of well-founded metrics. However, if not adequately defined, these metrics can lead to wrong decisions and economic losses. Therefore, based on a decision-oriented framework, we present a set of five requirements for data quality metrics. These requirements are relevant for a metric that aims to support an economically oriented management of data quality and decision making under uncertainty. We further demonstrate the applicability and efficacy of these requirements by evaluating five data quality metrics for different data quality dimensions. Moreover, we discuss practical implications when applying the presented requirements.}, pages = {1\u201332}, numpages = {32}, keywords = {Data quality, data quality assessment, requirements for metrics, data quality metrics}}
@inproceedings{10.1145/3206157.3206181,title = {Optimized Rank Estimator in Big Data Social Networks}, author = {Pirouz Matin , Parsa Sai Phani , Zhan Justin },year = {2018}, isbn = {9781450363587}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3206157.3206181}, doi = {10.1145/3206157.3206181}, abstract = {In this study, FAST Personalized PageRank is utilized to find the target node set. Using the mentioned target set, the algorithm gives an estimation of the closeness of any pair of nodes in the graph. Personalized Page Vector is used to find the most popular nodes, also known as hubs, in the network. The time taken by the estimation of Personalized PageRank is directly proportional to the network size. In this work, we proposed a node reduction method to prune the graph. To decrease the entropy and reduce the number of alternate paths to the target nodes, redundant popular nodes are identified and flagged. The flagged nodes are, then, given a lower priority in the computation. After pruning the graph, estimation results achieve an improved time complexity. The proposed method achieves a twice shorter computation time as compared to FAST PPR and Local Update.}, location = {Honolulu, HI, USA}, series = {ICBDE '18}, pages = {80\u201384}, numpages = {5}, keywords = {Graph Theory, Hub Nodes, Personalized PageRank}}
@inproceedings{10.1145/3152723.3152732,title = {A Platform for Supporting Automatic Data Storing and Visualization of Public and Private Big Data}, author = {Back Bong-Hyun , Ha Il-Kyu },year = {2017}, isbn = {9781450353564}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3152723.3152732}, doi = {10.1145/3152723.3152732}, abstract = {With the arrival of the Fourth Industrial Revolution, large amounts of data are being produced every day in various fields such as industry, culture, politics, education, medicine, and meteorology. In recent years, as the importance of data has increased, data that have been provided free of charge, such as existing social networks, have become available, and the supply period is being limited. Commercialization of data is a continuous process in all over the world; therefore, the data industry, which collects, processes, analyzes, predicts, and provides data is expected to be more active. In this research, using the universal resource locator (URL)and application programming interface (API) of the dataset provided by various public and personal data providers, we propose an automatic database building and visualization platform that can automatically convert the collected data into database (DB), regardless of the data format. Particularly, it is possible to restructure the dataset on the proposed platform, which allows the user to select only the necessary items in the collected dataset and visualize the data set by fusing it.}, location = {Osaka, Japan}, series = {ICBDR 2017}, pages = {12\u201317}, numpages = {6}, keywords = {Public Big Data, Big Data Visualization, Big Data Processing, Distributed Processing}}
@inproceedings{10.1145/3524383.3524402,title = {The Construction of Big Data College Employment Information Service Platform from the Perspective of Precise Assistance}, author = {Li Yatao , Deng Jiahong },year = {2022}, isbn = {9781450395793}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3524383.3524402}, doi = {10.1145/3524383.3524402}, abstract = {In order to solve the employment difficulties caused by the lag of vocational education in colleges and the asymmetry of information between schools and enterprises, this paper builds a college employment information service platform integrating recruitment, employment and education based on big data technology, using text mining and data analysis, starting from the perspective of accurate employment assistance and taking graduate employment as the core. The platform realizes massive data collection, accurate job recommendation, employment guidance, student career portraits, employer portraits, online full-process employment and other services to accurately help the online normalization of employment models and improve the quality of employment and the happiness of graduates.}, location = {Shanghai, China}, series = {ICBDE '22}, pages = {114\u2013120}, numpages = {7}, keywords = {Precision employment, Big data, Precision assistance}}
@inproceedings{10.1145/2928294.2928302,title = {Semantic question answering on big data}, author = {Tatu Marta , Werner Steven , Balakrishna Mithun , Erekhinskaya Tatiana , Moldovan Dan },year = {2016}, isbn = {9781450342995}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2928294.2928302}, doi = {10.1145/2928294.2928302}, abstract = {This article describes a high-precision semantic question answering (SQA) engine for large datasets. We employ an RDF store to index the semantic information extracted from large document collections and a natural language to SPARQL conversion module to find desired information. In order to be able to find answers to complex questions in structured/unstructured data resources, our system produces rich semantic structures from the data resources and then transforms the extracted knowledge into an RDF representation. In order to facilitate easy access to the information stored in the RDF semantic index, our system accepts a user's natural language questions, translates them into SPARQL queries and returns a precise answer back to the user. Our improvements in performance over a regular free text search index-based question answering engine prove that SQA can benefit greatly from the addition and consumption of deep semantic information.}, location = {San Francisco, California}, series = {SBD '16}, pages = {1\u20136}, numpages = {6}, keywords = {unstructured data, RDF, question answering, SPARQL}}
@inproceedings{10.1145/2957276.2997027,title = {Towards Re-Orienting the Big Data Rhetoric}, author = {Verma Nitya },year = {2016}, isbn = {9781450342766}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2957276.2997027}, doi = {10.1145/2957276.2997027}, abstract = {Data analytics and BI (business intelligence) systems are the most prominent user-facing manifestation of 'big data' and the related computational turn in thinking within organizations. However, the big data mythologies-specifically that data can offer more accurate, objective and truthful forms of intelligence and knowledge-impact, reinforce, and reproduce certain epistemological biases. In my research, I study these big data technologies in human services related contexts to examine knowledge claims and the strengths and limitations of big data.}, location = {Sanibel Island, Florida, USA}, series = {GROUP '16}, pages = {505\u2013508}, numpages = {4}, keywords = {data analytics, business intelligence, big data}}
@inproceedings{10.1145/3090354.3090369,title = {Survey of Plagiarism Detection Approaches and Big data Techniques related to Plagiarism Candidate Retrieval}, author = {Hourrane Oumaima , Benlahmar El Habib },year = {2017}, isbn = {9781450348522}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3090354.3090369}, doi = {10.1145/3090354.3090369}, abstract = {The easy and fast access to the web and the massive existence of databases of information systems today have led to an agile increase in the phenomenon of plagiarism as a serious problem for publishers and researchers. Fact, a number of researchers have discussed this problem by adopting several techniques that can detect plagiarism. Nevertheless, most of these techniques are still insufficient for the detection of intelligent plagiarism, which still needs to be improved. In this paper, we give an overview of the best-known methods of detection of plagiarism that exist. We start with defining the concept of plagiarism and its various forms most used by plagiarists. A thorough study of these approaches is then carried out, by establishing a comparative table of these approaches according to several criteria. Moreover, we finish by defining the concept of big data as well as one of these techniques that called Text mining, which applied in the phase of extraction of documents sources for plagiarism detection.}, location = {Tetouan, Morocco}, series = {BDCA'17}, pages = {1\u20136}, numpages = {6}, keywords = {Big data, Information Retrieval, Plagiarism, Plagiarism Detection, Text-mining}}
@inproceedings{10.1145/3352740.3352757,title = {The Construction of Big Data Credit Evaluation System in the Management of Indemnificatory Housing Access}, author = {Xiaomei Yang , Songyang Jiang },year = {2019}, isbn = {9781450372053}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3352740.3352757}, doi = {10.1145/3352740.3352757}, abstract = {There are numbers of issues in the management of indemnificatory housing access. The quantity of access cannot meet demand, the standard of access is inaccurate and the evaluation of post-access is passive. This leads to mismatch between supply and demand, as well as lack of liquidity in indemnificatory housing. It should introduce the big data credit evaluation system into the management of indemnificatory housing access, so as to create a binding access credit environment, balance supply and demand, and promote the utilization of housing resources. Specifically, big data platform of national social security credit should be constructed. The \"funnel type\" information sharing mechanism and \"dynamic assessment+ active early warning\" mechanism also should be constructed. In addition, the multi-dimensional big data credit evaluation measure index should be formed.}, location = {Guilin, China}, series = {EBDIT 2019}, pages = {98\u2013101}, numpages = {4}, keywords = {Management of Indemnificatory Housing Access, Evaluation System, Mechanism, Big Data Credit}}
@inproceedings{10.1145/3085228.3085319,title = {Can big data improve public policy analysis?}, author = {Shi Jing , Ai Xiaoyan , Cao Ziyi },year = {2017}, isbn = {9781450353175}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3085228.3085319}, doi = {10.1145/3085228.3085319}, abstract = {We conduct a systematic review targeting the idea of whether big data can improve public policy analysis. The ideas about the benefits and risks of applying big data technologies on public policy analysis are summarized. Big data can provide accurate, proactive, and participatory policy analysis but at the same time it raises risks in data privacy, data misuse and bias, and inequality. We then examine the detailed implementation of big data in each public policy stage. The experiments or practical applications from the relevant literature demonstrate how exactly big data is used to improve policy analysis. This study will guide governments on adopting big data for public policy analysis.}, location = {Staten Island, NY, USA}, series = {dg.o '17}, pages = {552\u2013561}, numpages = {10}, keywords = {policy analysis, big data, data privacy, Public policy}}
@inproceedings{10.1145/3396452.3396462,title = {Big Data in Ideological and Political Education in Colleges and Universities Application and Reflection}, author = {Lv Xin-ye },year = {2020}, isbn = {9781450374989}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3396452.3396462}, doi = {10.1145/3396452.3396462}, abstract = {Along with the era of big data are new opportunities and challenges for the ideological and political education in China. It is an important issue in institutions of higher education in China to aid ideological and political education in colleges and universities by making effective use of big data technology and thinking. The application of big data aids ideological and political education in colleges and universities by improving its relevance, empowering it with computing capability, and providing it with think tank support. At the same time, big data also plays an increasingly important role in and has a significant influence on the specific practice of ideological and political education in colleges and universities.}, location = {London, United Kingdom}, series = {ICBDE '20}, pages = {26\u201329}, numpages = {4}, keywords = {ideological and political education work, colleges and universities, application of big data}}
@inproceedings{10.1145/3322134.3322152,title = {A Probe into the Application of Big Data to Innovating the Education and Management of College Students}, author = {Gu Yang },year = {2019}, isbn = {9781450361866}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3322134.3322152}, doi = {10.1145/3322134.3322152}, abstract = {The Application of Big Data brings considerable benefits to the resource integration of the management of college students, a boost in technical advantages of the education and management of college students and an improvement in time efficiency of the education and management of college students. This paper, based on the analysis of the application of existing big data, discusses the merits of the application of big data to the education and management of college students from fostering a sense of big data, diversifying working methods to innovating education and management mechanism.}, location = {London, United Kingdom}, series = {ICBDE'19}, pages = {113\u2013117}, numpages = {5}, keywords = {education and management, innovation, Big data, college}}
@inproceedings{10.1145/3358528.3358584,title = {Big Data Service Request Prediction Based on Historical Behavior Time Series}, author = {Xu Jiangying , Du Lixin , Song Chenyang , Li Chao , Ren Zhi , Zhu Bo },year = {2019}, isbn = {9781450371926}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3358528.3358584}, doi = {10.1145/3358528.3358584}, abstract = {Big data analysis service has been used widely in our society. For example, in financial field, users often use big data analysis services to analyze stocks, assets, and accounts in real-time investment decision-making. Therefore, real-time service response is very important from the perspective of user experience. Caching data and analysis results have been used widely in industrial practice. But these caches generally are passive, rigid and inefficient. Proactive caching approach for time-consuming data services is a worthwhile research problem. In addition, we have encountered this problem in a practical enterprise application. In this paper, we propose a data service request prediction approach based on historical user behavior time series analysis. Results show that this approach can improve the response speed of backend data services effectively.}, location = {Jinan, China}, series = {ICBDT2019}, pages = {77\u201381}, numpages = {5}, keywords = {User Behavior Prediction, Service Scheduling, Time Series, Time Window}}
@inproceedings{10.1145/3335484,title = {Proceedings of the 4th International Conference on Big Data and Computing},year = {2019}, isbn = {9781450362788}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {With the proliferation of big data, emerging issues of multimedia systems, signal processing, soft computing, cloud computing, networking and computer security, have attracted an increasing amount of interest from researchers across different fields. ICBDC is an annual conference for researchers, academicians, practitioners and industry in big data and computing science communities. This platform provides opportunities for the delegates to exchange new ideas and application experiences face to face, to establish business and research relationships and to facilitate global collaborations in this exciting new research field.}, location = {Guangzhou, China}}
@inproceedings{10.1145/2609876,title = {Proceedings of the 2014 Workshop on Human Centered Big Data Research},year = {2014}, isbn = {9781450329385}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {We are pleased to welcome you to the proceedings of the first workshop on Human-Centered Big Data Research sponsored by the Laboratory for Analytic Sciences and hosted at the North Carolina State University.}, location = {Raleigh, NC, USA}}
@inproceedings{10.1145/3396452.3396454,title = {Improving the Students' Big Data Era Skills through the Online International Project X-culture}, author = {Baranova T. A. , Trostinskaya I. R. , Kobicheva A. M. , Tokareva E. Y. },year = {2020}, isbn = {9781450374989}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3396452.3396454}, doi = {10.1145/3396452.3396454}, abstract = {Among the most crucial for the specialist of the era of Big Data skills are critical-thinking, problem-solving, collaborative and digital skills. In this paper, we investigate how international online project \"X-culture\" that was implemented into the course influenced such vital students' skills. To evaluate the impact we used information from the X-culture project database that was collected through various surveys and conducted an interview with students. According to the results received students skills developed significantly after the participation in the project. Also we implemented a correlation analysis to determine weather students' critical-thinking, problem-solving, collaborative and digital skills affected the final reports scores on the international project. In our case collaborative skills played the most substantial role for getting the highest final reports scores while the relationship between digital skills and reports' results was the weakest.}, location = {London, United Kingdom}, series = {ICBDE '20}, pages = {15\u201320}, numpages = {6}, keywords = {critical thinking skills, collaborative skills, big data, problem-solving skills, digital skills}}
@inproceedings{10.1145/3436286.3436318,title = {Exploring the Relationship between Aviation Service Quality and Customer Satisfaction Based on Big Data Technology}, author = {Ling Hong , Weiguo Chen },year = {2020}, isbn = {9781450376457}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3436286.3436318}, doi = {10.1145/3436286.3436318}, abstract = {As the consumption level of my country's residents increases, customers' requirements for the quality of aviation services are also increasing. Airlines must constantly improve service quality to be satisfied with the increasing demands of customers. The application of big data technology is conducive to improving the quality of aviation services and improving customer satisfaction in the air transportation industry. The relation between aviation quality of service and customer satisfaction is studied in this article. At the same time, how big data technology should be used to improve aviation service quality and improve customer satisfaction is discussed in this article.}, location = {Johannesburg, South Africa}, series = {ISBDAI '20}, pages = {168\u2013173}, numpages = {6}, keywords = {Aviation, Customer Satisfaction, Big Data Technology, Service Quality}}