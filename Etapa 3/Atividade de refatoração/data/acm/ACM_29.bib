@inproceedings{10.1145/3495018.3495374,title = {Clothing Material Design Concept Based on Big Data and Information Technology}, author = {Hu Na },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495374}, doi = {10.1145/3495018.3495374}, abstract = {Fashion design belongs to the category of art and craft, which is an art form combining practicality and artistry. With the development of science and technology, network technology has been fully integrated into social life. The emergence of big data has changed the traditional economic and social form, and put forward new requirements for fashion design profession in the field of education. Based on big data, this paper studies the concept of Chinese clothing material design through hash algorithm of big data. Therefore, in order to adapt to the requirements of the era of big data, this paper believes that the fashion design major of higher professional universities should start with the education reform, strengthen the education mode in line with the times, improve the market demand, and provide professional fashion design talents for the society. Recently, China's garment industry needs a lot of experts. Many enterprises can't find talents suitable for many positions every year. Graduates are still facing employment problems. The imbalance of demand and supply is reflected in the talent training methods can not meet the requirements of the clothing industry. In the context of big data today, the innovation of fashion design should be the main task to develop, massive information can be easily presented in front of people. Therefore, we need to cater to the pace of fashion elements to meet the existing needs. In the source of fashion elements, the innovation of fashion design and the pursuit of fashion is no longer a lot of designers. Massive data resources use traditional fashion prediction methods to predict the results of its development trend, but the actual embodiment of life and culture. The trend of fashion design has been unable to achieve. In the context of the new era, the development of the Internet has led to information innovation, which is not limited to the structure of style and color, but in the development of clothing data.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {1244\u20131247}, numpages = {4}}
@inproceedings{10.1145/3495018.3495415,title = {Logistics Network Deployment Planning under the Background of Big Data Technology}, author = {Wang Xiaoxiao },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495415}, doi = {10.1145/3495018.3495415}, abstract = {In recent years, with the rapid development of e-commerce, the technology of online shopping platform is becoming more and more mature, followed by the emergence of a large number of e-commerce platforms, such as Taobao, JD, pinduoduo, etc. Most people live a fast-paced life and have little free time. Most people prefer online shopping and then distribute the purchased goods through third-party logistics. This method is simple, convenient and fast. In recent years, the rise of modern information technologies such as Internet of things, cloud computing and big data has had a significant impact on the development and innovation of logistics industry. By building e-commerce and other information platforms based on big data, manufacturers, distributors, consumers, logistics providers and financial service providers in the supply chain can be promoted to eliminate space barriers for communication and exchange. The application of big data technology can effectively integrate e-commerce platform and urban and rural logistics system, so that their data can be shared, which is conducive to reducing their dual uncertainty and improving operation efficiency. Based on the above analysis, this paper discusses the development and planning of logistics points under the background of big data from the perspective of big data. Based on the analysis of the research situation at home and abroad, this paper makes a theoretical and systematic research on the cost design of urban logistics system. Firstly, taking Hanzhong City as an example, this paper demonstrates the applicability of the prediction method, and then puts forward the principles and principles of urban logistics design. Secondly, combined with the characteristics of the city, this paper studies the prediction method and urban logistics demand prediction model, and the overall planning route of logistics city design. Finally, the layout and design method of urban logistics service function are put forward. Experiments show that the performance index E0 obtained by this method can reach 0.007%, which is in line with the expected results of the experiment.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {1432\u20131436}, numpages = {5}}
@inproceedings{10.1145/3495018.3495406,title = {Design of Intelligent Educational Administration System Based on Big Data Technology}, author = {Hao Jia },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495406}, doi = {10.1145/3495018.3495406}, abstract = {The main task of educational administration is educational administration. Nowadays, many schools have thousands or even thousands of teachers, leading to the increase of a large amount of data such as various systems and databases operated by the school, such as management of school conditions and management performance analysis. How to use these data to transform existing management data into useful knowledge, improve school management decision-making, and improve school management level and quality is an urgent problem for schools to solve. This paper studies the intelligent educational administration management system based on big data technology. After understanding the relevant theories of big data technology and intelligent educational administration management system based on literature data, the intelligent educational administration management system based on big data technology is designed, and the designed system is tested, and the test results show that when the number of users in the system is 40, the average response time of the system is 0.31s, and the average response time of student score query is 0.63s, which basically meets the design requirements of the system.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {1390\u20131394}, numpages = {5}}
@inproceedings{10.1145/3377812.3390811,title = {A practical, collaborative approach for modeling big data analytics application requirements}, author = {Khalajzadeh Hourieh , Simmons Andrew , Abdelrazek Mohamed , Grundy John , Hosking John , He Qiang , Ratnakanthan Prasanna , Zia Adil , Law Meng },year = {2020}, isbn = {9781450371223}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377812.3390811}, doi = {10.1145/3377812.3390811}, abstract = {Data analytics application development introduces many challenges including: new roles not in traditional software engineering practices - e.g. data scientists and data engineers; use of sophisticated machine learning (ML) model-based approaches; uncertainty inherent in the models; interfacing with models to fulfill software functionalities; deploying models at scale and rapid evolution of business goals and data sources. We describe our Big Data Analytics Modeling Languages (BiDaML) toolset to bring all stakeholders around one tool to specify, model and document big data applications. We report on our experience applying BiDaML to three real-world large-scale applications. Our approach successfully supports complex data analytics application development in industrial settings.}, location = {Seoul, South Korea}, series = {ICSE '20}, pages = {256\u2013257}, numpages = {2}}
@inproceedings{10.1145/2896387.2896423,title = {Efficient Security Framework for Sensitive Data Sharing and Privacy Preserving on Big-Data and Cloud Platforms}, author = {Pise Priya Dudhale , Uke Nilesh J. },year = {2016}, isbn = {9781450340632}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2896387.2896423}, doi = {10.1145/2896387.2896423}, abstract = {Now day's use of big data platforms is increasing for storing large amount of end user's data remotely on big data servers. Cloud computing storage was widely used for storing user's data, but cloud computing only providing the tasks of data storage but not supporting the important functionalities like computation and database operations. These operations are supported by big data systems and hence currently use of big data platform for storage in increases worldwide by enterprises. Sharing sensitive information and data resulted into big reduction in costs of enterprises for users to provide value added data and personalized services. As enterprises are sharing their important and sensitive information on big data platforms from different and many domains, it becomes necessary to provide the security and privacy in big data platform. Data security and privacy is gaining significant attentions of researchers. There are many security methods already proposed for cloud computing platform, now same methods slowly adopted on big data platform. For Big Data platforms, secure sharing of sensitive data is challenging research problem. In this paper, first we are introducing the different security and privacy preserving methods of cloud computing and big data platforms with their limitations, and then presenting the novel hybrid framework for secure sensitive data sharing and privacy preserving public auditing for shared data over big data systems including functionalities such as privacy preserving, public auditing, data security, storage, data access, deletion or secure data destruction using cloud services.}, location = {Cambridge, United Kingdom}, series = {ICC '16}, pages = {1\u20135}, numpages = {5}, keywords = {Privacy Preserving, Sensitive Data, Ring Search, Big Data, Data Security, Public Auditing, Proxy Re-encryption, Data Sharing}}
@inproceedings{10.1145/1966883.1966892,title = {Capturing data quality requirements for web applications by means of DQ_WebRE}, author = {Guerra-Garc\u00eda C\u00e9sar , Caballero Ismael , Piattini Mario },year = {2011}, isbn = {9781450306102}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1966883.1966892}, doi = {10.1145/1966883.1966892}, abstract = {The number and complexity of Web applications which are part of Business Intelligence (BI) applications had grown exponentially in recent years. The amount of data used in these applications has consequently also grown. Managing data with an acceptable level of quality is paramount to success in any organizational business process. In order to raise and maintain the adequate levels of Data Quality (DQ) it is indispensable for Web applications to be able to satisfy specific DQ requirements. In order to achieve this goal, DQ requirements should be captured and introduced into the development process together with the other software requirements needed in the applications. However, in the field of Web application development, and to the best of our knowledge, no proposals exist with regard to the way in which to manage specific DQ software requirements. This paper considers the MDA (Model Driven Architecture) approach and, principally, the benefits provided by Model Driven Web Engineering (MDWE) in order to put forward a proposal for two artifacts. These two artifacts are a metamodel and a UML profile for the management of Data Quality Software Requirements for Web Applications (DQ_WebRE).}, location = {Uppsala, Sweden}, series = {BEWEB '11}, pages = {28\u201335}, numpages = {8}, keywords = {model driven Web engineering, data quality, requirements engineering, Web engineering, requirements modeling}}
@inproceedings{10.1145/3358331.3358336,title = {Application of \"Artificial Intelligence and Big Data\" in Sports Rehabilitation for Chinese Judicial Administrative Drug Addicts}, author = {Jia Dong-Ming , Yuan Cun-Feng , Guo Song , Jiang Zu-Zhen , Xu Ding , Wang Da-An },year = {2019}, isbn = {9781450372022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3358331.3358336}, doi = {10.1145/3358331.3358336}, abstract = {Under the background of \"Wisdom Drug Rehabilitation\", we introduced \"Artificial Intelligence and Big Data\" into \"exercise rehabilitation\" work of drug addicts in judicial administrative system. It is a practical innovation of drug treatment in China. This article will elaborate this innovation of the construction and application of \"Exercise Rehabilitation\" intelligence platform system. This system will improve mental status, alleviate physical and psychological symptoms, ensure safety in places, lighten the burden of professional police officers, make rapid analysis, make accurate decisions and improve the integrity rate of the addict.}, location = {Dublin, Ireland}, series = {AIAM 2019}, pages = {1\u20135}, numpages = {5}, keywords = {rehabilitation training, big data, judicial administrative, Sports rehabilitation, artificial intelligence}}
@inproceedings{10.1145/3180457.3180463,title = {An Attribute-Based Access Control Model for Secure Big Data Processing in Hadoop Ecosystem}, author = {Gupta Maanak , Patwa Farhan , Sandhu Ravi },year = {2018}, isbn = {9781450356336}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3180457.3180463}, doi = {10.1145/3180457.3180463}, abstract = {Apache Hadoop is a predominant software framework for distributed compute and storage with capability to handle huge amounts of data, usually referred to as Big Data. This data collected from different enterprises and government agencies often includes private and sensitive information, which needs to be secured from unauthorized access. This paper proposes extensions to the current authorization capabilities offered by Hadoop core and other ecosystem projects, specifically Apache Ranger and Apache Sentry. We present a fine-grained attribute-based access control model, referred as HeABAC, catering to the security and privacy needs of multi-tenant Hadoop ecosystem. The paper reviews the current multi-layered access control model used primarily in Hadoop core (2.x), Apache Ranger (version 0.6) and Sentry (version 1.7.0), as well as a previously proposed RBAC extension (OT-RBAC). It then presents a formal attribute-based access control model for Hadoop ecosystem, including the novel concept of cross Hadoop services trust. It further highlights different trust scenarios, presents an implementation approach for HeABAC using Apache Ranger and, discusses the administration requirements of HeABAC operational model. Some comprehensive, real-world use cases are also discussed to reflect the application and enforcement of the proposed HeABAC model in Hadoop ecosystem.}, location = {Tempe, AZ, USA}, series = {ABAC'18}, pages = {13\u201324}, numpages = {12}, keywords = {attributes based, hadoop ecosystem, big data, authorization, role based, access control, data lake, trust}}
@inproceedings{10.1145/3210752,title = {Framework for implementing a big data ecosystem in organizations}, author = {Orenga-Rogl\u00e1 Sergio , Chalmeta Ricardo },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3210752}, doi = {10.1145/3210752}, abstract = {Featuring the various dimensions of data management, it guides organizations through implementation fundamentals.}, pages = {58\u201365}, numpages = {8}}
@inproceedings{10.1145/3287324.3287494,title = {A Module-based Approach to Teaching Big data and Cloud Computing Topics at CS Undergraduate Level}, author = {Deb Debzani , Fuad Muztaba , Irwin Keith },year = {2019}, isbn = {9781450358903}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3287324.3287494}, doi = {10.1145/3287324.3287494}, abstract = {Big data and cloud computing collectively offer a paradigm shift in the way businesses are now acquiring, using and managing information technology. This creates the need for every CS student to be equipped with foundational knowledge in this collective paradigm and to possess some hands-on experience in deploying and managing big data applications in the cloud. We argue that, for substantial coverage of big data and cloud computing concepts and skills, the relevant topics need to be integrated into multiple core courses across the undergraduate CS curriculum rather than creating additional standalone core or elective courses and performing a major overhaul of the curriculum. Our approach to including these topics is to develop autonomous learning modules for specific core courses in which their coverage might find an appropriate context. In this paper, three such modules are discussed and our classroom experiences during these interventions are documented. So far, we have achieved reasonable success in attaining student learning outcomes, enhanced engagement, and interests. Our objective is to share our experience with the academics who aim at incorporating similar pedagogy and to receive feedback about our approach.}, location = {Minneapolis, MN, USA}, series = {SIGCSE '19}, pages = {2\u20138}, numpages = {7}, keywords = {mapreduce, cloud computing, big data, apache spark, curriculum}}
@inproceedings{10.1145/3260511,title = {Session details: Session 3 - Big Data, Big Resources}, author = {Newton Glen },year = {2015}, isbn = {9781450335942}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3260511}, doi = {10.1145/3260511}, location = {Knoxville, Tennessee, USA}, series = {JCDL '15}, pages = {}}
@inproceedings{10.1145/3229049,title = {System and Architecture Level Characterization of Big Data Applications on Big and Little Core Server Architectures}, author = {Malik Maria , Rafatirad Setareh , Homayoun Houman },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3229049}, doi = {10.1145/3229049}, abstract = {The rapid growth in data yields challenges to process data efficiently using current high-performance server architectures such as big Xeon cores. Furthermore, physical design constraints, such as power and density, have become the dominant limiting factor for scaling out servers.\u00a0Low-power embedded cores in servers such as little Atom\u00a0have emerged as a promising solution to enhance energy-efficiency\u00a0to address these challenges.\u00a0Therefore, the question of whether to\u00a0process the big data\u00a0applications on big Xeon- or Little Atom-based servers becomes important.\u00a0In this work, through methodical investigation of power and performance measurements, and comprehensive application-level, system-level, and micro-architectural level analysis, we characterize dominant big data applications on big Xeon- and little Atom-based server architectures. The characterization results across a wide range of real-world big data applications, and various software stacks demonstrate how the choice of big- versus little-core-based server for energy-efficiency is significantly influenced by the size of data, performance constraints, and presence of accelerator.\u00a0In addition, we analyze processor resource utilization of this important class of applications,\u00a0such as\u00a0memory footprints, CPU\u00a0utilization, and disk bandwidth,\u00a0to understand their run-time behavior.\u00a0Furthermore, we perform micro-architecture-level analysis to highlight where improvement is needed in big- and little-core microarchitectures to address their performance bottlenecks.}, pages = {1\u201332}, numpages = {32}, keywords = {high-performance server, Performance, characterization, big data, accelerator, power, low-power server}}
@inproceedings{10.14778/3407790.3407848,title = {Approximate partition selection for big-data workloads using summary statistics}, author = {Rong Kexin , Lu Yao , Bailis Peter , Kandula Srikanth , Levis Philip },year = {2020}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3407790.3407848}, doi = {10.14778/3407790.3407848}, abstract = {Many big-data clusters store data in large partitions that support access at a coarse, partition-level granularity. As a result, approximate query processing via row-level sampling is inefficient, often requiring reads of many partitions. In this work, we seek to answer queries quickly and approximately by reading a subset of the data partitions and combining partial answers in a weighted manner without modifying the data layout. We illustrate how to efficiently perform this query processing using a set of pre-computed summary statistics, which inform the choice of partitions and weights. We develop novel means of using the statistics to assess the similarity and importance of partitions. Our experiments on several datasets and data layouts demonstrate that to achieve the same relative error compared to uniform partition sampling, our techniques offer from 2.7x to 70x reduction in the number of partitions read, and the statistics stored per partition require fewer than 100KB.}, pages = {2606\u20132619}, numpages = {14}}
@inproceedings{10.1145/3077839.3081670,title = {Big data analysis for an electric vehicle charging infrastructure using open data and software}, author = {Lee Junghoon , Park Gyung-Leen , Han Yeonju , Yoo Seunghee },year = {2017}, isbn = {9781450350365}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3077839.3081670}, doi = {10.1145/3077839.3081670}, abstract = {This paper describes a big data analysis strategy for electric vehicle charging infrastructure, mainly built upon open data sets and open software components. The data acquisition module periodically retrieves the real-time status information of each charger from the public data portal, while the downloaded XML files are parsed to extract fields of interest. At this stage, we present the distribution of charging facilities in Jeju City based on our own map viewer implementation, the city-wide dynamics of the number of chargers in operation based on MySQL queries, and the visualization of regional occupancy rates based on the R GISTools library. After combining a variety of statistical and machine learning techniques to understand the demand pattern of electric vehicle charging, we will integrate renewable energy to charging-intensive power grids as much as possible.}, location = {Shatin, Hong Kong}, series = {e-Energy '17}, pages = {252\u2013253}, numpages = {2}, keywords = {open software, data acquisition, occupancy rate, Charging infrastructure, big data analysis}}
@inproceedings{10.5555/2399776.2399802,title = {Crunching big data with Hadoop and BigInsights in the cloud}, author = {Petrazickis Leons , Butuc Marius , Steinfeld Bradley },year = {2012}, publisher = {IBM Corp.}, address = {USA}, abstract = {There is an ongoing information explosion in every field of human endeavour. Enormous, unstructured, immensely valuable data sets are being accumulated. Every device logs numbers, audio, video and text; data is aggregated and stored somewhere.Unfortunately, traditional techniques cannot analyze these data sets. There's too much data to query -- volume! And it's all different -- variety! And it's arriving too fast -- velocity! Finance firms need to analyze transactions to detect fraud and model risk; Energy firms need to analyze old rig performance and wind speeds; IT needs to analyze logs of every type; Service providers need to analyze the prices of various services worldwide; Healthcare providers need to analyze patient data and measurements.New techniques are needed to deal with this Big Data. Fortunately, cloud computing is allowing the emergence of technologies that rely on clusters of commodity hardware to crunch data. Google is one example of a company that had to face and solve a Big Data problem before it could revolutionize internet search and consign countless other early search engines to the dust heap of history. The page-rank algorithm that it uses to rank results is based on something called Map-Reduce. In the Map phase, the data set (all of the internet) is split into microscopic chunks, which are transformed from unstructured data (information about a web page) to useful data (the value of web page). In the Reduce phase, the transformed microscopic chunks are reassembled back together into a Google results page.Because the chunks are microscopic, the mapping could run on many off-the-shelf computers rather than one big server. As a result, Google is able to use cheap commodity hardware and put competitors that relied on expensive servers out of business. The hardware side of this approach created Cloud Computing, which is a way of organizing vast amounts of cheap hardware on demand. The software side of this created a lot of useful data analytics applications.Apache Hadoop is one useful data analytics application that's native to the cloud. Hadoop is an open source project led by Yahoo. It makes it straightforward to apply the idea of Map-Reduce to any data set. Many vendors such as Cloudera. HortonWorks, and IBM have their own distribution of Hadoop. The Hadoop ecosystem includes many other open source technologies. The Java libraries are enhanced by the Pig high level language, the HBase database, the Hive data warehouse system, and the Flume log aggregation service. Each of these makes Hadoop more powerful at dealing with larger volumes of data, greater varieties of data, and quicker velocities of data.IBM InfoSphere BigInsights is a distribution of Hadoop. It integrates an IBM-created open source query language called JAQL (Jackal) with the usual components such as Hive, HBase, and Pig. JAQL allows the user to query through large sets of data in JSON (JavaScript Object Notation) form, which is the native data format of Hadoop. The Basic edition of BigInsights is available for download at no charge. It can also be easily deployed on Amazon Elastic Compute Cloud or IBM SmartCloud Enterprise.}, location = {Toronto, Ontario, Canada}, series = {CASCON '12}, pages = {241\u2013242}, numpages = {2}}
@inproceedings{10.1145/2485922.2485944,title = {Navigating big data with high-throughput, energy-efficient data partitioning}, author = {Wu Lisa , Barker Raymond J. , Kim Martha A. , Ross Kenneth A. },year = {2013}, isbn = {9781450320795}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2485922.2485944}, doi = {10.1145/2485922.2485944}, abstract = {The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.}, location = {Tel-Aviv, Israel}, series = {ISCA '13}, pages = {249\u2013260}, numpages = {12}, keywords = {accelerator, streaming data, microarchitecture, specialized functional unit, data partitioning}}
@inproceedings{10.1145/2508148.2485944,title = {Navigating big data with high-throughput, energy-efficient data partitioning}, author = {Wu Lisa , Barker Raymond J. , Kim Martha A. , Ross Kenneth A. },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2508148.2485944}, doi = {10.1145/2508148.2485944}, abstract = {The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.}, pages = {249\u2013260}, numpages = {12}, keywords = {specialized functional unit, streaming data, microarchitecture, data partitioning, accelerator}}
@inproceedings{10.1145/3057148.3057149,title = {Describing Data Processing Pipelines in Scientific Publications for Big Data Injection}, author = {Mesbah Sepideh , Bozzon Alessandro , Lofi Christoph , Houben Geert-Jan },year = {2017}, isbn = {9781450352406}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3057148.3057149}, doi = {10.1145/3057148.3057149}, abstract = {The rise of Big Data analytics has been a disruptive game changer for many application domains, allowing the integration into domain-specific applications and systems of insights and knowledge extracted from external big data sets. The effective \"injection\" of external Big Data demands an understanding of the properties of available data sets, and expertise on the available and most suitable methods for data collection, enrichment and analysis. A prominent knowledge source is scientific literature, where data processing pipelines are described, discussed, and evaluated. Such knowledge is however not readily accessible, due to its distributed and unstructured nature. In this paper, we propose a novel ontology aimed at modeling properties of data processing pipelines, and their related artifacts, as described in scientific publications. The ontology is the result of a requirement analysis that involved experts from both academia and industry. We showcase the effectiveness of our ontology by manually applying it to a collection of publications describing data processing methods.}, location = {Cambridge, United Kingdom}, series = {SWM '17}, pages = {1\u20138}, numpages = {8}}
@inproceedings{10.1145/2815782.2815791,title = {On the prioritization of data quality challenges in e-health systems in South Africa}, author = {Botha Marna , Botha Adele , Herselman Marlien },year = {2015}, isbn = {9781450336833}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2815782.2815791}, doi = {10.1145/2815782.2815791}, abstract = {Data quality is one of many challenges experienced in e-health. The collection of data with substandard data quality leads to inappropriate information for health and management purposes. Given evidence of challenges with regards to data quality in electronic health systems, the purpose of the study is to prioritise data quality challenges as experienced by data users of electronic healthcare systems in South Africa. The study adopted a sequential QUAL-quan mixed method research design towards the realisation of the research purpose. After carrying out a literature review on the background of e-health and the current status of research on data quality challenges, a qualitative study was conducted to verify and extend the theoretical list of data quality challenges. A quantitative study followed to prioritise data quality challenges as experienced by data users of electronic healthcare systems. Data users of electronic healthcare systems in South Africa served as the unit of analysis in the study. The data collection process included the conducting of interviews with four data quality experts to verify and extend the theoretical list of data quality challenges. This was followed by a survey targeting 100 data users of electronic healthcare systems in South Africa for which 82 responses were received.From the results of the study, a prioritised list of data quality challenges has been developed which can be applied to assist data users of electronic health care systems in South Africa to improve the quality of data in electronic healthcare systems. The most important data challenge is training. The prioritised list of data quality challenges allowed for evidence-based recommendations which can assist health institutions in South Africa to ensure future data quality.}, location = {Stellenbosch, South Africa}, series = {SAICSIT '15}, pages = {1\u201310}, numpages = {10}, keywords = {IT, data quality challenges, E-health, data quality}}
@inproceedings{10.1145/3250292,title = {Session details: Workshop: evolutionary computation for big data and big learning}, author = {Bacardit Jaume , Arnaldo Ignacio , Veeramachaneni Kalyan , O'Reilly Una-May },year = {2014}, isbn = {9781450328814}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3250292}, doi = {10.1145/3250292}, abstract = {It is our great pleasure to welcome you to the first Workshop on Evolutionary Computation for Big Data and Big Learning ECBDL'14We live in a time of unprecedented access to cheap and vast amounts of computational resources, which is producing a big leap forward in the fields of machine learning and data mining. We can tackle datasets of a scale (be it instances, attributes, classes, etc.) that was unimaginable some years ago, in what is well known as big data. On the other hand we can also use all these vast computational resources with the aim of understanding better our machine learning methods, by performing large scale evaluations, parameter sweeps, etc. We refer to the overall use massive on-demand computation (cloud or GPUs) for machine learning as Big Learning. Evolutionary Machine Learning techniques are perfect candidates for big learning tasks due to their flexibility in knowledge representations, learning paradigms and their innate parallelism.In this workshop we have accepted two papers representing very different scenarios of big data and big learning. The first paper, \"Evolving Relational Hierarchical Classification Rules for Predicting Gene Ontology-Based Protein Functions\" by Ricardo Cerri, Rodrigo C. Barros, Alex A. Freitas and Andr\u00e9 C. P. L. F. Carvalho, focuses on an extremely complex and heterogeneous classification task, where instances have multiple classes organized hierarchically. The method is tested on real-world biological data, and explores the use of new rule representations to enhance knowledge discovery.The second paper, \"On the Application of GP to Streaming Data Classification Tasks with Label Budgets\" by Ali Vahdat, Aaron Atwater, Andrew R. McIntyre, Malcolm I. Heywood, focuses on a very important topic within big data/learning, streaming data classification, in the particular scenario where access to the real annotation (classes) of data is costly, and budgets need to be specified.}, location = {Vancouver, BC, Canada}, series = {GECCO Comp '14}, pages = {}}
@inproceedings{10.1145/3530050.3532928,title = {Enhancing data quality and process optimization for smart manufacturing lines in industry 4.0 scenarios}, author = {Paasche Simon , Groppe Sven },year = {2022}, isbn = {9781450393461}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3530050.3532928}, doi = {10.1145/3530050.3532928}, abstract = {An essential component of today's industry is data, which is generated during manufacturing. The goal of industry 4.0 is efficient collection, processing and analysis of this data. In our work, we address these three tasks and present an extensible system to solve them. To the best of our knowledge, the combination of a consistency checker (CC) for data preparation and a digital twin (DT) for analysis activities represents a novel approach. Consistency checking in combination with a DT leads to increased data quality, which in turn has a positive effect on analyses, like reducing errors to decrease costs, identifying relevant parameters to increase the productivity, and determining the bottleneck of a manufacturing line for enhanced production planning.}, location = {Philadelphia, Pennsylvania}, series = {BiDEDE '22}, pages = {1\u20137}, numpages = {7}, keywords = {digital twin, industry 4.0, consistency checking}}
@inproceedings{10.5555/2093889.2093935,title = {Crunching big data in the cloud with Hadoop and BigInsights}, author = {Petrazickis Leons , Steinfeld Bradley },year = {2011}, publisher = {IBM Corp.}, address = {USA}, abstract = {There is an ongoing information explosion in every field of human endeavour. Enormous, unstructured, immensely valuable data sets are being accumulated. Every device logs numbers and audio and video and text, and then this data is aggregated and stored somewhere. Unfortunately, traditional techniques cannot analyze these data sets. There's too much data to query -- volume! And it's all different -- variety! And it's arriving too fast -- velocity!Finance firms need to analyze transactions to detect fraud and model risk. Energy firms need to analyze old rig performance and wind speeds. IT needs to analyze logs of every type. Service providers need to analyze the prices of various services worldwide. Healthcare providers need to analyze patient data and measurements.New techniques are needed to deal with this Big Data. Fortunately, cloud computing is allowing the emergence of technologies that rely on clusters of commodity hardware to crunch data.Google is one example of a company that had to face and solve a Big Data problem before it could revolutionize internet search and consign countless other early search engines to the dust heap of history. Its page-rank algorithm that it uses to rank results is based on something called Map-Reduce. In the Map phase, the data set (all of the internet) is split into itsy-bitsy chunks, which are transformed from unstructured data (information about a web page) to useful data (the value of web page). In the Reduce phase, the transformed itsy-bitsy chunks are reassembled back together into a Google results page.Because the chunks are itsy-bitsy, the mapping could run on many off-the-shelf computers rather than one big server. This allowed Google to use cheap commodity hardware and put competitors that relied on expensive servers out of business. The hardware side of this approach created Cloud Computing, which is a way of organizing vast amounts of cheap hardware on demand. The software side of this created a lot of useful data analytics applications.Apache Hadoop is one useful data analytics application that's native to the cloud. Hadoop is an open source project led by Yahoo. It makes it straightforward to apply the idea of Map-Reduce to any data set. Many vendors such as Cloudera, HortonWorks, and IBM have their own distribution of Hadoop.The Hadoop ecosystem includes many other open source technologies. The Java libraries are enhanced by the Pig high level language, the HBase database, the Hive data warehouse system, and the Flume log aggregation service. Each of these makes Hadoop more powerful at dealing with larger volumes of data, greater varieties of data, and quicker velocities of data.IBM InfoSphere BigInsights is a distribution of Hadoop. It integrates an IBM-created open source query language called JAQL (Jackal) with the usual components such as Hive, HBase, and Pig. JAQL allows the user to query through large sets of data in JSON (JavaScript Object Notation) form, which is the native data format of Hadoop.The Basic edition of BigInsights is available for download at no charge. It can also be easily deployed on Amazon Elastic Compute Cloud or IBM SmartCloud Enterprise.}, location = {Toronto, Ontario, Canada}, series = {CASCON '11}, pages = {334\u2013335}, numpages = {2}}
@inproceedings{10.1145/2742854.2747282,title = {Big data analytics for climate change and biodiversity in the EUBrazilCC federated cloud infrastructure}, author = {Fiore Sandro , Mancini Marco , Elia Donatello , Nassisi Paola , Brasileiro Francisco Vilar , Blanquer Ignacio },year = {2015}, isbn = {9781450333580}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2742854.2747282}, doi = {10.1145/2742854.2747282}, abstract = {The analysis of large volumes of data is key for knowledge discovery in several scientific domains such as climate, astrophysics, life sciences among others. It requires a large set of computational and storage resources, as well as flexible and efficient software solutions able to dynamically exploit the available infrastructure and address issues related to data volume, distribution, velocity and heterogeneity. This paper presents a data-driven and cloud-based use case implemented in the context of the EUBrazilCC project for the analysis of climate change and biodiversity data. The use case architecture and main components, as well as a Platform as a Service (PaaS) framework for big data analytics named PDAS, together with its elastic deployment in the EUBrazilCC federated cloud infrastructure are presented and discussed in detail.}, location = {Ischia, Italy}, series = {CF '15}, pages = {1\u20138}, numpages = {8}, keywords = {cloud computing, scientific data management, big data analytics, federated clouds}}
@inproceedings{10.1145/2737909.2737912,title = {Towards an Understanding of Facets and Exemplars of Big Data Applications}, author = {Fox Geoffrey C. , Jha Shantenu , Qiu Judy , Luckow Andre },year = {2014}, isbn = {9781450330312}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2737909.2737912}, doi = {10.1145/2737909.2737912}, abstract = {We study many Big Data applications from a variety of research and commercial areas and suggest a set of characteristic features and possible kernel benchmarks that stress those features for data analytics. We draw conclusions for the hardware and software architectures that are suggested by this analysis.}, location = {Annapolis, MD, USA}, series = {Beowulf '14}, pages = {7\u201316}, numpages = {10}}
@inproceedings{10.1145/3219104.3229279,title = {Combining HPC and Big Data Infrastructures in Large-Scale Post-Processing of Simulation Data: A Case Study}, author = {Li Yu , Zhang Xiaohong , Srinath Ashwin , Getman Rachel B. , Ngo Linh B. },year = {2018}, isbn = {9781450364461}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3219104.3229279}, doi = {10.1145/3219104.3229279}, abstract = {Advances in scientific software and computing infrastructure have enabled researchers across disciplines to simulate and model highly complex systems. At the same time, these increases in simulation duration and scale have led to significant growths in the sizes of output data, which can be as much as hundreds of gigabytes or more. While there exist solutions to assist with most standard post-simulation analytics, researchers must develop their own code to support customized analytical tasks. Given the nature of these output data, most naive in-house sequential codes end up being inefficient, and in most cases, time-consuming. In this paper, we propose a solution to this issue by transparently combining the strengths of a high-performance computing cluster and a big data infrastructure to support an end-to-end scientific workflow. More specifically, we present a case study around the design of a research computing environment at Clemson University where these two computing systems are integrated and accessible from one another. This environment allows simulation data to be automatically transferred across systems and complex analytical tasks on these data to be developed using the Hadoop/Spark frameworks. Results show that a hybrid workflow for molecular dynamics simulation can provide significant performance improvements over a traditional workflow. Furthermore, code complexity of Hadoop/Spark solutions is shown to be less than that of a traditional solution.}, location = {Pittsburgh, PA, USA}, series = {PEARC '18}, pages = {1\u20137}, numpages = {7}, keywords = {Apache Hadoop, Apache Spark, HPC, Molecular Dynamics Simulation, Big Data}}
@inproceedings{10.5555/3382225.3382363,title = {A game-theoretic lexical link analysis for discovering high-value information from big data}, author = {Zhao Ying , Zhou Charles C. },year = {2018}, isbn = {9781538660515}, publisher = {IEEE Press}, abstract = {We demonstrate a machine learning and artificial intelligence method, i.e., lexical link analysis (LLA) to discover high-value information from big data. In this paper, high-value information refers to the information that has the potential to grow its value over time. LLA is a unsupervised learning method that does not require manually labeled training data. New value metrics are defined based on a game-theoretic framework for LLA. In this paper, we show the value metrics generated from LLA in a use case of analyzing business news. We show the results from LLA are validated and correlated with the ground truth. We show that by using game theory, the high-value information selected by LLA reaches a Nash equilibrium by superpositioning popular and anomalous information, and at the same time generates high social welfare, therefore, contains higher intrinsic value.}, location = {Barcelona, Spain}, series = {ASONAM '18}, pages = {621\u2013625}, numpages = {5}, keywords = {pareto superior, pareto efficient, lexical link analysis, nash equilibrium, game theory, social welfare, big data, unsupervised learning, high-value}}
@inproceedings{10.1145/2383276.2383302,title = {Data oriented challenges of service architectures a data quality perspective}, author = {Petkov Plamen , Helfert Markus },year = {2012}, isbn = {9781450311939}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2383276.2383302}, doi = {10.1145/2383276.2383302}, abstract = {Service oriented composition is a prospective approach, which enables flexible and loose composition of applications. Data, on the other hand, is an integral part of service. Despite the huge number of studies that have been done on service oriented environments, very little has been investigated about the data quality. In this paper we examine some problems in service-oriented architecture and in particular problem concerned with data quality. In addition, issues associated with data incorporation and how data consistency is realized through data modelling concepts will be investigated.}, location = {Ruse, Bulgaria}, series = {CompSysTech '12}, pages = {163\u2013170}, numpages = {8}, keywords = {SoA concepts, service oriented architectures, data quality issues data inconsistency and data incorporation}}
@inproceedings{10.1109/CCGRID.2017.147,title = {Cloud Resource Scaling for Big Data Streaming Applications Using A Layered Multi-dimensional Hidden Markov Model}, author = {Runsewe Olubisi , Samaan Nancy },year = {2017}, isbn = {9781509066100}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGRID.2017.147}, doi = {10.1109/CCGRID.2017.147}, abstract = {Recent advancements in technology have led to a deluge of data that require real-time analysis with strict latency constraints. A major challenge, however, is determining the amount of resources required by big data stream processing applications in response to heterogeneous data sources, streaming events, unpredictable data volume and velocity changes. Over-provisioning of resources for peak loads can be wasteful while under-provisioning can have a huge impact on the performance of the streaming applications. The majority of research efforts on resource scaling in the cloud are investigated from the cloud provider's perspective, they focus on web applications and do not consider multiple resource bottlenecks. We aim at analyzing the resource scaling problem from a big data streaming application provider's point of view such that efficient scaling decisions can be made for future resource utilization. This paper proposes a Layered Multi-dimensional Hidden Markov Model (LMD-HMM) for facilitating the management of resource auto-scaling for big data streaming applications in the cloud. Our detailed experimental evaluation shows that LMD-HMM performs best with an accuracy of 98%, outperforming the single-layer hidden markov model.}, location = {Madrid, Spain}, series = {CCGrid '17}, pages = {848\u2013857}, numpages = {10}, keywords = {Stream Processing, Resource Prediction, Big Data, Cloud Computing, Resource Scaling, Layered Hidden Markov Model}}
@inproceedings{10.1145/2949550,title = {Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale},year = {2016}, isbn = {9781450347556}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Miami, USA}}
@inproceedings{10.1145/1449814.1449820,title = {NOAO imaging meta data quality improvement: a case study of the evolution of a service oriented system}, author = {Lowry Sonya J. , Warner Phillip B. , Deaubl Evan },year = {2008}, isbn = {9781605582207}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1449814.1449820}, doi = {10.1145/1449814.1449820}, abstract = {Due to the natures of legacy astronomical imaging meta data acquisition and storage technologies and techniques at the National Optical Astronomy Observatory, the challenge of methodically improving the quality of such information has been insurmountable until now. The diversity of the sources and lack of cohesive effort along with technologies that enable silo style efforts have contributed to the current, disorganized state of the collection of astronomical imaging information. By meeting these issues with a solution that combines policy and technology support for implementing master data management, use of transformations that enable continued improvements and a history of changes made, and a modular architecture based upon services that are federated over a flexible, robust communications architecture, the NOAO Archive System can assure improvements in the quality of the imaging meta data now and into the future.}, location = {Nashville, TN, USA}, series = {OOPSLA Companion '08}, pages = {675\u2013684}, numpages = {10}, keywords = {virtual observatory, data quality, transformation, astronomy, soa, service-oriented architecture, service platform, meta data, integration}}
@inproceedings{10.1145/3429889.3429920,title = {Limitation of on Big Data or Nature Language Processing based algorithm for Clinical Decision Artificial Intelligence}, author = {Wu Ning , Cao Yanping , Chen Zhuo , Zhu Yifan },year = {2020}, isbn = {9781450388603}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3429889.3429920}, doi = {10.1145/3429889.3429920}, abstract = {Intelligent clinical decision is an important utility of artificial intelligence. At present, most of its algorithm is based on big data or nature language processing. The limitation of such algorithm is discussed and summarized. That clinical decision artificial intelligence should meet the requirements of clinical medicine and artificial intelligence technique is proposed.}, location = {Beijing, China}, series = {ISAIMS 2020}, pages = {161\u2013165}, numpages = {5}, keywords = {Big data, Algorithm t, Nature language processing, Artificial intelligence, Clinical decision}}
@inproceedings{10.1145/3376897.3379162,title = {Data Quality-Informed Multiple Occupant Localization using Floor Vibration Sensing}, author = {Shi Laixi , Zhang Yue , Pan Shijia , Chi Yuejie },year = {2020}, isbn = {9781450371162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3376897.3379162}, doi = {10.1145/3376897.3379162}, abstract = {Floor vibration-based sensing provides an alternative approach for multiple occupant localization, enabling various smart building applications such as elderly care. Prior work mainly focuses on detecting onsets of individual footstep from overlapping signals for localization. However, the error rate is higher than that of single footsteps. In this work, we present a data quality-informed time-sequence approach for accurate multi-people localization. The intuition is that when signals overlap, the overlapping part of the signal has a lower SNR, which can be quantified and used as estimation confidence. We conducted real-world experiments to validate our data quality-informed approach for location estimation.}, location = {Austin, TX, USA}, series = {HotMobile '20}, pages = {98}, numpages = {1}, keywords = {multiple pedestrian localization, tdoa, indoor localization, ambient vibration sensing, signal overlapping, data quality quantification}}
@inproceedings{10.1145/3368640.3368658,title = {Utilizing the buckshot algorithm for efficient big data clustering in the MapReduce model}, author = {Gerakidis Sergios , Mamalis Basilis },year = {2019}, isbn = {9781450372923}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3368640.3368658}, doi = {10.1145/3368640.3368658}, abstract = {Clustering is an efficient data mining as well as machine-learning method when we need to get an insight of the objects of a dataset that could be grouped together. The K-Means algorithm and the Hierarchical Agglomerative Clustering (HAC) algorithm are two of the most known and commonly used methods of clustering; the former due to its low time cost and the latter due to its accuracy. However, even the use of K-Means in document clustering over large-scale collections can lead to unpredictable time costs. In this paper, towards the direction of the efficient handling of big text data, we present a hybrid clustering approach based on a customized version of the Buckshot algorithm, which first applies a hierarchical clustering procedure on a sample of the input dataset and then uses the results as the initial centers for a K-Means based assignment of the remaining documents, with very few iterations. We also give a highly efficient adaptation of the proposed Buckshot-based approach in the MapReduce model which is then experimentally tested using Apache Hadoop over a real cluster environment. As it comes out of the experiments, it leads to acceptable clustering quality as well as to significant execution time improvements. Preliminary results drawn from relevant experiments using the Spark framework are also presented.}, location = {Nicosia, Cyprus}, series = {PCI '19}, pages = {112\u2013117}, numpages = {6}, keywords = {big data, spark, MapReduce, hierarchical agglomerative clustering, buckshot algorithm, K-Means}}
@inproceedings{10.1145/3156346.3156694,title = {Revealing deep proteome diversity with community-scale proteomics big data}, author = {Bandeira Nuno },year = {2017}, isbn = {9781450353502}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3156346.3156694}, doi = {10.1145/3156346.3156694}, abstract = {Translating the growing volumes of proteomics mass spectrometry data into reusable evidence of the occurrence and provenance of proteomics events requires the development of novel algorithms and community-scale computational workflows. MassIVE (http://massive.ucsd.edu) proposes to address this challenge in three stages.First, systematic annotation of human proteomics big data requires automated reanalysis of all public data using open source workflows with detailed records of search parameters and of individual Peptide Spectrum Matches (PSMs). As such, our large-scale reanalysis of tens of terabytes of human data has now increased the total number of proper public PSMs by over 10-fold to over 320 million PSMs whose coverage includes over 95Second, proper synthesis of community-scale search results into a reusable knowledge base (KB) requires scalable workflows imposing strict statistical controls. Our MassIVE-KB spectral library has thus properly assembled 2+ million precursors from over 1.5 million peptides covering over 6.2 million amino acids in the human proteome, all of which at least double the numbers covered by the popular NIST spectral libraries. Moreover, MassIVE-KB detects 723 novel proteins (PE 2-5) for a total of 16,852 proteins observed in non-synthetic LCMS runs and 19,610 total proteins when including the recent ProteomeTools data.Third, we show how advanced identification algorithms combine with public data to reveal dozens of unexpected putative modifications supported by multiple highly-correlated spectra. These show that protein regions can be observed in over 100 different variants with various combinations of post-translational modifications and cleavage events, thus suggesting that current coverage of proteome diversity (at 1.3 variants per protein region) is far below what is observable in experimental data.}, location = {Nha Trang City, Viet Nam}, series = {CSBio '17}, pages = {2}, numpages = {1}}
@inproceedings{10.1145/3482632.3482684,title = {Construction and Application of English Education Platform Based on Big Data}, author = {Cai Lei },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3482684}, doi = {10.1145/3482632.3482684}, abstract = {The English education platform overcomes the limitations of traditional education on both sides of English teaching in terms of time, space and instant interaction, making learners more flexible and free in terms of learning content, learning form, learning time, and learning location. Fully mobilize the enthusiasm of learners. The research purpose of this article is to build and apply English education platform based on big data, mainly introduces the application research of data mining technology in assisting teachers to make decisions and analyze learner information. In view of the problem of the scattered information of the English education platform and it is difficult to unify, firstly, it is proposed to use the statistical analysis module to comprehensively grasp the learner's activities, which is conducive to the teacher's overall grasp of the learner's learning situation, and also helps the teacher understand the learning situation of this course. The application process of association rules in the analysis of learners' basic information is described in detail, and some potential associations hidden in students' basic information and student activities are discovered through association rules mining, so as to facilitate teachers to grasp the learning situation of learners and make appropriate Formative evaluation. On the basis of the system design, the main technology of the development platform of the English education platform based on big data is analyzed and selected, and the system implementation of the big data infrastructure module is introduced with emphasis, and the test results are analyzed. The experimental results show that under the condition of 800G data volume, the current system data loss rate is within 0.3%, which can ensure the accuracy of data analysis and meet the stability requirements.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {250\u2013253}, numpages = {4}}
@inproceedings{10.1145/3314527.3314537,title = {Analytical Visualization of Higher Education Institutions' Big Data for Decision Making}, author = {Cabanban-Casem Christianne Lynnette },year = {2019}, isbn = {9781450366212}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3314527.3314537}, doi = {10.1145/3314527.3314537}, abstract = {Education is an important element towards learning and human development, thus, it is the key towards identifying competencies and better productivity for the workforce. As part of the Commission on Higher Education's (CHED) thrust for improving efficiency and effectiveness by simplifying the collection process for all the stakeholders, the developed system will drastically improve the availability of data for making informed decisions and efficient generation of reports.This research outlines opportunities and challenges associated with the implementation and governance of Big Data in higher education through development and implementation of data analytics tool.}, location = {Jeju Island, Republic of Korea}, series = {APIT 2019}, pages = {61\u201364}, numpages = {4}, keywords = {Knowledge Management, Data Science, Higher Education Data}}
@inproceedings{10.1145/3430665.3456325,title = {Deploying a Strategy to Unlock Big Data Research and Teaching Activities in the West Balkan Region}, author = {Graux Damien , Janev Valentina , Jabeen Hajira , Sallinger Emanuel },year = {2021}, isbn = {9781450382144}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3430665.3456325}, doi = {10.1145/3430665.3456325}, abstract = {Big Data Analytics is a crucial component of the Big Data paradigm and deals with the extraction of knowledge from the enormous amount of data. As the number of Big Data related methods, tools, frameworks, and solutions is growing, there is a need to systematize the knowledge about the domain. Moreover, as this domain is rapidly involving, it is difficult to keep up to date with its latest and most efficient technologies; and this is thereby especially challenging for countries which have so far suffered from a lack of infrastructure in Big Data.In this article, we report on the deployment of a strategy we design, fostering the knowledge and awareness around the challenges of Big Data analytics in the West Balkan region. In particular, we describe how we joined forces across multiple European institutions in order to design bespoke actions (from, e.g. classes, to student exchanges) to be applied in Serbia and the West Balkan region over several years.}, location = {Virtual Event, Germany}, series = {ITiCSE '21}, pages = {491\u2013497}, numpages = {7}, keywords = {teaching big data analytics, European collaboration, west Balkan region, summer school}}
@inproceedings{10.1145/3510249.3510256,title = {E-commerce Industrial Development Analysis from the Perspective of Big Data}, author = {ZHOU JIANG },year = {2021}, isbn = {9781450387392}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510249.3510256}, doi = {10.1145/3510249.3510256}, abstract = {This paper analyzes the industrial operation mode of e-commerce, summarizes the industrial development characteristics under the background of big data, and realizes the issues existing in the industry operation. In addition, through the analysis of various limiting factors, this paper elaborates the e-commerce industrial development mode and strengthen the industrial competitiveness in order to meet the industrial economic development requirements.}, location = {Sanya, China}, series = {EBEE 2021}, pages = {37\u201339}, numpages = {3}}
@inproceedings{10.1145/3448016.3457568,title = {Steering Query Optimizers: A Practical Take on Big Data Workloads}, author = {Negi Parimarjan , Interlandi Matteo , Marcus Ryan , Alizadeh Mohammad , Kraska Tim , Friedman Marc , Jindal Alekh },year = {2021}, isbn = {9781450383431}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3448016.3457568}, doi = {10.1145/3448016.3457568}, abstract = {In recent years, there has been tremendous interest in research that applies machine learning to database systems. Being one of the most complex components of a DBMS, query optimizers could benefit from adaptive policies that are learned systematically from the data and the query workload. Recent research has brought up novel ideas towards a learned query optimizer, however these ideas have not been evaluated on a commercial query processor or on large scale, real-world workloads. In this paper, we take the approach used by Marcus et al. in Bao and adapt it to SCOPE, a big data system used internally at Microsoft. Along the way, we solve multiple new challenges: we define how optimizer rules affect final query plans by introducing the concept of a rule signature, we devise a pipeline computing interesting rule configurations for recurring jobs, and we define a new learning problem allowing us to apply such interesting rule configurations to previously unseen jobs. We evaluate the efficacy of the approach on production workloads that include 150K daily jobs. Our results show that alternative rule configurations can generate plans with lower costs, and this can translate to runtime latency savings of 7-30% on average and up to 90% for a non trivial subset of the workload.}, location = {Virtual Event, China}, series = {SIGMOD '21}, pages = {2557\u20132569}, numpages = {13}, keywords = {distributed query optimization, learning for query optimization}}
@inproceedings{10.1145/3148055.3148072,title = {Genomics Analyser: A Big Data Framework for Analysing Genomics Data}, author = {Abdullah Tariq , Ahmet Ahmed },year = {2017}, isbn = {9781450355490}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3148055.3148072}, doi = {10.1145/3148055.3148072}, abstract = {Abstract Genomics data is unstructured and mostly stored on hard disks. It is both technically and culturally residing in big data domain due to the challenges of volume, velocity and variety. Huge volumes of data are generated from diverse sources in different formats and at a high frequency. Appropriate data models are required to accommodate these data formats for analysing and producing required results with a quick response time. Genomics data can be analysed for a variety of purposes. Existing genomics data analysis pipelines are disk I/O intensive and focus on optimizing data processing for individual analysis tasks. Intensive disk I/O operations and focus on optimizing individual analysis tasks are the biggest bottleneck of existing genomics analysis pipelines. Making any updates in genomics data require reading the whole data set again. In this paper, we present a genomics data analysis framework that addresses both the issues of existing genomics analysis pipelines. It reads unstructured genomics data from sources, transforms it in a structured format and stores this data into a NoSQL database. In this way, genomics data can be queried like any other data and an update in the genomics data does not require reading the whole data set. The framework also presents an efficient analysis pipeline for analysing the genomics data for a variety of purposes like genotype clustering, gene expression microarrays, chromosome variations or gene linkage analysis. A case study of genotype clustering is presented to demonstrate and evaluate the effectiveness of the presented framework. Our results show that the framework improves overall performance of the genomics data analysis pipeline by 49% from existing genomics data analysis pipelines. Furthermore, our approach is robust and is able sustain high performance with high system workloads.}, location = {Austin, Texas, USA}, series = {BDCAT '17}, pages = {189\u2013197}, numpages = {9}, keywords = {compute cluster, population scale clustering, resource management, in-memory computing, data analysis, machine learning, algorithms, big data}}
@inproceedings{10.1145/1014052.1016916,title = {A general approach to incorporate data quality matrices into data mining algorithms}, author = {Davidson Ian , Grover Ashish , Satyanarayana Ashwin , Tayi Giri K. },year = {2004}, isbn = {1581138881}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1014052.1016916}, doi = {10.1145/1014052.1016916}, abstract = {Data quality is a central issue for many information-oriented organizations. Recent advances in the data quality field reflect the view that a database is the product of a manufacturing process. While routine errors, such as non-existent zip codes, can be detected and corrected using traditional data cleansing tools, many errors systemic to the manufacturing process cannot be addressed. Therefore, the product of the data manufacturing process is an imprecise recording of information about the entities of interest (i.e. customers, transactions or assets). In this way, the database is only one (flawed) version of the entities it is supposed to represent. Quality assurance systems such as Motorola's Six-Sigma and other continuous improvement methods document the data manufacturing process's shortcomings. A widespread method of documentation is quality matrices. In this paper, we explore the use of the readily available data quality matrices for the data mining classification task. We first illustrate that if we do not factor in these quality matrices, then our results for prediction are sub-optimal. We then suggest a general-purpose ensemble approach that perturbs the data according to these quality matrices to improve the predictive accuracy and show the improvement is due to a reduction in variance.}, location = {Seattle, WA, USA}, series = {KDD '04}, pages = {794\u2013798}, numpages = {5}, keywords = {classification, data quality, ensemble approaches, decision trees, six-sigma}}
@inproceedings{10.1145/2939672.2939730,title = {Scalable Fast Rank-1 Dictionary Learning for fMRI Big Data Analysis}, author = {Li Xiang , Makkie Milad , Lin Binbin , Sedigh Fazli Mojtaba , Davidson Ian , Ye Jieping , Liu Tianming , Quinn Shannon },year = {2016}, isbn = {9781450342322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2939672.2939730}, doi = {10.1145/2939672.2939730}, abstract = {It has been shown from various functional neuroimaging studies that sparsity-regularized dictionary learning could achieve superior performance in decomposing comprehensive and neuroscientifically meaningful functional networks from massive fMRI signals. However, the computational cost for solving the dictionary learning problem has been known to be very demanding, especially when dealing with large-scale data sets. Thus in this work, we propose a novel distributed rank-1 dictionary learning (D-r1DL) model and apply it for fMRI big data analysis. The model estimates one rank-1 basis vector with sparsity constraint on its loading coefficient from the input data at each learning step through alternating least squares updates. By iteratively learning the rank-1 basis and deflating the input data at each step, the model is then capable of decomposing the whole set of functional networks. We implement and parallelize the rank-1 dictionary learning algorithm using Spark engine and deployed the resilient distributed dataset (RDDs) abstracts for the data distribution and operations. Experimental results from applying the model on the Human Connectome Project (HCP) data show that the proposed D-r1DL model is efficient and scalable towards fMRI big data analytics, thus enabling data-driven neuroscientific discovery from massive fMRI big data in the future.}, location = {San Francisco, California, USA}, series = {KDD '16}, pages = {511\u2013519}, numpages = {9}, keywords = {algorithm parallelization, distributed computation, sparse coding, fMRI}}
@inproceedings{10.1145/3012004,title = {The Challenge of Test Data Quality in Data Processing}, author = {Becker Christoph , Duretec Kresimir , Rauber Andreas },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3012004}, doi = {10.1145/3012004}, pages = {1\u20134}, numpages = {4}, keywords = {ground truth, quality model, model-based testing, test data, digital curation, digital preservation, data processing, data quality, test oracle, Benchmarking, data formats}}
@inproceedings{10.1145/3102254.3102272,title = {Mitigating linked data quality issues in knowledge-intense information extraction methods}, author = {Weichselbraun Albert , Kuntschik Philipp },year = {2017}, isbn = {9781450352253}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3102254.3102272}, doi = {10.1145/3102254.3102272}, abstract = {Advances in research areas such as named entity linking and sentiment analysis have triggered the emergence of knowledge-intensive information extraction methods that combine classical information extraction with background knowledge from the Web. Despite data quality concerns, linked data sources such as DBpedia, GeoNames and Wikidata which encode facts in a standardized structured format are particularly attractive for such applications.This paper addresses the problem of data quality by introducing a framework that elaborates on linked data quality issues relevant to different stages of the background knowledge acquisition process, their impact on information extraction performance and applicable mitigation strategies. Applying this framework to named entity linking and data enrichment demonstrates the potential of the introduced mitigation strategies to lessen the impact of different kinds of data quality problems. An industrial use case that aims at the automatic generation of image metadata from image descriptions illustrates the successful deployment of knowledge-intensive information extraction in real-world applications and constraints introduced by data quality concerns.}, location = {Amantea, Italy}, series = {WIMS '17}, pages = {1\u201312}, numpages = {12}, keywords = {applications, information extraction, named entity linking, semantic technologies, mitigation strategies, linked data quality}}
@inproceedings{10.1145/2614512.2614518,title = {Learning to think about broader implications of big data}, author = {Topi Heikki },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2614512.2614518}, doi = {10.1145/2614512.2614518}, pages = {24\u201325}, numpages = {2}}
@inproceedings{10.1145/3018896.3018950,title = {Subspace selection in high-dimensional big data using genetic algorithm in apache spark}, author = {Cheraghchi Fatemeh , Iranzad Arash , Raahemi Bijan },year = {2017}, isbn = {9781450347747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3018896.3018950}, doi = {10.1145/3018896.3018950}, abstract = {In high-dimensional space with large amounts of data, distances between data points tend to become relatively uniform. The notion of the nearest neighbours of a data point thus becomes meaningless, a phenomenon known as \"curse of dimensionality.\" Identifying outliers (data points with statistical characteristics significantly different than the majority of the data) in such a high-dimensional space can be a significant challenge. Mining for outliers in subspaces with relevant attributes is one of approaches for this problem, and identifying these attributes is the main objective of this work. In this paper, we scale a grid-based solution to search for subspaces that are candidates for outlier detection with regard to the subset of features in the subspace. We specify a population and a fitness function for a distributed genetic algorithm to heuristically search the subspaces within the high dimensional data, and find the subspace with maximal sparsity. We designed and implemented our proposed subspace selection algorithm in Apache Spark, a fast in-memory engine for large-scale data processing. The initial experimental results on a large dataset (77,000 records and 1,379 attributes) confirm that our proposed method can identify the most relevant subspaces for outlier detection.}, location = {Cambridge, United Kingdom}, series = {ICC '17}, pages = {1\u20137}, numpages = {7}, keywords = {apache spark, subspace selection, high-dimensional, outlier detection, big data, genetic algorithm}}
@inproceedings{10.1145/3465631.3465969,title = {Application of Video Big Data in Technical Art Training}, author = {Wang Xuan-Yao , Xiao Wei },year = {2021}, isbn = {9781450385015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3465631.3465969}, doi = {10.1145/3465631.3465969}, abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.}, location = {Jakarta, Indonesia}, series = {ICIMTECH 21}, pages = {1\u20134}, numpages = {4}}
@inproceedings{10.1145/3209281.3209300,title = {Big data analytics in social care provision: spatial and temporal evidence from Birmingham}, author = {Chotvijit Sarunkorn , Thiarai Malkiat , Jarvis Stephen },year = {2018}, isbn = {9781450365260}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3209281.3209300}, doi = {10.1145/3209281.3209300}, abstract = {There is significant national interest in tackling issues surrounding the needs of vulnerable children and adults. At the same time, UK local authorities face severe financial challenges as a result of decreasing financial settlements and increasing demands from growing urban populations. This research employs state-of-the-art data analytics and visualisation techniques to analyse six years of local government social care data for the city of Birmingham, the UK's second most populated city. This analysis identifies: (i) service cost profiles over time; (ii) geographical dimensions to service demand and delivery; (iii) patterns in the provision of services, and (iv) the extent to which data value and data protection interact. The research accesses data held by the local authority to discover patterns and insights that may assist in the understanding of service demand, support decision making and resource management, whilst protecting and safeguarding its most vulnerable citizens. The use of data in this manner could also inform the approach a local authority has to its data, its capture and use, and the potential for supporting data-led management and service improvements.}, location = {Delft, The Netherlands}, series = {dg.o '18}, pages = {1\u20138}, numpages = {8}, keywords = {data analytics, spatio-temporal analysis, service provision, local authority, Birmingham, social care}}
@inproceedings{10.1145/3495018.3495311,title = {Analysis on NSAW Reminder Based on Big Data Technology}, author = {Jin Ziheng },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495311}, doi = {10.1145/3495018.3495311}, abstract = {NSAWS is an intelligent and real-time large database management system. By analyzing the user identity data and access rights contained in the collected information, the NSAWS finds out the potential risks and issues an alarm notice in a timely manner. This paper mainly studies how to strengthen the prevention of network attacks in BD environment from the following aspects. This paper first introduces the common technology on the Internet in our country and its application status; secondly, it expounds the architecture, deployment mode and operation mode of the large database system based on the basic security facilities such as cloud computing platform and firewall. Then the traditional NSAWS is analyzed and the simulation platform is tested. The results show that the platform has high accuracy and stability in alerting network security hazards and can effectively protect network security.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {954\u2013958}, numpages = {5}}
@inproceedings{10.1145/2768830,title = {Biomedical Ontology Quality Assurance Using a Big Data Approach}, author = {Cui Licong , Tao Shiqiang , Zhang Guo-Qiang },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2768830}, doi = {10.1145/2768830}, abstract = {This article presents recent progresses made in using scalable cloud computing environment, Hadoop and MapReduce, to perform ontology quality assurance (OQA), and points to areas of future opportunity. The standard sequential approach used for implementing OQA methods can take weeks if not months for exhaustive analyses for large biomedical ontological systems. With OQA methods newly implemented using massively parallel algorithms in the MapReduce framework, several orders of magnitude in speed-up can be achieved (e.g., from three months to three hours). Such dramatically reduced time makes it feasible not only to perform exhaustive structural analysis of large ontological hierarchies, but also to systematically track structural changes between versions for evolutional analysis. As an exemplar, progress is reported in using MapReduce to perform evolutional analysis and visualization on the Systemized Nomenclature of Medicine\u2014Clinical Terms (SNOMED CT), a prominent clinical terminology system. Future opportunities in three areas are described: one is to extend the scope of MapReduce-based approach to existing OQA methods, especially for automated exhaustive structural analysis. The second is to apply our proposed MapReduce Pipeline for Lattice-based Evaluation (MaPLE) approach, demonstrated as an exemplar method for SNOMED CT, to other biomedical ontologies. The third area is to develop interfaces for reviewing results obtained by OQA methods and for visualizing ontological alignment and evolution, which can also take advantage of cloud computing technology to systematically pre-compute computationally intensive jobs in order to increase performance during user interactions with the visualization interface. Advances in these directions are expected to better support the ontological engineering lifecycle.}, pages = {1\u201328}, numpages = {28}, keywords = {lattice, Hadoop, SNOMED CT, terminology quality assurance}}
@inproceedings{10.1145/2934466.2934476,title = {Using IVML to model the topology of big data processing pipelines}, author = {Eichelberger Holger , Qin Cui , Sizonenko Roman , Schmid Klaus },year = {2016}, isbn = {9781450340502}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2934466.2934476}, doi = {10.1145/2934466.2934476}, abstract = {Creating product lines of Big Data stream processing applications introduces a number of novel challenges to variability modeling. In this paper, we discuss these challenges and demonstrate how advanced variability modeling capabilities can be used to directly model the topology of processing pipelines as well as their variability. We also show how such processing pipelines can be modeled, configured and validated using the Integrated Variability Modeling Language (IVML).}, location = {Beijing, China}, series = {SPLC '16}, pages = {204\u2013208}, numpages = {5}, keywords = {topologies, variability modeling, software product lines}}