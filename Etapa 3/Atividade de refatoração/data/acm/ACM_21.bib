@inproceedings{10.1145/3297156.3297233,title = {Dynamic Data Compression Algorithm Selection for Big Data Processing on Local File System}, author = {Fuzong Wang , Helin Guo , Jian Zhao },year = {2018}, isbn = {9781450366069}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297156.3297233}, doi = {10.1145/3297156.3297233}, abstract = {Data Compression has become a commodity feature for space efficiency and performance by reducing reading and writing traffic and space capacity demand. This technology is particularly valuable for a file system to manage and server the big data processing tasks. However, the fixed data compression scheme cannot fit all the big data workloads and data-set which have complex internal data structure and compressibility. This paper investigates a dynamic and smart data compression algorithm selection scheme for different big data processing cases in the local file system. To this end, we propose a dynamic algorithm selection module in the Linux ZFS which is an open source file system. This module will select a high compression ratio algorithm for high compressibility data, and select a fast compression algorithm for low compressibility data, and skip all data compression process for incompressibility data. The comprehensive evaluations validate that dynamic algorithm selection module can achieve up to 2.69x response time improvement for reading and writing operation in file system and reduce about 32.12% storage space for a large amount data-set.}, location = {Shenzhen, China}, series = {CSAI '18}, pages = {110\u2013114}, numpages = {5}, keywords = {Big Data, File system, Compression Algorithm, Data Compression}}
@inproceedings{10.5555/3382225.3382440,title = {AI robo-advisor with big data analytics for financial services}, author = {Day Min-Yuh , Cheng Tun-Kung , Li Jheng-Gang },year = {2018}, isbn = {9781538660515}, publisher = {IEEE Press}, abstract = {Robo-Advisors has been growing attraction from the financial industry for offering financial services by using algorithms and acting as like human advisors to support investors making investment decisions. During the investment planning stage, portfolio optimization plays a crucial role, especially for the medium and long-term investors, in determining the allocation weight of assets to achieve the balance between investors expectation return and risk tolerance. The literature on the topic of portfolio optimization has been offering plenty of theoretical and practical guidance for implementing the theory; however, there is a paucity of studies focusing on the applications which are designed for Robo-Advisors. In this research, we proposed a modular system and focused on integrating big data analysis, deep learning method and the Black-Litterman model to generate asset allocation weight. We developed a portfolio optimization module which takes the information from a variety of sources, such as stocks prices, investor profile and the other alternative data, and used them as input to calculate optimal weights of assets in the portfolio. The module we developed could be used as a sub-system for Robo-Advisors, which offers a customized optimal portfolio based on investors preference.}, location = {Barcelona, Spain}, series = {ASONAM '18}, pages = {1027\u20131031}, numpages = {5}, keywords = {portfolio optimization, investment management, black-litterman, big data analysis, financial technology, robo-advisors, deep learning}}
@inproceedings{10.14778/3368289.3368299,title = {Incorporating super-operators in big-data query optimizers}, author = {Leeka Jyoti , Rajan Kaushik },year = {2019}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3368289.3368299}, doi = {10.14778/3368289.3368299}, abstract = {The cost of big-data analytics is dominated by shuffle operations that induce multiple disk reads, writes and network transfers. This paper proposes a new class of optimization rules that are specifically aimed at eliminating shuffles where possible. The rules substitute multiple shuffle inducing operators (Join, UnionAll, Spool, GroupBy) with a single streaming operator which implements an entire sub-query. We call such operators super-operators.A key challenge with adding new rules that substitute sub-queries with super-operators is that there are many variants of the same sub-query that can be implemented via minor modifications to the same super-operator. Adding each as a separate rule leads to a search space explosion. We propose several extensions to the query optimizer to address this challenge. We propose a new abstract representation for operator trees that captures all possible sub-queries that a super-operator implements. We propose a new rule matching algorithm that can efficiently search for abstract operator trees. Finally we extend the physical operator interface to introduce new parametric super-operators.We implement our changes in SCOPE, a state-of-the-art production big-data optimizer used extensively at Microsoft. We demonstrate that the proposed optimizations provide significant reduction in both resource cost (average 1.7x) and latency (average 1.5x) on several production queries, and do so without increasing optimization time.}, pages = {348\u2013361}, numpages = {14}}
@inproceedings{10.1109/TCBB.2015.2454551,title = {Knowledge discovery using big data in biomedical systems}, author = {Janga Sarath Chandra , Zhu Dongxiao , Chen Jake Y. , Zaki Mohammed J. },year = {2015}, publisher = {IEEE Computer Society Press}, address = {Washington, DC, USA}, url = {https://doi.org/10.1109/TCBB.2015.2454551}, doi = {10.1109/TCBB.2015.2454551}, abstract = {The 13th International Workshop on Data Mining in Bioinformatics (BIOKDD'14) was organized in conjunction with the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining on August 24, 2014 in New York, USA. It brought together international researchers in the interacting disciplines of data mining, systems biology, and bioinformatics at the Bloomberg Headquarters venue. The goal of this workshop is to encourage Knowledge Discovery and Data mining (KDD) researchers to take on the numerous challenges that Bioinformatics offers. This year, the workshop featured the theme of \"Knowledge discovery using big data in biological/biomedical systems\".}, pages = {726\u2013728}, numpages = {3}}
@inproceedings{10.1145/3318464.3389770,title = {Qd-tree: Learning Data Layouts for Big Data Analytics}, author = {Yang Zongheng , Chandramouli Badrish , Wang Chi , Gehrke Johannes , Li Yinan , Minhas Umar Farooq , Larson Per-\u00c5ke , Kossmann Donald , Acharya Rajeev },year = {2020}, isbn = {9781450367356}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3318464.3389770}, doi = {10.1145/3318464.3389770}, abstract = {Corporations today collect data at an unprecedented and accelerating scale, making the need to run queries on large datasets increasingly important. Technologies such as columnar block-based data organization and compression have become standard practice in most commercial database systems. However, the problem of best assigning records to data blocks on storage is still open. For example, today's systems usually partition data by arrival time into row groups, or range/hash partition the data based on selected fields. For a given workload, however, such techniques are unable to optimize for the important metric of the number of blocks accessed by a query. This metric directly relates to the I/O cost, and therefore performance, of most analytical queries. Further, they are unable to exploit additional available storage to drive this metric down further. In this paper, we propose a new framework called a query-data routing tree, or qd-tree, to address this problem, and propose two algorithms for their construction based on greedy and deep reinforcement learning techniques. Experiments over benchmark and real workloads show that a qd-tree can provide physical speedups of more than an order of magnitude compared to current blocking schemes, and can reach within 2X of the lower bound for data skipping based on selectivity, while providing complete semantic descriptions of created blocks.}, location = {Portland, OR, USA}, series = {SIGMOD '20}, pages = {193\u2013208}, numpages = {16}, keywords = {deep reinforcement learning, big data, query processing, data layout, OLAP, deep learning, data analytics, indexing, storage, data partitioning}}
@inproceedings{10.1145/3436369.3437426,title = {Research on Optimization of Intelligent Public Transportation Scheduling Based on Big Data}, author = {Li Weiwei , Xu Huarong , Chen Guanhua , Qian Jianhong , Wen Xinping },year = {2020}, isbn = {9781450387835}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3436369.3437426}, doi = {10.1145/3436369.3437426}, abstract = {In order to solve the problem of urban traffic congestion, and according to the current status and operation of bus dispatching vehicles of public transportation enterprises, taking into account the interests of passengers and public transportation enterprises, an intelligent scheduling scheme based on big data was studied. By analyzing bus card swiping data, mining and processing the data, we get the number of passengers on the bus, the passenger flow data of the bus line is used to realize the optimization of the bus dispatch table..Based on the research and analysis of genetic algorithm and tabu search algorithm, the optimal scheduling schedule can be obtained by combining the two hybrid algorithms under certain constraints and aiming at minimizing the number of trips and the cost of passengers. Experiments show that the hybrid genetic algorithm can accelerate the convergence speed and get the optimal departure interval and the minimum number of buses in different time periods of a day.}, location = {Xiamen, China}, series = {ICCPR 2020}, pages = {467\u2013471}, numpages = {5}, keywords = {genetic algorithm, intelligent scheduling, hybrid algorithm, Big data}}
@inproceedings{10.1145/3361785.3361786,title = {Research on innovation of cross-border e-commerce business model based on big data}, author = {Alberic Minno Dekassan , YeZheng Liu , Rodrigue Dibonji Ndjansse Stephane , Vellem Vuyolwethu },year = {2019}, isbn = {9781450372329}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3361785.3361786}, doi = {10.1145/3361785.3361786}, abstract = {With the rapid development of the economy and the continuous deepening of the application of Internet technology, today's social economy has gradually entered the era of big data, which provides a good social environment for the development of e-commerce. This paper first introduces the background and significance of big data, and analyzes the connotation and advantages of e-commerce in the era of big data. On the basis of analyzing the development status of e-commerce industry under the background of big data, it summarizes the opportunities and challenges faced by e-commerce enterprises in the context of big data, and discusses the e-commerce service model under the era of big data. The main purpose of this research is to contribute to the improvement of e-commerce service level in the era of big data and promote the rapid development of e-commerce.}, location = {Paris, France}, series = {ICBIM '19}, pages = {1\u20134}, numpages = {4}, keywords = {cross-border e-commerce, business model, big data}}
@inproceedings{10.1145/3414274.3414280,title = {Statistic Analysis of Safety Accidents in Filling Stations Based on Big Data}, author = {Heng Li , Longfu Zhou , Kaiyou Yuan },year = {2020}, isbn = {9781450376044}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3414274.3414280}, doi = {10.1145/3414274.3414280}, abstract = {The big data technology which taking data analysis and mining as core can efficiently collect, store and process data of filling station safety accident cases, and further realize the statistical analysis, knowledge mining, principle summary and early warning forecast of filling station safety accidents. The article collected 461 cases of filling station safety accidents from 1981 to 2019 domestically and internationally. Given that those electronic text data are not highly standardized and cannot be directly analyzed and utilized, keyword extraction and structured storage were adopted by this article, in order to implement structured processing of electronic text data. Through the multi-dimensional statistical analysis of the structured data, high-quality, reliable, and practical analytical results were achieved. The results demonstrated that big data technology would become the main development stream and one of the most vital weapons of statistical analysis in various industries in the future.}, location = {Xiamen, China}, series = {DSIT 2020}, pages = {36\u201341}, numpages = {6}, keywords = {Structured storage, Keyword extraction, Safety accident case, Filling station, Statistic analysis, Big data}}
@inproceedings{10.1145/3543106.3543115,title = {A Study on the Economic Model of Volume in the Age of Big Data}, author = {Li Kai },year = {2022}, isbn = {9781450397162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3543106.3543115}, doi = {10.1145/3543106.3543115}, abstract = {With the rapid development of China's economy, the degree of integration of economics and management is deepening. Based on the statistical analysis method of big data, the trend and law of economic development can be obtained. Big data statistical methods are widely used in the field of economics and improve work efficiency. Effective statistical analysis of data can not only reflect the operation of the product in time, but also reflect the market demand for the product. Therefore, this paper studies the role of big data statistical analysis methods in the field of economic management. This paper first classifies and sorts out the representative quantitative research methods and models in the era of big data, and then based on the BP neural network model and combines 36 indicator data to make multivariate forecasts for China's consumer price index (CPI). The research results show that the prediction results of the BP neural network are good.}, location = {Seoul, Republic of Korea}, series = {ICEMC '22}, pages = {54\u201358}, numpages = {5}, keywords = {Quantitative economy, Big data, BP neural network, CPI}}
@inproceedings{10.1145/2905055.2905124,title = {Query Processing over Large RDF using SPARQL in Big Data}, author = {Khodke Priti , Lawange Saurabh , Bhagat Amol , Dongre Kiran , Ingole Chetan },year = {2016}, isbn = {9781450339629}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2905055.2905124}, doi = {10.1145/2905055.2905124}, abstract = {Internet search is done by exploring the link graph and keyword frequency. In 2012, Google released \"Knowledge Graph\" --Semantic Web. The human reasoning can be enhanced by the use semantic web an emerging area. Most of the current applications link open data views due to which there is huge flow of data in semantic web, particularly Resource Description Framework (RDF) data. In the semantic web research community this leads to design and development of scalable data processing techniques for RDF data. The aim of semantic web is to make available semantically connected data across the globe. This is a review paper giving analysis of techniques implemented to achieve the aim of semantic web, various approaches to processes RDF data. Within the semantic web community, RDF is a common acronym because it forms one of the basic building blocks for forming the web of semantic data, called a \"graph database\". This paper compares various methodologies followed by different researchers along with the results analysis of implemented techniques over different datasets.}, location = {Udaipur, India}, series = {ICTCS '16}, pages = {1\u20136}, numpages = {6}, keywords = {Hadoop, SPARQL, RDF Graph, Big Data, MapReduce, Semantic Web}}
@inproceedings{10.1145/3281375.3281393,title = {Toward big data analysis to improve enterprise information security}, author = {Alouneh Sahel , Hababeh Ismail , Alajrami Tamer },year = {2018}, isbn = {9781450356220}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3281375.3281393}, doi = {10.1145/3281375.3281393}, abstract = {In recent years, big data and cloud computing are considered key trends of modern computer technology. Extracting valuable information is the key purpose of analyzing big data that needs to be secured in order to avoid any potential risks. Most cloud systems applications contain sensitive data, such as; financial, legal and private information. Therefore, threats on such data may put cloud systems holding this data at high risk. The demand on securing cloud systems applications has been increasing rapidly; however, big data protection is still a challenge. This paper proposes a new methodology to protect big data during analysis by classifying data before any action such as moving, copying or processing take place. Big data files are classified according to the criticality level of their contents into three categories from the most to the least sensitive: restricted, confidential and public. Based on big data classification, the encryption algorithm AES 128 is applied on confidential big data, while the encryption algorithm AES 256 is applied on the restricted big data files. The experimental results show that our method enhances the performance of big data analysis systems and outperforms other approaches in the literature.}, location = {Tokyo, Japan}, series = {MEDES '18}, pages = {106\u2013109}, numpages = {4}, keywords = {data classification, threats, encryption}}
@inproceedings{10.1145/3465375,title = {PoBery: Possibly-complete Big Data Queries with Probabilistic Data Placement and Scanning}, author = {Song Jie , He Qiang , Chen Feifei , Yuan Ye , Yu Ge },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3465375}, doi = {10.1145/3465375}, abstract = {In big data query processing, there is a trade-off between query accuracy and query efficiency, for example, sampling query approaches trade-off query completeness for efficiency. In this article, we argue that query performance can be significantly improved by slightly losing the possibility of query completeness, that is, the chance that a query is complete. To quantify the possibility, we define a new concept, Probability of query Completeness (hereinafter referred to as PC). For example, If a query is executed 100 times, PC = 0.95 guarantees that there are no more than 5 incomplete results among 100 results. Leveraging the probabilistic data placement and scanning, we trade off PC for query performance. In the article, we propose PoBery (POssibly-complete Big data quERY), a method that supports neither complete queries nor incomplete queries, but possibly-complete queries. The experimental results conducted on HiBench prove that PoBery can significantly accelerate queries while ensuring the PC. Specifically, it is guaranteed that the percentage of complete queries is larger than the given PC confidence. Through comparison with state-of-the-art key-value stores, we show that while Drill-based PoBery performs as fast as Drill on complete queries, it is 1.7 \u00d7, 1.1 \u00d7, and 1.5 \u00d7 faster on average than Drill, Impala, and Hive, respectively, on possibly-complete queries.}, pages = {1\u201328}, numpages = {28}, keywords = {data placement, probability, query completeness, key-value stores, data query, Big data, scanning}}
@inproceedings{10.1145/3563042,title = {Prediction Method of Government Economic Situation Based on Big Data Analysis}, author = {Liu Yisheng , Tang Anying },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3563042}, doi = {10.1145/3563042}, abstract = {In order to improve the forecasting accuracy of economic situation, a government economic situation forecasting method based on big data analysis is proposed. According to the hardware structure of the system, STC12C5608AD is used as the data acquisition terminal chip to simplify the circuit. The proposed forecasting method can give real-time early warning to the government's economic situation. The software part screens the influencing factors of government economic development, constructs a government economic development index system, collects government economic index data, cleans, clusters, classifies and standardizes the government economic index data, and extracts the preprocessed government economic index data from the preprocessed government economic index data through data mining. The economic development features are extracted and then input into the neural network. After training and learning, the predicted value of the economic situation is output, and the economic situation level is classified. The experimental results show that the proposed method reduces the error rate of economic situation forecast, shortens the forecast time, improves the forecast accuracy and efficiency, with the peak error ratio not exceeding 15%.}, keywords = {Economic situation, Big data analysis, Situation forecast, Government economy}}
@inproceedings{10.5555/3291291.3291361,title = {CASCON workshop on developing big data applications and services}, author = {Arruda Darlan , Madhavji Nazim H. , Taylor Colin },year = {2018}, publisher = {IBM Corp.}, address = {USA}, abstract = {Research from Gartner (2015) indicates that, in 2017, 60% of Big Data projects failed or did not provide the expected benefits [1]. However, in November 2017, Nick Heudecker, a Gartner analyst, posted in his twitter account that they were too conservative. The Big Data project failure rate is now close to 85%. The reasons are not only related to technology itself [2]. It is a mix of environmental, technological and managerial problems. Some of the reasons for Big Data projects failure are: At the project level [3], [4]: missing link to business objectives, lacking big data skills, relying too much on the data, failing to convince executives, and poor planning; At the technical level [5]: Rapid technology changes, difficulty in selecting Big Data technologies to address the systems and project requirements, complex integration between new and old systems, computation of intensive analytics, and the necessity of high scalability, availability and reliability, to name a few. Further, a previous study [6] has shown that there is approximately a 80:20 split in the industry focus in favor of algorithms for analytics and infrastructure, thereby shortchanging the aspects of creating and evolving applications and services concerned with Big Data.}, location = {Markham, Ontario, Canada}, series = {CASCON '18}, pages = {407\u2013409}, numpages = {3}}
@inproceedings{10.1145/3149572.3149575,title = {Total Data Quality Management and Total Information Quality Management Applied to Costumer Relationship Management}, author = {Francisco Maritza M. C. , Alves-Souza Solange N. , Campos Edit G. L. , De Souza Luiz S. },year = {2017}, isbn = {9781450353373}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3149572.3149575}, doi = {10.1145/3149572.3149575}, abstract = {Data quality (DQ) is an important issue for modern organizations, mainly for decision-making based on information, using solutions such as CRM, Business Analytics, and Big Data. In order to obtain quality data, it is necessary to implement methods, processes, and specific techniques that handle information as a product, with well established, controlled, and managed production processes. The literature provides several types of quality data management methodologies that treat structured data, and few treating semi- and non-structured data. Choosing the methodology to be adopted is one the major issues faced by organizations, when challenged to treat the data quality in a systematic manner. This paper makes a comparative analysis between TDQM -- Total Data Quality Management and TIQM -- Total Information Quality Management approaches, focusing on data quality problems in the context of a CRM -- Costumer Relationship Management application. Such analysis identifies the strengths and weaknesses of each methodology and suggests the most suitable for the CRM scenario.}, location = {Barcelona, Spain}, series = {ICIME 2017}, pages = {40\u201345}, numpages = {6}, keywords = {data quality dimensions, Data quality, data quality methodology, data quality management, data quality problems}}
@inproceedings{10.1145/3274005.3274021,title = {Big Data Technologies on Commodity Workstations: A Basic Setup for Apache Impala}, author = {Fotache Marin , Greavu-\u015eerban Valeric\u0103 , Hrubaru Ionu\u0163 , Tic\u0103 Alexandru },year = {2018}, isbn = {9781450364256}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3274005.3274021}, doi = {10.1145/3274005.3274021}, abstract = {Big Data technologies brought the idea of parallel processing on cheaper commodity servers. When dealing with huge amount of data, instead of migrating to more performant and costly hardware platforms, or buying resources in cloud, it is more affordable to add a number of cheaper servers as nodes for data processing and/or storage. NoSQL data stores, Hadoop ecosystems, NewSQL platforms have proved viable for Big Data storage and processing. In this paper we were concerned with setting up a platform for big data processing using commodity workstations. Many small and medium sized companies have limited resources and their workstations remain unused for more than 12 hours a day. Here Beowulf Cluster Computing could prove useful. Apache Impala was installed as part of a Hadoop distribution on a 9-node cluster. Three TPC-H database schema were loaded for the scale factors of 1, 2 and 10GB. A series of 100 SQL queries were randomly generated and executed for each scale factor. Results were collected and analyzed for determining if the cluster can provide a decent level of data processing performance.}, location = {Ruse, Bulgaria}, series = {CompSysTech'18}, pages = {110\u2013115}, numpages = {6}, keywords = {Beowulf clustering, Query performance, Hadoop, Impala, Distributed computing}}
@inproceedings{10.1145/2945408.2945419,title = {A tool for verification of big-data applications}, author = {Bersani Marcello M. , Marconi Francesco , Rossi Matteo , Erascu Madalina },year = {2016}, isbn = {9781450344111}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2945408.2945419}, doi = {10.1145/2945408.2945419}, abstract = {Quality-driven frameworks for developing data-intensive applications are becoming more and more popular, following the remarkable popularity of Big Data approaches. The DICE framework, designed within the DICE project (www.dice-h2020.eu), has the goal of offering a novel profile and tools for data-aware quality-driven development. One of its tools is the DICE Verification Tool (D-VerT), which allows designers to evaluate their design against safety properties, such as reachability of undesired configurations of the system. This paper describes the first version of D-VerT, available open source at github.com/dice-project/DICE-Verification.}, location = {Saarbr\u00fccken, Germany}, series = {QUDOS 2016}, pages = {44\u201345}, numpages = {2}, keywords = {temporal logic, Formal verification}}
@inproceedings{10.1145/2972958.2972967,title = {Data Sets and Data Quality in Software Engineering: Eight Years On}, author = {Liebchen Gernot , Shepperd Martin },year = {2016}, isbn = {9781450347723}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2972958.2972967}, doi = {10.1145/2972958.2972967}, abstract = {Context: We revisit our review of data quality within the context of empirical software engineering eight years on from our PROMISE 2008 article.Objective: To assess the extent and types of techniques used to manage quality within data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets.Method: We update the 2008 mapping study through four subsequently published reviews and a snowballing exercise.Results: The original study located only 23 articles explicitly considering data quality. This picture has changed substantially as our updated review now finds 283 articles, however, our estimate is that this still represents perhaps 1% of the total empirical software engineering literature.Conclusions: It appears the community is now taking the issue of data quality more seriously and there is more work exploring techniques to automatically detect (and sometimes repair) noise problems. However, there is still little systematic work to evaluate the various data sets that are widely used for secondary analysis; addressing this would be of considerable benefit. It should also be a priority to work collab-oratively with practitioners to add new, higher quality data to the existing corpora.}, location = {Ciudad Real, Spain}, series = {PROMISE 2016}, pages = {1\u20134}, numpages = {4}, keywords = {data quality, mapping study, empirical software engineering}}
@inproceedings{10.1145/3327964.3328493,title = {A Linked Data Quality Assessment Framework for Network Data}, author = {To Alex , Meymandpour Rouzbeh , Davis Joseph G. , Jourjon Guillaume , Chan Jonathan },year = {2019}, isbn = {9781450367899}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3327964.3328493}, doi = {10.1145/3327964.3328493}, abstract = {For network analysts, understanding how traffic flows through a network is crucial to network management and forensics such as network monitoring, vulnerability assessment and defence. In order to understand how traffic flows through a network, network analysts typically access multiple, disparate data sources and mentally fuse this information. Providing some sort of automated support is crucial for network management. However, information about the quality of the network data sources is essential in order to build analyst's trust in automated tools. This paper presents SydNet, a novel Linked Data quality assessment framework which allows analysts to define quality dimensions and metrics which provide an accurate reflection of the quality of the data sources. The SydNet architecture also provides a number of novel fusion heuristics which can be used to fuse data from various network data sources. We demonstrate the utility of the SydNet architecture using CAIDA longitudinal topological data from a recent 24 months period and we demonstrate that our approach was able to detect dataset quality anomalies that would require further investigation.}, location = {Amsterdam, Netherlands}, series = {GRADES-NDA'19}, pages = {1\u20138}, numpages = {8}, keywords = {Network, CAIDA, Data quality}}
@inproceedings{10.1145/2998476.2998498,title = {A Novel Approach to Big Data Veracity using Crowdsourcing Techniques and Bayesian Predictors}, author = {Agarwal Bhoomika , Ravikumar Abhiram , Saha Snehanshu },year = {2016}, isbn = {9781450348089}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2998476.2998498}, doi = {10.1145/2998476.2998498}, abstract = {In today's world data is being generated at a tremendous pace and there have to be enough measures in place to verify the nature of big data. Analysis performed on 'dirty' data may lead to erroneous insights and thereby shaping decisions poorly. The aspect of big data that deals with its correctness is known as big data veracity. Trusting the data acquired goes a long way in implementing decisions from an automated decision-making system and veracity helps to validate the data acquired. In this paper, we present our solution to the big data veracity problem using crowdsourcing techniques. Our solution involves the use of sentiment analysis, which deals with identifying the sentiment expressed in a piece of text. As a proof of concept, we have developed an app that requires users to tag tweets as per the sentiment it evokes in them. Each tweet would therefore get ratified by hundreds of our participants and the sentiment associated to the tweet gets tagged. The tagged emotion was then evaluated against the verified emotion as compared to a verified data set. This analysis was then plotted on a ROC curve and also evaluated against verified data using a Bayesian predictor trained with a trinomial function. As can be seen, an accuracy of 81% was obtained as displayed by the ROC curve and 89% through the Bayesian predictor. Also, a MAP analysis of the Bayesian predictor yields neutral sentiment as the most probable hypothesis. By doing this, we have proven that crowdsourcing of sentiment analysis is a viable solution to the problem of big data veracity and therefore an aid in making better decisions.}, location = {Gandhinagar, India}, series = {COMPUTE '16}, pages = {153\u2013160}, numpages = {8}, keywords = {Machine Learning, Big Data, Bayesian Predictor, Tweet Mining, Crowdsourcing, Sentiment Analysis}}
@inproceedings{10.1145/2649387.2660825,title = {Promises and challenges in analysis of biological big data}, author = {McDermott Jason },year = {2014}, isbn = {9781450328944}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2649387.2660825}, doi = {10.1145/2649387.2660825}, abstract = {The advent of multiple new technologies for measuring many components in biological systems offers a huge opportunity and challenge for researchers. An important question is how to make sense of the mountains of data that describe different aspects of the same, or similar, biological systems. We are taking several approaches to this problem in terms of statistical methods and data mining, network and pathway analysis, and generation of testable biological hypotheses. We discuss applications of these approaches to study host-pathogen interactions and cancer, and talk about future opportunities and challenges in this area.}, location = {Newport Beach, California}, series = {BCB '14}, pages = {680}, numpages = {1}, keywords = {cancer, host-pathogen interactions, modeling, systems biology, proteomics, omics, network biology, big data, data integration}}
@inproceedings{10.1145/3109453.3109468,title = {A big-data layered architecture for analyzing molecular communications systems in blood vessels}, author = {Felicetti Luca , Femminella Mauro , Ivanov Todor , Lio' Pietro , Reali Gianluca },year = {2017}, isbn = {9781450349314}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3109453.3109468}, doi = {10.1145/3109453.3109468}, abstract = {We present a novel architecture for analyzing molecular communications systems in blood vessels for drug delivery and monitoring. This architecture leverages a big data platform for simultaneously using data produced by the existing simulation platforms, health records, and medical data acquisition systems. An included machine learning engine may provide useful insight for medical purposes.}, location = {Washington, D.C.}, series = {NanoCom '17}, pages = {1\u20132}, numpages = {2}, keywords = {blood vessels, big data, drug delivery, molecular communications}}
@inproceedings{10.1145/3089251.3089252,title = {Evolutionary feature manipulation in data mining/big data}, author = {Xue Bing , Zhang Mengjie },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3089251.3089252}, doi = {10.1145/3089251.3089252}, abstract = {Known as the GIGO (Garbage In, Garbage Out) principle, the quality of the input data highly influences or even determines the quality of the output of any machine learning, big data and data mining algorithm. The input data which is often represented by a set of features may suffer from many issues. Feature manipulation is an effective means to improve the feature set quality, but it is a challenging task. Evolutionary computation (EC) techniques have shown advantages and achieved good performance in feature manipulation. This paper reviews recent advances on EC based feature manipulation methods in classifcation, clustering, regression, incomplete data, and image analysis, to provide the community the state-of-the-art work in the field.}, pages = {4\u201311}, numpages = {8}}
@inproceedings{10.1145/2783258.2790458,title = {How Artificial Intelligence and Big Data Created Rocket Fuel: A Case Study}, author = {John George },year = {2015}, isbn = {9781450336642}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2783258.2790458}, doi = {10.1145/2783258.2790458}, abstract = {In 2008, Rocket Fuel's founders saw a gap in the digital advertising market. None of the existing players were building autonomous systems based on big data and artificial intelligence, but instead they were offering fairly simple technology and relying on human campaign managers to drive success. Five years later in 2013, Rocket Fuel had the best technology IPO of the year on NASDAQ, reported $240 million in revenue, and was ranked by accounting firm Deloitte as the #1 fastest-growing technology company in North America. Along the way we learned that it's okay to be bold in our expectations of what is possible with fully autonomous systems, we learned that mainstream customers will buy advanced technology if it's delivered in a familiar way, and we also learned that it's incredibly difficult to debug the complex \"robot psychology\" when a number of complex autonomous systems interact. We also had excellent luck and timing: as we were building the company, real-time ad impression-level auctions with machine-to-machine buying and selling became commonplace, and marketers became increasingly focused on delivering better results for their company and delivering better personalized and relevant digital experiences for their customers. The case study presentation will present a fast-paced overview of the business and technology context for Rocket Fuel at inception and at present, key learnings and decisions, and the road ahead.}, location = {Sydney, NSW, Australia}, series = {KDD '15}, pages = {1629}, numpages = {1}, keywords = {real-time bidding, big data analytics, computational advertising, artificial intelligence, advertising}}
@inproceedings{10.1145/3545897.3545903,title = {Study on Application of Big Data Technology to Operation of Fresh E-commerce}, author = {Li Ying , Wang Jinliang , Lu Kunxiu },year = {2022}, isbn = {9781450397322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3545897.3545903}, doi = {10.1145/3545897.3545903}, abstract = {As fresh e-commerce is believed to be a blue ocean in e-commerce sector, investors contribute to it successively, which results in the rapid development of fresh e-commerce. However, with the diversification of consumers\u2019 demands and the unprofitable circumstance of fresh e-commerce, fresh e-commerce faces great crisis currently. Its fast growth provides a wider platform for reasonable application of big data technology. Meanwhile, big data technology also brings new optimization ideas for operation of fresh e-commerce. This paper mainly started from the characteristics of fresh e-commerce and big data technology to analyze the status quo and problems of development of fresh e-commerce, so as to study the application status of big data technology to fresh e-commerce, with a focus on analyzing the application status of big data technology to logistics and target customers management in fresh e-commerce.}, location = {Madrid, Spain}, series = {ICIEB '22}, pages = {38\u201343}, numpages = {6}, keywords = {Operation, Big data technology, Fresh e-commerce}}
@inproceedings{10.1145/3456887.3456908,title = {Bilingual Teaching Mode of Economic Management Courses in the Era of Big Data}, author = {Le Fei , Tan Wenqian , Le Yimin },year = {2021}, isbn = {9781450389969}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456887.3456908}, doi = {10.1145/3456887.3456908}, abstract = {Economic management courses have the characteristics of specialty and systematization, which require higher comprehensive ability of students. In addition to mastering rich professional knowledge, they also need to use the second language for communication. Therefore, the construction of bilingual teaching mode has laid a foundation for the cultivation of modern comprehensive talents, which is conducive to promoting the overall development of students and improving the teaching level of economic management courses. Especially in the era of big data, we must change the traditional teaching ideas and methods to adapt to the requirements and characteristics of education reform. This paper will analyze the concept and level of bilingual teaching, put forward the necessity and existing problems of bilingual teaching of economic and management courses, and explore the construction strategy of bilingual teaching mode of economic management courses in the era of big data.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2021}, pages = {99\u2013102}, numpages = {4}, keywords = {Bilingual Teaching, Teaching Mode, Economic Management Courses, Big Data}}
@inproceedings{10.1145/2834118,title = {An Efficient Multidimensional Big Data Fusion Approach in Machine-to-Machine Communication}, author = {Ahmad Awais , Paul Anand , Rathore Mazhar , Chang Hangbae },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2834118}, doi = {10.1145/2834118}, abstract = {Machine-to-Machine communication (M2M) is nowadays increasingly becoming a world-wide network of interconnected devices uniquely addressable, via standard communication protocols. The prevalence of M2M is bound to generate a massive volume of heterogeneous, multisource, dynamic, and sparse data, which leads a system towards major computational challenges, such as, analysis, aggregation, and storage. Moreover, a critical problem arises to extract the useful information in an efficient manner from the massive volume of data. Hence, to govern an adequate quality of the analysis, diverse and capacious data needs to be aggregated and fused. Therefore, it is imperative to enhance the computational efficiency for fusing and analyzing the massive volume of data. Therefore, to address these issues, this article proposes an efficient, multidimensional, big data analytical architecture based on the fusion model. The basic concept implicates the division of magnitudes (attributes), i.e., big datasets with complex magnitudes can be altered into smaller data subsets using five levels of the fusion model that can be easily processed by the Hadoop Processing Server, resulting in formalizing the problem of feature extraction applications using earth observatory system, social networking, or networking applications. Moreover, a four-layered network architecture is also proposed that fulfills the basic requirements of the analytical architecture. The feasibility and efficiency of the proposed algorithms used in the fusion model are implemented on Hadoop single-node setup on UBUNTU 14.04 LTS core i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extracts various features (such as land and sea) from the massive volume of satellite data.}, pages = {1\u201325}, numpages = {25}, keywords = {data fusion, Hadoop processing server, Big Data, M2M}}
@inproceedings{10.1145/3372938.3372962,title = {Towards Swarm Intelligence Architectural Patterns: an IoT-Big Data-AI-Blockchain convergence perspective}, author = {Hibti Meryem , Ba\u00efna Karim , Benatallah Boualem },year = {2019}, isbn = {9781450372404}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3372938.3372962}, doi = {10.1145/3372938.3372962}, abstract = {The Internet of Things (IoT) is exploding. It is made up of billions of smart devices -from minuscule chips to mammoth machines - that use wireless technology to talk to each other (and to us). IoT infrastructures can vary from instrumented connected devices providing data externally to smart, and autonomous systems. To accompany data explosion resulting, among others, from IoT, Big data analytics processes examine large data sets to uncover hidden patterns, unknown correlations between collected events, either at a very technical level (incident/anomaly detection, predictive maintenance) or at business level (customer preferences, market trends, revenue opportunities) to provide improved operational efficiency, better customer service, competitive advantages over rival organizations, etc. In order to capitalize business value of the data generated by IoT sensors, IoT, Big Data Analytics/IA need to meet in the middle. One critical use case for IoT is to warn organizations when a product or service is at risk. The aim of this paper is to present a first proposal of IoT-Big Data-IA architectural patterns catalogues with a Blockchain implementation perspective in seek of design methodologies artifacts.}, location = {Rabat, Morocco}, series = {BDIoT'19}, pages = {1\u20138}, numpages = {8}, keywords = {AI, patterns, swarm intelligence, decision making, IoT, Big Data analytics}}
@inproceedings{10.1145/3236024.3264586,title = {BigSift: automated debugging of big data analytics in data-intensive scalable computing}, author = {Gulzar Muhammad Ali , Wang Siman , Kim Miryung },year = {2018}, isbn = {9781450355735}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3236024.3264586}, doi = {10.1145/3236024.3264586}, abstract = {Developing Big Data Analytics often involves trial and error debugging, due to the unclean nature of datasets or wrong assumptions made about data. When errors (e.g. program crash, outlier results, etc.) arise, developers are often interested in pinpointing the root cause of errors. To address this problem, BigSift takes an Apache Spark program, a user-defined test oracle function, and a dataset as input and outputs a minimum set of input records that reproduces the same test failure by combining the insights from delta debugging with data provenance. The technical contribution of BigSift is the design of systems optimizations that bring automated debugging closer to a reality for data intensive scalable computing. BigSift exposes an interactive web interface where a user can monitor a big data analytics job running remotely on the cloud, write a user-defined test oracle function, and then trigger the automated debugging process. BigSift also provides a set of predefined test oracle functions, which can be used for explaining common types of anomalies in big data analytics--for example, finding the origin of the output value that is more than k standard deviations away from the median. The demonstration video is available at https://youtu.be/jdBsCd61a1Q.}, location = {Lake Buena Vista, FL, USA}, series = {ESEC/FSE 2018}, pages = {863\u2013866}, numpages = {4}, keywords = {Automated debugging, data provenance, fault localization, data-intensive scalable computing (DISC), big data, and data cleaning}}
@inproceedings{10.1007/s00778-011-0219-9,title = {The SHARC framework for data quality in Web archiving}, author = {Denev Dimitar , Mazeika Arturas , Spaniol Marc , Weikum Gerhard },year = {2011}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg}, url = {https://doi.org/10.1007/s00778-011-0219-9}, doi = {10.1007/s00778-011-0219-9}, abstract = {Web archives preserve the history of born-digital content and offer great potential for sociologists, business analysts, and legal experts on intellectual property and compliance issues. Data quality is crucial for these purposes. Ideally, crawlers should gather coherent captures of entire Web sites, but the politeness etiquette and completeness requirement mandate very slow, long-duration crawling while Web sites undergo changes. This paper presents the SHARC framework for assessing the data quality in Web archives and for tuning capturing strategies toward better quality with given resources. We define data quality measures, characterize their properties, and develop a suite of quality-conscious scheduling strategies for archive crawling. Our framework includes single-visit and visit---revisit crawls. Single-visit crawls download every page of a site exactly once in an order that aims to minimize the \"blur\" in capturing the site. Visit---revisit strategies revisit pages after their initial downloads to check for intermediate changes. The revisiting order aims to maximize the \"coherence\" of the site capture(number pages that did not change during the capture). The quality notions of blur and coherence are formalized in the paper. Blur is a stochastic notion that reflects the expected number of page changes that a time-travel access to a site capture would accidentally see, instead of the ideal view of a instantaneously captured, \"sharp\" site. Coherence is a deterministic quality measure that counts the number of unchanged and thus coherently captured pages in a site snapshot. Strategies that aim to either minimize blur or maximize coherence are based on prior knowledge of or predictions for the change rates of individual pages. Our framework includes fairly accurate classifiers for change predictions. All strategies are fully implemented in a testbed and shown to be effective by experiments with both synthetically generated sites and a periodic crawl series for different Web sites.}, pages = {183\u2013207}, numpages = {25}, keywords = {Crawls strategies, Coherence, Blur, Web archiving, Data quality}}
@inproceedings{10.5555/3026877.3026922,title = {Big data analytics over encrypted datasets with seabed}, author = {Papadimitriou Antonis , Bhagwan Ranjita , Chandran Nishanth , Ramjee Ramachandran , Haeberlen Andreas , Singh Harmeet , Modi Abhishek , Badrinarayanan Saikrishna },year = {2016}, isbn = {9781931971331}, publisher = {USENIX Association}, address = {USA}, abstract = {Today, enterprises collect large amounts of data and leverage the cloud to perform analytics over this data. Since the data is often sensitive, enterprises would prefer to keep it confidential and to hide it even from the cloud operator. Systems such as CryptDB and Monomi can accomplish this by operating mostly on encrypted data; however, these systems rely on expensive cryptographic techniques that limit performance in true \"big data\" scenarios that involve terabytes of data or more.This paper presents Seabed, a system that enables efficient analytics over large encrypted datasets. In contrast to previous systems, which rely on asymmetric encryption schemes, Seabed uses a novel, additively symmetric homomorphic encryption scheme (ASHE) to perform large-scale aggregations efficiently. Additionally, Seabed introduces a novel randomized encryption scheme called Splayed ASHE, or SPLASHE, that can, in certain cases, prevent frequency attacks based on auxiliary data.}, location = {Savannah, GA, USA}, series = {OSDI'16}, pages = {587\u2013602}, numpages = {16}}
@inproceedings{10.1145/2670386.2670388,title = {Analysis of large call data records with big data}, author = {Goergen David , Mendiratta Veena , State Radu , Engel Thomas },year = {2014}, isbn = {9781450321242}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2670386.2670388}, doi = {10.1145/2670386.2670388}, abstract = {Mobile communication flows describe the on-going traffic on the network and are therefore a good indication of what is happening. Analysing these flows can improve the overall quality offered to the users and it can enable operators to detect abnormal patterns and react.This paper will focus on the analysis of cellular communications records. By using the collected call and message exchanges we present a method based on the PageRank algorithm that detects abnormal communications events. Taking the number of calls and the total call duration as parameters we use a weighted version of the PageRank algorithm to further investigate the influence of these parameters on the connected network graph. We proceed by correlating the results obtained with events happening in the respective region and at that time.}, location = {Chicago, Illinois}, series = {IPTComm '14}, pages = {1\u20136}, numpages = {6}, keywords = {page rank, call pattern detection, big data processing, call data record analysis}}
@inproceedings{10.1145/3460537.3460561,title = {Blockchain based Big Data Platform of City Brain}, author = {Liu Yu , Zeng Junfang },year = {2021}, isbn = {9781450389624}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3460537.3460561}, doi = {10.1145/3460537.3460561}, abstract = {Blockchain technology, with its distributed ledger, decentralization, high security, no tampering and other features, helps to solve the problems of data source confirmation, traceability and authorized data sharing. The effective utilization of data to achieve automatic governance and trusted decision-making of the city is known as \"City Brain\". The city brain is the key in smart city development, while the key to build a city brain is data resources. Aiming at the challenges existing in the construction of big data platform, this paper proposes a new solution based on blockchain, establishes the entity model and data model, analyzes the business model, designs the blockchain data platform framework and the cloud-blockchain integrated operation mode, and at last discusses the issues concerned in the application.}, location = {Shanghai, China}, series = {ICBCT '21}, pages = {82\u201389}, numpages = {8}, keywords = {Index on blockchain, Smart contract, Cloud-blockchain integrated, City brain}}
@inproceedings{10.1145/2743065.2743097,title = {A Comprehensive Survey on Variants And Its Extensions Of Big Data In Cloud Environment}, author = {Karthikeyan P. , Amudhavel J. , Abraham A. , Sathian D. , Raghav R. S. , Dhavachelvan P. },year = {2015}, isbn = {9781450334419}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2743065.2743097}, doi = {10.1145/2743065.2743097}, abstract = {As technology grows very fast with trendy outcome applications like Social networking, web analysis, bio-informatics network analysis, product analysis, etc., a huge amount of heterogeneous data is delivered in a wide range. Effective management of this huge data is interesting but faces many challenges in accuracy and processing. When a term huge data arrives then a recent and growing field namely BIG DATA comes into the act as it becomes a mass attracter of industry, academia and government for efficient processing of variety of huge data. This paper surveys a various technologies and the different areas where big data is implemented currently with a help of cloud environment [1] and its complete architecture [13]. Following it also explains about the different map reduce techniques and the framework that is being implanted for processing such huge data. Finally we discuss the future on big data processing with the cloud environment and the challenges [28] faced at these areas.}, location = {Unnao, India}, series = {ICARCSET '15}, pages = {1\u20135}, numpages = {5}, keywords = {Cloud computing, Hadoop, Big data, Security}}
@inproceedings{10.1145/872757.872875,title = {Data quality and data cleaning: an overview}, author = {Johnson Theodore , Dasu Tamraparni },year = {2003}, isbn = {158113634X}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/872757.872875}, doi = {10.1145/872757.872875}, abstract = {Data quality is a serious concern in any data-driven enterprise, often creating misleading findings during data mining, and causing process disruptions in operational databases. The manifestations of data quality problems can be very expensive- \"losing\" customers, \"misplacing\" billions of dollars worth of equipment, misallocated resources due to glitched forecasts, and so on. Solving data quality problems typically requires a very large investment of time and energy -- often 80% to 90% of a data analysis project is spent in making the data reliable enough that the results can be trusted.In this tutorial, we present a multi disciplinary approach to data quality problems. We start by discussing the meaning of data quality and the sources of data quality problems. We show how these problems can be addressed by a multidisciplinary approach, combining techniques from management science, statistics, database research, and metadata management. Next, we present an updated definition of data quality metrics, and illustrate their application with a case study. We conclude with a survey of recent database research that is relevant to data quality problems, and suggest directions for future research.}, location = {San Diego, California}, series = {SIGMOD '03}, pages = {681}, numpages = {1}}
@inproceedings{10.1145/2723372.2742784,title = {Why Big Data Industrial Systems Need Rules and What We Can Do About It}, author = {G.C. Paul Suganthan , Sun Chong , K. Krishna Gayatri , Zhang Haojun , Yang Frank , Rampalli Narasimhan , Prasad Shishir , Arcaute Esteban , Krishnan Ganesh , Deep Rohit , Raghavendra Vijay , Doan AnHai },year = {2015}, isbn = {9781450327589}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2723372.2742784}, doi = {10.1145/2723372.2742784}, abstract = {Big Data industrial systems that address problems such as classification, information extraction, and entity matching very commonly use hand-crafted rules. Today, however, little is understood about the usage of such rules. In this paper we explore this issue. We discuss how these systems differ from those considered in academia. We describe default solutions, their limitations, and reasons for using rules. We show examples of extensive rule usage in industry. Contrary to popular perceptions, we show that there is a rich set of research challenges in rule generation, evaluation, execution, optimization, and maintenance. We discuss ongoing work at WalmartLabs and UW-Madison that illustrate these challenges. Our main conclusions are (1) using rules (together with techniques such as learning and crowdsourcing) is fundamental to building semantics-intensive Big Data systems, and (2) it is increasingly critical to address rule management, given the tens of thousands of rules industrial systems often manage today in an ad-hoc fashion.}, location = {Melbourne, Victoria, Australia}, series = {SIGMOD '15}, pages = {265\u2013276}, numpages = {12}, keywords = {big data, classification, rule management}}
@inproceedings{10.5555/3204979.3205017,title = {A conceptual framework for designing a big data course}, author = {Singh Anshuman , Singh Sumi , Yousef Mahmoud },year = {2018}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {Big data is a fast changing area, and it is challenging to design a college course in big data that is up to date and is grounded in the fundamentals of the discipline. Some themes have emerged in the last few years that can be considered candidates for fundamentals of big data. In this paper, we identify and organize these fundamental concepts into a framework that can be used to design a big data course. We also present our course designs based on the framework and student feedback from our offerings in the last two years.}, pages = {192\u2013198}, numpages = {7}}
@inproceedings{10.1145/3349341.3349516,title = {Research on Knowledge Management System Construction of High-tech Enterprises Based on Big Data}, author = {Qi Baohua },year = {2019}, isbn = {9781450371506}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3349341.3349516}, doi = {10.1145/3349341.3349516}, abstract = {The wide application of big data technology is not only an important revolution in the field of information technology, but also a sharp tool to accelerate enterprise innovation worldwide. As an important main force of innovation in China, high-tech enterprises have the characteristics of knowledge-intensive and talent-intensive. Big data has brought about changes in thinking and technology for enterprise knowledge management. Under the background of big data era, the renewal of knowledge management concepts, the establishment of knowledge discovery system, knowledge integration and accumulation system, knowledge learning and application system and knowledge innovation system based on big data are effective measures to promote knowledge interaction and knowledge transformation of enterprises and enhance the efficiency of knowledge innovation.}, location = {Wuhan, Hubei, China}, series = {AICS 2019}, pages = {803\u2013807}, numpages = {5}, keywords = {High-tech enterprises, Big data, Knowledge management system}}
@inproceedings{10.1145/1951365.1951432,title = {Big data and cloud computing: current state and future opportunities}, author = {Agrawal Divyakant , Das Sudipto , El Abbadi Amr },year = {2011}, isbn = {9781450305280}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1951365.1951432}, doi = {10.1145/1951365.1951432}, abstract = {Scalable database management systems (DBMS)---both for update intensive application workloads as well as decision support systems for descriptive and deep analytics---are a critical part of the cloud infrastructure and play an important role in ensuring the smooth transition of applications from the traditional enterprise infrastructures to next generation cloud infrastructures. Though scalable data management has been a vision for more than three decades and much research has focussed on large scale data management in traditional enterprise setting, cloud computing brings its own set of novel challenges that must be addressed to ensure the success of data management solutions in the cloud environment. This tutorial presents an organized picture of the challenges faced by application developers and DBMS designers in developing and deploying internet scale applications. Our background study encompasses both classes of systems: (i) for supporting update heavy applications, and (ii) for ad-hoc analytics and decision support. We then focus on providing an in-depth analysis of systems for supporting update intensive web-applications and provide a survey of the state-of-the-art in this domain. We crystallize the design choices made by some successful systems large scale database management systems, analyze the application demands and access patterns, and enumerate the desiderata for a cloud-bound DBMS.}, location = {Uppsala, Sweden}, series = {EDBT/ICDT '11}, pages = {530\u2013533}, numpages = {4}}
@inproceedings{10.1145/3341161.3343518,title = {Multivariate motif detection in local weather big data}, author = {Xylogiannopoulos Konstantinos , Karampelas Panagiotis , Alhajj Reda },year = {2019}, isbn = {9781450368681}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341161.3343518}, doi = {10.1145/3341161.3343518}, abstract = {In recent years, there are very frequent reports of disasters attributed to the climate change and there are several reports that these extreme phenomena will further affect people not only as weather disasters but also indirectly with the shortage of natural resources such as water or food due to the climate change. Towards this direction, there is an on-going research that studies weather phenomena by collecting data not only in the surface of the globe but also at the different levels of the atmosphere. Having such a large volume of data, traditional numerical weather prediction models may not be able to assimilate those data and extract knowledge useful for the prediction of extreme phenomena. Thus, analysis of weather data has been transformed into a big data analytics problem which may enable weather scientists to better understand the interrelations of the weather variables and use the knowledge discovered to improve their prediction models. In this context, the current paper proposes a big data analytics methodology that is able to detect all common patterns between different weather variables in neighboring or distant points in a specific time window revealing useful associations between weather variables which is not possible to detect otherwise with the traditional numerical methods. The proposed methodology is based on a data structure that is able to store the magnitude of the weather data in different dimensions and a pattern detection algorithm which is able to detect all common patterns. The experimental results using weather data from the National Oceanic and Atmospheric Administration (NOAA) revealed interesting otherwise unknown patterns in two weather variables for two specific locations that were studied.}, location = {Vancouver, British Columbia, Canada}, series = {ASONAM '19}, pages = {749\u2013756}, numpages = {8}, keywords = {weather analysis, ARPaD, data mining, LERP-RSA}}
@inproceedings{10.1145/3511707,title = {Quality-Informed Process Mining: A Case for Standardised Data Quality Annotations}, author = {Goel Kanika , Leemans Sander J. J. , Martin Niels , Wynn Moe T. },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3511707}, doi = {10.1145/3511707}, abstract = {Real-life event logs, reflecting the actual executions of complex business processes, are faced with numerous data quality issues. Extensive data sanity checks and pre-processing are usually needed before historical data can be used as input to obtain reliable data-driven insights. However, most of the existing algorithms in process mining, a field focusing on data-driven process analysis, do not take any data quality issues or the potential effects of data pre-processing into account explicitly. This can result in erroneous process mining results, leading to inaccurate, or misleading conclusions about the process under investigation. To address this gap, we propose data quality annotations for event logs, which can be used by process mining algorithms to generate quality-informed insights. Using a design science approach, requirements are formulated, which are leveraged to propose data quality annotations. Moreover, we present the \u201cQuality-Informed visual Miner\u201d plug-in to demonstrate the potential utility and impact of data quality annotations. Our experimental results, utilising both synthetic and real-life event logs, show how the use of data quality annotations by process mining techniques can assist in increasing the reliability of performance analysis results.}, pages = {1\u201347}, numpages = {47}, keywords = {metadata, quality-informed conformance checking, Process mining, annotations, quality-informed performance analysis, data quality}}
@inproceedings{10.1145/3361785.3361803,title = {An empirical investigation on big data analytics (BDA) and innovation performance}, author = {Zareravasan Ahad , Ashrafi Amir },year = {2019}, isbn = {9781450372329}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3361785.3361803}, doi = {10.1145/3361785.3361803}, abstract = {Nowadays, big data analytics (BDA) have widely used in our business environment as an undeniable function for firms to not only survive in turbulence but also have the opportunity to be ahead of their major competitors. One of the promising aspects of BDA relates to its influence on innovation performance. In line, the present study proposed a conceptual model in order to investigate the relationship between BDA use and innovation performance by considering the role of dynamic capability (DC) theory. In this research, we consider firm agility in terms of DC theory and decompose it into three main factors contacting sensing agility, decision making agility, and acting agility. The research model and required data were analyzed using Partial Least Squares (PLS)/Structured Equation Modelling (SEM). The outcome of this study indicates that firms would be able to increase their innovation performance from a DC theory. This study also shows that BDA use has a positive influence on sensing agility of firms.}, location = {Paris, France}, series = {ICBIM '19}, pages = {97\u2013101}, numpages = {5}, keywords = {dynamic capabilities (DC), innovation performance, agility, big data analytics (BDA)}}
@inproceedings{10.1145/3523286.3524685,title = {Optimization of Big Data Mining Algorithm Based on Spark Framework: Preparation of Camera-Ready Contributions to SCITEPRESS Proceedings}, author = {Zeng Yan , Li Jun },year = {2022}, isbn = {9781450395755}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3523286.3524685}, doi = {10.1145/3523286.3524685}, abstract = {Abstract: Frequent itemsets mining is the core of association rule mining data. However, with the continuous increase of data, the traditional Apriori algorithm cannot meet people's daily needs, and the algorithm efficiency is low. This paper proposes the Eclat algorithm based on the Spark framework. In view of the shortcomings of serial algorithm in processing big data, it is modified. Using the vertical structure to avoid repetitive traversal of large amounts of data, while computing based on memory can greatly reduce I/O load and reduce computing time. Combined with the pruning strategy, the calculation of irrelevant itemsets is reduced, and the parallel computing capability of the algorithm is improved. The experimental results show that the efficiency of the Eclat algorithm based on the Spark framework is far better than that of the Eclat algorithm, and it has high efficiency and good scalability when processing massive data.}, location = {Harbin, China}, series = {BIC 2022}, pages = {333\u2013338}, numpages = {6}, keywords = {Spark framework, association rule mining, Eclat, big data}}
@inproceedings{10.1145/3152723.3152735,title = {Case-based Reasoning Intelligent Tutoring System: An Application of Big Data and IoT}, author = {Masood Mona , Mokmin Nur Azlina Mohamed },year = {2017}, isbn = {9781450353564}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3152723.3152735}, doi = {10.1145/3152723.3152735}, abstract = {The Big Data technology has paved a new way to provide inputs for the educational activities. The Case-based Reasoning (CBR) is an Artificial Intelligence (AI) algorithmthat is widely used for Big Data application. CBR has the ability to give a solution based on previous experiences like a human in making a decision. Thus, this study has designed and developed an Intelligent Tutoring System (ITS) that utilized the CBR to suggest the most suitable learning material to the students based on the real-time information of the students' profiles. The application has been tested in actual setting and the results show that the students that have been presented with personalized learning materials performed significantly better than the students that were presented with non-personalized lessons. The application also has the ability to calculate the similarity between cases accurately and presented the students with the most suitable learning materials.}, location = {Osaka, Japan}, series = {ICBDR 2017}, pages = {28\u201332}, numpages = {5}, keywords = {Big Data, Mathematics, Algebra, Learning Style, Intelligent Tutoring System, Case-based Reasoning, Self-Expressive, Instructional, Internet of Things, Understanding, Mastery, Artificial Intelligence, Interpersonal}}
@inproceedings{10.1145/2905055.2905326,title = {Big Data Analytics Based on In-Memory Infrastructure On Traditional HPC: A Survey}, author = {Shekhar Nikkita , Pawar Ambika V. },year = {2016}, isbn = {9781450339629}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2905055.2905326}, doi = {10.1145/2905055.2905326}, abstract = {As the capacity of main memory is growing, in-memory based big data analytics is becoming more popular. In-memory technologies support interactive analysis by providing high I/O throughput. On traditional high performance computing (HPC), big data processing needs data-intensive as well as computation-intensive systems for large data storage and high speed processing respectively. Currently, there are many such tools and technologies available which supports memory centric data processing to perform analysis on them. Taking advantage of in-memory on a HPC platform can result in a high speed, more reliable and fault tolerant data analysis. In this paper, we survey the existing storage and computation engines to perform big data analysis, and their performance while integrating together. Also, we discuss the contribution of such infrastructures in solving many I/O intensive analytical issues.}, location = {Udaipur, India}, series = {ICTCS '16}, pages = {1\u20135}, numpages = {5}, keywords = {I/O throughput, Big-data, Spark, High performance Computing, Analytics, In-memory, Hadoop, Tachyon}}
@inproceedings{10.1145/2963143,title = {The Six Pillars for Building Big Data Analytics Ecosystems}, author = {Khalifa Shadi , Elshater Yehia , Sundaravarathan Kiran , Bhat Aparna , Martin Patrick , Imam Fahim , Rope Dan , Mcroberts Mike , Statchuk Craig },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2963143}, doi = {10.1145/2963143}, abstract = {With almost everything now online, organizations look at the Big Data collected to gain insights for improving their services. In the analytics process, derivation of such insights requires experimenting-with and integrating different analytics techniques, while handling the Big Data high arrival velocity and large volumes. Existing solutions cover bits-and-pieces of the analytics process, leaving it to organizations to assemble their own ecosystem or buy an off-the-shelf ecosystem that can have unnecessary components to them. We build on this point by dividing the Big Data Analytics problem into six main pillars. We characterize and show examples of solutions designed for each of these pillars. We then integrate these six pillars into a taxonomy to provide an overview of the possible state-of-the-art analytics ecosystems. In the process, we highlight a number of ecosystems to meet organizations different needs. Finally, we identify possible areas of research for building future Big Data Analytics Ecosystems.}, pages = {1\u201336}, numpages = {36}, keywords = {analytics talent gap, consumable analytics, Orchestration}}
@inproceedings{10.1145/2661829.2661837,title = {Cleanix: A Big Data Cleaning Parfait}, author = {Wang Hongzhi , Li Mingda , Bu Yingyi , Li Jianzhong , Gao Hong , Zhang Jiacheng },year = {2014}, isbn = {9781450325981}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2661829.2661837}, doi = {10.1145/2661829.2661837}, abstract = {In this demo, we present Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate - the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. We demonstrate that Cleanix is a practical tool that supports effective and efficient data cleaning at the large scale.}, location = {Shanghai, China}, series = {CIKM '14}, pages = {2024\u20132026}, numpages = {3}, keywords = {data quality, big data, data cleaning}}
@inproceedings{10.1145/3344341.3368796,title = {An Evaluation of FaaS Platforms as a Foundation for Serverless Big Data Processing}, author = {Kuhlenkamp J\u00f6rn , Werner Sebastian , Borges Maria C. , El Tal Karim , Tai Stefan },year = {2019}, isbn = {9781450368940}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3344341.3368796}, doi = {10.1145/3344341.3368796}, abstract = {Function-as-a-Service (FaaS), offers a new alternative to operate cloud-based applications. FaaS platforms enable developers to define their application only through a set of service functions, relieving them of infrastructure management tasks, which are executed automatically by the platform. Since its introduction, FaaS has grown to support workloads beyond the lightweight use-cases it was originally intended for, and now serves as a viable paradigm for big data processing. However, several questions regarding FaaS platform quality are still unanswered. Specifically, the impact of automatic infrastructure management on serverless big data applications remains unexplored.In this paper, we propose a novel evaluation method (SIEM) to understand the impact of these tasks. For this purpose, we introduce new metrics to quantify quality in different big data application scenarios. We show an application of SIEM by evaluating the four major FaaS providers, and contribute results and new insights for FaaS-based big data processing.}, location = {Auckland, New Zealand}, series = {UCC'19}, pages = {1\u20139}, numpages = {9}, keywords = {serverless, big data processing, benchmarking, cloud computing}}
@inproceedings{10.1145/2791347.2791377,title = {Aggregation and multidimensional analysis of big data for large-scale scientific applications: models, issues, analytics, and beyond}, author = {Cuzzocrea Alfredo },year = {2015}, isbn = {9781450337090}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2791347.2791377}, doi = {10.1145/2791347.2791377}, abstract = {Aggregation and multidimensional analysis are well-known powerful tools for extracting useful knowledge, shaped in a summarized manner, which are being successfully applied to the annoying problem of managing and mining big data produced by large-scale scientific applications. Indeed, in the context of big data analytics, aggregation approaches allow us to provide meaningful descriptions of these data, otherwise impossible for alternative data-intensive analysis tools. On the other hand, multidimensional analysis methodologies introduce fortunate metaphors that significantly empathize the knowledge discovery phase from such huge amounts of data. Following this main trend, several big data aggregation and multidimensional analysis approaches have been proposed recently. The goal of this paper is to (i) provide a comprehensive overview of state-of-the-art techniques and (ii) depict open research challenges and future directions adhering to the reference scientific field.}, location = {La Jolla, California}, series = {SSDBM '15}, pages = {1\u20136}, numpages = {6}, keywords = {big data analytics, multidimensional analysis of big data, large-scale scientific applications, big data aggregation}}
@inproceedings{10.1145/3342827.3342843,title = {Big Data Framework for Scalable and Efficient Biomedical Literature Mining in the Cloud}, author = {Shen Zhengru , Wang Xi , Spruit Marco },year = {2019}, isbn = {9781450362795}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3342827.3342843}, doi = {10.1145/3342827.3342843}, abstract = {The massive size of available biomedical literature requires researchers to utilize novel big data technologies in data storage and analysis. Among them is cloud computing which has become the most popular solution for big data applications in industry. However, many bioinformaticians still rely on expensive and inefficient in-house infrastructure to discover knowledge from biomedical literature. Although some cloud-based solutions were constructed recently, they failed to sufficiently address a few key issues including scalability, flexibility, and reusability. Moreover, no study has taken computational cost into consideration. To fill the gap, we proposed a cloud-based big data framework that enables researchers to perform reproducible and scalable large-scale biomedical literature mining in an efficient and cost-effective way. Additionally, a cloud agnostic platform was constructed and then evaluated on two open access corpora with millions of full-text biomedical articles. The results indicate that our framework supports scalable and efficient large-scale biomedical literature mining.}, location = {Tokushima, Japan}, series = {NLPIR 2019}, pages = {80\u201386}, numpages = {7}, keywords = {document classification, text mining, big data, biomedical literature, topic modeling, cloud computing}}