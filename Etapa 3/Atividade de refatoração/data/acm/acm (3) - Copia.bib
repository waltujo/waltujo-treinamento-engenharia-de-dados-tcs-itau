@article{10.1145/62065.62068,
author = {Ballou, Donald P. and Tayi, Giri Kumar.},
title = {Methodology for Allocating Resources for Data Quality Enhancement},
year = {1989},
issue_date = {March 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/62065.62068},
doi = {10.1145/62065.62068},
abstract = {The issue of effective use of resources committed to data integrity maintenance is addressed via a heuristic which uses parameter estimates that are iteratively refined to partition available resources among multiple data sets.},
journal = {Commun. ACM},
month = {mar},
pages = {320–329},
numpages = {10}
}

@inproceedings{10.1145/3377817.3377836,
author = {Otoo-Arthur, David and Van Zyl, Terence},
title = {A Systematic Review on Big Data Analytics Frameworks for Higher Education - Tools and Algorithms},
year = {2020},
isbn = {9781450366496},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377817.3377836},
doi = {10.1145/3377817.3377836},
abstract = {The development of Big Data applications in education has drawn much attention in the last few years due to the enormous benefits it brings to improving teaching and learning. The integration of these Big Data applications in education generates massive data that put new demands to available processing technologies of data and extraction of useful information. Primarily, several higher educational institutions depend on the knowledge mined from these vast volumes of data to optimise the teaching and learning environment. However, Big Data in the higher education context has relied on traditional data techniques and platforms that are less efficient. This paper, therefore, conducts a Systematic Literature Review (SLR) that examines Big Data framework technologies in higher education outlining gaps that need a solution in Big Educational Data Analytics. We achieved this by summarising the current knowledge on the topic and recommend areas where educational institutions could focus on exploring the potential of Big Data Analytics. To this end, we reviewed 55 related articles out of 1543 selected from Six (6) accessible Computer Science databases between the period of 2007 and 2018, focusing on the development of the Big Data framework and its applicability in education for academic purposes. Our results show that very few researchers have tried to address the integrative use of Big Data framework and learning analytics in higher education. The review further suggests that there is an emerging best practice in applying Big Data Analytics to improve teaching and learning. However, this information does not appear to have been thoroughly examined in higher education. Hence, there is the need for a complete investigation to come up with comprehensive Big Data frameworks that build effective learning systems for instructors, learners, course designers and educational administrators.},
booktitle = {Proceedings of the 2019 2nd International Conference on E-Business, Information Management and Computer Science},
articleno = {15},
numpages = {9},
keywords = {MS, Data Mining, Big Educational Data, Big Data, Learning Analytics, Higher Education},
location = {Kuala Lumpur, Malaysia},
series = {EBIMCS '19}
}

@inproceedings{10.1145/3254563,
author = {Pollard, Tom},
title = {Session Details: Session 20: Big Data in Bioinformatics II},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254563},
doi = {10.1145/3254563},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1145/2980258.2980301,
author = {Chandrasekaran, Balaji and Balakrishnan, Ramadoss},
title = {Attribute Based Encryption Using Quadratic Residue for the Big Data in Cloud Environment},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2980301},
doi = {10.1145/2980258.2980301},
abstract = {Big data is the next frontier for modernization, rivalry, and profitability. It is the foundation of all the major trends such as social networks, mobile devices, healthcare, stock markets etc. Big data is efficiently stored in the cloud because of its high-volume, high-speed and high-assortment data resources. An unauthorized user access control is the gravest threat of huge information in the cloud environment because of the remote file storage. Attribute Based Encryption (ABE) is an efficient access control procedure to guarantee end-to-end security for huge information in the cloud. Most often existing ABE working principle is based on bilinear pairing. In this paper, we construct a peculiar ABE for big data in the cloud. Our proposed scheme is based on quadratic residue and attribute union which is based on fundamental arithmetic theorem.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {19},
numpages = {4},
keywords = {Cloud, Big Data, Quadratic Residue, ABE, Bilinear Pairing},
location = {Pondicherry, India},
series = {ICIA-16}
}

@proceedings{10.1145/3175684,
title = {BDIOT2017: Proceedings of the International Conference on Big Data and Internet of Thing},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {BDIOT provides a scientific platform for both local and international scientists, engineers and technologists who work in all aspects of big data and internet of things. In addition to the contributed papers, internationally known experts from several countries are also invited to deliver keynote speeches at BDIOT 2017.},
location = {London, United Kingdom}
}

@inproceedings{10.1145/3299869.3328524,
author = {Ketsman, Bas},
title = {Formal Approaches to Querying Big Data in Shared-Nothing Systems},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3328524},
doi = {10.1145/3299869.3328524},
abstract = {To meet today's data management needs, it is a widespread practice to use distributed data storage and processing systems. Since the publication of the MapReduce paradigm, a plethora of such systems arose, but although widespread, the capabilities of these systems are still poorly understood and putting them to effective use is often more of an art than a science. As one of the causes for this observation, we identify a lack of theoretical underpinnings for these systems, which makes it hard to understand what the advantages and disadvantages of the particular systems are and which, in addition, complicates the choice of a particular formalism for a particular task. In my PhD thesis, we zoom in on several important aspects of query evaluation using clusters of servers, including coordination and communication, data-skew, load balancing, and data partitioning, and propose a set of elegant and theoretically sound frameworks and theories that help to understand the applicable limitations and trade-offs.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1115–1116},
numpages = {2},
keywords = {distributed query evaluation, communication, coordination, worst-case optimality, shared-nothing systems},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/2933267.2933539,
author = {Della Valle, Emanuele and Dell'Aglio, Daniele and Margara, Alessandro},
title = {Taming Velocity and Variety Simultaneously in Big Data with Stream Reasoning: Tutorial},
year = {2016},
isbn = {9781450340212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933267.2933539},
doi = {10.1145/2933267.2933539},
abstract = {Many "big data" applications must tame velocity (processing data in-motion) and variety (processing many different types of data) simultaneously.The research on knowledge representation and reasoning has focused on the variety of data, devising data representation and processing techniques that promote integration and reasoning on available data to extract implicit information. On the other hand, the event and stream processing community has focused on the velocity of data, producing systems that efficiently operate on streams of data on-the-fly according to pre-deployed processing rules or queries. Several recent works explore the synergy between stream processing and reasoning to fully capture the requirements of modern data intensive applications, thus giving birth to the research domain of stream reasoning.This tutorial paper offers an overview of the theoretical and technological achievements in stream reasoning, highlighting the key benefits and limitations of existing approaches, and discussing the open challenges and the opportunities for future research. The paper mainly targets researchers and practitioners in the area of event and stream processing. The paper aims to stimulate the discussion on stream reasoning and to further promote the integration of reasoning techniques within event and stream processing systems in three ways: (i) by presenting an active research domain, where researchers on event and stream processing can apply their expertise; (ii) by discussing techniques and technologies that can help advancing the state of the art in event and stream processing; (iii) by identifying the open problems in the field of stream reasoning, and drawing attention to promising research directions.},
booktitle = {Proceedings of the 10th ACM International Conference on Distributed and Event-Based Systems},
pages = {394–401},
numpages = {8},
keywords = {stream processing, event processing, reasoning, stream reasoning, complex event processing},
location = {Irvine, California},
series = {DEBS '16}
}

@inproceedings{10.1145/3383972.3384034,
author = {Feng, Mingchen and Zheng, Jiangbin and Ren, Jinchang and Liu, Yanqin},
title = {Towards Big Data Analytics and Mining for UK Traffic Accident Analysis, Visualization &amp; Prediction},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384034},
doi = {10.1145/3383972.3384034},
abstract = {Road traffic accident (RTA) is a big issue to our society due to it is among the main causes of traffic congestion, human death, health problems, environmental pollution, and economic losses. Facing these fatal and unexpected traffic accidents, understanding what happened and discover factors that relate to them and then make alarms in advance play critical roles for possibly effective traffic management and reduction of accidents. This paper presents our work to establish a novel big data analytics platform for UK traffic accident analysis using machine learning and deep learning techniques. Our system consists of three parts in which we first cluster accident incidents in an interactive Google map to highlight some hotspots and then narratively visualize accident attributes to uncover potentially related factors, finally we explored several state-of-the-art machine learning, deep learning and time series forecasting models to predict the number of road accidents in the future. The experimental results show that our big data processing platform can not only effectively handle large amount of data but also give new insights into what happened and reasonably prediction of what will happen in the future to assist decision making, which will undoubtedly show its great value as a generic platform for other big data analytics fields.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {225–229},
numpages = {5},
keywords = {Big Data Analytics, Deep Learning, Time series Forecasting, Traffic Accident Analysis},
location = {Shenzhen, China},
series = {ICMLC 2020}
}

@inproceedings{10.1145/3195612.3195621,
author = {Srivastava, Riktesh},
title = {Exploration of In-Memory Computing for Big Data Analytics Using Queuing Theory},
year = {2018},
isbn = {9781450363372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195612.3195621},
doi = {10.1145/3195612.3195621},
abstract = {Assigning suitable memory chunk for Big Data analysis is posing serious problems for Business Analysts. There are plentiful solutions that came along to solve the issue of memory management. The noteworthy solutions to the problems included JVM based and Container based solutions. However, both of these solutions suffered from disk I/O bottleneck. To reduce disk, I/O bottleneck, in-memory system was introduced, which supports interactive data analytics. Present study conducts request time processing for in-memory system using three types of queue models- MG1, GM1 and GG1.},
booktitle = {Proceedings of the 2nd International Conference on High Performance Compilation, Computing and Communications},
pages = {11–16},
numpages = {6},
keywords = {G/G/1 queue, G/M/1 queue, M/M/1 queue, M/G/1 queue, in-memory computing (IMC)},
location = {Hong Kong, Hong Kong},
series = {HP3C}
}

@article{10.1145/2834118,
author = {Ahmad, Awais and Paul, Anand and Rathore, Mazhar and Chang, Hangbae},
title = {An Efficient Multidimensional Big Data Fusion Approach in Machine-to-Machine Communication},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/2834118},
doi = {10.1145/2834118},
abstract = {Machine-to-Machine communication (M2M) is nowadays increasingly becoming a world-wide network of interconnected devices uniquely addressable, via standard communication protocols. The prevalence of M2M is bound to generate a massive volume of heterogeneous, multisource, dynamic, and sparse data, which leads a system towards major computational challenges, such as, analysis, aggregation, and storage. Moreover, a critical problem arises to extract the useful information in an efficient manner from the massive volume of data. Hence, to govern an adequate quality of the analysis, diverse and capacious data needs to be aggregated and fused. Therefore, it is imperative to enhance the computational efficiency for fusing and analyzing the massive volume of data. Therefore, to address these issues, this article proposes an efficient, multidimensional, big data analytical architecture based on the fusion model. The basic concept implicates the division of magnitudes (attributes), i.e., big datasets with complex magnitudes can be altered into smaller data subsets using five levels of the fusion model that can be easily processed by the Hadoop Processing Server, resulting in formalizing the problem of feature extraction applications using earth observatory system, social networking, or networking applications. Moreover, a four-layered network architecture is also proposed that fulfills the basic requirements of the analytical architecture. The feasibility and efficiency of the proposed algorithms used in the fusion model are implemented on Hadoop single-node setup on UBUNTU 14.04 LTS core i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extracts various features (such as land and sea) from the massive volume of satellite data.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jun},
articleno = {39},
numpages = {25},
keywords = {Big Data, M2M, Hadoop processing server, data fusion}
}

@inproceedings{10.1145/3419604.3419620,
author = {Amazal, Houda and Ramdani, Mohammed and Kissi, Mohamed},
title = {A Frequency-Category Based Feature Selection in Big Data for Text Classification},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419620},
doi = {10.1145/3419604.3419620},
abstract = {In big data era, text classification is considered as one of the most important machine learning application domain. However, to build an efficient algorithm for classification, feature selection is a fundamental step to reduce dimensionality, achieve better accuracy and improve time execution. In the literature, most of the feature ranking techniques are document based. The major weakness of this approach is that it favours the terms occurring frequently in the documents and neglects the correlation between the terms and the categories. In this work, unlike the traditional approaches which deal with documents individually, we use mapreduce paradigm to process the documents of each category as a single document. Then, we introduce a parallel frequency-category feature selection method independently of any classifier to select the most relevant features. Experimental results on the 20-Newsgroups dataset showed that our approach improves the classification accuracy to 90.3%. Moreover, the system maintains the simplicity and lower execution time.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {14},
numpages = {6},
keywords = {Machine Learning, Text classification, MapReduce, Naive Bayes, Big Data, TF-IDF},
location = {Rabat, Morocco},
series = {SITA'20}
}

@article{10.14778/2536222.2536243,
author = {Tran, Nga and Bodagala, Sreenath and Dave, Jaimin},
title = {Designing Query Optimizers for Big Data Problems of the Future},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536222.2536243},
doi = {10.14778/2536222.2536243},
abstract = {The Vertica SQL Query Optimizer was written from the ground up for the Vertica Analytic Database. Its design, and the tradeoffs we encountered during implementation, support the case that the full power of novel database systems can be realized only with a custom Query Optimizer, carefully crafted exclusively for the system in which it operates.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1168–1169},
numpages = {2}
}

@inproceedings{10.1145/3372454.3372462,
author = {Wibisono, Ari and Adibah, Jihan and Mursanto, Petrus and Saputri, Mei Silviana},
title = {Improvement of Big Data Stream Mining Technique for Automatic Bone Age Assessment},
year = {2020},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372462},
doi = {10.1145/3372454.3372462},
abstract = {Rapid technology growth has stimulated automated systems development in the medical field. Automatic bone age evaluation is an example of the implementation of this technology in the medical field. The automated assessment is done based on the left-hand X-ray images. This assessment helps the radiologist and pediatrician evaluate children's growth. A system that can generate a precise and reliable prediction is essential. Thus the main challenge is to determine the suitable technology that can generate a reliable forecast, mainly when working with a large quantity of data. Big data is a growing trend; practical computing challenges created by data streams can be found in several types of applications. It is known that data streams are usually obtained from sensors and monitors which accumulatively can make data very large in volume. This can result in the inability of real-time processing to be carried out. In this paper, the data stream technique is utilized to assess and predict bone ages. The analysis process is carried out in real-time when the data arrives so that the process of storing new data is done after the data is analyzed. A 9 GB sized-dataset consisting of 12,611 images were used. The images have various resolutions. We extracted and analyzed image features by using Canny Edge feature extraction. To predict bone age from those extracted features, we enhance the data stream mining technique base on tree stream mining. We use MAE or Mean Absolute Error, RMSE or Root Mean Squared Error, and MAPE or Mean Absolute Percentage Error as metrics in this measurement. The outcomes of our experiment show that our approach in data stream mining can increase performance measurements. The MAPE of our approach gives a 7% lower error evaluation compared to the standard method.},
booktitle = {Proceedings of the 2019 3rd International Conference on Big Data Research},
pages = {119–123},
numpages = {5},
keywords = {Data Stream Mining, Bone Age, Big Data, Decision Support System, FIMT-DD},
location = {Cergy-Pontoise, France},
series = {ICBDR 2019}
}

@inproceedings{10.1145/3349341.3349431,
author = {Wang, Xiaodong and Wang, Qing and Tao, Ye},
title = {A User Profile Analysis Framework Driven by Distributed Machine Learning for Big Data},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349431},
doi = {10.1145/3349341.3349431},
abstract = {In recent years, big data has become the new focus of attention from all walks of life. The valuable information contained in big data becomes the driving force for people to process and analyze big data. Big data analytics helps enterprises to take better decisions to improve business output. As a user description tool, user profile is widely used in various fields. However, it is difficult to deal with large-scale datasets using traditional methods since the established processes was not designed to handle large volumes of data. In this paper, we propose a user profile analysis framework using machine learning approach which apply advanced machine learning programs to solve industrial scale problems. And this approach can be effective to speculate real and potential needs of various groups of users and precisely extract individual characteristics and group generality. By introducing high-level data parallel framework, the process of large-scale data processing can be executed efficiently. We use real-world data to validate the effectiveness of the proposed framework.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {358–363},
numpages = {6},
keywords = {Big Data Analysis, Distributed Machine Learning, User Profile, Computing Framework},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/2983402.2983431,
author = {Sidhu, Ravneet Kaur and Saroa, Charanjiv Singh},
title = {Efficient Batch Processing of Related Big Data Tasks Using Persistent MapReduce Technique},
year = {2016},
isbn = {9781450343015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983402.2983431},
doi = {10.1145/2983402.2983431},
abstract = {The data generated by today's enterprises has been increasing at exponential rates in size from most recent couple of years. Also, the need to process and break down the substantial volumes of data has likewise expanded. In order to handle this enormous amount of data and to analyze the same, an open-source usage of Apache system, Hadoop is utilized now-a-days. Hadoop presented a utility computing model which offer replacement of traditional databases and processing techniques. Scalability and high availability of MapReduce makes it the first choice for big data analysis. This paper provides a brief introduction to HDFS and MapReduce. After studying them in detail, it later made to work on related tasks and store the cached result of mapper function which can be used as an input for general reducers. By this additional triggering agent, we were able to achieve the analysis result in approximately half the actual time.},
booktitle = {Proceedings of the Third International Symposium on Computer Vision and the Internet},
pages = {106–109},
numpages = {4},
keywords = {Hadoop, Big Data, HDFS, MapReduce},
location = {Jaipur, India},
series = {VisionNet'16}
}

@inproceedings{10.1145/3268808.3268832,
author = {Lu, Ming-Che and Chen, Yi-Xuan and Yang, Yu-Ying and Sha, Min-Xuan and Chen, Yan-Wei and Lin, Sih-Ling},
title = {Tax Reduction and Corporate Investment - Applying Big Data to Tax Policy Formulation},
year = {2018},
isbn = {9781450365284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268808.3268832},
doi = {10.1145/3268808.3268832},
abstract = {In this paper, we apply big data to the tax reform evaluation for the Ministry of Finance of the Republic of China. From empirical data, we find strong evidences that the reduction of profit-seeking enterprise income tax rate indeed improves corporate investment. Some sensitivity tests are performed to ensure our results robust.},
booktitle = {Proceedings of the 2nd International Conference on E-Society, E-Education and E-Technology},
pages = {103–106},
numpages = {4},
keywords = {Profit-seeking enterprise income tax, Corporate investment, Government policy},
location = {Taipei, Taiwan},
series = {ICSET 2018}
}

@inproceedings{10.1145/3361785.3361786,
author = {Alberic, Minno Dekassan and YeZheng, Liu and Rodrigue, Dibonji Ndjansse Stephane and Vellem, Vuyolwethu},
title = {Research on Innovation of Cross-Border e-Commerce Business Model Based on Big Data},
year = {2019},
isbn = {9781450372329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361785.3361786},
doi = {10.1145/3361785.3361786},
abstract = {With the rapid development of the economy and the continuous deepening of the application of Internet technology, today's social economy has gradually entered the era of big data, which provides a good social environment for the development of e-commerce. This paper first introduces the background and significance of big data, and analyzes the connotation and advantages of e-commerce in the era of big data. On the basis of analyzing the development status of e-commerce industry under the background of big data, it summarizes the opportunities and challenges faced by e-commerce enterprises in the context of big data, and discusses the e-commerce service model under the era of big data. The main purpose of this research is to contribute to the improvement of e-commerce service level in the era of big data and promote the rapid development of e-commerce.},
booktitle = {Proceedings of the 3rd International Conference on Business and Information Management},
pages = {1–4},
numpages = {4},
keywords = {big data, business model, cross-border e-commerce},
location = {Paris, France},
series = {ICBIM '19}
}

@inproceedings{10.1145/2396556.2396578,
author = {Zahavi, Eitan and Keslassy, Isaac and Kolodny, Avinoam},
title = {Distributed Adaptive Routing for Big-Data Applications Running on Data Center Networks},
year = {2012},
isbn = {9781450316859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396556.2396578},
doi = {10.1145/2396556.2396578},
abstract = {With the growing popularity of big-data applications, Data Center Networks increasingly carry larger and longer traffic flows. As a result of this increased flow granularity, static routing cannot efficiently load-balance traffic, resulting in an increased network contention and a reduced throughput. Unfortunately, while adaptive routing can solve this load-balancing problem, network designers refrain from using it, because it also creates out-of-order packet delivery that can significantly degrade the reliable transport performance of the longer flows. In this paper, we show that by throttling each flow bandwidth to half of the network link capacity, a distributed-adaptive-routing algorithm is able to converge to a non-blocking routing assignment within a few iterations, causing minimal out-of-order packet delivery. We present a Markov chain model for distributed-adaptive-routing in the context of Clos networks that provides an approximation for the expected convergence time. This model predicts that for full-link-bandwidth traffic, the convergence time is exponential with the network size, so out-of-order packet delivery is unavoidable for long messages. However, with half-rate traffic, the algorithm converges within a few iterations and exhibits weak dependency on the network size. Therefore, we show that distributed-adaptive-routing may be used to provide a scalable and non-blocking routing even for long flows on a rearrangeably-non-blocking Clos network under half-rate conditions. The proposed model is evaluated and approximately fits the abstract system simulation model. Hardware implementation guidelines are provided and evaluated using a detailed flit-level InfiniBand simulation model. These results directly apply to adaptive-routing systems designed and deployed in various fields.},
booktitle = {Proceedings of the Eighth ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
pages = {99–110},
numpages = {12},
keywords = {adaptive routing, data center networks, big-data},
location = {Austin, Texas, USA},
series = {ANCS '12}
}

@inproceedings{10.1145/2533888.2533939,
author = {Cruz, Isabel F. and Ganesh, Venkat R. and Mirrezaei, Seyed Iman},
title = {Semantic Extraction of Geographic Data from Web Tables for Big Data Integration},
year = {2013},
isbn = {9781450322416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2533888.2533939},
doi = {10.1145/2533888.2533939},
abstract = {There are millions of web tables with geographic data that are pertinent for big data integration in a variety of domain applications, such as urban sustainability, transportation networks, policy studies, and public health. These tables, however, are heterogeneous in structure, concepts, and metadata. One of the challenges in semantically extracting geographic data is the need to resolve these heterogeneities so as to uncover a conceptual hierarchy, metadata associated with instances, and geographic information---corresponding respectively to ontologies, elements that we call features, and cell values that can be used to identify geographic coordinates. In this paper, we present an architecture with methods to: (1) extract feature-rich web tables; (2) identify features; (3) construct a schema and instances using RDF; (4) perform geocoding. Preliminary experiments led to high accuracy in table identification and feature naming even when compared to manual evaluation.},
booktitle = {Proceedings of the 7th Workshop on Geographic Information Retrieval},
pages = {19–26},
numpages = {8},
keywords = {geocoding, GIS, spatial databases, semantic data integration, web tables, information extraction, geographic data},
location = {Orlando, Florida},
series = {GIR '13}
}

@inproceedings{10.1145/3404687.3404693,
author = {Xin, Li and Tianyun, Shi and Xiaoning, Ma},
title = {Research on the Big Data Platform and Its Key Technologies for the Railway Locomotive System},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404693},
doi = {10.1145/3404687.3404693},
abstract = {In order to improve the efficiency of locomotive organization and the quality of locomotive operation, this paper analyzes and discusses the big data platform and some key technologies suitable for the big data application of the railway locomotive system. Firstly, the definition of big data of the railway locomotive system is proposed, and the current data characteristics of the railway locomotive system are summarized, then the status quo and demands of big data application of the railway locomotive system are analyzed. Secondly, the overall architecture of the big data platform for the railway locomotive system is proposed. Furthermore, seven application scenarios available for the big data platform are analyzed, including locomotive running organization, high-speed railway, repair, maintenance and other fields. Finally, some key technologies, which consist of data collection system of front-line operations, locomotive equipment portrait analysis, staff portrait analysis, transmission and analysis of locomotive video, intelligent auxiliary driving system, are provided to increase efficiency of the locomotive organization and capability of safety management. The obtained results can play a positive role in the construction and application of big data of the railway locomotive system.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {6–12},
numpages = {7},
keywords = {Key technology, Big data, Railway, Locomotive system, Application platform},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1145/3219104.3229276,
author = {Rivera, Sergio and Griffioen, James and Fei, Zongming and Hayashida, Mami and Shi, Pinyi and Chitre, Bhushan and Chappell, Jacob and Song, Yongwook and Pike, Lowell and Carpenter, Charles and Nasir, Hussamuddin},
title = {Navigating the Unexpected Realities of Big Data Transfers in a Cloud-Based World},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3229276},
doi = {10.1145/3219104.3229276},
abstract = {The emergence of big data has created new challenges for researchers transmitting big data sets across campus networks to local (HPC) cloud resources, or over wide area networks to public cloud services. Unlike conventional HPC systems where the network is carefully architected (e.g., a high speed local interconnect, or a wide area connection between Data Transfer Nodes), today's big data communication often occurs over shared network infrastructures with many external and uncontrolled factors influencing performance.This paper describes our efforts to understand and characterize the performance of various big data transfer tools such as rclone, cyberduck, and other provider-specific CLI tools when moving data to/from public and private cloud resources. We analyze the various parameter settings available on each of these tools and their impact on performance. Our experimental results give insights into the performance of cloud providers and transfer tools, and provide guidance for parameter settings when using cloud transfer tools. We also explore performance when coming from HPC DTN nodes as well as researcher machines located deep in the campus network, and show that emerging SDN approaches such as the VIP Lanes system can deliver excellent performance even from researchers' machines.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {22},
numpages = {8},
keywords = {Software-Defined Networks, Big Data Flows, Data Transfer Tools},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@inproceedings{10.1145/2695664.2696078,
author = {Rekha, A. G.},
title = {A Fast Support Vector Data Description System for Anomaly Detection Using Big Data},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2696078},
doi = {10.1145/2695664.2696078},
abstract = {Anomaly (or outlier) detection is one of the most studied data mining techniques due to its importance and inherent challenges. Recently, Support Vector Data Description (SVDD) driven approaches are shown as having good predictive accuracy. 'Big Data' as input enhances the inferential power of algorithms, but it challenges even state-of-the-art computation methods. The runtime complexity of SVDD is linear in the number of support vectors. Hence speeding up the decision function becomes important for applications requiring fast real time performance. This work aims to provide solutions that reduce the computational complexity of SVDD so as to make one-class classification practical on Big Data. The proposed work first reduces the complexity of SVDD by computing a point known as the agent of the SVDD sphere center in the input space during the training phase. It retains the benefit of the kernel trick also. We then propose to hadoopize this lightly trained SVDD named as LT-SVDD so that a faster classification can be achieved in a big-data setting.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {931–932},
numpages = {2},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3093338.3104155,
author = {Rivera, Sergio and Hayashida, Mami and Griffioen, James and Fei, Zongming},
title = {Dynamically Creating Custom SDN High-Speed Network Paths for Big Data Science Flows},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3104155},
doi = {10.1145/3093338.3104155},
abstract = {Existing campus network infrastructure is not designed to effectively handle the transmission of big data sets. Performance degradation in these networks is often caused by middleboxes -- appliances that enforce campus-wide policies by deeply inspecting all traffic going through the network (including big data transmissions). We are developing a Software-Defined Networking (SDN) solution for our campus network that grants privilege to science flows by dynamically calculating routes that bypass certain middleboxes to avoid the bottlenecks they create. Using the global network information provided by an SDN controller, we are developing graph databases approaches to compute custom paths that not only bypass middleboxes to achieve certain requirements (e.g., latency, bandwidth, hop-count) but also insert rules that modify packets hop-by-hop to create the illusion of standard routing/forward despite the fact that packets are being rerouted. In some cases, additional functionality needs to be added to the path using network function virtualization (NFV) techniques (e.g., NAT). To ensure that path computations are run on an up-to-date snapshot of the topology, we introduce a versioning mechanism that allows for lazy topology updates that occur only when "important" network changes take place and are requested by big data flows.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {59},
numpages = {4},
keywords = {Big Data Flows, Software-Defined Networks, Path Calculation},
location = {New Orleans, LA, USA},
series = {PEARC17}
}

@inproceedings{10.1145/3349341.3349516,
author = {Qi, Baohua},
title = {Research on Knowledge Management System Construction of High-Tech Enterprises Based on Big Data},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349516},
doi = {10.1145/3349341.3349516},
abstract = {The wide application of big data technology is not only an important revolution in the field of information technology, but also a sharp tool to accelerate enterprise innovation worldwide. As an important main force of innovation in China, high-tech enterprises have the characteristics of knowledge-intensive and talent-intensive. Big data has brought about changes in thinking and technology for enterprise knowledge management. Under the background of big data era, the renewal of knowledge management concepts, the establishment of knowledge discovery system, knowledge integration and accumulation system, knowledge learning and application system and knowledge innovation system based on big data are effective measures to promote knowledge interaction and knowledge transformation of enterprises and enhance the efficiency of knowledge innovation.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {803–807},
numpages = {5},
keywords = {High-tech enterprises, Big data, Knowledge management system},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3210506.3210514,
author = {Fang, Yinjie},
title = {The Real Option Approach for Assessment of Big Data Asset Based on Prospect Theory},
year = {2018},
isbn = {9781450363808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210506.3210514},
doi = {10.1145/3210506.3210514},
abstract = {This paper combines prospect theory with real option pricing model to construct a new value assessment model for big data assets. For the high uncertainty of future earnings and risk of big data assets, this paper analyzes that their value characteristics are in line with the American call option firstly. On this basis, we use the value function in the prospect theory to calculate decision makers' subjective judgments on the value of underlying big data assets under each state, and use the weighting function to calculate the decision makers' subjective judgment on the weight of expansion right, downsize right and abandon right. In example, we use least-squares Monte Carlo simulation method to perform a simulation which verifies the real option pricing method based on perspective of prospect theory can obtain more reasonable assessment result for big data assets.},
booktitle = {Proceedings of the 2018 International Conference on E-Business, Information Management and Computer Science},
pages = {40–45},
numpages = {6},
keywords = {Assessment, Prospect Theory, Big Data Asset, Real Option, Big Data},
location = {Hong Kong, Hong Kong},
series = {EBIMCS '18}
}

@inproceedings{10.1145/3377672.3378049,
author = {Jie, Liu},
title = {Knowledge Maps of Tourism Big Data Research in China Based on Visualization Analysis},
year = {2020},
isbn = {9781450362481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377672.3378049},
doi = {10.1145/3377672.3378049},
abstract = {OBJECTIVE:We scientifically analyze knowledge structure, development stages, research hotspots and research frontiers of tourism big data in China to provide practical and useful references for researchers to understand the research status and development trends of this field.METHODS:Published journal literatures were retrieved. A scientific collaboration analysis was conducted to visualize the relations of authors and institutions. A co-occurrence analysis was used to visualize the network of key words that was classified by the clustering analysis. Burst detection was conducted to visualize emerging words across the entire research field.RESULTS:We retrieved 964 literatures, from which 668 literatures were identified after screening. Wang Dong has published the most papers. A cooperative group of scientific research institutions with Beijing Union University as the core has been formed. The key words were classified into 6 clusters, and the frequency of "tourism industry" is the largest, and top 14 key words with the highest emergence intensity were detected.CONCLUSIONS:The literature of tourism big data research in China has been increasing rapidly since 2016. Three cooperative groups with Wang Dong, Liu Ligang and Pan Xinqin as the core respectively were formed, and a cooperative group of scientific research institutions with Beijing Union University as the core has been formed. The research hotspots of tourism big data in China mainly focus on six aspects: tourism industry development, key technologies of tourism big data, global tourism, tourism public service, tourists behavior, problems and countermeasures. The evolution of in this field can basically be divided into three stages: exploration (before 2012), start-up (2013-2016) and rapid development (from 2017 to present).},
booktitle = {Proceedings of the 2019 Annual Meeting on Management Engineering},
pages = {144–153},
numpages = {10},
keywords = {Citespace, Tourism Big Data, Knowledge Map, Scientific Collaboration, Burst Detection, Co-occurrence Analysis},
location = {Kuala Lumpur, Malaysia},
series = {AMME 2019}
}

@proceedings{10.1145/2663715,
title = {PSBD '14: Proceedings of the First International Workshop on Privacy and Secuirty of Big Data},
year = {2014},
isbn = {9781450315838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 1st International Workshop on Privacy and Security of Big Data (PSBD 2014) focuses the attention on privacy and security research issues in the context of Big Data, a vibrant and challenging research context which is playing a leading role in the Database research community. Indeed, while Big Data is gaining the attention from the research community, also driven by some relevant technological innovations (like Clouds) as well as novel paradigms (like social networks), the issues of privacy and security of Big Data represent a fundamental problem in this research context, due to the fact Big Data are typically published online for supporting knowledge management and fruition processes and, in addition to this, such data are usually handled by multiple owners, with possible secure multi-part computation issues. Some of the hot topics in the context privacy and security of Big Data include: (i) privacy and security of Big Data integration and exchange; (ii) privacy and security of Big Data in data-intensive Cloud computing; (iii) system architectures in support of privacy and security of Big Data, e.g., GPUs: (iv) privacy and security issues of Big Data querying and analysis. These topics are first-class aspects to be addressed and investigated by PSBD 2014.These proceedings contain the papers selected for presentation at the workshop. We received 12 submissions from countries in North America, Europe and Asia. After careful review, the program committee selected 5 papers for presentation at the workshop. The accepted papers were presented in 2 sessions: scalable privacy-preserving and security-control methods for Big Data processing, user-oriented and data-oriented privacy methods for Big Data processing. A panel discussed advanced aspects of privacy and security of Big Data. We hope that these proceedings will serve as a valuable reference for researchers and practitioners focusing on privacy and security of Big Data.},
location = {Shanghai, China}
}

@article{10.1145/3190578,
author = {Bors, Christian and Gschwandtner, Theresia and Kriglstein, Simone and Miksch, Silvia and Pohl, Margit},
title = {Visual Interactive Creation, Customization, and Analysis of Data Quality Metrics},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3190578},
doi = {10.1145/3190578},
abstract = {During data preprocessing, analysts spend a significant part of their time and effort profiling the quality of the data along with cleansing and transforming the data for further analysis. While quality metrics—ranging from general to domain-specific measures—support assessment of the quality of a dataset, there are hardly any approaches to visually support the analyst in customizing and applying such metrics. Yet, visual approaches could facilitate users’ involvement in data quality assessment. We present MetricDoc, an interactive environment for assessing data quality that provides customizable, reusable quality metrics in combination with immediate visual feedback. Moreover, we provide an overview visualization of these quality metrics along with error visualizations that facilitate interactive navigation of the data to determine the causes of quality issues present in the data. In this article, we describe the architecture, design, and evaluation of MetricDoc, which underwent several design cycles, including heuristic evaluation and expert reviews as well as a focus group with data quality, human-computer interaction, and visual analytics experts.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {3},
numpages = {26},
keywords = {visual exploration, Data profiling, data quality metrics}
}

@inproceedings{10.1145/3090354.3090386,
author = {Tourad, Mohamedou Cheikh and Abdali, Abdelmounaim},
title = {Toward Efficient Ranked-Key Algorithm for the Web Notification of Big Data Systems},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090386},
doi = {10.1145/3090354.3090386},
abstract = {The progress in information distribute on the network, and its users has drive to the evolution of notification systems (Web syndication), such as the Publish/Subscribe system (Pub/sub) that aid the user to trace information managed by the information collectors RSS1 in real time. For relevant use of the information issue from large data streams (Big Data) that currently exist on the net, the improvement of new filtering systems is necessary. Different algorithms have been developed to meet this end. In this work, we propose a new algorithm for indexing structures that will allow us to develop efficient systems.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {31},
numpages = {4},
keywords = {Pub/sub, Ranked-Key, subscription indexing, Big Data},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/2484838.2484884,
author = {Franklin, Michael J.},
title = {Making Sense of Big Data with the Berkeley Data Analytics Stack},
year = {2013},
isbn = {9781450319218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484838.2484884},
doi = {10.1145/2484838.2484884},
abstract = {The Berkeley AMPLab was founded on the idea that the challenges of emerging Big Data applications require a new approach to analytics systems. Launching in early 2011, the project set out to rethink the traditional analytics stack, breaking down technical and intellectual barriers that had arisen during decades of evolutionary development. The vision of the lab is to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (such as machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and en masse, as with crowd-sourced human computation). To pursue this goal, we assembled a research team with diverse interests across computer science, forged relationships with domain experts on campus and elsewhere, and obtained the support of leading industry partners and major government sponsors. The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the nearly three years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark in-memory computation framework, and the Shark query processing system. In this talk I'll describe the current state of BDAS with an emphasis on the key components that have been released to date. I'll then discuss ongoing efforts on machine learning scalability and ease of use, including the MLbase system, as our focus moves higher up the stack. Finally I will present our longer-term views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.},
booktitle = {Proceedings of the 25th International Conference on Scientific and Statistical Database Management},
articleno = {1},
numpages = {1},
location = {Baltimore, Maryland, USA},
series = {SSDBM}
}

@inproceedings{10.1145/3318464.3384705,
author = {Zhang, Liang and Alghamdi, Noura and Eltabakh, Mohamed Y. and Rundensteiner, Elke A.},
title = {Big Data Series Analytics Using TARDIS and Its Exploitation in Geospatial Applications},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384705},
doi = {10.1145/3318464.3384705},
abstract = {The massive amounts of data series data continuously generated and collected by applications require new indices to speed up data series similarity queries on which various data mining techniques rely. However, the state-of-the-art iSAX-based indexing techniques do not scale well due to the binary fanout that leads to a highly deep index tree and suffer from accuracy degradation due to the character-level cardinality that leads to poor maintenance of the proximity. To address this problem, we recently proposed TARDIS to supports indexing and querying billion-scale data series datasets. It introduces a new iSAX-T signatures to reduce the cardinality conversion cost and corresponding sigTree to construct a compact index structure to preserve better similarity. The framework consists of one centralized index and local distributed indices to efficiently re-partition and index dimensional datasets. Besides, effective query strategies based on sigTree structure are proposed to greatly improve the accuracy. In this demonstration, we present GENET, a new interactive exploration demonstration that allows users to support Big Data Series Approximate Retrieval and Recursive Interactive Clustering in large-scale geospatial datasets using TARDIS index techniques.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2785–2788},
numpages = {4},
keywords = {approximate query processing, data series, distributed indexing, clustering, geospatial, iSAX-T, sigtree, GENET, KNN approximate query, TARDIS, word-level cardinality},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1145/269012.269022,
author = {Wang, Richard Y.},
title = {A Product Perspective on Total Data Quality Management},
year = {1998},
issue_date = {Feb. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/269012.269022},
doi = {10.1145/269012.269022},
journal = {Commun. ACM},
month = {feb},
pages = {58–65},
numpages = {8}
}

@inproceedings{10.1145/3148055.3148079,
author = {Chen, Shouwei and Rodero, Ivan},
title = {Understanding Behavior Trends of Big Data Frameworks in Ongoing Software-Defined Cyber-Infrastructure},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148079},
doi = {10.1145/3148055.3148079},
abstract = {As data analytics applications become increasingly important in a wide range of domains, the ability to develop large-scale and sustainable platforms and software infrastructure to support these applications has significant potential to drive research and innovation in both science and business domains. This paper characterizes performance and power-related behavior trends and tradeoffs of the two predominant frameworks for Big Data analytics (i.e., Apache Hadoop and Spark) for a range of representative applications. It also evaluates system design knobs, such as storage and network technologies and power capping techniques. Experimental results from empirical executions provide meaningful data points for exploring the potential of software-defined infrastructure for Big Data processing systems through simulation. The results provide better understanding of the design space to build multi-criteria application-centric models as well as show significant advantages of software-defined infrastructure in terms of execution time, energy and cost. It motivates further research focused on in-memory processing formulations regarding systems with deeper memory hierarchies and software-defined infrastructure.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {199–208},
numpages = {10},
keywords = {big data processing frameworks, software-defined infrastructure, characterization and tradeoffs},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.5555/3195638.3195649,
author = {Chen, Renhai and Shao, Zili and Li, Tao},
title = {Bridging the I/O Performance Gap for Big Data Workloads: A New NVDIMM-Based Approach},
year = {2016},
publisher = {IEEE Press},
abstract = {The long I/O latency posts significant challenges for many data-intensive applications, such as the emerging big data workloads. Recently, the NVDIMM (Non-Volatile Dual In-line Memory Module) technologies provide a promising solution to this problem. By employing non-volatile NAND flash memory as storage media and connecting them via DIMM (Dual Inline Memory Module) slots, the NVDIMM devices are exposed to memory bus so the access latencies due to going through I/O controllers can be significantly mitigated. However, placing NVDIMM on the memory bus introduces new challenges. For instance, by mixing I/O and memory traffic, NVDIMM can cause severe performance degradation on memory-intensive applications. Besides, there exists a speed mismatch between fast memory access and slow flash read/write operations. Moreover, garbage collection (GC) in NAND flash may cause up to several millisecond latency.This paper presents novel, enabling mechanisms that allow NVDIMM to more effectively bridge the I/O performance gap for big data workloads. To address the workload heterogeneity challenge, we develop a scheduling scheme in memory controller to minimize the interference between the native and the I/O-derived memory traffic by exploiting both data access criticality and resource utilization. For NVDIMM controller, several mechanisms are designed to better orchestrate traffic between the memory controller and NAND flash to alleviate the speed discrepancy issue. To mitigate the lengthy GC period, we propose a proactive GC scheme for the NVDIMM controller and flash controller to intelligently synchronize and transfer data involving in forthcoming GC operations. We present detailed evaluation and analysis to quantify how well our techniques fit with the NVDIMM design. Our experimental results show that overall the proposed techniques yield 10%~35% performance improvements over the state-of-the-art baseline schemes.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {9},
numpages = {12},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.1145/2383276.2383302,
author = {Petkov, Plamen and Helfert, Markus},
title = {Data Oriented Challenges of Service Architectures a Data Quality Perspective},
year = {2012},
isbn = {9781450311939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2383276.2383302},
doi = {10.1145/2383276.2383302},
abstract = {Service oriented composition is a prospective approach, which enables flexible and loose composition of applications. Data, on the other hand, is an integral part of service. Despite the huge number of studies that have been done on service oriented environments, very little has been investigated about the data quality. In this paper we examine some problems in service-oriented architecture and in particular problem concerned with data quality. In addition, issues associated with data incorporation and how data consistency is realized through data modelling concepts will be investigated.},
booktitle = {Proceedings of the 13th International Conference on Computer Systems and Technologies},
pages = {163–170},
numpages = {8},
keywords = {SoA concepts, data quality issues data inconsistency and data incorporation, service oriented architectures},
location = {Ruse, Bulgaria},
series = {CompSysTech '12}
}

@inproceedings{10.1145/2661020.2661028,
author = {Chan, Yu and Wellings, Andy and Gray, Ian and Audsley, Neil},
title = {On the Locality of Java 8 Streams in Real-Time Big Data Applications},
year = {2014},
isbn = {9781450328135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661020.2661028},
doi = {10.1145/2661020.2661028},
abstract = {Typical Big Data frameworks do not consider the architecture of the servers that make up the cluster. However, these computers are increasingly heterogeneous and are based on a ccNUMA architecture. In such architectures, main memory access times differ depending on the core on which access is requested. Hence, as well as locality of data access throughout a cluster of servers, locality of memory access within individual servers can have an impact on performance.Java is a commonly-used language for Big Data applications (through the popularity of Hadoop) and the newly-released Java 8 introduces streams to simplify data-parallel programming. However, this paper argues that there are no built-in parallel stream sources that can efficiently operate on very large datasets and take data locality into account. This paper details recent work from the JUNIPER project, an EU Framework 7 Project, which is investigating how the Java 8 platform (augmented by the Real-Time Specification for Java) can be used for real-time Big Data applications. JUNIPER introduces architecture-aware stream sources which are suitable for Big Data systems and which preserve locality of data. Our results show that when reading data from disk, thread affinity can seriously degrade the performance of standard Java streams, but JUNIPER's architecture-aware streams maintain their performance.},
booktitle = {Proceedings of the 12th International Workshop on Java Technologies for Real-Time and Embedded Systems},
pages = {20–28},
numpages = {9},
location = {Niagara Falls, NY, USA},
series = {JTRES '14}
}

@inproceedings{10.1145/3418688.3418697,
author = {Liou, Teau-Jiuan and Weng, Ming-Wei and Lee, Liza},
title = {The Effect of Big Data Platforms on Multi-Stage Production System in Industrie 4.0},
year = {2020},
isbn = {9781450387866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418688.3418697},
doi = {10.1145/3418688.3418697},
abstract = {The aim of this paper is to analyze how Industrie 4.0 triggers changes in the business models of manufacturing SMEs (small and medium-sized enterprises) by big data platforms in selected casting manufacturer in Taiwan. A generalized model is presented to determine the optimal production run time, production rate, the advertising effort and demand with observation features that minimize the total cost per unit time. Advances in science and technology such as IoT technology, big data platform to investigate information asymmetry between manufacturer and customers. Numerical examples and sensitivity analysis are then provided by the collecting real data from Taiwan. Finally, concluding remarks are offered.},
booktitle = {2020 the 3rd International Conference on Computing and Big Data},
pages = {48–54},
numpages = {7},
keywords = {Big data, Multi-stage assembly, Industrie 4.0, Digital transformation},
location = {Taichung, Taiwan},
series = {ICCBD '20}
}

@inproceedings{10.1145/3109453.3109468,
author = {Felicetti, Luca and Femminella, Mauro and Ivanov, Todor and Lio', Pietro and Reali, Gianluca},
title = {A Big-Data Layered Architecture for Analyzing Molecular Communications Systems in Blood Vessels},
year = {2017},
isbn = {9781450349314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109453.3109468},
doi = {10.1145/3109453.3109468},
abstract = {We present a novel architecture for analyzing molecular communications systems in blood vessels for drug delivery and monitoring. This architecture leverages a big data platform for simultaneously using data produced by the existing simulation platforms, health records, and medical data acquisition systems. An included machine learning engine may provide useful insight for medical purposes.},
booktitle = {Proceedings of the 4th ACM International Conference on Nanoscale Computing and Communication},
articleno = {14},
numpages = {2},
keywords = {blood vessels, drug delivery, molecular communications, big data},
location = {Washington, D.C.},
series = {NanoCom '17}
}

@inproceedings{10.1145/2479787.2479806,
author = {Hu, Yuh-Jong and Cheng, Kua-Ping and Huang, Ya-Ling},
title = {Crafting a Balance between Big Data Utility and Protection in the Semantic Data Cloud},
year = {2013},
isbn = {9781450318501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479787.2479806},
doi = {10.1145/2479787.2479806},
abstract = {Structured big data of Personal Identifiable Information (PII) are acquired from everywhere and stored as microdata in a statistical database. Given a statistical disclosure control method, big data analysis and protection are enacted for outsourcing data sources. We flexibly glean the data utility to achieve effective data-driven decision-making. However, we still comply with the privacy protection principles while applying data analysis. In this paper, we propose three types of semantics-enabled policies for controlling access, handling data, and releasing data to craft a balance between data utility and protection. Structured big data are tagged with semantic metadata to enable semantics-enabled policy's direct processing and interpretation. Finally, we demonstrate how to craft a balance between data utility and protection with these types of semantics-enabled policies, combined with various statistical disclosure control methods.},
booktitle = {Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics},
articleno = {18},
numpages = {12},
keywords = {data utility, data protection, big data, semantics-enabled policy, world wide web, semantic data cloud, statistical disclosure control},
location = {Madrid, Spain},
series = {WIMS '13}
}

@inproceedings{10.1145/3029806.3029841,
author = {Niculaescu, Oana-Georgiana and Maruseac, Mihai and Ghinita, Gabriel},
title = {Differentially-Private Big Data Analytics for High-Speed Research Network Traffic Measurement},
year = {2017},
isbn = {9781450345231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3029806.3029841},
doi = {10.1145/3029806.3029841},
abstract = {High-speed research networks (e.g., Internet2, Geant) represent the backbone of large-scale research projects that bring together stakeholders from academia, industry and government. Such projects have increasing demands on throughput (e.g., 100Gbps line rates), and require a high amount of configurability. Collecting and sharing traffic data for such networks can help in detecting hotspots, troubleshooting, and designing novel routing protocols. However, sharing network data directly introduces serious privacy breaches, as an adversary may be able to derive private details about individual users (e.g., personal preferences or activity patterns). Our objective is to sanitize high-speed research network data according to the de-facto standard of differential privacy (DP), thus supporting benefic applications of traffic measurement without compromising individuals' privacy. In this paper, we present an initial framework for computing DP-compliant big data analytics for high-speed research network data. Specifically, we focus on sharing data at flow-level granularity, and we describe our initial steps towards an environment that relies on Hadoop and HBase to support privacy-preserving NetFlow analytics.},
booktitle = {Proceedings of the Seventh ACM on Conference on Data and Application Security and Privacy},
pages = {151–153},
numpages = {3},
keywords = {network measurement, differential privacy},
location = {Scottsdale, Arizona, USA},
series = {CODASPY '17}
}

@article{10.1145/3236644.3236649,
author = {Maurya, Abhinav},
title = {IEEE Big Data 2017 Panel Discussion on Bias and Transparency},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3236644.3236649},
doi = {10.1145/3236644.3236649},
abstract = {In January 2017, the ACM US Public Policy Council released a report on algorithmic transparency and accountability (ACM US Public Policy Council, 2017) which outlined several characteristics for algorithms to be considered transparent and accountable:},
journal = {AI Matters},
month = {jul},
pages = {13–20},
numpages = {8}
}

@inproceedings{10.1145/3092255.3092272,
author = {Bruno, Rodrigo and Oliveira, Lu\'{\i}s Picciochi and Ferreira, Paulo},
title = {NG2C: Pretenuring Garbage Collection with Dynamic Generations for HotSpot Big Data Applications},
year = {2017},
isbn = {9781450350440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092255.3092272},
doi = {10.1145/3092255.3092272},
abstract = {Big Data applications suffer from unpredictable and unacceptably high pause times due to Garbage Collection (GC). This is the case in latency-sensitive applications such as on-line credit-card fraud detection, graph-based computing for analysis on social networks, etc. Such pauses compromise latency requirements of the whole application stack and result from applications' aggressive buffering/caching of data, exposing an ill-suited GC design, which assumes that most objects will die young and does not consider that applications hold large amounts of middle-lived data in memory. To avoid such pauses, we propose NG2C, a new GC algorithm that combines pretenuring with user-defined dynamic generations. By being able to allocate objects into different generations, NG2C is able to group objects with similar lifetime profiles in the same generation. By allocating objects with similar lifetime profiles close to each other, i.e. in the same generation, we avoid object promotion (copying between generations) and heap fragmentation (which leads to heap compactions) both responsible for most of the duration of HotSpot GC pause times. NG2C is implemented for the OpenJDK 8 HotSpot Java Virtual Machine, as an extension of the Garbage First GC. We evaluate NG2C using Cassandra, Lucene, and GraphChi with three different GCs: Garbage First (G1), Concurrent Mark Sweep (CMS), and NG2C. Results show that NG2C decreases the worst observable GC pause time by up to 94.8% for Cassandra, 85.0% for Lucene and 96.45% for GraphChi, when compared to current collectors (G1 and CMS). In addition, NG2c has no negative impact on application throughput or memory usage.},
booktitle = {Proceedings of the 2017 ACM SIGPLAN International Symposium on Memory Management},
pages = {2–13},
numpages = {12},
keywords = {Latency, Garbage Collection, Big Data},
location = {Barcelona, Spain},
series = {ISMM 2017}
}

@article{10.1145/3156685.3092272,
author = {Bruno, Rodrigo and Oliveira, Lu\'{\i}s Picciochi and Ferreira, Paulo},
title = {NG2C: Pretenuring Garbage Collection with Dynamic Generations for HotSpot Big Data Applications},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/3156685.3092272},
doi = {10.1145/3156685.3092272},
abstract = {Big Data applications suffer from unpredictable and unacceptably high pause times due to Garbage Collection (GC). This is the case in latency-sensitive applications such as on-line credit-card fraud detection, graph-based computing for analysis on social networks, etc. Such pauses compromise latency requirements of the whole application stack and result from applications' aggressive buffering/caching of data, exposing an ill-suited GC design, which assumes that most objects will die young and does not consider that applications hold large amounts of middle-lived data in memory. To avoid such pauses, we propose NG2C, a new GC algorithm that combines pretenuring with user-defined dynamic generations. By being able to allocate objects into different generations, NG2C is able to group objects with similar lifetime profiles in the same generation. By allocating objects with similar lifetime profiles close to each other, i.e. in the same generation, we avoid object promotion (copying between generations) and heap fragmentation (which leads to heap compactions) both responsible for most of the duration of HotSpot GC pause times. NG2C is implemented for the OpenJDK 8 HotSpot Java Virtual Machine, as an extension of the Garbage First GC. We evaluate NG2C using Cassandra, Lucene, and GraphChi with three different GCs: Garbage First (G1), Concurrent Mark Sweep (CMS), and NG2C. Results show that NG2C decreases the worst observable GC pause time by up to 94.8% for Cassandra, 85.0% for Lucene and 96.45% for GraphChi, when compared to current collectors (G1 and CMS). In addition, NG2c has no negative impact on application throughput or memory usage.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {2–13},
numpages = {12},
keywords = {Latency, Big Data, Garbage Collection}
}

@article{10.1145/3404193,
author = {Pitoura, Evaggelia},
title = {Social-Minded Measures of Data Quality: Fairness, Diversity, and Lack of Bias},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3404193},
doi = {10.1145/3404193},
abstract = {For decades, research in data-driven algorithmic systems has focused on improving efficiency (making data access faster and lighter) and effectiveness (providing relevant results to users). As data-driven decision making becomes prevalent, there is an increasing need for new measures for evaluating the quality of data systems. In this article, we make the case for social-minded measures, that is, measures that evaluate the effect of a system in society. We focus on three such measures, namely diversity (ensuring that all relevant aspects are represented), lack of bias (processing data without unjustifiable concentration on a particular side), and fairness (non discriminating treatment of data and people).},
journal = {J. Data and Information Quality},
month = {jul},
articleno = {12},
numpages = {8},
keywords = {bias, Fairness, diversity}
}

@article{10.1145/2978570,
author = {Wu, Dapeng and Yang, Boran and Wang, Honggang and Wang, Chonggang and Wang, Ruyan},
title = {Privacy-Preserving Multimedia Big Data Aggregation in Large-Scale Wireless Sensor Networks},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2978570},
doi = {10.1145/2978570},
abstract = {To preserve the privacy of multimedia big data and achieve the efficient data aggregation in wireless multimedia sensor networks (WMSNs), a distributed compressed sensing--based privacy-preserving data aggregation (DCSPDA) approach is proposed in this article. First, in this approach, the original multimedia sensor data are compressed and measured by distributed compressed sensing (DCS) and the compressed data measurements are uploaded to the sink, by which the inherent characteristics between sensor data can be obtained. Second, the original multimedia data are jointly recovered and the common and innovation sparse components are obtained through solving the optimization problem and linear equations at the sink. Third, through least squares support vector machine (LSSVM) learning of the sparse components, the sparse position configuration can be determined and disseminated for each node to conduct the privacy-preserving data configuration. After receiving the configuration message, original multimedia sensor data are accordingly customized, compressed, and measured by the common measurement matrix, aggregated at the cluster heads, and transmitted to the sink. Finally, the aggregated multimedia sensor data are recovered by the sink according to the data configuration to achieve the privacy-preserving data aggregation and transmission. Our comparative simulation results validate the efficiency and scalability of DCSPDA and demonstrate that the proposed approach can effectively reduce the communication overheads and provide reliable privacy-preserving with low computational complexity for WMSNs.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {sep},
articleno = {60},
numpages = {19},
keywords = {data aggregation, distributed compressed sensing, privacy-preserving method, Wireless multimedia sensor networks}
}

@inproceedings{10.1145/3178461.3178464,
author = {Tahat, Ashraf and Aburub, Ruba and Al-Zyoude, Aseel and Talhi, Chamseddine},
title = {A Smart City Environmental Monitoring Network and Analysis Relying on Big Data Techniques},
year = {2018},
isbn = {9781450354387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178461.3178464},
doi = {10.1145/3178461.3178464},
abstract = {A new integrated environmental monitoring system to carry-out real-time measurements on board a moving vehicle is presented. It is composed of an arbitrary number of Electronic Measurements Units (EMU), a smart phone application to relay collected data, and a cloud Central Processing Platform (CPP) to perform analysis utilizing big data techniques and algorithms. Each EMU consists of an electric circuit that incorporates an ultra violet (UV) sensor, an air particles concentration sensor, a temperature sensor and a humidity sensor that all interface to a microcontroller. Bluetooth is employed for communication between the EMU and the smart phone application, while a 3G/4G cellular communications network furnishes the wireless connectivity to the remote CPP. When the collected data reaches the designated cloud server (CPP), it is immediately stored for subsequent analysis. Finally, big data statistical analysis (clustering and classification), mapping and plotting are performed to deduce correlations and to facilitate inferencing. Moreover, the scalability and low-cost of selected components of this realistic system makes it very feasible for large scale deployments in the context of smart cities initiatives, ad-hoc designs, or educational projects.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
pages = {82–86},
numpages = {5},
keywords = {UV index, telemetry, smart phone, air particles, temperature sensor, Big data, environment},
location = {Casablanca, Morocco},
series = {ICSIM2018}
}

@article{10.14778/3115404.3115414,
author = {Chung, Yeounoh and Krishnan, Sanjay and Kraska, Tim},
title = {A Data Quality Metric (DQM): How to Estimate the Number of Undetected Errors in Data Sets},
year = {2017},
issue_date = {June 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3115404.3115414},
doi = {10.14778/3115404.3115414},
abstract = {Data cleaning, whether manual or algorithmic, is rarely perfect leaving a dataset with an unknown number of false positives and false negatives after cleaning. In many scenarios, quantifying the number of remaining errors is challenging because our data integrity rules themselves may be incomplete, or the available gold-standard datasets may be too small to extrapolate. As the use of inherently fallible crowds becomes more prevalent in data cleaning problems, it is important to have estimators to quantify the extent of such errors. We propose novel species estimators to estimate the number of distinct remaining errors in a dataset after it has been cleaned by a set of crowd workers -- essentially, quantifying the utility of hiring additional workers to clean the dataset. This problem requires new estimators that are robust to false positives and false negatives, and we empirically show on three real-world datasets that existing species estimators are unstable for this problem, while our proposed techniques quickly converge.},
journal = {Proc. VLDB Endow.},
month = {jun},
pages = {1094–1105},
numpages = {12}
}

@inproceedings{10.1145/2818869.2818898,
author = {Tang, Bo and Chen, Zhen and Hefferman, Gerald and Wei, Tao and He, Haibo and Yang, Qing},
title = {A Hierarchical Distributed Fog Computing Architecture for Big Data Analysis in Smart Cities},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818898},
doi = {10.1145/2818869.2818898},
abstract = {The ubiquitous deployment of various kinds of sensors in smart cities requires a new computing paradigm to support Internet of Things (IoT) services and applications, and big data analysis. Fog Computing, which extends Cloud Computing to the edge of network, fits this need. In this paper, we present a hierarchical distributed Fog Computing architecture to support the integration of massive number of infrastructure components and services in future smart cities. To secure future communities, it is necessary to build large-scale, geospatial sensing networks, perform big data analysis, identify anomalous and hazardous events, and offer optimal responses in real-time. We analyze case studies using a smart pipeline monitoring system based on fiber optic sensors and sequential learning algorithms to detect events threatening pipeline safety. A working prototype was constructed to experimentally evaluate event detection performance of the recognition of 12 distinct events. These experimental results demonstrate the feasibility of the system's city-wide implementation in the future.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {28},
numpages = {6},
keywords = {distributed computing architecture, pipeline safety monitoring, Fog computing, smart city, big data analysis},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@inproceedings{10.1145/3358528.3358552,
author = {Chen, Rui-Yang},
title = {Iot-Enabled Supply Chain Finance Risk Management Performance Big Data Analysis Using Fuzzy Qfd},
year = {2019},
isbn = {9781450371926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358528.3358552},
doi = {10.1145/3358528.3358552},
abstract = {For inventory finance in Supply chain finances (SCF) is carried out based on collateral, the risk assessment to the liquidity capacity and turnover ability is necessary. The solution of risk evaluation only based on digital data is unable to predict the risk influence accurately. Traditional SCF is difficult to realize practical business market because of lack of physical things self-tracked capability intelligently. The proposed IoT-based risk management performance big data analysis is capable of predicting different risk states by tracking transaction status changes to control risk management in SCF. The proposed approach is proved success solution for SCF according to the performance index evaluation. Moreover, heuristic fuzzy QFD algorithms are provided to explore the risk management performance big data analysis. Successful case study with simulations demonstrated the performance of the proposed approach.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
pages = {82–86},
numpages = {5},
keywords = {Supply chain finances, big data, fuzzy QFD, risk management performance},
location = {Jinan, China},
series = {ICBDT2019}
}

@inproceedings{10.1145/3373419.3373440,
author = {Zhai, Chenggong and Zhou, Shanbo and Zhang, Gaoyang},
title = {Research on Optimizing the Support of Conscription Recruits Cloths Based on Big Data},
year = {2020},
isbn = {9781450376754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373419.3373440},
doi = {10.1145/3373419.3373440},
abstract = {This paper mainly uses big data technology to analyze and statistics the distribution of conscription number over the years. Based on this result, the budget and preset of conscription material are made. Finally, the classification of conscription number is adjusted according to the characteristics of conscription recruits and the feedback of conscription distribution.},
booktitle = {Proceedings of the 2019 3rd International Conference on Advances in Image Processing},
pages = {119–123},
numpages = {5},
keywords = {Big Data, Cloths, conscription recruits},
location = {Chengdu, China},
series = {ICAIP 2019}
}

@inproceedings{10.1145/3350546.3352531,
author = {Chatzidimitriou, Kyriakos and Papamichail, Michail and Oikonomou, Napoleon-Christos and Lampoudis, Dimitrios and Symeonidis, Andreas},
title = {Cenote: A Big Data Management and Analytics Infrastructure for the Web of Things},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352531},
doi = {10.1145/3350546.3352531},
abstract = {In the era of Big Data, Cloud Computing and Internet of Things, most of the existing, integrated solutions that attempt to solve their challenges are either proprietary, limit functionality to a predefined set of requirements, or hide the way data are stored and accessed. In this work we propose Cenote, an open source Big Data management and analytics infrastructure for the Web of Things that overcomes the above limitations. Cenote is built on component-based software engineering principles and provides an all-inclusive solution based on components that work well individually.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {282–285},
numpages = {4},
keywords = {restful api, internet of things, apache kafka, infrastructure, web of things, analytics, cockroachdb, apache storm},
location = {Thessaloniki, Greece},
series = {WI '19}
}

