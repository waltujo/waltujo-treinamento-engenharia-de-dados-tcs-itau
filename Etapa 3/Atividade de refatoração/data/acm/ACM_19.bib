@inproceedings{10.1145/3006299.3006304,title = {A real-time big data analysis framework on a CPU/GPU heterogeneous cluster: a meteorological application case study}, author = {Hassaan Mohamed , Elghandour Iman },year = {2016}, isbn = {9781450346177}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3006299.3006304}, doi = {10.1145/3006299.3006304}, abstract = {It is important to analyze and predict meteorological phenomena in real-time. Parallel programming by exploiting thousands of threads in GPUs can be efficiently used to speed up the execution of many applications. However, GPUs have limitations when used for processing big data, which can be better analyzed using distributed computing platforms such as Hadoop and Spark. In this paper, we propose DAMB a system that processes streamed data on a heterogeneous cluster of CPUs and GPUs in real-time. The core of DAMB is SparkGPU, a platform that extends Apache Spark to allow it to manage a heterogeneous cluster that has both CPUs and GPUs and to execute tasks on GPUs. DAMB also provides data visualization tools that present the analyzed data in an interactive way in real-time. As a case study, we focus on a meteorological application that analyzes lightening discharges. We show that DAMB can successfully process and analyze the meteorological data streamed to it and visualize the results in real-time on a cluster of size 12 nodes, each is equipped with one or more GPU cards. This is a speedup of two orders of magnitude as compared to a sequential program implementation for the same application.}, location = {Shanghai, China}, series = {BDCAT '16}, pages = {168\u2013177}, numpages = {10}, keywords = {GPU programming, in-memory cluster computing, heterogeneous clusters}}
@inproceedings{10.1145/2451856.2451869,title = {Telling the story in big data}, author = {Kim Jeffrey , Lund Arnie , Dombrowski Caroline },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2451856.2451869}, doi = {10.1145/2451856.2451869}, pages = {48\u201351}, numpages = {4}}
@inproceedings{10.1145/3130983,title = {Predicting Commercial Activeness over Urban Big Data}, author = {Yang Su , Wang Minjie , Wang Wenshan , Sun Yi , Gao Jun , Zhang Weishan , Zhang Jiulong },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3130983}, doi = {10.1145/3130983}, abstract = {This study aims at revealing how commercial hotness of urban commercial districts (UCDs) is shaped by social contexts of surrounding areas so as to render predictive business planning. We define social contexts for a given region as the number of visitors, the region functions, the population and buying power of local residents, the average price of services, and the rating scores of customers, which are computed from heterogeneous data including taxi GPS trajectories, point of interests, geographical data, and user-generated comments. Then, we apply sparse representation to discover the impactor factor of each variable of the social contexts in terms of predicting commercial activeness of UCDs under a linear predictive model. The experiments show that a linear correlation between social contexts and commercial activeness exists for Beijing and Shanghai based on an average prediction accuracy of 77.69% but the impact factors of social contexts vary from city to city, where the key factors are rich life services, diversity of restaurants, good shopping experience, large number of local residents with relatively high purchasing power, and convenient transportation. This study reveals the underlying mechanism of urban business ecosystems, and promise social context-aware business planning over heterogeneous urban big data.}, pages = {1\u201320}, numpages = {20}, keywords = {Economic Ecosystems, Context Awareness, Social Intelligence, Crowdsourcing, Urban Informatics}}
@inproceedings{10.1145/2629568,title = {Process-driven data quality management: A critical review on the application of process modeling languages}, author = {Glowalla Paul , Sunyaev Ali },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2629568}, doi = {10.1145/2629568}, abstract = {Data quality is critical to organizational success. In order to improve and sustain data quality in the long term, process-driven data quality management (PDDQM) seeks to redesign processes that create or modify data. Consequently, process modeling is mandatory for PDDQM. Current research examines process modeling languages with respect to representational capabilities. However, there is a gap, since process modeling languages for PDDQM are not considered. We address this research gap by providing a synthesis of the varying applications of process modeling languages for PDDQM. We conducted a keyword-based literature review in conferences as well as 74 highranked information systems and computer science journals, reviewing 1,555 articles from 1995 onwards. For practitioners, it is possible to integrate the quality perspective within broadly applied process models. For further research, we derive representational requirements for PDDQM that should be integrated within existing process modeling languages. However, there is a need for further representational analysis to examine the adequacy of upcoming process modeling languages. New or enhanced process modeling languages may substitute for PDDQM-specific process modeling languages and facilitate development of a broadly applicable and accepted process modeling language for PDDQM.}, pages = {1\u201330}, numpages = {30}, keywords = {Information quality, data quality, process modeling, data and knowledge visualization., conceptual modeling}}
@inproceedings{10.1145/2783258.2788573,title = {Forecasting Fine-Grained Air Quality Based on Big Data}, author = {Zheng Yu , Yi Xiuwen , Li Ming , Li Ruiyuan , Shan Zhangqing , Chang Eric , Li Tianrui },year = {2015}, isbn = {9781450336642}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2783258.2788573}, doi = {10.1145/2783258.2788573}, abstract = {In this paper, we forecast the reading of an air quality monitoring station over the next 48 hours, using a data-driven method that considers current meteorological data, weather forecasts, and air quality data of the station and that of other stations within a few hundred kilometers. Our predictive model is comprised of four major components: 1) a linear regression-based temporal predictor to model the local factors of air quality, 2) a neural network-based spatial predictor to model global factors, 3) a dynamic aggregator combining the predictions of the spatial and temporal predictors according to meteorological data, and 4) an inflection predictor to capture sudden changes in air quality. We evaluate our model with data from 43 cities in China, surpassing the results of multiple baseline methods. We have deployed a system with the Chinese Ministry of Environmental Protection, providing 48-hour fine-grained air quality forecasts for four major Chinese cities every hour. The forecast function is also enabled on Microsoft Bing Map and MS cloud platform Azure. Our technology is general and can be applied globally for other cities.}, location = {Sydney, NSW, Australia}, series = {KDD '15}, pages = {2267\u20132276}, numpages = {10}, keywords = {air quality forecast, big data, urban computing, urban air}}
@inproceedings{10.1145/2676536.2676543,title = {A MapReduce algorithm to create contiguity weights for spatial analysis of big data}, author = {Li Xun , Li Wenwen , Anselin Luc , Rey Sergio , Koschinsky Julia },year = {2014}, isbn = {9781450331326}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2676536.2676543}, doi = {10.1145/2676536.2676543}, abstract = {Spatial analysis of Big data is a key component of Cyber-GIS. However, how to utilize existing cyberinfrastructure (e.g. large computing clusters) to perform parallel and distributed spatial analysis on Big data remains a huge challenge. Problems such as efficient spatial weights creation, spatial statistics and spatial regression of Big data still need investigation. In this research, we propose a MapReduce algorithm for creating contiguity-based spatial weights. This algorithm provides the ability to create spatial weights from very large spatial datasets efficiently by using computing resources that are organized in the Hadoop framework. It works in the paradigm of MapReduce: mappers are distributed in computing clusters to find contiguous neighbors in parallel, then reducers collect the results and generate the weights matrix. To test the performance of this algorithm, we design experiment to create contiguity-based weights matrix from artificial spatial data with up to 190 million polygons using Amazon's Hadoop framework called Elastic MapReduce. The experiment demonstrates the scalability of this parallel algorithm which utilizes large computing clusters to solve the problem of creating contiguity weights on Big data.}, location = {Dallas, Texas}, series = {BigSpatial '14}, pages = {50\u201353}, numpages = {4}, keywords = {big data, spatial weights, mapreduce}}
@inproceedings{10.1145/3512576.3512618,title = {Artificial intelligence and big data analysis implementation in electronic medical records}, author = {Sardjono Wahyu , Retnowardhani Astari , Emil Kaburuan Robert , Rahmasari Aninda },year = {2021}, isbn = {9781450384971}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3512576.3512618}, doi = {10.1145/3512576.3512618}, abstract = {Industry 4.0 is the pioneer of the Internet of Things (IoT). The Internet of Things (IoT) are often heard and successful in business revolution from all sectors. The IoT are widely used in numerous sectors including medical services and have become the rise of Internet of Medical Things (IoMT). One of the implementations is the Electronic Health Record (EHR) systems. Previously the health records were used in traditional manner such as print-out health record of a patient and stored to an archive room. With the innovation of EHR, patients\u2019 health records are digitalized which provides advantages from space efficiency and paperless forms. EHR helps medical service management to provide better healthcare services. With the integration of Artificial Intelligence (AI) and Big Data Analysis in EHR, healthcare services provide more accurate and reliable diagnosis.}, location = {Guangzhou, China}, series = {ICIT 2021}, pages = {231\u2013237}, numpages = {7}, keywords = {big data, artificial intelligence, Analysis, electronic medical records, internet of things}}
@inproceedings{10.1145/3368691.3368713,title = {Call data record based big data analytics for smart cities}, author = {Mouchili Mama Nsangou , Atwood John William , Aljawarneh Shadi },year = {2019}, isbn = {9781450372848}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3368691.3368713}, doi = {10.1145/3368691.3368713}, abstract = {A Call Data Record (CDR) is produced for each call (or other interaction) handled by a telephone company. CDRs have traditionally been used for billing and network engineering purposes. Given how mobile phones have become an integral part of - and have undoubtedly transformed - the everyday life of a great part of the earth's population, and given that 90% or more of phone subscriptions are registered in any city, if CDRs are collected from the mobile phone and cellular networks, and combined with other data from the organization or from elsewhere, this will allow managers to identify trends, detect patterns, and glean other valuable findings from the data.This paper highlights the applicability of CDR-based big data, to gathering such insights. By stitching events together into clusters of related events across runtime environments and/or geographies, raw data becomes business insight to make decisions either to understand customer needs, to mitigate problems, or to ultimately gain a competitive advantage.}, location = {Dubai, United Arab Emirates}, series = {DATA '19}, pages = {1\u20137}, numpages = {7}, keywords = {traffic congestion, Hadoop, CDR, insights, big data, mining algorithms, JSON, smart city, data analytics, city management, traffic management, random forest}}
@inproceedings{10.1145/2757218.2757221,title = {Towards a Cross-Disciplinary Pedagogy for Big Data}, author = {Eckroth Joshua },year = {2015}, isbn = {9781450335973}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2757218.2757221}, doi = {10.1145/2757218.2757221}, abstract = {Many disciplines are confronting the challenge of \"big data,\" i.e., databases or data streams that are so massive or deliver data at such high velocity that a single commodity machine is unable to store or process the data. Mining and analyzing big data requires specialized algorithms and methodologies due to the fundamentally distributed and parallel natures of the workloads. To our knowledge, existing pedagogies in most disciplines do not prepare students to work with big data. We aim to develop a cross-disciplinary pedagogy that prepares all students to tackle data mining and analysis challenges of the future.}, location = {DeLand, FL, USA}, series = {CIC '15}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/2642769.2642798,title = {Architecture-Awareness for Real-Time Big Data Systems}, author = {Gray Ian , Chan Yu , Audsley Neil C. , Wellings Andy },year = {2014}, isbn = {9781450328753}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2642769.2642798}, doi = {10.1145/2642769.2642798}, abstract = {Existing programming models for distributed and cloud-based systems tend to abstract away from the architectures of individual target nodes, concentrating instead on higher-level issues of algorithm representation (MapReduce etc.). However, as programmers begin to tackle the issue of Big Data, increasing data volumes are forcing developers to reconsider this approach and to optimise their software heavily. JUNIPER is an EU-funded project which assists Big Data developers to create architecture-aware software in a way that is suitable for the target domain, and provides higher performance, portability, and real-time guarantees.}, location = {Kyoto, Japan}, series = {EuroMPI/ASIA '14}, pages = {151\u2013156}, numpages = {6}}
@inproceedings{10.1145/3063955.3063995,title = {Composed sketch framework for quantiles and cardinality queries over big data streams}, author = {Zhang Shuzhuang , Luo Hao , Wu Zhigang , Wang Yi },year = {2017}, isbn = {9781450348737}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3063955.3063995}, doi = {10.1145/3063955.3063995}, abstract = {Quantiles and Cardinality queries are important tools to analyze statistical information from big data streams. Due to the features of the streams, such as huge volume and high velocity, it is a challenging problem to quickly provide responses for the two types of queries using constrained space over big data streams. In this paper, we propose a composed sketch framework, which can support both quantiles queries and cardinality queries over the data streams. We introduce cardinality estimators into a baseline q-digest structure and propose unified sketch merging and query processing operations. Our approach can support these two types of queries simultaneously. We conduct detailed theoretical and experimental analysis in terms of query accuracy and query response time. The analytical and experimental results show that our approach can obtain accurate estimates quicker than traditional method and system in big data streams environments, and it just produces less than 0.8\u2030 storage overhead in TB-scale real-world data sets.}, location = {Shanghai, China}, series = {ACM TUR-C '17}, pages = {1\u201310}, numpages = {10}, keywords = {big data, data stream, sketch, cardinality query, quantiles query}}
@inproceedings{10.1145/2910019.2910033,title = {A Big Data Approach to Support Information Distribution in Crisis Response}, author = {Netten Niels , van den Braak Susan , Choenni Sunil , van Someren Maarten },year = {2016}, isbn = {9781450336406}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2910019.2910033}, doi = {10.1145/2910019.2910033}, abstract = {Crisis response organizations operate in very dynamic environments, in which it is essential for responders to acquire all information critical to their task execution in time. In reality, the responders are often faced with information overload, incomplete information, or a combination of both. This hampers their decision-making process, workflow, situational awareness and, consequently, effective execution of collaborative crisis response. Therefore, getting the right information to the right person at the right time is of crucial importance.The task of processing all data during crisis response situations and determining for whom at a particular moment the information is relevant is not straightforward. When developing an information system to support this task, some important challenges have to be taken into account. These challenges relate to the structure and truthfulness of the used data, the assessment of information relevance, and the dissemination of relevant information in time. While methods and techniques from big data can be used to collect and integrate data, machine learning can be used to build a model for relevance assessments. An example implementation of such a framework of big data is the TAID software system that collects and integrates data communicated between first responders and may send information to crisis responders that were not addressed in the initial communication. As an example of the impact of TAID on crisis response, we show its effect in a simulated crisis response scenario.}, location = {Montevideo, Uruguay}, series = {ICEGOV '15-16}, pages = {266\u2013275}, numpages = {10}, keywords = {Relevance Assessments, Machine Learning, Information Distribution, Big Data, Crisis Response for Public Safety}}
@inproceedings{10.1145/3544109.3544142,title = {Financial Big Data Analysis Service System}, author = {Guo Jian },year = {2022}, isbn = {9781450395786}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544109.3544142}, doi = {10.1145/3544109.3544142}, location = {Dalian, China}, series = {IPEC '22}, pages = {183\u2013186}, numpages = {4}}
@inproceedings{10.1145/3210506.3210514,title = {The Real Option Approach for Assessment of Big Data Asset Based on Prospect Theory}, author = {Fang Yinjie },year = {2018}, isbn = {9781450363808}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3210506.3210514}, doi = {10.1145/3210506.3210514}, abstract = {This paper combines prospect theory with real option pricing model to construct a new value assessment model for big data assets. For the high uncertainty of future earnings and risk of big data assets, this paper analyzes that their value characteristics are in line with the American call option firstly. On this basis, we use the value function in the prospect theory to calculate decision makers' subjective judgments on the value of underlying big data assets under each state, and use the weighting function to calculate the decision makers' subjective judgment on the weight of expansion right, downsize right and abandon right. In example, we use least-squares Monte Carlo simulation method to perform a simulation which verifies the real option pricing method based on perspective of prospect theory can obtain more reasonable assessment result for big data assets.}, location = {Hong Kong, Hong Kong}, series = {EBIMCS '18}, pages = {40\u201345}, numpages = {6}, keywords = {Big Data, Assessment, Real Option, Prospect Theory, Big Data Asset}}
@inproceedings{10.1145/3318299.3318384,title = {Air Big Data Outlier Detection Based on Infinite Gauss Bayesian and CNN}, author = {Zhou LiangQi , Xu HongZhen , Wei Li , Zhang Quan , Zhou Fei , Li ZhuoPei },year = {2019}, isbn = {9781450366007}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3318299.3318384}, doi = {10.1145/3318299.3318384}, abstract = {Air quality has always been a hot issue of concern to the people, the environmental protection department and the government. Among the massive air quality data, abnormal data can interfere with subsequent experiments and analysis. Therefore, it is necessary to detect abnormal data to improve the accuracy of the data. However, traditional air outlier detection methods require at least one year's data to make inferences about air quality. This paper firstly analyzes the characteristics of air quality big data, and then proposes a framework based on Bayesian non-parametric clustering, namely Dirichlet Process (DP) clustering framework, to realize the outlier detection of air quality. The framework optimizes Gaussian mixture model into infinite Gaussian mixture model according to the results of data analysis, and uses neural network to cluster the data processed by infinite Gaussian mixture model, which effectively improves the clustering accuracy and avoids the need of collecting a large number of training data.}, location = {Zhuhai, China}, series = {ICMLC '19}, pages = {317\u2013321}, numpages = {5}, keywords = {Dirichlet process, outlier detection, Bayesian clustering, Air quality, neural Network}}
@inproceedings{10.1145/3320326.3320356,title = {Big-Data Architecture for Electrical Consumption Forecasting in Educational Institutions Buildings}, author = {Daki Houda , El Hannani Asmaa , Ouahmane Hassan },year = {2019}, isbn = {9781450366458}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3320326.3320356}, doi = {10.1145/3320326.3320356}, abstract = {Recently, educational institutions suffer from high electrical consumption due to their new practices and activities. One of the promising solutions to overcome this challenge is to improve their energy management strategies using smart grids which ensure efficiency, reliability and energy saving. For this same reason, the National School of Applied Sciences of El Jadida -- Morocco has decided to install a private smart grid based on photovoltaic panels that will cover 40% of its electricity needs. But the problem that arises when using this new approach is the high level of complexity in term of data management due to the variety, veracity and the volume of the data. So, to meet these needs the use of Big Data technologies is required. In this paper, we propose a Big Data solution based on Lambda architecture to handle electrical consumption data in the National School of Applied Sciences of El Jadida -- Morocco. This system collects all parameters that might influence electrical consumption with Kafka, then it applies Spark libraries to analyze it. The solution allows also electrical energy forecasting using Spark machine learning library and the data persistence using HBase storage system.}, location = {Rabat, Morocco}, series = {NISS19}, pages = {1\u20136}, numpages = {6}, keywords = {Machine learning, HBase storage system, Big Data, Spark, Electrical forecasting, Kafka, Smart grid, Lambda architecture}}
@inproceedings{10.1145/2382416.2382418,title = {Big data privacy and security challenges}, author = {Weber Samuel },year = {2012}, isbn = {9781450316613}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2382416.2382418}, doi = {10.1145/2382416.2382418}, abstract = {The ability to collect and organize large data sets has proven to be transformational: instead of just a linear improvement of older techniques, the ability to effectively process huge amounts of information creates radically new abilities and opportunities. Unfortunately, these abilities come with new security, privacy and legal risks: privacy that was previously protected only by the fact that data gathering was difficult can now be trivially violated. This talk will describe research challenges that arise in this field.}, location = {Raleigh, North Carolina, USA}, series = {BADGERS '12}, pages = {1\u20132}, numpages = {2}, keywords = {security, legal aspects, privacy}}
@inproceedings{10.1145/3331453.3360973,title = {Application Research of Big Data for Launch Support System at Space Launch Site}, author = {Dong Wei , Xiao Litian , Niu Shengfen , Niu Jianjun , Wang Fei },year = {2019}, isbn = {9781450362948}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3331453.3360973}, doi = {10.1145/3331453.3360973}, abstract = {At the space launch site, the big data of the launch support system comes from the construction of the launch site, the ground service, the comprehensive support process, and launch mission organization and command. The big data is extensive sources, various types, large scale, and rapid growth. The big data application can improve the data processing and management efficiency for the launch support system. Then the application can enhance the support capability of flight mission and success rate. This paper analyzes the existing data application of launch support system. The challenges and requirements of big data application are studied by the construction of intelligent launch site. The application pattern and target are put forward from four aspects of launch mission organization and command, mission application, comprehensive support, and information security. The classification of big data is proposed for a launch support system. The architecture of big data application system is designed, which meets the application pattern and target. It lays a foundation for the future big data project at the launch site.}, location = {Sanya, China}, series = {CSAE 2019}, pages = {1\u20136}, numpages = {6}, keywords = {Space launch site, Application research, Launch support system, Big data}}
@inproceedings{10.1145/3448748.3448776,title = {Research on Big Data Risk Control Model of Venture Capital}, author = {Cui Boao },year = {2021}, isbn = {9781450390002}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3448748.3448776}, doi = {10.1145/3448748.3448776}, abstract = {The two main characteristics of venture capital are high risk and participation in management. Risk identification and risk evaluation before investing, risk supervise and control are important process that affect the success of venture investment. First of all, a risk evaluation index system is constructed. Partial correlation analysis is used to explore the indicators that can significantly affect the success of a company's investment, and to provide suggestions for the types of risks that start-ups should focus on controlling during the startup period. Then the principal component analysis method and the Logistic regression analysis method are combined to predict the success rate of investment, which can make up for the deficiency of the Logistic model and improve the prediction accuracy rate. Then use the test set data to calculate the accuracy of the model, and conduct an Omnibus test of the model coefficients to verify the significance of the equation. Then the SE-DEA model is constructed to calculate and compare the efficiency values of enterprises that accept different levels of post-investment services, and test the robustness of the SE-DEA model by adjusting the input and output indicators. Then through the Mann-Whitney U test and analysis, it is concluded that if the post-investment service is to have a good effect, how VC should choose the degree of intervention according to the state of the enterprise. That is to say, which indicators of the enterprise can be optimized by VC intervention in management. Finally, the models are evaluated and future research prospects in related fields are proposed.}, location = {Harbin, China}, series = {BIC 2021}, pages = {173\u2013181}, numpages = {9}, keywords = {models, Big data, risk management}}
@inproceedings{10.5555/2616606.2617095,title = {Energy efficient neural networks for big data analytics}, author = {Wang Yu , Li Boxun , Luo Rong , Chen Yiran , Xu Ningyi , Yang Huazhong },year = {2014}, isbn = {9783981537024}, publisher = {European Design and Automation Association}, address = {Leuven, BEL}, abstract = {The world is experiencing a data revolution to discover knowledge in big data. Large scale neural networks are one of the mainstream tools of big data analytics. Processing big data with large scale neural networks includes two phases: the training phase and the operation phase. Huge computing power is required to support the training phase. And the energy efficiency (power efficiency) is one of the major considerations of the operation phase. We first explore the computing power of GPUs for big data analytics and demonstrate an efficient GPU implementation of the training phase of large scale recurrent neural networks (RNNs). We then introduce a promising ultra-high energy efficient implementation of neural networks' operation phase by taking advantage of the emerging memristor technique. Experiment results show that the proposed GPU implementation of RNNs is able to achieve 2 ~ 11\u00d7 speed-up compared with the basic CPU implementation. And the scaled-up recurrent neural network trained with GPUs realizes an accuracy of 47% on the Microsoft Research Sentence Completion Challenge, the best result achieved by a single RNN on the same dataset. In addition, the proposed memristor-based implementation of neural networks demonstrates power efficiency of > 400 GFLOPS/W and achieves energy savings of 22\u00d7 on the HMAX model compared with its pure digital implementation counterpart.}, location = {Dresden, Germany}, series = {DATE '14}, pages = {1\u20132}, numpages = {2}}
@inproceedings{10.1145/2463676.2467801,title = {Workload management for big data analytics}, author = {Aboulnaga Ashraf , Babu Shivnath },year = {2013}, isbn = {9781450320375}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2463676.2467801}, doi = {10.1145/2463676.2467801}, location = {New York, New York, USA}, series = {SIGMOD '13}, pages = {929\u2013932}, numpages = {4}, keywords = {workload management, analytics, mapreduce, parallel database systems}}
@inproceedings{10.1145/3494885.3494891,title = {Exploring Support Vector Machines for Big Data Analyses}, author = {Lu Siyang , Chen Yihong , Zhu Xiaolin , Wang Ziyi , Ou Yangjun , Xie Yuhang },year = {2021}, isbn = {9781450390675}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3494885.3494891}, doi = {10.1145/3494885.3494891}, abstract = {The traditional support vector machines perform well in classification and prediction on small and medium-sized data sets, but there are some problems such as the low training efficiency and the low accuracy in large sample number, high dimension and large-scale data sets. Meanwhile, with the rise of distributed computing platforms such as the Spark suitable for big data analyses, more and more scholars at home and abroad turn their research direction to the distributed machine learning algorithms Therefore, in order to carry out the research on support vector machine for big data analyses, this paper explores the related researches and current situations of support vector machine, including: in-depth analysis of the algorithm principle of support vector machines, systematical investigation of the improved methods of support vector machines for the big data analyses, and distributed support vector machines under the Spark platform. Then, combined with the parallelization mechanism of the Spark, the some future research directions of support vector machine are investigated: for optimizing the accuracy of training results, some special matrix calculation skills should be added; In term of the research on SVM under the Spark platform, some better optimization methods from the perspective of dimension and partition can be found.}, location = {Singapore, Singapore}, series = {CSSE 2021}, pages = {31\u201337}, numpages = {7}, keywords = {Distribution, Machine Learning, SVM, Spark}}
@inproceedings{10.1145/3371425.3371435,title = {A rule based data quality assessment architecture and application for electrical data}, author = {Liu He , Wang Xiaohui , Lei Shuya , Zhang Xi , Liu Weiwei , Qin Ming },year = {2019}, isbn = {9781450376334}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3371425.3371435}, doi = {10.1145/3371425.3371435}, abstract = {Data quality assessment plays an important role in electricity consumption big data. It can help business people master the overall data situation, which can provide a strong guarantee for subsequent data improvement, analysis and decision. According to the electrical data quality issues, we design a rule-based data quality assessment architecture for electrical big data. It includes six types of data quality assessment indexes (such as comprehensiveness, accuracy, completeness), and the related data quality rules (such as non-empty rule and range rule), which can be used to guide the electrical data quality inspection. Meanwhile, for the accuracy, we propose an outlier detection method based on time time-relevant k-means, which is used to detect the voltage, curve and power data issues in electricity data. The experimental and simulation results show that the proposed architecture and method can work well for the comprehensive data quality assessment of electrical data.}, location = {Sanya, China}, series = {AIIPCC '19}, pages = {1\u20136}, numpages = {6}, keywords = {outlier, data quality, electrical data, quality assessment}}
@inproceedings{10.1145/2745844.2745889,title = {Flexible Transformations For Learning Big Data}, author = {Mirhoseini Azalia , Songhori Ebrahim M. , Darvish Rouhani Bita , Koushanfar Farinaz },year = {2015}, isbn = {9781450334860}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2745844.2745889}, doi = {10.1145/2745844.2745889}, abstract = {This paper proposes a domain-specific solution for iterative learning of big and dense (non-sparse) datasets. A large host of learning algorithms, including linear and regularized regression techniques, rely on iterative updates on the data connectivity matrix in order to converge to a solution. The performance of such algorithms often severely degrade when it comes to large and dense data. Massive dense datasets not only induce obligatory large number of arithmetics, but they also incur unwanted message passing cost across the processing nodes. Our key observation is that despite the seemingly dense structures, in many applications, data can be transformed into a new space where sparse structures become revealed. We propose a scalable data transformation scheme that enables creating versatile sparse representations of the data. The transformation can be tuned to benefit the underlying platform's cost and constraints. Our evaluations demonstrate significant improvement in energy usage, runtime, and mem}, location = {Portland, Oregon, USA}, series = {SIGMETRICS '15}, pages = {453\u2013454}, numpages = {2}, keywords = {performance optimization, sparse factorization, subspace sampling, big and dense data}}
@inproceedings{10.1145/2796314.2745889,title = {Flexible Transformations For Learning Big Data}, author = {Mirhoseini Azalia , Songhori Ebrahim M. , Darvish Rouhani Bita , Koushanfar Farinaz },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2796314.2745889}, doi = {10.1145/2796314.2745889}, abstract = {This paper proposes a domain-specific solution for iterative learning of big and dense (non-sparse) datasets. A large host of learning algorithms, including linear and regularized regression techniques, rely on iterative updates on the data connectivity matrix in order to converge to a solution. The performance of such algorithms often severely degrade when it comes to large and dense data. Massive dense datasets not only induce obligatory large number of arithmetics, but they also incur unwanted message passing cost across the processing nodes. Our key observation is that despite the seemingly dense structures, in many applications, data can be transformed into a new space where sparse structures become revealed. We propose a scalable data transformation scheme that enables creating versatile sparse representations of the data. The transformation can be tuned to benefit the underlying platform's cost and constraints. Our evaluations demonstrate significant improvement in energy usage, runtime, and mem}, pages = {453\u2013454}, numpages = {2}, keywords = {big and dense data, subspace sampling, performance optimization, sparse factorization}}
@inproceedings{10.1145/3241748.3241773,title = {Research on Mobile Learning and Micro Course in the Big Data Environment}, author = {Dai Hong , Tao Ye , Shi Tian-Wei },year = {2018}, isbn = {9781450364812}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3241748.3241773}, doi = {10.1145/3241748.3241773}, abstract = {Put forward design of micro course mobile teaching system through analyzing the change of knowledge acquisition way and teaching role in the big data environment. The system platform includes the learner terminal platform and the teacher terminal platform. Learners use mobile terminals to active learn utilizing fragments of time. Mine the value information to take advantage of collecting education data in the process of using system. At the same time, the paper sets forth learning resource organization model. Form a complete data base of teaching process through the integration of a variety of teaching resources. Carry out data mining analysis according to the learner's behavior of micro course mobile learning. The results of the mining analysis provide a basis for teachers to adjust teaching content and improve teaching methods. The paper also presents the design of system network structure from the perspective of big data. The system architecture has been implemented. The paper proposes an innovative education evaluation mechanism through the mining analysis of education data. Form the objective and innovative evaluation mechanism between teachers and learners.}, location = {Beijing, China}, series = {ICEBT '18}, pages = {48\u201351}, numpages = {4}, keywords = {Big Data, Mobile Learning, Micro Course, Evaluation Mechanism}}
@inproceedings{10.1145/3123024.3124411,title = {Smart urban planning using big data analytics based internet of things}, author = {Babar Muhammad , Arif Fahim },year = {2017}, isbn = {9781450351904}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3123024.3124411}, doi = {10.1145/3123024.3124411}, abstract = {The extensive growth of the Internet of Things (IoT) is providing direction towards the smart urban. The smart urban is favored because it improves the standard of living of the citizens and provides excellence in the community services. The services may include but not limited to health, parking, transport, water, environment, power, and so forth. The diverse and heterogeneous environment of IoT and smart urban is challenged by real-time data processing and decision-making. In this research article, we propose IoT based smart urban architecture using Big Data analytics. The proposed architecture is divided into three different tiers: (1) data acquisition and aggregation, (2) data computation and processing, and (3) decision making and application. The proposed architecture is implemented and validated on Hadoop Ecosystem using reliable and authentic datasets. The research shows that the proposed system presents valuable imminent into the community development systems to get better the existing smart urban architecture.}, location = {Maui, Hawaii}, series = {UbiComp '17}, pages = {397\u2013402}, numpages = {6}, keywords = {smart city, IoT, big data analytics}}
@inproceedings{10.1145/3352411.3352450,title = {Research on Safety Control of Refined Oil Depot based on Big Data Technology}, author = {Heng Li , Longfu Zhou , Qian Yang },year = {2019}, isbn = {9781450371414}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3352411.3352450}, doi = {10.1145/3352411.3352450}, abstract = {Big data technology can efficiently collect, store, analyze and mine the massive safety control data which is generated in the daily operation of refined oil depot. It can also realize the early warning of safety risk, complete the safety assessment as well as help make the safety decision of refined oil depot. This paper will firstly introduce the traditional safety control information systems of refined oil depot. Secondly, the study will dive into the different data types of safety control, and then will construct the big data platform for security control. By discussing the key technologies of security control big data platform, this research aims to help build big data of refined oil depot, to promote the development of big data technology within the sector of safety control of refined oil depot, and finally to make it as a \"sharp weapon \" in safety control of refined oil depot.}, location = {Seoul, Republic of Korea}, series = {DSIT 2019}, pages = {249\u2013254}, numpages = {6}, keywords = {Safety control, Big data platform, Refined oil depot, Big data technology}}
@inproceedings{10.1145/2740908.2745843,title = {Mining Social and Urban Big Data}, author = {Yuan Nicholas Jing },year = {2015}, isbn = {9781450334730}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2740908.2745843}, doi = {10.1145/2740908.2745843}, abstract = {In recent years, with the rapid development of positioning technologies, online social networks, sensors and smart devices, large scale human behavioral data are now readily available. The growing availability of such behavioral data provides us unprecedented opportunities to gain more in depth understanding of users in both the physical world and cyber world, especially in online social networks. In this talk, I will introduce our recent research efforts in social and urban mining based on large-scale human behavioral datasets showcased by two projects: 1) LifeSpec: Modeling the spectrum of urban lifestyles based on heterogeneous online social network data. 2) L2P: Inferring demographic attributes from location check-ins.}, location = {Florence, Italy}, series = {WWW '15 Companion}, pages = {1103}, numpages = {1}, keywords = {algorithms, performance, experimentation}}
@inproceedings{10.1145/2983402.2983431,title = {Efficient Batch Processing of Related Big Data Tasks using Persistent MapReduce Technique}, author = {Sidhu Ravneet Kaur , Saroa Charanjiv Singh },year = {2016}, isbn = {9781450343015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2983402.2983431}, doi = {10.1145/2983402.2983431}, abstract = {The data generated by today's enterprises has been increasing at exponential rates in size from most recent couple of years. Also, the need to process and break down the substantial volumes of data has likewise expanded. In order to handle this enormous amount of data and to analyze the same, an open-source usage of Apache system, Hadoop is utilized now-a-days. Hadoop presented a utility computing model which offer replacement of traditional databases and processing techniques. Scalability and high availability of MapReduce makes it the first choice for big data analysis. This paper provides a brief introduction to HDFS and MapReduce. After studying them in detail, it later made to work on related tasks and store the cached result of mapper function which can be used as an input for general reducers. By this additional triggering agent, we were able to achieve the analysis result in approximately half the actual time.}, location = {Jaipur, India}, series = {VisionNet'16}, pages = {106\u2013109}, numpages = {4}, keywords = {Big Data, HDFS, Hadoop, MapReduce}}
@inproceedings{10.1145/1370788.1370799,title = {Data sets and data quality in software engineering}, author = {Liebchen Gernot A. , Shepperd Martin },year = {2008}, isbn = {9781605580364}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1370788.1370799}, doi = {10.1145/1370788.1370799}, abstract = {OBJECTIVE - to assess the extent and types of techniques used to manage quality within software engineering data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets. METHOD - we perform a systematic review of available empirical software engineering studies. RESULTS - only 23 out of the many hundreds of studies assessed, explicitly considered data quality. CONCLUSIONS - first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need more research into means of identifying, and ideally repairing, noisy cases. Third, it should become routine to use sensitivity analysis to assess conclusion stability with respect to the assumptions that must be made concerning noise levels.}, location = {Leipzig, Germany}, series = {PROMISE '08}, pages = {39\u201344}, numpages = {6}, keywords = {data quality, data sets, prediction, empirical research}}
@inproceedings{10.1145/3335656.3335688,title = {Innovation of Data Mining & Screening System under Big Data: Take a case as NIMBY}, author = {Li Minxuan },year = {2019}, isbn = {9781450360906}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3335656.3335688}, doi = {10.1145/3335656.3335688}, abstract = {With the growing maturity of web crawler technology and the advent of the era of big data, when you want to study some problems, you can directly get all the data related to them through web crawlers and other means, but it is more important to mining and filter the data to get valuable data for the research content. This study which based on the word list of keywords uses CRN network to construct semantic distance table and TOPSIS evaluation system to sort data to make sure researchers can obtain quantitative screening data with research value and to provide researchers with scientific screening methods.}, location = {Hong Kong, Hong Kong}, series = {ICDMML 2019}, pages = {70\u201374}, numpages = {5}, keywords = {Data Mining, CRN network, Big Data, TOPSIS evaluation system, Data screening, Keyword list of NIMBY event}}
@inproceedings{10.1145/2912160.2912205,title = {Big Data-based Smart City Platform: Real-Time Crime Analysis}, author = {Ghosh Debopriya , Chun Soon Ae , Shafiq Basit , Adam Nabil R. },year = {2016}, isbn = {9781450343398}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2912160.2912205}, doi = {10.1145/2912160.2912205}, abstract = {One of the challenges governments and communities face to achieve smart city goals is dealing with enormous amount of data available - sensors, devices, social media, Web activities and commerce, tracking devices, all generate enormous amount of data, so called Big Data. Our goal is to empower the city government and its citizens to create a safer city by enabling crime and risk analysis of unstructured crime reports, criminal history of suspects, auto-license data, location-specific data, etc. for crime fighting efforts. We present intelligent solutions for Data-based Smart City Platform in Newark, NJ. We used a Machine Learning approach to automate and help crime analysts identify the connected entities and events by collecting, integrating and analyzing diverse data sources to generate alerts and predictions for new knowledge and insights that lead to better decision making and optimized actions.}, location = {Shanghai, China}, series = {dg.o '16}, pages = {58\u201366}, numpages = {9}, keywords = {smart city, Local Government, machine learning, crime analysis}}
@inproceedings{10.1145/3268891.3268892,title = {Full View Scenario Model of Big Data Governance in Community Safety Service}, author = {Liu Zhao-ge , Li Xiang-yang },year = {2018}, isbn = {9781450365024}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3268891.3268892}, doi = {10.1145/3268891.3268892}, abstract = {In community safety service, big data governance is the prime mode to achieve community safety big data sharing and service value increasing. Although existing researches have preliminarily established the general big data governance framework, identification and governance of specific sharing problems lack comprehensive and systematic scenario description. Applying software engineering method, this paper proposes a kind of scenario expression model of big data governance in community safety service. Considering the common features of big data governance scenarios, construct the meta-scenario model of big data governance in community safety services. Considering the scenario expression difference under different levels, scales and particle sizes, construct the full view scenario model of big data governance in community safety services by meta-models nesting to complete the scenario expression under different applying situation. Finally, a use case is proposed to verify the rationality and effectiveness of the scenario expression models.}, location = {Edinburgh, United Kingdom}, series = {ICICM '18}, pages = {44\u201349}, numpages = {6}, keywords = {community safety, scenario model, big data governance, safety service}}
@inproceedings{10.1145/3299887.3299892,title = {Stream Processing Languages in the Big Data Era}, author = {Hirzel Martin , Baudart Guillaume , Bonifati Angela , Della Valle Emanuele , Sakr Sherif , Akrivi Vlachou Akrivi },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3299887.3299892}, doi = {10.1145/3299887.3299892}, abstract = {This paper is a survey of recent stream processing languages, which are programming languages for writing applications that analyze data streams. Data streams, or continuous data flows, have been around for decades. But with the advent of the big-data era, the size of data streams has increased dramatically. Analyzing big data streams yields immense advantages across all sectors of our society. To analyze streams, one needs to write a stream processing application. This paper showcases several languages designed for this purpose, articulates underlying principles, and outlines open challenges.}, pages = {29\u201340}, numpages = {12}}
@inproceedings{10.1145/3017995.3017998,title = {Towards Integration of Big Data Analytics in Internet of Things Mashup Tools}, author = {Mahapatra Tanmaya , Gerostathopoulos Ilias , Prehofer Christian },year = {2016}, isbn = {9781450348744}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3017995.3017998}, doi = {10.1145/3017995.3017998}, abstract = {The increasing number and sensing capabilities of connected devices offer unique opportunities for developing sophisticated applications that employ data analysis as part of their business logic to make informed decisions based on sensed data. So far, mashup tools have been successful in supporting application development for Internet of Things. At the same time, Big Data analytics tools have allowed the analysis of very large and diverse data sets. The problem is that there is no consolidated development approach for integrating the two fields, IoT mashups and Big Data analytics. Such integration should go beyond merely specifying IoT mashups that only act as data providers. Mashup developers should also be able to specify Big Data analytics jobs and consume their results within a single application model. In this paper, we contribute to the direction of integrating Big Data analytics with IoT mashup tools by highlighting the need for such integration and the challenges that it entails via concrete examples. We also provide a research and development roadmap that can pave the way forward.}, location = {Stuttgart, Germany}, series = {WoT '16}, pages = {11\u201316}, numpages = {6}, keywords = {IoT mashups, Development support, Big Data analytics}}
@inproceedings{10.1145/2660168.2660177,title = {Prospects for a Big Data History of Music}, author = {Rose Stephen , Tuppen Sandra },year = {2014}, isbn = {9781450330022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2660168.2660177}, doi = {10.1145/2660168.2660177}, abstract = {This position paper sets out the possibility of a musicology based on the analysis of musical-bibliographical metadata as Big Data. It outlines the work underway, as part of the AHRC-funded project A Big Data History of Music, to align seven major datasets of musical-bibliographical metadata. After discussing some of the technical challenges of data alignment, it suggests how analysis and visualization of this data might transform musicological understandings of cultural transmission and canon formation.}, location = {London, United Kingdom}, series = {DLfM '14}, pages = {1\u20133}, numpages = {3}, keywords = {bibliography, Musicology, metadata, canon, music publishing}}
@inproceedings{10.1145/2905055.2905211,title = {Implementing and Analyzing Big Data Techniques withSpring Frame Work in Java& J2EEBased Application}, author = {Saxena Ankur , Kaushik Neeraj , Kaushik Nidhi },year = {2016}, isbn = {9781450339629}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2905055.2905211}, doi = {10.1145/2905055.2905211}, abstract = {In the time of big data techniques with spring framework on java, web servers or application server as the significant channel in big data should be updated to meet execution and force imperatives. Significant endeavors have been put resources into web server or application server conveyance and web storing procedures, but very few efforts have been paid to improve hardware-favored web type services. Big Data with spring framework in java is a promising business and computing model in web framework. Spring is the most popular open source Java application Framework. It combines all the industry-standard frameworks (for e.g. Struts and Hibernate) and approaches into one bundle.The expense and working costs of data centers have skyrocketed with the increase in computing capacity.Big data is a concept that defines the large volume of both structured and unstructured data -- that inundates a business on a day-to-day environment. This research argues the need to provide novel method and tools to bolster programming engineers meaning to enhance vitality productivity and minimize the subsequent from outlining, creating, sending and running programming in Big Data with spring framework.}, location = {Udaipur, India}, series = {ICTCS '16}, pages = {1\u20136}, numpages = {6}, keywords = {spring, Hadoop Framework, J2ee, Big Data, Java}}
@inproceedings{10.1145/3388142.3388164,title = {Fast Automatic Determination of Cluster Numbers for High Dimensional Big Data}, author = {Safari Zohreh , Mursi Khalid T. , Zhuang Yu },year = {2020}, isbn = {9781450376440}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3388142.3388164}, doi = {10.1145/3388142.3388164}, abstract = {For a large volume of data, the clustering algorithm is of significant importance to categorize and analyze data. Accordingly, choosing the optimal number of clusters (K) is an essential factor, but it also is a tricky problem in big data analysis. More importantly, it is to efficiently determine the best K automatically, which is the main issue in clustering algorithms. Indeed, considering both the quality and efficiency of the clustering algorithm during defining K can be a trade-off that is our primary purpose to overcome. K-Means is still one of the popular clustering algorithms, which has a shortcoming that K needs to be pre-set. We introduce a new process with fewer K-Means running, which selects the most promising time to run the K-Means algorithm. To achieve this goal, we applied Bisecting K-Means and a different splitting measure, which all are contributed to efficiently determine the number of clusters automatically while maintaining the quality of clustering for a large set of high dimensional data. We carried out our experimental studies on different data sets and found that our procedure has the flexibility of choosing different criteria for determining the optimal K under each of them. Experiments indicate higher efficiency through decreasing of computation cost compared with the Ray&Turi method or with the use of only the K-Means algorithm.}, location = {Silicon Valley, CA, USA}, series = {ICCDA 2020}, pages = {50\u201357}, numpages = {8}, keywords = {Big Data, K-Means, Clustering, Cluster Validity, Bisecting K-Means}}
@inproceedings{10.1145/2989214,title = {Introduction to Special Issue on Multimedia Big Data: Networking}, author = {Dong Mianxiong , Piuri Vincenzo , Chan Shueng-Han Gary , Jain Ramesh },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2989214}, doi = {10.1145/2989214}, pages = {1\u20133}, numpages = {3}}
@inproceedings{10.1145/3105831.3105842,title = {Evaluating SQL-on-Hadoop for Big Data Warehousing on Not-So-Good Hardware}, author = {Santos Maribel Yasmina , Costa Carlos , Galv\u00e3o Jo\u00e3o , Andrade Carina , Martinho Bruno Augusto , Lima Francisca Vale , Costa Eduarda },year = {2017}, isbn = {9781450352208}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3105831.3105842}, doi = {10.1145/3105831.3105842}, abstract = {Big Data is currently conceptualized as data whose volume, variety or velocity impose significant difficulties in traditional techniques and technologies. Big Data Warehousing is emerging as a new concept for Big Data analytics. In this context, SQL-on-Hadoop systems increased notoriety, providing Structured Query Language (SQL) interfaces and interactive queries on Hadoop. A benchmark based on a denormalized version of the TPC-H is used to compare the performance of Hive on Tez, Spark, Presto and Drill. Some key contributions of this work include: the direct comparison of a vast set of technologies; unlike previous scientific works, SQL-on-Hadoop systems were connected to Hive tables instead of raw files; allow to understand the behaviour of these systems in scenarios with ever-increasing requirements, but not-so-good hardware. Besides these benchmark results, this paper also makes available interesting findings regarding an architecture and infrastructure in SQL-on-Hadoop for Big Data Warehousing, helping practitioners and fostering future research.}, location = {Bristol, United Kingdom}, series = {IDEAS '17}, pages = {242\u2013252}, numpages = {11}, keywords = {Data Warehouse, Big Data Warehousing, Hadoop, Drill, Presto, Hive, Spark, Big Data, SQL-on-Hadoop, Benchmark}}
@inproceedings{10.1109/CCGrid.2016.85,title = {Towards memory-optimized data shuffling patterns for big data analytics}, author = {Nicolae Bogdan , Costa Carlos , Misale Claudia , Katrinis Kostas , Park Yoonho },year = {2016}, isbn = {9781509024520}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2016.85}, doi = {10.1109/CCGrid.2016.85}, abstract = {Big data analytics is an indispensable tool in transforming science, engineering, medicine, healthcare, finance and ultimately business itself. With the explosion of data sizes and need for shorter time-to-solution, in-memory platforms such as Apache Spark gain increasing popularity. However, this introduces important challenges, among which data shuffling is particularly difficult: on one hand it is a key part of the computation that has a major impact on the overall performance and scalability so its efficiency is paramount, while on the other hand it needs to operate with scarce memory in order to leave as much memory available for data caching. In this context, efficient scheduling of data transfers such that it addresses both dimensions of the problem simultaneously is non-trivial. State-of-the-art solutions often rely on simple approaches that yield sub-optimal performance and resource usage. This paper contributes a novel shuffle data transfer strategy that dynamically adapts to the computation with minimal memory utilization, which we briefly underline as a series of design principles.}, location = {Cartagena, Columbia}, series = {CCGRID '16}, pages = {409\u2013412}, numpages = {4}, keywords = {memory-efficient I/O, elastic buffering, data shuffling, big data analytics}}
@inproceedings{10.1109/CCGrid.2015.174,title = {Cloud-based OLAP over big data: application scenarios and performance analysis}, author = {Cuzzocrea Alfredo , Moussa Rim , Xu Guandong , Grasso Giorgio Mario },year = {2015}, isbn = {9781479980062}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2015.174}, doi = {10.1109/CCGrid.2015.174}, abstract = {Following our previous research results, in this paper we provide two authoritative application scenarios that build on top of OLAP*, a middleware for parallel processing of OLAP queries that truly realizes effective and efficiently OLAP over Big Data. We have provided two authoritative case studies, namely parallel OLAP data cube processing and virtual OLAP data cube design, for which we also propose a comprehensive performance evaluation and analysis. Derived analysis clearly confirms the benefits of our proposed framework.}, location = {Shenzhen, China}, series = {CCGRID '15}, pages = {921\u2013927}, numpages = {7}}
@inproceedings{10.1145/3492324,title = {2021 IEEE/ACM 8th International Conference on Big Data Computing, Applications and Technologies (BDCAT '21)},year = {2021}, isbn = {9781450391641}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Leicester, United Kingdom}}
@inproceedings{10.1145/3234698.3234758,title = {Adaptive Management Approach for more Availability of Big Data Business Analytics}, author = {Chehbi-Gamoura Samia , Derrouiche Ridha , Malhotra Manisha , Koruca Halil-Ibrahim },year = {2018}, isbn = {9781450363921}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3234698.3234758}, doi = {10.1145/3234698.3234758}, abstract = {With the Big Data management, and the propagation of business lines into complex networks, activities are ever more subject to disasters than ever. It is nearly impossible to forecast their happening and degree of related costs. Accordingly, organizations try to collaborate in risk management. This paper outlines and discusses a generic approach based on Fuzzy Cognitive Maps (FCM) for cross-management of Disaster Recovery Plans (DRP). A set of basics of disaster planning is also provided. The proposed approach is focused on risk assessment methodology. The method is able to aggregate all assessment variables of the whole stakeholders involved in the business network. The main outcomes of this study aim to support networked enterprises in improving risk readiness capability and disaster recovery. Finally, we indicate the open challenges for further researches and an outlook on our future research.}, location = {Istanbul, Turkey}, series = {ICEMIS '18}, pages = {1\u20138}, numpages = {8}, keywords = {Fuzzy Cognitive Map, Disaster Recovery Plan, adaptive management, Big Data, networked enterprises}}
@inproceedings{10.1145/3554729,title = {AUC Maximization in the Era of Big Data and AI: A Survey}, author = {Yang Tianbao , Ying Yiming },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3554729}, doi = {10.1145/3554729}, abstract = {Area under the ROC curve, a.k.a. AUC, is a measure of choice for assessing the performance of a classifier for imbalanced data. AUC maximization refers to a learning paradigm that learns a predictive model by directly maximizing its AUC score. It has been studied for more than two decades dating back to late 90s and a huge amount of work has been devoted to AUC maximization since then. Recently, stochastic AUC maximization for big data and deep AUC maximization (DAM) for deep learning have received increasing attention and yielded dramatic impact for solving real-world problems. However, to the best our knowledge there is no comprehensive survey of related works for AUC maximization. This paper aims to address the gap by reviewing the literature in the past two decades. We not only give a holistic view of the literature but also present detailed explanations and comparisons of different papers from formulations to algorithms and theoretical guarantees. We also identify and discuss remaining and emerging issues for DAM, and provide suggestions on topics for future work.}, keywords = {ROC, deep learning, AUC, big data}}
@inproceedings{10.1145/3299869.3320240,title = {Cost-Effective, Workload-Adaptive Migration of Big Data Applications to the Cloud}, author = {Giannakouris Victor , Fernandez Alejandro , Simitsis Alkis , Babu Shivnath },year = {2019}, isbn = {9781450356435}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3299869.3320240}, doi = {10.1145/3299869.3320240}, abstract = {More than 10,000 enterprises worldwide use the big data stack composed of multiple distributed systems. At Unravel, we build the next-generation APM platform for the big data stack, and we have worked with a representative sample of these enterprises that covers most industry verticals. This sample covers the spectrum of choices for deploying the big data stack across on-premises datacenters, private and public cloud deployments, and hybrid combinations of these. In this paper, we present a solution for assisting enterprises planning the migration of their big data stacks from on-premises deployments to the cloud. Our solution is goal driven and adapts to various migration scenarios. We present the system architecture we built and several cloud mapping options. We also describe a demonstration script that involves practical, real-world use-cases of the path to cloud adoption.}, location = {Amsterdam, Netherlands}, series = {SIGMOD '19}, pages = {1909\u20131912}, numpages = {4}, keywords = {big data stack, cloud migration, application performance management}}
@inproceedings{10.1145/1651291.1651303,title = {Generating data quality rules and integration into ETL process}, author = {Rodic Jasna , Baranovic Mirta },year = {2009}, isbn = {9781605588018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1651291.1651303}, doi = {10.1145/1651291.1651303}, abstract = {Many data quality projects are integrated into data warehouse projects without enough time allocated for the data quality part, which leads to a need for a quicker data quality process implementation that can be easily adopted as the first stage of data warehouse implementation. We will see that many data quality rules can be implemented in a similar way, and thus generated based on metadata tables that store information about the rules. These generated rules are then used to check data in designated tables and mark erroneous records, or to do certain updates of invalid data. We will also store information about the rules violations in order to provide analysis of such data. This could give a significant insight into our source systems. Entire data quality process will be integrated into ETL process in order to achieve load of data warehouse that is as automated, as correct and as quick as possible. Only small number of records would be left for manual inspection and reprocessing.}, location = {Hong Kong, China}, series = {DOLAP '09}, pages = {65\u201372}, numpages = {8}, keywords = {metadata, rules, generator, oracle, data quality}}
@inproceedings{10.1145/3482632.3483143,title = {English Teaching Evaluation System Based on Big Data}, author = {Zhang Yan },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483143}, doi = {10.1145/3482632.3483143}, abstract = {With the advent of the era of big data, big data application systems in various fields emerge as the times require. In English teaching evaluation system is no exception. As a widely used language in the world, English education evaluation system is also multifarious, which attracts the attention of people from all walks of life. English automatic teaching evaluation system is an evaluation system based on big data, which meets the requirements of teaching informatization and new curriculum standard. In order to explore the teaching effect of the existing English teaching evaluation system, find out the problems, and then put forward the corresponding suggestions. Based on the research of related literature, using the teaching experiment method, questionnaire survey method and analytic hierarchy process, this paper analyzes the students' style, the teaching effect of the automatic evaluation system, the specific vocabulary accumulation and accuracy, the use of sentence length and total number of sentences, and the students' satisfaction with the teaching evaluation system. There are subtle differences in the effect of teaching evaluation, whether it is from the initial vocabulary accumulation of 10 to the later vocabulary accumulation of 15, or from the initial total number of sentences of 10 to 12, it shows the improvement of teaching effect. In the satisfaction survey of the evaluation system, the satisfaction is as high as 50%. Therefore, for the automatic English teaching evaluation system, its effect and satisfaction is excellent, but the lack of style diversity, which requires the continuous improvement of English teaching evaluation system.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1321\u20131325}, numpages = {5}}
@inproceedings{10.1145/253769.253804,title = {Data quality in context}, author = {Strong Diane M. , Lee Yang W. , Wang Richard Y. },year = {1997}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/253769.253804}, doi = {10.1145/253769.253804}, pages = {103\u2013110}, numpages = {8}}
@inproceedings{10.1145/3293614.3293624,title = {Building Software Products with use Open Data and Big Data in Smart Cities}, author = {Santos Anne C. M. , Pereira \u00c1vner J. S. , Oliveira Manoela R. , Macedo Hendrik T. , Nascimento Rog\u00e9rio P. C. },year = {2018}, isbn = {9781450365727}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3293614.3293624}, doi = {10.1145/3293614.3293624}, abstract = {The use of Big Data and Open Data has been increasing and becoming a tendency in the last years. Big Data is about collect, store and analysis and interpretation of datasets so big and complex that traditional applications of data processing are not appropriate to your treatment. Open Data is related to opening of data: by opening, it is understood that data must be public and available free. Looking for information transparency, government data should be open. Every day more cities and countries are opening your data. The Open Data emerge as a special paradigm in smart cities. The main goal of Big and Open Data technologies in a smart city is provide system development that can be useful to the citizens. In this work, we analyze the state of utilization of Big Data and Open data in technological solutions with interoperability between software products to smart cities. 79 publications were found and 25 of them were selected, listing some technologies able to answer the research questions of this work.}, location = {Fortaleza, Brazil}, series = {EATIS '18}, pages = {1\u20137}, numpages = {7}, keywords = {Open Data, Smart City, Big Data}}