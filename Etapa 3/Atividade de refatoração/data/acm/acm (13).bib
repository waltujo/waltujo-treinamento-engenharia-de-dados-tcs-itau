@inproceedings{10.1145/3197091.3205834,
author = {Sooriamurthi, Raja},
title = {Introducing Big Data Analytics in High School and College},
year = {2018},
isbn = {9781450357074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197091.3205834},
doi = {10.1145/3197091.3205834},
abstract = {In this teaching tip and courseware note we describe a series of hands on activities and exercises that we've used to introduce the notion of big data analytics to a wide range of audience. These exercises range in complexity from a paper and pencil thought exercise, to using Google Trends for simple explorations, to using a spread sheet to simulate the iterative nature of Google's PageRank algorithm, to programming with a Python based map-reduce framework. These exercises have been used in courses to train high school teachers in data science, full semester university courses (undergraduate and graduate), and CS education outreach efforts. Feedback has been positive as to their efficacy.},
booktitle = {Proceedings of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education},
pages = {373–374},
numpages = {2},
keywords = {Big data, Outreac, PageRank, Google Trends, Map Reduce},
location = {Larnaca, Cyprus},
series = {ITiCSE 2018}
}

@inproceedings{10.1145/3335484.3335545,
author = {Zhi-peng, Sun and Gui-ming, Chen and Hui, Zhang},
title = {Evaluation of Large-Scale Complex Systems Effectiveness Based on Big Data},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335545},
doi = {10.1145/3335484.3335545},
abstract = {With the advent of the information age, big data technology came into being. The wide application of big data brings new opportunities and challenges to the construction of national defense and military information. Under the background of information-based joint operations characterized by large complex systems, how to scientifically and rationally plan the construction of large complex systems, and maximize the effectiveness of the complex system has become a key concern for system construction decision makers and researchers. This paper combines the application of big data in the construction of large complex systems, and focuses on the evaluation of the effectiveness of large complex systems based on big data, which can be used for reference by relevant researchers.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {72–76},
numpages = {5},
keywords = {large-scale complex systems, big data, effectiveness, evaluation},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@inproceedings{10.1145/3220199.3220221,
author = {Lv, Bin and Yu, Xuemin and Xu, Guokun and Yin, Qilei and Shi, Zhixin},
title = {Network Traffic Monitoring System Based on Big Data Technology},
year = {2018},
isbn = {9781450364263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220199.3220221},
doi = {10.1145/3220199.3220221},
abstract = {With the rapid growth of network traffic and the increasing rich methods of network attacks, traditional network traffic monitoring system cannot meet the requirements of data storage and query in real time. Therefore, how to monitor the large scale network traffic effectively has become an important challenge for network security management. Aiming at it, we propose a new network monitoring system where Netflow as the monitoring object based on big data technology, which has four main functions: it can use Filebeat to collect Netflow in real time; it transfers the data reliably based on Logstash; it stores the data in ElasticSearch, it analyzes and displays the data in real time through Kabana. The experimental results show that our system is capable of meeting millisecond responses to 100 million of Netflows. It can meet the requirements of real-time monitoring for large-scale network traffic, and provide the basis for network security control.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Computing},
pages = {27–32},
numpages = {6},
keywords = {Kibana, Network traffic monitoring, ElasticSearch, Logstash, Filebeat, Netflow},
location = {Shenzhen, China},
series = {ICBDC '18}
}

@inproceedings{10.1145/2024587.2024599,
author = {Sneed, Harry M. and Majnar, Rudolf},
title = {A Process for Assessing Data Quality},
year = {2011},
isbn = {9781450308519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024587.2024599},
doi = {10.1145/2024587.2024599},
abstract = {Abstract: This industrial report stems from practical experience in assessing the quality of customer databases. The process it describes unites three automated audits, - an audit of the database schema, an audit of the database structure and an audit of the database content. The audit of the database schema checks for design smells and rule violations. The audit of the database structure measures the size, complexity and quality of the database model. The audit of the database content processes the data itself to uncover invalid data values, missing records and redundant records. The purpose of these audits is to assess the quality of the database and to determine whether a data reengineering or data clean-up project is required.},
booktitle = {Proceedings of the 8th International Workshop on Software Quality},
pages = {50–57},
numpages = {8},
keywords = {data content validation, data auditing, data quality, data metrics},
location = {Szeged, Hungary},
series = {WoSQ '11}
}

@inproceedings{10.1145/3230348.3230425,
author = {Xia, Huan and Tang, Shiqi and Li, Shuang and Yu, Xiaomin},
title = {Application Research of Big Data E-Commerce in Closed Community},
year = {2018},
isbn = {9781450363754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230348.3230425},
doi = {10.1145/3230348.3230425},
abstract = {Big data e-commerce is melting into our lives with an overwhelming trend, the establishment of big data e-commerce in closed communities is much easier to meet the consumer demand of community residents. This article is based on the analysis of the development of big data electronic commerce, presented establish big data electronic commerce in a closed community, to take Guizhou University of Finance and Economics "Trust Me" platform as an example. It introduces the functional requirements, platform structure, idea, operation mechanism and operation mode of the platform, and summarizes the advantages and improvement.},
booktitle = {Proceedings of the 2018 1st International Conference on Internet and E-Business},
pages = {43–46},
numpages = {4},
keywords = {big data, e-commerce, closed community},
location = {Singapore, Singapore},
series = {ICIEB '18}
}

@inproceedings{10.1145/3175684.3175688,
author = {Tan, Chekfoung and Haji, Mohammed},
title = {Big Data Educational Portal for Small and Medium Sized Enterprises (SMEs)},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175688},
doi = {10.1145/3175684.3175688},
abstract = {Big Data refers to the massive amount of data generated from IT systems, sensors, and mobile devices. The values of big data are achieved by descriptive, predictive and prescriptive analytics. Small and Medium Sized Enterprises (SMEs) play a significant role in contributing to economic development. Big data is seen as a strategic and innovative tool for SMEs to stay competitive in the marketplace. However, there is lack of research in studying the value of big data to SMEs. Moreover, due to the shortage of quality learning platforms, SMEs have limited understanding of the potential benefits big data offers their businesses. This research aims to propose an educational portal of big data for SMEs by incorporating the pedagogy aspects. The research is underpinned by design science research. The portal contributes theoretically and methodologically by deriving the design knowledge of such portal and practically by increasing big data knowledge among SMEs.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {11–15},
numpages = {5},
keywords = {Educational Portal, Pedagogy, Design Science Research, Small and Medium Sized Enterprises, Big Data},
location = {London, United Kingdom},
series = {BDIOT2017}
}

@inproceedings{10.1145/2534921.2534924,
author = {Chen, Jiaoyan and Chen, Huajun and Pan, Jeff Z. and Wu, Ming and Zhang, Ningyu and Zheng, Guozhou},
title = {When Big Data Meets Big Smog: A Big Spatio-Temporal Data Framework for China Severe Smog Analysis},
year = {2013},
isbn = {9781450325349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2534921.2534924},
doi = {10.1145/2534921.2534924},
abstract = {Recently, the appearing disaster of severe smog has been attacking many cities in China such as the capital Beijing. The chief culprit of China smog, namely PM2.5, is affected by various factors including air pollutants, weather, climate, geographical location, urbanization, etc. To analyze the factors, we collect about 35,000,000 air quality records and about 30,000,000 weather records from the sensors in 77 China's cities in 2013. Moreover, two big data sets named Geoname and DBPedia are also combined for the data of climate, geographical location and urbanization. To deal with big spatio-temporal data for big smog analysis, we propose a MapReduce-based framework named BigSmog. It mainly conducts parallel correlation analysis of the factors and scalable training of artificial neural networks for spatio-temporal approximation of the concentration of PM2.5. In the experiments, BigSmog displays high scalability for big smog analysis with big spatio-temporal data. The analysis result shows that the air pollutants influence the short-term concentration of PM2.5 more than the weather and the factors of geographical location and climate rather than urbanization play a major role in determining a city's long-term pollution level of PM2.5. Moreover, the trained ANNs can accurately approximate the concentration of PM2.5.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {13–22},
numpages = {10},
keywords = {MapReduce, PM2.5, artificial neural network, China smog, spatio-temporal, correlation analysis},
location = {Orlando, Florida},
series = {BigSpatial '13}
}

@inproceedings{10.1145/3006386.3006391,
author = {Sorokine, Alexandre and Karthik, Rajasekar and King, Anthony and Budhendra, Bhaduri},
title = {Big Data as a Service from an Urban Information System},
year = {2016},
isbn = {9781450345811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006386.3006391},
doi = {10.1145/3006386.3006391},
abstract = {Big Data has already proven itself as a valuable tool that lets geographers and urban researchers utilize large data resources to generate new insights. However, wider adoption of Big Data techniques in these areas is impeded by a number of difficulties in both knowledge discovery and data and science production. Typically users face such problems as disparate and scattered data, data management, spatial searching, insufficient computational capacity for data-driven analysis and modelling, and the lack of tools to quickly visualize and summarize large data and analysis results. Here we propose an architecture for an Urban Information System (UrbIS) that mitigates these problems by utilizing the Big Data as a Service (BDaaS) concept. With technological roots in High-performance Computing (HPC), BDaaS is based on the idea of outsourcing computations to different computing paradigms, scalable to super-computers. UrbIS aims to incorporate federated metadata search, integrated modeling and analysis, and geovisualization into a single seamless workflow. The system is under active development and is built around various emerging technologies that include hybrid and NoSQL databases, massively parallel systems, GPU computing, and WebGL-based geographic visualization. UrbIS is designed to facilitate the use of Big Data across multiple cities to better understand how urban areas impact the environment and how climate change and other environmental change impact urban areas.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {34–41},
numpages = {8},
keywords = {environmental change impact, big data as a service, high-performance geocomputing, urban informatics},
location = {Burlingame, California},
series = {BigSpatial '16}
}

@inproceedings{10.1145/2938503.2938539,
author = {Cassavia, Nunziato and Ciampi, Mario and De Pietro, Giuseppe and Masciari, Elio},
title = {A Big Data Approach For Querying Data in EHR Systems},
year = {2016},
isbn = {9781450341189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938503.2938539},
doi = {10.1145/2938503.2938539},
abstract = {Information management in healthcare is nowadays experiencing a great revolution. After the impressive progress in digitizing medical data by private organizations, also the federal government and other public stakeholders have also started to make use of healthcare data for data analysis purposes in order to extract actionable knowledge. In this paper, we propose an architecture for supporting interoperability in healthcare systems by exploiting Big Data techniques. In particular, we describe a proposal based on big data techniques to implement a nationwide system able to improve EHR data access efficiency and reduce costs.},
booktitle = {Proceedings of the 20th International Database Engineering &amp; Applications Symposium},
pages = {212–217},
numpages = {6},
keywords = {Big data, Healthcare, Interoperability},
location = {Montreal, QC, Canada},
series = {IDEAS '16}
}

@inproceedings{10.5555/2555523.2555560,
author = {Whitmer, Barbara},
title = {CIVDDD Collaborative Research in Big Data Analytics and Visualization},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {The Centre for Innovation in Information Visualization and Data-Driven Design (CIVDDD) is a Big Data research project collaboration funded by the Ontario Research Fund -- Research Excellence (ORF-RE). Research collaborators in the project include York University, OCAD University, the University of Toronto, and private sector partners (PSPs) to develop the next generation of data discovery, design, analytics, and visualization techniques for new computational tools, representational strategies, and interfaces. As the preeminent research hub for information analytics and scientific visualization in Ontario, CIVDDD has fifteen research teams in the four theme areas of Bioinformatics and Medical Applications, Interactive Visualization, Textual Visualization, and Scientific Visualization. The Workshop included a brief overview of CIVDDD research by the Principal Investigator Dr. Amir Asif, followed by three CIVDDD team presentations and demonstrations related to CASCON 2013 themes. These included: Graph Analytics and Biological Network Structures (Big Data and Cloud Computing), Social Media Data Visualization (Social Computing), and Dynamic Carbon Mapping in Urban Environments (Mobile Computing). Each Workshop presentation contained academic researchers and their private sector partner research collaborators. Each presentation was followed by a demonstration of the research application or visualization, and Q&amp;A. An open discussion concluded the Workshop.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {344–346},
numpages = {3},
location = {Ontario, Canada},
series = {CASCON '13}
}

@inproceedings{10.1145/3183519.3183528,
author = {Selakovic, Marija and Barnett, Michael and Musuvathi, Madan and Mytkowicz, Todd},
title = {Cross-Language Optimizations in Big Data Systems: A Case Study of SCOPE},
year = {2018},
isbn = {9781450356596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183519.3183528},
doi = {10.1145/3183519.3183528},
abstract = {Building scalable big data programs currently requires programmers to combine relational (SQL) with non-relational code (Java, C#, Scala). Relational code is declarative - a program describes what the computation is and the compiler decides how to distribute the program. SQL query optimization has enjoyed a rich and fruitful history, however, most research and commercial optimization engines treat non-relational code as a black-box and thus are unable to optimize it.This paper empirically studies over 3 million SCOPE programs across five data centers within Microsoft and finds programs with non-relational code take between 45-70% of data center CPU time. We further explore the potential for SCOPE optimization by generating more native code from the non-relational part. Finally, we present 6 case studies showing that triggering more generation of native code in these jobs yields significant performance improvement: optimizing just one portion resulted in as much as 25% improvement for an entire program.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
pages = {45–54},
numpages = {10},
location = {Gothenburg, Sweden},
series = {ICSE-SEIP '18}
}

@inproceedings{10.5555/2460396.2460415,
author = {Sadiq, Shazia and Yeganeh, Naiem Khodabandehloo and Indulska, Marta},
title = {20 Years of Data Quality Research: Themes, Trends and Synergies},
year = {2011},
isbn = {9781920682958},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Data Quality is a cross-disciplinary and often domain specific problem due to the importance of fitness for use in the definition of data quality metrics. It has been the target of research and development for over 4 decades by business analysts, solution architects, database experts and statisticians to name a few. However, the changing landscape of data quality challenges indicate the need for holistic solutions. As a first step towards bridging any gaps between the various research communities, we undertook a comprehensive literature study of data quality research published in the last two decades. In this study we considered a broad range of Information System (IS) and Computer Science (CS) publication (conference and journal) outlets. The main aims of the study were to understand the current landscape of data quality research, to create better awareness of (lack of) synergies between various research communities, and, subsequently, to direct attention towards holistic solutions. In this paper, we present a summary of the findings from the study, that include a taxonomy of data quality problems, identification of the top themes, outlets and main trends in data quality research, as well as a detailed thematic analysis that outlines the overlaps and distinctions between the focus of IS and CS publications.},
booktitle = {Proceedings of the Twenty-Second Australasian Database Conference - Volume 115},
pages = {153–162},
numpages = {10},
keywords = {research framework, literature survey, data quality},
location = {Perth, Australia},
series = {ADC '11}
}

@article{10.1145/2641225,
author = {Greengard, Samuel},
title = {Weathering a New Era of Big Data},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/2641225},
doi = {10.1145/2641225},
abstract = {Increased computing power combined with new and more advanced models are changing weather forecasting.},
journal = {Commun. ACM},
month = {sep},
pages = {12–14},
numpages = {3}
}

@inproceedings{10.1145/2611286.2611311,
author = {Stojanovic, Nenad and Stojanovic, Ljiljana and Xu, Yongchun and Stajic, Boban},
title = {Mobile CEP in Real-Time Big Data Processing: Challenges and Opportunities},
year = {2014},
isbn = {9781450327374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611286.2611311},
doi = {10.1145/2611286.2611311},
abstract = {The expansion of the mobile applications in various domains has opened the issue of the optimal usage of the limited resources (e.g. data storage capacity and processing power). This is especially important for the so-called data-intensive applications, which deal with huge amount of mobile data, like in the case of m-Health (i.e. wearable sensing). Many of these applications are oriented towards detecting of particular real-time situations, which brings them to the domain of Event Processing. However, we argue that real-time, big data driven applications require a novel infrastructure for distributed complex event processing that is only partially executed on the mobile devices. This tutorial paper presents a foundation for an efficient development of such mobile applications, by introducing a mobile-driven distributed CEP infrastructure. We present the technical details of the infrastructure and its initial implementation.},
booktitle = {Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems},
pages = {256–265},
numpages = {10},
keywords = {remote personal monitoring, mobile complex event processing, semantic technologies, fast and big data},
location = {Mumbai, India},
series = {DEBS '14}
}

@inproceedings{10.1145/3358331.3358341,
author = {Zhang, Lihong and Wang, Juan and He, Wei and Zhang, Peng and Zhang, Shuangshi},
title = {WeChat Rumor Analysis and Governance Based on Big Data},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358341},
doi = {10.1145/3358331.3358341},
abstract = {Based on the 200 rumor hot lists in recent three years, which were refuted by WeChat platform rumor filter, this paper conducted qualitative and quantitative analysis on the word frequency distribution, theme characteristics and hot categories of rumors, and proposed the rumor governance strategy based on big data thinking. The research results help us to grasp the mechanism and rules of rumor generation and propagation under the background of new media, and the proposed rumor governance strategies can also provide guidance for the online rumor regulatory authorities.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {10},
numpages = {4},
keywords = {word frequency, WeChat, rumor, TF-IDF, governance},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@inproceedings{10.1145/3264560.3266429,
author = {Cuzzocrea, Alfredo},
title = {Scalable Privacy-Preserving Big Data Management and Analytics: Where We Are and Where We Are Going},
year = {2018},
isbn = {9781450364744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264560.3266429},
doi = {10.1145/3264560.3266429},
abstract = {While several research efforts have been developed in the context of privacy-preserving big data management and analytics re- cently, relevant challenges arise when such models, techniques and algorithms must be delivered on top of massive, distributed big data repositories. This problem opens the door to the design of innovative models, techniques and algorithms that, contrary to actual proposals, are able to inject the scalability feature during the privacy-preserving big data management and analytics phase. On the basis of these considerations, this paper provides an overview on actual problems and limitations of state-of-the-art techniques, along with the proposal of an effective framework for supporting scalable privacy-preserving big data management and analytics.},
booktitle = {Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing},
pages = {52–56},
numpages = {5},
keywords = {Privacy-Preserving Big Data Management and Analysis, Privacy- Presering Big Data Frameworks, Scalable Privacy-Preserving Big Data Management and Analysis},
location = {Barcelona, Spain},
series = {ICCBDC'18}
}

@inproceedings{10.1145/3230744.3230753,
author = {Prophet, Jane and Kow, Yong Ming and Hurry, Mark},
title = {Small Trees, Big Data: Augmented Reality Model of Air Quality Data via the Chinese Art of "Artificial" Tray Planting},
year = {2018},
isbn = {9781450358170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230744.3230753},
doi = {10.1145/3230744.3230753},
abstract = {Our prototype app, Pocket Penjing, built using Unity3D, takes its name from the Chinese "Penjing." These tray plantings of miniature trees pre-date bonsai, often including miniature benches or figures to allude to people's relationship to the tree. App users choose a species, then create and name their tree. Swiping rotates a 3D globe showing flagged locations. Each flag represents a live online air quality monitoring station data stream that the app can scrape. Data is pulled in from the selected station and the AR window loads. The AR tree grows in real-time 3D. Its L-Systems form is determined by the selected live air quality data. We used this prototype as the basis of a two-part formative participatory design workshop with 63 participants.},
booktitle = {ACM SIGGRAPH 2018 Posters},
articleno = {16},
numpages = {2},
keywords = {augmented reality, polyaesthetics, gamification},
location = {Vancouver, British Columbia, Canada},
series = {SIGGRAPH '18}
}

@inproceedings{10.1145/3255779,
author = {Candan, Selcuk K.},
title = {Session Details: Research Session 28: Big Data},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255779},
doi = {10.1145/3255779},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/3406601.3406628,
author = {Chalumporn, Gantaphon and Hewett, Rattikorn},
title = {Health Data Analytics with an Opportunistic Big Data Algorithm},
year = {2020},
isbn = {9781450377591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406601.3406628},
doi = {10.1145/3406601.3406628},
abstract = {In data-driven society, health data can lead to profound impacts on public safety policies, epidemic modeling, and advancement of health science and medicine. This paper presents an approach to automatically elucidating useful information from "Big" health data. In particular, we analyze manufactured cosmetic products containing chemicals that are known or suspected to cause cancer, birth defects, or developmental and reproductive harm. Our analysis is based on the Apriori algorithm, the heart of the popular Association Rule Mining to discover associations among sets of influencing factors. However, with rapid growth of huge amount of data, including ours, existing data analytics algorithms designed for in-memory data are not adequate. Most Big data analytics algorithms are implemented on MapReduce framework for execution in parallel and distributed environments. Unlike traditional implementation, our approach employs an opportunistic MapReduce-based Apriori algorithm to fully exploit parallelism. The paper describes the algorithm and presents our findings, from 113, 179 data instances, both in terms of the execution times and the discovered associations among product profiles. For a support threshold of 10% (5%,), 20 (53) association rules are obtained with an improved execution time over that of the traditional MapReduce-based algorithm by 14.6% (40.3%) on the average over three machines.},
booktitle = {Proceedings of the 11th International Conference on Advances in Information Technology},
articleno = {23},
numpages = {9},
keywords = {Association rules mining, MapReduce, Big Data Algorithms},
location = {Bangkok, Thailand},
series = {IAIT2020}
}

@inproceedings{10.1145/3257759,
author = {April, Alain},
title = {Session Details: Big Data Analytics for Health},
year = {2016},
isbn = {9781450342247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257759},
doi = {10.1145/3257759},
booktitle = {Proceedings of the 6th International Conference on Digital Health Conference},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {DH '16}
}

@inproceedings{10.1145/3415958.3433077,
author = {Suleykin, Alexander and Bakhtadze, Natalya and Panfilov, Peter},
title = {Big-Data Driven Digital Ecosystem Framework for Online Predictive Control},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433077},
doi = {10.1145/3415958.3433077},
abstract = {In this paper, Big-Data Driven Digital Ecosystem Framework (BDDDEF) for Online Predictive Control Systems is created. The proposed framework consists of different Agents, where each Agent is a distributed and virtual service. In our work, we provide solutions to the Big Data challenges in building Digital Ecosystems for Online Control including high volumes, velocity and variety of data, and the need for low data latency. We propose to use BDDDEF for building robust, reliable, fault-tolerant, scalable and high-loaded data pipelines for Online Predictive Control Systems. We review Big Data Main Systems for Online Predictive Control Architecture, review the literature for Digital Ecosystems design for Control Systems Online, design and describe main features, main architectural components and functional architecture of the framework, and finally, propose new Predictive Control methodology for Online Predictions.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {92–95},
numpages = {4},
keywords = {Inmemory computing, Digital ecosystem framework, Big Data, Predictive control, Message-oriented middleware},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.5555/2819009.2819014,
author = {Zhou, Hucheng and Lou, Jian-Guang and Zhang, Hongyu and Lin, Haibo and Lin, Haoxiang and Qin, Tingting},
title = {An Empirical Study on Quality Issues of Production Big Data Platform},
year = {2015},
publisher = {IEEE Press},
abstract = {Big Data computing platform has evolved to be a multi-tenant service. The service quality matters because system failure or performance slowdown could adversely affect business and user experience. To date, there is few study in literature on service quality issues of production Big Data computing platform. In this paper, we present an empirical study on the service quality issues of Microsoft ProductA, which is a company-wide multi-tenant Big Data computing platform, serving thousands of customers from hundreds of teams. ProductA has a well-defined escalation process (i.e., incident management process), which helps customers report service quality issues on 24/7 basis. This paper investigates the common symptom, causes and mitigation of service quality issues in Big Data platform. We conduct a comprehensive empirical study on 210 real service quality issues of ProductA. Our major findings include (1) 21.0% of escalations are caused by hardware faults; (2) 36.2% are caused by system side defects; (3) 37.2% are due to customer side faults. We also studied the general diagnosis process and the commonly adopted mitigation solutions. Our study results provide valuable guidance on improving existing development and maintenance practice of production Big Data platform, and motivate tool support.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {17–26},
numpages = {10},
keywords = {fault tolerance, quality issues, escalations, empirical study, big data computing},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3010089.3010144,
author = {Hamdan, Hani},
title = {Mixture Model Approaches to Big Data Clustering and Classification},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010144},
doi = {10.1145/3010089.3010144},
abstract = {Technological evolution leads to increasingly large and heterogeneous data acquisitions (signals, images, measurement results, texts, etc.), and the growing share of information technology and digital simulation leads to the production of increasingly important quantities of data. This is the case for example in web mining, in biology with the sequencing of different genomes, in astronomy with the proliferation of images from probes or observatories, in meteorology with the simulation of atmospheric phenomena and their visualization, in environmental studies with earth observations, or also in social and human sciences with the digitization of corpus and dictionaries. In such contexts, the formalism of binned data is perfectly suited to deal with the massive data issues. This is particularly interesting in many industrial applications where conventional processing algorithms induce unreasonable computation times. In this talk, the importance and advantages of binning data, in big data clustering and classification, will be shown. Then, the fundamental and basic concepts of mixture models estimation from binned data will be presented. A special attention will be paid to the binned-EM algorithm and the bin-EM-CEM algorithm, and their application to data clustering and classification. A feedback on the implementation and use of these algorithms will be provided. In addition, some hard and open problems (data heterogeneity, new data structures and types, imprecision and variability of data, validation of the obtained partition structure, model selection, choice of the clusters number, etc.) and some promising solutions will be suggested. In order to show the usefulness of the presented approaches, some examples from real applications will be illustrated.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {4},
numpages = {2},
keywords = {classification, mixture model, bin-EM-CEM algorithm, binned uncertain data, Big data, fuzzy clustering, binned data, semi fuzzy clustering, clustering, binned EM algorithm},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/3230833.3232816,
author = {Velthuis, Paul J. E. and Sch\"{a}fer, Marcel and Steinebach, Martin},
title = {New Authentication Concept Using Certificates for Big Data Analytic Tools},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3232816},
doi = {10.1145/3230833.3232816},
abstract = {Companies analyse large amounts of data on clusters of machines, using big data analytic tools such as Apache Spark and Apache Flink to analyse the data. Big data analytic tools are mainly tested regarding speed and reliability. Efforts about Security and thus authentication are spent only at second glance. In such big data analytic tools, authentication is achieved with the help of the Kerberos protocol that is basically built as authentication on top of big data analytic tools. However, Kerberos is vulnerable to attacks, and it lacks providing high availability when users are all over the world. To improve the authentication, this work presents first an analysis of the authentication in Hadoop and the data analytic tools. Second, we propose a concept to deploy Transport Layer Security (TLS) not only for the security of data transportation but as well for authentication within the big data tools. This is done by establishing the connections using certificates with a short lifetime.The proof of concept is realized in Apache Spark, where Kerberos is replaced by the method proposed. We deploy new short living certificates for authentication that are less vulnerable to abuse. With our approach the requirements of the industry regarding multi-factor authentication and scalability are met.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {40},
numpages = {7},
keywords = {Apache Spark, Big Data Analytic Tools, Multi Factor Authentication, Kerberos, Transport Layer Security (TLS)},
location = {Hamburg, Germany},
series = {ARES 2018}
}

@inproceedings{10.1145/2507157.2508005,
author = {de Gemmis, Marco and Di Noia, Tommaso and Lassila, Ora and Lops, Pasquale and Lukasiewicz, Thomas and Semeraro, Giovanni},
title = {Workshop on Recommender Systems Meet Big Data &amp; Semantic Technologies: SeRSy 2013},
year = {2013},
isbn = {9781450324090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2507157.2508005},
doi = {10.1145/2507157.2508005},
abstract = {The primary goal of the workshop is to showcase cutting edge research on the intersection of Recommender Systems and Semantic Technologies, by taking the best of the two worlds. This combination may provide the RecSys community with important scenarios where the potential of Semantic Technologies can be effectively exploited into systems performing complex tasks, such as recommendation engines processing Big Data.},
booktitle = {Proceedings of the 7th ACM Conference on Recommender Systems},
pages = {483–484},
numpages = {2},
keywords = {big data, linked data, recommendation algorithms, semantic technologies},
location = {Hong Kong, China},
series = {RecSys '13}
}

@inproceedings{10.1109/WI-IAT.2014.185,
author = {Twardowski, Bartlomiej and Ryzko, Dominik},
title = {Multi-Agent Architecture for Real-Time Big Data Processing},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.185},
doi = {10.1109/WI-IAT.2014.185},
abstract = {The paper describes the architecture for processing of Big Data in real-time based on multi-agent system paradigms. The overall approach to processing of offline and online data is presented. Possible applications of the architecture in the area of recommendation system is shown, however it is argued the approach is general purpose.},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03},
pages = {333–337},
numpages = {5},
series = {WI-IAT '14}
}

@article{10.14778/2733004.2733056,
author = {Zhang, Rui and Jain, Reshu and Sarkar, Prasenjit and Rupprecht, Lukas},
title = {Getting Your Big Data Priorities Straight: A Demonstration of Priority-Based QoS Using Social-Network-Driven Stock Recommendation},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733056},
doi = {10.14778/2733004.2733056},
abstract = {As we come to terms with various big data challenges, one vital issue remains largely untouched. That is the optimal multiplexing and prioritization of different big data applications sharing the same underlying infrastructure, for example, a public cloud platform. Given these demanding applications and the necessary practice to avoid over-provisioning, resource contention between applications is inevitable. Priority must be given to important applications (or sub workloads in an application) in these circumstances.This demo highlights the compelling impact prioritization could make, using an example application that recommends promising combinations of stocks to purchase based on relevant Twitter sentiment. The application consists of a batch job and an interactive query, ran simultaneously. Our underlying solution provides a unique capability to identify and differentiate application workloads throughout a complex big data platform. Its current implementation is based on Apache Hadoop and the IBM GPFS distributed storage system. The demo showcases the superior interactive query performance achievable by prioritizing its workloads and thereby avoiding I/O bandwidth contention. The query time is 3.6 \texttimes{} better compared to no prioritization. Such a performance is within 0.3% of that of an idealistic system where the query runs without contention. The demo is conducted on around 3 months of Twitter data, pertinent to the S &amp; P 100 index, with about 4 \texttimes{} 1012 potential stock combinations considered.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1665–1668},
numpages = {4}
}

@inproceedings{10.1145/2554850.2555076,
author = {Evermann, Joerg and Assadipour, Ghazal},
title = {Big Data Meets Process Mining: Implementing the Alpha Algorithm with Map-Reduce},
year = {2014},
isbn = {9781450324694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554850.2555076},
doi = {10.1145/2554850.2555076},
abstract = {Process mining is an approach to extract process models from event logs. Given the distributed nature of modern information systems, event logs are likely to be distributed across different physical machines. Map-Reduce is a scalable approach for efficient computations on distributed data. In this paper we present the design of a Map-Reduce implementation of the Alpha process mining algorithm, to take advantage of the scalability of the Map-Reduce approach. We provide a experimental results that show the performance and scalability of our implementation.},
booktitle = {Proceedings of the 29th Annual ACM Symposium on Applied Computing},
pages = {1414–1416},
numpages = {3},
keywords = {workflow management, process mining, alpha algorithm, map-reduce},
location = {Gyeongju, Republic of Korea},
series = {SAC '14}
}

@inproceedings{10.1145/3006299.3006337,
author = {Hameed, Aqsa and Ali, Saqib and Cottrell, Rodger Les and White, Bebo},
title = {Applying Big Data Warehousing and Visualization Techniques on PingER Data},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006337},
doi = {10.1145/3006299.3006337},
abstract = {Nowadays, the Internet has turned into a crucial piece of our cutting edge society. It is a stage of exploration, financial development, democratic participation and speech. The operations of the Internet have prompted a huge development and collection of information known as Big Data. Therefore, it is important to monitor and measure the Quality of Service (QoS) of Internet traffic. The SLAC National Accelerator Laboratory started the PingER project in 1995 to measure the End-to-End Internet performance history of servers and routers worldwide. The project involves measurements of the 700 monitored sites in over 160 countries. PingER Monitoring Agents (MAs) ping a list of monitored sites after every 30 minutes to obtain Round Trip Time (RTT) values revealing interesting information about Internet performance (e.g., RTT, jitter, packet loss and unreachability) major events (e.g., fiber cuts, earthquakes, and social upheavals). Thus, the project has collected a vast amount of historical Internet Performance data worldwide since 1995. Currently, the data is stored in flat text files, making it difficult to analyze collectively. In addition, this simplistic format limits the analytical potential of this data. In this paper, we propose an approach to process, store, analyze and visualize PingER data. A Data warehouse is created which combines Hadoop Big Data techniques. The data are processed by using Sci-cumulus MR workflow, stored in HDFS, analyzed by Impala queries and visualized by using Google API's. This approach makes PingER data more accessible and enhances its potential contribution to ongoing research and application development.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {67–72},
numpages = {6},
keywords = {HDFS, visualization, mapreduce, pingER, big data, data mining},
location = {Shanghai, China},
series = {BDCAT '16}
}

@inproceedings{10.1145/2351316.2351323,
author = {Xue, Zhenghua and Shen, Geng and Li, Jianhui and Xu, Qian and Zhang, Yang and Shao, Jing},
title = {Compression-Aware I/O Performance Analysis for Big Data Clustering},
year = {2012},
isbn = {9781450315470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351316.2351323},
doi = {10.1145/2351316.2351323},
abstract = {As the data volume increases, I/O bottleneck has become a great challenge for data analysis. Data compression can alleviate the bottleneck effectively. Taking K-means algorithm as an example, this paper proposes a compression-aware performance improvement model for big-data clustering. The model quantitatively analyzes the effect of a variety of factors related to compression during the entire computational process. We perform clustering experiments on 10 dimensional data with up to 1.114 TB in size on a cluster computer with hundreds of computing cores. The measurement validates that using compression contributes significantly to improving the I/O performance, and confirms our theoretical analysis empirically. Furthermore, the proposed model can effectively determine when and how to use compression to improve I/O performance for big-data analysis.},
booktitle = {Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
pages = {45–52},
numpages = {8},
keywords = {compression contribution model, I/O bottleneck, big data clustering},
location = {Beijing, China},
series = {BigMine '12}
}

@article{10.1145/3144592.3144597,
author = {de Laat, Paul B.},
title = {Big Data and Algorithmic Decision-Making: Can Transparency Restore Accountability?},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {3},
issn = {0095-2737},
url = {https://doi.org/10.1145/3144592.3144597},
doi = {10.1145/3144592.3144597},
abstract = {Decision-making assisted by algorithms developed by machine learning is increasingly determining our lives. Unfortunately, full opacity about the process is the norm. Can transparency contribute to restoring accountability for such systems? Several objections are examined: the loss of privacy when data sets become public, the perverse effects of disclosure of the very algorithms themselves ('gaming the system' in particular), the potential loss of competitive edge, and the limited gains in answerability to be expected since sophisticated algorithms are inherently opaque. It is concluded that transparency is certainly useful, but only up to a point: extending it to the public at large is normally not to be advised. Moreover, in order to make algorithmic decisions understandable, models of machine learning to be used should either be interpreted ex post or be interpretable by design ex ante.},
journal = {SIGCAS Comput. Soc.},
month = {sep},
pages = {39–53},
numpages = {15},
keywords = {accountability, interpretability, transparency, opacity, algorithm, machine learning}
}

@inproceedings{10.1145/3286606.3286782,
author = {Bouhafer, Fadwa and Heyouni, Mohammed and El Haddadi, Anass and Boulouard, Zakaria},
title = {ACO-FFDP in Incremental Clustering for Big Data Analysis},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286782},
doi = {10.1145/3286606.3286782},
abstract = {The development of dyamic information analysis, like incremental clustering, is becoming a very important concern in big data. In this paper, we will propose a new incremental clustering algorithm, called "ACO-FFDP-Incremental-Cluster". This algorithm is a combination between "FFDP" a large graph visualization algorithm developed by our team, and "ACO Algorithm". FFDP will set an equilibrium positioning of the large graph; then it will provide the nodes final positions as a vector of coordinates. ACO algorithm will take this vector into consideration and try to find the best clustering configuration possible for new data.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {5},
numpages = {7},
keywords = {Incremental clustering, Swarm Intelligence, Ant Colony Optimization},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.1145/3358505.3358519,
author = {G\'{a}rate-Escamilla, Anna Karen and El Hassani, Amir Hajjam and Andres, Emmanuel},
title = {Big Data Execution Time Based on Spark Machine Learning Libraries},
year = {2019},
isbn = {9781450371650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358505.3358519},
doi = {10.1145/3358505.3358519},
abstract = {The paper focuses on exploring the time consumption of supervised and unsupervised models of Apache Spark framework in massive datasets. Big Data analytics has been relevant in the industry due to the need to convert information into knowledge. Among the challenge of big data is the creation of strategies to improve the execution costs of running machine learning models to make a prediction. Apache Spark is a powerful in-memory platform that offers an extensive machine learning library for regression, classification, clustering, and rule extraction. This investigation, from a computation cost perspective, performs different experiments using real datasets. The main contribution of the paper is to compare the execution time of different machine learning models, such as random forests, decision tree, logistic regression, linear support vector machine, and kNN. The present work expects to combine the areas of big data and machine learning, comparing the results with different configurations and the use of the optimization methods, cache and persist. The evaluation experiments show that logistic regression performed the shortest execution time of the Spark MLlib models.},
booktitle = {Proceedings of the 2019 3rd International Conference on Cloud and Big Data Computing},
pages = {78–83},
numpages = {6},
keywords = {Performance prediction model, Apache Spark, Execution time prediction, Machine Learning},
location = {Oxford, United Kingdom},
series = {ICCBDC 2019}
}

@inproceedings{10.1145/2967938.2967957,
author = {Jia, Zhen and Xue, Chao and Chen, Guancheng and Zhan, Jianfeng and Zhang, Lixin and Lin, Yonghua and Hofstee, Peter},
title = {Auto-Tuning Spark Big Data Workloads on POWER8: Prediction-Based Dynamic SMT Threading},
year = {2016},
isbn = {9781450341219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2967938.2967957},
doi = {10.1145/2967938.2967957},
abstract = {Much research work devotes to tuning big data analytics in modern data centers, since %the truth that even a small percentage of performance improvement immediately translates to huge cost savings because of the large scale. Simultaneous multithreading (SMT) receives great interest from data center communities, as it has the potential to boost performance of big data analytics by increasing the processor resources utilization. For example, the emerging processor architectures like POWER8 support up to 8-way multithreading. However, as different big data workloads have disparate architectural characteristics, how to identify the most efficient SMT configuration to achieve the best performance is challenging in terms of both complex application behaviors and processor architectures. In this paper, we specifically focus on auto-tuning SMT configuration for Spark-based big data workloads on POWE-R8. However, our methodology could be generalized and extended to other programming software stacks and other architectures.We propose a prediction-based dynamic SMT threading (PBDST) framework to adjust the thread count in SMT cores on POWER8 processors by using versatile machine learning algorithms.Its innovation lies in adopting online SMT configuration predictions derived from micro-architecture level profiling, to regulate the thread counts that could achieve nearly optimal performance. Moreover it is implemented at Spark software stack layer and transparent to user applications. After evaluating a large set of machine learning algorithms, we choose the most efficient ones to perform online predictions. The experimental results demonstrate that our approach can achieve up to 56.3% performance improvement and an average performance gain of 16.2% in comparison with the default configuration---the maximum SMT configuration---SMT8 on our system.},
booktitle = {Proceedings of the 2016 International Conference on Parallel Architectures and Compilation},
pages = {387–400},
numpages = {14},
keywords = {spark big data, power8, dynamic smt tuning},
location = {Haifa, Israel},
series = {PACT '16}
}

@inproceedings{10.1145/3219819.3219941,
author = {Teh, Yee Whye},
title = {On Big Data Learning for Small Data Problems},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219941},
doi = {10.1145/3219819.3219941},
abstract = {Much recent progress in machine learning have been fueled by the explosive growth in the amount and diversity of data available, and the computational resources needed to crunch through the data. This begs the question of whether machine learning systems necessarily need large amounts of data to solve a task well. An exciting recent development, under the banners of meta-learning, lifelong learning, learning to learn, multitask learning etc., has been the observation that often there is heterogeneity within the data sets at hand, and in fact a large data set can be viewed more productively as many smaller data sets, each pertaining to a different task. For example, in recommender systems each user can be said to be a different task with a small associated data set, and in AI one holy grail is how to develop systems that can learn to solve new tasks quickly from small amounts of data. In such settings, the problem is then how to "learn to learn quickly", by making use of similarities among tasks. One perspective for how this is achievable is that exposure to lots of previous tasks allows the system to learn a rich prior knowledge about the world in which tasks are sampled from, and it is with rich world knowledge that the system is able to solve new tasks quickly. This is a very active, vibrant and diverse area of research, with many different approaches proposed recently. In this talk I will describe a view of this problem from probabilistic and deep learning perspectives, and describe a number of efforts in this direction that I have recently been involved in.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3},
numpages = {1},
keywords = {learn to learn, multitask learning, learning to learn, small data, lifelong learning, machine learning, big data, meta-learning},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3297663.3310302,
author = {Talluri, Sacheendra and \L{}uszczak, Alicja and Abad, Cristina L. and Iosup, Alexandru},
title = {Characterization of a Big Data Storage Workload in the Cloud},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3310302},
doi = {10.1145/3297663.3310302},
abstract = {The proliferation of big data processing platforms has led to radically different system designs, such as MapReduce and the newer Spark. Understanding the workloads of such systems facilitates tuning and could foster new designs. However, whereas MapReduce workloads have been characterized extensively, relatively little public knowledge exists about the characteristics of Spark workloads in representative environments. To address this problem, in this work we collect and analyze a 6-month Spark workload from a major provider of big data processing services, Databricks. Our analysis focuses on a number of key features, such as the long-term trends of reads and modifications, the statistical properties of reads, and the popularity of clusters and of file formats. Overall, we present numerous findings that could form the basis of new systems studies and designs. Our quantitative evidence and its analysis suggest the existence of daily and weekly load imbalances, of heavy-tailed and bursty behaviour, of the relative rarity of modifications, and of proliferation of big data specific formats.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {33–44},
numpages = {12},
keywords = {long-term trend, big data, cloud storage, interarrival time, popularity, apache spark, characterization, file formats},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/3274005.3274015,
author = {Petrova-Antonova, Dessislava and Ilieva, Sylvia},
title = {Using Big Data Value Chain to Create Government Education Policies},
year = {2018},
isbn = {9781450364256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274005.3274015},
doi = {10.1145/3274005.3274015},
abstract = {The Big Data Value Chain aims to discover patterns, correlations, and pattern deviations hidden in a dataset. This paper investigates (1) the possible approaches to applying Big Data Value Chain to decision making in the Public sector, specifically in Education; and (2) the ways in which such activities can be automated. The models created can be customized depending on a school's dropout rate and number of students with learning difficulties. This research is part of a current project at the Ministry of Education and Science (MES) of Bulgaria, funded by the Operational Programme Science and Education for Smart Growth which aims to increase student engagement.},
booktitle = {Proceedings of the 19th International Conference on Computer Systems and Technologies},
pages = {42–49},
numpages = {8},
keywords = {Big Data Value Chain, Big Data, Decision support, Data model, Education},
location = {Ruse, Bulgaria},
series = {CompSysTech'18}
}

@inproceedings{10.5555/2840819.2840927,
author = {Zhu, Yada and Xiong, Jinjun},
title = {Modern Big Data Analytics for "Old-Fashioned" Semiconductor Industry Applications},
year = {2015},
isbn = {9781467383899},
publisher = {IEEE Press},
abstract = {Big data analytics is the latest spotlight with all the glare of fame ranging from media coverage to booming start-up companies to eye-catching merges and acquisitions. On the contrary, the $336 billion industry of semiconductor was seen as an "old-fashioned" business, with fading interests from the best and brightest among young graduates and engineers. How will modern big data analytics help the semiconductor industry walk through this transition? This paper answers this question via a number of practical but challenging problems arising from semiconductor manufacturing process. We show that many existing machine learning algorithms are not well positioned to solve these problems, and novel techniques involving temporal, structural and hierarchical properties need to be developed to solve these problems.},
booktitle = {Proceedings of the IEEE/ACM International Conference on Computer-Aided Design},
pages = {776–780},
numpages = {5},
keywords = {manufacturing, analytics, semiconductor, Big data},
location = {Austin, TX, USA},
series = {ICCAD '15}
}

@inproceedings{10.1145/3371158.3371161,
author = {Maivizhi, Radhakrishnan and Yogesh, Palanichamy},
title = {Concealed Multidimensional Data Aggregation in Big Data Wireless Sensor Networks},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371161},
doi = {10.1145/3371158.3371161},
abstract = {Wireless sensor networks (WSNs) deployed in a plethora of applications produce a significant portion of big data. Handling these huge volume of data is a critical challenge in a resource constrained wireless sensor networks. Data aggregation is the most practical and important paradigm in big data wireless sensor networks. It reduces the huge volume of data by combining the similar data and eliminating data redundancy and reduces thereby the resource consumption. However preserving data confidentiality and integrity along with en-route aggregation is a great challenge. In this paper, we propose a novel Concealed Multidimensional Data Aggregation (CMDA) protocol for big data wireless sensor networks. CMDA integrates super-increasing sequence and homomorphic encryption to structure the multidimensional data and protect the data privacy and a homomorphic signature to check the integrity of data. In addition, the proposed protocol filters false data packets and achieves data freshness. Security analysis reveals that the proposed protocol achieves end-to-end security and performance evaluation shows that CMDA incurs less communication overhead and consequently reduces energy consumption which enhances the lifetime of sensor networks. To the best of our knowledge, this is the first work that achieves end-to-end security in multidimensional data aggregation.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {19–27},
numpages = {9},
keywords = {multidimensional data, wireless sensor networks, energy efficiency, concealed data aggregation, privacy homomorphism},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@article{10.14778/3303753.3303762,
author = {Xu, Lijie and Guo, Tian and Dou, Wensheng and Wang, Wei and Wei, Jun},
title = {An Experimental Evaluation of Garbage Collectors on Big Data Applications},
year = {2019},
issue_date = {January 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3303753.3303762},
doi = {10.14778/3303753.3303762},
abstract = {Popular big data frameworks, ranging from Hadoop MapReduce to Spark, rely on garbage-collected languages, such as Java and Scala. Big data applications are especially sensitive to the effectiveness of garbage collection (i.e., GC), because they usually process a large volume of data objects that lead to heavy GC overhead. Lacking in-depth understanding of GC performance has impeded performance improvement in big data applications. In this paper, we conduct the first comprehensive evaluation on three popular garbage collectors, i.e., Parallel, CMS, and G1, using four representative Spark applications. By thoroughly investigating the correlation between these big data applications' memory usage patterns and the collectors' GC patterns, we obtain many findings about GC inefficiencies. We further propose empirical guidelines for application developers, and insightful optimization strategies for designing big-data-friendly garbage collectors.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {570–583},
numpages = {14}
}

@inproceedings{10.1145/3255781,
author = {Abiteboul, Serge},
title = {Session Details: Web Queries and Big Data},
year = {2014},
isbn = {9781450323758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255781},
doi = {10.1145/3255781},
booktitle = {Proceedings of the 33rd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
location = {Snowbird, Utah, USA},
series = {PODS '14}
}

@article{10.1145/3418896,
author = {Christophides, Vassilis and Efthymiou, Vasilis and Palpanas, Themis and Papadakis, George and Stefanidis, Kostas},
title = {An Overview of End-to-End Entity Resolution for Big Data},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3418896},
doi = {10.1145/3418896},
abstract = {One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows&nbsp;for&nbsp;Big Data, critically review the pros and cons of existing methods, and conclude with the main open research&nbsp;directions.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {127},
numpages = {42},
keywords = {crowdsourcing, Entity blocking and matching, batch and incremental entity resolution workflows, block processing, deep learning, strongly and nearly similar entities}
}

@inproceedings{10.1145/3019612.3019700,
author = {Taherkordi, Amir and Eliassen, Frank and Horn, Geir},
title = {From IoT Big Data to IoT Big Services},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019700},
doi = {10.1145/3019612.3019700},
abstract = {The large-scale deployments of Internet of Things (IoT) systems have introduced several new challenges in terms of processing their data. The massive amount of IoT-generated data requires design solutions to speed up data processing, scale up with the data volume and improve data adaptability and extensibility. Beyond existing techniques for IoT data collection, filtering, and analytics, innovative service computing technologies are required for provisioning data-centric and scalable IoT services. This paper presents a service-oriented design model and framework for realizing scalable and efficient acquisition, processing and integration of data-centric IoT services. In this approach, data-centric IoT services are organized in a service integrating tree structure, adhering to the architecture of many large-scale IoT systems, including recent fog-based IoT computing models. A service node in the tree is called a Big Service and acts as an integrator, collecting data from lower level Big Services, processing them, and delivering the result to higher level IoT Big Services. The service tree thereby encapsulates required data processing functions in a hierarchical manner in order to achieve scalable and real-time data collection and processing. We have implemented the IoT Big Services framework leveraging a popular cloud-based service and data platform called Firebase, and evaluated its performance in terms of real-time requirements.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {485–491},
numpages = {7},
keywords = {big services, internet of things, big data},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/3368756.3369096,
author = {Samir, Tetouani and Abdelsamad, Chouar and ElAlami, Jamila},
title = {Big Data Analysis from the Smart-Logistics for Smart-Cities},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369096},
doi = {10.1145/3368756.3369096},
abstract = {Recent advances in information and communication technologies (ICT) have contributed to the evolution of the supply chain and logistics sector. Indeed, the analysis of massive data (Big Data) coming from smart-products makes it possible to extract enormous values for the decision-making of strategic choice: commercial or technical. But this also causes research problems because of the speed of data transmission, the huge volume, and the non-homogeneous types of data. This work provides an overview of the analysis of Big-Data (BD) from the Internet of Things (IoT) in new Logistics. This article begins with a discussion of the needs and challenges of the Internet of Things (IoT) and Big Data (BD) analysis in logistics. Then, major data analysis technologies are examined and discussed. In addition, this article also describes future directions in this promising area.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {109},
numpages = {4},
keywords = {big data analytics, Logistics 4.0, internet of things},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@article{10.1145/3332301,
author = {Barika, Mutaz and Garg, Saurabh and Zomaya, Albert Y. and Wang, Lizhe and Moorsel, Aad Van and Ranjan, Rajiv},
title = {Orchestrating Big Data Analysis Workflows in the Cloud: Research Challenges, Survey, and Future Directions},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3332301},
doi = {10.1145/3332301},
abstract = {Interest in processing big data has increased rapidly to gain insights that can transform businesses, government policies, and research outcomes. This has led to advancement in communication, programming, and processing technologies, including cloud computing services and technologies such as Hadoop, Spark, and Storm. This trend also affects the needs of analytical applications, which are no longer monolithic but composed of several individual analytical steps running in the form of a workflow. These big data workflows are vastly different in nature from traditional workflows. Researchers are currently facing the challenge of how to orchestrate and manage the execution of such workflows. In this article, we discuss in detail orchestration requirements of these workflows as well as the challenges in achieving these requirements. We also survey current trends and research that supports orchestration of big data workflows and identify open research challenges to guide future developments in this area.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {95},
numpages = {41},
keywords = {workflow orchestration, research taxonomy, Big data, and techniques, cloud computing, approaches}
}

@inproceedings{10.1145/3079452.3079474,
author = {Altena, Allard Jan-Jaap van and Delgado Olabarriaga, S\'{\i}lvia},
title = {(Bio)Medical Publications in the Age of Big Data: Yes, They Are Different},
year = {2017},
isbn = {9781450352499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079452.3079474},
doi = {10.1145/3079452.3079474},
abstract = {In 2011 the term "Big Data" was introduced by Gartner [5], and since then its use in literature has ever increased, also in the (bio)medical research field [1]. Although the term Big Data is widely used, studies show that its meaning is much debated and many different definitions exist [10]. This variety of definitions may lead to different understandings and therefore difficulties in communication. For example, a researcher that is looking for "Big Data" solutions might miss an interesting method that is not tagged as such. In previous work we studied major topics that appear in Big Data literature using a Topic Modelling approach [8]. However, from that study it was not possible to know whether those topics are exclusive to publications self-identified as Big Data (BD), or not. Therefore, here we investigate the research question: What are the differences between topics in BD and non-Big Data (NBD) corpora?},
booktitle = {Proceedings of the 2017 International Conference on Digital Health},
pages = {221–222},
numpages = {2},
keywords = {big data, topic modelling, biomedical literature},
location = {London, United Kingdom},
series = {DH '17}
}

@article{10.5555/2946645.3007028,
author = {Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin},
title = {Distributed Coordinate Descent Method for Learning with Big Data},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method for solving loss minimization problems with big data. We initially partition the coordinates (features) and assign each partition to a different node of a cluster. At every iteration, each node picks a random subset of the coordinates from those it owns, independently from the other computers, and in parallel computes and applies updates to the selected coordinates based on a simple closed-form formula. We give bounds on the number of iterations sufficient to approximately solve the problem with high probability, and show how it depends on the data and on the partitioning. We perform numerical experiments with a LASSO instance described by a 3TB matrix.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2657–2681},
numpages = {25},
keywords = {stochastic methods, boosting, distributed algorithms, parallel coordinate descent}
}

@inproceedings{10.1145/3331453.3361308,
author = {Jing, Furong and Cao, Yongsheng and Fang, Wei and Chen, Yanqing},
title = {Construction and Implementation of Big Data Framework for Crop Germplasm Resources},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3361308},
doi = {10.1145/3331453.3361308},
abstract = {Based on understanding the application of big data and the research status of crop germplasm resources, this paper proposes a system architecture that is suitable for crop germplasm resources big data. Among them, the overall architecture of germplasm resources is elaborated through six functional modules, including data source, data integration, data processing, data application, big data operation and maintenance platform, and data management and security. The logical functional architecture specification was formulated and the technical implementation and selection are defined. The technical implementation framework describes the technical implementation of germplasm resources big data, and jointly supports the construction and operation of germplasm resources big data. Finally, a verification system is established to verify the feasibility of the big data system framework for germplasm resources.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {27},
numpages = {7},
keywords = {Data management, Crop germplasm resources, Data analysis, Big data architecture},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/3010089.3010126,
author = {Lahmidani, Hind and El Beqqali, Omar},
title = {Mining Public Administration Big Data for a Financial Benefit Word},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010126},
doi = {10.1145/3010089.3010126},
abstract = {Our research study is related to data Mining software and analyzing big Data especially in public administration. The purpose of this study is to compare softwares and algorithms used for statistical analysis. Following the comparative study of supply chain management (SCM) softwares, we chose the best to make a comparative study of descriptive and predictive algorithms using dataset collected from the new IT solution developed in our PHD research study.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {41},
numpages = {5},
keywords = {Claim, Logistics, Supply chain management, Tanagra, Customers, Data mining, Big Data, Information system},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@proceedings{10.1145/3341620,
title = {BDE 2019: Proceedings of the 2019 International Conference on Big Data Engineering},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The Big Data Era has arrived, in order to promote the communication and cooperation in the field of Big Data Engineering. 2019 International Conference on Big Data Engineering (BDE 2019) was successfully held in Regal Oriental Hotel, Hong Kong during June 11-13, 2019. BDE brought together researchers, engineers, academicians as well as industrial professionals from all over the world who are interested in Big Data and its current applications.},
location = {Hong Kong, Hong Kong}
}

