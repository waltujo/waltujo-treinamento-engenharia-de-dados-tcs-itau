@article{10.14778/3352063.3352130,
author = {Kandula, Srikanth and Lee, Kukjin and Chaudhuri, Surajit and Friedman, Marc},
title = {Experiences with Approximating Queries in Microsoft's Production Big-Data Clusters},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352130},
doi = {10.14778/3352063.3352130},
abstract = {With the rapidly growing volume of data, it is more attractive than ever to leverage approximations to answer analytic queries. Sampling is a powerful technique which has been studied extensively from the point of view of facilitating approximation. Yet, there has been no large-scale study of effectiveness of sampling techniques in big data systems. In this paper, we describe an in-depth study of the sampling-based approximation techniques that we have deployed in Microsoft's big data clusters. We explain the choices we made to implement approximation, identify the usage cases, and study detailed data that sheds insight on the usefulness of doing sampling based approximation.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2131–2142},
numpages = {12}
}

@inproceedings{10.1145/3373376.3380611,
author = {Kaplan, Fr\'{e}d\'{e}ric},
title = {Big Data of the Past, from Venice to Europe},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3380611},
doi = {10.1145/3373376.3380611},
abstract = {In 2012, the Ecole Polytechnique F\'{e}d\'{e}rale de Lausanne (EPFL) and the University Ca'Foscari launched a program called the Venice Time Machine, whose goal was to develop a large-scale digitisation program to transform Venice's heritage into 'Big Data of the Past'. Millions of register pages and photographs have been scanned at the State Archive in Venice and at the Fondazione Giorgio Cini. These documents were analysed using the deep-learning artificial-intelligence methods developed at EPFL's Digital Humanities Laboratory in order to extract their textual and iconographic content and to make the data accessible via a search engine. The project has now expand to a European scale, including more than 500 institutions and 20 new cities jointly constructing a distributed digital information system mapping the social, cultural and geographical evolution of Europe. The project build upon existing platforms such as Europeana, and accelerate their development. While Europeana drives transformation throughout the cultural heritage sector with innovative standards, infrastructure and networks, Time Machine aims to design and implement advanced new digitisation and artificial intelligence technologies to mine Europe's vast cultural heritage, providing fair and free access to information that will support future scientific and technological developments in Europe.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1},
numpages = {1},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@article{10.1145/2629568,
author = {Glowalla, Paul and Sunyaev, Ali},
title = {Process-Driven Data Quality Management: A Critical Review on the Application of Process Modeling Languages},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2629568},
doi = {10.1145/2629568},
abstract = {Data quality is critical to organizational success. In order to improve and sustain data quality in the long term, process-driven data quality management (PDDQM) seeks to redesign processes that create or modify data. Consequently, process modeling is mandatory for PDDQM. Current research examines process modeling languages with respect to representational capabilities. However, there is a gap, since process modeling languages for PDDQM are not considered. We address this research gap by providing a synthesis of the varying applications of process modeling languages for PDDQM. We conducted a keyword-based literature review in conferences as well as 74 highranked information systems and computer science journals, reviewing 1,555 articles from 1995 onwards. For practitioners, it is possible to integrate the quality perspective within broadly applied process models. For further research, we derive representational requirements for PDDQM that should be integrated within existing process modeling languages. However, there is a need for further representational analysis to examine the adequacy of upcoming process modeling languages. New or enhanced process modeling languages may substitute for PDDQM-specific process modeling languages and facilitate development of a broadly applicable and accepted process modeling language for PDDQM.},
journal = {J. Data and Information Quality},
month = {sep},
articleno = {7},
numpages = {30},
keywords = {data quality, process modeling, data and knowledge visualization., conceptual modeling, Information quality}
}

@article{10.1145/2809793,
author = {Dopplick, Renee},
title = {Expanding Minds to Big Data and Data Sciences},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2153-2184},
url = {https://doi.org/10.1145/2809793},
doi = {10.1145/2809793},
journal = {ACM Inroads},
month = {aug},
pages = {88},
numpages = {1}
}

@inproceedings{10.1145/2896387.2900334,
author = {Boubiche, Djallel Eddine},
title = {Secure and Efficient Big Data Gathering in Heterogeneous Wireless Sensor Networks},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900334},
doi = {10.1145/2896387.2900334},
abstract = {The emergence of heterogeneous wireless sensor networks in recent years has helped to address the limitations of conventional WSNs resources and has opened new research areas. When more than one type of node is integrated in a WSN, it is called heterogeneous. Placing heterogeneous nodes in a WSN is an effective way to increase the life of the network. While most of the existing civil and military applications of the heterogeneous WSNs are not materially different from their homogeneous counterparts, there are compelling reasons of incorporating the heterogeneity in the network. These reasons include: improving scalability of wireless sensor networks, reducing energy requirements without sacrificing performance, the equilibration of the cost and the network functionality, improving the security mechanisms using more complex protocols and supporting new broadband applications and big data.Recently, and because of the growth of the amount of data transmitted in the heterogeneous sensor networks, the term big data has emerged as a widely recognized trend. The term Big Data does not only concern the volume of data but also the high-speed transmission and the variety of information which are difficult to collect, store, and process using available technologies. Although the data generated by the individual sensors may not appear to be significant, all the data generated through the many sensors are capable of producing large volumes of data. The management of big data imposes additional constraints on WSNs. Effectively manage and secure big data gathering is a challenge in heterogeneous WSNs. Therefore, it represents an interesting research area. In this talk, I will address the emerging big data concept in heterogeneous wireless sensor networks and point out its main research issues.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {3},
numpages = {1},
keywords = {Big data, Secure and efficient data gathering, Heterogeneous Sensor Networks},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1145/2396636.2396678,
author = {Leftheriotis, Ioannis},
title = {Scalable Interaction Design for Collaborative Visual Exploration of Big Data},
year = {2012},
isbn = {9781450312097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396636.2396678},
doi = {10.1145/2396636.2396678},
abstract = {Novel input devices such as tangibles, smartphones, multi-touch surfaces etc. have given impetus to new interaction techniques. In this PhD research, the main motivation is to study novel interaction techniques and designs that augment collaboration in a collocated environment. Furthermore, the main research aim is to take advantage of scalable interaction design techniques and tools that can be applied in a variety of devices so as to help users to work together on a problem with an abstract big data set, using visualizations on a collocated context.},
booktitle = {Proceedings of the 2012 ACM International Conference on Interactive Tabletops and Surfaces},
pages = {271–276},
numpages = {6},
keywords = {user interfaces, interaction design, collaboration, big data, visual exploration, scalability},
location = {Cambridge, Massachusetts, USA},
series = {ITS '12}
}

@article{10.1145/1563821.1563874,
author = {Jacobs, Adam},
title = {The Pathologies of Big Data: Scale up Your Datasets Enough and All Your Apps Will Come Undone. What Are the Typical Problems and Where Do the Bottlenecks Generally Surface?},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {6},
issn = {1542-7730},
url = {https://doi.org/10.1145/1563821.1563874},
doi = {10.1145/1563821.1563874},
abstract = {What is "big data" anyway? Gigabytes? Terabytes? Petabytes? A brief personal memory may provide some perspective. In the late 1980s at Columbia University I had the chance to play around with what at the time was a truly enormous "disk": the IBM 3850 MSS (Mass Storage System). The MSS was actually a fully automatic robotic tape library and associated staging disks to make random access, if not exactly instantaneous, at least fully transparent. In Columbia’s configuration, it stored a total of around 100 GB. It was already on its way out by the time I got my hands on it, but in its heyday, the early to mid-1980s, it had been used to support access by social scientists to what was unquestionably "big data" at the time: the entire 1980 U.S. Census database.},
journal = {Queue},
month = {jul},
pages = {10–19},
numpages = {10}
}

@inproceedings{10.1145/3331453.3362052,
author = {Shu, Jiangbo and Peng, Liyuan and Hu, Qianqian and Tan, Fengxia and Ge, Xiong},
title = {Analysis of Behavioral Characteristics Based on Student's Personal Big Data},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3362052},
doi = {10.1145/3331453.3362052},
abstract = {With the continuous improvement of the information construction of colleges and universities, the daily life and learning behaviors of college students are recorded and stored by major business systems, and they are accumulated, which has initially formed a large-scale and multi-type student personal big data environment.This paper mainly classifies and summarizes the students' data from the three aspects of student basic information, campus learning and campus life. It focuses on the feature extraction and index mining of students' campus consumption, curriculum and performance data, and constructs the student's personal big data behavior analysis model. In-depth analysis and mining of student consumption behavior data to explore students' dietary rules and consumption level. Through data analysis, the following rules were found: 1)The total number of students eating at school decreases year by year, and the breakfast rate decreases year by year; 2) Freshmen are one hour ahead of the "peak period" of breakfast meals for the whole group;3) The students' academic scores are highly correlated with the meal rate, breakfast meal rate and eating consumption level, and are less correlated with variables such as window selection stability, etc. 4) The more regular the student's diet, the more stable the level of consumption, and the higher the level of learning effort, the better the student's academic performance.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {28},
numpages = {7},
keywords = {Education big data, Behavior analysis, Student personal big data, Correlation analysis},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/3037697.3037699,
author = {Zhou, Jingren},
title = {Big Data Analytics and Intelligence at Alibaba Cloud},
year = {2017},
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3037697.3037699},
doi = {10.1145/3037697.3037699},
booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Xi'an, China},
series = {ASPLOS '17}
}

@article{10.1145/3037699,
author = {Zhou, Jingren},
title = {Big Data Analytics and Intelligence at Alibaba Cloud},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/3037699},
doi = {10.1145/3037699},
journal = {SIGARCH Comput. Archit. News},
month = {may},
numpages = {1}
}

@inproceedings{10.1109/CCGrid.2015.139,
author = {Ros\`{a}, Andrea and Chen, Lydia Y. and Binder, Walter},
title = {Predicting and Mitigating Jobs Failures in Big Data Clusters},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.139},
doi = {10.1109/CCGrid.2015.139},
abstract = {In large-scale datacenters, software and hardware failures are frequent, resulting in failures of job executions that may cause significant resource waste and performance deterioration. To proactively minimize the resource inefficiency due to job failures, it is important to identify them in advance using key job attributes. However, so far, prevailing research on datacenter workload characterization has overlooked job failures, including their patterns, root causes, and impact. In this paper, we aim to develop prediction models and mitigation policies for unsuccessful jobs, so as to reduce the resource waste in big datacenters. In particular, we base our analysis on Google cluster traces, consisting of a large number of big-data jobs with a high task fanout. We first identify the time-varying patterns of failed jobs and the contributing system features. Based on our characterization study, we develop an on-line predictive model for job failures by applying various statistical learning techniques, namely Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Logistic Regression (LR). Furthermore, we propose a delay-based mitigation policy which, after a certain grace period, proactively terminates the execution of jobs that are predicted to fail. The particular objective of postponing job terminations is to strike a good tradeoff between resource waste and false prediction of successful jobs. Our evaluation results show that the proposed method is able to significantly reduce the resource waste by 41.9% on average, and keep false terminations of jobs low, i.e., only 1%.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {221–230},
numpages = {10},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/2912845.2912869,
author = {Luyen, LE Ngoc and Tireau, Anne and Venkatesan, Aravind and Neveu, Pascal and Larmande, Pierre},
title = {Development of a Knowledge System for Big Data: Case Study to Plant Phenotyping Data},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912869},
doi = {10.1145/2912845.2912869},
abstract = {In the recent years, the data deluge in many areas of scientific research brings challenges in the treatment and improvement of agricultural data. Research in bioinformatics field does not outside this trend. This paper presents some approaches aiming to solve the Big Data problem by combining the increase in semantic search capacity on existing data in the plant research laboratories. This helps us to strengthen user experiments on the data obtained in this research by infering new knowledge. To achieve this, there exist several approaches having different characteristics and using different platforms. Nevertheless, we can summarize it in two main directions: the query re-writing and data transformation to RDF graphs. In reality, we can solve the problem from origin of increasing capacity on semantic data with triplets. Thus, data transformation to RDF graphs direction was chosen to work on the practical part. However, the synchronization data in the same format is required before processing the triplets because our current data are heterogeneous. The data obtained for triplets are larger that regular triplestores could manage. So we evaluate some of them thus we can compare the benefits and drawbacks of each and choose the best system for our problem.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {27},
numpages = {9},
keywords = {Big Data, Ontology, NoSQL, Knowledge base, Triplestore, xR2RML, SPARQL, Reasoning, Inference, Benchmark},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@article{10.1145/3274572,
author = {Hourcade, Juan Pablo and Antle, Alissa N. and Anthony, Lisa and Fails, Jerry Alan and Iversen, Ole Sejer and Rubegni, Elisa and Skov, Mikael and Slovak, Petr and Walsh, Greg and Zeising, Anja},
title = {Child-Computer Interaction, Ubiquitous Technologies, and Big Data},
year = {2018},
issue_date = {November - December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {6},
issn = {1072-5520},
url = {https://doi.org/10.1145/3274572},
doi = {10.1145/3274572},
abstract = {In this forum we celebrate research that helps to successfully bring the benefits of computing technologies to children, older adults, people with disabilities, and other populations that are often ignored in the design of mass-marketed products. --- Juan Pablo Hourcade, Editor},
journal = {Interactions},
month = {oct},
pages = {78–81},
numpages = {4}
}

@inproceedings{10.1145/3297662.3365797,
author = {Musto, Jiri and Dahanayake, Ajantha},
title = {Integrating Data Quality Requirements to Citizen Science Application Design},
year = {2020},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365797},
doi = {10.1145/3297662.3365797},
abstract = {Data quality is an important aspect in many fields. In citizen science application databases, data quality is often found lacking, which is why there needs to be a method of integrating data quality into the design. This paper tackles the problem by dividing data quality into separate characteristics according to the ISO / IEC 25012 standard. These characteristics are integrated into a conceptual model of the system and data model for citizen science applications. Furthermore, the paper describes a way to measure data quality using the data quality characteristics. The models and measuring methods are theoretical and can be adapted into case specific designs.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {166–173},
numpages = {8},
keywords = {Data Quality requirements, Conceptual model, Data quality, Data Quality Characteristics, Citizen science},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1145/3379247.3379282,
author = {Liyao, Zhou and Xiaofang, Liu and Chunyu, Hu},
title = {Evaluation Method of Equipment Combat Effectiveness Based On Big Data Mining},
year = {2020},
isbn = {9781450376730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379247.3379282},
doi = {10.1145/3379247.3379282},
abstract = {In the evaluation of equipment combat effectiveness, it is necessary to comprehensively analyze the data of outfield test and infield test, including a variety of audio-visual, image and other combat test data. These data can be classified, extracted, stored and managed by building data model through big data mining technology. The evaluation method of equipment combat effectiveness based on big data mining is based on massive data, through machine learning, statistical analysis, neural network, database and other methods to analyze and process the data, mining the correlation between test data, evaluation index and evaluation conclusion, and extracting useful information and finding new knowledge from it to realize the evaluation of the combat effectiveness of the tested system.},
booktitle = {Proceedings of 2020 the 6th International Conference on Computing and Data Engineering},
pages = {131–135},
numpages = {5},
keywords = {Big data mining, combat effectiveness evaluation, combat test},
location = {Sanya, China},
series = {ICCDE 2020}
}

@inproceedings{10.1145/3357292.3357302,
author = {Kun-fa, Li and Jing-chun, Chen and Yan-xi, Wang},
title = {Big Data Informatization Applied to Optimization of Human Resource Performance Management},
year = {2019},
isbn = {9781450371445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357292.3357302},
doi = {10.1145/3357292.3357302},
abstract = {With the development of technology in the era of digital big data in the network and the promotion of network technology, big data is simultaneously integrated into different industry sectors to achieve Internet performance management, and enhance the new perspective of enterprise human resources performance management activities. Today's Internet, cloud computing, Internet of Things and other industrial technologies have undergone repeated changes, showing an unprecedented picture. At present, the subjective awareness of enterprise human resources performance management is too strong, lack of objective data understanding, and the theoretical framework of big data human resource management is not fully applied. This paper reconstructs the data system from four aspects: data source, collection, integration and analysis. Innovate the human resources performance management method from the system to provide more scientific and specific ideas for human resource performance management.},
booktitle = {Proceedings of the 2019 2nd International Conference on Information Management and Management Sciences},
pages = {12–17},
numpages = {6},
keywords = {Big data, human resources, performance management},
location = {Chengdu, China},
series = {IMMS 2019}
}

@article{10.1145/2788516,
author = {Sachs, Karen and Chen, Tiffany},
title = {Big Data Comes in Tiny Packages: Single-Cell Driven Science and Health},
year = {2015},
issue_date = {Summer 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1528-4972},
url = {https://doi.org/10.1145/2788516},
doi = {10.1145/2788516},
abstract = {Single-cell data creates computational opportunities for discovery in disease and human health.},
journal = {XRDS},
month = {jul},
pages = {54–59},
numpages = {6}
}

@inproceedings{10.1145/3076113.3076115,
author = {Karnagel, Tomas and Ben-Nun, Tal and Werner, Matthias and Habich, Dirk and Lehner, Wolfgang},
title = {Big Data Causing Big (TLB) Problems: Taming Random Memory Accesses on the GPU},
year = {2017},
isbn = {9781450350259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3076113.3076115},
doi = {10.1145/3076113.3076115},
abstract = {GPUs are increasingly adopted for large-scale database processing, where data accesses represent the major part of the computation. If the data accesses are irregular, like hash table accesses or random sampling, the GPU performance can suffer. Especially when scaling such accesses beyond 2GB of data, a performance decrease of an order of magnitude is encountered. This paper analyzes the source of the slowdown through extensive micro-benchmarking, attributing the root cause to the Translation Lookaside Buffer (TLB). Using the micro-benchmarks, the TLB hierarchy and structure are fully analyzed on two different GPU architectures, identifying never-before-published TLB sizes that can be used for efficient large-scale application tuning. Based on the gained knowledge, we propose a TLB-conscious approach to mitigate the slowdown for algorithms with irregular memory access. The proposed approach is applied to two fundamental database operations - random sampling and hash-based grouping - showing that the slowdown can be dramatically reduced, and resulting in a performance increase of up to 13\texttimes{}.},
booktitle = {Proceedings of the 13th International Workshop on Data Management on New Hardware},
articleno = {6},
numpages = {10},
keywords = {grouping, random memory access, virtual memory, GPU, TLB},
location = {Chicago, Illinois},
series = {DAMON '17}
}

@article{10.14778/2556549.2556557,
author = {Chandramouli, Badrish and Goldstein, Jonathan and Quamar, Abdul},
title = {Scalable Progressive Analytics on Big Data in the Cloud},
year = {2013},
issue_date = {September 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {14},
issn = {2150-8097},
url = {https://doi.org/10.14778/2556549.2556557},
doi = {10.14778/2556549.2556557},
abstract = {Analytics over the increasing quantity of data stored in the Cloud has become very expensive, particularly due to the pay-as-you-go Cloud computation model. Data scientists typically manually extract samples of increasing data size (progressive samples) using domain-specific sampling strategies for exploratory querying. This provides them with user-control, repeatable semantics, and result provenance. However, such solutions result in tedious workflows that preclude the reuse of work across samples. On the other hand, existing approximate query processing systems report early results, but do not offer the above benefits for complex ad-hoc queries. We propose a new progressive analytics system based on a progress model called Prism that (1) allows users to communicate progressive samples to the system; (2) allows efficient and deterministic query processing over samples; and (3) provides repeatable semantics and provenance to data scientists. We show that one can realize this model for atemporal relational queries using an unmodified temporal streaming engine, by re-interpreting temporal event fields to denote progress. Based on Prism, we build Now!, a progressive data-parallel computation framework for Windows Azure, where progress is understood as a first-class citizen in the framework. Now! works with "progress-aware reducers"- in particular, it works with streaming engines to support progressive SQL over big data. Extensive experiments on Windows Azure with real and synthetic workloads validate the scalability and benefits of Now! and its optimizations, over current solutions for progressive analytics.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {1726–1737},
numpages = {12}
}

@inproceedings{10.1145/3340531.3412182,
author = {Berns, Fabian and Beecks, Christian},
title = {Automatic Gaussian Process Model Retrieval for Big Data},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412182},
doi = {10.1145/3340531.3412182},
abstract = {Gaussian Process Models (GPMs) are widely regarded as a prominent tool for capturing the inherent characteristics of data. These bayesian machine learning models allow for data analysis tasks such as regression and classification. Usually a process of automatic GPM retrieval is needed to find an optimal model for a given dataset, despite prevailing default instantiations and existing prior knowledge in some scenarios, which both shortcut the way to an optimal GPM. Since non-approximative Gaussian Processes only allow for processing small datasets with low statistical versatility, we propose a new approach that allows to efficiently and automatically retrieve GPMs for large-scale data. The resulting model is composed of independent statistical representations for non-overlapping segments of the given data. Our performance evaluation of the new approach demonstrates the quality of resulting models, which clearly outperform default GPM instantiations, while maintaining reasonable model training time.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {1965–1968},
numpages = {4},
keywords = {performance evaluation, gaussian processes, regression, information retrieval, bayesian machine learning},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3014812.3014869,
author = {Yang, Jie and Ma, Jun and Howard, Sarah K. and Ciao, Matthew and Srikhanta, Rangan},
title = {A Big Data Analytic Framework for Investigating Streaming Educational Data},
year = {2017},
isbn = {9781450347686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014812.3014869},
doi = {10.1145/3014812.3014869},
abstract = {Last decade has witnessed the dramatic expansion of online user-generated content. Making full use of this data to discover behaviour patterns has become an increasingly appealing research topic. In this pilot study, a big data analytic framework is proposed, particularly taking streaming data from students' activity on their laptop usage as an illustrative example. Three modules are implemented to harvest raw streaming records, storage heterogeneous data, and apply the fuzzy representation and rule-mining algorithm for a modelling purpose.The efficiency of the proposed framework is then evaluated using a nationwide streaming dataset. The exploratory simulation of results demonstrates the flexibility and applicability of the proposed framework for processing complex streaming data, and revealing patterns from digital engagement which be used to inform decision makers.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {55},
numpages = {4},
keywords = {user-generated content, big data analytics, rule mining, streaming data},
location = {Geelong, Australia},
series = {ACSW '17}
}

@inproceedings{10.1145/2783258.2788563,
author = {Dhurandhar, Amit and Graves, Bruce and Ravi, Rajesh and Maniachari, Gopikrishanan and Ettl, Markus},
title = {Big Data System for Analyzing Risky Procurement Entities},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2788563},
doi = {10.1145/2783258.2788563},
abstract = {An accredited biennial 2014 study by the Association of Certified Fraud Examiners claims that on average 5% of a company's revenue is lost because of unchecked fraud every year. The reason for such heavy losses are that it takes around 18 months for a fraud to be caught and audits catch only 3% of the actual fraud. This begs the need for better tools and processes to be able to quickly and cheaply identify potential malefactors. In this paper, we describe a robust tool to identify procurement related fraud/risk, though the general design and the analytical components could be adapted to detecting fraud in other domains. Besides analyzing standard transactional data, our solution analyzes multiple public and private data sources leading to wider coverage of fraud types than what generally exists in the marketplace. Moreover, our approach is more principled in the sense that the learning component, which is based on investigation feedback has formal guarantees. Though such a tool is ever evolving, a deployment of this tool over the past 12 months has found many interesting cases from compliance risk and fraud point of view across more than 150 countries and 65000+ vendors, increasing the number of true positives found by over 80% compared with other state-of-the-art tools that the domain experts were previously using.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1741–1750},
numpages = {10},
keywords = {collusion, online learning, social network, risk, big data, procurement, fraud},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@inproceedings{10.1145/2463676.2465290,
author = {Mishne, Gilad and Dalton, Jeff and Li, Zhenghua and Sharma, Aneesh and Lin, Jimmy},
title = {Fast Data in the Era of Big Data: Twitter's Real-Time Related Query Suggestion Architecture},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465290},
doi = {10.1145/2463676.2465290},
abstract = {We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time "twist": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of "big data". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a "big data" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle "big" as well as "fast" data.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1147–1158},
numpages = {12},
keywords = {hadoop, mapreduce, log analysis},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2382416.2382425,
author = {Mell, Peter},
title = {Big Data Technology and Implications for Security Research},
year = {2012},
isbn = {9781450316613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382416.2382425},
doi = {10.1145/2382416.2382425},
booktitle = {Proceedings of the 2012 ACM Workshop on Building Analysis Datasets and Gathering Experience Returns for Security},
pages = {15–16},
numpages = {2},
location = {Raleigh, North Carolina, USA},
series = {BADGERS '12}
}

@inproceedings{10.1145/2616498.2616525,
author = {Sondhi, Sukrit and Arora, Ritu},
title = {Applying Lessons from E-Discovery to Process Big Data Using HPC},
year = {2014},
isbn = {9781450328937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2616498.2616525},
doi = {10.1145/2616498.2616525},
abstract = {The term 'Big Data' defines large datasets that are difficult to use and manage through conventional software tools. Legal Electronic Discovery (e-Discovery) is a business domain which has massive consumption of Big Data, where electronic records such as e-mail, documents, databases and social media postings are processed in order to discover evidence that may be pertinent to legal/compliance needs, litigation or other investigations. Numerous vendors exist in the market to provide organizations with services such as data collection, digital forensics and electronic discovery. High-end instrumentation and modern information technologies are creating data at an ever increasing rate. The challenges associated with managing the large datasets are related to the capture, storage, search, sharing, analytics, and visualization of the data. Big Data also offers unprecedented opportunities in other fields, ranging from astronomy and biology to marketing and e-commerce. This paper presents lessons learnt from the legal e-Discovery domain that can be adapted to process Big Data effectively on HPC resources, thereby benefitting the various disciplines of science, engineering and business that are grappling with a deluge of Big Data challenges and opportunities.},
booktitle = {Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment},
articleno = {8},
numpages = {2},
keywords = {Big Data, predictive analytics, e-Discovery, parallel programming},
location = {Atlanta, GA, USA},
series = {XSEDE '14}
}

@inproceedings{10.1145/3297067.3297093,
author = {Meng, Hailang and Wang, Xinhong and Wang, Xuesong},
title = {Expressway Crash Prediction Based on Traffic Big Data},
year = {2018},
isbn = {9781450366052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297067.3297093},
doi = {10.1145/3297067.3297093},
abstract = {With the development of society, the number of vehicles increases rapidly. The vehicle plays an important role in people's life, however the problem of traffic safety caused by vehicles has also become increasingly prominent. In China, the high crash rate and casualty rate on expressways have always troubled traffic management department. So crash prediction on expressway becomes vital. Conventionally, crash prediction is based on traffic flow data. These data do not contain all the necessary factors. In this paper, we propose a method of prediction using real-world data, including historical accident data, road geometry data, vehicle speed data, and weather data. We treat the crash prediction problem as a binary classification problem. For classification, sample imbalanced is a great challenge in practice. Modifying sample weights is applied to handle this challenge. Three machine learning classification techniques, namely Random Forest (RF), Gradient Boosting Decision Tree (GBDT) and Xgboost, are considered to carry out the crash prediction task respectively. The best recall and precision rate of these models are respectively 0.764253 and 0.01062. The proposed method can be integrated into urban traffic control systems toward police dispatch and crash prevention.},
booktitle = {Proceedings of the 2018 International Conference on Signal Processing and Machine Learning},
pages = {11–16},
numpages = {6},
keywords = {machine learning, feature extraction and selection, Crash prediction, sample imbalance},
location = {Shanghai, China},
series = {SPML '18}
}

@article{10.1145/2992786,
author = {Debattista, Jeremy and Auer, S\"{O}ren and Lange, Christoph},
title = {Luzzu—A Methodology and Framework for Linked Data Quality Assessment},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2992786},
doi = {10.1145/2992786},
abstract = {The increasing variety of Linked Data on the Web makes it challenging to determine the quality of this data and, subsequently, to make this information explicit to data consumers. Despite the availability of a number of tools and frameworks to assess Linked Data Quality, the output of such tools is not suitable for machine consumption, and thus consumers can hardly compare and rank datasets in the order of fitness for use. This article describes a conceptual methodology for assessing Linked Datasets, and Luzzu; a framework for Linked Data Quality Assessment. Luzzu is based on four major components: (1) an extensible interface for defining new quality metrics; (2) an interoperable, ontology-driven back-end for representing quality metadata and quality problems that can be re-used within different semantic frameworks; (3) scalable dataset processors for data dumps, SPARQL endpoints, and big data infrastructures; and (4) a customisable ranking algorithm taking into account user-defined weights. We show that Luzzu scales linearly against the number of triples in a dataset. We also demonstrate the applicability of the Luzzu framework by evaluating and analysing a number of statistical datasets against a variety of metrics. This article contributes towards the definition of a holistic data quality lifecycle, in terms of the co-evolution of linked datasets, with the final aim of improving their quality.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {4},
numpages = {32},
keywords = {linked data, quality assessment, Data quality}
}

@inproceedings{10.1145/3408127.3408180,
author = {Yang, Pinglin and Guo, Gaizhi},
title = {Research on Big Data Parallel Processing Platform Based on Postal Industry},
year = {2020},
isbn = {9781450376877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408127.3408180},
doi = {10.1145/3408127.3408180},
abstract = {With the development of cloud computing, big data, and the Internet of Things, for the data collection and daily drama of the postal express industry, in the face of such a large-scale data set, the traditional storage and calculation related theories and methods can no longer meet the massive and multi-source access and processing of heterogeneous data. The article analyzes the characteristics of postal data, focuses on the Hadoop and Spark platform architectures, and compares the performance differences between the two platforms through experiments. At the same time, according to the characteristics of Hadoop and Spark big data platforms, they learn from each other's strengths and apply them to different stages of the postal big data system.},
booktitle = {Proceedings of the 2020 4th International Conference on Digital Signal Processing},
pages = {305–309},
numpages = {5},
keywords = {Big data, Hadoop, Big data platform, Postal, Spark},
location = {Chengdu, China},
series = {ICDSP 2020}
}

@inproceedings{10.1145/2342441.2342462,
author = {Wang, Guohui and Ng, T.S. Eugene and Shaikh, Anees},
title = {Programming Your Network at Run-Time for Big Data Applications},
year = {2012},
isbn = {9781450314770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342441.2342462},
doi = {10.1145/2342441.2342462},
abstract = {Recent advances of software defined networking and optical switching technology make it possible to program the network stack all the way from physical topology to flow level traffic control. In this paper, we leverage the combination of SDN controller with optical switching to explore the tight integration of application and network control. We particularly study the run-time network configuration for big data applications to jointly optimize application performance and network utilization. We use Hadoop as an example to discuss the integrated network control architecture, job scheduling, topology and routing configuration mechanisms for Hadoop jobs. Our analysis suggests that such an integrated control has great potential to improve application performance with relatively small configuration overhead. We believe our study shows early promise of achieving the long-term goal of tight network and application integration using SDN.},
booktitle = {Proceedings of the First Workshop on Hot Topics in Software Defined Networks},
pages = {103–108},
numpages = {6},
keywords = {big data applications, optical circuit switching, software defined networking},
location = {Helsinki, Finland},
series = {HotSDN '12}
}

@inproceedings{10.1145/3335656.3335688,
author = {Li, Minxuan},
title = {Innovation of Data Mining &amp; Screening System under Big Data: Take a Case as NIMBY},
year = {2019},
isbn = {9781450360906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335656.3335688},
doi = {10.1145/3335656.3335688},
abstract = {With the growing maturity of web crawler technology and the advent of the era of big data, when you want to study some problems, you can directly get all the data related to them through web crawlers and other means, but it is more important to mining and filter the data to get valuable data for the research content. This study which based on the word list of keywords uses CRN network to construct semantic distance table and TOPSIS evaluation system to sort data to make sure researchers can obtain quantitative screening data with research value and to provide researchers with scientific screening methods.},
booktitle = {Proceedings of the 2019 International Conference on Data Mining and Machine Learning},
pages = {70–74},
numpages = {5},
keywords = {Data screening, CRN network, Data Mining, Keyword list of NIMBY event, Big Data, TOPSIS evaluation system},
location = {Hong Kong, Hong Kong},
series = {ICDMML 2019}
}

@article{10.1145/2925686.2925691,
title = {New Chapters Focus on Big Data and Resource-Constrained Environments},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/2925686.2925691},
doi = {10.1145/2925686.2925691},
abstract = {SIGHPC is expanding its "virtual chapter" offerings through two new chapters: one focused on topics at the intersection of HPC and Big Data (SIGHPC-BigData), and the other on developing cyberinfrastructure and workforce development in resourceconstrained environments (SIGHPC-RCE). These join SIGHPC's first virtual chapter on Education in HPC (SIGHPC-Edu).},
journal = {ACM SIGHPC Connect},
month = {apr},
pages = {7–8},
numpages = {2}
}

@inproceedings{10.1145/3011141.3011185,
author = {Mountasser, Imadeddine and Ouhbi, Brahim and Frikh, Bouchra},
title = {Hybrid Large-Scale Ontology Matching Strategy on Big Data Environment},
year = {2016},
isbn = {9781450348072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011141.3011185},
doi = {10.1145/3011141.3011185},
abstract = {Ontology matching is one of the essential methodologies to overcome heterogeneity issues. Multiple knowledge-based and information systems perform ontology matching strategies to find correspondences between several ontologies for the purpose of discovering valuable information across various domains. The design and implementation of matching systems raises several challenges, especially, the matching accuracy and the performance issues. Accordingly, adapting the system to the requirements of Big Data era brings additional perspectives and challenges. Furthermore, to provide on-the-fly matching and in-time processing, the system must handle matching accuracy, runtime complexity and performance issues as an entire matching strategy. To this end, this paper presents a new hybrid ontology matching approach that benefit on one hand from the opportunities offered by parallel platforms, and on the other hand from ontology matching techniques, while applying a resource-based decomposition to improve the performance of the system.},
booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
pages = {282–287},
numpages = {6},
keywords = {large-scale ontology matching, big data, parallel platforms, parallel matching, heterogeneity resolution},
location = {Singapore, Singapore},
series = {iiWAS '16}
}

@proceedings{10.1145/3152723,
title = {ICBDR 2017: Proceedings of the 2017 International Conference on Big Data Research},
year = {2017},
isbn = {9781450353564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to introduce you to the Proceedings of 2017 International Conference on Big Data Research (ICBDR 2017) which was held on October 22-24, 2017 at Osaka, Japan.},
location = {Osaka, Japan}
}

@inproceedings{10.1145/3265007.3265015,
author = {Zhang, Bin and Zhu, Guobin and Yu, Riji and Wei, Shaoyan and Peng, Ling and Fei, Dingzhou and Yu, Xuesong and Pan, Peiwen},
title = {Research on the Innovation of Trajectory Big Data in Social Governance},
year = {2018},
isbn = {9781450365741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265007.3265015},
doi = {10.1145/3265007.3265015},
abstract = {With the development of modern society. The unprecedented prosperity of science &amp; technology and finance. Objects formed a huge amount of track data in its movement. The large amount of track data contains rich spatio-temporal characteristics information, it exposes the privacy information such as the behavior characteristics, interests and social habits of mobile objects. Through trajectory data processing technology. It can excavate information such as human activity pattern and behavior characteristic, urban vehicle movement characteristic, atmospheric environment change law and so on. The large amount of track data also reveals the privacy information, such as the behavior characteristics, interests and social habits of mobile objects, which is rich in spatio-temporal characteristics information. This paper begins with the significance of the study of trajectory big data. Introducing track big data acquisition mode and social application in various fields, In the specific application With the development of modern society. The unprecedented prosperity of science &amp; technology and finance. Objects formed a huge amount of track data in its movement. The large amount of track data contains rich spatio-temporal characteristics information, it exposes the privacy information such as the behavior characteristics, interests and social habits of mobile objects. Through trajectory data processing technology. It can excavate information such as human activity pattern and behavior characteristic, urban vehicle movement characteristic, atmospheric environment change law and so on. The large amount of track data also reveals the privacy information, such as the behavior characteristics, interests and social habits of mobile objects, which is rich in spatio-temporal characteristics information. This paper begins with the significance of the study of trajectory big data. Introducing track big data acquisition mode and social application in various fields, In the specific application, we pay more attention to the object's trajectory privacy protection. Applying the big data of trajectory to social governance; In addition, the application of big data in social governance is summarized and the future work prospect is discussed. We pay more attention to the object's trajectory privacy protection. Applying the big data of trajectory to social governance; In addition, the application of big data in social governance is summarized and the future work prospect is discussed.},
booktitle = {Proceedings of the 6th ACM/ACIS International Conference on Applied Computing and Information Technology},
pages = {38–42},
numpages = {5},
keywords = {Social Computing, Trajectory Big Data, Social Governance, Privacy Protection},
location = {Kunming, China},
series = {ACIT 2018}
}

@article{10.1145/966389.966395,
author = {Pierce, Elizabeth M.},
title = {Assessing Data Quality with Control Matrices},
year = {2004},
issue_date = {February 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/966389.966395},
doi = {10.1145/966389.966395},
abstract = {The control matrix, long used by IS auditors to evaluate information integrity, can be modified to assess the reliability of information products.},
journal = {Commun. ACM},
month = {feb},
pages = {82–86},
numpages = {5}
}

@inproceedings{10.1145/3318265.3318268,
author = {Li, Zhongwei and Duan, Feng and Che, Hao},
title = {A Unified Scaling Model in the Era of Big Data Analytics},
year = {2019},
isbn = {9781450366380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318265.3318268},
doi = {10.1145/3318265.3318268},
abstract = {As scale-out execution of big data analytics has become predominate datacenter workloads, it is of paramount importance to faithfully characterize the scaling properties for such workloads. To date, the most widely cited scaling laws for big data analytics is the traditional Amdahl's law, which was discovered well before the era of big data analytics. A key observation made in this paper is that both the system and workload models underlying the traditional scaling laws are too simplistic to fully characterize the scaling properties for big data analytics workloads. In this paper, we put forward a Unified Scaling model for Big data Analytics (USBA), based on a multi-stage system model and a discretized workload model. USBA allows for flexible workload scaling unifying the fixed-size and fixed-time workload models underlying Amdahl's and Gustafson's laws, respectively, and flexible system scaling in terms of both number of stages and degree of parallelism per stage. Moreover, to faithfully characterize the scaling properties for big data analytics workloads, USBA accounts for variabilities of task response times and barrier synchronization. Finally, application of USBA to the scaling analysis of four Spark-based data mining and graph benchmarks demonstrates that USBA is able to adequately characterize the scaling design space and predict the scaling properties of real-world big data analytics workloads. This makes it possible to use USBA as a useful tool to facilitate job resource provisioning for big data analytics in datacenters.},
booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
pages = {67–77},
numpages = {11},
keywords = {MapReduce, performance modeling, Gustafson's law, Amdahl's law, spark, big data analytics},
location = {Xi'an, China},
series = {HP3C '19}
}

@inproceedings{10.1145/3314221.3314650,
author = {Wang, Chenxi and Cui, Huimin and Cao, Ting and Zigman, John and Volos, Haris and Mutlu, Onur and Lv, Fang and Feng, Xiaobing and Xu, Guoqing Harry},
title = {Panthera: Holistic Memory Management for Big Data Processing over Hybrid Memories},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314650},
doi = {10.1145/3314221.3314650},
abstract = {Modern data-parallel systems such as Spark rely increasingly on in-memory computing that can significantly improve the efficiency of iterative algorithms. To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy-inefficient. Emerging non-volatile memory (NVM) technologies offers high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages (e.g., Scala and Java) and executed on top of a managed runtime (e.g., the Java Virtual Machine) that already performs various dimensions of memory management. Supporting hybrid physical memories adds in a new dimension, creating unique challenges in data replacement and migration. This paper proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed down to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division is accurate enough to guide GC for data layout, which hardly incurs data monitoring and moving overhead. We have implemented Panthera in OpenJDK and Apache Spark. An extensive evaluation with various datasets and applications demonstrates that Panthera reduces energy by 32 – 52% at only a 1 – 9% execution time overhead.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {347–362},
numpages = {16},
keywords = {hybrid memories, Big Data systems, memory management, garbage collection},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/2896825.2896834,
author = {Klein, John and Buglak, Ross and Blockow, David and Wuttke, Troy and Cooper, Brenton},
title = {A Reference Architecture for Big Data Systems in the National Security Domain},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896834},
doi = {10.1145/2896825.2896834},
abstract = {Acquirers, system builders, and other stakeholders of big data systems need to define requirements, develop and evaluate solutions, and integrate systems together. A reference architecture enables these software engineering activities by standardizing nomenclature, defining key solution elements and their relationships, collecting relevant solution patterns, and classifying existing technologies. Within the national security domain, existing reference architectures for big data systems have not been useful because they are too general or are not vendor-neutral. We present a reference architecture for big data systems that is focused on addressing typical national defence requirements and that is vendor-neutral, and we demonstrate how to use this reference architecture to define solutions in one mission area.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {51–57},
numpages = {7},
keywords = {big data, reference architecture},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/3129292.3129296,
author = {Costa, Constantinos and Chatzimilioudis, Georgios and Zeinalipour-Yazti, Demetrios and Mokbel, Mohamed F.},
title = {Towards Real-Time Road Traffic Analytics Using Telco Big Data},
year = {2017},
isbn = {9781450354257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129292.3129296},
doi = {10.1145/3129292.3129296},
abstract = {A telecommunication company (telco) is traditionally only perceived as the entity that provides telecommunication services, such as telephony and data communication access to users. However, the IP backbone infrastructure of such entities spanning densely urban spaces and widely rural areas, provides nowadays a unique opportunity to collect immense amounts of mobility data that can provide valuable insights for road traffic management and avoidance. In this paper we outline the components of the Traffic-TBD (Traffic Telco Big Data) architecture, which aims to become an innovative road traffic analytic and prediction system with the following desiderata: i) provide micro-level traffic modeling and prediction that goes beyond the current state provided by Internet-based navigation enterprises utilizing crowdsourcing; ii) retain the location privacy boundaries of users inside their mobile network operator, to avoid the risks of exposing location data to third-party mobile applications; and iii) be available with minimal costs and using existing infrastructure (i.e., cell towers and TBD data streams are readily available inside a telco). Road traffic understanding, management and analytics can minimize the number of road accidents, optimize fuel and energy consumption, avoid unexpected delays, contribute to a macroscopic spatio-temporal understanding of traffic in cities but also to "smart" societies through applications in city planning, public transportation, logistics and fleet management for enterprises, startups and governmental bodies.},
booktitle = {Proceedings of the International Workshop on Real-Time Business Intelligence and Analytics},
articleno = {5},
numpages = {5},
keywords = {Road Traic, Telco, Big Data, Data Analytics},
location = {Munich, Germany},
series = {BIRTE '17}
}

@inproceedings{10.1145/3167486.3167565,
author = {Mohammed, Zouiten},
title = {Machine Learning Algorithms for Oncology Big Data Treatment},
year = {2017},
isbn = {9781450353069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167486.3167565},
doi = {10.1145/3167486.3167565},
abstract = {Two-dimensional arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 m and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements. Our work is part of user-centered healthcare decision-making systems based on a process of predicting cancer distribution. This process should lead to a set of knowledge in Datamining, Ontologies and Geographical Information Systems. It is in the same time iterative and interactive. Therefore, it seems essential to take into account principles and methods of Human-Machine Interaction in the development of such systems. In this respect, development of interactive decision-making systems is currently being approached using two opposing approaches. In the first one, technology is fundamental; the second one is user centered placing the human actors in a central position. Although the first approach is still present in healthcare organizations, the current trend is definitely the user centric. In our framework we propose an approach that aims to integrate the steps of the predicting future from data process into a development model enriched from human-machine interactions. Our application context is the fight against breast cancer in hospitals. We demonstrate that medical decision can be based on a spatial analysis of the geographical distribution of many cancers. Several factors explain our choice of datamining for assistance of health decision-makers for learning in the CART algorithm about patients who are future actors of suspicion.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Wireless Communication Systems},
articleno = {76},
numpages = {6},
keywords = {CART, Redundancy, GIS health, datamining, Machine learning},
location = {Larache, Morocco},
series = {ICCWCS'17}
}

@inproceedings{10.5555/2888619.2888709,
author = {Gaku, Rie and Takakuwa, Soemon},
title = {Big Data-Driven Service Level Analysis for a Retail Store},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {Using simulation technology, a procedure is proposed for a big data-driven service-level analysis for a real retail store. First, a data generator is designed to randomly select a sample of an expected number of customers or sampling data on a certain day from a large-scale dataset of sales predefined. Second, the clerk schedules are inputted into a data table created using Excel. Finally, simulation modeling mimics the service process of the retail store to examine and analyze the customer service level based on the selected data and the inputted clerk schedules. The proposed procedure for big data-driven service-level analysis shows the relations between the influencing service-level elements between the number of customers coming into stores, the frequency of customers, and the average customer service time. The procedure is generic and can easily be used to examine the service level in the remote past or to analyze and forecast the future.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {791–799},
numpages = {9},
location = {Huntington Beach, California},
series = {WSC '15}
}

@inproceedings{10.1145/1651415.1651421,
author = {Mehmood, Kashif and Si-Said Cherfi, Samira and Comyn-Wattiau, Isabelle},
title = {Data Quality through Model Quality: A Quality Model for Measuring and Improving the Understandability of Conceptual Models},
year = {2009},
isbn = {9781605588162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1651415.1651421},
doi = {10.1145/1651415.1651421},
abstract = {Data quality has emerged as an important and challenging topic in recent years. This article addresses the conceptual model quality as it has been widely accepted that better conceptual models produce better information systems and thus implicitly improve the data quality. Conceptual Models are designed as part of the analysis phase and serve as a communicating mediator between the users and the development team. Consequently, their understandability is a real challenge to avoid the propagation of inaccurate interpretation of the user requirements to the underlying system design and implementation. In this paper, we propose an adaptive quality model. We illustrate its usefulness by describing how it can be used to model and evaluate the understandability of conceptual models. Our quality evaluation is enriched with corrective actions provided to the designer, leading to a guidance modeling process. A first validation based on a survey is proposed.},
booktitle = {Proceedings of the First International Workshop on Model Driven Service Engineering and Data Quality and Security},
pages = {29–32},
numpages = {4},
keywords = {conceptual modeling quality, quality metrics, conceptual model understandability, quality factor, conceptual model},
location = {Hong Kong, China},
series = {MoSE+DQS '09}
}

@inproceedings{10.5555/2819289.2819298,
author = {DeLine, Robert},
title = {Research Opportunities for the Big Data Era of Software Engineering},
year = {2015},
publisher = {IEEE Press},
abstract = {Big Data Analysis is becoming a widespread practice on many software development projects, and statisticians and data analysts are working alongside developers, testers and program managers. Because data science is still an emerging discipline in software projects, there are many opportunities where software engineering researchers can help improve practice. In terms of productivity, data scientists need support for exploratory analysis of large datasets, relief from clerical tasks like data cleaning, and easier paths for live deployment of new analyses. In terms of correctness, data scientists need help in preserving data meaning and provenance, and non-experts need help avoiding analysis errors. In terms of communication and coordination, teams need more approachable ways to discuss uncertainty and risk, and support for data-driven decision making needs to become available to all roles. This position paper describes these open problems and points to ongoing research beginning to tackle them.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {26–29},
numpages = {4},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@article{10.1145/2998575,
author = {Labouseur, Alan G. and Matheus, Carolyn C.},
title = {An Introduction to Dynamic Data Quality Challenges},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2998575},
doi = {10.1145/2998575},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {6},
numpages = {3},
keywords = {relational systems, big data, internet of things, graph systems, Dynamic data quality}
}

@article{10.1007/s00778-018-0514-9,
author = {To, Quoc-Cuong and Soto, Juan and Markl, Volker},
title = {A Survey of State Management in Big Data Processing Systems},
year = {2018},
issue_date = {December  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-018-0514-9},
doi = {10.1007/s00778-018-0514-9},
abstract = {The concept of state and its applications vary widely across big data processing systems. This is evident in both the research literature and existing systems, such as Apache Flink, Apache Heron, Apache Samza, Apache Spark, and Apache Storm. Given the pivotal role that state management plays, particularly, for iterative batch and stream processing, in this survey, we present examples of state as an enabler, discuss the alternative approaches used to handle and implement state, capture the many facets of state management, and highlight new research directions. Our aim is to provide insight into disparate state management techniques, motivate others to pursue research in this area, and draw attention to open problems.},
journal = {The VLDB Journal},
month = {dec},
pages = {847–872},
numpages = {26},
keywords = {Big data processing systems, Survey, State management}
}

@inproceedings{10.1145/2882903.2915229,
author = {Shkapsky, Alexander and Yang, Mohan and Interlandi, Matteo and Chiu, Hsuan and Condie, Tyson and Zaniolo, Carlo},
title = {Big Data Analytics with Datalog Queries on Spark},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2915229},
doi = {10.1145/2882903.2915229},
abstract = {There is great interest in exploiting the opportunity provided by cloud computing platforms for large-scale analytics. Among these platforms, Apache Spark is growing in popularity for machine learning and graph analytics. Developing efficient complex analytics in Spark requires deep understanding of both the algorithm at hand and the Spark API or subsystem APIs (e.g., Spark SQL, GraphX). Our BigDatalog system addresses the problem by providing concise declarative specification of complex queries amenable to efficient evaluation. Towards this goal, we propose compilation and optimization techniques that tackle the important problem of efficiently supporting recursion in Spark. We perform an experimental comparison with other state-of-the-art large-scale Datalog systems and verify the efficacy of our techniques and effectiveness of Spark in supporting Datalog-based analytics.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1135–1149},
numpages = {15},
keywords = {recursive queries, datalog, spark, monotonic aggregates},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/2538542.2538565,
author = {Brim, Michael J. and Dillow, David A. and Oral, Sarp and Settlemyer, Bradley W. and Wang, Feiyi},
title = {Asynchronous Object Storage with QoS for Scientific and Commercial Big Data},
year = {2013},
isbn = {9781450325059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538542.2538565},
doi = {10.1145/2538542.2538565},
abstract = {This paper presents our design for an asynchronous object storage system intended for use in scientific and commercial big data workloads. Use cases from the target workload domains are used to motivate the key abstractions used in the application programming interface (API). The architecture of the Scalable Object Store (SOS), a prototype object storage system that supports the API's facilities, is presented. The SOS serves as a vehicle for future research into scalable and resilient big data object storage. We briefly review our research into providing efficient storage servers capable of providing quality of service (QoS) contracts relevant for big data use cases.},
booktitle = {Proceedings of the 8th Parallel Data Storage Workshop},
pages = {7–13},
numpages = {7},
keywords = {HPC storage, object storage, cloud storage, storage QoS},
location = {Denver, Colorado},
series = {PDSW '13}
}

@article{10.1145/2076450.2076453,
author = {Stonebraker, Michael and Hong, Jason},
title = {Researchers' Big Data Crisis; Understanding Design and Functionality},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/2076450.2076453},
doi = {10.1145/2076450.2076453},
abstract = {The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.twitterFollow us on Twitter at http://twitter.com/blogCACMhttp://cacm.acm.org/blogs/blog-cacmMichael Stonebraker issues a call to arms about research groups' data-management problems. Jason Hong discusses the nature of functionality with respect to design.},
journal = {Commun. ACM},
month = {feb},
pages = {10–11},
numpages = {2}
}

@inproceedings{10.1145/3291801.3291827,
author = {Hu, Xinwu and Luo, Pengcheng and Zhang, Xiaonan and Wang, Jun and Zhou, Tianren},
title = {Research on the Effectiveness Evaluation of Big Data in Combat Simulation},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291827},
doi = {10.1145/3291801.3291827},
abstract = {With the development of complex system simulation techniques, computational capabilities, and data management capabilities, the simulation results tend to be big data. There are also high-dimension, high-redundancy, and high-correlation issues among indexes. Based on the above background, a two-layer Autoencoder neural network is used for feature extraction and dimensionality reduction. Then, 10 deep neural network models are established for index learning. The experimental results show that the 32-layer Resent network works best for low-dimensional data effectiveness evaluation.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {70–75},
numpages = {6},
keywords = {neural network, system simulation, big data, effectiveness evaluation, data dimension reduction},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/2808492.2808532,
author = {Chen, Boyang and Ge, Shiming and Xie, Kaixuan},
title = {Centroid Location Algorithm in Three Dimensions Based on Big Data},
year = {2015},
isbn = {9781450335287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808492.2808532},
doi = {10.1145/2808492.2808532},
abstract = {Conventional centroid location algorithms are all in two dimensions. In order to solve the problem that the conventional centroid location algorithms are useless when the point spread function is smaller than the size of the detector, the research is about the centroid location algorithm in three dimensions based on big data. By using the time parameter to link the big data of energy received by the detector at different time, not only the single image but the time sequence images are used in the algorithm, based on the geometric theorem, the exact position at the special time is calculated out. It is sure that, the algorithm is very steady when the sample number is enough, that means the phase of the sample point is nothing, and the error of the position got by the algorithm is less than 0.06 pixel when the non-uniformity of the detectors is smaller than 5%, that is usually the upper limit of the non-uniformity of the detector.},
booktitle = {Proceedings of the 7th International Conference on Internet Multimedia Computing and Service},
articleno = {40},
numpages = {6},
keywords = {point target, centroid location, three dimensions},
location = {Zhangjiajie, Hunan, China},
series = {ICIMCS '15}
}

@inproceedings{10.1109/CCGrid.2015.122,
author = {Fox, Geoffrey C. and Qiu, Judy and Kamburugamuve, Supun and Jha, Shantenu and Luckow, Andre},
title = {HPC-ABDS High Performance Computing Enhanced Apache Big Data Stack},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.122},
doi = {10.1109/CCGrid.2015.122},
abstract = {We review the High Performance Computing Enhanced Apache Big Data Stack HPC-ABDS and summarize the capabilities in 21 identified architecture layers. These cover Message and Data Protocols, Distributed Coordination, Security &amp; Privacy, Monitoring, Infrastructure Management, DevOps, Interoperability, File Systems, Cluster &amp; Resource management, Data Transport, File management, NoSQL, SQL (NewSQL), Extraction Tools, Object-relational mapping, In-memory caching and databases, Inter-process Communication, Batch Programming model and Runtime, Stream Processing, High-level Programming, Application Hosting and PaaS, Libraries and Applications, Workflow and Orchestration. We summarize status of these layers focusing on issues of importance for data analytics. We highlight areas where HPC and ABDS have good opportunities for integration.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {1057–1066},
numpages = {10},
keywords = {HPC, apache big data stack},
location = {Shenzhen, China},
series = {CCGRID '15}
}

