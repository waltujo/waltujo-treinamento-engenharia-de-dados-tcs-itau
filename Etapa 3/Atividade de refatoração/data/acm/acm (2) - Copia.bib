@inproceedings{10.1145/2967878.2967919,
author = {Sunaina and S., Sowmya Kamath},
title = {Query-Oriented Unsupervised Multi-Document Summarization on Big Data},
year = {2016},
isbn = {9781450341790},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2967878.2967919},
doi = {10.1145/2967878.2967919},
abstract = {Real time document summarization is a critical need nowadays, owing to the large volume of information available for our reading, and our inability to deal with this entirely due to limitations of time and resources. Oftentimes, information is available in multiple sources, offering multiple contexts and viewpoints on a single topic of interest. Automated multi-document summarization (MDS) techniques aim to address this problem. However, current techniques for automated MDS suffer from low precision and accuracy with reference to a given subject matter, when compared to those summaries prepared by humans and takes large time to create the summary when the input given is too huge. In this paper, we propose a hybrid MDS technique combining feature based algorithms and dynamic programming for generating a summary from multiple documents based on user provided query. Further, in real-world scenarios, Web search serves up a large number of URLs to users, and the work of making sense of these with reference to a particular query is left to the user. In this context, an efficient parallelized MDS technique based on Hadoop is also presented, for serving a concise summary of multiple Webpage contents for a given user query in reduced time duration.},
booktitle = {Proceedings of the 7th International Conference on Computing Communication and Networking Technologies},
articleno = {37},
numpages = {6},
keywords = {natural language processing, multi-document summarization, dynamic programming, map-reduce},
location = {Dallas, TX, USA},
series = {ICCCNT '16}
}

@inproceedings{10.1145/3185768.3186299,
author = {Uta, Alexandru and Obaseki, Harry},
title = {A Performance Study of Big Data Workloads in Cloud Datacenters with Network Variability},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3186299},
doi = {10.1145/3185768.3186299},
abstract = {Public cloud computing platforms are a cost-effective solution for individuals and organizations to deploy various types of workloads, ranging from scientific applications, business-critical workloads, e-governance to big data applications. Co-locating all such different types of workloads in a single datacenter leads not only to performance degradation, but also to large degrees of performance variability, which is the result of virtualization, resource sharing and congestion. Many studies have already assessed and characterized the degree of resource variability in public clouds. However, we are missing a clear picture on how resource variability impacts big data workloads. In this work, we take a step towards characterizing the behavior of big data workloads under network bandwidth variability. Emulating real-world clouds» bandwidth distribution, we characterize the performance achieved by running real-world big data applications. We find that most big data workloads are slowed down under network variability scenarios, even those that are not network-bound. Moreover, the maximum average slowdown for the cloud setup with highest variability is 1.48 for CPU-bound workloads, and 1.79 for network-bound workloads.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {113–118},
numpages = {6},
keywords = {big data, network variability, performance variability, spark, heterogeneity, performance predictability},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1145/3291801.3291837,
author = {Cui, Zhengzheng and Wang, Danbei and Chen, Sisi and Ding, Huiming and Xie, Zhifeng},
title = {The Visualized Analysis System for Big Data of Movies' Box Office in China},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291837},
doi = {10.1145/3291801.3291837},
abstract = {Chinese film industry has massive data, but the overall industrial system is still in infancy. At present, there is still a short board in the field of analysis for big data based on movies' box office in many studies. In this paper, we propose a Visualized Analysis System for Big-data of Movies' Box-office in China, which includes three parts: data real-timely collection, data visualization and data analysis. The system has a comprehensive collection of film data that includes all box offices for Chinese films in recent years. And it also has a clear system structure, which can better display the intrinsic value of Chinese movie's box office.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {90–94},
numpages = {5},
keywords = {Visualized analysis, Film data, Big Data},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/2554688.2554694,
author = {Wang, Chao and Li, Xi and Zhou, Xuehai and Chen, Yunji and Cheung, Ray C.C.},
title = {Big Data Genome Sequencing on Zynq Based Clusters (Abstract Only)},
year = {2014},
isbn = {9781450326711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554688.2554694},
doi = {10.1145/2554688.2554694},
abstract = {Next-generation sequencing (NGS) problems have attracted many attentions of researchers in biological and medical computing domains. The current state-of-the-art NGS computing machines are dramatically lowering the cost and increasing the throughput of DNA sequencing. In this paper, we propose a practical study that uses Xilinx Zynq board to summarize acceleration engines using FPGA accelerators and ARM processors for the state-of-the-art short read mapping approaches. The heterogeneous processors and accelerators are coupled with each other using a general Hadoop distributed processing framework. First the reads are collected by the central server, and then distributed to multiple accelerators on the Zynq for hardware acceleration. Therefore, the combination of hardware acceleration and Map-Reduce execution flow could greatly accelerate the task of aligning short length reads to a known reference genome. Our approach is based on preprocessing the reference genomes and iterative jobs for aligning the continuous incoming reads. The hardware acceleration is based on the creditable read-mapping algorithm RMAP software approach. Furthermore, the speedup analysis on a Hadoop cluster, which concludes 8 development boards, is evaluated. Experimental results demonstrate that our proposed architecture and methods has the speedup of more than 112X, and is scalable with the number of accelerators. Finally, the Zynq based cluster has efficient potential to accelerate even general large scale big data applications.This work was supported by the NSFC grants No. 61379040, No. 61272131 and No. 61202053.},
booktitle = {Proceedings of the 2014 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {247},
numpages = {1},
keywords = {genome sequencing, rmap, fpga, hardware acceleration., bioinformatics},
location = {Monterey, California, USA},
series = {FPGA '14}
}

@inproceedings{10.1145/2232817.2232911,
author = {Stebe, Janez},
title = {Responsibility for Research Data Quality in Open Access: A Slovenian Case},
year = {2012},
isbn = {9781450311540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2232817.2232911},
doi = {10.1145/2232817.2232911},
abstract = {In the framework of a project aiming to realize a strategy of open research data access in Slovenia in accordance with OECD principles, we conducted a series of interviews with different target audiences in order to assess the initial conditions in the area of data handling. The data creators and data services expressed a high level of awareness about data quality issues, especially in relation to good publication potential. Barriers to ensuring the greater accessibility of data in the future include the little recognition and reputation for doing the related extra work involved in preparing data and documentation, the need for financial rewards for such additional work, and the undeveloped culture of data exchange in general. The motivation to provide open access to such data will involve a combination of requirements prescribed for data delivery, and the provision of support services and financial rewards, in particular changing the views held by the professional scientific community about the benefits of open data for research activities.},
booktitle = {Proceedings of the 12th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {401–402},
numpages = {2},
keywords = {open data, data quality, culture, stakeholders attitudes},
location = {Washington, DC, USA},
series = {JCDL '12}
}

@inproceedings{10.1145/3344341.3368796,
author = {Kuhlenkamp, J\"{o}rn and Werner, Sebastian and Borges, Maria C. and El Tal, Karim and Tai, Stefan},
title = {An Evaluation of FaaS Platforms as a Foundation for Serverless Big Data Processing},
year = {2019},
isbn = {9781450368940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344341.3368796},
doi = {10.1145/3344341.3368796},
abstract = {Function-as-a-Service (FaaS), offers a new alternative to operate cloud-based applications. FaaS platforms enable developers to define their application only through a set of service functions, relieving them of infrastructure management tasks, which are executed automatically by the platform. Since its introduction, FaaS has grown to support workloads beyond the lightweight use-cases it was originally intended for, and now serves as a viable paradigm for big data processing. However, several questions regarding FaaS platform quality are still unanswered. Specifically, the impact of automatic infrastructure management on serverless big data applications remains unexplored.In this paper, we propose a novel evaluation method (SIEM) to understand the impact of these tasks. For this purpose, we introduce new metrics to quantify quality in different big data application scenarios. We show an application of SIEM by evaluating the four major FaaS providers, and contribute results and new insights for FaaS-based big data processing.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing},
pages = {1–9},
numpages = {9},
keywords = {big data processing, benchmarking, serverless, cloud computing},
location = {Auckland, New Zealand},
series = {UCC'19}
}

@inproceedings{10.1145/3393527.3393532,
author = {Shi, Bin and YabinXu},
title = {Research on Copyright Protection Method of Big Data Based on Nash Equilibrium and Constraint Optimization},
year = {2020},
isbn = {9781450375344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393527.3393532},
doi = {10.1145/3393527.3393532},
abstract = {Data watermarking technology is an effective means to protect the copyright of big data. In order to embed robust and highly available data watermarks, firstly, based on the game theory, a Nash equilibrium model between watermark robustness and data quality is established to solve the optimal number of data partitioning. Then, the mapping relationship between data partitioning and watermark bit is established by using secure hash algorithm. Finally, under the constraint of data usability, the improved particle swarm optimization algorithm is used to calculate the optimal solution of data change for each data partitioning, and then the data is changed accordingly to complete the embedding of watermark bit. In order to verify the copyright ownership of big data, this paper also gives the corresponding watermark extraction method. Watermark extraction is the inverse process of watermark embedding. First, traverse all partitions and extract the possible embedded bit values in each data partitioning. Then, the actual embedded watermark bit is finally determined by majority voting strategies. The experimental results show that our proposed method can not only detect watermarks under different attack conditions, ensure the robustness of big data watermarks, but also achieve better data quality, and the comprehensive effect of data watermarks is better than the existing methods.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
pages = {21–25},
numpages = {5},
keywords = {Particle swarm optimization algorithm, Majority voting strategy, Copyright protection, Big data, Nash equilibrium, Data watermarking, Constrained optimization},
location = {Hefei, China},
series = {ACM TURC'20}
}

@proceedings{10.1145/3141128,
title = {ICCBDC 2017: Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to introduce you to the proceedings of 2017 International Conference on Cloud and Big Data Computing (ICCBDC 2017). Big data is a rapidly expanding research area spanning the fields of computer science and information management, and has become a ubiquitous term in understanding and solving complex problems in different disciplinary fields such as engineering, applied mathematics, medicine, computational biology, healthcare, social networks, finance, business, government, education, transportation and telecommunications.},
location = {London, United Kingdom}
}

@inproceedings{10.5555/3192424.3192600,
author = {Xylogiannopoulos, Konstantinos F. and Karampelas, Panagiotis and Alhajj, Reda},
title = {Frequent and Non-Frequent Pattern Detection in Big Data Streams: An Experimental Simulation in 1 Trillion Data Points},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {Big data streaming analysis nowadays has become one of the most important topic in the list of data analysts since enormous amount of data are produced daily by the numerous smart devices. The analysis of such data is very important and the detection of frequent or even non-frequent patterns can be critical for many aspects of our lives. In the current paper, we propose a new methodology based on our previous work regarding the detection of all repeated patterns in a string in order to analyze a very big data stream with 1 Trillion digits, composed from 1 thousand subsequences of 1 billion digits each one. More specifically, using the novel data structure, LERP Reduced Suffix Array, and the innovative ARPaD algorithm which allows the detection of all repeated patterns in a string we managed to analyze each one of the 1 billion data points, using 10 computers with standard hardware configuration, in 33 minutes which outperforms to the best of our knowledge any other existing methodology, which is equivalent to data point generation every 2 microseconds.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {931–938},
numpages = {8},
keywords = {ARPaD, big data, data stream, LERP-RSA},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.1145/3105831.3105842,
author = {Santos, Maribel Yasmina and Costa, Carlos and Galv\~{a}o, Jo\~{a}o and Andrade, Carina and Martinho, Bruno Augusto and Lima, Francisca Vale and Costa, Eduarda},
title = {Evaluating SQL-on-Hadoop for Big Data Warehousing on Not-So-Good Hardware},
year = {2017},
isbn = {9781450352208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105831.3105842},
doi = {10.1145/3105831.3105842},
abstract = {Big Data is currently conceptualized as data whose volume, variety or velocity impose significant difficulties in traditional techniques and technologies. Big Data Warehousing is emerging as a new concept for Big Data analytics. In this context, SQL-on-Hadoop systems increased notoriety, providing Structured Query Language (SQL) interfaces and interactive queries on Hadoop. A benchmark based on a denormalized version of the TPC-H is used to compare the performance of Hive on Tez, Spark, Presto and Drill. Some key contributions of this work include: the direct comparison of a vast set of technologies; unlike previous scientific works, SQL-on-Hadoop systems were connected to Hive tables instead of raw files; allow to understand the behaviour of these systems in scenarios with ever-increasing requirements, but not-so-good hardware. Besides these benchmark results, this paper also makes available interesting findings regarding an architecture and infrastructure in SQL-on-Hadoop for Big Data Warehousing, helping practitioners and fostering future research.},
booktitle = {Proceedings of the 21st International Database Engineering &amp; Applications Symposium},
pages = {242–252},
numpages = {11},
keywords = {SQL-on-Hadoop, Big Data Warehousing, Presto, Hadoop, Benchmark, Data Warehouse, Hive, Spark, Drill, Big Data},
location = {Bristol, United Kingdom},
series = {IDEAS '17}
}

@inproceedings{10.1145/2663876.2663885,
author = {Freudiger, Julien and Rane, Shantanu and Brito, Alejandro E. and Uzun, Ersin},
title = {Privacy Preserving Data Quality Assessment for High-Fidelity Data Sharing},
year = {2014},
isbn = {9781450331517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663876.2663885},
doi = {10.1145/2663876.2663885},
abstract = {In a data-driven economy that struggles to cope with the volume and diversity of information, data quality assessment has become a necessary precursor to data analytics. Real-world data often contains inconsistencies, conflicts and errors. Such dirty data increases processing costs and has a negative impact on analytics. Assessing the quality of a dataset is especially important when a party is considering acquisition of data held by an untrusted entity. In this scenario, it is necessary to consider privacy risks of the stakeholders.This paper examines challenges in privacy-preserving data quality assessment. A two-party scenario is considered, consisting of a client that wishes to test data quality and a server that holds the dataset. Privacy-preserving protocols are presented for testing important data quality metrics: completeness, consistency, uniqueness, timeliness and validity. For semi-honest parties, the protocols ensure that the client does not discover any information about the data other than the value of the quality metric. The server does not discover the parameters of the client's query, the specific attributes being tested and the computed value of the data quality metric. The proposed protocols employ additively homomorphic encryption in conjunction with condensed data representations such as counting hash tables and histograms, serving as efficient alternatives to solutions based on private set intersection.},
booktitle = {Proceedings of the 2014 ACM Workshop on Information Sharing &amp; Collaborative Security},
pages = {21–29},
numpages = {9},
keywords = {privacy and confidentiality, cryptographic protocols, data quality assessment},
location = {Scottsdale, Arizona, USA},
series = {WISCS '14}
}

@inproceedings{10.1145/3207677.3278079,
author = {Zhao, Zhuo and Li, Xingying and Li, Shanzi and Wu, Yixuan and Zhao, Xin},
title = {Towards the Big Data in Official Statistics: An Analytic Service Framework for Distributed Multiple Sourced Heterogeneous Datasets},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3278079},
doi = {10.1145/3207677.3278079},
abstract = {High volumes1 of business data is continuously produced by different kinds of information system, which provides big values for the official statistics. However, it is not easy to leverage big volume of business data for the statistical analysis under existing technologies, since they are generally distributed multiple sourced heterogeneous data set. In this paper, we first present the problem scenario and discuss in details the challenges confronting with the problem. Then, we propose an analytical framework for the distributed multiple sourced heterogeneous data set based on the service oriented architecture. Finally, we present a prototype of elementary statistical services for the primary data analysis tasks based on the proposed analytic service framework.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {32},
numpages = {5},
keywords = {Big data, web services, multiple sourced dataset, data mining, official statistics, distributed computing},
location = {Hohhot, China},
series = {CSAE '18}
}

@inproceedings{10.1109/BDC.2014.21,
author = {Kuehn, Eileen and Fischer, Max and Jung, Christopher and Petzold, Andreas and Streit, Achim},
title = {Monitoring Data Streams at Process Level in Scientific Big Data Batch Clusters},
year = {2014},
isbn = {9781479918973},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BDC.2014.21},
doi = {10.1109/BDC.2014.21},
abstract = {The operation of scientific big data centres requires an overall monitoring and perception of system components. Insights into internal and external network traffic is of high importance for understanding specific data flows regarding storage accesses, firewall configurations, and the scheduling of batch jobs on clusters for computing/analysis of data. However, wide adoptions of federated storage, the handling of numerous job on many-core nodes, or the execution of job pilots inside the batch system complicate current data stream monitoring attempts. Therefore, the rising complexity requires new approaches to extend available solutions. As existing batch system monitoring and related system monitoring tools do not support measurements at batch job level, a new tool has been developed and put into operation at the Grid Ka data and computing centre at KIT for monitoring continuous data streams. Obtained results can for example be used to realise an optimisation of LAN/WAN setups based on measured data flows to adapt to the actual needs. This paper describes the current approach being implemented at the Grid Ka batch cluster and presents first analysis results showing the significance of measurements. The described approach is consecutively applied to the context of computing for high-energy physics.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing},
pages = {90–95},
numpages = {6},
keywords = {network monitoring, performance measurements, data streams, big data, distributed systems, data analysis},
series = {BDC '14}
}

@inproceedings{10.1145/3139243.3139254,
author = {Rodrigues, Jo\~{a}o G. P. and Pereira, Jo\~{a}o P. and Aguiar, Ana},
title = {Impact of Crowdsourced Data Quality on Travel Pattern Estimation},
year = {2017},
isbn = {9781450355551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139243.3139254},
doi = {10.1145/3139243.3139254},
abstract = {Mobile crowdsensing can provide mobility researchers with fine grained spatio-temporal location data. But crowdsourcing impacts data quality both due to device and OS heterogeneity, and to annotation errors. Additionally, it is often necessary to deal with multimodality, i.e. participants using different travel modes often in the same trip. In this paper, we address how to draw value from a crowdsensed dataset for characterising mobility demand through origin-destination (OD) matrices, highlighting challenges and providing some solutions.First, we identify typical errors in heterogeneous location data, propose and compare methods to automatically improve data quality. Then, we devise a method to detect among 5 transport modes (walk, car, bus, metro, bike) offline a posteriori. We segment trips on stopped periods and propose a random forest model to detect transportation mode per segment using only location data. Our results show that with adequate pre-processing and robust features, an RF classifier is able to achieve accuracy and precision of 85% in trip segments. This is similar to the literature, but our work uses a very heterogeneous crowdsourced trajectory dataset when compared to the others. Finally, we quantify the impact of the model on mulit-modal OD matrices and whole trip characterisation. We can correctly identify used transportation modes accurately, but the precision is impaired by the high likelihood of at least one false positive in the whole trip.},
booktitle = {Proceedings of the First ACM Workshop on Mobile Crowdsensing Systems and Applications},
pages = {38–43},
numpages = {6},
keywords = {Transportation Mode Estimation, Data Mining, Crowdsensing},
location = {Delft, Netherlands},
series = {CrowdSenSys '17}
}

@inproceedings{10.1145/2905055.2905211,
author = {Saxena, Ankur and Kaushik, Neeraj and Kaushik, Nidhi},
title = {Implementing and Analyzing Big Data Techniques WithSpring Frame Work in Java&amp; J2EEBased Application},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905211},
doi = {10.1145/2905055.2905211},
abstract = {In the time of big data techniques with spring framework on java, web servers or application server as the significant channel in big data should be updated to meet execution and force imperatives. Significant endeavors have been put resources into web server or application server conveyance and web storing procedures, but very few efforts have been paid to improve hardware-favored web type services. Big Data with spring framework in java is a promising business and computing model in web framework. Spring is the most popular open source Java application Framework. It combines all the industry-standard frameworks (for e.g. Struts and Hibernate) and approaches into one bundle.The expense and working costs of data centers have skyrocketed with the increase in computing capacity.Big data is a concept that defines the large volume of both structured and unstructured data -- that inundates a business on a day-to-day environment. This research argues the need to provide novel method and tools to bolster programming engineers meaning to enhance vitality productivity and minimize the subsequent from outlining, creating, sending and running programming in Big Data with spring framework.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {5},
numpages = {6},
keywords = {Hadoop Framework, Big Data, spring, J2ee, Java},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3242840.3242842,
author = {Xu, Menghan and Huang, Gaopan and Zhang, Mingming and Cui, Peng and Wang, Chong},
title = {Load Forecasting Research Based on High Performance Intelligent Data Processing of Power Big Data},
year = {2018},
isbn = {9781450365093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242840.3242842},
doi = {10.1145/3242840.3242842},
abstract = {The method proposed in this paper is a data analysis method that intelligently analyzes power big data and realizes the load forecasting of power grid. The method calls for the corresponding data from each database of big data platform by accepting the load forecast request from the client, and performs the load forecasting in the big data by improving the gray model of chaos genetic algorithm (CGA). After the completion of load forecasting, the final output to the client load forecasting results.},
booktitle = {Proceedings of the 2018 2nd International Conference on Algorithms, Computing and Systems},
pages = {55–60},
numpages = {6},
keywords = {Power big data, power grid, chaos genetic algorithm (CGA), load forecasting},
location = {Beijing, China},
series = {ICACS '18}
}

@inproceedings{10.1145/3354153.3354156,
author = {Suwansrikham, Parinya and She, Kun},
title = {Protection of Big Data Privacy on Multiple Cloud Providers by Asymmetric Security Scheme},
year = {2019},
isbn = {9781450372169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3354153.3354156},
doi = {10.1145/3354153.3354156},
abstract = {Big data is the name that defines data which has enormous size and unstructured. Due to the file size is pretty huge. It is impracticable to store a large file in one storage volume. However, cloud computing is a solution to this impossible. Data owner can store the file in a cloud storage provider (CSP). Nevertheless, the new dilemma has arisen. Relying on single cloud storage may generate trouble for the customer. A CSP may stop its service anytime. Moreover, the CSP is the third party that user have to trust without verification. In that case, the privacy or unauthorized accessing of data may be violated without notice. To overcome this risk, we propose secure data storage scheme for big data storing on multiple CSPs. The one big data file is split into chunks and distributed to multiple cloud storage provider. After splitting the file, metadata is generated. Metadata is a place to keep chunks information, includes; chunk locations, access paths, username and password of the data owner, methods to connect each CSP. The metadata is encrypted and transferred to the user who requests to access the file. The user utilizes the metadata and chunks of the file to compose the original file. This method will minimize the risk of privacy. The goal of this paper is to provide the method to protect the privacy of data stored on multiple cloud storage providers. Furthermore, we discuss and analyze how this data storage scheme promote the protection of big data privacy.},
booktitle = {Proceedings of the 2019 2nd International Conference on Data Storage and Data Engineering},
pages = {47–53},
numpages = {7},
keywords = {Data Security and Privacy, Cloud Storage, Big Data},
location = {Jeju, Republic of Korea},
series = {DSDE 2019}
}

@inproceedings{10.5555/2486788.2486842,
author = {Shang, Weiyi and Jiang, Zhen Ming and Hemmati, Hadi and Adams, Bram and Hassan, Ahmed E. and Martin, Patrick},
title = {Assisting Developers of Big Data Analytics Applications When Deploying on Hadoop Clouds},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {402–411},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3365871.3365900,
author = {Papst, Franz and Saukh, Olga and R\"{o}mer, Kay and Grandl, Florian and Jakovljevic, Igor and Steininger, Franz and Mayerhofer, Martin and Duda, J\"{u}rgen and Egger-Danner, Christa},
title = {Embracing Opportunities of Livestock Big Data Integration with Privacy Constraints},
year = {2019},
isbn = {9781450372077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365871.3365900},
doi = {10.1145/3365871.3365900},
abstract = {Today's herd management undergoes a major transformation triggered by the penetration of cheap sensor solutions into cattle farms, and the promise of predictive analytics to detect animal health issues and product-related problems before they occur. The latter is particularly important to prevent disease spread, ensure animal health, animal welfare and product quality. Sensor businesses entering the market tend to build their solutions as end-to-end pipelines spanning sensors, proprietary algorithms, cloud services, and mobile apps. Since data privacy is an important issue in this industry, as a result, disconnected data silos, heterogeneity of APIs, and lack of common standards limit the value the sensor technologies could provide for herd management. In the last few years, researchers and communities proposed a number of data integration architectures to enable exchange between streams of sensor data. This paper surveys the existing efforts and outlines the opportunities they fail to address by treating sensor data as a black box. We discuss alternative solutions to the problem based on privacy-preserving collaborative learning, and provide a set of scenarios to show their benefits for both farmers and businesses.},
booktitle = {Proceedings of the 9th International Conference on the Internet of Things},
articleno = {27},
numpages = {4},
keywords = {agriculture, privacy-preserving data analysis, data privacy},
location = {Bilbao, Spain},
series = {IoT 2019}
}

@inproceedings{10.1145/1529282.1529334,
author = {H\"{u}ner, Kai M. and Ofner, Martin and Otto, Boris},
title = {Towards a Maturity Model for Corporate Data Quality Management},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529334},
doi = {10.1145/1529282.1529334},
abstract = {High-quality corporate data is a prerequisite for world-wide business process harmonization, global spend analysis, integrated service management, and compliance with regulatory and legal requirements. Corporate Data Quality Management (CDQM) describes the quality oriented organization and control of a company's key data assets such as material, customer, and vendor data. With regard to the aforementioned business drivers, companies demand an instrument to assess the progress and performance of their CDQM initiative. This paper proposes a reference model for CDQM maturity assessment. The model is intended to be used for supporting the build process of CDQM. A case study shows how the model has been successfully implemented in a real-world scenario.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {231–238},
numpages = {8},
keywords = {reference modeling, maturity models, corporate data quality, action research, design research, data quality management},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.1145/1458527.1458538,
author = {Askira Gelman, Irit and Barletta, Anthony L.},
title = {A "Quick and Dirty" Website Data Quality Indicator},
year = {2008},
isbn = {9781605582597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458527.1458538},
doi = {10.1145/1458527.1458538},
abstract = {This short paper outlines a research study in progress, which is motivated by the perception that the spelling error rate of a document can serve as a rudimentary proxy for the degree of quality control exercised in its creation, and, subsequently, indicate its quality. One objective of this research is to validate this understanding. Ultimately, the goal of this research is to take advantage of such an association. In particular, we propose a simple, "quick and dirty" metric for assisting in the evaluation of the quality of websites. This metric utilizes the reported hit counts of search engine queries on a pre-determined set of commonly misspelled words.},
booktitle = {Proceedings of the 2nd ACM Workshop on Information Credibility on the Web},
pages = {43–46},
numpages = {4},
keywords = {data quality indicator, information quality, data quality, indicator, quick and dirty, web data, information credibility},
location = {Napa Valley, California, USA},
series = {WICOW '08}
}

@inproceedings{10.1145/3017680.3022436,
author = {Deb, Debzani},
title = {On the Integration of Big Data and Cloud Computing Topics (Abstract Only)},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3022436},
doi = {10.1145/3017680.3022436},
abstract = {Big data and cloud computing (BDCloud) collectively offer a paradigm shift in the way businesses are now acquiring, using and managing information technology. With the fast growth of this paradigm, we argue that each and every CS and IT students should be equipped with foundation knowledge in this collective paradigm and should possess hand-on-experiences in managing big data applications in clouds to acquire skills that are necessary to meet current and future industry demands. This poster presents our research that proposes gradual and systematic integration of big data and cloud computing related topics into multiple core (required) courses of CS/IT curriculum. The poster, supported by a NSF grant, will be useful for CS/IT students and their instructors as it identifies big data and cloud computing related topics that are important to cover, finds a sequence of the prescribed topics that can be incorporated into existing core courses most effectively, and suggests specific core courses in which their coverage might find an appropriate context. The poster further identifies the major challenges this proposed intervention may encounter and provides a deeper analysis of them. Finally, the poster describes our experience of implementing one such course with proposed interventions during Fall of 2016 semester. The pre- post- test results that measure student opinion and understanding of big data and cloud computing topics are presented in the poster and demonstrate improved student interest and learning.},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {706},
numpages = {1},
keywords = {CS curriculum, big data, IT curriculum, cloud computing},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@proceedings{10.1145/2694730,
title = {PABS '15: Proceedings of the 1st Workshop on Performance Analysis of Big Data Systems},
year = {2015},
isbn = {9781450333382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2015 ACM Workshop on Performance Analysis of Big Data Systems -- PABS'15 in conjunction with ICPE2015.The main objective of the workshop is to discuss the performance challenges imposed by big data systems and the different state-of-the-art solutions proposed to overcome these challenges. The workshop aims at providing a platform for scientific researchers, academicians and practitioners to discuss techniques, models, benchmarks, tools and experiences while dealing with performance issues in big data systems.The program committee reviewed 4 and accepted 2 full technical papers with acceptance rate as 50%.We welcome attendees to attend the keynote, invited talk and paper presentations. These valuable and insightful talks can and will guide us to a better understanding of the future: Accelerating Big Data Processing on Modern Clusters, Prof. D.K. Panda (Ohio State University, USA)Experimentation as a Tool for the Performance Evaluation of Big Data Systems, Prof. Amy W. Apon (Clemson University, USA)},
location = {Austin, Texas, USA}
}

@inproceedings{10.1145/2993318.2993351,
author = {Meissner, Roy and Junghanns, Kurt},
title = {Using DevOps Principles to Continuously Monitor RDF Data Quality},
year = {2016},
isbn = {9781450347525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993318.2993351},
doi = {10.1145/2993318.2993351},
abstract = {One approach to continuously achieve a certain data quality level is to use an integration pipeline that continuously checks and monitors the quality of a data set according to defined metrics. This approach is inspired by Continuous Integration pipelines, that have been introduced in the area of software development and DevOps to perform continuous source code checks. By investigating in possible tools to use and discussing the specific requirements for RDF data sets, an integration pipeline is derived that joins current approaches of the areas of software-development and semantic-web as well as reuses existing tools. As these tools have not been built explicitly for CI usage, we evaluate their usability and propose possible workarounds and improvements. Furthermore, a real-world usage scenario is discussed, outlining the benefit of the usage of such a pipeline.},
booktitle = {Proceedings of the 12th International Conference on Semantic Systems},
pages = {189–192},
numpages = {4},
keywords = {Continuous Integration, Instant Feedback, Data Quality, DevOps, Data Integration, RDF, Quality Monitoring},
location = {Leipzig, Germany},
series = {SEMANTiCS 2016}
}

@inproceedings{10.1145/3297662.3365803,
author = {Al Chami, Zahi and Al Bouna, Bechara and Jaoude, Chady Abou and Chbeir, Richard},
title = {A Real-Time Multimedia Data Quality Assessment Framework},
year = {2020},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365803},
doi = {10.1145/3297662.3365803},
abstract = {Nowadays, social media runs a significant portion of people's daily lives. Millions of people use social media applications to share photos. The huge volume of images shared on social media presents serious challenges and requires large computational infrastructure to ensure successful data processing. However, image gets distorted somehow during the processing, transmission, sharing or from a combination of many factors. So, there is a need to guarantee an acceptable delivery content, especially for image processing applications. In this paper, we present a framework developed to process a large amount of images in real-time while estimating the image quality. Our quality evaluation is measured based on two methods: Perceptual Coherence Measure and Structural Similarity Index. A set of experiments is conducted to evaluate our proposed approach.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {270–276},
numpages = {7},
keywords = {real time data processing, image quality assessment, image functions adaptation},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1145/3321408.3322865,
author = {Zhao, Junmin and Li, Dengao and Wang, Xiaoyu and Bai, Xiaohong and Zhuang, Shasha},
title = {Research on the Cultivation of New Engineering Talents Based on Educational Big Data},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3322865},
doi = {10.1145/3321408.3322865},
abstract = {With the development and application of education and big data, big data continues to focus on classroom teaching and learning. In view of the normal application of the teaching platform and the demand of social talents, it is of practical significance to make an empirical study on the educational big data analysis based on the educational platform. The interaction of education platform can be described by "STM triangle model", including teacher-media interaction, student-media interaction, teacher-student interaction, student-teacher interaction, student-student interaction and data relationship. The data mining process of educational platform is divided into target understanding, data cleaning, data analysis, data presentation and other steps. Normal response to Education platform Based on the real data, an index analysis model of teacher-student interaction is constructed, and an empirical analysis is carried out to provide a practical example for the analysis and application of education big data.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {88},
numpages = {2},
keywords = {educational platform, education big data, data mining, STM mode},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/3297730.3297731,
author = {Chen, Rui-Yang},
title = {Target Data Optimization Based on Big Data-Streaming for Two-Stage Fuzzy Extraction System},
year = {2018},
isbn = {9781450365826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297730.3297731},
doi = {10.1145/3297730.3297731},
abstract = {How to extract target data effectively and intelligently is key point in the big data-streaming operation. Data extractions need focus on priority of data selection to reduce impact of 4Vs because of requirement of real-time computation. Corresponding big data-streaming for three-stage extraction system is presented in terms of hierarchal base and fuzzy representations. The proposed approach is based on a combination of clustering, classification and relationships method with fuzzy weighted similarity under hierarchical feature-based model. Moreover, heuristic fuzzy CBR-FDT- algorithms are provided to explore the target data optimization. Successful case study and experiment with simulations demonstrated the performance of the proposed approach.},
booktitle = {Proceedings of the 2018 International Conference on Big Data Engineering and Technology},
pages = {26–30},
numpages = {5},
keywords = {big data-streaming, fuzzy decision tree, Target data optimization, fuzzy case-based reasoning},
location = {Chengdu, China},
series = {BDET 2018}
}

@inproceedings{10.1145/3093241.3093265,
author = {Madani, Youness and Bengourram, Jemaa and Erritali, Mohammed},
title = {Social Login and Data Storage in the Big Data File System HDFS},
year = {2017},
isbn = {9781450352413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093241.3093265},
doi = {10.1145/3093241.3093265},
abstract = {Studies have shown that the registration forms on Websites are ineffective because many people give false data, forget their login information to the site or just refuse to register, to overcome these problems a new type of authentication is born is the social authentication or social login which is a type of SSO(Single Sign-On),due to this type of authentication enrollment increases to a platform because the user registered to the platform with a simple click of a button authentication without passing by the step of filling a form, choose a username and a secure password. In this article, we will study the social authentication how it works, and how after the authorization of the user we can Retrieve personal data to complete registration, we can also use its social authorization on our facebook application to register its data on HDFS in a Big data system to analyze them and personalize its member space in the platform, using the Hadoop framework based on the MapReduce programming.},
booktitle = {Proceedings of the International Conference on Compute and Data Analysis},
pages = {91–97},
numpages = {7},
keywords = {SSO(Single Sign-On), Hadoop, social authentication, Authentication, HDFS, Big Data, MapReduce, authorization},
location = {Lakeland, FL, USA},
series = {ICCDA '17}
}

@inproceedings{10.1145/2998476.2998498,
author = {Agarwal, Bhoomika and Ravikumar, Abhiram and Saha, Snehanshu},
title = {A Novel Approach to Big Data Veracity Using Crowdsourcing Techniques and Bayesian Predictors},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998498},
doi = {10.1145/2998476.2998498},
abstract = {In today's world data is being generated at a tremendous pace and there have to be enough measures in place to verify the nature of big data. Analysis performed on 'dirty' data may lead to erroneous insights and thereby shaping decisions poorly. The aspect of big data that deals with its correctness is known as big data veracity. Trusting the data acquired goes a long way in implementing decisions from an automated decision-making system and veracity helps to validate the data acquired. In this paper, we present our solution to the big data veracity problem using crowdsourcing techniques. Our solution involves the use of sentiment analysis, which deals with identifying the sentiment expressed in a piece of text. As a proof of concept, we have developed an app that requires users to tag tweets as per the sentiment it evokes in them. Each tweet would therefore get ratified by hundreds of our participants and the sentiment associated to the tweet gets tagged. The tagged emotion was then evaluated against the verified emotion as compared to a verified data set. This analysis was then plotted on a ROC curve and also evaluated against verified data using a Bayesian predictor trained with a trinomial function. As can be seen, an accuracy of 81% was obtained as displayed by the ROC curve and 89% through the Bayesian predictor. Also, a MAP analysis of the Bayesian predictor yields neutral sentiment as the most probable hypothesis. By doing this, we have proven that crowdsourcing of sentiment analysis is a viable solution to the problem of big data veracity and therefore an aid in making better decisions.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {153–160},
numpages = {8},
keywords = {Sentiment Analysis, Tweet Mining, Big Data, Crowdsourcing, Machine Learning, Bayesian Predictor},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.5555/1315451.1315499,
author = {Luebbers, Dominik and Grimmer, Udo and Jarke, Matthias},
title = {Systematic Development of Data Mining-Based Data Quality Tools},
year = {2003},
isbn = {0127224424},
publisher = {VLDB Endowment},
abstract = {Data quality problems have been a persistent concern especially for large historically grown databases. If maintained over long periods, interpretation and usage of their schemas often shifts. Therefore, traditional data scrubbing techniques based on existing schema and integrity constraint documentation are hardly applicable. So-called data auditing environments circumvent this problem by using machine learning techniques in order to induce semantically meaningful structures from the actual data, and then classifying outliers that do not fit the induced schema as potential errors.However, as the quality of the analyzed database is a-priori unknown, the design of data auditing environments requires special methods for the calibration of error measurements based on the induced schema. In this paper, we present a data audit test generator that systematically generates and pollutes artificial benchmark databases for this purpose. The test generator has been implemented as part of a data auditing environment based on the well-known machine learning algorithm C4.5.Validation in the partial quality audit of a large service-related database at Daimler-Chrysler shows the usefulness of the approach as a complement to standard data scrubbing.},
booktitle = {Proceedings of the 29th International Conference on Very Large Data Bases - Volume 29},
pages = {548–559},
numpages = {12},
location = {Berlin, Germany},
series = {VLDB '03}
}

@inproceedings{10.1145/3297156.3297233,
author = {Fuzong, Wang and Helin, Guo and Jian, Zhao},
title = {Dynamic Data Compression Algorithm Selection for Big Data Processing on Local File System},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297233},
doi = {10.1145/3297156.3297233},
abstract = {Data Compression has become a commodity feature for space efficiency and performance by reducing reading and writing traffic and space capacity demand. This technology is particularly valuable for a file system to manage and server the big data processing tasks. However, the fixed data compression scheme cannot fit all the big data workloads and data-set which have complex internal data structure and compressibility. This paper investigates a dynamic and smart data compression algorithm selection scheme for different big data processing cases in the local file system. To this end, we propose a dynamic algorithm selection module in the Linux ZFS which is an open source file system. This module will select a high compression ratio algorithm for high compressibility data, and select a fast compression algorithm for low compressibility data, and skip all data compression process for incompressibility data. The comprehensive evaluations validate that dynamic algorithm selection module can achieve up to 2.69x response time improvement for reading and writing operation in file system and reduce about 32.12% storage space for a large amount data-set.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {110–114},
numpages = {5},
keywords = {Big Data, Compression Algorithm, File system, Data Compression},
location = {Shenzhen, China},
series = {CSAI '18}
}

@inproceedings{10.1145/3342827.3342843,
author = {Shen, Zhengru and Wang, Xi and Spruit, Marco},
title = {Big Data Framework for Scalable and Efficient Biomedical Literature Mining in the Cloud},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342843},
doi = {10.1145/3342827.3342843},
abstract = {The massive size of available biomedical literature requires researchers to utilize novel big data technologies in data storage and analysis. Among them is cloud computing which has become the most popular solution for big data applications in industry. However, many bioinformaticians still rely on expensive and inefficient in-house infrastructure to discover knowledge from biomedical literature. Although some cloud-based solutions were constructed recently, they failed to sufficiently address a few key issues including scalability, flexibility, and reusability. Moreover, no study has taken computational cost into consideration. To fill the gap, we proposed a cloud-based big data framework that enables researchers to perform reproducible and scalable large-scale biomedical literature mining in an efficient and cost-effective way. Additionally, a cloud agnostic platform was constructed and then evaluated on two open access corpora with millions of full-text biomedical articles. The results indicate that our framework supports scalable and efficient large-scale biomedical literature mining.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {80–86},
numpages = {7},
keywords = {biomedical literature, cloud computing, topic modeling, big data, text mining, document classification},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.5555/2648668.2648699,
author = {Li, Yan and Wang, Kun and Guo, Qi and Li, Xin and Zhang, Xiaochen and Chen, Guancheng and Liu, Tao and Li, Jian},
title = {Breaking the Boundary for Whole-System Performance Optimization of Big Data},
year = {2013},
isbn = {9781479912353},
publisher = {IEEE Press},
abstract = {MapReduce plays an critical role in finding insights in Big Data. The performance optimization of MapReduce programs is challenging because it requires a comprehensive understanding of the whole system including both hardware layers (processors, storages, networks and etc), and software stacks (operating systems, JVM, runtime, applications and etc). However, most of the existing performance tuning and optimization are based on empirical and heuristic attempts. It remains a blank on how to build a systematical framework which breaks the boundary of multiple layers for performance optimization.In this paper, we propose a performance evaluation framework by correlating performance metrics from different layers, which provides insights to efficiently pinpoint the performance issue. This framework is composed of a series of predefined patterns. Each pattern indicates one or more potential issues. The behavior of a MapReduce program is mapped to the corresponding resource utilization. The framework provides a holistic approach which allows users at different levels of experience to conduct MapReduce program performance optimization.We use Terasort benchmark running on a 10-node Power7R2 cluster as a real case to show how this framework improves the performance. By this framework, we finally get the Terasort result improved from 47 mins to less than 8 mins. In addition to the best practice on performance tuning, several key findings are summarized as valuable workload analysis for JVM, MapReduce runtime and application design.},
booktitle = {Proceedings of the 2013 International Symposium on Low Power Electronics and Design},
pages = {126–131},
numpages = {6},
location = {Beijing, China},
series = {ISLPED '13}
}

@inproceedings{10.1145/3350546.3352507,
author = {Tantalaki, Nicoleta and Souravlas, Stavros and Roumeliotis, Manos and Katsavounis, Stefanos},
title = {Linear Scheduling of Big Data Streams on Multiprocessor Sets in the Cloud},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352507},
doi = {10.1145/3350546.3352507},
abstract = {Nowadays, there is an accelerating need to efficiently and timely handle large amounts of data that arrives continuously. Streams of big data led to the emergence of Distributed Stream Processing Systems (DSPS) that assign processing tasks to the available resources (dynamically or not) and route streaming data between them. Efficient scheduling of processing tasks of data flows can reduce application latencies and eliminate network congestion. In this work, we propose a linear complexity scheme for the task allocation and scheduling problem to improve system’s performance, load balancing and memory efficiency, in applications where there is need for heavy communication (all-to-all) between the tasks assigned to pairs of components.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {107–115},
numpages = {9},
keywords = {Scheduling, Big Data, Stream Processing, Apache Storm, Distributed Systems},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@inproceedings{10.1145/3097983.3097999,
author = {Cohen, Reuven and Katzir, Liran and Yehezkel, Aviv},
title = {A Minimal Variance Estimator for the Cardinality of Big Data Set Intersection},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3097999},
doi = {10.1145/3097983.3097999},
abstract = {In recent years there has been a growing interest in developing "streaming algorithms" for efficient processing and querying of continuous data streams. These algorithms seek to provide accurate results while minimizing the required storage and the processing time, at the price of a small inaccuracy in their output. A fundamental query of interest is the intersection size of two big data streams. This problem arises in many different application areas, such as network monitoring, database systems, data integration and information retrieval. In this paper we develop a new algorithm for this problem, based on the Maximum Likelihood (ML) method. We show that this algorithm outperforms all known schemes in terms of the estimation's quality (lower variance) and that it asymptotically achieves the optimal variance.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {95–103},
numpages = {9},
keywords = {set intersection, cardinality estimation, streaming algorithms, data mining},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@article{10.1145/2500468.2500471,
author = {Stonebraker, Michael and Robertson, Judy},
title = {Big Data is 'buzzword Du Jour;' CS Academics 'Have the Best Job'},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/2500468.2500471},
doi = {10.1145/2500468.2500471},
abstract = {The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.twitterFollow us on Twitter at http://twitter.com/blogCACMhttp://cacm.acm.org/blogs/blog-cacmMichael Stonebraker analyzes the different varieties of Big Data, while Judy Robertson considers the rewards of teaching computer science.},
journal = {Commun. ACM},
month = {sep},
pages = {10–11},
numpages = {2}
}

@inproceedings{10.1145/3358505.3358512,
author = {Demchenko, Yuri},
title = {Big Data Platforms and Tools for Data Analytics in the Data Science Engineering Curriculum},
year = {2019},
isbn = {9781450371650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358505.3358512},
doi = {10.1145/3358505.3358512},
abstract = {This paper presents experiences of development and teaching courses on Big Data Infrastructure Technologies for Data Analytics (BDIT4DA) as a part of the general Data Science curricula. The authors built the discussed course based on the EDISON Data Science Framework (EDSF), in particular, Data Science Body of Knowledge (DS-BoK) related to Data Science Engineering knowledge area group (KAG-DSENG). The paper provides overview of the cloud based platforms and tools for Big Data Analytics and stresses importance of including into curriculum the practical work with clouds for future graduates or specialists workplace adaptability. The paper discusses a relationship between the DSENG BoK and Big Data technologies and platforms, in particular Hadoop based applications and tools for data analytics that should be promoted through all course activities: lectures, practical activities and self-study.},
booktitle = {Proceedings of the 2019 3rd International Conference on Cloud and Big Data Computing},
pages = {60–64},
numpages = {5},
keywords = {Cloud Computing, Big Data Infrastructure Technologies, Data Science Engineering, Data Science Body of Knowledge (DS-BoK), Hadoop ecosystem, EDISON Data Science Framework (EDSF)},
location = {Oxford, United Kingdom},
series = {ICCBDC 2019}
}

@inproceedings{10.1145/3007818.3007824,
author = {Cuzzocrea, Alfredo},
title = {Big Data Compression Paradigms for Supporting Efficient and Scalable Data-Intensive IoT Frameworks},
year = {2016},
isbn = {9781450347549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3007818.3007824},
doi = {10.1145/3007818.3007824},
abstract = {In this paper we focus on big data compression paradigms within reference data-intensive IoT frameworks, which are currently recognized as one of the emerging scientific in a rich interdisciplinary field that comprises service-oriented infrastructures, Cloud computing, big data management and analytics. Basically, big data compression techniques allow to tame the complexity of big data management tasks within such frameworks, hence beneficially influencing all the other activities, perhaps delivered as services in a reference Cloud architecture. Inspired by these considerations, in this paper we provide an overview on noticeable state-of-the-art big data compression techniques, and depict future research directions on the investigated scientific topic to be considered during future years.},
booktitle = {Proceedings of the Sixth International Conference on Emerging Databases: Technologies, Applications, and Theory},
pages = {67–71},
numpages = {5},
location = {Jeju, Republic of Korea},
series = {EDB '16}
}

@inproceedings{10.1145/3254549,
author = {Kalyanaraman, Ananth},
title = {Session Details: Session 6: Big Data in Bioinformatics I},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254549},
doi = {10.1145/3254549},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1145/2905055.2905124,
author = {Khodke, Priti and Lawange, Saurabh and Bhagat, Amol and Dongre, Kiran and Ingole, Chetan},
title = {Query Processing over Large RDF Using SPARQL in Big Data},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905124},
doi = {10.1145/2905055.2905124},
abstract = {Internet search is done by exploring the link graph and keyword frequency. In 2012, Google released "Knowledge Graph" --Semantic Web. The human reasoning can be enhanced by the use semantic web an emerging area. Most of the current applications link open data views due to which there is huge flow of data in semantic web, particularly Resource Description Framework (RDF) data. In the semantic web research community this leads to design and development of scalable data processing techniques for RDF data. The aim of semantic web is to make available semantically connected data across the globe. This is a review paper giving analysis of techniques implemented to achieve the aim of semantic web, various approaches to processes RDF data. Within the semantic web community, RDF is a common acronym because it forms one of the basic building blocks for forming the web of semantic data, called a "graph database". This paper compares various methodologies followed by different researchers along with the results analysis of implemented techniques over different datasets.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {66},
numpages = {6},
keywords = {SPARQL, Hadoop, Semantic Web, MapReduce, Big Data, RDF Graph},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3323878.3325807,
author = {Holubov\'{a}, Irena and Scherzinger, Stefanie},
title = {Unlocking the Potential of NextGen Multi-Model Databases for Semantic Big Data Projects},
year = {2019},
isbn = {9781450367660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323878.3325807},
doi = {10.1145/3323878.3325807},
abstract = {A new vision in semantic big data processing is to create enterprise data hubs, with a 360° view on all data that matters to a corporation. As we discuss in this paper, a new generation of multi-model database systems seems a promising architectural choice for building such scalable, non-native triple stores. In this paper, we first characterize this new generation of multi-model databases. Then, discussing an example scenario, we show how they allow for agile and flexible schema management, spanning a large design space for creative and incremental data modelling. We identify the challenge of generating sound triple-views from data stored in several, interlinked models, for SPARQL querying. We regard this as one of several appealing research challenges where the semantic big data and the database architecture community may join forces.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {6},
numpages = {6},
keywords = {multi-model DBMS, semantic data management, schema evolution},
location = {Amsterdam, Netherlands},
series = {SBD '19}
}

@inproceedings{10.1145/2902961.2902984,
author = {Kulkarni, Amey and Abtahi, Tahmid and Smith, Emily and Mohsenin, Tinoosh},
title = {Low Energy Sketching Engines on Many-Core Platform for Big Data Acceleration},
year = {2016},
isbn = {9781450342742},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2902961.2902984},
doi = {10.1145/2902961.2902984},
abstract = {Almost 90% of the data available today was created within the last couple of years, thus Big Data set processing is of utmost importance. Many solutions have been investigated to increase processing speed and memory capacity, however I/O bottleneck is still a critical issue. To tackle this issue we adopt Sketching technique to reduce data communications. Reconstruction of the sketched matrix is performed using Orthogonal Matching Pursuit (OMP). Additionally we propose Gradient Descent OMP (GD-OMP) algorithm to reduce hardware complexity. Big data processing at real-time imposes rigid constraints on sketching kernel, hence to further reduce hardware overhead both algorithms are implemented on a low power domain specific many-core platform called Power Efficient Nano Clusters (PENC). GD-OMP algorithm is evaluated for image reconstruction accuracy and the PENC many-core architecture. Implementation results show that for large matrix sizes GD-OMP algorithm is 1.3x faster and consumes 1.4x less energy than OMP algorithm implementations. Compared to GPU and Quad-Core CPU implementations the PENC many-core reconstructs 5.4x and 9.8x faster respectively for large signal sizes with higher sparsity.},
booktitle = {Proceedings of the 26th Edition on Great Lakes Symposium on VLSI},
pages = {57–62},
numpages = {6},
keywords = {many-core, OMP, high performance and reconfigurable architecture, compressive sensing},
location = {Boston, Massachusetts, USA},
series = {GLSVLSI '16}
}

@inproceedings{10.1145/3209582.3209599,
author = {Gong, Xiaowen and Shroff, Ness},
title = {Incentivizing Truthful Data Quality for Quality-Aware Mobile Data Crowdsourcing},
year = {2018},
isbn = {9781450357708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209582.3209599},
doi = {10.1145/3209582.3209599},
abstract = {Mobile data crowdsourcing has found a broad range of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the "wisdom" of a potentially large crowd of "workers" (i.e., mobile users). A key metric of crowdsourcing is data accuracy, which relies on the quality of the participating workers' data (e.g., the probability that the data is equal to the ground truth). However, the data quality of a worker can be its own private information (which the worker learns, e.g., based on its location) that it may have incentive to misreport, which can in turn mislead the crowdsourcing requester about the accuracy of the data. This issue is further complicated by the fact that the worker can also manipulate its effort made in the crowdsourcing task and the data reported to the requester, which can also mislead the requester. In this paper, we devise truthful crowdsourcing mechanisms for Quality, Effort, and Data Elicitation (QEDE), which incentivize strategic workers to truthfully report their private worker quality and data to the requester, and make truthful effort as desired by the requester. The truthful design of the QEDE mechanisms overcomes the lack of ground truth and the coupling in the joint elicitation of worker quality, effort, and data. Under the QEDE mechanisms, we characterize the socially optimal and the requester's optimal task assignments, and analyze their performance. We show that the requester's optimal assignment is determined by the largest "virtual valuation" rather than the highest quality among workers, which depends on the worker's quality and the quality's distribution. We evaluate the QEDE mechanisms using simulations which demonstrate the truthfulness of the mechanisms and the performance of the optimal task assignments.},
booktitle = {Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {161–170},
numpages = {10},
keywords = {incentive mechanism, Mobile data crowdsourcing, data quality},
location = {Los Angeles, CA, USA},
series = {Mobihoc '18}
}

@inproceedings{10.1145/2928294.2928295,
author = {Lee, Sangkeun and Chinthavali, Supriya and Duan, Sisi and Shankar, Mallikarjun},
title = {Utilizing Semantic Big Data for Realizing a National-Scale Infrastructure Vulnerability Analysis System},
year = {2016},
isbn = {9781450342995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2928294.2928295},
doi = {10.1145/2928294.2928295},
abstract = {Critical Infrastructure systems (CIs) such as energy, water, transportation, and communication are highly interconnected and mutually dependent in complex ways. Robust modeling of CIs' interconnections is crucial to identify vulnerabilities in the CIs. We present a vision of national-scale Infrastructure Vulnerability Analysis System (IVAS) leveraging Semantic Big Data (SBD) tools, Big Data, and Geographical Information Systems (GIS) tools. We first survey existing approaches on vulnerability analysis of critical infrastructures and discuss relevant systems and tools aligned with our vision. Next, we present a generic system architecture and discuss challenges including: (1) Constructing and managing a CI network-of-networks graph, (2) Performing analytic operations at scale, and (3) Interactive visualization of analytic output to generate meaningful insights. We argue that this architecture acts as a baseline to realize a national-scale network based vulnerability analysis system.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {3},
numpages = {6},
keywords = {critical infrastructure network, vulnerability analysis, graph analysis, interdependency, large-scale, big data},
location = {San Francisco, California},
series = {SBD '16}
}

@inproceedings{10.1145/2875475.2875489,
author = {Landwehr, Carl E.},
title = {How Can We Enable Privacy in an Age of Big Data Analytics?},
year = {2016},
isbn = {9781450340779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875475.2875489},
doi = {10.1145/2875475.2875489},
abstract = {Even though some seem to think privacy is dead, we are all still wearing clothes, as Bruce Schneier observed at a recent conference on surveillance[1]. Yet big data and big data analytics are leaving some of us feeling a bit more naked than before. This talk will provide some personal observations on privacy today and then outline some research areas where progress is needed to enable society to gain the benefits of analyzing large datasets without giving up more privacy than necessary. Not since the early 1970s, when computing pioneer Willis Ware chaired the committee that produced the initial Fair Information Practice Principles [2] has privacy been so much in the U.S. public eye. Snowden's revelations, as well as a growing awareness that merely living our lives seems to generate an expanding "digital exhaust." Have triggered many workshops and meetings. A national strategy for privacy research is in preparation by a Federal interagency group. The ability to analyze large datasets rapidly and to extract commercially useful insights from them is spawning new industries. Must this industrial growth come at the cost of substantial privacy intrusions?},
booktitle = {Proceedings of the 2016 ACM on International Workshop on Security And Privacy Analytics},
pages = {47},
numpages = {1},
keywords = {privacy, surveillance, fair information practice principles},
location = {New Orleans, Louisiana, USA},
series = {IWSPA '16}
}

@article{10.1007/s00778-019-00557-w,
author = {Giatrakos, Nikos and Alevizos, Elias and Artikis, Alexander and Deligiannakis, Antonios and Garofalakis, Minos},
title = {Complex Event Recognition in the Big Data Era: A Survey},
year = {2019},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-019-00557-w},
doi = {10.1007/s00778-019-00557-w},
abstract = {The concept of event processing is established as a generic computational paradigm in various application fields. Events report on state changes of a system and its environment. Complex event recognition (CER) refers to the identification of composite events of interest, which are collections of simple, derived events that satisfy some pattern, thereby providing the opportunity for reactive and proactive measures. Examples include the recognition of anomalies in maritime surveillance, electronic fraud, cardiac arrhythmias and epidemic spread. This survey elaborates on the whole pipeline from the time CER queries are expressed in the most prominent languages, to algorithmic toolkits for scaling-out CER to clustered and geo-distributed architectural settings. We also highlight future research directions.},
journal = {The VLDB Journal},
month = {jul},
pages = {313–352},
numpages = {40},
keywords = {Distributed processing, Complex event recognition languages, Elasticity, Complex event recognition, Parallelism, Big Data}
}

@inproceedings{10.1145/3128572.3140452,
author = {Caliskan, Aylin},
title = {Beyond Big Data: What Can We Learn from AI Models? Invited Keynote},
year = {2017},
isbn = {9781450352024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128572.3140452},
doi = {10.1145/3128572.3140452},
abstract = {My research involves the heavy use of machine learning and natural language processing in novel ways to interpret big data, develop privacy and security attacks, and gain insights about humans and society through these methods. I do not use machine learning only as a tool but I also analyze machine learning models? internal representations to investigate how the artificial intelligence perceives the world. This work [3] has been recently featured in Science where I showed that societal bias exists at the construct level of machine learning models, namely semantic space word embeddings which are dictionaries for machines to understand language. When I use machine learning as a tool to uncover privacy and security problems, I characterize and quantify human behavior in language, including programming languages, by coming up with a linguistic fingerprint for each individual. By extracting linguistic features from natural language or programming language texts of humans, I show that humans have unique linguistic fingerprints since they all learn language on an individual basis. Based on this finding, I can de-anonymize humans that have written certain text, source code, or even executable binaries of compiled code [2, 4, 5]. This is a serious privacy threat for individuals that would like to remain anonymous, such as activists, programmers in oppressed regimes, or malware authors. Nevertheless, being able to identify authors of malicious code enhances security. On the other hand, identifying authors can be used to resolve copyright disputes or detect plagiarism. The methods in this realm [1] have been used to identify so called doppelg\"{a}ngers to link the accounts that belong to the same identities across platforms, especially underground forums that are business platforms for cyber criminals. By analyzing machine learning models? internal representation and linguistic human fingerprints, I am able to uncover facts about the world, society, and the use of language, which have implications for privacy, security, and fairness in machine learning.},
booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
pages = {1},
numpages = {1},
keywords = {invited keynote},
location = {Dallas, Texas, USA},
series = {AISec '17}
}

@inproceedings{10.1145/3093338.3104145,
author = {Tahmassebi, Amirhessam and Gandomi, Amir H. and Meyer-B\"{a}se, Anke},
title = {High Performance GP-Based Approach for FMRI Big Data Classification},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3104145},
doi = {10.1145/3093338.3104145},
abstract = {We consider resting-state Functional Magnetic Resonance Imaging (fMRI) of two classes of patients: one that took the drug N-acetylcysteine (NAC) and the other one a placebo before and after a smoking cessation treatment. Our goal was to classify the relapse in nicotine-dependent patients as treatment or non-treatment based on their fMRI scans. 80% accuracy was obtained using Independent Component Analysis (ICA) along with Genetic Programming (GP) classifier using High Performance Computing (HPC) which we consider significant enough to suggest that there is a difference in the resting-state fMRI images of a smoker that undergoes this smoking cessation treatment compared to a smoker that receives a placebo.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {57},
numpages = {4},
keywords = {High Performance Computing, Classification, Generic Programming, fMRI Big Data},
location = {New Orleans, LA, USA},
series = {PEARC17}
}

@inproceedings{10.1145/3396452.3396462,
author = {Lv, Xin-ye},
title = {Big Data in Ideological and Political Education in Colleges and Universities Application and Reflection},
year = {2020},
isbn = {9781450374989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396452.3396462},
doi = {10.1145/3396452.3396462},
abstract = {Along with the era of big data are new opportunities and challenges for the ideological and political education in China. It is an important issue in institutions of higher education in China to aid ideological and political education in colleges and universities by making effective use of big data technology and thinking. The application of big data aids ideological and political education in colleges and universities by improving its relevance, empowering it with computing capability, and providing it with think tank support. At the same time, big data also plays an increasingly important role in and has a significant influence on the specific practice of ideological and political education in colleges and universities.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Big Data and Education},
pages = {26–29},
numpages = {4},
keywords = {application of big data, colleges and universities, ideological and political education work},
location = {London, United Kingdom},
series = {ICBDE '20}
}

@inproceedings{10.1145/3394486.3406477,
author = {Jain, Abhinav and Patel, Hima and Nagalapatti, Lokesh and Gupta, Nitin and Mehta, Sameep and Guttula, Shanmukha and Mujumdar, Shashank and Afzal, Shazia and Sharma Mittal, Ruhi and Munigala, Vitobha},
title = {Overview and Importance of Data Quality for Machine Learning Tasks},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406477},
doi = {10.1145/3394486.3406477},
abstract = {It is well understood from literature that the performance of a machine learning (ML) model is upper bounded by the quality of the data. While researchers and practitioners have focused on improving the quality of models (such as neural architecture search and automated feature selection), there are limited efforts towards improving the data quality. One of the crucial requirements before consuming datasets for any application is to understand the dataset at hand and failure to do so can result in inaccurate analytics and unreliable decisions. Assessing the quality of the data across intelligently designed metrics and developing corresponding transformation operations to address the quality gaps helps to reduce the effort of a data scientist for iterative debugging of the ML pipeline to improve model performance. This tutorial highlights the importance of analysing data quality in terms of its value for machine learning applications. This tutorial surveys all the important data quality related approaches discussed in literature, focusing on the intuition behind them, highlighting their strengths and similarities, and illustrates their applicability to real-world problems. Finally we will discuss the interesting work IBM Research is doing in this space.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3561–3562},
numpages = {2},
keywords = {quality metrics, machine learning, data quality},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

