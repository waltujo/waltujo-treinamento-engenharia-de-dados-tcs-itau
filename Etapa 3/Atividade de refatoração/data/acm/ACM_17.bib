@inproceedings{10.1145/3134271.3134296,title = {Establishment of Business Intelligence and Big Data Analysis for Higher Education}, author = {Peng Michael Yao-Ping , Tuan Sheng-Hwa , Liu Feng-Chi },year = {2017}, isbn = {9781450352765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3134271.3134296}, doi = {10.1145/3134271.3134296}, abstract = {The applications on business intelligence and big data analysis to extract useful information are getting more mature, but the development and operations in higher education institutions are still be lack. This study aims to explore how HEIs employ business intelligence to analysis and mining student learning and HEIs' operation data from database. The outcomes are benefit for universities to support the management of decision-making.}, location = {Bei Jing, China}, series = {ICBIM 2017}, pages = {121\u2013125}, numpages = {5}, keywords = {Big data, Institutional Research, Business Intelligence, Database}}
@inproceedings{10.1145/3178212.3178229,title = {Real-time Analysis and Visualization for Big Data of Energy Consumption}, author = {Li Jiaxue , Song Wei , Fong Simon },year = {2017}, isbn = {9781450354882}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3178212.3178229}, doi = {10.1145/3178212.3178229}, abstract = {This paper proposes a research on real-time analysis and visualization for big data of energy consumption. In this research, we access real-time energy consumption data from cloud storage by a Transmission Control Protocol/Internet Protocol (TCP/IP). In order to optimize K-Means clustering algorithm, we implement CUDA C programming to finish data-intensive calculation in the Graphic Processing Unit (GPU), which enhances the efficiency of analysis for big data of energy consumption. Meanwhile, to realize data visualization, we draw the data mining results in a multidimensional plane utilizing DirectX, which is a standard graphics API. We also render the original energy consumption data directly in the form of four-dimensional geometry with the plane together, so as to obtain more useful information intuitively.}, location = {Hong Kong, Hong Kong}, series = {ICSEB 2017}, pages = {13\u201316}, numpages = {4}, keywords = {DirectX, K-Means, energy consumption, CUDA, big data}}
@inproceedings{10.1145/2448917.2448923,title = {Big data, situated people: humane approaches to communication design}, author = {McNely Brian },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2448917.2448923}, doi = {10.1145/2448917.2448923}, abstract = {In his 2005 book Ambient Findability, Peter Morville argued that what we find changes who we become. In 2012 and beyond---in an information environment of filter bubbles, contextual advertising, and friend-of-friend chains that push ordinary folks well beyond the Dunbar number---perhaps Morville is in need of some updating: what finds us changes who we become.}, pages = {27\u201330}, numpages = {4}}
@inproceedings{10.1145/3510003.3510619,title = {Big data = big insights? operationalising brooks' law in a massive GitHub data set}, author = {Gote Christoph , Mavrodiev Pavlin , Schweitzer Frank , Scholtes Ingo },year = {2022}, isbn = {9781450392211}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510003.3510619}, doi = {10.1145/3510003.3510619}, abstract = {Massive data from software repositories and collaboration tools are widely used to study social aspects in software development. One question that several recent works have addressed is how a software project's size and structure influence team productivity, a question famously considered in Brooks' law. Recent studies using massive repository data suggest that developers in larger teams tend to be less productive than smaller teams. Despite using similar methods and data, other studies argue for a positive linear or even super-linear relationship between team size and productivity, thus contesting the view of software economics that software projects are diseconomies of scale.In our work, we study challenges that can explain the disagreement between recent studies of developer productivity in massive repository data. We further provide, to the best of our knowledge, the largest, curated corpus of GitHub projects tailored to investigate the influence of team size and collaboration patterns on individual and collective productivity. Our work contributes to the ongoing discussion on the choice of productivity metrics in the operationalisation of hypotheses about determinants of successful software projects. It further highlights general pitfalls in big data analysis and shows that the use of bigger data sets does not automatically lead to more reliable insights.}, location = {Pittsburgh, Pennsylvania}, series = {ICSE '22}, pages = {262\u2013273}, numpages = {12}}
@inproceedings{10.1145/3484377.3484388,title = {Mapping the Intellectual Structure of Cloud Computing, Big Data and Healthcare Research}, author = {Kuo Jen-hwa , B. J. Kuo Terry , C. H. Yang Cheryl },year = {2021}, isbn = {9781450385909}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3484377.3484388}, doi = {10.1145/3484377.3484388}, abstract = {In order to explore the knowledge structure relevance of cloud computing, big data and healthcare research in the past ten years, this study identified the most important publications and the most influential papers, countries, research institutions, and the relationship between these scholars\u2019 publications. While analyzing the development and influence of the main prominent keywords. In this study, bibliometrics and social network analysis techniques were used to investigate the knowledge pillars of cloud computing, big data, and healthcare literature. This research draws a knowledge network of research by analyzing the citations of 2,358 articles in the field of cloud computing, big data, and healthcare published in SCI and SSCI journals from July 2011 to June 2021. The mapping results help determine the research direction of cloud computing, big data and healthcare research, and provide valuable knowledge and information for researchers to obtain literature in this field.}, location = {Macau, China}, series = {ICIMH 2021}, pages = {64\u201370}, numpages = {7}, keywords = {healthcare, bibliometrics, cloud computing, big data}}
@inproceedings{10.1145/2949550.2949651,title = {Introducing a New Client/Server Framework for Big Data Analytics with the R Language}, author = {Schmidt Drew , Chen Wei-Chen , Ostrouchov George },year = {2016}, isbn = {9781450347556}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2949550.2949651}, doi = {10.1145/2949550.2949651}, abstract = {Historically, large scale computing and interactivity have been at odds. This is a particularly sore spot for data analytics applications, which are typically interactive in nature. To help address this problem, we introduce a new client/server framework for the R language. This framework allows the R programmer to remotely control anywhere from one to thousands of batch servers running as cooperating instances of R. And all of this is done from the user's local R session. Additionally, no specialized software environment is needed; the framework is a series of R packages, available from CRAN. The communication between client and server(s) is handled by the well-known ZeroMQ library. To handle server side computations, we use our established pbdR packages for large scale distributed computing. These packages utilize HPC standards like MPI and ScaLAPACK to handle complex, tightly-coupled computations on large datasets. In this paper, we outline the new client/server architecture components, discuss the pros and cons to this approach, and provide several example workflows that bring interactivity to potentially terabyte size computations.}, location = {Miami, USA}, series = {XSEDE16}, pages = {1\u20139}, numpages = {9}, keywords = {Big Data, R, Remote Computing, Analytics}}
@inproceedings{10.1145/3011141.3011185,title = {Hybrid large-scale ontology matching strategy on big data environment}, author = {Mountasser Imadeddine , Ouhbi Brahim , Frikh Bouchra },year = {2016}, isbn = {9781450348072}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3011141.3011185}, doi = {10.1145/3011141.3011185}, abstract = {Ontology matching is one of the essential methodologies to overcome heterogeneity issues. Multiple knowledge-based and information systems perform ontology matching strategies to find correspondences between several ontologies for the purpose of discovering valuable information across various domains. The design and implementation of matching systems raises several challenges, especially, the matching accuracy and the performance issues. Accordingly, adapting the system to the requirements of Big Data era brings additional perspectives and challenges. Furthermore, to provide on-the-fly matching and in-time processing, the system must handle matching accuracy, runtime complexity and performance issues as an entire matching strategy. To this end, this paper presents a new hybrid ontology matching approach that benefit on one hand from the opportunities offered by parallel platforms, and on the other hand from ontology matching techniques, while applying a resource-based decomposition to improve the performance of the system.}, location = {Singapore, Singapore}, series = {iiWAS '16}, pages = {282\u2013287}, numpages = {6}, keywords = {big data, parallel platforms, heterogeneity resolution, parallel matching, large-scale ontology matching}}
@inproceedings{10.1145/3230833.3232816,title = {New authentication concept using certificates for big data analytic tools}, author = {Velthuis Paul J. E. , Sch\u00e4fer Marcel , Steinebach Martin },year = {2018}, isbn = {9781450364485}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3230833.3232816}, doi = {10.1145/3230833.3232816}, abstract = {Companies analyse large amounts of data on clusters of machines, using big data analytic tools such as Apache Spark and Apache Flink to analyse the data. Big data analytic tools are mainly tested regarding speed and reliability. Efforts about Security and thus authentication are spent only at second glance. In such big data analytic tools, authentication is achieved with the help of the Kerberos protocol that is basically built as authentication on top of big data analytic tools. However, Kerberos is vulnerable to attacks, and it lacks providing high availability when users are all over the world. To improve the authentication, this work presents first an analysis of the authentication in Hadoop and the data analytic tools. Second, we propose a concept to deploy Transport Layer Security (TLS) not only for the security of data transportation but as well for authentication within the big data tools. This is done by establishing the connections using certificates with a short lifetime.The proof of concept is realized in Apache Spark, where Kerberos is replaced by the method proposed. We deploy new short living certificates for authentication that are less vulnerable to abuse. With our approach the requirements of the industry regarding multi-factor authentication and scalability are met.}, location = {Hamburg, Germany}, series = {ARES 2018}, pages = {1\u20137}, numpages = {7}, keywords = {Apache Spark, Transport Layer Security (TLS), Multi Factor Authentication, Big Data Analytic Tools, Kerberos}}
@inproceedings{10.1145/3127404.3130250,title = {Brain Big Data Based Wisdom Service: A Brain Informatics Based Systematic Approach}, author = {Zhong Ning },year = {2017}, isbn = {9781450353526}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3127404.3130250}, doi = {10.1145/3127404.3130250}, abstract = {In this talk, I demonstrate a Brain Informatics based systematic approach to an integrated understanding of macroscopic and microscopic level working principles of the brain by means of experimental, computational, and cognitive neuroscience studies, as well as utilizing advanced Web intelligence centric information technologies. I discuss research issues and challenges with respect to brain computing from three aspects of Brain Informatics studies that deserve closer attention: systematic investigations for complex brain science problems, new information technologies for supporting systematic brain science studies, and Brain Informatics studies based on Web intelligence (AI on the Internet) research needs. These three aspects offer different ways to study traditional cognitive science, neuroscience, brain and mental health, and artificial intelligence.}, location = {Chongqing, China}, series = {ChineseCSCW '17}, pages = {1}, numpages = {1}, keywords = {brain big data, Brain informatics, wisdom service, cognitive science}}
@inproceedings{10.1145/2611286.2611311,title = {Mobile CEP in real-time big data processing: challenges and opportunities}, author = {Stojanovic Nenad , Stojanovic Ljiljana , Xu Yongchun , Stajic Boban },year = {2014}, isbn = {9781450327374}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2611286.2611311}, doi = {10.1145/2611286.2611311}, abstract = {The expansion of the mobile applications in various domains has opened the issue of the optimal usage of the limited resources (e.g. data storage capacity and processing power). This is especially important for the so-called data-intensive applications, which deal with huge amount of mobile data, like in the case of m-Health (i.e. wearable sensing). Many of these applications are oriented towards detecting of particular real-time situations, which brings them to the domain of Event Processing. However, we argue that real-time, big data driven applications require a novel infrastructure for distributed complex event processing that is only partially executed on the mobile devices. This tutorial paper presents a foundation for an efficient development of such mobile applications, by introducing a mobile-driven distributed CEP infrastructure. We present the technical details of the infrastructure and its initial implementation.}, location = {Mumbai, India}, series = {DEBS '14}, pages = {256\u2013265}, numpages = {10}, keywords = {semantic technologies, fast and big data, mobile complex event processing, remote personal monitoring}}
@inproceedings{10.14778/3007263.3007324,title = {Trends and challenges in big data processing}, author = {Stoica Ion },year = {2016}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3007263.3007324}, doi = {10.14778/3007263.3007324}, abstract = {Almost six years ago we started the Spark project at UC Berkeley. Spark is a cluster computing engine that is optimized for in-memory processing, and unifies support for a variety of workloads, including batch, interactive querying, streaming, and iterative computations. Spark is now the most active big data project in the open source community, and is already being used by over one thousand organizations.One of the reasons behind Spark's success has been our early bet on the continuous increase in the memory capacity and the feasibility to fit many realistic workloads in the aggregate memory of typical production clusters. Today, we are witnessing new trends, such as Moore's law slowing down, and the emergence of a variety of computation and storage technologies, such as GPUs, FPGAs, and 3D Xpoint. In this talk, I'll discuss some of the lessons we learned in developing Spark as a unified computation platform, and the implications of today's hardware and software trends on the development of the next generation of big data processing systems.}, pages = {1619}, numpages = {1}}
@inproceedings{10.1145/3319535.3363267,title = {Data Quality for Security Challenges: Case Studies of Phishing, Malware and Intrusion Detection Datasets}, author = {Verma Rakesh M. , Zeng Victor , Faridi Houtan },year = {2019}, isbn = {9781450367479}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3319535.3363267}, doi = {10.1145/3319535.3363267}, abstract = {Techniques from data science are increasingly being applied by researchers to security challenges. However, challenges unique to the security domain necessitate painstaking care for the models to be valid and robust. In this paper, we explain key dimensions of data quality relevant for security, illustrate them with several popular datasets for phishing, intrusion detection and malware, indicate operational methods for assuring data quality and seek to inspire the audience to generate high quality datasets for security challenges.}, location = {London, United Kingdom}, series = {CCS '19}, pages = {2605\u20132607}, numpages = {3}, keywords = {data difficulty, semiotics, data quality, data poisoning}}
@inproceedings{10.1145/2647908.2655958,title = {Resource-optimizing adaptation for big data applications}, author = {Eichelberger Holger , Schmid Klaus },year = {2014}, isbn = {9781450327398}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2647908.2655958}, doi = {10.1145/2647908.2655958}, abstract = {The resource requirements of Big Data applications may vary dramatically over time, depending on changes in the context. If resources should not be defined for the maximum case, but available resources are mostly static, there is a need to adapt resource usage by modifying the processing behavior. The QualiMaster project researches such an approach for the analysis of systemic risks in the financial markets.}, location = {Florence, Italy}, series = {SPLC '14}, pages = {10\u201311}, numpages = {2}, keywords = {adaptive systems, financial markets, resource adaptation, systematic-risks, QualiMaster, stream-processing}}
@inproceedings{10.1145/3352740,title = {Proceedings of the 2019 3rd International Workshop on Education, Big Data and Information Technology},year = {2019}, isbn = {9781450372053}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Guilin, China}}
@inproceedings{10.1145/3443467.3443723,title = {Analysis and Research on Book Recommendation Model Based on Big Data}, author = {Yuan Ming , Yang Shulin , Gu Mengdie , Gu Huijie },year = {2020}, isbn = {9781450387811}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3443467.3443723}, doi = {10.1145/3443467.3443723}, abstract = {In the context of big data, how to define user behavior models and provide personalized reading services for readers by mining large amounts of user data is a problem that the current reading platform needs to optimize urgently. First, we need to analyze the user behavior model to construct the research status and existing problems, in order to provide large data personalized services, targeted user behavior model based on reading platform construction strategies and construction methods, and designs a user logging library is utilized to extract the user interest in dominant and recessive demand ontology of personalized service plan. The user behavior model based on ontology can be technically seamlessly connected with the big data analysis platform, so as to provide real-time and accurate services, which can effectively deal with the challenges of \"knowledge trek\", \"information overload\" and \"emotional loss\" faced by the personalized service of the reading platform in the current big data environment.}, location = {Xiamen, China}, series = {EITCE 2020}, pages = {21\u201325}, numpages = {5}, keywords = {Personalized Service, Ontology, Linked Data, User Behavior Model, Big Data}}
@inproceedings{10.1145/3544109.3544166,title = {Design of Network Public Opinion Management System Based on Big Data}, author = {Yu Lejie },year = {2022}, isbn = {9781450395786}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544109.3544166}, doi = {10.1145/3544109.3544166}, abstract = {With the development of IT technology and the popularization of Internet applications, the scale and value of information carried and disseminated on the Internet are increasing. It has become one of the most important sources of information for all walks of life, institutions and individuals. The core of the network public opinion system is to use intelligent mining, machine learning and other computer technologies to identify, mine and analyze the public opinion information existing on the network, and solve the problem of the inability to realize the timely supervision of massive and dynamic Internet content by manual means. This paper takes network public opinion management as the research object, and constructs a public opinion management system model under the big data network environment. In the context of big data, this paper focuses on the work flow of the public opinion management system, and designs the system components and implementation methods in detail. The public opinion management system based on big data designed in this paper can meet the needs of users from many aspects. The system can efficiently mine and identify public opinion information from a large amount of data, thus providing a solution for government and enterprise public opinion monitoring.}, location = {Dalian, China}, series = {IPEC '22}, pages = {309\u2013313}, numpages = {5}, keywords = {Big data, Information resource management, Hadoop, Network public opinion}}
@inproceedings{10.1109/CCGrid.2013.53,title = {Towards an optimized big data processing system}, author = {Ghi\u0163 Bogdan , Iosup Alexandru , Epema Dick },year = {2013}, isbn = {9780768549965}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2013.53}, doi = {10.1109/CCGrid.2013.53}, abstract = {Scalable by design to very large computing systems such as grids and clouds, MapReduce is currently a major big data processing paradigm. Nevertheless, existing performance models for MapReduce only comply with specific workloads that process a small fraction of the entire data set, thus failing to assess the capabilities of the MapReduce paradigm under heavy workloads that process exponentially increasing data volumes. The goal of my PhD is to build and analyze a scalable and dynamic big data processing system, including storage (distributed file system), execution engine (MapReduce), and query language (Pig). My contributions for the first two years of PhD research are the following: 1) the design and implementation of a resource management system part of a MapReduce-based processing system for deploying and resizing MapReduce clusters over multicluster systems, 2) the design and implementation of a benchmarking tool for the MapReduce processing system, and 3) the evaluation and modeling of MapReduce using workloads with very large data sets. Furthermore, based on the first two years research, we will optimize the MapReduce system to efficiently process terabytes of data.}, location = {Delft, Netherlands}, series = {CCGRID '13}, pages = {83\u201386}, numpages = {4}}
@inproceedings{10.1145/2693182.2693185,title = {High-Volume Performance Test Framework using Big Data}, author = {Yesudas Michael , S Girish Menon , Nair Satheesh K },year = {2015}, isbn = {9781450333375}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2693182.2693185}, doi = {10.1145/2693182.2693185}, abstract = {The inherent issues with handling large files and complex scenarios cause the data-driven approach [1] to be rarely used for performance tests. Volume and scalability testing of enterprise solutions typically requires custom-made test frameworks because of the complexity and uniqueness of data flow. The generation, transformation and transmission of large sets of data pose a unique challenge for testing a highly transactional back-end system like the IBM Sterling Order Management (OMS). This paper describes a test framework built on document-oriented NoSQL database, a design that helps validate the functionality and scalability of the solution simultaneously. This paper also describes various phases of planning, development, and testing of the OMS solution that was executed for a large retailer in Europe to test an extremely high online sales scenario. An out-of-the-box configuration of the OMS with the feature support for database sharding was used to drive scalability. The exercise was a success, and it is the world's largest IBM Sterling Order Management benchmark in terms of sales order volume, to date.}, location = {Austin, Texas, USA}, series = {LT '15}, pages = {13\u201316}, numpages = {4}, keywords = {load testing, rapid prototyping, document oriented storage, test harness, big data, order management, test automation tool}}
@inproceedings{10.1145/3319647.3325854,title = {Big data skipping in the cloud}, author = {Feder Oshrit , Khazma Guy , Lushi Gal , Moatti Yosef , Ta-Shma Paula },year = {2019}, isbn = {9781450367493}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3319647.3325854}, doi = {10.1145/3319647.3325854}, abstract = {According to today's best practices, cloud compute and storage services should be deployed and managed independently. However, this generates a problem for big data analytics in the cloud: potentially huge datasets need to be shipped from the storage service to the compute service to analyse the data. To address this, minimizing the amount of data sent across the network is critical to achieve good performance and low cost. Data skipping is a technique which achieves this for SQL style analytics on structured data.Data skipping stores summary metadata for each object (or file) in a dataset. For each column in the object, the summary might include minimum and maximum values, a list or bloom filter of the appearing values, or other metadata which succinctly represents the data in that column. This metadata can then be indexed to support efficient retrieval, although since it can be orders of magnitude smaller than the data itself, this step may not be essential. This metadata can be used during query evaluation to skip over objects which have no relevant data. False positives for object relevance are acceptable since the query execution engine will ultimately filter the data at the row level. However false negatives must be avoided to ensure correctness of query results.Unlike fully inverted database indexes, data skipping indexes are much smaller than the data itself. This property is critical in the cloud, since otherwise a full index scan could increase the amount of data sent across the network instead of reducing it. In the context of database systems, data skipping is used as an additional technique which complements classical indexes. It is referred to as synopsis in DB2 [6] and zone maps in Oracle [9], where in both cases it is limited to min/max metadata. Data skipping and the associated topic of data layout, has been addressed in recent research papers [7, 8] and is also used in cloud analytics platforms [3,4]. Data skipping can also be built into specific data formats [1].We implemented data skipping support for Apache Spark SQL [2] without changing core Spark, in the form of an addon Scala library which can be added to the classpath and used in Spark applications. Our work applies to storage systems which implement the Hadoop FileSystem API, which includes various object storage systems as well as HDFS. Metadata is stored in Elasticsearch (ES) [5], and additional metadata stores can be supported in future using a pluggable API. Our approach prunes the list of candidate objects for any given Spark SQL query according to the associated data skipping metadata, stored and indexed in ES. Our technique applies to all Spark supported native formats e.g. JSON, CSV, Avro, Parquet, ORC, and can benefit from the latest optimizations built in to those formats in Spark. Unlike approaches which embed data skipping metadata inside the data format itself [1], which require reading at least part of the object, our approach avoids touching irrelevant objects altogether.}, location = {Haifa, Israel}, series = {SYSTOR '19}, pages = {193}, numpages = {1}}
@inproceedings{10.1145/3219788.3219809,title = {File System Performance Tuning for Standard Big Data Benchmarks}, author = {Ren Da Qi , Xia Bing },year = {2018}, isbn = {9781450363938}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3219788.3219809}, doi = {10.1145/3219788.3219809}, abstract = {Modern file system manages super large data sets to perform data intensive and cost-effective analytical processing. Performance of a file system relies on storages, software, workload characteristic and configurations. Complex techniques have to be used in analysis because the data are often hybrid mix of different formats and different structured datasets. Performance study helps to optimize these factors and improve the design of a file system to meet the requirements of a specific application. A promising approach is to allocate the diverse data of various applications on different file systems according to their individual properties, in order to support the best possible performance to every particular application. Some basis that simulate the characters and scenarios of each step of data execution procedures are addressed in this paper. Based on workload characteristic analysis, administrator can implement tuning methods in the large and high-dimensional configuration parameter settings provided by the platform accordingly. Preliminary results are provided by running standard benchmark TPCx-HS, TPCx-BB, TPC-H and HiBench K-means on Ext4 and Btrfs file systems, and the impactions of workload characteristics to the benchmark performance have been analysed.}, location = {Shanghai, China}, series = {ICCDE 2018}, pages = {22\u201326}, numpages = {5}, keywords = {File system, Big data, Benchmarks, Performance model}}
@inproceedings{10.1145/3365109.3368775,title = {A Fused Intelligent Computing Approach Using Stock Big Data for Near Future Trend Prediction}, author = {Yan Tao , Han Chongzhao , Jia Yong },year = {2019}, isbn = {9781450370165}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3365109.3368775}, doi = {10.1145/3365109.3368775}, abstract = {This research makes an attempt of using historical stock big data to predict near future trend of the stock. To fulfill this task, a novel approach based on fused intelligent computing is introduced and investigated. It is composited of four main parts, including data discretization, attribute reduction, classification and decision fusion. Further, one or two algorithms are adopted to realize specific function in each part, respectively. The given stock indexes are selected by the reduction algorithm of discernibility matrix, and the outputs of multiple classifiers are fused by the decision fusion algorithm. These processes and other ones are dedicated to enhancing the accuracy of stock trend prediction. To demonstrate the effectiveness of our approach, a variety of experimental simulations utilizing historical data of three stocks in NASDAQ are carried out, and the prediction accuracy of the proposed approach are compared as well. The experimental results prove that our approach could accomplish the prediction task with high accuracy.}, location = {Auckland, New Zealand}, series = {BDCAT '19}, pages = {113\u2013116}, numpages = {4}, keywords = {data discretization, decision fusion, attribute reduction, stock trend prediction, fused intelligent computing}}
@inproceedings{10.1145/1577840.1577845,title = {Representing Data Quality in Sensor Data Streaming Environments}, author = {Klein A. , Lehner W. },year = {2009}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1577840.1577845}, doi = {10.1145/1577840.1577845}, abstract = {Sensors in smart-item environments capture data about product conditions and usage to support business decisions as well as production automation processes. A challenging issue in this application area is the restricted quality of sensor data due to limited sensor precision and sensor failures. Moreover, data stream processing to meet resource constraints in streaming environments introduces additional noise and decreases the data quality. In order to avoid wrong business decisions due to dirty data, quality characteristics have to be captured, processed, and provided to the respective business task. However, the issue of how to efficiently provide applications with information about data quality is still an open research problem.In this article, we address this problem by presenting a flexible model for the propagation and processing of data quality. The comprehensive analysis of common data stream processing operators and their impact on data quality allows a fruitful data evaluation and diminishes incorrect business decisions. Further, we propose the data quality model control to adapt the data quality granularity to the data stream interestingness.}, pages = {1\u201328}, numpages = {28}, keywords = {data quality, smart items, Data stream processing}}
@inproceedings{10.1145/3486189.3490018,title = {A brief introduction to geospatial big data analytics with apache AsterixDB}, author = {Sevim Akil , Mahin Mehnaz Tabassum , Vu Tin , Maxon Ian , Eldawy Ahmed , Carey Michael , Tsotras Vassilis },year = {2021}, isbn = {9781450391030}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3486189.3490018}, doi = {10.1145/3486189.3490018}, abstract = {There is immense potential with spatial data, which is even more significant when combined with temporal or textual features, or both. However, it is expensive to store and analyze spatial data, and it is even more challenging with the combined features due to the additional optimization requirements. There are numerous successful solutions for big spatial data management, but they do not well support non-spatial operations. The options for the systems are even smaller for the open sources systems, and there are not a handful of options that provide good coverage of care about the spatial and non-spatial operations. This tutorial introduces Apache AsterixDB, a scalable open-source Big Data Management System, which supports standard vector spatial data types as well as non-spatial attributes, e.g., numerical, temporal, and textual. The participants will get hands-on experience on how Apache AsterixDB can efficiently process complex SQL++ queries that require multiple special handling by a team from its kitchen.}, location = {Beijing, China}, series = {SpatialAPI '21}, pages = {1\u20132}, numpages = {2}, keywords = {spatial analysis, big data analytics, geospatial big data}}
@inproceedings{10.1145/3053600.3053624,title = {Modeling Expands Value of Performance Testing for Big Data Applications}, author = {Zibitsker Boris , Lupersolsky Alex },year = {2017}, isbn = {9781450348997}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3053600.3053624}, doi = {10.1145/3053600.3053624}, abstract = {Performance testing of Big Data applications is performed typically on small test environment with limited volume of data. The results of these types of tests do not take into consideration differences between test and production hardware and software environment and contention for resources with many applications in production environments. In this paper we will review application of the modeling for extending the results of performance testing, predicting how new application will perform in production environment. We will review how modeling results can be used to evaluate different options and justify decisions during design, development, implementation and performance management of the production environment.}, location = {L&apos;Aquila, Italy}, series = {ICPE '17 Companion}, pages = {119\u2013123}, numpages = {5}, keywords = {performance prediction., performance models, performance assurance, performance engineering, big data infrastructure, big data applications, performance testing, benchmark}}
@inproceedings{10.1145/2851141.2851187,title = {Preemption-aware planning on big-data systems}, author = {Rabozzi Marco , Mazzucchelli Matteo , Cordone Roberto , Fumarola Giovanni Matteo , Santambrogio Marco D. },year = {2016}, isbn = {9781450340922}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2851141.2851187}, doi = {10.1145/2851141.2851187}, abstract = {Recent developments in Big Data frameworks are moving towards reservation based approaches as a mean to manage the increasingly complex mix of computations, whereas preemption techniques are employed to meet strict jobs deadlines. Within this work we propose and evaluate a new planning algorithm in the context of reservation based scheduling. Our approach is able to achieve high cluster utilization while minimizing the need for preemption that causes system overheads and planning mispredictions.}, location = {Barcelona, Spain}, series = {PPoPP '16}, pages = {1\u20132}, numpages = {2}}
@inproceedings{10.1145/3016078.2851187,title = {Preemption-aware planning on big-data systems}, author = {Rabozzi Marco , Mazzucchelli Matteo , Cordone Roberto , Fumarola Giovanni Matteo , Santambrogio Marco D. },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3016078.2851187}, doi = {10.1145/3016078.2851187}, abstract = {Recent developments in Big Data frameworks are moving towards reservation based approaches as a mean to manage the increasingly complex mix of computations, whereas preemption techniques are employed to meet strict jobs deadlines. Within this work we propose and evaluate a new planning algorithm in the context of reservation based scheduling. Our approach is able to achieve high cluster utilization while minimizing the need for preemption that causes system overheads and planning mispredictions.}, pages = {1\u20132}, numpages = {2}}
@inproceedings{10.1145/2660517.2660529,title = {Big data analysis and query optimization improve HadoopDB performance}, author = {Bissiriou Cherif A. A. , Chaoui Habiba },year = {2014}, isbn = {9781450329279}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2660517.2660529}, doi = {10.1145/2660517.2660529}, abstract = {High performance and scalability are two essentials requirements for data analytics systems as the amount of data being collected, stored and processed continue to grow rapidly. In this paper, we propose a new approach based on HadoopDB. Our main goal is to improve HadoopDB performance by adding some components. To achieve this, we incorporate a fast and space-efficient data placement structure in MapReduce-based Warehouse systems and another SQL-to-MapReduce translator. We also replace the initial Database implemented in HadoopDB with other column oriented Database. In addition we add security mechanism to protect MapReduce processing integrity.}, location = {Leipzig, Germany}, series = {SEM '14}, pages = {1\u20134}, numpages = {4}, keywords = {Hadoop, big data, query execution, MapReduce, optimization}}
@inproceedings{10.1145/3293614.3293626,title = {A Computer Cluster for Big Data and Data Analytics Management: Design, Implementation, and Assessment}, author = {Garc\u00eda-Ojeda J. C. , Ort\u00edz Meleny Luna , Garc\u00eda Rodolfo S\u00e1nchez , C\u00e1ceres Javier Hern\u00e1ndez , Argoti Andr\u00e9s },year = {2018}, isbn = {9781450365727}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3293614.3293626}, doi = {10.1145/3293614.3293626}, abstract = {This paper deals with the implementation of a computer cluster based on Apache Hadoop technology to provide Big Data and Data Analytics services. The cluster is an innovative proposal of the TIC consulting office (CTIC) of the Universidad Santo Tom\u00e1s - Bucaramanga, which aims at satisfying the needs regarding Big Data and Data Analytics of the Bucaramanga Metropolitan Area. The cluster implementation includes the description of the architecture and configuration of the equipment involved as well as the series of experiments carried out to assess the cluster's performance in terms of computing time, energy consumption, cost, and carbon footprint.}, location = {Fortaleza, Brazil}, series = {EATIS '18}, pages = {1\u20138}, numpages = {8}, keywords = {Big Data, Data Analytics, Apache Hadoop, IT services}}
@inproceedings{10.1145/3006386.3006391,title = {Big data as a service from an urban information system}, author = {Sorokine Alexandre , Karthik Rajasekar , King Anthony , Budhendra Bhaduri },year = {2016}, isbn = {9781450345811}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3006386.3006391}, doi = {10.1145/3006386.3006391}, abstract = {Big Data has already proven itself as a valuable tool that lets geographers and urban researchers utilize large data resources to generate new insights. However, wider adoption of Big Data techniques in these areas is impeded by a number of difficulties in both knowledge discovery and data and science production. Typically users face such problems as disparate and scattered data, data management, spatial searching, insufficient computational capacity for data-driven analysis and modelling, and the lack of tools to quickly visualize and summarize large data and analysis results. Here we propose an architecture for an Urban Information System (UrbIS) that mitigates these problems by utilizing the Big Data as a Service (BDaaS) concept. With technological roots in High-performance Computing (HPC), BDaaS is based on the idea of outsourcing computations to different computing paradigms, scalable to super-computers. UrbIS aims to incorporate federated metadata search, integrated modeling and analysis, and geovisualization into a single seamless workflow. The system is under active development and is built around various emerging technologies that include hybrid and NoSQL databases, massively parallel systems, GPU computing, and WebGL-based geographic visualization. UrbIS is designed to facilitate the use of Big Data across multiple cities to better understand how urban areas impact the environment and how climate change and other environmental change impact urban areas.}, location = {Burlingame, California}, series = {BigSpatial '16}, pages = {34\u201341}, numpages = {8}, keywords = {big data as a service, urban informatics, high-performance geocomputing, environmental change impact}}
@inproceedings{10.1145/3148055,title = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},year = {2017}, isbn = {9781450355490}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {It is with great pleasure, on behalf of the program committee, that we welcome you to the fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT2017), to be held in Austin, Texas, USA.BDCAT, as an international conference series, has established itself as the forum for researchers and practitioners in the varied spectrum of human endeavors where data is produced and consumed; from health and personalized medicine, to social services, to industrial processes, to security, to retail business and to high energy physics to identify elementary particle to unlock the secrets of the universe, among many other fields. Big data is an all-encompassing term combining the various characteristics of data that includes their volume, the velocity of data generation and consumption, the variety of data sources and formats, and the variability in their characteristics. The Big Data ecosystem encompasses theoretical and computational frameworks, the applications that deal with such data, and the emerging technologies that ultimately benefit the masses.Since its birth in 2014 in London, UK, BDCAT has become one of the premier forums for sharing of new advances in the methodology, the applications and technologies for big data. Today, BDCAT continues its success. This year we have received 93 submissions from 22 countries. Of these submissions, 27 were accepted for publication, leading to an acceptance rate of 29%.A monumental effort such as BDCAT2017 would not come to fruition without the vision and cooperative and dedicated work of many individuals across the globe. In particular we would like to thank the experts comprising the BDCAT Technical Program Committee for preserving the tradition of rigorous, high-quality peer reviews through their dedication, hard work, and discussions leading up to the selection of the papers. We acknowledge the relentless support that we received from our honorary leadership, Professors Rajkumar Buyya at the University of Melbourne, Australia, Geoffrey Fox at Indian University, USA and Beng Chin OOI of the National University of Singapore, Singapore. We also kindly acknowledge the dedicated support of the local organizing committee chairs: Professors Tim Cockerill of Texas Advanced Computing Center, Jerry Perez, Texas Tech University, Ravi Vadapalli, Texas Tech University and Zhangxi Lin, Texas Tech University, all of the USA. With efforts that spanned almost a year, we also acknowledge the efforts of the publicity chairs, professors David Chiu, University of Puget Sound, USA, Ningfang Mi, Northeastern University, USA, Gleb Radchenko, South Ural State University, Russia, Andrei Tchernykh, CICESE Research Center, Mexico, Yan Tang, Hohai University, China and Iman Elghandour, Alexandria University, Egypt.}, location = {Austin, Texas, USA}}
@inproceedings{10.1145/3349341.3349371,title = {Quality Control Framework of Big Data for Early Warning of Agricultural Meteorological Disasters}, author = {Li Jiale , Liao Shunbao },year = {2019}, isbn = {9781450371506}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3349341.3349371}, doi = {10.1145/3349341.3349371}, abstract = {Agricultural meteorological disasters, including floods, droughts, dry hot winds, low temperature chills, typhoons, hail and continuous rain, can lead to significant reduction in agricultural output. Big data platform for early warning of agricultural meteorological disaster is the basis of business operation system for early warning of agricultural meteorological disasters, and the data quality is an important guarantee for success of the early warning. Quality control of big data for early warning of agricultural meteorological disaster involves names of data sets, metadata, data documents and content of data sets. The quality control for contents of data sets is divided into quality control of attribute data and that of spatial data, and quality control of spatial data is divided into quality control of vector data and that of raster data. Methods for data quality control are divided into fully automatic, semi-automatic and full manual control methods.}, location = {Wuhan, Hubei, China}, series = {AICS 2019}, pages = {74\u201378}, numpages = {5}, keywords = {early warning, big data, agro-meteorological disasters, framework, quality control}}
@inproceedings{10.1007/s00778-018-0514-9,title = {A survey of state management in big data processing systems}, author = {To Quoc-Cuong , Soto Juan , Markl Volker },year = {2018}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg}, url = {https://doi.org/10.1007/s00778-018-0514-9}, doi = {10.1007/s00778-018-0514-9}, abstract = {The concept of state and its applications vary widely across big data processing systems. This is evident in both the research literature and existing systems, such as Apache Flink, Apache Heron, Apache Samza, Apache Spark, and Apache Storm. Given the pivotal role that state management plays, particularly, for iterative batch and stream processing, in this survey, we present examples of state as an enabler, discuss the alternative approaches used to handle and implement state, capture the many facets of state management, and highlight new research directions. Our aim is to provide insight into disparate state management techniques, motivate others to pursue research in this area, and draw attention to open problems.}, pages = {847\u2013872}, numpages = {26}, keywords = {Big data processing systems, Survey, State management}}
@inproceedings{10.1145/3219819.3219941,title = {On Big Data Learning for Small Data Problems}, author = {Teh Yee Whye },year = {2018}, isbn = {9781450355520}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3219819.3219941}, doi = {10.1145/3219819.3219941}, abstract = {Much recent progress in machine learning have been fueled by the explosive growth in the amount and diversity of data available, and the computational resources needed to crunch through the data. This begs the question of whether machine learning systems necessarily need large amounts of data to solve a task well. An exciting recent development, under the banners of meta-learning, lifelong learning, learning to learn, multitask learning etc., has been the observation that often there is heterogeneity within the data sets at hand, and in fact a large data set can be viewed more productively as many smaller data sets, each pertaining to a different task. For example, in recommender systems each user can be said to be a different task with a small associated data set, and in AI one holy grail is how to develop systems that can learn to solve new tasks quickly from small amounts of data. In such settings, the problem is then how to \"learn to learn quickly\", by making use of similarities among tasks. One perspective for how this is achievable is that exposure to lots of previous tasks allows the system to learn a rich prior knowledge about the world in which tasks are sampled from, and it is with rich world knowledge that the system is able to solve new tasks quickly. This is a very active, vibrant and diverse area of research, with many different approaches proposed recently. In this talk I will describe a view of this problem from probabilistic and deep learning perspectives, and describe a number of efforts in this direction that I have recently been involved in.}, location = {London, United Kingdom}, series = {KDD '18}, pages = {3}, numpages = {1}, keywords = {small data, meta-learning, lifelong learning, machine learning, learning to learn, multitask learning, learn to learn, big data}}
@inproceedings{10.1145/2744700.2744705,title = {Big data spatial analytics for enterprise applications}, author = {Ravada Siva },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2744700.2744705}, doi = {10.1145/2744700.2744705}, abstract = {Big data is present everywhere and it can help organization in any industry in many different ways. One of the main sources of this data is the increased digitization in every aspect of life. In general terms, digitization is the process of making something digital. That is, use computer technology in the middle of an activity that used to be done without computers. For example, people used to shoot pictures on film, but now most of the pictures are digital. People used to pay tolls with cash, and now it is digital. People used to drive cars with rack-and-pinion steering, now they are all drive-by-wire, fully digital rolling computers.}, pages = {34\u201341}, numpages = {8}}
@inproceedings{10.1145/3158421.3158427,title = {Big Data, the Internet of Things, and the Revised Knowledge Pyramid}, author = {Jennex Murray E. },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3158421.3158427}, doi = {10.1145/3158421.3158427}, abstract = {The knowledge pyramid has been used for several years to illustrate the hierarchical relationships between data, information, knowledge, and wisdom. An earlier version of this paper presented a revised knowledge-KM pyramid that included processes such as filtering and sense making, reversed the pyramid by positing there was more knowledge than data, and showed knowledge management as an extraction of the pyramid. This paper expands the revised knowledge pyramid to include the Internet of Things and Big Data. The result is a revision of the data aspect of the knowledge pyramid. Previous thought was of data as reflections of reality as recorded by sensors. Big Data and the Internet of Things expand sensors and readings to create two layers of data. The top layer of data is the traditional transaction / operational data and the bottom layer of data is an expanded set of data reflecting massive data sets and sensors that are near mirrors of reality. The result is a knowledge pyramid that appears as an hourglass.}, pages = {69\u201379}, numpages = {11}, keywords = {internet of things, knowledge pyramid, big data, knowledge management, analytics}}
@inproceedings{10.1145/3561801,title = {Proceedings of the 2022 5th International Conference on Big Data and Internet of Things},year = {2022}, isbn = {9781450390361}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Chongqing, China}}
@inproceedings{10.5555/1325851.1325890,title = {Improving data quality: consistency and accuracy}, author = {Cong Gao , Fan Wenfei , Geerts Floris , Jia Xibei , Ma Shuai },year = {2007}, isbn = {9781595936493}, publisher = {VLDB Endowment}, abstract = {Two central criteria for data quality are consistency and accuracy. Inconsistencies and errors in a database often emerge as violations of integrity constraints. Given a dirty database D, one needs automated methods to make it consistent, i.e., find a repair D' that satisfies the constraints and \"minimally\" differs from D. Equally important is to ensure that the automatically-generated repair D' is accurate, or makes sense, i.e., D' differs from the \"correct\" data within a predefined bound. This paper studies effective methods for improving both data consistency and accuracy. We employ a class of conditional functional dependencies (CFDs) proposed in [6] to specify the consistency of the data, which are able to capture inconsistencies and errors beyond what their traditional counterparts can catch. To improve the consistency of the data, we propose two algorithms: one for automatically computing a repair D' that satisfies a given set of CFDs, and the other for incrementally finding a repair in response to updates to a clean database. We show that both problems are intractable. Although our algorithms are necessarily heuristic, we experimentally verify that the methods are effective and efficient. Moreover, we develop a statistical method that guarantees that the repairs found by the algorithms are accurate above a predefined rate without incurring excessive user interaction.}, location = {Vienna, Austria}, series = {VLDB '07}, pages = {315\u2013326}, numpages = {12}}
@inproceedings{10.1145/2429376.2429382,title = {Streaming big data with self-adjusting computation}, author = {Acar Umut A. , Chen Yan },year = {2013}, isbn = {9781450318716}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2429376.2429382}, doi = {10.1145/2429376.2429382}, abstract = {Many big data computations involve processing data that changes incrementally or dynamically over time. Using existing techniques, such computations quickly become impractical. For example, computing the frequency of words in the first ten thousand paragraphs of a publicly available Wikipedia data set in a streaming fashion using MapReduce can take as much as a full day. In this paper, we propose an approach based on self-adjusting computation that can dramatically improve the efficiency of such computations. As an example, we can perform the aforementioned streaming computation in just a couple of minutes.}, location = {Rome, Italy}, series = {DDFP '13}, pages = {15\u201318}, numpages = {4}, keywords = {incremental mapreduce, self-adjusting computation}}
@inproceedings{10.1145/2743065.2743121,title = {Big Data Scalability, Methods and its Implications: A Survey of Current Practice}, author = {Amudhavel J. , Sathian D. , Raghav R. S. , Rao Dhanawada Nirmala , Dhavachelvan P. , Kumar K. Prem },year = {2015}, isbn = {9781450334419}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2743065.2743121}, doi = {10.1145/2743065.2743121}, abstract = {In the recent years; with the rise in usage of the devices that could connect itself to the network and could share data, there is a steady increase in the number of applications that are being introduced for providing various services to the users who rely on the devices that are being connected on the network to use the application. The biggest issue that these applications will face is how these applications will have to handle the data that is being generated by its users and also how these applications will provide the security to the data. For any application it is important to provide the security to the data of its users. Some of the major applications will involve high privacy data of the users which providing security will play a vital role and any compromise in the security [7] aspects of the applications will lead to enormous loss. The second issue that the application must focus upon is the scalability. There are two important key points why the scalability [8] is important. One, when the applications is being created it is the services that is being more focused upon rather than the count of the users that could use so providing a scalable system that could incorporate as many users as the users rise [9] is important for the application. Second, the hardware and the software configuration for the system will not be more focused upon during the development of the system, even though the hardware and the software configuration would be focused upon it is to be seen than they are satisfied for the services [10] the application provide. So providing a scalable system that can adapt the change of the hardware and of the software as they are being upgraded is an important element [11] in any part of the applications.}, location = {Unnao, India}, series = {ICARCSET '15}, pages = {1\u20135}, numpages = {5}, keywords = {big-data, privacy driven data, Scalability}}
@inproceedings{10.1145/3366030.3366044,title = {Big Data Management and Analytics in Intelligent Smart Environments: State-of-the-Art Analysis and Future Research Directions}, author = {Cuzzocrea Alfredo },year = {2019}, isbn = {9781450371797}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3366030.3366044}, doi = {10.1145/3366030.3366044}, abstract = {This paper focuses on big data management and analytics in intelligent smart environments, with particular regards to intelligent transportation and logistics systems, and provides relevant research directions that may represent a milestone for future years.}, location = {Munich, Germany}, series = {iiWAS2019}, pages = {5\u20137}, numpages = {3}, keywords = {Big data analytics, Intelligent smart environments, Big data management}}
@inproceedings{10.1145/2912845.2912869,title = {Development of a knowledge system for Big Data: Case study to plant phenotyping data}, author = {Luyen LE Ngoc , Tireau Anne , Venkatesan Aravind , Neveu Pascal , Larmande Pierre },year = {2016}, isbn = {9781450340564}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2912845.2912869}, doi = {10.1145/2912845.2912869}, abstract = {In the recent years, the data deluge in many areas of scientific research brings challenges in the treatment and improvement of agricultural data. Research in bioinformatics field does not outside this trend. This paper presents some approaches aiming to solve the Big Data problem by combining the increase in semantic search capacity on existing data in the plant research laboratories. This helps us to strengthen user experiments on the data obtained in this research by infering new knowledge. To achieve this, there exist several approaches having different characteristics and using different platforms. Nevertheless, we can summarize it in two main directions: the query re-writing and data transformation to RDF graphs. In reality, we can solve the problem from origin of increasing capacity on semantic data with triplets. Thus, data transformation to RDF graphs direction was chosen to work on the practical part. However, the synchronization data in the same format is required before processing the triplets because our current data are heterogeneous. The data obtained for triplets are larger that regular triplestores could manage. So we evaluate some of them thus we can compare the benefits and drawbacks of each and choose the best system for our problem.}, location = {N\u00eemes, France}, series = {WIMS '16}, pages = {1\u20139}, numpages = {9}, keywords = {xR2RML, Inference, Triplestore, Big Data, SPARQL, Benchmark, Reasoning, NoSQL, Ontology, Knowledge base}}
@inproceedings{10.1145/3456565.3460026,title = {A Big Data Learning Platform for the West Balkans and Beyond}, author = {Graux Damien , Janev Valentina , Jabeen Hajira , Sallinger Emanuel },year = {2021}, isbn = {9781450383974}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456565.3460026}, doi = {10.1145/3456565.3460026}, abstract = {As the number of Big Data related methods, tools, frameworks, and solutions is growing, there is a need to classify and make available the knowledge related to this domain. This is especially useful for countries which, so far, have suffered from some lack of infra-structure in Big Data. In this article, we describe the deployment of an open online platform gathering lectures and resources, from multiple partnered European institutions, tailored for West Balkan students with a particular focus on local Big Data challenges.}, location = {Virtual Event, Germany}, series = {ITiCSE '21}, pages = {617\u2013618}, numpages = {2}, keywords = {teaching big data analytics, west Balkans, European collaboration}}
@inproceedings{10.1145/3170521.3170535,title = {Imbalanced big data classification: a distributed implementation of SMOTE}, author = {Rastogi Avnish Kumar , Narang Nitin , Siddiqui Zamir Ahmad },year = {2018}, isbn = {9781450363976}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3170521.3170535}, doi = {10.1145/3170521.3170535}, abstract = {In the domain of machine learning, quality of data is most critical component for building good models. Predictive analytics is an AI stream used to predict future events based on historical learnings and is used in diverse fields like predicting online frauds, oil slicks, intrusion attacks, credit defaults, prognosis of disease cells etc. Unfortunately, in most of these cases, traditional learning models fail to generate required results due to imbalanced nature of data. Here imbalance denotes small number of instances belonging to the class under prediction like fraud instances in the total online transactions. The prediction in imbalanced classification gets further limited due to factors like small disjuncts which get accentuated during the partitioning of data when learning at scale. Synthetic generation of minority class data (SMOTE [<u>1</u>]) is one pioneering approach by Chawla [<u>1</u>] to offset said limitations and generate more balanced datasets. Although there exists a standard implementation of SMOTE in python, it is unavailable for distributed computing environments for large datasets. Bringing SMOTE to distributed environment under spark is the key motivation for our research. In this paper we present our algorithm, observations and results for synthetic generation of minority class data under spark using Locality Sensitivity Hashing [LSH]. We were able to successfully demonstrate a distributed version of Spark SMOTE which generated quality artificial samples preserving spatial distribution1.}, location = {Varanasi, India}, series = {Workshops ICDCN '18}, pages = {1\u20136}, numpages = {6}, keywords = {SMOTE, imbalanced classification, locality sensitivity hashing, map reduce, nearest neighbors, spark}}
@inproceedings{10.1145/3453187.3453361,title = {Survey of College Students' Career Planning Based on Big Data Statistical Analysis}, author = {Quanli Wang },year = {2020}, isbn = {9781450389099}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3453187.3453361}, doi = {10.1145/3453187.3453361}, abstract = {Big data is of direct significance for analyzing the career planning of college students. The current career planning of college students has problems such as unclear goals and inaccurate identity recognition. Based on this consideration, this article uses big data analysis to select five representative colleges and universities for analysis, and proposes several recommended measures to optimize the career planning of college students.}, location = {Wuhan, China}, series = {EBIMCS 2020}, pages = {363\u2013369}, numpages = {7}, keywords = {Big data, Survey, Career planning, College students}}
@inproceedings{10.1145/3503928.3503933,title = {Business Planning and Big Data, Budget Modelling Upgrade Through Data Science}, author = {Faccia Alessio , Pandey Vishal },year = {2021}, isbn = {9781450385220}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3503928.3503933}, doi = {10.1145/3503928.3503933}, abstract = {The business planning process can be considered a strategic phase of any business. Since the business plan is a management accounting tool, countless approaches can be adopted to prepare it since there is no legal requirement other than financial accounting obligations. A general structure usually consists of numerical statements and descriptive notes. This research is based on the authors\u2019 experiences and commonly used theories, and it highlights a standard process that can be adaptable to the business plan of any activity. The use of big data is an essential part of feeding the data of almost all Budget steps. The authors then determine a generally applicable standard process, indicating all the data necessary to prepare an accurate and reliable business plan. A case study will provide adequate support to the demonstration of the immediate applicability of the proposed model.}, location = {Shanghai, China}, series = {ICISE 2021}, pages = {21\u201325}, numpages = {5}, keywords = {Business strategy, Budgeting, Business plan, Big Data}}
@inproceedings{10.1145/3365109,title = {Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},year = {2019}, isbn = {9781450370165}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {On behalf of the program committee, it is our pleasure to welcome you to the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies, being held in Auckland, New Zealand.Rapid advances in digital sensors, networks, storage, and computation along with their availability at low cost is leading to the creation of huge collections of data - dubbed as Big Data. As a result, Big Data Computing paradigm has emerged, enabling new insights that can change the way business, science, and governments deliver services to their consumers, and can impact society as a whole. The IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT) is an annual conference series aiming to provide a platform for researchers from both academia and industry to present new discoveries in the broad area of big data computing and applications.BDCAT 2019 received 47 submissions from 17 countries. The conference accepted 13 papers as regular papers, leading to acceptance rate of 27.7%. The conference also accepted an additional 10 papers as short papers. For this we would like to acknowledge the dedication and tremendous efforts of the program committee and reviewers, who provided nearly 200 reviews in a 3-week turnaround time.}, location = {Auckland, New Zealand}}
@inproceedings{10.1145/2968219.2968282,title = {Enhancing location prediction with big data: evidence from dhaka}, author = {Matekenya Dunstan , Ito Masaki , Shibasaki Ryosuke , Sezaki Kaoru },year = {2016}, isbn = {9781450344623}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2968219.2968282}, doi = {10.1145/2968219.2968282}, abstract = {In recent years, the study of location prediction has received heightened attention due to its applications in LBS and other areas. However, most of the techniques and subsequent conclusions drawn from previous research works are specific to the data used in the study. For instance, resolution of location data and inclusion of external data (e.g., from social networks) may limit application of previous techniques to new situations. Therefore, we explore ways of enhancing location prediction techniques which leverage big data without the need for external data sources. To this end, we study a large CDR dataset with more than 3.5 billion calls from a leading cellular network provider in Dhaka, Bangladesh. The research question we tackle is how we can leverage big data to enhance performance of location predictors? Based on spatio-temporal analysis of call activity, we devise a scheme to compute prior probabilities from cell call activity. With this reasoning, we develop an enhanced Bayes predictor which uses a distance threshold and the users' regular location to improve generation of prior probabilities. Experimental results show that overall the enhanced Bayes predictor improves accuracy by 17 percentage points.}, location = {Heidelberg, Germany}, series = {UbiComp '16}, pages = {753\u2013762}, numpages = {10}, keywords = {data mining, human mobility, location prediction. supervised classification}}
@inproceedings{10.1145/1659225.1659228,title = {Dual Assessment of Data Quality in Customer Databases}, author = {Even Adir , Shankaranarayanan G. },year = {2009}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1659225.1659228}, doi = {10.1145/1659225.1659228}, abstract = {Quantitative assessment of data quality is critical for identifying the presence of data defects and the extent of the damage due to these defects. Quantitative assessment can help define realistic quality improvement targets, track progress, evaluate the impacts of different solutions, and prioritize improvement efforts accordingly. This study describes a methodology for quantitatively assessing both impartial and contextual data quality in large datasets. Impartial assessment measures the extent to which a dataset is defective, independent of the context in which that dataset is used. Contextual assessment, as defined in this study, measures the extent to which the presence of defects reduces a dataset\u2019s utility, the benefits gained by using that dataset in a specific context. The dual assessment methodology is demonstrated in the context of Customer Relationship Management (CRM), using large data samples from real-world datasets. The results from comparing the two assessments offer important insights for directing quality maintenance efforts and prioritizing quality improvement solutions for this dataset. The study describes the steps and the computation involved in the dual-assessment methodology and discusses the implications for applying the methodology in other business contexts and data environments.}, pages = {1\u201329}, numpages = {29}, keywords = {CRM, customer relationship management, total data quality management, Data quality, information value, databases}}
@inproceedings{10.1145/3330482.3330510,title = {Using Big Data Analysis to Retain Customers for Telecom Industry}, author = {Gu Yuanhu , Malicdem Alvin R. , Cruz Josephine S. Dela , Palaoag Thelma Domingo },year = {2019}, isbn = {9781450361064}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3330482.3330510}, doi = {10.1145/3330482.3330510}, abstract = {Nowadays, telecommunication markets are becoming more and more competitive, and customer churn is becoming more and more serious. In the tough competitive mobile market, Customer Churn Management is becoming more and more critical. In developing countries, most customers switch service providers because of good promotional incentives and lower monthly costs offered by competitive service providers. How to predict customer churn quickly and accurately becomes very important. In this paper, the researchers successfully analyzed the customer churn using big data feature analysis and multi-feature analysis. User data were modeled by XGBoost algorithm. The model is optimized repeatedly with GridSearchCV as a parameter tool. The accuracy of the model on the test set is 85.1%. The researchers predicted about 11000 customer lists per month that may be about to churn. Using K-means clustering method, 11000 churn target customers per month were classified into three categories and telecom companies are suggested to take some solutions which are found by feature analysis to retain customers. This big data analysis can be used to retain customers for the telecom industry.}, location = {Bali, Indonesia}, series = {ICCAI '19}, pages = {38\u201343}, numpages = {6}, keywords = {customer churn, telecom industry, Big data analysis, retain customers, feature analysis}}
@inproceedings{10.1145/3407947.3407977,title = {A Survey of System Scheduling for HPC and Big Data}, author = {Wang Bo , Chen Zhiguang , Xiao Nong },year = {2020}, isbn = {9781450376914}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3407947.3407977}, doi = {10.1145/3407947.3407977}, abstract = {In the rapidly expanding field of parallel processing, job schedulers act as the \"operating systems\" of the clusters, including modern big data architectures and supercomputing systems. Job schedulers manage and allocate system resources, dispatch the queued jobs, and control the execution of processes on the allocated resources. In this paper, we firstly make an introduction to the cluster schedulers. Then according to the scenarios, we make a comprehensive survey of schedulers for HPC and Big Data. We can conclude that most of these current schedulers are centralized, which means master assigns jobs to the slaves. We call this mode Push, which is different from our new idea that introduces Pull to the schedulers. We proposed a novel scheduling model that allow slaves to actively pull jobs from master to execute. By analyzing the execution time and resource requests of jobs in \"Tianhe-II\", we will clarify that scheduling based on Push & Pull is a direction worthy of in-depth study in the future.}, location = {Guangzhou, China}, series = {HP3C 2020}, pages = {178\u2013183}, numpages = {6}, keywords = {Job scheduler, high performance computing, decentralized scheduling, big data cluster}}
@inproceedings{10.1145/2968456.2976765,title = {Big data analytics on heterogeneous accelerator architectures}, author = {Neshatpour Katayoun , Sasan Avesta , Homayoun Houman },year = {2016}, isbn = {9781450344838}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2968456.2976765}, doi = {10.1145/2968456.2976765}, abstract = {In this paper, we present the implementation of big data analytics applications in a heterogeneous CPU+FPGA accelerator architecture. We develop the MapReduce implementation of K-means, K nearest neighbor, support vector machine and Naive Bayes in a Hadoop Streaming environment that allows developing mapper/reducer functions in a non-Java based language suited for interfacing with FPGA-based hardware accelerating environment. We present a full implementation of the HW+SW mappers on the Zynq FPGA platform. A promising speedup as well as energy-efficiency gains of upto 4.5X and 22X is achieved, respectively, in an end-to-end Hadoop implementation.}, location = {Pittsburgh, Pennsylvania}, series = {CODES '16}, pages = {1\u20133}, numpages = {3}}