@inproceedings{10.1145/3341620.3341621,title = {Smart Meter for Smart Homes: Data Mining Applications of Produced Big Data}, author = {Srivastava Shashi Kant },year = {2019}, isbn = {9781450360913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341620.3341621}, doi = {10.1145/3341620.3341621}, abstract = {We attempt to address the opportunities spawned from the voluminous data generated by the incorporation of smart technology within a building. Furthermore, we suggest a framework to discover the acceptance pattern of smart technologies. In the absence of the actual data, we simulated data to perform our research. The main objective of the research is to demonstrate the potential applications of the data obtained from the smart meter to different stakeholders involved in business and policy. Since buildings are one of the largest concerns for various business and government organizations, our research provides multiple future avenues to researchers. The paper demonstrates exploration process of technology adoption behavior of building occupants.}, location = {Hong Kong, Hong Kong}, series = {BDE 2019}, pages = {74\u201378}, numpages = {5}, keywords = {Technology diffusion, smart grid, Temporal data mining (TDM), smart home, IoT, Smart meter, smart devices}}
@inproceedings{10.1145/3482632.3484095,title = {Environmental big data model and recognition of abnormal emission from enterprise data}, author = {Wu Rui , Cheng Qian , He Lisong , Cao Zhenyu },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3484095}, doi = {10.1145/3482632.3484095}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {2041\u20132046}, numpages = {6}}
@inproceedings{10.1145/3257761,title = {Session details: Big Data and Social Media for Public Health Surveillance}, author = {Kostkova Patty },year = {2016}, isbn = {9781450342247}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3257761}, doi = {10.1145/3257761}, location = {Montr\u00e9al, Qu\u00e9bec, Canada}, series = {DH '16}, pages = {}}
@inproceedings{10.1145/2486767.2486771,title = {Scalable I/O-bound parallel incremental gradient descent for big data analytics in GLADE}, author = {Qin Chengie , Rusu Florin },year = {2013}, isbn = {9781450322027}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2486767.2486771}, doi = {10.1145/2486767.2486771}, abstract = {Incremental gradient descent is a general technique to solve a large class of convex optimization problems arising in many machine learning tasks. GLADE is a parallel infrastructure for big data analytics providing a generic task specification interface. In this paper, we present a scalable and efficient parallel solution for incremental gradient descent in GLADE. We provide empirical evidence that our solution is limited only by the physical hardware characteristics, uses effectively the available resources, and achieves maximum scalability. When deployed in the cloud, our solution has the potential to dramatically reduce the cost of complex analytics over massive datasets.}, location = {New York, New York}, series = {DanaC '13}, pages = {16\u201320}, numpages = {5}}
@inproceedings{10.1145/2676723.2691948,title = {Using Big Data and BKT to Evaluate Course Resources (Abstract Only)}, author = {MacHardy Zachary , Garcia Daniel D. },year = {2015}, isbn = {9781450329668}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2676723.2691948}, doi = {10.1145/2676723.2691948}, abstract = {Now finding footing in objective research methodology, MOOCs have made significant strides toward developing into mature platforms for not only offering educational materials but also performing exploratory analysis of educational methods. Bayesian Knowledge Tracing (BKT) has been repeatedly shown to be successful at providing an accurate model of student knowledge in more traditional classroom settings, and recent research has explored the application of BKT to MOOCs with promising results. Using data from several MOOCs run by Stanford university, we propose to extend earlier research into the application of BKT to MOOCS by developing a framework within which the use of course resources and student performance can be leveraged both to increase the predictive accuracy of BKT modeling and to provide an evaluative metric for the utility of those resources. We hope that such a framework can contribute not only to MOOC courses, but traditional classrooms as well.}, location = {Kansas City, Missouri, USA}, series = {SIGCSE '15}, pages = {683}, numpages = {1}, keywords = {moocs, machine learning, prediction, bkt, student modeling, course resources}}
@inproceedings{10.5555/2984464.2984475,title = {Gesture-based cyber-physical in-home therapy system in a big data environment: demo abstract}, author = {Rahman Mohamed Abdur },year = {2016}, publisher = {IEEE Press}, abstract = {This demo provides an overview of a gesture-based cyber-physical therapy system, which integrates entities in the physical as well as cyber world for therapy sensing, therapeutic data computation, interaction between cyber and physical world, and holistic in-home therapy support through a cloud-based big data architecture. To provide appropriate therapeutic services and environment, the CPS uses a multi-modal multimedia sensory framework to support therapy recording and playback of a therapy session and visualization of effectiveness of an assigned therapy. The physical world interaction with the cyber world is stored as a rich gesture semantics with the help of multiple media streams, which is then uploaded to a tightly synchronized cyber physical cloud environment for deducing real-time and historical whole-body Range of Motion (ROM) kinematic data.}, location = {Vienna, Austria}, series = {ICCPS '16}, pages = {1}, numpages = {1}, keywords = {gesture recognition, in-home therapy, multimedia sensors, therapy CPS}}
@inproceedings{10.1145/3452446.3452658,title = {Using Big Data to Enhance the Targeted Research of College Students' Ideological Education}, author = {Yang Hongxia },year = {2021}, isbn = {9781450389815}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3452446.3452658}, doi = {10.1145/3452446.3452658}, abstract = {With the development of big data (BD) technology, the wide application of BD has opened up new perspectives and approaches for ideological education research. Carrying out ideological education for college student (CS)s with the help of BD technology is conducive to the implementation of the party's educational policy, the implementation of the fundamental task of cultivating people with morality, and the training of qualified builders and reliable successors of socialism with Chinese characteristics, who are both red and professional, have both ability and political integrity, and comprehensive development people. The purpose of this article is to analyze the role of BD in the process of targeted ideological education for CSs from the perspective of BD, and use this as an opportunity to identify the current problems encountered in the process of using BD to carry out targeted ideological and political education (IAPE). Promote the pertinence of CSs' IAPE. The methods used in this article include literature research method, induction method and case analysis method, using the four major business system data of CSs during the school period and applying BD technology to study the pertinence of contemporary CSs' ideological education. The study found that the use of BD methods can make up for the shortcomings of traditional research, and many practical conclusions have been obtained. For example, the comprehensive student data obtained by the application of BD technology shows that the overall scores of students\u2019 thoughts tend to be stable between two higher scores. The numbers are 46% and 38%, indicating that most students perform well in school and the overall evaluation is good. The scores of students who have committed violations of discipline and penalties are relatively low, but only account for 6%, which is relatively low.}, location = {Dalian, China}, series = {IPEC2021}, pages = {887\u2013891}, numpages = {5}, keywords = {ideological education for CSs, targeted research, BD technology}}
@inproceedings{10.1145/3544109.3544320,title = {Research on the Current Status of Primary School English Online Attention Based on Big Data}, author = {Shi Heng , Fu Xinyu },year = {2022}, isbn = {9781450395786}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544109.3544320}, doi = {10.1145/3544109.3544320}, abstract = {With the modern development of education in China, the related data in the education field has rapidly expanded, and national education has entered the \"big data era\". Aiming at the characteristics of the large-capacity, multi-type, and high-growth data collection in the education field, the use of big data technology for aggregation, classification, analysis, and reorganization is the only way to promote the high-quality development of education and complete the modernization of education. Based on the big data mining of the National Bureau of Statistics and Baidu Index, using literature data, knowledge graphs, mathematical statistics and logical analysis, the time characteristics of primary school English online attention, attributes of concerned people and factors affecting the English learning needs of netizens are explored and analyzed. The results show that there is no seasonally significant difference in the time characteristics of primary school English online attention. \"Learning\" is the main demand for primary school English attention, but there is a group preference of \"focusing on knowledge and neglecting skills\". Therefore, in the context of the era of \"big data + education\", giving play to the guiding role of education policy and education evaluation, stimulating and leading netizens to correct and healthy online attention, is crucial to achieving high-quality development of primary school English education.}, location = {Dalian, China}, series = {IPEC '22}, pages = {601\u2013607}, numpages = {7}}
@inproceedings{10.1145/3472163.3472185,title = {A Zone-Based Data Lake Architecture for IoT, Small and Big Data}, author = {Zhao Yan , Megdiche Imen , Ravat Franck , Dang Vincent-nam },year = {2021}, isbn = {9781450389914}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472163.3472185}, doi = {10.1145/3472163.3472185}, abstract = {Data lakes are supposed to enable analysts to perform more efficient and efficacious data analysis by crossing multiple existing data sources, processes and analyses. However, it is impossible to achieve that when a data lake does not have a metadata governance system that progressively capitalizes on all the performed analysis experiments. The objective of this paper is to have an easily accessible, reusable data lake that capitalizes on all user experiences. To meet this need, we propose an analysis-oriented metadata model for data lakes. This model includes the descriptive information of datasets and their attributes, as well as all metadata related to the machine learning analyzes performed on these datasets. To illustrate our metadata solution, we implemented a web application of data lake metadata management. This application allows users to find and use existing data, processes and analyses by searching relevant metadata stored in a NoSQL data store within the data lake. To demonstrate how to easily discover metadata with the application, we present two use cases, with real data, including datasets similarity detection and machine learning guidance.}, location = {Montreal, QC, Canada}, series = {IDEAS '21}, pages = {94\u2013102}, numpages = {9}, keywords = {Zone-based, Technical Architecture, Data Lake, Metadata, Stream IoT Data}}
@inproceedings{10.1145/3537693.3537703,title = {The Effect of Skepticism, Big Data Analytics to Financial Fraud Detection Moderated by Forensic Accounting}, author = {Handoko Bambang Leo , Rosita Ameliya },year = {2022}, isbn = {9781450396523}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3537693.3537703}, doi = {10.1145/3537693.3537703}, abstract = {The number of cases of fraud, especially fraud in the preparation and publication of financial statements is increasing. Both in the international world and also in Indonesia. The number of cases that have occurred for several years have only been uncovered, resulting in large losses for users of financial statements and stakeholders. The industrial revolution 4.0 opens up new opportunities, with the existence of big data analytics and forensic audits, which are expected to increase professional skepticism of auditors to be more observant in detecting fraudulent financial reports. Our research is a quantitative study, we tested the hypothesis between the independent variable and the moderating variable on the dependent variable. The independent variables in our research are professional skepticism, and big data analytics, then the moderating variable is forensic accounting, and the dependent variable is financial statement fraud detection. The object of our research is the auditor in a public accounting firm. We used the structural equation modeling partial least square as our data analysis. The results of our study state that professional skepticism, big data analytics have a significant impact on financial statement fraud detection. Forensic accounting moderate both professional skepticism and big data analytics.}, location = {Plymouth, United Kingdom}, series = {ICEEG '22}, pages = {59\u201366}, numpages = {8}, keywords = {Skepticism, Financial Fraud, Analytics, Detection, 64TBig data, Auditor}}
@inproceedings{10.1145/3102304.3102307,title = {Towards Sustainable Smart Society: Big Data Driven Approaches}, author = {Han Liangxiu },year = {2017}, isbn = {9781450348447}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3102304.3102307}, doi = {10.1145/3102304.3102307}, abstract = {By 2020, the total size of digital data generated by social networks, sensors, biomedical imaging and simulation devices, will reach an estimated 44 Zettabytes (e.g. 44 trillion gigabytes) according to IDC report. This type of 'big data', together with the advances in information and communication technologies such as Internet of Things (IoT), connected smart objects, wearable technology, ubiquitous computing, is transforming every aspect of modern life and bringing great challenges and spectacular opportunities to fulfill our dream of a sustainable smart society.This talk will focus on new developments and methods based on big data driven approaches to address society challenges. The talk will also present real case studies to demonstrate how we apply big data approaches in various application domains such as Health, Food, Smart Cities, etc. to realize the smart society.}, location = {Cambridge, United Kingdom}, series = {ICFNDS '17}, pages = {}, keywords = {Smart cities, Internet of Things, Big data, Societal challenges}}
@inproceedings{10.1145/3495018.3501100,title = {A Computer Numerical Cooperation Depth Index Using the Analytic Hierarchy Process and Big Data Technology}, author = {Liu Yingchen , Liang Yinuo , Fu Chenkai , Zheng Hui },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3501100}, doi = {10.1145/3495018.3501100}, abstract = {This paper constructs an evaluation model for a fishery trade cooperation depth index based on big data, which includes four factors: cooperation policy, infrastructure, economy and trade, and technology and humanities. In addition, 14 secondary indicators are set up under four first-level indicators. The big data of the 14 secondary indicators of 34 regional representative countries along the Maritime Silk Road were selected as samples to be calculated, and their scores for the fishery trade cooperation depth index were calculated by using the analytic hierarchy process (AHP) in order to evaluate their fishery trade cooperation achievements. This paper uses big data to put forward relevant policy recommendations to help China participate in international trade cooperation and avoid trade risks effectively.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {2341\u20132350}, numpages = {10}}
@inproceedings{10.1109/TNET.2019.2934026,title = {Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality}, author = {Gong Xiaowen , Shroff Ness B. },year = {2019}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/TNET.2019.2934026}, doi = {10.1109/TNET.2019.2934026}, abstract = {Mobile crowdsensing has found a variety of applications e.g., spectrum sensing, environmental monitoring by leveraging the \u201cwisdom\u201d of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users\u2019 data e.g., users\u2019 received SNRs for measuring a transmitter\u2019s transmit signal strength. However, the quality of a user can be its private information which, e.g., may depend on the user\u2019s location that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data\u2019s accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data\u2019s accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation QEE, which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user\u2019s data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester\u2019s optimal RO effort assignment assigns effort only to the best user that has the smallest \u201cvirtual valuation\u201d, which depends on the user\u2019s quality and the quality\u2019s distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.}, pages = {1959\u20131972}, numpages = {14}}
@inproceedings{10.1145/3361821.3361825,title = {Social Media Data Processing Infrastructure by Using Apache Spark Big Data Platform: Twitter Data Analysis}, author = {Podhoranyi Michal , Vojacek Lukas },year = {2019}, isbn = {9781450372411}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3361821.3361825}, doi = {10.1145/3361821.3361825}, abstract = {Social media provide continuous data streams that contain information with different level of sensitivity, validity and accuracy. Therefore, this type of information has to be properly filtered, extracted and processed to avoid noisy and inaccurate results. The main goal of this work is to propose architecture and workflow able to process Twitter social network data in near real-time. The primary design of the introduced modern architecture covers all processing aspects from data ingestion and storing to data processing and analysing. This paper presents Apache Spark and Hadoop implementation. The secondary objective is to analyse tweets with the defined topic --- floods. The word frequency method (Word Clouds) is shown as a major tool to analyse the content of the input dataset. The experimental architecture confirmed the usefulness of many well-known functions of Spark and Hadoop in the social data domain. The platforms which were used provided effective tools for optimal data ingesting, storing as well as processing and analysing. Based on the analytical part, it was observed that the word frequency method (n-grams) can effectively reveal the tweets content. According to the results of this study, the tweets proved their high informative potential regarding data quality and quantity.}, location = {Tokyo, Japan}, series = {CCIOT 2019}, pages = {1\u20136}, numpages = {6}, keywords = {data processing architecture, Apache Spark, Twitter, social network data}}
@inproceedings{10.1145/3482632.3484002,title = {Research on Particle Swarm Optimization Clustering Algorithm for Big Data Based on Cloud Storage Environment}, author = {Liu Dan },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3484002}, doi = {10.1145/3482632.3484002}, abstract = {With the development of the times, the amount of various data increases exponentially in the process of information processing, and all walks of life use computers to manage and count a large amount of data. There are more and more data piled up in the database, and the scale of data is unprecedented. It is difficult for ordinary database technology to efficiently manage and utilize these data and their hidden important information. Under the background of cloud computing, the main purpose of big data information processing is to realize data clustering. Cluster analysis is an unsupervised learning process, which aims to classify the data of a group of category attribute positions under certain requirements, and make the data similarity between these categories as low as possible, and the data similarity within the categories as high as possible. In this paper, an optimized clustering algorithm for big data in cloud storage based on optimized particle swarm optimization algorithm is proposed, and the improved design of big data clustering algorithm is carried out by using particle swarm optimization algorithm.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1610\u20131613}, numpages = {4}}
@inproceedings{10.1145/3495018.3495278,title = {Research on the Influence of Big Data Information Processing Technology on English Writing Anxiety}, author = {Li Jingtai , Huang Fei },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495278}, doi = {10.1145/3495018.3495278}, abstract = {For English learners, English writing is the most difficult to master, because in the process of writing, it is likely to be affected by mother tongue transfer and other factors. It is the difficulty and importance that lead to learners' fear of difficulties and anxiety in the process of writing practice, which affects their English learning level and expression. This article discusses the current English writing anxiety problems, analyzes the main causes of the problems, and puts forward relevant countermeasures and suggestions to solve the English writing anxiety problems with the help of the platform combined with big data technology.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {793\u2013795}, numpages = {3}}
@inproceedings{10.1145/3498338,title = {Spatial Data Quality in the Internet of Things: Management, Exploitation, and Prospects}, author = {Li Huan , Lu Hua , Jensen Christian S. , Tang Bo , Cheema Muhammad Aamir },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3498338}, doi = {10.1145/3498338}, abstract = {With the continued deployment of the Internet of Things (IoT), increasing volumes of devices are being deployed that emit massive spatially referenced data. Due in part to the dynamic, decentralized, and heterogeneous architecture of the IoT, the varying and often low quality of spatial IoT data (SID) presents challenges to applications built on top of this data. This survey aims to provide unique insight to practitioners who intend to develop IoT-enabled applications and to researchers who wish to conduct research that relates to data quality in the IoT setting. The survey offers an inventory analysis of major data quality dimensions in SID and covers significant data characteristics and associated quality considerations. The survey summarizes data quality related technologies from both task and technique perspectives. Organizing the technologies from the task perspective, it covers recent progress in SID quality management, encompassing location refinement, uncertainty elimination, outlier removal, fault correction, data integration, and data reduction; and it covers low-quality SID exploitation, encompassing querying, analysis, and decision-making techniques. Finally, the survey covers emerging trends and open issues concerning the quality of SID.}, pages = {1\u201341}, numpages = {41}, keywords = {location refinement, quality management, spatial queries, spatiotemporal data cleaning, spatiotemporal dependencies, spatial computing, geo-sensory data, Internet of Things}}
@inproceedings{10.1109/CCGrid.2012.143,title = {Towards Ontology-based Data Quality Inference in Large-Scale Sensor Networks}, author = {Esswein Sam , Goasguen Sebastien , Post Chris , Hallstrom Jason , White David , Eidson Gene },year = {2012}, isbn = {9780769546919}, publisher = {IEEE Computer Society}, address = {USA}, url = {https://doi.org/10.1109/CCGrid.2012.143}, doi = {10.1109/CCGrid.2012.143}, abstract = {This paper presents an ontology-based approach for data quality inference on streaming observation data originating from large-scale sensor networks. We evaluate this approach in the context of an existing river basin monitoring program called the Intelligent River\u00ae. Our current methods for data quality evaluation are compared with the ontology-based inference methods described in this paper. We present an architecture that incorporates semantic inference into a publish/subscribe messaging middleware, allowing data quality inference to occur on real-time data streams. Our preliminary benchmark results indicate delays of 100ms for basic data quality checks based on an existing semantic web software framework. We demonstrate how these results can be maintained under increasing sensor data traffic rates by allowing inference software agents to work in parallel. These results indicate that data quality inference using the semantic sensor network paradigm is viable solution for data intensive, large-scale sensor networks.}, series = {CCGRID '12}, pages = {898\u2013903}, numpages = {6}, keywords = {Distributed Computing, Wireless Sensor Networks, Semantic Web}}
@inproceedings{10.1145/3105831.3105847,title = {BDgen: A Universal Big Data Generator}, author = {Falt\u00edn Tom\u00e1\u0161 , Hanzeli Michal , \u0160\u00edpek Vojt\u011bch , \u0160kva\u0159il Jan , Vari\u0161 Du\u0161an , Ml\u00fdnkov\u00e1 Irena Holubov\u00e1 },year = {2017}, isbn = {9781450352208}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3105831.3105847}, doi = {10.1145/3105831.3105847}, abstract = {This paper introduces BDgen, a generator of Big Data targeting various types of users, implemented as a general and easily extensible framework. It is divided into a scalable backend designed to generate Big Data on clusters and a frontend for user-friendly definition of the structure of the required data, or its automatic inference from a sample data set. In the first release we have implemented generators of two commonly used formats (JSON and CSV) and the support for general grammars. We have also performed preliminary experimental comparisons confirming the advantages and competitiveness of the solution.}, location = {Bristol, United Kingdom}, series = {IDEAS '17}, pages = {200\u2013208}, numpages = {9}, keywords = {benchmarking, synthetic data, SW testing, Big Data generator}}
@inproceedings{10.1145/2663715.2669612,title = {SAFE: Secure and Big Data-Adaptive Framework for Efficient Cross-Domain Communication}, author = {Srinivasan Avinash , Wu Jie , Zhu Wen },year = {2014}, isbn = {9781450315838}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2663715.2669612}, doi = {10.1145/2663715.2669612}, abstract = {Today's Cross Domain Communication (CDC) infrastructure primarily consists of vendor-specific guard products that have little inter-domain coordination at runtime. Unaware of the context and the semantics of the CDC message that is being processed, the guard heavily relies on rudimentary filtering techniques. Consequently, the information domains are rendered vulnerable to an array of attacks, and countering these attacks often necessitates time-consuming human intervention to adjudicate messages in order to meet the desired security and privacy requirements of the communicating domains. Subsequently, this causes significant performance bottlenecks. In this paper, we present a set of key requirements and design principles for a service oriented CDC security infrastructure in form of a CDC Reference Architecture, featuring Domain Associated Guards (DOGs) as active work ow participants. Our proposed framework, SAFE, is secure and adaptable. SAFE also provide the foundation for the development of protocols and ontologies enabling run-time coordination among CDC elements. This enables more flexible, interoperable, and efficient CDC designs to serve mission needs, specifically among critical infrastructure domains as well as domains with significantly differing security and privacy vocabulary. To the best of our knowledge, SAFE is the first effort to employ DOG for secure CDC, unlike existing solutions with link-associated guards. Because of the DOG approach, SAFE overcomes the scalability problems encountered by exiting solutions.}, location = {Shanghai, China}, series = {PSBD '14}, pages = {19\u201328}, numpages = {10}, keywords = {privacy, security, security guard, cross domain communication, reference architecture, ontology, protocol, big data}}
@inproceedings{10.1145/3482632.3484007,title = {A method of constructing distributed big data analysis model for machine learning based on Cloud Computing}, author = {Li Jicai , Liu Dan },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3484007}, doi = {10.1145/3482632.3484007}, abstract = {There are many big data analysis methods, and it is an effective method to build big data analysis model through machine learning. Big data is characterized by large data scale and long calculation cycle. In order to speed up the calculation speed and shorten the calculation cycle, distributed computing method is one of the effective methods to solve the above problems. With the wide application and rapid development of information technology, cloud computing as a new business computing model has attracted more and more attention. However, the security of cloud computing data storage model lacks reliability. Under the mainstream cloud computing and big data basic environment, building a better model from resource aggregation to analysis and mining, and modeling distributed big data analysis can provide high-reliability, high-security, high-efficiency analysis services for practical analysis and mining applications such as intelligence judgment, information deployment and control, stakeholder analysis and intelligent decision-making.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1634\u20131638}, numpages = {5}}
@inproceedings{10.1145/3495018.3495364,title = {Optimization of Modern Teaching System with Computer Technology under the Background of Big Data}, author = {Ding Gaohu },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495364}, doi = {10.1145/3495018.3495364}, abstract = {The rapid development of science and technology has promoted the rapid development and wide application of CT(computer Technology), especially the emergence of BDT(big data technology) in recent years. The integration and improvement of CT and it has promoted great changes in all aspects of society, and these changes are beneficial, and they have brought us great convenience and help Help. For the education industry, under the background of BDT, the integration of CT and modern teaching system can provide new development thinking and new direction for the reform of modern education. In order to study what effect the combination of the two will bring, this paper selects two universities and their students as the experimental research objects to explore how the modern teaching system will be innovated and developed under the effect of this new technology. The experimental results show that the students of a university who have applied CT in the modern teaching system have a high degree of satisfaction, and the percentage of those who are satisfied has reached 67%. Moreover, the score of teaching and research group of a university is relatively high, and the highest score is 95.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {1197\u20131200}, numpages = {4}}
@inproceedings{10.1145/3482632.3483147,title = {The Establishment of Machine Translation Bilingual Corpus Based on Artificial Intelligence and Big Data Technology}, author = {Li Hanhui },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483147}, doi = {10.1145/3482632.3483147}, abstract = {With the rapid development of artificial intelligence and big data technology, all areas of society have gradually begun to use big data technology. Machine translation is a very popular and challenging research content in the field of natural language processing. From the idea of machine translation to the integration of syntactic information into machine translation, scholars at home and abroad have designed many formal models and algorithms for machine translation. They have made positive contributions to the research of machine translation in natural language processing and have made them increasingly mature. This article uses artificial intelligence big data technology to establish a machine translation bilingual prediction database. The content of the experiment is to conduct translation research on Chinese and English bilinguals through the machine translation bilingual prediction database. The main test is to test the accuracy of the number of sentences under different retrieval structures. Perform experimental tests on the recall rate, and analyze the data based on the test, and draw relevant conclusions.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1342\u20131346}, numpages = {5}}
@inproceedings{10.1145/3456529.3456535,title = {An Extended Bond Pricing Model and Its Algorithm Based on the Big Data Simulation}, author = {Chen Gen },year = {2021}, isbn = {9781450389112}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456529.3456535}, doi = {10.1145/3456529.3456535}, abstract = {In the previous pricing models that employs Double(or Mixed) Exponential or Asymmetrically Displaced Double Gamma (AD-DG) Jump-Diffusion Processes,the shape parameters are confined to positive integers,which may cause no solution to the parameter estimation equations or larger error. This paper, to overcome the obstacle, assumes that the asset return follows the AD-DG jump-diffusion process whose shape parameters can be any non-negative real numbers to tackle the problems.The algorithm in this article , making use of the additivity of Gamma distribution and the Monte Carlo Simulation ,overcomes the curse of dimensionality based on the big data simulation to price the bond, which is simpler and more precise. Therefore,the pricing approach in this paper has extended and improved other models mentioned above.CCS CONCEPTS \u2022 Computing methodologies\u223cModeling and simulation\u223cModel development and analysis\u223cModeling methodologies}, location = {Sanya, China}, series = {ICCDA 2021}, pages = {30\u201335}, numpages = {6}, keywords = {Bond, Pricing, AD-DG Jump-diffusion Process, Default}}
@inproceedings{10.1145/3465631.3465787,title = {Teaching Reform of \"Market Research and Analysis\" Course in Big Data Era}, author = {Xiu Juhua },year = {2021}, isbn = {9781450385015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3465631.3465787}, doi = {10.1145/3465631.3465787}, abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.}, location = {Jakarta, Indonesia}, series = {ICIMTECH 21}, pages = {1\u20134}, numpages = {4}}
@inproceedings{10.1109/CCGrid.2015.109,title = {BigDataDIRAC: deploying distributed big data applications}, author = {Fern\u00e1ndez Victor , M\u00e9ndez Victor , Pena Tom\u00e1s F. },year = {2015}, isbn = {9781479980062}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2015.109}, doi = {10.1109/CCGrid.2015.109}, abstract = {The Distributed Infrastructure with Remote Agent Control (DIRAC) software framework allows a user community to manage computing activities in different grid and cloud environments. Many communities from several fields (LHCb, Belle II, Creatis, DIRAC4EGI multiple community portal, etc.) use DIRAC to run jobs in distributed environments. Google created the MapReduce programming model offering an efficient way of performing distributed computation over large data sets. Several enterprises are providing Hadoop cloud based resources to their users, and are trying to simplify the usage of Hadoop in the cloud.Based in these two robust technologies, we have created BigDataDIRAC, a solution which gives users the opportunity to access multiple Big Data resources scattered in different geographical areas, such as access to grid resources. This approach opens the possibility of offering not only grid and cloud to the users, but also Big Data resources from the same DIRAC environment. Proof of concept is shown using three computing centers in two countries, and with four Hadoop clusters. Our results demonstrate the ability of BigDataDIRAC to manage jobs driven by dataset location stored in the Hadoop File System (HDFS) of the Hadoop distributed clusters. DIRAC is used to monitor the execution, collect the necessary statistical data, and upload the results from the remote HDFS to the SandBox Storage machine. The tests produced the equivalent of 5 days continuous processing.}, location = {Shenzhen, China}, series = {CCGRID '15}, pages = {1177\u20131180}, numpages = {4}, keywords = {hive, cloud computing, DIRAC, MapReduce, big data, hadoop, multi-cloud environment}}
@inproceedings{10.1145/3465631.3465641,title = {Sichuan Consumers' Mining of Agricultural Product Brand Value Based on Big Data}, author = {Zhuang Huaxue , Chen Shoudong , Wang Zheng , Tu Yinjiang , Xie Xiaoyun , Li Liangqiang },year = {2021}, isbn = {9781450385015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3465631.3465641}, doi = {10.1145/3465631.3465641}, abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.}, location = {Jakarta, Indonesia}, series = {ICIMTECH 21}, pages = {1\u20135}, numpages = {5}}
@inproceedings{10.5555/2675327.2675342,title = {Developing concentrations in big data analytics and software development at a small liberal arts university}, author = {Mahadev Aparna , Wurst Karl R. },year = {2015}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {In this paper, the authors share their experiences with the creation and implementation of two concentrations in an undergraduate Computer Science degree program -- one in Big Data Analytics and one in Software Development. We describe the reasoning behind these two concentrations and how these two concentrations came to exist. All Computer Science majors must choose one of these 12 credit hour concentrations after completing the core Computer Science courses. Though there have been many model undergraduate programs and graduate programs in Data Science in the nation [14], [15], [16] we believe that our concentration in Big Data Analytics is a unique offering, especially in our state. The concentration in Big Data Analytics covers predictive analytics, cloud and distributed computing and data mining. The concentration in Software Development follows a modified version of the IEEE/ACM software engineering curriculum recommendations [3]. Both concentrations finish with a semester-long capstone course.}, pages = {92\u201398}, numpages = {7}}
@inproceedings{10.1145/3465631.3465777,title = {Big Data Platform Construction of Ideological and Political Education under Information Technology}, author = {Fu Xiaoxia },year = {2021}, isbn = {9781450385015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3465631.3465777}, doi = {10.1145/3465631.3465777}, abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.}, location = {Jakarta, Indonesia}, series = {ICIMTECH 21}, pages = {1\u20134}, numpages = {4}}
@inproceedings{10.1145/3482632.3484132,title = {Ecotourism Data Mining and Spatiotemporal Differentiation Feature Extraction under the Background of Big Data}, author = {Wei Yi , Tang Liujin },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3484132}, doi = {10.1145/3482632.3484132}, abstract = {In order to study the characteristics of temporal and spatial changes in the development of ecotourism in the era of big data, based on the Baidu Index, superimposed keywords and 100 national sports leisure ecotourism demonstration sites were used to conduct a 2015-2019 ecotourism network attention search, and combined with the 2019. The ecological contribution rate was compared and analyzed. The results show that the average daily attention of ecotourism from 2015 to 2019 has increased significantly; within a week, the attention of ecotourism network is highest on Tuesday and lowest on Saturday; the peak travel season is from March to June and October to December; May Day and the Dragon Boat Festivals are highly concerned; the ecological development of Central China, South China, and Northeast China is better than eco-tourism, and the development of eco-tourism in Southwest and Northwest China is better than that of the region.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {2213\u20132219}, numpages = {7}}
@inproceedings{10.1145/3465631.3465632,title = {Innovation Research on College Student Affairs Management Based on Big Data Environment}, author = {Li Jiancai },year = {2021}, isbn = {9781450385015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3465631.3465632}, doi = {10.1145/3465631.3465632}, abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.}, location = {Jakarta, Indonesia}, series = {ICIMTECH 21}, pages = {1\u20135}, numpages = {5}}
@inproceedings{10.1145/3482632.3487514,title = {Research on the Application of Big Data Cloud Cleaning System in Physical Function Sports Training Management}, author = {Wang Wenwen },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3487514}, doi = {10.1145/3482632.3487514}, abstract = {The main purpose of physical function training is to solve the problems of many injuries, incorrect movement patterns, movement compensation and so on. In the era of big data, the scientific research field of sports training is gradually beginning to adopt the big data model for development, which will better form a new insight after data generation, collection, analysis and transformation. Therefore, this paper analyzes the application of big data intelligent cleaning system based on cloud computing in physical function training management. Students can quickly query running results through WEB query system, and administrators can download and modify students' results through WEB system. By using big data analysis technology, students are grouped according to their physical fitness test scores and BMI index, and their sports plans are formulated accordingly. A high-reliability real-time wireless receiving and sending sports monitoring system for big data is adopted, which realizes the functions of networking, unmanned and intelligent sports management.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {2783\u20132786}, numpages = {4}}
@inproceedings{10.1145/3482632.3483161,title = {Cloud Learning Platform Technology of Online Preschool Education Based on Big Data}, author = {Zhong Wenli },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483161}, doi = {10.1145/3482632.3483161}, abstract = {At present, information technology and Internet technology are developing rapidly, and more and more schools and teachers are adopting network information teaching methods to carry out teaching activities. According to incomplete statistics, people spend at least 15 years in school for education in their lifetime, which does not include the graduate level. To lay the foundation for this long period of school education is the pre-school education stage. The early childhood period is the most important stage of the intellectual development of life, and it is also the foundation of education. This paper uses Web data mining technology to provide personalized services to the online preschool education and learning platform, to study the interests and learning habits of preschool children, to choose more needed resources for students, and to provide better services. The survey report shows that more than 50% of students use the online learning platform for more than one hour a day, and even 5% of students use the cloud platform to study for more than 3 hours a day. The survey report also shows that 92% of students\u2019 parents are satisfied with the online learning platform, and 63% of them are very satisfied. As the technology becomes more and more perfect, the quality of teaching will gradually improve in the future.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1405\u20131409}, numpages = {5}}
@inproceedings{10.1145/2637002.2637056,title = {Measuring and improving data quality of media collections for professional tasks}, author = {Traub Myriam C. },year = {2014}, isbn = {9781450329767}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2637002.2637056}, doi = {10.1145/2637002.2637056}, abstract = {Carrying out research tasks on data collections is hampered, or even made impossible, by data quality issues of different types, such as incompleteness or inconsistency, and severity. We identify research tasks carried out by professional users of data collections that are hampered by inherent quality issues. We investigate what types of issues exist and how they influence these research tasks. To measure the quality perceived by professional users, we develop a quality metric. This allows us to measure the suitability of the data quality for a chosen user task. For a chosen task, we study how the data quality can be improved using crowdsourcing. We validate our quality metric by investigating whether professionals perform better on the chosen research task.}, location = {Regensburg, Germany}, series = {IIiX '14}, pages = {333\u2013335}, numpages = {3}}
@inproceedings{10.1145/3430984.3431062,title = {Linear Discriminant Analysis for Data Aggregation in Big Data Wireless Sensor Networks}, author = {Maivizhi Radhakrishnan , Yogesh Palanichamy },year = {2021}, isbn = {9781450388177}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3430984.3431062}, doi = {10.1145/3430984.3431062}, location = {Bangalore, India}, series = {CODS COMAD 2021}, pages = {417}, numpages = {1}}
@inproceedings{10.1145/3465631.3465739,title = {Marketing Principles of College Students\u2019 Employment in the Era of Big Data}, author = {Wang Xiaozhu , Song Ping , Lu Xiaoxue },year = {2021}, isbn = {9781450385015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3465631.3465739}, doi = {10.1145/3465631.3465739}, abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.}, location = {Jakarta, Indonesia}, series = {ICIMTECH 21}, pages = {1\u20134}, numpages = {4}}
@inproceedings{10.1145/2535708.2535714,title = {The impact of data quality on spatial analysis of cancer registry data: the example of missing stage at diagnosis and late-stage colorectal cancer}, author = {Sherman Recinda , Henry Kevin , Lee David },year = {2013}, isbn = {9781450325295}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2535708.2535714}, doi = {10.1145/2535708.2535714}, abstract = {Most disease surveillance systems currently geocode case data. This, coupled with advances in geographic analysis tools, has led to a rise in epidemiologic studies on distribution of disease that rely on analysis of secondary data, e.g. from cancer registries. However, while the data and tools are available for performing geospatial analyses, there are challenges with which methodologies to apply, how to interpret and translate results, and how results are impacted by data quality. The issue of data quality is the subject of this paper.Mapping cancer rates highlights spatial patterns that can help elucidate environmental, clinical, or social causality pathways that drive differences in disease burden by geographic locations. Locating areas with high rates of cancer incidence or variations by stage at diagnoses can help prioritize cancer control efforts. Once the geographic patterns of cancer are mapped, the ideal action is to follow with effective public health interventions for the high risk communities. However, before using results of spatial research to inform public health response, it is important to consider whether the results are spurious due to methodological issues, such as data quality. Missing or incorrect data can distort research conclusions and result in ineffective public health policy.Using colorectal cancer (CRC) as an example, the impact of missing stage at diagnosis on late-stage at diagnosis cluster detection is evaluated. The impact on cluster detection, area-based modeling, and distance from services analysis is described.}, location = {Orlando, Florida}, series = {HealthGIS '13}, pages = {18\u201326}, numpages = {9}, keywords = {colorectal cancer, area-based measures, stage at diagnosis, data quality, screening disparities, cluster detection}}
@inproceedings{10.1109/TCBB.2016.2581460,title = {Guest Editorial for Special Section on Big Data Computing and Processing in Computational Biology and Bioinformatics}, author = {Wang Chao , Yu Hong , Wang Aili , Xia Kai },year = {2016}, publisher = {IEEE Computer Society Press}, address = {Washington, DC, USA}, url = {https://doi.org/10.1109/TCBB.2016.2581460}, doi = {10.1109/TCBB.2016.2581460}, abstract = {The papers in this special section focus on big data computing in the field of bioinformatics and biocomputing. Big data has emerged as an important application field which has shown its huge impact in different scientific research domains. In particular, the big data bioinformatics applications such as DNA sequence analysis have posed significant challenges to the state-of-the-art processing and computing systems. With the growing explosive data scale, the collection, storage, retrieval, processing, scheduling, and visualization are key big data issues to be tackled. Up to now, many researchers have been seeking high-level parallelism using novel big data computing architectures and processing mechanisms.}, pages = {810\u2013811}, numpages = {2}}
@inproceedings{10.1145/3335484.3335505,title = {A Model Selection Method for Machine Learning by Differential Evolution}, author = {Chiu Yi-Chuan , Lin Hsing-Hung , Jou Yung-Tsan },year = {2019}, isbn = {9781450362788}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3335484.3335505}, doi = {10.1145/3335484.3335505}, abstract = {While the application of big data attracts more and more attention, machine learning algorithms are developing with each passing day, and the models produced by machine learning are increasingly diversified. The focus of big data applications has gradually shifted from model training to the prediction and inference. Choosing the most suitable model for enterprise application scenarios among many machine learning models has become a topic of research that has attracted much attention. Though ensemble methods have been proposed to discover best model by multiple training phase, studies of finding best combination within multiple modes are still few. Configuring different machine learning models with appropriate parameters and looking for parameters is an NP-hard problem, which requires an optimization algorithm. This study proposes a differential evolution algorithm to integrate multiple trained machine learning models into an appropriate model. In this paper, the regression model is taken as an example and the differential evolution algorithm is compared with the genetic algorithm. Three benchmark datasets are used to examine, and the results show that the differential evolution algorithm outperforms genetic algorithm.}, location = {Guangzhou, China}, series = {ICBDC '19}, pages = {135\u2013139}, numpages = {5}, keywords = {Machine learning, big data, differential evolution}}
@inproceedings{10.1145/3357223.3362738,title = {TagSniff: Simplified Big Data Debugging for Dataflow Jobs}, author = {Contreras-Rojas Bertty , Quian\u00e9-Ruiz Jorge-Arnulfo , Kaoudi Zoi , Thirumuruganathan Saravanan },year = {2019}, isbn = {9781450369732}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3357223.3362738}, doi = {10.1145/3357223.3362738}, abstract = {Although big data processing has become dramatically easier over the last decade, there has not been matching progress over big data debugging. It is estimated that users spend more than 50% of their time debugging their big data applications, wasting machine resources and taking longer to reach valuable insights. One cannot simply transplant traditional debugging techniques to big data. In this paper, we propose the TagSniff model, which can dramatically simplify data debugging for dataflows (the de-facto programming model for big data). It is based on two primitives -- tag and sniff -- that are flexible and expressive enough to model all common big data debugging scenarios. We then present Snoopy -- a general purpose monitoring and debugging system based on the TagSniff model. It supports both online and post-hoc debugging modes. Our experimental evaluation shows that Snoopy incurs a very low overhead on the main dataflow, 6% on average, as well as it is highly responsive to system events and users instructions.}, location = {Santa Cruz, CA, USA}, series = {SoCC '19}, pages = {453\u2013464}, numpages = {12}, keywords = {big data, data debugging, dataflow systems, distributed systems}}
@inproceedings{10.1145/2674026.2674029,title = {Twitter analytics: a big data management perspective}, author = {Goonetilleke Oshini , Sellis Timos , Zhang Xiuzhen , Sathe Saket },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2674026.2674029}, doi = {10.1145/2674026.2674029}, abstract = {With the inception of the Twitter microblogging platform in 2006, a myriad of research efforts have emerged studying different aspects of the Twittersphere. Each study exploits its own tools and mechanisms to capture, store, query and analyze Twitter data. Inevitably, platforms have been developed to replace this ad-hoc exploration with a more structured and methodological form of analysis. Another body of literature focuses on developing languages for querying Tweets. This paper addresses issues around the big data nature of Twitter and emphasizes the need for new data management and query language frameworks that address limitations of existing systems. We review existing approaches that were developed to facilitate twitter analytics followed by a discussion on research issues and technical challenges in developing integrated solutions.}, pages = {11\u201320}, numpages = {10}}
@inproceedings{10.1145/3208040.3225055,title = {Cambrian explosion of computing and big data in the post-moore era}, author = {Matsuoka Satoshi },year = {2018}, isbn = {9781450357852}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3208040.3225055}, doi = {10.1145/3208040.3225055}, abstract = {The so-called \"Moore's Law\", by which the performance of the processors will increase exponentially by factor of 4 every 3 years or so, is slated to be ending in 10--15 year timeframe due to the lithography of VLSIs reaching its limits around that time, and combined with other physical factors. We are now embarking on a project to revolutionize the total system architectural stack in a holistic fashion in the Post-Moore era, from devices and hardware, abstracted by system software and programming models and languages, and optimized according to the device characteristics with new algorithms and applications that exploit them. Such systems will have multitudes of varieties according to the matching characteristics of applications to the underlying architecture, leading to what can be metaphorically described as Cambrian Explosion of computing systems. The diverse elements of such systems will be interconnected with next-generation terabit optics and networks, allowing metropolitan-scale computing infrastructure that would truly realize high performance parallel and distributed computing. However, which algorithms and applications would benefit the most from such future computing, given that some physical constants, e.g., communication latency, cannot be improved? We speculate on some of the scenarios that would change the nature of current Cloud-centric infrastructures towards the Post-Moore era.}, location = {Tempe, Arizona}, series = {HPDC '18}, pages = {105}, numpages = {1}}
@inproceedings{10.1145/3352460.3358266,title = {Towards Efficient NVDIMM-based Heterogeneous Storage Hierarchy Management for Big Data Workloads}, author = {Chen Renhai , Shao Zili , Liu Duo , Feng Zhiyong , Li Tao },year = {2019}, isbn = {9781450369381}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3352460.3358266}, doi = {10.1145/3352460.3358266}, abstract = {In this paper, we propose a holistic solution to address several important and challenging issues in storage data management in light of emerging NVDIMM-based architecture: namely, new performance modeling, NVDIMM-based migration, and architectural support for NVDIMMs on migration optimization. In particular, a novel NVDIMM-based heterogeneous storage performance model is proposed to effectively address bus contention issues caused by placing NVDIMMs on the memory bus. We also develop an NVDIMM-based lazy migration scheme to effectively minimize adverse effects caused by memory traffic interferences during storage data management processes. Finally, the NVDIMM-based architectural support for migration optimization is proposed to increase channel parallelism in the destination NVDIMMs and bypass buffer caches in the source NVDIMMs, so that the impact of memory traffic can be alleviated. We present detailed evaluation and analysis to quantify how well our techniques can enhance the I/O performances of big workloads via efficient heterogeneous storage hierarchy management. Our experimental results show that overall the proposed techniques yield up to 98% performance improvement over the state-of-the-art techniques.}, location = {Columbus, OH, USA}, series = {MICRO '52}, pages = {849\u2013860}, numpages = {12}, keywords = {NVDIMM, machine learning, heterogeneous storage, bus contention}}
@inproceedings{10.5555/3382225.3382460,title = {Multilayer value metrics using lexical link analysis and game theory for discovering innovation from big data and crowd-sourcing}, author = {Zhao Ying , Zhou Charles C. , Bellonio Jennie K. },year = {2018}, isbn = {9781538660515}, publisher = {IEEE Press}, abstract = {We demonstrated a machine learning and artificial intelligence method, i.e., lexical link analysis (LLA) to discover different layers of semantic network that contribute to innovative ideas from big data. The LLA is an unsupervised machine learning paradigm that does not require manually labeled training data. Multilayer value metrics are defined based on game theory for LLA. We showed the following results: 1) the value metrics generated from LLA in a use case of an internet game and crowd-sourcing; 2) the results from LLA are validated and correlated with the ground truth; 3) the game-theoretic LLA can help an information provider to present the information in the most valuable way. The information presentation can solve a problem (e.g., a search request of innovation) that no other information providers can solve (i.e., expertise). In addition, it ties also to a broader context that the unique value can propagate through the consensus. Based on the game-theoretic LLA, an information provider should not always present expertise content or authoritative content but rather with a mixed strategy where each type of content is presented with certain probabilities for the best value overall.}, location = {Barcelona, Spain}, series = {ASONAM '18}, pages = {1145\u20131151}, numpages = {7}, keywords = {crowd-sourcing, big data, unsupervised learning, pareto efficient, social welfare, nash equilibrium, pareto superior, lexical link analysis, game theory}}
@inproceedings{10.1145/2351316,title = {Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},year = {2012}, isbn = {9781450315470}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {Recent years have witnessed a dramatic increase in our ability to collect data from various sensors, devices, in different formats, from independent or connected applications. This data flood has outpaced our capability to process, analyze, store and understand these datasets. Consider the Internet data. The web pages indexed by Google were around one million in 1998, but quickly reached 1 billion in 2000 and have already exceeded 1 trillion in 2008. This rapid expansion is accelerated by the dramatic increase in acceptance of social networking applications, such as Facebook, Twitter, Weibo, etc., that allow users to create contents freely and amplify the already huge Web volume. Furthermore, with mobile phones becoming the sensory gateway to get real-time data on people from different aspects, the vast amount of data that mobile carrier can potentially process to improve our daily life has significantly outpaced our past CDR (call data record)- based processing for billing purposes only. It can be foreseen that Internet of things (IoT) applications will raise the scale of data to an unprecedented level. People and devices (from home coffee machines to cars, to buses, railway stations and airports) are all loosely connected. Trillions of such connected components will generate a huge data ocean, and valuable information must be discovered from the data to help improve quality of life and make our world a better place. For example, after we get up every morning, in order to optimize our commute time to work and complete the optimization before we arrive at office, the system needs to process information from traffic, weather, construction, police activities to our calendar schedules, and perform deep optimization under the tight time constraints. In all these applications, we are facing significant challenges in leveraging the vast amount of data, including challenges in (1) system capabilities (2) algorithmic design (3) business models.}, location = {Beijing, China}}
@inproceedings{10.5555/2648668.2648748,title = {An ultralow-power memory-based big-data computing platform by nonvolatile domain-wall nanowire devices}, author = {Wang Yuhao , Yu Hao },year = {2013}, isbn = {9781479912353}, publisher = {IEEE Press}, abstract = {As one recently introduced non-volatile memory (NVM) device, domain-wall nanowire (or race-track) has shown potential for main memory storage but also computing capability. In this paper, the domain-wall nanowire is studied for a memory-based computing platform towards ultra-low-power big-data processing. One domain-wall nanowire based logic-in-memory architecture is proposed for big-data processing, where the domain-wall nanowire memory is deployed as main memory for data storage as well as XOR-logic for comparison and addition operations. The domain-wall nanowire based logic-in-memory circuits are evaluated by SPICE-level verifications. Further evaluated by applications of general-purpose SPEC2006 benchmark and also web-searching oriented Phoenix benchmark, the proposed computing platform can exhibit a significant power saving on both main memory and ALU under the similar performance when compared to CMOS based designs.}, location = {Beijing, China}, series = {ISLPED '13}, pages = {329\u2013334}, numpages = {6}}
@inproceedings{10.1145/2751205.2751230,title = {Towards Lightweight and Swift Storage Resource Management in Big Data Cloud Era}, author = {Zhou Ruijin , Chen Huixiang , Li Tao },year = {2015}, isbn = {9781450335591}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2751205.2751230}, doi = {10.1145/2751205.2751230}, abstract = {Workload IO behavior in modern data centers is fluctuating and unpredictable due to the rapidly adopted, public cloud environment. Nevertheless, existing storage resource management systems, such as VMware SDRS, are incapable of performing real time policy-based storage management due to the high cost of migrating large size virtual disks. Hence, the traditional storage management schemes become ineffective due to the lack of quick response to the frequent IO bursts and the inaccurate storage latency prediction in the light of a highly fluctuating environment. To address the aforementioned issues, we propose LightSRM, which can work properly in a time-variant cloud environment. To mitigate the storage migration cost, we leverage copy-on-write/read snapshots to redirect the IO requests without moving the virtual disk. To support snapshots in storage management, we also build a performance model specifically for snapshots. We employ exponentially weighted moving average with adjustable sliding window to provide quick and accurate performance prediction. Furthermore, we propose a hybrid management scheme, which can dynamically choose either snapshot or migration for fastest performance tuning. We build our prototype in a QEMU/KVM based virtualized environment. Our empirical evaluation results show that snapshot can redirect IO requests in a faster manner than migration can do when the virtual disk size is large. Besides, snapshot method has less disk performance impact on the applications. By employing hybrid snapshot/migration method, LightSRM yields less overall latency, better load balance, and less IO traffic overhead.}, location = {Newport Beach, California, USA}, series = {ICS '15}, pages = {133\u2013142}, numpages = {10}, keywords = {storage virtualization, distributed storage management, snapshot, storage migration}}
@inproceedings{10.1145/3508072.3508196,title = {A Big Data framework based on Apache Spark for Industry-specific Lexicon Generation for Stock Market Prediction}, author = {Angioni Simone , Carta Salvatore , Consoli Sergio , Reforgiato Recupero Diego , Stanciu Maria Madalina },year = {2021}, isbn = {9781450387347}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3508072.3508196}, doi = {10.1145/3508072.3508196}, location = {Dubai, United Arab Emirates}, series = {ICFNDS 2021}, pages = {616\u2013624}, numpages = {9}, keywords = {Natural Language Processing, Big Data, Machine Learning, Apache Spark, Stock Market Forecasting, Financial Technology.}}
@inproceedings{10.1145/3482632.3483087,title = {A Comparative Study of It Education in Chinese and Japanese Universities Based on Big Data Analysis}, author = {Nan Xiang , Chen Huilin },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483087}, doi = {10.1145/3482632.3483087}, abstract = {Over time, various novel methods and techniques have been proposed. Over the years, three technologies have emerged most frequently, namely big data, cloud computing, and artificial intelligence. Big data and cloud computing are actually extensions and paving the way for artificial intelligence technology. Especially in today's society, with the increase of energy resources, the application of big data is more and more extensive, and in education big data is also used in it. Therefore, it is now more and more convenient for us to know the excellence and gap between universities in various countries and regions. Therefore, the purpose of this paper is to study the comparative in the context of large data, Chinese and Japanese universities have the advantages and disadvantages of it education. Therefore, after consulting the relevant parties about it essence and it education, as well as the teaching background and teaching attitude of Chinese and Japanese universities, this paper integrates the information found by using big data-related algorithms, then analyzes and processes the relevant data using hierarchical bat algorithms, and then produces the experimental results we need. Experimental results show that the use of big data and hierarchical bat algorithms can better study the characteristics of it education in Chinese and Japanese universities.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1079\u20131083}, numpages = {5}}
@inproceedings{10.1145/3482632.3483187,title = {Construction of Practice Teaching System in Big Data Application and Management Major based on Information Technology}, author = {Wang Xiaozhu , Ma Yingqiao },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483187}, doi = {10.1145/3482632.3483187}, abstract = {Big data management and application specialty is an emerging major in our school of information and business management. Based on the Internet plus and big data era, we mainly study the application of big data analysis theory and methods in economic management and the management and management of big data. This paper aims to explore how to improve the innovation and practical ability of students in this major through practical teaching. This paper mainly elaborates from four aspects, namely, the basic principles of practical teaching, the path of students' ability improvement, the construction of the completed practical teaching system and the application of practical teaching in this major.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1516\u20131519}, numpages = {4}}