@inproceedings{10.1145/3216122.3216154,title = {Efficient Big Data Clustering}, author = {Ianni Michele , Masciari Elio , Mazzeo Giuseppe M. , Zaniolo Carlo },year = {2018}, isbn = {9781450365277}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3216122.3216154}, doi = {10.1145/3216122.3216154}, abstract = {The need to support advanced analytics on Big Data is driving data scientist' interest toward massively parallel distributed systems and software platforms, such as Map-Reduce and Spark, that make possible their scalable utilization. However, when complex data mining algorithms are required, their fully scalable deployment on such platforms faces a number of technical challenges that grow with the complexity of the algorithms involved. Thus algorithms, that were originally designed for a sequential nature, must often be redesigned in order to effectively use the distributed computational resources. In this paper, we explore these problems, and then propose a solution which has proven to be very effective on the complex hierarchical clustering algorithm CLUBS+. By using four stages of successive refinements, CLUBS+ delivers high-quality clusters of data grouped around their centroids, working in a totally unsupervised fashion. Experimental results confirm the accuracy and scalability of CLUBS+.}, location = {Villa San Giovanni, Italy}, series = {IDEAS '18}, pages = {103\u2013109}, numpages = {7}, keywords = {Spark, Clustering, Big Data}}
@inproceedings{10.1145/2656346.2656358,title = {Big data architecture evolution: 2014 and beyond}, author = {Mohammad Atif , Mcheick Hamid , Grant Emanuel },year = {2014}, isbn = {9781450330282}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2656346.2656358}, doi = {10.1145/2656346.2656358}, abstract = {This paper aims at developing the Big Data Architecture, and its relation with Analytics, Cloud Services as well as Business Intelligence. The chief aim from all mentioned is to enable the Enterprise Architecture and the Vision of an Organizational target to utilize all the data they are ingesting and regressing data for their short-term or long-terms analytical needs, while making sure that they are addressing during the design phase of such data architecture for both directly and indirectly related stakeholder. Since all stakeholders have their relative interests to utilize the transformed data-sets. This paper also identifies most of the Big Data Architecture, threat analysis within a Big Data System and Big Data Analytic Roadmaps, in terms of smaller components by conducting a gap-analysis that has significant importance as Baseline Big Data Architecture, targeting the end resultant Architectures, once the distillation process of main Big Data Architecture is completed by the Data Architects.}, location = {Montreal, QC, Canada}, series = {DIVANet '14}, pages = {139\u2013144}, numpages = {6}, keywords = {cloud computing, big data}}
@inproceedings{10.1145/3447879.3447883,title = {Approximate computation for big data analytics}, author = {Ma Shuai , Huai Jinpeng },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3447879.3447883}, doi = {10.1145/3447879.3447883}, abstract = {Over the past a few years, research and development has made significant progresses on big data analytics. A fundamental issue for big data analytics is the efficiency. If the optimal solution is unable to attain or unnecessary or has a price to high to pay, it is reasonable to sacrifice optimality with a \"good\" feasible solution that can be computed efficiently. Existing approximation techniques can be in general classified into approximation algorithms, approximate query processing for aggregate SQL queries and approximation computing for multiple layers of the system stack. In this article, we systematically introduce approximate computation, i.e., query approximation and data approximation, for efficient and effective big data analytics. We explain the ideas and rationales behind query and data approximation, and show efficiency can be obtained with high effectiveness, and even without sacrificing for effectiveness, for certain data analytic tasks.}, pages = {1\u20138}, numpages = {8}}
@inproceedings{10.1145/3291801.3291827,title = {Research on the effectiveness evaluation of big data in combat simulation}, author = {Hu Xinwu , Luo Pengcheng , Zhang Xiaonan , Wang Jun , Zhou Tianren },year = {2018}, isbn = {9781450364768}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3291801.3291827}, doi = {10.1145/3291801.3291827}, abstract = {With the development of complex system simulation techniques, computational capabilities, and data management capabilities, the simulation results tend to be big data. There are also high-dimension, high-redundancy, and high-correlation issues among indexes. Based on the above background, a two-layer Autoencoder neural network is used for feature extraction and dimensionality reduction. Then, 10 deep neural network models are established for index learning. The experimental results show that the 32-layer Resent network works best for low-dimensional data effectiveness evaluation.}, location = {Weihai, China}, series = {ICBDR 2018}, pages = {70\u201375}, numpages = {6}, keywords = {system simulation, big data, neural network, effectiveness evaluation, data dimension reduction}}
@inproceedings{10.1145/3195528.3195533,title = {Big data: deep learning for detecting malware}, author = {Masabo Emmanuel , Kaawaase Kyanda Swaib , Sansa-Otim Julianne },year = {2018}, isbn = {9781450357197}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3195528.3195533}, doi = {10.1145/3195528.3195533}, abstract = {Malicious software, commonly known as malware are constantly getting smarter with the capabilities of undergoing self-modifications. They are produced in big numbers and widely deployed very fast through the Internet-capable devices. This is therefore a big data problem and remains challenging in the research community. Existing detection methods should be enhanced in order to effectively deal with today's malware. In this paper, we propose a novel real-time monitoring, analysis and detection approach that is achieved by applying big data analytics and machine learning in the development of a general detection model. The learnings achieved through big data render machine learning more efficient. Using the deep learning approach, we designed and developed a scalable detection model that brings improvement to the existing solutions. Our experiments achieved an accuracy of 97% and ROC of 0.99.}, location = {Gothenburg, Sweden}, series = {SEiA '18}, pages = {20\u201326}, numpages = {7}, keywords = {malware detection, big data analytics, machine learning, deep learning}}
@inproceedings{10.1145/3090354.3090370,title = {Towards Clustering Validation in Big Data Context}, author = {Zerabi Soumeya , Meshoul Souham , Merniz Amina , Melal Radia },year = {2017}, isbn = {9781450348522}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3090354.3090370}, doi = {10.1145/3090354.3090370}, abstract = {Clustering1is an essential task in many areas such as machine learning, data mining and computer vision among others. Cluster validation aims to assess the quality of partitions obtained by clustering algorithms. Several indexes have been developed for cluster validation purpose. They can be external or internal depending on the availability of ground truth clustering. This paper deals with the issue of cluster validation of large data set. Indeed, in the era of big data this task becomes even more difficult to handle and requires parallel and distributed approaches. In this work, we are interested in external validation indexes. More specifically, this paper proposes a model for purity based cluster validation in parallel and distributed manner using Map-Reduce paradigm in order to be able to scale with increasing dataset sizes.The experimental results show that our proposed model is valid and achieves properly cluster validation of large datasets.}, location = {Tetouan, Morocco}, series = {BDCA'17}, pages = {1\u20136}, numpages = {6}, keywords = {partitioned clustering, distributed computing, cluster validation index, purity index, parallel computing}}
@inproceedings{10.1145/3341620.3341622,title = {Review and Investigate the Mapping Knowledge Domain of Financial Big Data Research}, author = {Zhou Wei , Luo Danxue , Chen Jin },year = {2019}, isbn = {9781450360913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341620.3341622}, doi = {10.1145/3341620.3341622}, abstract = {In today's society, large data is widely used in various fields owing to its quantitative and objective characteristics. Therefore, there have been an increasing number of investigations researching on financial issues related to big data in recent years. It is believed that analyzing the status quo and emerging trends of financial research and big data research and the beginner who are interested in financial and big data research. To do so, this paper provides the mapping knowledge domain of financial and big data research based on 724 papers on Web of Science (WoS) from 1992 to 2018 by using CiteSpace, which is an effective tool for scientometric studies. The visualization analyses of cited reference cluster, collaborations networks, author co-citation and timeline view are presented in this study to show the research streams and the papers that made significant theoretical contributions. Also, the authors in this research area are analyzed in detail. Besides, the specific hot spots and emerging trends can be identified. There are two contributions in this study. Firstly, we give the comprehensive investigation about the status quo and emerging trends of financial and big data research in the recent 26 years. Secondly, we make the development of financial research and big data easier and direct to learn for beginners.}, location = {Hong Kong, Hong Kong}, series = {BDE 2019}, pages = {1\u20139}, numpages = {9}, keywords = {Big data, Financial research, Science mapping, Knowledge domain visualization}}
@inproceedings{10.1145/3474944.3474952,title = {Privacy Security Status and Countermeasures in the Era of Big Data}, author = {Huang Yuanpeng },year = {2021}, isbn = {9781450389280}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3474944.3474952}, doi = {10.1145/3474944.3474952}, abstract = {Big data is a data set which contains a lot of information and has high value. In recent years, big data technology is gradually popularized and applied, which brings great convenience to people. But it also leads to many problems, one of which is the hidden danger of privacy leakage. The purpose of this paper is to find the possible causes of privacy leakage. Based on the research results of others, this study collected the data through the Internet and conducted some investigations. It can be concluded that there are three main reasons leading to big data privacy problems. First, individuals lack privacy protection awareness and awareness of the harm of privacy leakage. Second, enterprises lack legal awareness, do not protect the user's privacy, and even intentionally violate the law to intercept the user's privacy information. Third, with the development of data industry, government is required to update the privacy protection laws in time. Legal Research on how to protect users' privacy in big data has also been ignored.}, location = {Singapore, Singapore}, series = {BDET 2021}, pages = {50\u201353}, numpages = {4}, keywords = {Face change, Mobile APP, Privacy, Big data, AI}}
@inproceedings{10.1145/2331042.2331058,title = {Interactive analysis of big data}, author = {Heer Jeffrey , Kandel Sean },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2331042.2331058}, doi = {10.1145/2331042.2331058}, abstract = {New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.}, pages = {50\u201354}, numpages = {5}}
@inproceedings{10.1145/3322134.3322149,title = {Big Data and Higher Vocational and Technical Education: Green Tourism Curriculum}, author = {Yang Qiang , Li Jian , Zou Xiaohui },year = {2019}, isbn = {9781450361866}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3322134.3322149}, doi = {10.1145/3322134.3322149}, abstract = {This paper aims to explore the road to innovation in big data and higher vocational and technical education with the green tourism curriculum as an example. The method is as follows: first, introducing a statistical-based machine learning method to deal with large probability events, secondly, introducing a human-computer interaction interface technology to deal with small probability events, and third, introducing an expert knowledge acquisition technique to deal with special exceptions. It is characterized by a combination of three approaches, focusing on the interdisciplinary, cross-domain and cross-industry smart system construction, and converging to the knowledge module of the green tourism curriculum. The result is: not only highlights the comprehensive innovative concept of the green tourism curriculum, but also forms a smart guide system that combines personalization and standardization, through conceptual maps, knowledge graphs and methodological tools that express scientific principles, combined with typical examples and representative figures and featured scenic spots, and a new paradigm for computer-assisted instruction. The significance lies in: not only is it conducive to the creation of quality courses, but it is also beneficial to the teachers and students to theoretically and practically carry out the characteristics of the green tourism curriculum, namely: a series of problems, difficulties and pain points for international and domestic tourism, forming a reasonable division of labor is necessary to further develop the green tourism curriculum and its supporting smart systems of interpersonal, human-machine, inter-machine, and machine-to-person.}, location = {London, United Kingdom}, series = {ICBDE'19}, pages = {108\u2013112}, numpages = {5}, keywords = {E-education, Data Management, Big Data Applications}}
@inproceedings{10.1145/3011286.3011307,title = {Environmental Big Data: a systematic mapping study}, author = {Castelluccia Daniela , Caldarola Enrico G. , Boffoli Nicola },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3011286.3011307}, doi = {10.1145/3011286.3011307}, abstract = {Big data sets and analytics are increasingly being used by government agencies, non-governmental organizations, and privatecompanies to forward environmental protection. Improving energy efficiency, promoting environmental justice, tracking climate change, and monitoring water quality are just a few of the objectives being furthered by the use of Big Data. The authors provide a more detailed analysis of the emerging evidence-based insights on Environmental Big Data (EBD), by applying the well-defined method of systematic mapping. The analysis of results throws light on the current open issues of Environmental Big Data. Moreover, different facets of the study can be combined nto answer more specific research questions. The report reveals the need for more empirical research able to provide new metrics measuring efficiency and effectiveness of the proposed analytics and new methods and tools supporting data processing workflow in EBD}, pages = {1\u20134}, numpages = {4}, keywords = {Data Management, Big Data, Data Integration, Systematic Mapping, Environment}}
@inproceedings{10.1145/3341620.3341635,title = {Application of Online/Offline Sales Big Data in Household Medical Device Industry}, author = {Liu Lixia , Hu Gang , Zhang Min , Xue Xiaoqiao },year = {2019}, isbn = {9781450360913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341620.3341635}, doi = {10.1145/3341620.3341635}, abstract = {In this paper, we discuss the current situation of the application of big data in the sales of household medical devices industry. Four problems existing in current commercial big data mining are analyzed. We put forward two main targets of big sales data in this area which are on-line and off-line mutual drainage and accurate marketing maximization. Subsequently we analyze the paths to achieve these goals which include introduction of professional data analysts, establishment of sales data analysis model, online and offline data access and pilot operation of analysis model. Finally a simple conclusion of the whole paper is given.}, location = {Hong Kong, Hong Kong}, series = {BDE 2019}, pages = {63\u201367}, numpages = {5}, keywords = {Sales, Big Data, Precision Marketing, Household Medical Devices}}
@inproceedings{10.1145/3322134.3322150,title = {Big Data and Higher Vocational and Technical Education: Green Food and Its Industry Orientation}, author = {Li Jian , Yang Qiang , Zou Xiaohui },year = {2019}, isbn = {9781450361866}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3322134.3322150}, doi = {10.1145/3322134.3322150}, abstract = {This paper aims to focus on a series of issues related to green food and its industry orientation through big data and higher vocational and technical education technologies. The method consists of three steps the first step is to lock in the research objectives, namely: a series of problems in green food and its industry orientation; the second step is to distinguish three types of problems, namely: the first category is a large probability event, the predictable problem is directly solved by machine learning; the second category is a small probability event, which relies solely on machine automatic processing or batch processing can't solve problems, using a variety of human-computer interaction methods to deal with; the third category is cases and very special exceptions, usually only rely on the corresponding human experts to find the ways, and then, through the knowledge acquisition path to develop a dedicated artificial intelligence system. The third step is to incorporate them into the actual classroom teaching practice, or find the special daily life circle, it can be artificially set, even virtual, and tested in various application environments. The result is: through the teachers and students to continue to explore a series of issues related to green food and its industry forming the characteristics of big data and higher vocational and technical education technology. The significance is that is, a series of results of the research of the smart system can be directly used to study a series of problems focusing on green food and its industrial orientation.}, location = {London, United Kingdom}, series = {ICBDE'19}, pages = {118\u2013123}, numpages = {6}, keywords = {Big Data Applications, E-education, Data Management}}
@inproceedings{10.1145/3305275.3305285,title = {Considerations of the Paradigms of Urban Design Teaching Application about Big Data}, author = {Yang Junyan , Schultz Henrik , Zheng Yi , Cao Jun },year = {2018}, isbn = {9781450365703}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3305275.3305285}, doi = {10.1145/3305275.3305285}, abstract = {The refinement and rationalization of urban design has become an important trend of restructuring and development during the crucial stage of China's urban transformation. At the same time, big data technology is developing rapidly, with the availability of various types of high-precision data being gradually increased. The combination of big data and urban design plays an important role in the urban design of transition period. This paper describes the design of urban education system under the big data platform. With the traditional teaching model, which include four stage (research, topic, design and expression), big data technologies are divided into four aspects to consider how to apply the big data to the teaching of urban design. Specifically summarized: big data \"accelerate\" researching, big data \"secondary\" analysis, big data \"enhanced\" design and big data of \"optimization\" expression. The aim of this research is to let the traditional teaching system and cutting-edge technology of large data collide with each other fusion, which can not only enrich the existing education system, but also establish a complete technical structure. On this basis, it can help students establish and improve the construction, and the ability to learn new technologies, to promote the progress of urban design techniques.}, location = {Hong Kong, Hong Kong}, series = {ISBDAI '18}, pages = {48\u201352}, numpages = {5}, keywords = {urban design, Big data, teaching system, application Paradigm}}
@inproceedings{10.1145/3323878.3325802,title = {Parallel RDF generation from heterogeneous big data}, author = {Haesendonck Gerald , Maroy Wouter , Heyvaert Pieter , Verborgh Ruben , Dimou Anastasia },year = {2019}, isbn = {9781450367660}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3323878.3325802}, doi = {10.1145/3323878.3325802}, abstract = {To unlock the value of increasingly available data in high volumes, we need flexible ways to integrate data across different sources. While semantic integration can be provided through RDF generation, current generators insufficiently scale in terms of volume. Generators are limited by memory constraints. Therefore, we developed the RMLStreamer, a generator that parallelizes the ingestion and mapping tasks of RDF generation across multiple instances. In this paper, we analyze what aspects are parallelizable and we introduce an approach for parallel RDF generation. We describe how we implemented our proposed approach, in the frame of the RMLStreamer, and how the resulting scaling behavior compares to other RDF generators. The RMLStreamer ingests data at 50% faster rate than existing generators through parallel ingestion.}, location = {Amsterdam, Netherlands}, series = {SBD '19}, pages = {1\u20136}, numpages = {6}, keywords = {linked data, big data, semantic web, RDF generation}}
@inproceedings{10.1145/2699026.2699136,title = {Big Data Security and Privacy}, author = {Thuraisingham Bhavani },year = {2015}, isbn = {9781450331913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2699026.2699136}, doi = {10.1145/2699026.2699136}, abstract = {This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.}, location = {San Antonio, Texas, USA}, series = {CODASPY '15}, pages = {279\u2013280}, numpages = {2}, keywords = {privacy, big data, security}}
@inproceedings{10.1145/2640087.2644173,title = {Fast Mode Regression in Big Data Analysis}, author = {Yu Keming , Aristodemou Katerina , Becker Frauke , Lord Joann },year = {2014}, isbn = {9781450328913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2640087.2644173}, doi = {10.1145/2640087.2644173}, abstract = {Grasping hidden patterns and unknown correlations of big data quickly and precisely is required in our time. Developing methodology for big data is the focus of big data analysis. Methods do not need to be expensive but meaningful and easy to implement. This paper proposes a mode-based fast pattern grasping method and mode-based fully parametric regression for big data analysis. By taking more than a decade of the 'Health Survey for England' data as an example, we apply the method to uncover the dependency of a response variable on other factors. The dependency relation can be used to check the effect of covariances on the pattern and make pattern or trend prediction easily and quickly.}, location = {Beijing, China}, series = {BigDataScience '14}, pages = {1\u20133}, numpages = {3}, keywords = {mode regression, BMI, mode, Gamma distribution}}
@inproceedings{10.1145/3264560.3266429,title = {Scalable Privacy-Preserving Big Data Management and Analytics: Where We Are and Where We Are Going}, author = {Cuzzocrea Alfredo },year = {2018}, isbn = {9781450364744}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3264560.3266429}, doi = {10.1145/3264560.3266429}, abstract = {While several research efforts have been developed in the context of privacy-preserving big data management and analytics re- cently, relevant challenges arise when such models, techniques and algorithms must be delivered on top of massive, distributed big data repositories. This problem opens the door to the design of innovative models, techniques and algorithms that, contrary to actual proposals, are able to inject the scalability feature during the privacy-preserving big data management and analytics phase. On the basis of these considerations, this paper provides an overview on actual problems and limitations of state-of-the-art techniques, along with the proposal of an effective framework for supporting scalable privacy-preserving big data management and analytics.}, location = {Barcelona, Spain}, series = {ICCBDC'18}, pages = {52\u201356}, numpages = {5}, keywords = {Scalable Privacy-Preserving Big Data Management and Analysis, Privacy- Presering Big Data Frameworks, Privacy-Preserving Big Data Management and Analysis}}
@inproceedings{10.1145/3451400.3451411,title = {Literature Research on Patriotism Education in Colleges and Universities Based on Big Data}, author = {XU Ling },year = {2021}, isbn = {9781450389389}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3451400.3451411}, doi = {10.1145/3451400.3451411}, abstract = {This paper studies patriotism education in China's colleges and universities through big data, and analyzes the annual publication, subject distribution, highly cited literature, literature subject, organization distribution and keyword co-occurrence clustering of CNKI literature. It is found that there are a large number of domestic literatures on patriotism education in Colleges and universities, and there is a trend of rapid development. The research is mainly based on the ideological and political education, national defense education, socialist core values education, red culture education, national spirit and so on. In the future research, we should make full use of big data to provide intellectual support for patriotic education, combine big data with traditional patriotic education, and carry out patriotic education activities more accurately and effectively.}, location = {London, United Kingdom}, series = {ICBDE 2021}, pages = {69\u201374}, numpages = {6}, keywords = {patriotism education, big data, CiteSpace, CNKI}}
@inproceedings{10.1145/3481646.3481648,title = {A Comparative Study of MongoDB, ArangoDB and CouchDB for Big Data Storage}, author = {Mavrogiorgos Konstanitnos , Kiourtis Athanasios , Mavrogiorgou Argyro , Kyriazis Dimosthenis },year = {2021}, isbn = {9781450390408}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3481646.3481648}, doi = {10.1145/3481646.3481648}, abstract = {A distinctive aspect of the current era is the ferocious amount of data that is generated and processed in a daily basis. There is no wonder that this epoch is generally characterized as the \u201cEra of Big Data\u201d. Thus, many enterprises and research initiatives strive to find a way to effectively and efficiently collect, store and analyze Big Data in order to improve their services and make efficient decisions. Those approaches refer to several domains such as healthcare, transportation, governance, or insurance. Towards this direction, in this paper we contribute into the selection of the most appropriate database for efficiently storing and retrieving Big Data. More specifically, taking into account the nature of Big Data and the main categories of databases that currently exist, three (3) NoSQL document-based databases were considered for this comparative study, namely the ArangoDB, the MongoDB and the CouchDB. The performance of these databases was measured based on specific metrics and criteria, including the total execution time for the same CRUD operations and their corresponding demands for resources, concluding to the most suitable database for storing Big Data.}, location = {Liverpool, United Kingdom}, series = {ICCBDC '21}, pages = {8\u201314}, numpages = {7}, keywords = {MongoDB, Storage, Big Data, CouchDB, ArangoDB, Document-based}}
@inproceedings{10.1145/3220199.3220220,title = {SAT-based Important Data Reliability Enhancement Model for Big Data Storage}, author = {Hong Huang , Khan Latifur , Xiaojuan Liao },year = {2018}, isbn = {9781450364263}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3220199.3220220}, doi = {10.1145/3220199.3220220}, abstract = {Disk reliability is a serious problem in the big data foundation environment. Although the reliability of disk drives has greatly improved over the past few years, they are still the most vulnerable core components in the server. If they fail, the result can be catastrophic: it can take some days to recover data, sometimes data lost forever. These are unacceptable for some important data. XOR parity is a typical method to generate reliability syndrome, thus improving the reliability of the data. In practice, we find that the data is still likely to be lost. In most storage systems reliability improvements are achieved through the allocation of additional disks in Redundant Arrays of Independent Disks (RAID), which will increase the hardware costs, thus it will be very difficult for cost-constrained environments. Therefore, how to improve the data integrity without raising the hardware cost has aroused much interest of big data researchers. This challenge is when creating non-traditional RAID geometries, care must be taken to respect data dependence relationships to ensure that the new RAID strategy improves reliability, which is a NP-hard problem. In this paper, we present an approach for characterizing these challenges using high-dimension variants of the n-queens problem that enables performable solutions via the SAT solver MiniSAT, and use the greedy algorithm to analyze the queen's attack domain, as a basis for reliability syndrome generation. A large number of experiments show that the approach proposed in this paper is feasible in software-defined data centers and the performance of the algorithm can meet the current requirements of the big data environment.}, location = {Shenzhen, China}, series = {ICBDC '18}, pages = {20\u201326}, numpages = {7}, keywords = {Boolean Satisfiability Problem, n-queens, data reliability, big data, NP-hard}}
@inproceedings{10.1145/3555962.3555969,title = {Unstructured Over Structured, Big Data Analytics and Applications In Accounting and Management}, author = {Faccia Alessio , Cavaliere Luigi Pio Leonardo , Petratos Pythagoras , Mosteanu Narcisa Roxana },year = {2022}, isbn = {9781450396578}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3555962.3555969}, doi = {10.1145/3555962.3555969}, abstract = {Generating value from big data is a task that requires models\u2019 preparation and use of advanced technologies but which, above all, is based on the ability to extract, manage and analyse data. These processes\u2019 effectiveness depends on the data's quality and their structured or unstructured nature. We are witnessing a growing number of applications based on unstructured data mining in the accounting and management fields. This research aims to demonstrating that despite the traditional association between accounting and quantitative analyses (expected to be based mainly on structured financial data). The findings show that several useful applications now rely on unstructured data in this field. A basic analysis of the cybersecurity risks is also presented, along with mitigating strategies to allow companies to comply with current regulations such as the GDPR. The result might appear surprising from the business perspective, but it is not from a data science perspective. In conclusion the growing number of unsctructured data business applications should orientate a better understanding of their potential and target better training of finance specialist on data processing skills.}, location = {Birmingham, United Kingdom}, series = {ICCBDC '22}, pages = {37\u201341}, numpages = {5}, keywords = {Management, Unstructured data, Structured data, Accounting, Analytics, Applications, Big Data}}
@inproceedings{10.1145/2351316.2351318,title = {Big data, big business: bridging the gap}, author = {Gopalkrishnan Vivekanand , Steier David , Lewis Harvey , Guszcza James },year = {2012}, isbn = {9781450315470}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2351316.2351318}, doi = {10.1145/2351316.2351318}, abstract = {Business analytics, occupying the intersection of the worlds of management science, computer science and statistical science, is a potent force for innovation in both the private and public sectors. The successes of business analytics in strategy, process optimization and competitive advantage has led to data being increasingly recognized as a valuable asset in many organizations. In recent years, thanks to a dramatic increase in the volume, variety and velocity of data, the loosely defined concept of \"Big Data\" has emerged as a topic of discussion in its own right -- with different viewpoints in both the business and technical worlds. From our perspective, it is important for discussions of \"Big Data\" to start from a well-defined business goal, and remain moored to fundamental principles of both cost/benefit analysis as well as core statistical science. This note discusses some business case considerations for analytics projects involving \"Big Data\", and proposes key questions that businesses should ask. With practical lessons from Big Data deployments in business, we also pose a number of research challenges that may be addressed to enable the business analytics community bring best data analytic practices when confronted with massive data sets.}, location = {Beijing, China}, series = {BigMine '12}, pages = {7\u201311}, numpages = {5}}
@inproceedings{10.1145/3006299.3006337,title = {Applying big data warehousing and visualization techniques on pingER data}, author = {Hameed Aqsa , Ali Saqib , Cottrell Rodger Les , White Bebo },year = {2016}, isbn = {9781450346177}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3006299.3006337}, doi = {10.1145/3006299.3006337}, abstract = {Nowadays, the Internet has turned into a crucial piece of our cutting edge society. It is a stage of exploration, financial development, democratic participation and speech. The operations of the Internet have prompted a huge development and collection of information known as Big Data. Therefore, it is important to monitor and measure the Quality of Service (QoS) of Internet traffic. The SLAC National Accelerator Laboratory started the PingER project in 1995 to measure the End-to-End Internet performance history of servers and routers worldwide. The project involves measurements of the 700 monitored sites in over 160 countries. PingER Monitoring Agents (MAs) ping a list of monitored sites after every 30 minutes to obtain Round Trip Time (RTT) values revealing interesting information about Internet performance (e.g., RTT, jitter, packet loss and unreachability) major events (e.g., fiber cuts, earthquakes, and social upheavals). Thus, the project has collected a vast amount of historical Internet Performance data worldwide since 1995. Currently, the data is stored in flat text files, making it difficult to analyze collectively. In addition, this simplistic format limits the analytical potential of this data. In this paper, we propose an approach to process, store, analyze and visualize PingER data. A Data warehouse is created which combines Hadoop Big Data techniques. The data are processed by using Sci-cumulus MR workflow, stored in HDFS, analyzed by Impala queries and visualized by using Google API's. This approach makes PingER data more accessible and enhances its potential contribution to ongoing research and application development.}, location = {Shanghai, China}, series = {BDCAT '16}, pages = {67\u201372}, numpages = {6}, keywords = {data mining, pingER, HDFS, visualization, big data, mapreduce}}
@inproceedings{10.1145/3490322.3490346,title = {Research Hotspots and Trends Visualization Analysis of Chinese E-commerce Big Data}, author = {Lu Tingting , Zhao Jiandong , Deng Xiongna , Dong Lirong , Huang Peng },year = {2021}, isbn = {9781450385091}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3490322.3490346}, doi = {10.1145/3490322.3490346}, abstract = {In order to explore the research hotspots and trends of e-commerce big data in China, the paper takes the relevant journal literatures with the theme of \"e-commerce\" and \"big data\" in CNKI as the research samples. With the help of the software CiteSpace, the visual analysis of the literature quantity, authors and institutions, keywords co-occurrence, keywords clustering and burst keywords were carried out respectively. The results show that the research on e-commerce big data in China first started in 2012 and entered a white-hot stage from 2013 to 2018. \"Precision marketing\", \"cloud computing\", \"Internet +\" and other fields are the main research hotspots at present. Observing the burst keywords in recent years, it is found that \"precision marketing\", \"cost control\", \"supply chain\" and \"optimization strategy\" may become the frontier hot spots and development trends of e-commerce big data research in the future.}, location = {Zibo, China}, series = {ICBDT '21}, pages = {147\u2013153}, numpages = {7}, keywords = {E-commerce Big Data, Visual Analysis, CiteSpace, E-commerce}}
@inproceedings{10.1145/3090354.3090388,title = {Secure confidential big data sharing in cloud computing using KP-ABE}, author = {Sara Amghar , Yassine Tabaa , Abdellatif Medouri },year = {2017}, isbn = {9781450348522}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3090354.3090388}, doi = {10.1145/3090354.3090388}, abstract = {In recent year, cloud computing and big data have become the hottest research topics which attract the attention of researchers due to its potential to provide major benefits to the industry and the community. Currently, the Organizations have a new option to outsource their massive data in the cloud without having to worry about the size of data or the capacity of memory. However, moving confidential and sensitive data from trusted domain of the data owners to public cloud will cause various security and privacy risks. Furthermore, the increasing amount of big data outsourced in the cloud increases the chance of breaching the privacy and security of these data. Despite all the research that has been done in this area, big data storage security and privacy remains one of the major concerns of organizations that adopt the cloud computing and big data technologies. Thus to ensure better data security we need for focus on two major problems which are the access control and encryption policies.In this paper, we propose a new hybrid model that enhances the security and privacy of big data shared in cloud environment using an access control scheme based on KP-ABE and authentication system. Our approach provides a flexible fine-grained access control of big data stored and shared in the cloud computing such that the encrypted data can only be accessed by authorized users.}, location = {Tetouan, Morocco}, series = {BDCA'17}, pages = {1\u20134}, numpages = {4}, keywords = {Data Encryption, Attributed Based Encryption, Access Control, Big Data Security, Authentication, Cloud Computing, KP-ABE}}
@inproceedings{10.1145/3422713.3422738,title = {Design and Application of Digital Platform for Big Data Eco-system}, author = {Yu Bai , Chuncheng Wei , Dongping Zhou , Yang Yu },year = {2020}, isbn = {9781450387859}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3422713.3422738}, doi = {10.1145/3422713.3422738}, abstract = {Ecological assets are an integral part of natural resources assets and the foundation for human survival and development. To achieve the goal of a unified management of \"mountains, rivers, forests, farmlands, lakes and grasslands\", aiming at the current online value evaluation issue on the green development of ecological assets in urban areas. This paper probes into the various information and data resources from the ecological factors, completes a demand analysis of the basic platform of urban ecological factors, establishes a monitoring analysis system for urban green development ecological system, and constructs a green development ecological system digital monitoring platform with high digital integrity, strong analysis ability and accurate decision support. The construction of platform layer, cleaning, storage and operation modeling of data as well as data service, sharing and exchange of the platform, all help to solve the isolated data situation of various information systems, and lay a foundation for subsequent data mining value, and establishing a data-driven city. Besides, it also provides decision support for urban green development, and helps to further build a model green development city, and promote urban ecological civilization. Meanwhile, this Big Data platform could reflect real situation of ecological assets in a more scientific and rational way, and provide certain basis and reference for the off-office auditing of leading cadres in relevant posts of natural assets.}, location = {Qingdao, China}, series = {ICBDT 2020}, pages = {69\u201373}, numpages = {5}, keywords = {Big Data, Design and Application, Ecological Assets, Platform}}
@inproceedings{10.1145/3010089.3010126,title = {Mining public administration Big Data for a financial benefit Word}, author = {Lahmidani Hind , El Beqqali Omar },year = {2016}, isbn = {9781450347792}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3010089.3010126}, doi = {10.1145/3010089.3010126}, abstract = {Our research study is related to data Mining software and analyzing big Data especially in public administration. The purpose of this study is to compare softwares and algorithms used for statistical analysis. Following the comparative study of supply chain management (SCM) softwares, we chose the best to make a comparative study of descriptive and predictive algorithms using dataset collected from the new IT solution developed in our PHD research study.}, location = {Blagoevgrad, Bulgaria}, series = {BDAW '16}, pages = {1\u20135}, numpages = {5}, keywords = {Big Data, Claim, Customers, Data mining, Information system, Logistics, Supply chain management, Tanagra}}
@inproceedings{10.1145/3363459.3363533,title = {Big Data: A Decade of Energy Characteristics of Single-Family Homes in Florida}, author = {Elias Rita , Issa Raja R. A. },year = {2019}, isbn = {9781450370141}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3363459.3363533}, doi = {10.1145/3363459.3363533}, abstract = {Housing in Florida is mainly driven by population growth of between 300,000 and 400,000 people per year, which makes Florida the third largest homebuilding state in the U.S. This high rate of population growth sheds light on the importance of building healthy residential houses that are energy-efficient throughout all of Florida, in all its climate zones. The purpose of this study is to investigate the direction Florida's single-family homes constructed between the years 2009 and 2018 are taking with respect to energy efficiency by analyzing their energy characteristics through big data analysis. Therefore, this study (a) developed a comprehensive literature review of the existing energy-efficient design strategies adopted in Florida. In addition, (b) explained the process of collecting information about materials and design strategies used in single-family houses constructed between the years 2009 and 2018, from energy forms prepared in the framework of the permit application process at every local building permit department in different counties all over Florida. Finally, (c) analyzed the evolution of information collected throughout the years, including but not limited to wall types, ceiling types, ducts' location, windows' features, heating and cooling systems, in the different climate zones of Florida influencing the building performance. The results of this big data analysis indicated that, on the average, single-family homes in Florida tended to get more energy-efficient and sustainable throughout the last decade (2009-2018). This conclusion cannot be totally confirmed unless all economic, environmental, and social aspects of sustainability are also taken into consideration.}, location = {New York, NY, USA}, series = {UrbSys'19}, pages = {101\u2013111}, numpages = {11}, keywords = {single-family houses, Florida, sustainability, design strategies, Energy}}
@inproceedings{10.1145/3507524.3507534,title = {Enterprise big data management based on Knowledge Graph}, author = {Luo Peng , Chen Jiayi , Li Jian },year = {2021}, isbn = {9781450387194}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3507524.3507534}, doi = {10.1145/3507524.3507534}, abstract = {With the continuous development of enterprises, the accumulated multi-source big data has gradually become the core resource of enterprises. Therefore, enterprise big data management is currently an important compet- itiveness of enterprise development. By constructing a multi-dimensional enterprises knowledge graph, the article identifies hidden data characteristics in enterprises. Take enterprise risk monitoring as an example, our method provides a perspective for risk prevention and rational evaluation of enterprises from a macro perspective. Based on the perspective of knowledge graph theory, using the top-down construction method, the knowledge graph of the enterprise is constructed from five dimensions, and then the enterprise judicial risk and public opinion risk are found. The research shows that: the knowledge graph can help enterprises and its external personnel fully grasp enterprise information, provide accurate and reliable knowledge for enterprise growth, industry development, government supervision, market investment and other aspects, which has important research value for early detection and prevention of enterprise risk.}, location = {Wuhan, China}, series = {ICCBD 2021}, pages = {54\u201359}, numpages = {6}, keywords = {data fusion, Additional Key Words and Phrases: knowledge graph, enterprise risk}}
@inproceedings{10.1145/2659651.2659655,title = {Big Data Information Security Maintenance}, author = {Miloslavskaya Natalia , Senatorov Mikhail , Tolstoy Alexander , Zapechnikov Sergey },year = {2014}, isbn = {9781450330336}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2659651.2659655}, doi = {10.1145/2659651.2659655}, abstract = {The need to protect big data, particularly those relating to information security maintenance (ISM) of an enterprise's IT infrastructure (ITI), and their processing is shown. Related worldwide experience of addressing big data ISM issues is summarized. An attempt to formulate a big data ISM problem statement is undertaken. An infrastructure for big data ISM is proposed. The importance of big data visualization is discussed.}, location = {Glasgow, Scotland, UK}, series = {SIN '14}, pages = {89\u201394}, numpages = {6}, keywords = {Information Security, Big Data, Secure Infrastructure}}
@inproceedings{10.1145/2896825.2896838,title = {Understanding quality requirements in the context of big data systems}, author = {Noorwali Ibtehal , Arruda Darlan , Madhavji Nazim H. },year = {2016}, isbn = {9781450341523}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2896825.2896838}, doi = {10.1145/2896825.2896838}, abstract = {While the domain of big data is anticipated to affect many aspects of human endeavour, there are numerous challenges in building big data applications among which is how to address big data characteristics in quality requirements. In this paper, we propose a novel, unified, approach for specifying big data characteristics (e.g., velocity of data arrival) in quality requirements (i.e., those requirements specifying attributes such as performance, reliability, availability, security, etc.). Several examples are given to illustrate the integrated specifications. As this is early work, further experimentation is needed in different big data situations and quality requirements and, beyond that, in a variety of project settings.}, location = {Austin, Texas}, series = {BIGDSE '16}, pages = {76\u201379}, numpages = {4}, keywords = {quality requirements, big data, specification, software engineering, requirements engineering}}
@inproceedings{10.1145/3474944.3474951,title = {Big Data Analytics in Association Rule Mining: A Systematic Literature Review}, author = {Shahin Mahtab , Arakkal Peious Sijo , Sharma Rahul , Kaushik Minakshi , Ben Yahia Sadok , Shah Syed Attique , Draheim Dirk },year = {2021}, isbn = {9781450389280}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3474944.3474951}, doi = {10.1145/3474944.3474951}, abstract = {Due to the rapid impact of IT technology, data across the globe is growing exponentially as compared to the last decade. Therefore, the efficient analysis and application of big data require special technologies. The present study performs a systematic literature review to synthesize recent research on the applicability of big data analytics in association rule mining (ARM). Our research strategy identified 4797 scientific articles, 27 of which were identified as primary papers relevant to our research. We have extracted data from these papers to identify various technologies and algorithms of using big data in association rule mining and identified their limitations in regards to the big data categories (volume, velocity, variety, and veracity).}, location = {Singapore, Singapore}, series = {BDET 2021}, pages = {40\u201349}, numpages = {10}, keywords = {MapReduce, Spark, Association rule mining, Big data analytics, systematic literature review}}
@inproceedings{10.1109/BDC.2014.10,title = {A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning}, author = {Wang Jianwu , Tang Yan , Nguyen Mai , Altintas Ilkay },year = {2014}, isbn = {9781479918973}, publisher = {IEEE Computer Society}, address = {USA}, url = {https://doi.org/10.1109/BDC.2014.10}, doi = {10.1109/BDC.2014.10}, abstract = {In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.}, series = {BDC '14}, pages = {16\u201325}, numpages = {10}, keywords = {Scientific workflow, Kepler, Ensemble learning, Hadoop, Big Data, Distributed computing, Bayesian network}}
@inproceedings{10.1145/3437075.3437083,title = {Data Privacy of Enterprises in the Era of Big data: Evidence from China}, author = {ZiJie Yang , Feng Liu , YaQiong Chi , CaiYun Fan },year = {2020}, isbn = {9781450375061}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3437075.3437083}, doi = {10.1145/3437075.3437083}, abstract = {The development of science and technology has brought explosive growth of data volume, which provides great convenience for the operation of enterprises. With the advent of the era of big data, enterprises are more vulnerable to the risk of data privacy leakage when they are transmitting cross-border trade. In this paper, this phenomenon is investigated. Affected by the epidemic situation, we conducted in depth communication with nine different types of enterprises by means of cloud interview. After classifying them, about 30% of enterprises in each category are selected for empirical research through questionaires survey. The results show that the nine enterprises have a weak understanding of the existing laws on data privacy protection at home and abroad. Based on this, after the simulation experiment, this paper puts forward corresponding suggestions for enterprises and government departments, which has certain reference value for related enterprises to formulate privacy policies and strategies in China.}, location = {Manchester, United Kingdom}, series = {ICBDM 2020}, pages = {13\u201316}, numpages = {4}, keywords = {Enterprises, Big data, Interview, Cross-border trade, Privacy protection}}
@inproceedings{10.1145/3352740.3352756,title = {Application of Big Data Technology in Emergency Decision System}, author = {Haikun Teng , Shiying Wang , Yue Xiao-Guang },year = {2019}, isbn = {9781450372053}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3352740.3352756}, doi = {10.1145/3352740.3352756}, abstract = {Due to the randomness and ambiguity of decision information, when emergencies occur, multiple scenarios may occur, and the probability and consequences of each scenario are different after using different emergency solutions. Decision makers often have psychological characteristics such as reference dependence, loss avoidance, and probability judgment distortion when making decisions. Therefore, this paper proposes a multi-attribute group decision making method based on foreground theory and language decision information based on big data technology. The method makes full use of the qualitative and quantitative conversion characteristics of the cloud model, and solves the problems of ambiguity and randomness of emergency decision information. At the same time, based on the foreground theory, the influence of the psychological factors of the decision makers on the decision results is fully considered, and the emergency response is emergency. Management and decision making provide supplementary reference.}, location = {Guilin, China}, series = {EBDIT 2019}, pages = {93\u201397}, numpages = {5}, keywords = {Emergencies, Big data technology, Decision making methods, Emergency management, Decision information}}
@inproceedings{10.1145/3216122.3216124,title = {Quality awareness for a Successful Big Data Exploitation}, author = {Cappiello Cinzia , Sam\u00e1 Walter , Vitali Monica },year = {2018}, isbn = {9781450365277}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3216122.3216124}, doi = {10.1145/3216122.3216124}, abstract = {The combination of data and technology is having a high impact on the way we live. The world is getting smarter thanks to the quantity of collected and analyzed data. However, it is necessary to consider that such amount of data is continuously increasing and it is necessary to deal with novel requirements related to variety, volume, velocity, and veracity issues. In this paper we focus on veracity that is related to the presence of uncertain or imprecise data: errors, missing or invalid data can compromise the usefulness of the collected values. In such a scenario, new methods and techniques able to evaluate the quality of the available data are needed. In fact, the literature provides many data quality assessment and improvement techniques, especially for structured data, but in the Big Data era new algorithms have to be designed. We aim to provide an overview of the issues and challenges related to Data Quality assessment in the Big Data scenario. We also propose a possible solution developed by considering a smart city case study and we describe the lessons learned in the design and implementation phases.}, location = {Villa San Giovanni, Italy}, series = {IDEAS '18}, pages = {37\u201344}, numpages = {8}, keywords = {Data Quality Assessment, Veracity, Big Data}}
@inproceedings{10.1145/3175684.3175687,title = {A Data-based Method for Industrial Big Data Project Prioritization}, author = {Kuschicke Felix , Thiele Thomas , Meisen Tobias , Jeschke Sabina },year = {2017}, isbn = {9781450354301}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3175684.3175687}, doi = {10.1145/3175684.3175687}, abstract = {The application of Big Data Techniques (BDT) in discrete manufacturing appears to be very promising, considering lighthouse projects in this area. In general, the goal is to collect all data from manufacturing systems comprehensively, in order to enable new findings and decision support by means of appropriate Industrial Big Data (IBD) analysis procedures. However, due to limited human and economic resources, potential IBD projects need to get prioritized -- in the best case according to their cost-benefit ratio. Available methods for this purpose are insufficient, due to their limited ability to be operationalized, error-proneness, and lack of scientific evidence. In this paper, we discuss how cost-benefit-analysis frameworks can be applied to the preliminary selection of production use cases for the implementation of BDT in larger production systems. It supports the use case selection process from information about production needs, available BDT, and given condition(s) per use case. This concept paper attempts to consolidate the hitherto fragmented discourse on how to prioritize IBD projects, evaluates the challenges of prioritization in this field, and presents a prioritization concept to overcome these challenges.}, location = {London, United Kingdom}, series = {BDIOT2017}, pages = {6\u201310}, numpages = {5}, keywords = {Project Prioritization, Manufacturing, Industrial Big Data, Framework, Project Selection}}
@inproceedings{10.1145/3018896.3025151,title = {Where big data meets 5G?}, author = {Sultan Kashif , Ali Hazrat },year = {2017}, isbn = {9781450347747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3018896.3025151}, doi = {10.1145/3018896.3025151}, abstract = {Due to massive increase in data collection from wireless devices, wireless sensor networks, and network operators, data processing has become a challenge. The massive data can broadly be categorized into raw data and right data. Future generation network (5G) can be optimized if right data is extracted efficiently from such a massive raw data. Such a solution is provided through big data analytics. In this article, we discuss big data analytics solution for 5G network. We also outline existing big data architectures proposed for the network optimization. We propose a generalized flow structure for big data based analytics in 5G. Finally, we summarize our article by highlighting some challenges for big data analytics in 5G.}, location = {Cambridge, United Kingdom}, series = {ICC '17}, pages = {1\u20134}, numpages = {4}, keywords = {big data, data analtyics, 5G networks, mobile communication}}
@inproceedings{10.1109/BDC.2014.15,title = {Genetic Algorithm Based Data-Aware Group Scheduling for Big Data Clouds}, author = {Kune Raghavendra , Konugurthi Pramod Kumar , Agarwal Arun , Chillarige Raghavendra Rao , Buyya Rajkumar },year = {2014}, isbn = {9781479918973}, publisher = {IEEE Computer Society}, address = {USA}, url = {https://doi.org/10.1109/BDC.2014.15}, doi = {10.1109/BDC.2014.15}, abstract = {Cloud computing is a promising cost efficient service oriented computing platform in the fields of science, engineering, business and social networking for delivering the resources on demand. Big Data Clouds is a new generation data analytics platform using Cloud computing as a back end technologies, for information mining, knowledge discovery and decision making based on statistical and empirical tools. MapReduce scheduling models for Big Data computing operate in the cluster mode, where the data nodes are pre-configured with the computing facility for processing. These MapReduce models are based on compute push model-pushing the logic to the data node for analysis, which is primarily for minimizing or eliminating data migration overheads between computing resources and data nodes. Such models, however, substantially perform well in the cluster setups, but are infelicitous for the platforms having the decoupled data storage and computing resources. In this paper, we propose a Genetic Algorithm based scheduler for such Big Data Cloud where decoupled computational and data services are offered as services. The approach is based on evolutionary methods focussed on data dependencies, computational resources and effective utilization of bandwidth thus achieving higher throughputs.}, series = {BDC '14}, pages = {96\u2013104}, numpages = {9}, keywords = {Big Data, Big Data Clouds, Cloud computing, Data Intensive Scheduling, Genetic algorithms}}
@inproceedings{10.1145/3291801.3291837,title = {The Visualized Analysis System for Big Data of Movies' Box Office in China}, author = {Cui Zhengzheng , Wang Danbei , Chen Sisi , Ding Huiming , Xie Zhifeng },year = {2018}, isbn = {9781450364768}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3291801.3291837}, doi = {10.1145/3291801.3291837}, abstract = {Chinese film industry has massive data, but the overall industrial system is still in infancy. At present, there is still a short board in the field of analysis for big data based on movies' box office in many studies. In this paper, we propose a Visualized Analysis System for Big-data of Movies' Box-office in China, which includes three parts: data real-timely collection, data visualization and data analysis. The system has a comprehensive collection of film data that includes all box offices for Chinese films in recent years. And it also has a clear system structure, which can better display the intrinsic value of Chinese movie's box office.}, location = {Weihai, China}, series = {ICBDR 2018}, pages = {90\u201394}, numpages = {5}, keywords = {Film data, Big Data, Visualized analysis}}
@inproceedings{10.1145/2505515.2527109,title = {Scholarly big data: information extraction and data mining}, author = {Giles C. Lee },year = {2013}, isbn = {9781450322638}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2505515.2527109}, doi = {10.1145/2505515.2527109}, abstract = {Collections of scholarly documents are usually not thought of as big data. However, large collections of scholarly documents often have many millions of publications, authors, citations, equations, figures, etc., and large scale related data and structures such as social networks, slides, data sets, etc. We discuss scholarly big data challenges, insights, methodologies and applications. We illustrate scholarly big data issues with examples of specialized search engines and recommendation systems that use information extraction and data mining in various areas such as computer science, chemistry, archaeology, acknowledgements, reference recommendation, collaboration recommendation, and others.}, location = {San Francisco, California, USA}, series = {CIKM '13}, pages = {1\u20132}, numpages = {2}, keywords = {data mining, digital libraries, entity resolution, information extraction, big data, information retrieval}}
@inproceedings{10.1145/3148055.3148068,title = {Characterization of Big Data Stream Processing Pipeline: A Case Study using Flink and Kafka}, author = {Javed M. Haseeb , Lu Xiaoyi , Panda Dhabaleswar K. (DK) },year = {2017}, isbn = {9781450355490}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3148055.3148068}, doi = {10.1145/3148055.3148068}, abstract = {In recent years there has been a surge in applications focusing on streaming data to generate insights in real-time. Both academia, as well as industry, have tried to address this use case by developing a variety of Stream Processing Engines (SPEs) with a diverse feature set. On the other hand, Big Data applications have started to make use of High-Performance Computing (HPC) which possess superior memory, I/O, and networking resources compared to typical Big Data clusters. Recent studies evaluating the performance of SPEs have focused on commodity clusters. However, exhaustive studies need to be performed to profile individual stages of a stream processing pipeline and how best to optimize each of these stages to best leverage the resources provided by HPC clusters. To address this issue, we profile the performance of a big data streaming pipeline using Apache Flink as the SPE and Apache Kafka as the intermediate message queue. We break the streaming pipeline into two distinct phases and evaluate percentile latencies for two different networks, namely 40GbE and InfiniBand EDR (100Gbps), to determine if a typical streaming application is network intensive enough to benefit from a faster interconnect. Moreover, we explore whether the volume of input data stream has any effect on the latency characteristics of the streaming pipeline, and if so how does it compare for different stages in the streaming pipeline and different network interconnects. Our experiments show an increase of over 10x in 98 percentile latency when input stream volume is increased from 128MB/s to 256MB/s. Moreover, we find the intermediate stages of the stream pipeline to be a significant contributor to the overall latency of the system.}, location = {Austin, Texas, USA}, series = {BDCAT '17}, pages = {1\u201310}, numpages = {10}, keywords = {hpc clusters, stream processing, profiling, real time, big data, message queue}}
@inproceedings{10.1145/3378904.3378923,title = {The Role of Big Data for Interactive Online Learning: A Case Study in Students' Participations and Perceptions}, author = {Pane Murty Magda , Siregar Christian , Rumeser Johannes A. A. },year = {2020}, isbn = {9781450376839}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3378904.3378923}, doi = {10.1145/3378904.3378923}, abstract = {The study aimed to gain the performance of students' participations and perceptions in interactive online learning and to understand the role of big data in enhancing it. The study adapted the quantitative method. The respondents amounted 201 students of more than five departments with a different range of semesters. The study measured the participations and perceptions of the respondents when doing teamwork virtually using the online learning management system with the Likert scale on the scale of 1 to 5 with corrected item-total correlations 0.472, and \u03b1 0.613 (> 0.5). The performance of it is moderate to low for most of respondents.}, location = {Singapore, China}, series = {BDET 2020}, pages = {30\u201334}, numpages = {5}, keywords = {perceptions, online learning, participations, learning management system, Big data}}
@inproceedings{10.1145/3341620.3341637,title = {Application Strategy of Big Data in the Development of Complex Industrial Products (CIPs)}, author = {Zhang Baolei , Zhao Fuquan , Liu Zongwei },year = {2019}, isbn = {9781450360913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341620.3341637}, doi = {10.1145/3341620.3341637}, abstract = {The product development is the key business of manufacturing and determines the competitive advantage of manufacturing enterprises, and has high difficulty in implementation. The product development of Complex Industrial Products (CIPs) is a great practical challenge for most enterprises. The demand for mass customization products makes enterprises to face more complicated product development situation. The deep integration of information technology and manufacturing technology makes big data an important value source for enterprises. Full application of big data to promote product development of CIPs has become a feasible approach for product development of enterprises. The value of big data needs to be applied through the knowledge-based application of data. The core work is to develop the functional data model. The application of big data in product development will eventually move towards knowledge-based intelligent. The case study provides the mechanism verification for the application strategy of big data in the development of CIPs.}, location = {Hong Kong, Hong Kong}, series = {BDE 2019}, pages = {114\u2013120}, numpages = {7}, keywords = {Complex Industrial Products (CIPs), Smart manufacturing, Big data, Data processing}}
@inproceedings{10.1145/3206157.3206184,title = {The Study of Learner Autonomy in Foreign Language Learning in Big Data Era}, author = {Shaosen Cao },year = {2018}, isbn = {9781450363587}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3206157.3206184}, doi = {10.1145/3206157.3206184}, abstract = {Learner autonomy has been considered as the ultimate goal of education, and it has become a hot topic in education field since its birth. In Big Data era, the learning pattern is changing dramatically and learning activities can be carried out through an interactive process between learners and the online learning systems. This article first introduces Big Data and learner autonomy, then discusses the reforms big data brings to learner autonomy, finally analyzes the learning mode of learner autonomy in foreign language learning in Big Data era.}, location = {Honolulu, HI, USA}, series = {ICBDE '18}, pages = {101\u2013105}, numpages = {5}, keywords = {interactive process, foreign language learning, learner autonomy, Big Data}}
@inproceedings{10.1145/3141128.3141143,title = {From Big data platforms to smarter solution, with intelligent learning: [PAV] 4 - Pave the way for Intelligence}, author = {Fathi F. , Abghour N. , Ouzzif M. },year = {2017}, isbn = {9781450353434}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3141128.3141143}, doi = {10.1145/3141128.3141143}, abstract = {In today's time when data is generating by everyone at every moment, and the word is moving so fast with exponential growth of new technologies and innovations in all science and engineering domains, the age of big data is coming, and the potential of learning from this huge amount of data and from different sources is undoubtedly significant to uncover underlying structure and facilitate the development of more intelligent solution. Intelligence is around us, and the concept of big data and learning from it has existed since the emergence of the human being. In this article we focus on data from; sensors, images, and text, and we incorporate the principles of human intelligence; brain - body - environment, as a source of inspiration that allows us to put a new concept based on big data - machine learning--domain and pave the way for intelligent platform.}, location = {London, United Kingdom}, series = {ICCBDC 2017}, pages = {11\u201316}, numpages = {6}, keywords = {smart city, Big data, machine learning, intelligent solution, Hadoop}}
@inproceedings{10.1145/3006299.3006311,title = {Towards a comprehensive data lifecycle model for big data environments}, author = {Sinaeepourfard Amir , Garcia Jordi , Masip-Bruin Xavier , Mar\u00edn-Torder Eva },year = {2016}, isbn = {9781450346177}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3006299.3006311}, doi = {10.1145/3006299.3006311}, abstract = {A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource; however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation.Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity; and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.}, location = {Shanghai, China}, series = {BDCAT '16}, pages = {100\u2013106}, numpages = {7}, keywords = {data complexity, data lifecycle, big data, vs challenges, data management, data organization}}
@inproceedings{10.1145/3507524.3507533,title = {Analysis on Big Data Based Intelligence Processing Method of Electronic Reconnaissance Satellites}, author = {Rao Shijun , Hong Jun , Dong Hang },year = {2021}, isbn = {9781450387194}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3507524.3507533}, doi = {10.1145/3507524.3507533}, abstract = {In response to the problems of data missing, data exception and data duplication in electronic reconnaissance satellite intelligence processing, based on big data architecture and by comprehensive use of big data analysis techniques such as data cleaning and extraction, we optimized the processing methods for missing values, outliers and duplicate values of electronic reconnaissance satellite intelligence data while analyzing the quality characteristics of original satellite intelligence reconnaissance, and verified the effectiveness and accuracy of this intelligence processing method through simulation calculations. This method played an important role in improving the efficiency of electronic reconnaissance satellite intelligence processing, and had good applicability in naval and air intelligence processing of ships.}, location = {Wuhan, China}, series = {ICCBD 2021}, pages = {49\u201353}, numpages = {5}, keywords = {Python, electronic reconnaissance satellite, Newton interpolation, intelligence processing, big data cleaning}}
@inproceedings{10.1145/3451400.3451412,title = {An Intelligence Probe of College Student Management in the Big Data Era}, author = {Wang Junru },year = {2021}, isbn = {9781450389389}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3451400.3451412}, doi = {10.1145/3451400.3451412}, abstract = {Stepping into the big data era, network data resources contain abundant application values. College students multiple value culture brings new problem and also challenge to college student management. Current college students\u2019 ability of management informatization is weak in using data intellectual analysis. By constructing \u201cFour fine types\u201d students educational service system, strengthening the collection processing and analysis of education data and carrying on students evaluation and employment recommendation and other intelligent typical applications, to benefit students management pattern's reform and innovation.}, location = {London, United Kingdom}, series = {ICBDE 2021}, pages = {75\u201379}, numpages = {5}, keywords = {intelligence, college student management, big data era}}