@inproceedings{10.1145/3314527.3314537,
author = {Cabanban-Casem, Christianne Lynnette},
title = {Analytical Visualization of Higher Education Institutions' Big Data for Decision Making},
year = {2019},
isbn = {9781450366212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314527.3314537},
doi = {10.1145/3314527.3314537},
abstract = {Education is an important element towards learning and human development, thus, it is the key towards identifying competencies and better productivity for the workforce. As part of the Commission on Higher Education's (CHED) thrust for improving efficiency and effectiveness by simplifying the collection process for all the stakeholders, the developed system will drastically improve the availability of data for making informed decisions and efficient generation of reports.This research outlines opportunities and challenges associated with the implementation and governance of Big Data in higher education through development and implementation of data analytics tool.},
booktitle = {Proceedings of the 2019 Asia Pacific Information Technology Conference},
pages = {61–64},
numpages = {4},
keywords = {Data Science, Knowledge Management, Higher Education Data},
location = {Jeju Island, Republic of Korea},
series = {APIT 2019}
}

@inproceedings{10.1145/3070607.3075961,
author = {Sroka, Jacek and Leundefinedniewski, Artur and Kowaluk, Miros\l{}aw and Stencel, Krzysztof and Tyszkiewicz, Jerzy},
title = {Towards Minimal Algorithms for Big Data Analytics with Spreadsheets},
year = {2017},
isbn = {9781450350198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3070607.3075961},
doi = {10.1145/3070607.3075961},
abstract = {The motivation for our research is the need for a simple and accessible method to process large datasets available to the public. We aim to significantly lower the technological barrier, which currently prevents average users from analyzing big datasets available as CSV files. Spreadsheets are perfectly suited for this task, being the most popular and accessible data analysis tool. We present ideally balanced algorithms for specifying MapReduce computations of high number of range queries. With our algorithms, it will be possible to automatically perform analysis defined with a spreadsheet on data of size exceeding the dimensions of the spreadsheet grid by orders of magnitude.},
booktitle = {Proceedings of the 4th ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond},
articleno = {1},
numpages = {4},
keywords = {minimal algorithms, Big data, range queries, Hadoop, spreadsheet, MapReduce},
location = {Chicago, IL, USA},
series = {BeyondMR'17}
}

@article{10.14778/3407790.3407848,
author = {Rong, Kexin and Lu, Yao and Bailis, Peter and Kandula, Srikanth and Levis, Philip},
title = {Approximate Partition Selection for Big-Data Workloads Using Summary Statistics},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407848},
doi = {10.14778/3407790.3407848},
abstract = {Many big-data clusters store data in large partitions that support access at a coarse, partition-level granularity. As a result, approximate query processing via row-level sampling is inefficient, often requiring reads of many partitions. In this work, we seek to answer queries quickly and approximately by reading a subset of the data partitions and combining partial answers in a weighted manner without modifying the data layout. We illustrate how to efficiently perform this query processing using a set of pre-computed summary statistics, which inform the choice of partitions and weights. We develop novel means of using the statistics to assess the similarity and importance of partitions. Our experiments on several datasets and data layouts demonstrate that to achieve the same relative error compared to uniform partition sampling, our techniques offer from 2.7x to 70x reduction in the number of partitions read, and the statistics stored per partition require fewer than 100KB.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {2606–2619},
numpages = {14}
}

@inproceedings{10.5555/2093889.2093935,
author = {Petrazickis, Leons and Steinfeld, Bradley},
title = {Crunching Big Data in the Cloud with Hadoop and BigInsights},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {There is an ongoing information explosion in every field of human endeavour. Enormous, unstructured, immensely valuable data sets are being accumulated. Every device logs numbers and audio and video and text, and then this data is aggregated and stored somewhere. Unfortunately, traditional techniques cannot analyze these data sets. There's too much data to query -- volume! And it's all different -- variety! And it's arriving too fast -- velocity!Finance firms need to analyze transactions to detect fraud and model risk. Energy firms need to analyze old rig performance and wind speeds. IT needs to analyze logs of every type. Service providers need to analyze the prices of various services worldwide. Healthcare providers need to analyze patient data and measurements.New techniques are needed to deal with this Big Data. Fortunately, cloud computing is allowing the emergence of technologies that rely on clusters of commodity hardware to crunch data.Google is one example of a company that had to face and solve a Big Data problem before it could revolutionize internet search and consign countless other early search engines to the dust heap of history. Its page-rank algorithm that it uses to rank results is based on something called Map-Reduce. In the Map phase, the data set (all of the internet) is split into itsy-bitsy chunks, which are transformed from unstructured data (information about a web page) to useful data (the value of web page). In the Reduce phase, the transformed itsy-bitsy chunks are reassembled back together into a Google results page.Because the chunks are itsy-bitsy, the mapping could run on many off-the-shelf computers rather than one big server. This allowed Google to use cheap commodity hardware and put competitors that relied on expensive servers out of business. The hardware side of this approach created Cloud Computing, which is a way of organizing vast amounts of cheap hardware on demand. The software side of this created a lot of useful data analytics applications.Apache Hadoop is one useful data analytics application that's native to the cloud. Hadoop is an open source project led by Yahoo. It makes it straightforward to apply the idea of Map-Reduce to any data set. Many vendors such as Cloudera, HortonWorks, and IBM have their own distribution of Hadoop.The Hadoop ecosystem includes many other open source technologies. The Java libraries are enhanced by the Pig high level language, the HBase database, the Hive data warehouse system, and the Flume log aggregation service. Each of these makes Hadoop more powerful at dealing with larger volumes of data, greater varieties of data, and quicker velocities of data.IBM InfoSphere BigInsights is a distribution of Hadoop. It integrates an IBM-created open source query language called JAQL (Jackal) with the usual components such as Hive, HBase, and Pig. JAQL allows the user to query through large sets of data in JSON (JavaScript Object Notation) form, which is the native data format of Hadoop.The Basic edition of BigInsights is available for download at no charge. It can also be easily deployed on Amazon Elastic Compute Cloud or IBM SmartCloud Enterprise.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {334–335},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@inproceedings{10.1145/3102254.3102259,
author = {Caruccio, Loredana and Deufemia, Vincenzo and Polese, Giuseppe},
title = {Evolutionary Mining of Relaxed Dependencies from Big Data Collections},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102259},
doi = {10.1145/3102254.3102259},
abstract = {Many modern application contexts, especially those related to the semantic Web, advocate for automatic techniques capable of extracting relationships between semi-structured data, for several purposes, such as the identification of inconsistencies or patterns of semantically related data, query rewriting, and so forth. One way to represent such relationships is to use relaxed functional dependencies (rfds), since they can embed approximate matching paradigms to compare unstructured data, and admit the possibility of exceptions for them. To this end, thresholds might need to be specified in order to limit the similarity degree in approximate comparisons or the occurrence of exceptions. Thanks to the availability of huge amount of data, including unstructured data available on the Web, nowadays it is possible to automatically discover rfds from data. However, due to the many different combinations of similarity and exception thresholds, the discovery process has an exponential complexity. Thus, it is vital devising proper optimization strategies, in order to make the discovery process feasible. To this end, in this paper, we propose a genetic algorithm to discover rfds from data, also providing an empirical evaluation demonstrating its effectiveness.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {5},
numpages = {10},
keywords = {genetic algorithm, discovery from data, functional dependency},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3141128.3141143,
author = {Fathi, F. and Abghour, N. and Ouzzif, M.},
title = {From Big Data Platforms to Smarter Solution, with Intelligent Learning: [PAV] 4 - Pave the Way for Intelligence},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141143},
doi = {10.1145/3141128.3141143},
abstract = {In today's time when data is generating by everyone at every moment, and the word is moving so fast with exponential growth of new technologies and innovations in all science and engineering domains, the age of big data is coming, and the potential of learning from this huge amount of data and from different sources is undoubtedly significant to uncover underlying structure and facilitate the development of more intelligent solution. Intelligence is around us, and the concept of big data and learning from it has existed since the emergence of the human being. In this article we focus on data from; sensors, images, and text, and we incorporate the principles of human intelligence; brain - body - environment, as a source of inspiration that allows us to put a new concept based on big data - machine learning--domain and pave the way for intelligent platform.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {11–16},
numpages = {6},
keywords = {Hadoop, smart city, intelligent solution, Big data, machine learning},
location = {London, United Kingdom},
series = {ICCBDC 2017}
}

@inproceedings{10.5555/2399776.2399802,
author = {Petrazickis, Leons and Butuc, Marius and Steinfeld, Bradley},
title = {Crunching Big Data with Hadoop and BigInsights in the Cloud},
year = {2012},
publisher = {IBM Corp.},
address = {USA},
abstract = {There is an ongoing information explosion in every field of human endeavour. Enormous, unstructured, immensely valuable data sets are being accumulated. Every device logs numbers, audio, video and text; data is aggregated and stored somewhere.Unfortunately, traditional techniques cannot analyze these data sets. There's too much data to query -- volume! And it's all different -- variety! And it's arriving too fast -- velocity! Finance firms need to analyze transactions to detect fraud and model risk; Energy firms need to analyze old rig performance and wind speeds; IT needs to analyze logs of every type; Service providers need to analyze the prices of various services worldwide; Healthcare providers need to analyze patient data and measurements.New techniques are needed to deal with this Big Data. Fortunately, cloud computing is allowing the emergence of technologies that rely on clusters of commodity hardware to crunch data. Google is one example of a company that had to face and solve a Big Data problem before it could revolutionize internet search and consign countless other early search engines to the dust heap of history. The page-rank algorithm that it uses to rank results is based on something called Map-Reduce. In the Map phase, the data set (all of the internet) is split into microscopic chunks, which are transformed from unstructured data (information about a web page) to useful data (the value of web page). In the Reduce phase, the transformed microscopic chunks are reassembled back together into a Google results page.Because the chunks are microscopic, the mapping could run on many off-the-shelf computers rather than one big server. As a result, Google is able to use cheap commodity hardware and put competitors that relied on expensive servers out of business. The hardware side of this approach created Cloud Computing, which is a way of organizing vast amounts of cheap hardware on demand. The software side of this created a lot of useful data analytics applications.Apache Hadoop is one useful data analytics application that's native to the cloud. Hadoop is an open source project led by Yahoo. It makes it straightforward to apply the idea of Map-Reduce to any data set. Many vendors such as Cloudera. HortonWorks, and IBM have their own distribution of Hadoop. The Hadoop ecosystem includes many other open source technologies. The Java libraries are enhanced by the Pig high level language, the HBase database, the Hive data warehouse system, and the Flume log aggregation service. Each of these makes Hadoop more powerful at dealing with larger volumes of data, greater varieties of data, and quicker velocities of data.IBM InfoSphere BigInsights is a distribution of Hadoop. It integrates an IBM-created open source query language called JAQL (Jackal) with the usual components such as Hive, HBase, and Pig. JAQL allows the user to query through large sets of data in JSON (JavaScript Object Notation) form, which is the native data format of Hadoop. The Basic edition of BigInsights is available for download at no charge. It can also be easily deployed on Amazon Elastic Compute Cloud or IBM SmartCloud Enterprise.},
booktitle = {Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {241–242},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {CASCON '12}
}

@proceedings{10.1145/3335484,
title = {ICBDC '19: Proceedings of the 4th International Conference on Big Data and Computing},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {With the proliferation of big data, emerging issues of multimedia systems, signal processing, soft computing, cloud computing, networking and computer security, have attracted an increasing amount of interest from researchers across different fields. ICBDC is an annual conference for researchers, academicians, practitioners and industry in big data and computing science communities. This platform provides opportunities for the delegates to exchange new ideas and application experiences face to face, to establish business and research relationships and to facilitate global collaborations in this exciting new research field.},
location = {Guangzhou, China}
}

@inproceedings{10.1145/2694730.2694734,
author = {Apon, Amy W.},
title = {Experimentation as a Tool for the Performance Evaluation of Big Data Systems},
year = {2015},
isbn = {9781450333382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694730.2694734},
doi = {10.1145/2694730.2694734},
abstract = {The complex big data systems of today are difficult, if not impossible, to model analytically. The challenges of these distributed and parallel data processing systems include heterogeneous network communication, a mix of storage, memory, and computing devices, and common failures of communication and devices. Particular challenges with big data systems include the variety and volume of data that place previously unseen stresses on distributed computing systems. Experimentation using production-quality hardware and software and realistic data is required to understand system tradeoffs. At the same time, experimental evaluation has challenges, including access to hardware resources at scale, robust workload characterization, data characterization, configuration management of software and systems, and sometimes insidious optimization issues around the mix of software stacks or hardware/software resource allocation. In this talk we present a number of the research challenges when experimentation is used as a tool for the performance evaluation of big data systems, some approaches to solutions, and open questions for this area.},
booktitle = {Proceedings of the 1st Workshop on Performance Analysis of Big Data Systems},
pages = {3},
numpages = {1},
keywords = {experimentation, big data systems, performance evaluation, workload characterization, data characterization},
location = {Austin, Texas, USA},
series = {PABS '15}
}

@inproceedings{10.1145/3257971,
author = {Parashar, Manish},
title = {Session Details: Big Data Processing and I/O},
year = {2016},
isbn = {9781450343145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257971},
doi = {10.1145/3257971},
booktitle = {Proceedings of the 25th ACM International Symposium on High-Performance Parallel and Distributed Computing},
location = {Kyoto, Japan},
series = {HPDC '16}
}

@inproceedings{10.1145/3378904.3378923,
author = {Pane, Murty Magda and Siregar, Christian and Rumeser, Johannes A. A.},
title = {The Role of Big Data for Interactive Online Learning: A Case Study in Students' Participations and Perceptions},
year = {2020},
isbn = {9781450376839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378904.3378923},
doi = {10.1145/3378904.3378923},
abstract = {The study aimed to gain the performance of students' participations and perceptions in interactive online learning and to understand the role of big data in enhancing it. The study adapted the quantitative method. The respondents amounted 201 students of more than five departments with a different range of semesters. The study measured the participations and perceptions of the respondents when doing teamwork virtually using the online learning management system with the Likert scale on the scale of 1 to 5 with corrected item-total correlations 0.472, and α 0.613 (&gt; 0.5). The performance of it is moderate to low for most of respondents.},
booktitle = {Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology},
pages = {30–34},
numpages = {5},
keywords = {participations, online learning, learning management system, Big data, perceptions},
location = {Singapore, China},
series = {BDET 2020}
}

@proceedings{10.1145/2536714,
title = {SENSEMINE'13: Proceedings of First International Workshop on Sensing and Big Data Mining},
year = {2013},
isbn = {9781450324304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are glad to welcome you to the First International Workshop on Sensing and Big Data Mining (SenseMine 2013). This workshop presents a new forum, established to bring together researchers, practitioners, and academics in the fields of sensor networks, distributed systems, and big data mining and machine learning, with the goal of driving cross-disciplinary research in support of novel emerging applications in our daily lives.},
location = {Roma, Italy}
}

@article{10.1145/3297720,
author = {M\"{u}ller, Daniel and Jain, Pratiksha and Te, Yieh-Funk},
title = {Augmenting Data Quality through High-Precision Gender Categorization},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3297720},
doi = {10.1145/3297720},
abstract = {Mappings of first name to gender have been widely recognized as a critical tool for the completion, study, and validation of data records in a range of areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 4 million people, which was provided by a car insurance company. Then, we study how naming conventions have changed over time and how they differ by nationality. Next, we build a probabilistic first-name-to-gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in two-label and three-label settings and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies’ outcomes and find that our mapping produces high-precision results. We validate that the additional information of nationality and year of birth improve the precision scores of name-to-gender mappings. Therefore, the proposed approach constitutes an efficient process for improving the data quality of organizations’ records, if the gender attribute is missing or unreliable.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {8},
numpages = {18},
keywords = {record completion, patenting, Data quality improvement, gender name mapping}
}

@inproceedings{10.1145/3341620.3341635,
author = {Liu, Lixia and Hu, Gang and Zhang, Min and Xue, Xiaoqiao},
title = {Application of Online/Offline Sales Big Data in Household Medical Device Industry},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341635},
doi = {10.1145/3341620.3341635},
abstract = {In this paper, we discuss the current situation of the application of big data in the sales of household medical devices industry. Four problems existing in current commercial big data mining are analyzed. We put forward two main targets of big sales data in this area which are on-line and off-line mutual drainage and accurate marketing maximization. Subsequently we analyze the paths to achieve these goals which include introduction of professional data analysts, establishment of sales data analysis model, online and offline data access and pilot operation of analysis model. Finally a simple conclusion of the whole paper is given.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {63–67},
numpages = {5},
keywords = {Precision Marketing, Big Data, Sales, Household Medical Devices},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/2939672.2939730,
author = {Li, Xiang and Makkie, Milad and Lin, Binbin and Sedigh Fazli, Mojtaba and Davidson, Ian and Ye, Jieping and Liu, Tianming and Quinn, Shannon},
title = {Scalable Fast Rank-1 Dictionary Learning for FMRI Big Data Analysis},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939730},
doi = {10.1145/2939672.2939730},
abstract = {It has been shown from various functional neuroimaging studies that sparsity-regularized dictionary learning could achieve superior performance in decomposing comprehensive and neuroscientifically meaningful functional networks from massive fMRI signals. However, the computational cost for solving the dictionary learning problem has been known to be very demanding, especially when dealing with large-scale data sets. Thus in this work, we propose a novel distributed rank-1 dictionary learning (D-r1DL) model and apply it for fMRI big data analysis. The model estimates one rank-1 basis vector with sparsity constraint on its loading coefficient from the input data at each learning step through alternating least squares updates. By iteratively learning the rank-1 basis and deflating the input data at each step, the model is then capable of decomposing the whole set of functional networks. We implement and parallelize the rank-1 dictionary learning algorithm using Spark engine and deployed the resilient distributed dataset (RDDs) abstracts for the data distribution and operations. Experimental results from applying the model on the Human Connectome Project (HCP) data show that the proposed D-r1DL model is efficient and scalable towards fMRI big data analytics, thus enabling data-driven neuroscientific discovery from massive fMRI big data in the future.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {511–519},
numpages = {9},
keywords = {distributed computation, algorithm parallelization, sparse coding, fMRI},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3209281.3209300,
author = {Chotvijit, Sarunkorn and Thiarai, Malkiat and Jarvis, Stephen},
title = {Big Data Analytics in Social Care Provision: Spatial and Temporal Evidence from Birmingham},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209300},
doi = {10.1145/3209281.3209300},
abstract = {There is significant national interest in tackling issues surrounding the needs of vulnerable children and adults. At the same time, UK local authorities face severe financial challenges as a result of decreasing financial settlements and increasing demands from growing urban populations. This research employs state-of-the-art data analytics and visualisation techniques to analyse six years of local government social care data for the city of Birmingham, the UK's second most populated city. This analysis identifies: (i) service cost profiles over time; (ii) geographical dimensions to service demand and delivery; (iii) patterns in the provision of services, and (iv) the extent to which data value and data protection interact. The research accesses data held by the local authority to discover patterns and insights that may assist in the understanding of service demand, support decision making and resource management, whilst protecting and safeguarding its most vulnerable citizens. The use of data in this manner could also inform the approach a local authority has to its data, its capture and use, and the potential for supporting data-led management and service improvements.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {5},
numpages = {8},
keywords = {social care, Birmingham, local authority, data analytics, service provision, spatio-temporal analysis},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.5555/3291291.3291307,
author = {Lopez, Eduardo and Sartipi, Kamran},
title = {Feature Engineering in Big Data for Detection of Information Systems Misuse},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {The increasing availability of very large volumes of digital data (i.e. Big Data) enables many interesting research streams on a wide variety of phenomena. However, there has been a paucity of Big Data sets in the area of cybersecurity in information systems, as organizations are reluctant to share data that may provide too much unrestricted visibility into their operations. In this study, we explore the use of a real-life, anonymized, very large dataset containing user behavior - as captured in log files - including both regular usage as well as misuse, typifying the dynamics found in a situation with compromised user credentials. Through the experiment, we validate that the existence of a large user behavior dataset in itself does not necessarily guarantee that abnormal behaviors can be found. It is essential that researchers apply deep domain knowledge, critical thinking and practical focus to ensure the data can produce the knowledge required for the ultimate objective of detecting an insider's threat. In this paper we develop, formulate and calculate the features that best represent user behavior in the underlying information systems, maintaining a parsimonious balance between complexity, resource demands and detection effectiveness. We test the use of a classification model that proves the usefulness and aplicability of the features extracted.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {145–156},
numpages = {12},
keywords = {anomaly detection, predicting misuse, insider's threat, big data security, feature engineering},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/2737909.2737912,
author = {Fox, Geoffrey C. and Jha, Shantenu and Qiu, Judy and Luckow, Andre},
title = {Towards an Understanding of Facets and Exemplars of Big Data Applications},
year = {2014},
isbn = {9781450330312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737909.2737912},
doi = {10.1145/2737909.2737912},
abstract = {We study many Big Data applications from a variety of research and commercial areas and suggest a set of characteristic features and possible kernel benchmarks that stress those features for data analytics. We draw conclusions for the hardware and software architectures that are suggested by this analysis.},
booktitle = {Proceedings of the 20 Years of Beowulf Workshop on Honor of Thomas Sterling's 65th Birthday},
pages = {7–16},
numpages = {10},
location = {Annapolis, MD, USA},
series = {Beowulf '14}
}

@inproceedings{10.1145/1651291.1651303,
author = {Rodic, Jasna and Baranovic, Mirta},
title = {Generating Data Quality Rules and Integration into ETL Process},
year = {2009},
isbn = {9781605588018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1651291.1651303},
doi = {10.1145/1651291.1651303},
abstract = {Many data quality projects are integrated into data warehouse projects without enough time allocated for the data quality part, which leads to a need for a quicker data quality process implementation that can be easily adopted as the first stage of data warehouse implementation. We will see that many data quality rules can be implemented in a similar way, and thus generated based on metadata tables that store information about the rules. These generated rules are then used to check data in designated tables and mark erroneous records, or to do certain updates of invalid data. We will also store information about the rules violations in order to provide analysis of such data. This could give a significant insight into our source systems. Entire data quality process will be integrated into ETL process in order to achieve load of data warehouse that is as automated, as correct and as quick as possible. Only small number of records would be left for manual inspection and reprocessing.},
booktitle = {Proceedings of the ACM Twelfth International Workshop on Data Warehousing and OLAP},
pages = {65–72},
numpages = {8},
keywords = {rules, generator, data quality, metadata, oracle},
location = {Hong Kong, China},
series = {DOLAP '09}
}

@article{10.1109/TNET.2019.2943884,
author = {Yun, Daqing and Wu, Chase Q. and Rao, Nageswara S. V. and Kettimuthu, Rajkumar},
title = {Advising Big Data Transfer Over Dedicated Connections Based on Profiling Optimization},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2943884},
doi = {10.1109/TNET.2019.2943884},
abstract = {Big data transfer in next-generation scientific applications is now commonly carried out over dedicated channels in high-performance networks (HPNs), where transport protocols play a critical role in maximizing application-level throughput. Optimizing the performance of these protocols is challenging: i) transport protocols perform differently in various network environments, and the protocol choice is not straightforward; ii) even for a given protocol in a given environment, different parameter settings of the protocol may lead to significantly different performance and oftentimes the default setting does not yield the best performance. However, it is prohibitively time-consuming to conduct exhaustive transport profiling due to the large parameter space. In this paper, we propose a PRofiling Optimization Based DAta Transfer Advisor (ProbData) to help end users determine the most effective transport method with the most appropriate parameter settings to achieve satisfactory performance for big data transfer over dedicated connections in HPNs. ProbData employs a fast profiling scheme based on the Simultaneous Perturbation Stochastic Approximation algorithm, namely, FastProf, to accelerate the exploration of the optimal operational zones of various transport methods to improve profiling efficiency. We first present a theoretical background of the optimized profiling approach in ProbData and then detail its design and implementation. The advising procedure and performance benefits of FastProf and ProbData are illustrated and evaluated by both extensive emulations based on real-life performance measurements and experiments over various physical connections in existing production HPNs.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {2280–2293},
numpages = {14}
}

@inproceedings{10.1145/2789168.2802150,
author = {Aguiar, Rui Luis and Benhabiles, Nora and Pfeiffer, Tobias and Rodriguez, Pablo and Viswanathan, Harish and Wang, Jia and Zang, Hui},
title = {Big Data, IoT, .... Buzz Words for Academia or Reality for Industry?},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2802150},
doi = {10.1145/2789168.2802150},
abstract = {The concepts of Big Data have became intertwined with those of the Internet of Things, creating mental pictures of a fully connected, all-encompassing, cyber-physical world, where each and every object will contribute with information to a "fully aware" society. Academic works are presenting this as the natural evolution for our current technologies. The panel looks at these promises from the hard perspective of reality: what is being done, how much it cost, what needs to be developed, and what can be expected in the near and mid-term.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {550–551},
numpages = {2},
keywords = {IoT, big data, industry-applications},
location = {Paris, France},
series = {MobiCom '15}
}

@article{10.14778/3368289.3368292,
author = {Kandula, Srikanth and Orr, Laurel and Chaudhuri, Surajit},
title = {Pushing Data-Induced Predicates through Joins in Big-Data Clusters},
year = {2019},
issue_date = {November 2019},
publisher = {VLDB Endowment},
volume = {13},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3368289.3368292},
doi = {10.14778/3368289.3368292},
abstract = {Using data statistics, we convert predicates on a table into data induced predicates (diPs) that apply on the joining tables. Doing so substantially speeds up multi-relation queries because the benefits of predicate pushdown can now apply beyond just the tables that have predicates. We use diPs to skip data exclusively during query optimization; i.e., diPs lead to better plans and have no overhead during query execution. We study how to apply diPs for complex query expressions and how the usefulness of diPs varies with the data statistics used to construct diPs and the data distributions. Our results show that building diPs using zone-maps which are already maintained in today's clusters leads to sizable data skipping gains. Using a new (slightly larger) statistic, 50% of the queries in the TPC-H, TPC-DS and JoinOrder benchmarks can skip at least 33% of the query input. Consequently, the median query in a production big-data cluster finishes roughly 2x faster.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {252–265},
numpages = {14}
}

@inproceedings{10.1145/3097983.3105813,
author = {Berglund, Andy},
title = {Mining Big Data in NeuroGenetics to Understand Muscular Dystrophy},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3105813},
doi = {10.1145/3097983.3105813},
abstract = {The recent advances in genome sequencing and analyses of the billions of base pairs in genomic data have been a boon for moving forward our understanding of human disease. In this talk I will describe how genome sequencing has dramatically improved our understanding of the most common adult form of muscular dystrophy, which is myotonic dystrophy. Two different genetic mutations cause thousands of changes in the cells and tissues of myotonic dystrophy patients. Genome sequencing has allowed us to precisely determine the degree of changes across patients, correlate these changes to disease symptoms and allow us to determine quickly in cell and animal models the effectiveness of therapeutic strategies for myotonic dystrophy.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {11},
numpages = {1},
keywords = {muscular dystrophy, genome sequencing, genomic data, data mining., myotonic dystrophy},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3055167.3055180,
author = {Qi, Danrui},
title = {On Concise Explanations of Non-Answers over Big Data},
year = {2017},
isbn = {9781450341998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055167.3055180},
doi = {10.1145/3055167.3055180},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {10–12},
numpages = {3},
keywords = {data cleaning, explanation},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3305275.3305287,
author = {Liu, Lan and Shong, YuChen},
title = {Study on Innovation Performance of Big Data and Artificial Intelligence Listed Companies},
year = {2018},
isbn = {9781450365703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3305275.3305287},
doi = {10.1145/3305275.3305287},
abstract = {In the rapidly changing world economy, big data and artificial intelligence listed companies may be the future industrial giants. The main body of such enterprises is the emerging technology industry. Innovation represents the theme of today's world development. To innovate, the first thing is to enhance one's own scientific and technological capabilities, and increased R&amp;D investment is the key to innovative development of such enterprises. The government has become the leading force in the national innovation system. The government's R&amp;D investment, as an important means and method for the government to lead the national innovation system, is attracting more and more attention. Meanwhile, as the key leader grasping enterprise direction, executives play a decisive role in the development decision of enterprises. The executives of big data and artificial intelligence listed companies in China must realize that the core of development is independent innovation. It is especially of practical significance and research value to study the relationship between executive incentives of big data and artificial intelligence listed companies, government technology subsidies and innovation performance, and propose corresponding suggestions for the future development direction of new economy.},
booktitle = {Proceedings of the International Symposium on Big Data and Artificial Intelligence},
pages = {57–62},
numpages = {6},
keywords = {government R&amp;D investment, big data and artificial intelligence listed companies, innovation performance, executive incentives},
location = {Hong Kong, Hong Kong},
series = {ISBDAI '18}
}

@inproceedings{10.1145/2903220.2903255,
author = {Tsapanos, Nikolaos and Tefas, Anastasios and Nikolaidis, Nikolaos and Pitas, Ioannis},
title = {Efficient MapReduce Kernel K-Means for Big Data Clustering},
year = {2016},
isbn = {9781450337342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2903220.2903255},
doi = {10.1145/2903220.2903255},
abstract = {Data clustering is an unsupervised learning task that has found many applications in various scientific fields. The goal is to find subgroups of closely related data samples (clusters) in a set of unlabeled data. A classic clustering algorithm is the so-called k-Means. It is very popular, however, it is also unable to handle cases in which the clusters are not linearly separable. Kernel k-Means is a state of the art clustering algorithm, which employs the kernel trick, in order to perform clustering on a higher dimensionality space, thus overcoming the limitations of classic k-Means regarding the non linear separability of the input data. It has recently received a distributed implementation, named Trimmed Kernel k-Means, following the MapReduce distributed computing model. In addition to performing the computations in a distributed manner, Trimmed Kernel k-Means also trims the kernel matrix, in order to reduce the memory requirements and improve performance. The trimming of each row of the kernel matrix is achieved by attempting to estimate the cardinality of the cluster that the corresponding sample belongs to, and removing the kernel matrix entries connecting the sample to samples that probably belong to another cluster. The Spark cluster computing framework was used for the distributed implementation. In this paper, we present a distributed clustering scheme that is based on Trimmed Kernel k-Means, which employs subsampling, in order to be able to efficiently perform clustering on an extremely large dataset. The results indicate that the proposed method run much faster than the original Trimmed Kernel k-Means, while still providing clustering performance competitive with other state of the art kernel approaches.},
booktitle = {Proceedings of the 9th Hellenic Conference on Artificial Intelligence},
articleno = {28},
numpages = {5},
keywords = {MapReduce, distributed computing, Big Data, Kernel k-Means, clustering},
location = {Thessaloniki, Greece},
series = {SETN '16}
}

@inproceedings{10.1145/3366030.3366044,
author = {Cuzzocrea, Alfredo},
title = {Big Data Management and Analytics in Intelligent Smart Environments: State-of-the-Art Analysis and Future Research Directions},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366044},
doi = {10.1145/3366030.3366044},
abstract = {This paper focuses on big data management and analytics in intelligent smart environments, with particular regards to intelligent transportation and logistics systems, and provides relevant research directions that may represent a milestone for future years.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {5–7},
numpages = {3},
keywords = {Big data analytics, Intelligent smart environments, Big data management},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3321454.3321461,
author = {Zhao, Haijun and Zhao, Bang and Cheng, Susu},
title = {The Mechanism of Confirming Big Data Property Rights Based on Smart Contract},
year = {2019},
isbn = {9781450366335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321454.3321461},
doi = {10.1145/3321454.3321461},
abstract = {The confirmation of data property rights is one of the most important functions of big data trading institutions. This paper builds a specialized classifier which aims to the normalization process of data property confirmation in the process of big data transaction and proposes the mechanism of confirming big data property rights based on Smart Contract.},
booktitle = {Proceedings of the 2019 4th International Conference on Intelligent Information Technology},
pages = {78–82},
numpages = {5},
keywords = {Big Data Exchange, Smart contract, Confirmation of Data Property, Mechanism},
location = {Da, Nang, Viet Nam},
series = {ICIIT '19}
}

@article{10.1145/2740965,
author = {Sha, Kewei and Zeadally, Sherali},
title = {Data Quality Challenges in Cyber-Physical Systems},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2–3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2740965},
doi = {10.1145/2740965},
journal = {J. Data and Information Quality},
month = {jun},
articleno = {8},
numpages = {4},
keywords = {Data quality, cyber physical systems, faculty data detection}
}

@inproceedings{10.1145/3006299.3006317,
author = {Razaq, Abdul and Tianfield, Huaglory and Barrie, Peter},
title = {A Big Data Analytics Based Approach to Anomaly Detection},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006317},
doi = {10.1145/3006299.3006317},
abstract = {We present a novel Cyber Security analytics framework. We demonstrate a comprehensive cyber security monitoring system to construct cyber security correlated events with feature selection to anticipate behaviour based on various sensors.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {187–193},
numpages = {7},
keywords = {SIEM, security analytics, advanced persistent threats, event correlation, IDS/IPS, process auditing},
location = {Shanghai, China},
series = {BDCAT '16}
}

@inproceedings{10.5555/2627817.2627920,
author = {Feldman, Dan and Schmidt, Melanie and Sohler, Christian},
title = {Turning Big Data into Tiny Data: Constant-Size Coresets for k-Means, PCA and Projective Clustering},
year = {2013},
isbn = {9781611972511},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {We prove that the sum of the squared Euclidean distances from the n rows of an n x d matrix A to any compact set that is spanned by k vectors in @@@@d can be approximated up to (1 + ε)-factor, for an arbitrary small ε &gt; 0, using the O(k/ε2)-rank approximation of A and a constant. This implies, for example, that the optimal k-means clustering of the rows of A is (1 + ε)-approximated by an optimal k-means clustering of their projection on the O(k/ε2) first right singular vectors (principle components) of A.A (j, k)-coreset for projective clustering is a small set of points that yields a (1 + ε)-approximation to the sum of squared distances from the n rows of A to any set of k affine subspaces, each of dimension at most j. Our embedding yields (0, k)-coresets of size O(k) for handling k-means queries, (j, 1)-coresets of size O(j) for PCA queries, and (j, k)-coresets of size (log n)O(jk) for any j, k ≥ 1 and constant ε ε (0, 1/2). Previous coresets usually have a size which is linearly or even exponentially dependent of d, which makes them useless when d ~ n.Using our coresets with the merge-and-reduce approach, we obtain embarrassingly parallel streaming algorithms for problems such as k-means, PCA and projective clustering. These algorithms use update time per point and memory that is polynomial in log n and only linear in d.For cost functions other than squared Euclidean distances we suggest a simple recursive coreset construction that produces coresets of size @@@@ for k-means and a special class of bregman divergences that is less dependent on the properties of the squared Euclidean distance.},
booktitle = {Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {1434–1453},
numpages = {20},
location = {New Orleans, Louisiana},
series = {SODA '13}
}

@inproceedings{10.1145/2660517.2660529,
author = {Bissiriou, Cherif A. A. and Chaoui, Habiba},
title = {Big Data Analysis and Query Optimization Improve HadoopDB Performance},
year = {2014},
isbn = {9781450329279},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660517.2660529},
doi = {10.1145/2660517.2660529},
abstract = {High performance and scalability are two essentials requirements for data analytics systems as the amount of data being collected, stored and processed continue to grow rapidly. In this paper, we propose a new approach based on HadoopDB. Our main goal is to improve HadoopDB performance by adding some components. To achieve this, we incorporate a fast and space-efficient data placement structure in MapReduce-based Warehouse systems and another SQL-to-MapReduce translator. We also replace the initial Database implemented in HadoopDB with other column oriented Database. In addition we add security mechanism to protect MapReduce processing integrity.},
booktitle = {Proceedings of the 10th International Conference on Semantic Systems},
pages = {1–4},
numpages = {4},
keywords = {query execution, optimization, MapReduce, big data, Hadoop},
location = {Leipzig, Germany},
series = {SEM '14}
}

@inproceedings{10.1145/3396452.3396464,
author = {Chen, Chongyang and Xu, Wei},
title = {Innovation and Application of College Students' Education and Management Based on Big Data},
year = {2020},
isbn = {9781450374989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396452.3396464},
doi = {10.1145/3396452.3396464},
abstract = {With the rapid development and popularization of big data technology in the world and the world's vigorous promotion of informatization, the education and management of college students are facing new opportunities and challenges. This paper takes the innovation and application of big data technology in college students' education and management as the core, analyzes the significance and advantages of big data in the education and management of college students, and deeply studies the innovative application of big data from the aspects of education and management resource sharing, digital education and management and open education and management. In addition, It will take the innovative application of big data in college information announcement system as an example to comprehensively improve the level of college students' education and management by big data mining in the system and other ways.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Big Data and Education},
pages = {5–9},
numpages = {5},
keywords = {application, innovation, college, big data, education and management},
location = {London, United Kingdom},
series = {ICBDE '20}
}

@inproceedings{10.1145/3236024.3264586,
author = {Gulzar, Muhammad Ali and Wang, Siman and Kim, Miryung},
title = {BigSift: Automated Debugging of Big Data Analytics in Data-Intensive Scalable Computing},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264586},
doi = {10.1145/3236024.3264586},
abstract = {Developing Big Data Analytics often involves trial and error debugging, due to the unclean nature of datasets or wrong assumptions made about data. When errors (e.g. program crash, outlier results, etc.) arise, developers are often interested in pinpointing the root cause of errors. To address this problem, BigSift takes an Apache Spark program, a user-defined test oracle function, and a dataset as input and outputs a minimum set of input records that reproduces the same test failure by combining the insights from delta debugging with data provenance. The technical contribution of BigSift is the design of systems optimizations that bring automated debugging closer to a reality for data intensive scalable computing. BigSift exposes an interactive web interface where a user can monitor a big data analytics job running remotely on the cloud, write a user-defined test oracle function, and then trigger the automated debugging process. BigSift also provides a set of predefined test oracle functions, which can be used for explaining common types of anomalies in big data analytics--for example, finding the origin of the output value that is more than k standard deviations away from the median. The demonstration video is available at https://youtu.be/jdBsCd61a1Q.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {863–866},
numpages = {4},
keywords = {data-intensive scalable computing (DISC), Automated debugging, big data, and data cleaning, fault localization, data provenance},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3341069.3342996,
author = {Liu, Jun},
title = {Research on Knowledge Management Technology of Aerospace Engineering Based on Big Data},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3342996},
doi = {10.1145/3341069.3342996},
abstract = {In the era of big data, mass production, analysis and application of data have become a new trend. In the long-term design, production, operation and testing process of aerospace enterprises, a large number of valuable data have been generated. Collection and analysis of these data can improve the management of aerospace enterprises and gain competitive advantages. With the increase of semi-structured and unstructured data produced by aerospace enterprises year by year, how to store and analyze data, how to mine and share knowledge has become a major problem. The existing knowledge management system cannot meet the diversified needs of users only by traditional database technology. It also needs to combine distributed computing and storage technology to solve the problems of knowledge storage, knowledge sharing, knowledge mining, knowledge retrieval and recommendation in big data environment. Aerospace enterprises need to build a knowledge management system based on big data technology to support knowledge innovation and knowledge application. From the perspective of data operation and relying on Hadoop ecosystem related big data technology, this paper constructs a knowledge management framework model for aerospace enterprises based on Hadoop.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {172–176},
numpages = {5},
keywords = {Hadoop, knowledge management, knowledge management system, Big data},
location = {Guangzhou, China},
series = {HPCCT '19}
}

@proceedings{10.1145/3366650,
title = {ICCBD 2019: Proceedings of the 2nd International Conference on Computing and Big Data},
year = {2019},
isbn = {9781450372909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 2nd International Conference on Computing and Big Data (ICCBD 2019) and its workshop the International Conference on Computer, Software Engineering and Applications (CSEA 2019) were held in Taichung Software Park, Taichung, Taiwan during October 18-20, 2019.},
location = {Taichung, Taiwan}
}

@inproceedings{10.1145/2783258.2790458,
author = {John, George},
title = {How Artificial Intelligence and Big Data Created Rocket Fuel: A Case Study},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2790458},
doi = {10.1145/2783258.2790458},
abstract = {In 2008, Rocket Fuel's founders saw a gap in the digital advertising market. None of the existing players were building autonomous systems based on big data and artificial intelligence, but instead they were offering fairly simple technology and relying on human campaign managers to drive success. Five years later in 2013, Rocket Fuel had the best technology IPO of the year on NASDAQ, reported $240 million in revenue, and was ranked by accounting firm Deloitte as the #1 fastest-growing technology company in North America. Along the way we learned that it's okay to be bold in our expectations of what is possible with fully autonomous systems, we learned that mainstream customers will buy advanced technology if it's delivered in a familiar way, and we also learned that it's incredibly difficult to debug the complex "robot psychology" when a number of complex autonomous systems interact. We also had excellent luck and timing: as we were building the company, real-time ad impression-level auctions with machine-to-machine buying and selling became commonplace, and marketers became increasingly focused on delivering better results for their company and delivering better personalized and relevant digital experiences for their customers. The case study presentation will present a fast-paced overview of the business and technology context for Rocket Fuel at inception and at present, key learnings and decisions, and the road ahead.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1629},
numpages = {1},
keywords = {big data analytics, artificial intelligence, advertising, real-time bidding, computational advertising},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@inproceedings{10.1145/3156346.3156694,
author = {Bandeira, Nuno},
title = {Revealing Deep Proteome Diversity with Community-Scale Proteomics Big Data},
year = {2017},
isbn = {9781450353502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3156346.3156694},
doi = {10.1145/3156346.3156694},
abstract = {Translating the growing volumes of proteomics mass spectrometry data into reusable evidence of the occurrence and provenance of proteomics events requires the development of novel algorithms and community-scale computational workflows. MassIVE (http://massive.ucsd.edu) proposes to address this challenge in three stages.First, systematic annotation of human proteomics big data requires automated reanalysis of all public data using open source workflows with detailed records of search parameters and of individual Peptide Spectrum Matches (PSMs). As such, our large-scale reanalysis of tens of terabytes of human data has now increased the total number of proper public PSMs by over 10-fold to over 320 million PSMs whose coverage includes over 95Second, proper synthesis of community-scale search results into a reusable knowledge base (KB) requires scalable workflows imposing strict statistical controls. Our MassIVE-KB spectral library has thus properly assembled 2+ million precursors from over 1.5 million peptides covering over 6.2 million amino acids in the human proteome, all of which at least double the numbers covered by the popular NIST spectral libraries. Moreover, MassIVE-KB detects 723 novel proteins (PE 2-5) for a total of 16,852 proteins observed in non-synthetic LCMS runs and 19,610 total proteins when including the recent ProteomeTools data.Third, we show how advanced identification algorithms combine with public data to reveal dozens of unexpected putative modifications supported by multiple highly-correlated spectra. These show that protein regions can be observed in over 100 different variants with various combinations of post-translational modifications and cleavage events, thus suggesting that current coverage of proteome diversity (at 1.3 variants per protein region) is far below what is observable in experimental data.},
booktitle = {Proceedings of the 8th International Conference on Computational Systems-Biology and Bioinformatics},
pages = {2},
numpages = {1},
location = {Nha Trang City, Viet Nam},
series = {CSBio '17}
}

@proceedings{10.1145/3220199,
title = {ICBDC '18: Proceedings of the 3rd International Conference on Big Data and Computing},
year = {2018},
isbn = {9781450364263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We would like to extend our warmest welcome to all of you to Shenzhen city, China's first Special Economic Zone, to attend 2018 International Conference on Big Data and Computing (ICBDC 2018), from 28 April to 30 April, 2018.},
location = {Shenzhen, China}
}

@article{10.1145/269012.269024,
author = {Kaplan, David and Krishnan, Ramayya and Padman, Rema and Peters, James},
title = {Assessing Data Quality in Accounting Information Systems},
year = {1998},
issue_date = {Feb. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/269012.269024},
doi = {10.1145/269012.269024},
journal = {Commun. ACM},
month = {feb},
pages = {72–78},
numpages = {7}
}

@inproceedings{10.1145/3416028.3416029,
author = {Shiau, Yeajou},
title = {University Dropout Prevention through the Application of Big Data},
year = {2020},
isbn = {9781450375467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416028.3416029},
doi = {10.1145/3416028.3416029},
abstract = {This study explores the reasons for the suspension and dropout of full-time university students at a university in Taiwan and suggests better timing and strategies for student counseling. In this study, the narrative statistical analysis is used to analyze and discuss the sample objects, and then use data mining technology to find characteristic phenomena and classification conditions of the students who are suspended or dropout. Other studies related to dropouts rarely use leading indicators to predict the student dropout probability in real-time, most likely because of the timeliness and availability of student data. Therefore, this study proposes to use daily changes in absence indicators as predictive variables. Through the use of discriminant analysis to construct discriminant functions, the coefficient value of each student's withdrawal from university and early warning threshold for determining withdrawal from university can be presented in real-time in order to effectively provide the student with immediate counseling.},
booktitle = {Proceedings of the 2020 3rd International Conference on Information Management and Management Science},
pages = {1–7},
numpages = {7},
keywords = {Bayesian probability classification table, Counseling decision, Data mining, Decision tree model},
location = {London, United Kingdom},
series = {IMMS 2020}
}

@inproceedings{10.1145/3299902.3311063,
author = {Nurvitadhi, Eriko},
title = {FPGA-Based Computing in the Era of AI and Big Data},
year = {2019},
isbn = {9781450362535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299902.3311063},
doi = {10.1145/3299902.3311063},
abstract = {The continued rapid growth of data, along with advances in Artificial Intelligence (AI) to extract knowledge from such data, is reshaping the computing ecosystem landscape. With AI becoming an essential part of almost every end-user application, our current computing platforms are facing several challenges. The data-intensive nature of current AI models requires minimizing data movement. Furthermore, interactive intelligent datacenter-scale services require scalable and real-time solutions to provide a compelling user experience. Finally, algorithmic innovations in AI demand a flexible and programmable computing platform that can keep up with this rapidly changing field. We believe that these trends and their accompanying challenges present tremendous opportunities for FPGAs. FPGAs are a natural substrate to provide a programmable, near-data, real-time, and scalable platform for AI analytics. FPGAs are already embedded in several places where data flows throughout the computing ecosystem (e.g., "smart" network/storage, near image/audio sensors). Intel FPGAs are System-in-Package (SiP), scalable with 2.5D chiplets. They are also scalable at datacenter-scale as reconfigurable cloud, enabling real-time AI services. Using overlays, FPGAs can be programmed through software without needing long-running RTL synthesis. With further innovations, and leveraging their existing strengths, FPGAs can leap forward to realize their true potentials in AI analytics. In this talk, we first discuss the current trends in AI and big data. We then present trends in FPGA and opportunities for FPGAs in the era of AI and big data. Finally, we highlight selected research efforts to seize some of these opportunities: (1) 2.5D SiP integration of FPGA and AI chiplets to improve the performance and efficiency of AI workloads, and (2) AI overlay for FPGA to facilitate software-level programmability and compilation-speed.},
booktitle = {Proceedings of the 2019 International Symposium on Physical Design},
pages = {35},
numpages = {1},
keywords = {fpga, artificial intelligence, big data},
location = {San Francisco, CA, USA},
series = {ISPD '19}
}

@inproceedings{10.1145/2905055.2905326,
author = {Shekhar, Nikkita and Pawar, Ambika V.},
title = {Big Data Analytics Based on In-Memory Infrastructure On Traditional HPC: A Survey},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905326},
doi = {10.1145/2905055.2905326},
abstract = {As the capacity of main memory is growing, in-memory based big data analytics is becoming more popular. In-memory technologies support interactive analysis by providing high I/O throughput. On traditional high performance computing (HPC), big data processing needs data-intensive as well as computation-intensive systems for large data storage and high speed processing respectively. Currently, there are many such tools and technologies available which supports memory centric data processing to perform analysis on them. Taking advantage of in-memory on a HPC platform can result in a high speed, more reliable and fault tolerant data analysis. In this paper, we survey the existing storage and computation engines to perform big data analysis, and their performance while integrating together. Also, we discuss the contribution of such infrastructures in solving many I/O intensive analytical issues.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {110},
numpages = {5},
keywords = {Tachyon, Big-data, In-memory, I/O throughput, Hadoop, Spark, High performance Computing, Analytics},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3318464.3384677,
author = {Wang, Jin and Xiao, Guorui and Gu, Jiaqi and Wu, Jiacheng and Zaniolo, Carlo},
title = {RASQL: A Powerful Language and Its System for Big Data Applications},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384677},
doi = {10.1145/3318464.3384677},
abstract = {There is a growing interest in supporting advanced Big Data applications on distributed data processing platforms. Most of these systems support SQL or its dialect as the query interface due to its portability and declarative nature. However, current SQL standard cannot effectively express advanced analytical queries due to its limitation in supporting recursive queries. In this demonstration, we show that this problem can be resolved via a simple SQL extension that delivers greater expressive power by allowing aggregates in recursion. To this end, we propose the Recursive-aggregate-SQL (RASQL) language and its system on top of Apache Spark to express and execute complex queries and declarative algorithms in many applications, such as graph search and machine learning. With a variety of examples, we will (i) show how complicated analytic queries can be expressed with RASQL; (ii) illustrate formal semantics of the powerful new constructs; and (iii) present a user-friendly interface to interact with the RASQL system and monitor the query results.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2673–2676},
numpages = {4},
keywords = {recursive query, query language, big data},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3017611.3017627,
author = {Liu, Kuien and Wang, Haozhou and Yao, Yandong},
title = {On Storing and Retrieving Geospatial Big-Data in Cloud},
year = {2016},
isbn = {9781450345804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017611.3017627},
doi = {10.1145/3017611.3017627},
abstract = {Cloud storage is a kind of external storage which can provide by unlimited storage space with high availability and low cost on maintenance. On the other side, the size of geospatial data is too large and is increasing dramatically which makes such data is hard to be stored in the local data warehouse. Hence following the benefits of Cloud storage, such geospatial data is suitable to be stored in Cloud storage and managed by local data warehouse. However, there is a gap between Cloud storages and data warehouses built on traditional infrastructures, such as the mostly adopted massive parallel processing (MPP) based data warehouse. Therefore, in this paper, we propose a middleware-like architecture to connect MPP data warehouse and Cloud storage. It supports traditional geospatial data retrieving while integrating the Cloud storage lineage by a set of technical designs. Based on the prototype system and practical data, we demonstrate the appreciable performance and the flexibility for other third parties' development. Another major contribution of this paper is that we implement the prototype on open-source data warehouse and we make it open-sourced to public.},
booktitle = {Proceedings of the Second ACM SIGSPATIALInternational Workshop on the Use of GIS in Emergency Management},
articleno = {16},
numpages = {4},
keywords = {cloud storage, geospatial, data warehouse},
location = {Burlingame, California},
series = {EM-GIS '16}
}

@inproceedings{10.1145/3017995.3017998,
author = {Mahapatra, Tanmaya and Gerostathopoulos, Ilias and Prehofer, Christian},
title = {Towards Integration of Big Data Analytics in Internet of Things Mashup Tools},
year = {2016},
isbn = {9781450348744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017995.3017998},
doi = {10.1145/3017995.3017998},
abstract = {The increasing number and sensing capabilities of connected devices offer unique opportunities for developing sophisticated applications that employ data analysis as part of their business logic to make informed decisions based on sensed data. So far, mashup tools have been successful in supporting application development for Internet of Things. At the same time, Big Data analytics tools have allowed the analysis of very large and diverse data sets. The problem is that there is no consolidated development approach for integrating the two fields, IoT mashups and Big Data analytics. Such integration should go beyond merely specifying IoT mashups that only act as data providers. Mashup developers should also be able to specify Big Data analytics jobs and consume their results within a single application model. In this paper, we contribute to the direction of integrating Big Data analytics with IoT mashup tools by highlighting the need for such integration and the challenges that it entails via concrete examples. We also provide a research and development roadmap that can pave the way forward.},
booktitle = {Proceedings of the Seventh International Workshop on the Web of Things},
pages = {11–16},
numpages = {6},
keywords = {Big Data analytics, IoT mashups, Development support},
location = {Stuttgart, Germany},
series = {WoT '16}
}

@proceedings{10.1145/3206157,
title = {ICBDE '18: Proceedings of the 2018 International Conference on Big Data and Education},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICBDE 2018 provides a scientific platform for local and international scientists, engineers, and technologists, who work in all aspects of Big Data and Education. In addition to the contributed papers, internationally-known experts from several countries are also invited to deliver keynote speeches at ICBDE 2018.},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1145/3123024.3124411,
author = {Babar, Muhammad and Arif, Fahim},
title = {Smart Urban Planning Using Big Data Analytics Based Internet of Things},
year = {2017},
isbn = {9781450351904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123024.3124411},
doi = {10.1145/3123024.3124411},
abstract = {The extensive growth of the Internet of Things (IoT) is providing direction towards the smart urban. The smart urban is favored because it improves the standard of living of the citizens and provides excellence in the community services. The services may include but not limited to health, parking, transport, water, environment, power, and so forth. The diverse and heterogeneous environment of IoT and smart urban is challenged by real-time data processing and decision-making. In this research article, we propose IoT based smart urban architecture using Big Data analytics. The proposed architecture is divided into three different tiers: (1) data acquisition and aggregation, (2) data computation and processing, and (3) decision making and application. The proposed architecture is implemented and validated on Hadoop Ecosystem using reliable and authentic datasets. The research shows that the proposed system presents valuable imminent into the community development systems to get better the existing smart urban architecture.},
booktitle = {Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers},
pages = {397–402},
numpages = {6},
keywords = {IoT, big data analytics, smart city},
location = {Maui, Hawaii},
series = {UbiComp '17}
}

@inproceedings{10.1145/3388142.3388164,
author = {Safari, Zohreh and Mursi, Khalid T. and Zhuang, Yu},
title = {Fast Automatic Determination of Cluster Numbers for High Dimensional Big Data},
year = {2020},
isbn = {9781450376440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388142.3388164},
doi = {10.1145/3388142.3388164},
abstract = {For a large volume of data, the clustering algorithm is of significant importance to categorize and analyze data. Accordingly, choosing the optimal number of clusters (K) is an essential factor, but it also is a tricky problem in big data analysis. More importantly, it is to efficiently determine the best K automatically, which is the main issue in clustering algorithms. Indeed, considering both the quality and efficiency of the clustering algorithm during defining K can be a trade-off that is our primary purpose to overcome. K-Means is still one of the popular clustering algorithms, which has a shortcoming that K needs to be pre-set. We introduce a new process with fewer K-Means running, which selects the most promising time to run the K-Means algorithm. To achieve this goal, we applied Bisecting K-Means and a different splitting measure, which all are contributed to efficiently determine the number of clusters automatically while maintaining the quality of clustering for a large set of high dimensional data. We carried out our experimental studies on different data sets and found that our procedure has the flexibility of choosing different criteria for determining the optimal K under each of them. Experiments indicate higher efficiency through decreasing of computation cost compared with the Ray&amp;Turi method or with the use of only the K-Means algorithm.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis},
pages = {50–57},
numpages = {8},
keywords = {K-Means, Clustering, Cluster Validity, Bisecting K-Means, Big Data},
location = {Silicon Valley, CA, USA},
series = {ICCDA 2020}
}

@inproceedings{10.1145/3400903.3400932,
author = {Uzunbaz, Serkan and Aref, Walid G.},
title = {Shared Execution Techniques for Business Data Analytics over Big Data Streams},
year = {2020},
isbn = {9781450388146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400903.3400932},
doi = {10.1145/3400903.3400932},
abstract = {Business Data Analytics require processing of large numbers of data streams and the creation of materialized views in order to provide near real-time answers to user queries. Materializing the view of each query and refreshing it continuously as a separate query execution plan is not efficient and is not scalable. In this paper, we present a global query execution plan to simultaneously support multiple queries, and minimize the number of input scans, operators, and tuples flowing between the operators. We propose shared-execution techniques for creating and maintaining materialized views in support of business data analytics queries. We utilize commonalities in multiple business data analytics queries to support scalable and efficient processing of big data streams. The paper highlights shared execution techniques for select predicates, group, and aggregate calculations. We present how global query execution plans are run in a distributed stream processing system, called INGA which is built on top of Storm. In INGA, we are able to support online view maintenance of 2500 materialized views using 237 queries by utilizing the shared constructs between the queries. We are able to run all 237 queries using a single global query execution plan tree with depth of 21.},
booktitle = {32nd International Conference on Scientific and Statistical Database Management},
articleno = {25},
numpages = {4},
keywords = {big data, INGA, stream processing, business data analytics, online view maintenance},
location = {Vienna, Austria},
series = {SSDBM 2020}
}

