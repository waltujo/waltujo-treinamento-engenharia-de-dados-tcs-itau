@inproceedings{10.1145/3014812.3014886,
author = {Qu, Youyang and Xu, Jiyang and Yu, Shui},
title = {Privacy Preserving in Big Data Sets through Multiple Shuffle},
year = {2017},
isbn = {9781450347686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014812.3014886},
doi = {10.1145/3014812.3014886},
abstract = {Big data privacy-preserving has attracted increasing attention of researchers in recent years. But existing models are so complicated and time-consuming that they are not easy to implement. In this paper, we propose a more feasible and efficient model for big data sets privacy-preserving using shuffling multiple attributes(M-Shuffle) to achieve a tradeoff between data utility and privacy. Our strategy is firstly categorize all the records into some groups using K-means algorithm according to the sensitive attributes. Then we choose the columns to be shuffled using entropy. At last we introduce the random shuffle algorithm to our model to break the correlation among the columns of big data sets. Experiments on real-world datasets show that our framework achieves excellent data utility and efficiency while satisfying privacy-preserving.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {72},
numpages = {8},
keywords = {privacy-preserving, M-shuffle machenism, K-means},
location = {Geelong, Australia},
series = {ACSW '17}
}

@article{10.14778/3352063.3352128,
author = {Chen, Zhimin and Wang, Yue and Narasayya, Vivek and Chaudhuri, Surajit},
title = {Customizable and Scalable Fuzzy Join for Big Data},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352128},
doi = {10.14778/3352063.3352128},
abstract = {Fuzzy join is an important primitive for data cleaning. The ability to customize fuzzy join is crucial to allow applications to address domain-specific data quality issues such as synonyms and abbreviations. While efficient indexing techniques exist for single-node implementations of customizable fuzzy join, the state-of-the-art scale-out techniques do not support customization, and exhibit poor performance and scalability characteristics. We describe the design of a scale-out fuzzy join operator that supports customization. We use a locality-sensitive-hashing (LSH) based signature scheme, and introduce optimizations that result in significant speed up with negligible impact on recall. We evaluate our implementation on the Azure Databricks version of Spark using several real-world and synthetic data sets. We observe speedups exceeding 50X compared to the best-known prior scale-out technique, and close to linear scalability with data size and number of nodes.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2106–2117},
numpages = {12}
}

@article{10.1145/3141248,
author = {Cappiello, C. and Cerletti, C. and Fratto, C. and Pernici, B.},
title = {Validating Data Quality Actions in Scoring Processes},
year = {2018},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3141248},
doi = {10.1145/3141248},
abstract = {Data quality has gained momentum among organizations upon the realization that poor data quality might cause failures and/or inefficiencies, thus compromising business processes and application results. However, enterprises often adopt data quality assessment and improvement methods based on practical and empirical approaches without conducting a rigorous analysis of the data quality issues and outcome of the enacted data quality improvement practices. In particular, data quality management, especially the identification of the data quality dimensions to be monitored and improved, is performed by knowledge workers on the basis of their skills and experience. Control methods are therefore designed on the basis of expected and evident quality problems; thus, these methods may not be effective in dealing with unknown and/or unexpected problems. This article aims to provide a methodology, based on fault injection, for validating the data quality actions used by organizations. We show how it is possible to check whether the adopted techniques properly monitor the real issues that may damage business processes. At this stage, we focus on scoring processes, i.e., those in which the output represents the evaluation or ranking of a specific object. We show the effectiveness of our proposal by means of a case study in the financial risk management area.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {11},
numpages = {27},
keywords = {Data quality, assessment, decision processes, decision support}
}

@inproceedings{10.1145/2649387.2660821,
author = {Biswas, Abhishek and Gauthier, David and Ranjan, Desh and Zubair, Mohammad},
title = {Big Data Challenges for Estimating Genome Assembler Quality},
year = {2014},
isbn = {9781450328944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2649387.2660821},
doi = {10.1145/2649387.2660821},
abstract = {The selection of an appropriate assembler is important to obtain best assembly of a fragment dataset, avoid misassembles and minimize further finishing effort. It is known that the assembly quality of assemblers is dependent on the input data parameters such as DNA fragmentation parameters and genome sequence structure. To the best of our knowledge no large scale systematic effort has been made in quantifying the quality of the assembly generated by various assemblers over a range of input parameters. The correlation between input parameters and assembler quality can be used to define the characteristics of an assembler and design an optimal assembler selection algorithm. The critical barrier is the computational challenge of assembling simulated high-throughput sequence libraries of thousands of genomes with input parameters varied to cover the spectrum of values obtained from major sequencers available to biologists today. We present a study to show that a quantifiable correlation can be drawn between their input and output characteristics for four major open-source assemblers. Based on our result we propose a simple model to estimate the quality of assemblies generated by these assemblers for given input parameters.},
booktitle = {Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {653–660},
numpages = {8},
keywords = {big data, genome fragmentation parameters, assembly quality model, assembler characteristics},
location = {Newport Beach, California},
series = {BCB '14}
}

@inproceedings{10.1145/2460999.2461024,
author = {Bosu, Michael Franklin and MacDonell, Stephen G.},
title = {Data Quality in Empirical Software Engineering: A Targeted Review},
year = {2013},
isbn = {9781450318488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460999.2461024},
doi = {10.1145/2460999.2461024},
abstract = {Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to September 2012. A total of 221 relevant studies met our inclusion criteria and were characterized in terms of their consideration and treatment of data quality. Results: We obtained useful insights as to how the ESE community considers these three elements of data quality. Only 23 of these 221 studies reported on all three elements of data quality considered in this paper. Conclusion: The reporting of data collection procedures is not documented consistently in ESE studies. It will be useful if data collection challenges are reported in order to improve our understanding of why there are problems with software engineering data sets and the models developed from them. More generally, data quality should be given far greater attention by the community. The improvement of data sets through enhanced data collection, pre-processing and quality assessment should lead to more reliable prediction models, thus improving the practice of software engineering.},
booktitle = {Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering},
pages = {171–176},
numpages = {6},
keywords = {literature review, data quality, data sets, empirical software engineering},
location = {Porto de Galinhas, Brazil},
series = {EASE '13}
}

@inproceedings{10.1145/3053600.3053630,
author = {Ciavotta, Michele and Gianniti, Eugenio and Ardagna, Danilo},
title = {Capacity Allocation for Big Data Applications in the Cloud},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053630},
doi = {10.1145/3053600.3053630},
abstract = {The aim of this work is to present the problem of Capacity Allocation for multiple classes of Big Data applications running in the Cloud. The objective is the minimization of the renting out costs subject to the fulfillment of QoS requirements expressed in terms of application deadlines. We propose a preliminary version of a tool embedding a local-search-based algorithm exploiting also an integer nonlinear mathematical formulation and a queueing network simulation to solve the problem.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {175–176},
numpages = {2},
keywords = {big data, cloud, qos, capacity allocation.},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/3148055.3148068,
author = {Javed, M. Haseeb and Lu, Xiaoyi and Panda, Dhabaleswar K. (DK)},
title = {Characterization of Big Data Stream Processing Pipeline: A Case Study Using Flink and Kafka},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148068},
doi = {10.1145/3148055.3148068},
abstract = {In recent years there has been a surge in applications focusing on streaming data to generate insights in real-time. Both academia, as well as industry, have tried to address this use case by developing a variety of Stream Processing Engines (SPEs) with a diverse feature set. On the other hand, Big Data applications have started to make use of High-Performance Computing (HPC) which possess superior memory, I/O, and networking resources compared to typical Big Data clusters. Recent studies evaluating the performance of SPEs have focused on commodity clusters. However, exhaustive studies need to be performed to profile individual stages of a stream processing pipeline and how best to optimize each of these stages to best leverage the resources provided by HPC clusters. To address this issue, we profile the performance of a big data streaming pipeline using Apache Flink as the SPE and Apache Kafka as the intermediate message queue. We break the streaming pipeline into two distinct phases and evaluate percentile latencies for two different networks, namely 40GbE and InfiniBand EDR (100Gbps), to determine if a typical streaming application is network intensive enough to benefit from a faster interconnect. Moreover, we explore whether the volume of input data stream has any effect on the latency characteristics of the streaming pipeline, and if so how does it compare for different stages in the streaming pipeline and different network interconnects. Our experiments show an increase of over 10x in 98 percentile latency when input stream volume is increased from 128MB/s to 256MB/s. Moreover, we find the intermediate stages of the stream pipeline to be a significant contributor to the overall latency of the system.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {1–10},
numpages = {10},
keywords = {hpc clusters, stream processing, profiling, real time, big data, message queue},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.1145/2487575.2487677,
author = {Canny, John and Zhao, Huasha},
title = {Big Data Analytics with Small Footprint: Squaring the Cloud},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487677},
doi = {10.1145/2487575.2487677},
abstract = {This paper describes the BID Data Suite, a collection of hardware, software and design patterns that enable fast, large-scale data mining at very low cost. By co-designing all of these elements we achieve single-machine performance levels that equal or exceed reported cluster implementations for common benchmark problems. A key design criterion is rapid exploration of models, hence the system is interactive and primarily single-user. The elements of the suite are: (i) the data engine, a hardware design pattern that balances storage, CPU and GPU acceleration for typical data mining workloads, (ii) BIDMat, an interactive matrix library that integrates CPU and GPU acceleration and novel computational kernels (iii), BIDMach, a machine learning system that includes very efficient model optimizers, (iv) Butterfly mixing, a communication strategy that hides the latency of frequent model updates needed by fast optimizers and (v) Design patterns to improve performance of iterative update algorithms. We present several benchmark problems to show how the above elements combine to yield multiple orders-of-magnitude improvements for each problem.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {95–103},
numpages = {9},
keywords = {gpu, toolkit, cluster, data mining, machine learning},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/3281375.3281386,
author = {Rinaldi, Antonio M. and Russo, Cristiano},
title = {A Semantic-Based Model to Represent Multimedia Big Data},
year = {2018},
isbn = {9781450356220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281375.3281386},
doi = {10.1145/3281375.3281386},
abstract = {The use of formal representation is a key task in the era of big data. In the context of multimedia big data this issue is stressed due to the intrinsic complexity nature of this kind of data. Moreover, the relations among objects should be clearly expressed and formalized to give the right meaning of data correlation. For this reason the design of formal models to represent and manage information is a necessary task to implement intelligent information systems. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of big data applications. Using these models the visualization of data and information become an intrinsic and strategic task for the analysis and exploration of multimedia BigData. In this paper we propose the use of a semantic approach to formalize the model structure of multimedia BigData. In addition, the recognition of multimodal features to represent concepts and linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. The proposed model has been implemented in a NoSQL graph database populated from different knowledge sources and a visualization of this very large knowledge base has been presented and discussed as a case study.},
booktitle = {Proceedings of the 10th International Conference on Management of Digital EcoSystems},
pages = {31–38},
numpages = {8},
keywords = {semantic bigdata, semantics, multimedia ontologies},
location = {Tokyo, Japan},
series = {MEDES '18}
}

@inproceedings{10.1109/CCGRID.2017.143,
author = {Salaria, Shweta and Brown, Kevin and Jitsumoto, Hideyuki and Matsuoka, Satoshi},
title = {Evaluation of HPC-Big Data Applications Using Cloud Platforms},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.143},
doi = {10.1109/CCGRID.2017.143},
abstract = {The path to HPC-Big Data convergence has resulted in numerous researches that demonstrate the performance trade-off between running applications on supercomputers and cloud platforms. Previous studies typically focus either on scientific HPC benchmarks or previous cloud configurations, failing to consider all the new opportunities offered by current cloud offerings. We present a comparative study of the performance of representative big data benchmarks, or "Big Data Ogres", and HPC benchmarks running on supercomputer and cloud. Our work distinguishes itself from previous studies in a way that we explore the latest generation of compute-optimized Amazon Elastic Compute Cloud instances, C4 for our experimentation on cloud. Our results reveal that Amazon C4 instances with increased compute performance and low variability in results make EC2-based cluster feasible for scientific computing and its applications in simulations, modeling and analysis.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {1053–1061},
numpages = {9},
keywords = {Supercomputers, Amazon EC2 C4, Performance evaluation, Graph500},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3400903.3409117,
author = {Cuzzocrea, Alfredo},
title = {Multidimensional Clustering over Big Data: Models, Issues, Analysis, Emerging Trends},
year = {2020},
isbn = {9781450388146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400903.3409117},
doi = {10.1145/3400903.3409117},
abstract = {Clustering is an essential task of the whole pattern recognition process, and it can serve under several roles, for instance in terms of data pre-processing tool for better (i.e., more accurate) pattern recognition analysis and mining. In this vest, a critical applicative setting is represented by applying pattern recognition tools over emerging big data. Here, clustering specially plays a challenging role within the context of this conceptual mining framework, and, under a larger vision, it can act as pre-processing task for general big data clustering problems. In this paper, we first focus on state-of-the-art solutions for big data clustering in the specific pattern recognition context, by highlighting benefits and limitations. Then, we focus the attention on the problem of effectively and efficiently clustering big data via innovative multidimensional metaphors, thus achieving the definition of so-called multidimensional clustering over big data. In this so-delineated research setting, based on the well-known challenges of big data management (e.g., volume, velocity, variety, and veracity), we provide critical review and discussion, complemented by a rich set of research directions, development perspectives and emerging trends of the investigated topics, as contextualized in the reference big-data-analytics scientific area.},
booktitle = {32nd International Conference on Scientific and Statistical Database Management},
articleno = {32},
numpages = {6},
keywords = {Multidimensional Clustering over Big Data, Big Data, Big Data Clustering, Big Data Analytics},
location = {Vienna, Austria},
series = {SSDBM 2020}
}

@inproceedings{10.1145/2910896.2925466,
author = {Farag, Mohamed and Nakate, Pranav and Fox, Edward A.},
title = {Big Data Processing of School Shooting Archives},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2925466},
doi = {10.1145/2910896.2925466},
abstract = {Web archives about school shootings consist of webpages that may or may not be relevant to the events of interest. There are 3 main goals of this work; first is to clean the webpages, which involves getting rid of the stop words and non-relevant parts of a webpage. The second goal is to select just webpages relevant to the events of interest. The third goal is to upload the cleaned and relevant webpages to Apache Solr so that they are easily accessible. We show the details of all the steps required to achieve these goals. The results show that representative Web archives are noisy, with 2% - 40% relevant content. By cleaning the archives, we aid researchers to focus on relevant content for their analysis.},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {271–272},
numpages = {2},
keywords = {classification, big data processing, digital libraries, web archives},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.1145/2743065.2743110,
author = {Padmapriya, V. and Amudhavel, J. and Gowri, V. and Lakshmipriya, K. and Vinothini, S. and Kumar, K. Prem},
title = {Demystifying Challenges, Opportunities and Issues of Big Data Frameworks},
year = {2015},
isbn = {9781450334419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2743065.2743110},
doi = {10.1145/2743065.2743110},
abstract = {The Big data has a huge volume of data sets, which is used to process the conventional data processing applications. In this paper, the Flex Analytics framework is used to enhance the scalability and flexibility of analyzing and processing the data and provide visualization of data. The MapReduce framework of the fuzzy logic model is used in big data for the degree of uncertainty and to reduce the imbalance of data. The MapReduce framework for large-scale extreme learning machine for massive and dispersed data and machine learning approach is used to find the bottleneck in the peer-peer network. The emerging distributed cloud data centers with the security framework using Hadoop technology is used to process larger datasets. The financial data standard is utilized to eliminate the data redundancy and noise.},
booktitle = {Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering &amp; Technology (ICARCSET 2015)},
articleno = {45},
numpages = {5},
keywords = {exploration, Big data, infringement, application, Data analytics, unstructured data, monitor},
location = {Unnao, India},
series = {ICARCSET '15}
}

@inproceedings{10.1145/2213598.2213606,
author = {Endler, Gregor},
title = {Data Quality and Integration in Collaborative Environments},
year = {2012},
isbn = {9781450313261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2213598.2213606},
doi = {10.1145/2213598.2213606},
abstract = {The trend to merge medical practices into cooperatively operating networks and organizational units like Medical Supply Centers generates new challenges for an adequate IT support. In particular, new use cases for common economic planning, controlling and treatment coordination arise. This requires consolidation of data originating from heterogeneous and autonomous software systems. Heterogeneity and autonomy are core reasons for low data quality. The intuitive approach of initially integrating heterogeneous systems into a federated system creates a very high upfront effort before the system can become operable and does not adequately consider the fact that data quality requirements might change over time. To remedy this, we propose an approach for continuous data quality improvement which enables a demand driven step by step system integration. By adapting the generic Total Data Quality Management process to healthcare specific use cases, we are developing an extended model for continuous data quality management in cooperative healthcare settings. The IT tools which are needed to provide the information that drives this process are currently in development within a government supported project involving both industry and academia.},
booktitle = {Proceedings of the on SIGMOD/PODS 2012 PhD Symposium},
pages = {21–26},
numpages = {6},
keywords = {co-operative healthcare delivery, demand-driven improvement, data quality management},
location = {Scottsdale, Arizona, USA},
series = {PhD '12}
}

@inproceedings{10.1145/3299869.3319898,
author = {Ge, Chang and Li, Yinan and Eilebrecht, Eric and Chandramouli, Badrish and Kossmann, Donald},
title = {Speculative Distributed CSV Data Parsing for Big Data Analytics},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3319898},
doi = {10.1145/3299869.3319898},
abstract = {There has been a recent flurry of interest in providing query capability on raw data in today's big data systems. These raw data must be parsed before processing or use in analytics. Thus, a fundamental challenge in distributed big data systems is that of efficient parallel parsing of raw data. The difficulties come from the inherent ambiguity while independently parsing chunks of raw data without knowing the context of these chunks. Specifically, it can be difficult to find the beginnings and ends of fields and records in these chunks of raw data. To parallelize parsing, this paper proposes a speculation-based approach for the CSV format, arguably the most commonly used raw data format. Due to the syntactic and statistical properties of the format, speculative parsing rarely fails and therefore parsing is efficiently parallelized in a distributed setting. Our speculative approach is also robust, meaning that it can reliably detect syntax errors in CSV data. We experimentally evaluate the speculative, distributed parsing approach in Apache Spark using more than 11,000 real-world datasets, and show that our parser produces significant performance benefits over existing methods.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {883–899},
numpages = {17},
keywords = {parsing, csv, distributed, parallel},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@proceedings{10.1145/3066911,
title = {SBD '17: Proceedings of The International Workshop on Semantic Big Data},
year = {2017},
isbn = {9781450349871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The currentWorld-WideWeb enables an easy, instant access to a vast amount of online information. However, the content in theWeb is typically for human consumption, and is not tailored for machine processing. The Semantic Web is hence intended to establish a machine-understandable Web, and is currently also used in many other domains and not only in the Web. The World Wide Web Consortium (W3C) has developed a number of standards around this vision. Among them is the Resource Description Framework (RDF), which is used as the data model of the Semantic Web. The W3C has also defined SPARQL as the RDF query language, RIF as the rule language, and the ontology languages RDFS and OWL to describe schemas of RDF. The usage of common ontologies increases interoperability between heterogeneous data sets, and the proprietary ontologies with the additional abstraction layer facilitate the integration of these data sets. Therefore, we can argue that the Semantic Web is ideally designed to work in heterogeneous Big Data environments.We define Semantic Big Data as the intersection of Semantic Web data and Big Data. There are masses of Semantic Web data freely available to the public - thanks to the efforts of the linked data initiative. According to http://stats.lod2.eu/ the current freely available Semantic Web data is approximately 150 billion triples in over 2,800 datasets, many of which are accessible via SPARQL query servers called SPARQL endpoints. Everyone can submit SPARQL queries to SPARQL endpoints via a standardized protocol, where the queries are processed on the datasets of the SPARQL endpoints and the query results are sent back in a standardized format. Hence, not only Semantic Big Data is freely available, but also distributed execution environments for Semantic Big Data are freely accessible. This makes the Semantic Web an ideal playground for Big Data research.The goal of this workshop is to bring together academic researchers and industry practitioners to address the challenges and report and exchange the research findings in Semantic Big Data, including new approaches, techniques and applications, make substantial theoretical and empirical contributions to, and significantly advance the state of the art of Semantic Big Data.},
location = {Chicago, Illinois}
}

@inproceedings{10.1145/2379436.2379440,
author = {Chang, Jichuan and Lim, Kevin T. and Byrne, John and Ramirez, Laura and Ranganathan, Parthasarathy},
title = {Workload Diversity and Dynamics in Big Data Analytics: Implications to System Designers},
year = {2012},
isbn = {9781450314442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2379436.2379440},
doi = {10.1145/2379436.2379440},
abstract = {The emergence of big data analytics and the need for cost/energy efficient IT infrastructure motivate a new focus on data-centric designs. In this paper, we aim to better understand the design implications of data analytics systems by quantifying workload requirements and runtime dynamics. We examine four workloads representing big data analytics trends for fast decisions, total integration, deep analysis and fresh insights: an archive store, a columnar database enhanced with table compression, an analytics engine with distributed R, and a transaction/analytics hybrid system. These appliations demonstrate diverse resource requirements both within and across workloads as well as load imbalance due to data skew. Our observations suggest several directions to design balanced data analytics systems, including tight integration of heterogeneous, active data stores, support for efficient communication and data-centric load balancing.},
booktitle = {Proceedings of the 2nd Workshop on Architectures and Systems for Big Data},
pages = {21–26},
numpages = {6},
keywords = {big data, balanced system designs, analytics system, system architectures, workload diversity},
location = {Portland, Oregon, USA},
series = {ASBD '12}
}

@article{10.14778/2733004.2733069,
author = {Suchanek, Fabian M. and Weikum, Gerhard},
title = {Knowledge Bases in the Age of Big Data Analytics},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733069},
doi = {10.14778/2733004.2733069},
abstract = {This tutorial gives an overview on state-of-the-art methods for the automatic construction of large knowledge bases and harnessing them for data and text analytics. It covers both big-data methods for building knowledge bases and knowledge bases being assets for big-data applications. The tutorial also points out challenges and research opportunities.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1713–1714},
numpages = {2}
}

@inproceedings{10.1145/1012453.1012465,
author = {Cappiello, Cinzia and Francalanci, Chiara and Pernici, Barbara},
title = {Data Quality Assessment from the User's Perspective},
year = {2004},
isbn = {1581139020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1012453.1012465},
doi = {10.1145/1012453.1012465},
abstract = {The quality of data is often defined as "fitness for use", i.e., the ability of a data collection to meet user requirements. The assessment of data quality dimensions should consider the degree to which data satisfy users' needs. User expectations are clearly related to the selected services and at the same time a service can have different characteristics depending on the type of user that accesses it. The data quality assessment process has to consider both aspects and, consequently, select a suitable evaluation function to obtain a correct interpretation of results. This paper proposes a model that ties the assessment phase to user requirements. Multichannel information systems are considered as an example to show the applicability of the proposed model.},
booktitle = {Proceedings of the 2004 International Workshop on Information Quality in Information Systems},
pages = {68–73},
numpages = {6},
keywords = {user requirements, data quality, quality assessment},
location = {Paris, France},
series = {IQIS '04}
}

@inproceedings{10.5555/2819289.2819293,
author = {Cerqueus, Thomas and de Almeida, Eduardo Cunha and Scherzinger, Stefanie},
title = {Safely Managing Data Variety in Big Data Software Development},
year = {2015},
publisher = {IEEE Press},
abstract = {We consider the task of building Big Data software systems, offered as software-as-a-service. These applications are commonly backed by NoSQL data stores that address the proverbial Vs of Big Data processing: NoSQL data stores can handle large volumes of data and many systems do not enforce a global schema, to account for structural variety in data. Thus, software engineers can design the data model on the go, a flexibility that is particularly crucial in agile software development. However, NoSQL data stores commonly do not yet account for the veracity of changes when it comes to changes in the structure of persisted data. Yet this is an inevitable consequence of agile software development. In most NoSQL-based application stacks, schema evolution is completely handled within the application code, usually involving object mapper libraries. Yet simple code refactorings, such as renaming a class attribute at the source code level, can cause data loss or runtime errors once the application has been deployed to production. We address this pain point by contributing type checking rules that we have implemented within an IDE plugin. Our plugin ControVol statically type checks the object mapper class declarations against the code release history. ControVol is thus capable of detecting common yet risky cases of mismatched data and schema, and can even suggest automatic fixes.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {4–10},
numpages = {7},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@inproceedings{10.1145/3368756.3369080,
author = {Tantaoui, Mouad and Laanaoui, My Driss and Kabil, Mustapha},
title = {Towards an Efficient Vehicle Traffic Management Using Big Data},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369080},
doi = {10.1145/3368756.3369080},
abstract = {Big data has become essential given the mass of data generated by different domains and which has become impossible to manage by traditional data management tools; vehicular ad-hoc network is one of those areas that use the tools and technologies of data management of big data. In the present paper, we propose a method that aims to detect anomalies in the road and calculate the time spent in each road section in real time, which will allow us to have a base containing the estimated time spent in all sections in real time, it will help us to send to the vehicles the exact estimated time of arrival all along their way. This base will also allow us to detect an accident or anomaly in a section in real time as well.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {93},
numpages = {6},
keywords = {traffic congestions prediction, VANET, big data, traffic management, intelligent transportation systems (ITS)},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@article{10.14778/1920841.1921063,
author = {Agrawal, Divyakant and Das, Sudipto and El Abbadi, Amr},
title = {Big Data and Cloud Computing: New Wine or Just New Bottles?},
year = {2010},
issue_date = {September 2010},
publisher = {VLDB Endowment},
volume = {3},
number = {1–2},
issn = {2150-8097},
url = {https://doi.org/10.14778/1920841.1921063},
doi = {10.14778/1920841.1921063},
abstract = {Cloud computing is an extremely successful paradigm of service oriented computing and has revolutionized the way computing infrastructure is abstracted and used. Three most popular cloud paradigms include: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). The concept however can also be extended to Database as a Service and many more. Elasticity, pay-per-use, low upfront investment, low time to market, and transfer of risks are some of the major enabling features that make cloud computing a ubiquitous paradigm for deploying novel applications which were not economically feasible in a traditional enterprise infrastructure settings. This has seen a proliferation in the number of applications which leverage various cloud platforms, resulting in a tremendous increase in the scale of the data generated as well as consumed by such applications. Scalable database management systems (DBMS) -- both for update intensive application workloads, as well as decision support systems for descriptive and deep analytics -- are thus a critical part of cloud infrastructures.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {1647–1648},
numpages = {2}
}

@article{10.5555/3344051.3344077,
author = {DePratti, Roland},
title = {Using Jupyter Notebooks in a Big Data Programming Course},
year = {2019},
issue_date = {April 2019},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {34},
number = {6},
issn = {1937-4771},
abstract = {In a Big Data Programming course, students often need basic instruction in a new programming language, i.e. Python, Scala, R. Traditional programming language instruction involves a textbook and an Interactive Development Environment (IDE). In a course that already included two textbooks and instruction on Big Data frameworks, the author was looking for an effective way to deliver instructional text and the interactive development capabilities of an IDE that would not add additional cost to the student.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {157–159},
numpages = {3}
}

@inproceedings{10.1145/3265689.3265706,
author = {Huang, Yue},
title = {Impacts of Big Data on Data Mining Research: An Empirical Study of Chinese Journals},
year = {2018},
isbn = {9781450365871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265689.3265706},
doi = {10.1145/3265689.3265706},
abstract = {With the advent of big data, data mining theories and methods face new challenges. This paper tries to find the impacts of big data on data mining research through 23377 data mining-related papers published in Chinese academic journals during 1996--2016. By utilization of various methods of bibliometrics, this study conducts three different levels of analysis to gradually dig deeper into the contents of literature. For the macro-level, paper amount analysis results show that big data-related research began in 2012 and has brought new growth to data mining area. For the meso-level, journal distribution analysis results indicate that many other disciplines, such as arts and agriculture science, began to apply data mining techniques with the wide spread of big data. For the micro-level, co-word-based research topic clustering results imply that new topics emerged due to the easy access of big data, such as 'clouding computing' and 'teaching and learning analysis'.},
booktitle = {Proceedings of the 3rd International Conference on Crowd Science and Engineering},
articleno = {17},
numpages = {5},
keywords = {Bibliometrics, Data mining, Big data, Impact},
location = {Singapore, Singapore},
series = {ICCSE'18}
}

@inproceedings{10.1109/BDC.2014.10,
author = {Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay},
title = {A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning},
year = {2014},
isbn = {9781479918973},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BDC.2014.10},
doi = {10.1109/BDC.2014.10},
abstract = {In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing},
pages = {16–25},
numpages = {10},
keywords = {Distributed computing, Ensemble learning, Bayesian network, Kepler, Big Data, Scientific workflow, Hadoop},
series = {BDC '14}
}

@inproceedings{10.1145/2939672.2939859,
author = {Li, Qingyang and Qiu, Shuang and Ji, Shuiwang and Thompson, Paul M. and Ye, Jieping and Wang, Jie},
title = {Parallel Lasso Screening for Big Data Optimization},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939859},
doi = {10.1145/2939672.2939859},
abstract = {Lasso regression is a widely used technique in data mining for model selection and feature extraction. In many applications, it remains challenging to apply the regression model to large-scale problems that have massive data samples with high-dimensional features. One popular and promising strategy is to solve the Lasso problem in parallel. Parallel solvers run multiple cores in parallel on a shared memory system to speedup the computation, while the practical usage is limited by the huge dimension in the feature space. Screening is a promising method to solve the problem of high dimensionality by discarding the inactive features and removing them from optimization. However, when integrating screening methods with parallel solvers, most of solvers cannot guarantee the convergence on the reduced feature matrix. In this paper, we propose a novel parallel framework by parallelizing screening methods and integrating it with our proposed parallel solver. We propose two parallel screening algorithms: Parallel Strong Rule (PSR) and Parallel Dual Polytope Projection (PDPP). For the parallel solver, we proposed an Asynchronous Grouped Coordinate Descent method (AGCD) to optimize the regression problem in parallel on the reduced feature matrix. AGCD is based on a grouped selection strategy to select the coordinate that has the maximum descent for the objective function in a group of candidates. Empirical studies on the real-world datasets demonstrate that the proposed parallel framework has a superior performance compared to the state-of-the-art parallel solvers.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1705–1714},
numpages = {10},
keywords = {aynchronized coordinate descent, coordinate descent, lasso regression, screening rules, parallel computing},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3433996.3434027,
author = {Ding, Shifu and Liu, Yan and Zhang, Jianjun and Tan, Yaqi and Li, Xiaoxia and Tang, RuiChun},
title = {The Planning and Construction of Healthcare Big Data Platform},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434027},
doi = {10.1145/3433996.3434027},
abstract = {Healthcare Big Data Platform is the important content in the process of medical information industry. To a certain extent, it represents the overall level of the regional informatization. It is also a data exchange and sharing platform connecting the basic systems of local various medical and health institutions, and it is also the base and carrier to integrate the regional information system. This paper introduces the local regional medical informatization construction, planning architecture, data center construction mode and technical realization methods. Through this project, the informatization level of basic health agencies and all hospitals will have been greatly improved. It can provide more convenient and high-quality medical service for patients, alleviates "difficulty and expensive" problem effectively.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {170–176},
numpages = {7},
keywords = {Healthcare, SOA, Electronic Health Record, Big Data, EMR},
location = {Taiyuan, China},
series = {CAIH2020}
}

@proceedings{10.1145/3391274,
title = {SBD '20: Proceedings of The International Workshop on Semantic Big Data},
year = {2020},
isbn = {9781450379748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goal of this workshop is to bring together academic researchers and industry practitioners to address the challenges and report and exchange the research findings in Semantic Big Data, including new approaches, techniques and applications, make substantial theoretical and empirical contributions to, and significantly advance the state of the art of Semantic Big Data.},
location = {Portland, Oregon}
}

@inproceedings{10.1145/2524224.2524227,
author = {Theera-Ampornpunt, Nawanol and Bagchi, Saurabh and Joshi, Kaustubh R. and Panta, Rajesh K.},
title = {Using Big Data for More Dependability: A Cellular Network Tale},
year = {2013},
isbn = {9781450324571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2524224.2524227},
doi = {10.1145/2524224.2524227},
abstract = {There are many large infrastructures that instrument everything from network performance metrics to user activities. However, the collected data are generally used for long-term planning instead of improving reliability and user experience in real time. In this paper, we present our vision of how such collections of data can be used in real time to enhance the dependability of cellular network services. We first discuss mitigation mechanisms that can be used to improve reliability, but incur a high cost which prohibit them to be used except in certain conditions. We present two case studies where analyses of real cellular network traffic data show that we can identify these conditions.},
booktitle = {Proceedings of the 9th Workshop on Hot Topics in Dependable Systems},
articleno = {2},
numpages = {5},
keywords = {big data, cellular, data mining, wireless},
location = {Farmington, Pennsylvania},
series = {HotDep '13}
}

@inproceedings{10.1145/3056662.3056667,
author = {Shi, Yingjie and Wang, Lei and Du, Fang},
title = {Performance and Energy Efficiency of Big Data Systems: Characterization, Implication and Improvement},
year = {2017},
isbn = {9781450348577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3056662.3056667},
doi = {10.1145/3056662.3056667},
abstract = {Large volume of data is produced by various applications in the world, processing such scale of data has great challenges in not only performance but also energy efficiency. Researchers propose various techniques to either improve the performance or the energy efficiency. The techniques of these two trends, however, are significantly different. When both performance and energy efficiency are concerned in the big data systems, how to get balance has become an issuing and challenging problem for data center administrators and hardware designers. In this paper, we conduct comprehensive evaluations on two representative platforms with different types of processors. We quantify the performance and energy efficiency, relating the evaluation results to micro-architectural activities and application characteristics. Two interesting findings are made from our evaluations: (1) the performance and energy efficiency are not only determined by the hardware technology, but also associated with the application characteristics; (2) there is no ever-victorious microprocessor in terms of both performance and energy efficiency in all the big data workloads. Based on the findings and quantified evaluation results, we provide great guidance and implications for both data center administrators and big data system designers, and we argue that a hybrid-core is an efficient way to improve the energy efficiency of big data systems with minimum performance degradation.},
booktitle = {Proceedings of the 6th International Conference on Software and Computer Applications},
pages = {55–61},
numpages = {7},
keywords = {hybrid core, big data systems, performance, energy efficiency},
location = {Bangkok, Thailand},
series = {ICSCA '17}
}

@inproceedings{10.5555/2735522.2735575,
author = {Khalifa, Shadi and Martin, Patrick},
title = {Smart Big Data Analytics as a Service Framework: A Proposal},
year = {2014},
publisher = {IBM Corp.},
address = {USA},
abstract = {We propose the Smart Big Data Analytics as a Service framework. A framework to empower in-house business users with intelligent assistance throughout the analytics process. It provides distributed in-memory data processing and an easy-to-learn-and-use analytics query language for data exploration, preprocessing and analytical workflow orchestration. The framework is designed as a service to take advantage of the Cloud's features.},
booktitle = {Proceedings of 24th Annual International Conference on Computer Science and Software Engineering},
pages = {327–330},
numpages = {4},
location = {Markham, Ontario, Canada},
series = {CASCON '14}
}

@inproceedings{10.1145/3314074.3314097,
author = {Kachaoui, Jabrane and Belangour, Abdessamad},
title = {Challenges and Benefits of Deploying Big Data Storage Solution},
year = {2019},
isbn = {9781450361293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314074.3314097},
doi = {10.1145/3314074.3314097},
abstract = {Since data is at the heart of information systems, new technologies and approaches dealing with storing, processing and analyzing data have proliferated. Data Warehouses are among the most known approaches that tackle data storing and processing. However, they reached their limits in dealing with large quantities of data as those of Big Data. Consequently, a new concept which is an evolution of Data Warehouse known as "Data Lake" is emerging. This paper presents a detailed analysis that compares Data Lake and Data Warehouse key concepts. It sheds lights on the aspects and characteristics for the sake of revealing similarities and differences. It also emphasizes the complementary of the two technologies by showing the most appropriate use case of each of them.},
booktitle = {Proceedings of the New Challenges in Data Sciences: Acts of the Second Conference of the Moroccan Classification Society},
articleno = {22},
numpages = {5},
keywords = {Data Warehouse, Data Lake, NoSQL, Ad Hoc, Data Mart, Distributed databases, Repository, Big Data, Hadoop},
location = {Kenitra, Morocco},
series = {SMC '19}
}

@inproceedings{10.1145/3151759.3151780,
author = {Hirchoua, Badr and Ouhbi, Brahim and Frikh, Bouchra},
title = {A New Knowledge Capitalization Framework in Big Data Context},
year = {2017},
isbn = {9781450352994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3151759.3151780},
doi = {10.1145/3151759.3151780},
abstract = {In many companies data is used as a source for creating knowledge in their sphere of business. Therefore this paper presents a knowledge capitalization framework in big data context. Based on the technology derived from distributed systems, our research concerns the design and development of a knowledge engineering framework in big data context. It can be integrated in any knowledge management system. The proposed framework is based on four layers: we start by extracting hidden topics, using the LDA approach in batch processing to handle the complexity of multi knowledge domains and to keep the semantic relations between knowledge entities. Then we use clustering mechanisms to pick the best combination between topics from different sources. As a result, we get, in every distributed site, the related topics (knowledge), in order to facilitate the research and access, to get the useful knowledge in real time processing.},
booktitle = {Proceedings of the 19th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {40–48},
numpages = {9},
keywords = {topic modeling, big data computing, knowledge capitalization, data intelligence, machine learning},
location = {Salzburg, Austria},
series = {iiWAS '17}
}

@inproceedings{10.1145/2588555.2618215,
author = {\"{O}zcan, Fatma and Tatbul, Nesime and Abadi, Daniel J. and Kornacker, Marcel and Mohan, C. and Ramasamy, Karthik and Wiener, Janet},
title = {Are We Experiencing a Big Data Bubble?},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2618215},
doi = {10.1145/2588555.2618215},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {1407–1408},
numpages = {2},
keywords = {NewSQL, SQL-on-HADOOP, NoSQL},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/3090354.3090363,
author = {Sid, Karima and Batouche, Mohamed Chawki},
title = {Big Data Analytics Techniques in Virtual Screening for Drug Discovery},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090363},
doi = {10.1145/3090354.3090363},
abstract = {Virtual screening (VS) is a computational method used in the drug discovery process by searching large libraries of small molecules to identify that represent leads for certain target. According to the use of information about the ligand, the target or both, virtual screening techniques are classified into ligand-based and structure-based methods. These methods can be combined to build a hierarchical schema in order to benefit the advantages of each one. With the rapid development of High-Throughput Technologies in structural biology, that allows producing massive libraries of small molecules include tens of millions of molecules, led to define VS as Big Data analytics problem. MapReduce is a parallel programming model produced by Google, designed for Large Scale Data processing. Apache Hadoop is the most widely used open source MapReduce implementation. It was for many years, the leading Big Data framework. Recently, with the emergence of Apache Spark as a Big Data processing framework, it has become the most popular, due to their improvement of some deficiencies known with Hadoop's MapReduce such as, speed, pipelining, and iterative jobs. In this paper, we review the Molecular Docking (MD) workflow. Next, we analyze the two most applied Big Data analytic tools in VS field which are Hadoop's MapReduce and Spark. We identify some known shortcomings that make Hadoop's MapReduce not suitable for MD issue, and point out the need of a novel MD workflow in Spark.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {9},
numpages = {7},
keywords = {Virtual Screening, Drug Discovery, Big Data, MapReduce, Hadoop, Hierarchical Virtual Screening, Spark, Molecular Docking},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1109/CCGRID.2017.55,
author = {Ca\'{\i}no-Lores, Silvina and Isaila, Florin and Carretero, Jes\'{u}s},
title = {Data-Aware Support for Hybrid HPC and Big Data Applications},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.55},
doi = {10.1109/CCGRID.2017.55},
abstract = {Nowadays there is a raising interest in bridging the gap between Big Data application models and data-intensive HPC. This work explores the effects that Big Data-inspired paradigms could have in current scientific applications through the evaluation of a real-world application from the hydrology domain. This evaluation led to experience that portrayed the key aspects of the HPC and Big Data paradigms that made them successful in their respective worlds. With this information, we established a research roadmap to build a platform suitable for HPC hybrid applications, with a focus on efficient data management and fault-tolerance.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {719–722},
numpages = {4},
keywords = {high-performance computing, data management, big data, scientific computing, data-intensive computing},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@article{10.14778/2752939.2752945,
author = {Loghin, Dumitrel and Tudor, Bogdan Marius and Zhang, Hao and Ooi, Beng Chin and Teo, Yong Meng},
title = {A Performance Study of Big Data on Small Nodes},
year = {2015},
issue_date = {February 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/2752939.2752945},
doi = {10.14778/2752939.2752945},
abstract = {The continuous increase in volume, variety and velocity of Big Data exposes datacenter resource scaling to an energy utilization problem. Traditionally, datacenters employ x86-64 (big) server nodes with power usage of tens to hundreds of Watts. But lately, low-power (small) systems originally developed for mobile devices have seen significant improvements in performance. These improvements could lead to the adoption of such small systems in servers, as announced by major industry players. In this context, we systematically conduct a performance study of Big Data execution on small nodes in comparison with traditional big nodes, and present insights that would be useful for future development. We run Hadoop MapReduce, MySQL and in-memory Shark workloads on clusters of ARM big. LITTLE boards and Intel Xeon server systems. We evaluate execution time, energy usage and total cost of running the workloads on self-hosted ARM and Xeon nodes. Our study shows that there is no one size fits all rule for judging the efficiency of executing Big Data workloads on small and big nodes. But small memory size, low memory and I/O bandwidths, and software immaturity concur in canceling the lower-power advantage of ARM servers. We show that I/O-intensive MapReduce workloads are more energy-efficient to run on Xeon nodes. In contrast, database query processing is always more energy-efficient on ARM servers, at the cost of slightly lower throughput. With minor software modifications, CPU-intensive MapReduce workloads are almost four times cheaper to execute on ARM servers.},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {762–773},
numpages = {12}
}

@article{10.1145/2771299,
author = {Goth, Gregory},
title = {Bringing Big Data to the Big Tent},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2771299},
doi = {10.1145/2771299},
abstract = {Open source tools assist data science.},
journal = {Commun. ACM},
month = {jun},
pages = {17–19},
numpages = {3}
}

@inproceedings{10.1145/3380688.3380705,
author = {Ngo, Vuong M. and Kechadi, M-Tahar},
title = {Crop Knowledge Discovery Based on Agricultural Big Data Integration},
year = {2020},
isbn = {9781450376310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380688.3380705},
doi = {10.1145/3380688.3380705},
abstract = {Nowadays, the agricultural data can be generated through various sources, such as: Internet of Thing (IoT), sensors, satellites, weather stations, robots, farm equipment, agricultural laboratories, farmers, government agencies and agribusinesses. The analysis of this big data enables farmers, companies and agronomists to extract high business and scientific knowledge, improving their operational processes and product quality. However, before analysing this data, different data sources need to be normalised, homogenised and integrated into a unified data representation. In this paper, we propose an agricultural data integration method using a constellation schema which is designed to be flexible enough to incorporate other datasets and big data models. We also apply some methods to extract knowledge with the view to improve crop yield; these include finding suitable quantities of soil properties, herbicides and insecticides for both increasing crop yield and protecting the environment.},
booktitle = {Proceedings of the 4th International Conference on Machine Learning and Soft Computing},
pages = {46–50},
numpages = {5},
keywords = {crop yield, herbicides, Decision support, insecticides, soil properties},
location = {Haiphong City, Viet Nam},
series = {ICMLSC 2020}
}

@inproceedings{10.1145/2460625.2460642,
author = {Shaer, Orit and Mazalek, Ali and Ullmer, Brygg and Konkel, Miriam},
title = {From Big Data to Insights: Opportunities and Challenges for TEI in Genomics},
year = {2013},
isbn = {9781450318983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460625.2460642},
doi = {10.1145/2460625.2460642},
abstract = {The combination of advanced genomic technologies and computational tools enables researchers to conduct large-scale experiments that answer biological questions in unprecedented ways. However, interaction tools in this area currently remain immature. We propose that tangible, embedded, and embodied interaction (TEI) offers unique opportunities for enhancing discovery and learning in genomics. Also, designing for problems in genomics can help move forward the theory and practice of TEI. We present challenges and key questions for TEI research in genomics, lessons learned from three case studies, and potential areas of focus for TEI research and design.},
booktitle = {Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction},
pages = {109–116},
numpages = {8},
keywords = {interactive tabletops, computational genomics, tangible interaction, biology, big data, scientific discovery, learning},
location = {Barcelona, Spain},
series = {TEI '13}
}

@inproceedings{10.1145/1077501.1077503,
author = {Garcia-Molina, Hector},
title = {Handling Data Quality in Entity Resolution},
year = {2005},
isbn = {1595931600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1077501.1077503},
doi = {10.1145/1077501.1077503},
abstract = {Entity resolution (ER) is a problem that arises in many information integration scenarios: We have two or more sources containing records on the same set of real-world entities (e.g., customers).However, there are no unique identifiers that tell us what records from one source correspond to those in the other sources.Furthermore, the records representing the same entity may have differing information, e.g., one record may have the address misspelled, another record may be missing some fields.An ER algorithm attempts to identify the matching records from multiple sources (i.e., those corresponding to the same real-world entity), and merges the matching records as best it can.In many ER applications the input data has data quality or uncertainty values associated with it. Furthermore, the ER process itself introduces additional uncertainties, e.g., we may only be 90% confident that two given records actually correspond to the same real-world entity.In this talk Hector Garcia-Molina will discuss the challenges in representing quality/uncertainty/confidences in a way that is useful for the ER process.He will also present some preliminary ideas on how to perform ER with uncertain data. (This work is joint with Omar Benjelloun, David Menestrina, Qi Su, and Jennifer Widom).},
booktitle = {Proceedings of the 2nd International Workshop on Information Quality in Information Systems},
pages = {1},
numpages = {1},
location = {Baltimore, Maryland},
series = {IQIS '05}
}

@inproceedings{10.1145/3010089.3016031,
author = {Hamdan, Hani},
title = {A Mixture Model Approach to Big Data Clustering and Classification},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3016031},
doi = {10.1145/3010089.3016031},
abstract = {In this paper, the importance and advantages of binning data, for big data clustering and classification, are shown. Then, the fundamental and basic concepts of mixture models estimation from binned data are presented. A special attention is paid to the binned-EM algorithm, and its application to data clustering and classification. A feedback on the implementation and use of this algorithm is provided. The binned-EM algorithm is summarized so that it is easy to program. In order to show the usefulness and the good performances of the presented approach to big data clustering, an example of application to image segmentation is illustrated.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {63},
numpages = {6},
keywords = {Big data, fuzzy clustering, classification, binned data, mixture model, clustering, binned-EM algo-rithm},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/2837060.2837105,
author = {Kim, Kyeongsik and Lim, Byung-Muk and Kim, Ji-Dae and Chi, Su-Young and Cho, Wan-Sup and Yoo, Kwan-Hee},
title = {Schemes for Modeling Flexible Manufacturing Processes in Big Data Environment},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837105},
doi = {10.1145/2837060.2837105},
abstract = {This paper designs a flexible method for modeling manufacturing processes in big data environment. The purpose of the study is to design a modeling tool broadly used in various manufacture industry areas. To support the functionalities, we provide types of the process symbols, which can be used to facilitate convenient design for process modeling, and web-based visualization which will lead the light-weight modeling tool. This flexible modeling method is expected to be broadly used for modeling process in wider manufacture area in big data environment.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {242–245},
numpages = {4},
keywords = {Manufacturing Process, Web-based Design, Big Data},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/2539150.2539224,
author = {Girtelschmid, Sylva and Steinbauer, Matthias and Kumar, Vikash and Fensel, Anna and Kotsis, Gabriele},
title = {Big Data in Large Scale Intelligent Smart City Installations},
year = {2013},
isbn = {9781450321136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2539150.2539224},
doi = {10.1145/2539150.2539224},
abstract = {This paper highlights how the domain of Smart Cities is often modeled by ontologies to create applications and services that are highly flexible, (re)configurable, and inter-operable. However, ontology repositories and their accompanying reasoning and rule languages face the disadvantage of bad runtime behavior, especially if the models grow large in size. We propose an architecture that uses tools and methods from the domain of Big Data processing in conjunction with an ontology repository and a rule engine to overcome potential performance bottlenecks that will occur in this scenario.},
booktitle = {Proceedings of International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {428–432},
numpages = {5},
keywords = {Energy Efficiency, Real-time Streaming, Ontology, Semantic Modeling, Smart City, Big Data},
location = {Vienna, Austria},
series = {IIWAS '13}
}

@inproceedings{10.1145/3264560.3264572,
author = {Fathi, F. and Abghour, N. and Ouzzif, M.},
title = {From Big Data to Better Behavior in Self-Driving Cars},
year = {2018},
isbn = {9781450364744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264560.3264572},
doi = {10.1145/3264560.3264572},
abstract = {Diversity and heterogeneity are one of the key aspects of big data. Including both structured and also unstructured data. Understanding and making sense of the huge amount of data requires better approaches for deduction and novel learning systems to address the different difficulties in many domains of application, especially in transportation. Autonomous cars are the future of transportation, in this paper, we will discuss the best combination with big data; text analytics, image processing and big data sensors, from unstructured data and then we will showcase how this fusion of data gathered by different sources can improve reliability and efficiency in self-driving and, may lead us to rethink a new theories and models altogether, and finally develop a behavior vehicle, Capable, like a human, to develop a better understanding from perception and intuition.},
booktitle = {Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing},
pages = {42–46},
numpages = {5},
keywords = {NLP, Big data, CNN, self-driving, Sensor fusion},
location = {Barcelona, Spain},
series = {ICCBDC'18}
}

@inproceedings{10.5555/1274453.1274459,
author = {Gomes, Pedro and Farinha, Jos\'{e} and Trigueiros, Maria Jos\'{e}},
title = {A Data Quality Metamodel Extension to CWM},
year = {2007},
isbn = {192068285X},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {The importance of metadata has been broadly referred in the last years, mainly in the field of data warehousing and decision support systems. Contemporarily, in the adjacent field of data quality, several approaches and tools have been set out for the purpose of data profiling and cleaning. However, little effort has been made in order to formally specify metrics and techniques for data quality in a structured way. As a matter of fact, little relevance has been assigned to metadata regarding data quality and data cleaning issues. This paper aims at filling this gap, proposing a conceptual metamodel for data quality and cleaning, both applicable to operational and data warehousing contexts. The presented metadata model is integrated with OMG's CWM, offering a possible extension of this standard toward data quality.},
booktitle = {Proceedings of the Fourth Asia-Pacific Conference on Comceptual Modelling - Volume 67},
pages = {17–26},
numpages = {10},
keywords = {data cleaning, data warehouses, CWM, data quality, metamodel, metadata, standards},
location = {Ballarat, Australia},
series = {APCCM '07}
}

@inproceedings{10.1145/2837060.2837112,
author = {Lee, Jae-Won and Jeong, Ji-Seong and Kim, Mihye and Yoo, Kwan-Hee},
title = {Safe-Return-Home Service Based on Big Data Analytics},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837112},
doi = {10.1145/2837060.2837112},
abstract = {In modern society, various forms of crime are constantly occurring. Accordingly, several safe-return systems for the socially vulnerable are being developed. However, those systems are mainly focused on responding to dangerous situations that have already occurred, and they do not predict the possibility of crime reflected by information about the user's surroundings in real time. This paper proposes a new safe-return-home service that allows users to be notified of, and therefore handle, possibly dangerous situations surrounding them in real time. This is accomplished by collecting and analyzing various types of big data about the user's surroundings in real time. Collected and analyzed data include the locations of users, the locations of CCTV (Closed-Circuit Television) cameras, crime/disaster/accident-related real-time news data, the locations of shelters, real-time CCTV video data, and social network service data. Through the analysis of these data, the prediction of potential surrounding dangers is visualized on user devices, and ideas for counteracting those dangers is suggested to users in real time.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {270–271},
numpages = {2},
keywords = {Safe-return-home Service, Big data analytics, Safe-return Service},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@article{10.14778/2824032.2824141,
author = {Walter, Todd},
title = {Big Plateaus of Big Data on the Big Island},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824141},
doi = {10.14778/2824032.2824141},
abstract = {In ancient texts, 40 was a magic number. It meant a "lot" or "a long time." 40 years represented the time it took for a new generation to arise. A look back at 40 years of VLDB suggests that this applies to database researchers as well -- the young researchers of the early VLDBs are now the old folks of the database world, and a new generation is creating afresh. Over this period many plateaus of "Big Data" have challenged the database community and been conquered. But there is still no free lunch -- database research is really the science of trade-offs, many of which are no different today than 40 years ago. And of course the evolution of hardware technology continues to swing the trade-off pendulum while enabling new plateaus to be reached. Todd will take a look back at customer big data plateaus of the past. He will look at where we are today, then use his crystal ball and the lessons of the past to extrapolate the next several plateaus -- how they will be the same and how will they be different. Along the way we will have a little fun with some VLDB and Teradata history.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2057},
numpages = {1}
}

@article{10.1145/2744700.2744703,
author = {Ali, Reem Y. and Gunturi, Venkata M. V. and Shekhar, Shashi},
title = {Spatial Big Data for Eco-Routing Services: Computational Challenges and Accomplishments},
year = {2015},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/2744700.2744703},
doi = {10.1145/2744700.2744703},
abstract = {The size, variety, and update rate of spatial datasets are increasingly exceeding the capacity of commonly used spatial computing technologies to learn, manage, and process the data with reasonable effort. We refer to these datasets as Spatial Big Data (SBD). Examples of emerging SBD datasets include temporally detailed (TD) roadmaps that provide speeds every minute for every road-segment, GPS track data from cell-phones, and engine measurements of fuel consumption, greenhouse gas (GHG) emissions, etc. Harnessing SBD has a transformative potential. For example, a 2011 McKinsey Global Institute report estimates savings of "about $600 billion annually by 2020" in terms of fuel and time saved by helping vehicles avoid congestion and reduce idling at red lights or left turns. In this paper, we discuss the challenges posed by SBD for a next generation of routing services and we present our work towards addressing these challenges.},
journal = {SIGSPATIAL Special},
month = {mar},
pages = {19–25},
numpages = {7}
}

@inproceedings{10.1145/3193063.3193069,
author = {Cheng, Susu and Zhao, Haijun},
title = {An Overview of Techniques for Confirming Big Data Property Rights},
year = {2018},
isbn = {9781450363785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193063.3193069},
doi = {10.1145/3193063.3193069},
abstract = {The major premise of big data circulation is to identify the ownership of data resource. This paper summed some feasible techniques and methods for confirming big data property which are data citation technology, data provenance technology, data reversible hiding technology, computer forensic technology and block chain technology. The ownership of information property which from different sizes, different formats and different storage condition on distributed heterogeneous platforms can be confirmed by comprehensive application of these techniques and methods based on the coupling interface between them in the practice of big data.},
booktitle = {Proceedings of the 2018 International Conference on Intelligent Information Technology},
pages = {59–64},
numpages = {6},
keywords = {Information property index, Method for confirming information property rights, Big Data, Confirmation of Information Property},
location = {Ha Noi, Viet Nam},
series = {ICIIT 2018}
}

