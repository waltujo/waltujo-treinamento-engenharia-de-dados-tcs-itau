@inproceedings{10.1145/3260383,
author = {Kumar, Deepak},
title = {Session Details: Paper Session: Big Data},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3260383},
doi = {10.1145/3260383},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/3006299.3006306,
author = {Salloum, Salman and Huang, Joshua Zhexue and He, Yulin},
title = {Empirical Analysis of Asymptotic Ensemble Learning for Big Data},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006306},
doi = {10.1145/3006299.3006306},
abstract = {In many application areas, data that is being generated and processed goes beyond the petabyte scale. Analyzing such an increasing massive volume of data faces computational, as well as, statistical challenges. In order to solve these challenges, distributed and parallel processing frameworks have been used for implementing scalable data analysis algorithms. Nevertheless, processing the whole big data set at one time may exceed the available computing resources and the time requirements for some applications. Thus, approximate approaches can be used to achieve asymptotic analysis results, especially when data analysis algorithms are amenable to an approximate result rather than an exact one. However, most approximation approaches require taking a random sample of the data which is a nontrivial task when working with big data sets. In this paper, we employ ensemble learning as an approach for asymptotic analysis using randomly selected subsets (i.e. data blocks) of a big data set. We propose an asymptotic ensemble learning framework which depends on block-based sampling rather than record-based sampling. In order to demonstrate the feasibility and performance of this framework, we present an empirical analysis on real data sets. In addition to the scalability advantage, the experimental results show that several blocks of a data set are enough to get approximately the same results as those from using the whole data set.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {8–17},
numpages = {10},
keywords = {distributed and parallel processing, randomness, big data, ensemble learning, asymptotic analysis},
location = {Shanghai, China},
series = {BDCAT '16}
}

@article{10.1109/TCBB.2015.2454551,
author = {Janga, Sarath Chandra and Zhu, Dongxiao and Chen, Jake Y. and Zaki, Mohammed J.},
title = {Knowledge Discovery Using Big Data in Biomedical Systems},
year = {2015},
issue_date = {July/August 2015},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {12},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2454551},
doi = {10.1109/TCBB.2015.2454551},
abstract = {The 13th International Workshop on Data Mining in Bioinformatics (BIOKDD'14) was organized in conjunction with the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining on August 24, 2014 in New York, USA. It brought together international researchers in the interacting disciplines of data mining, systems biology, and bioinformatics at the Bloomberg Headquarters venue. The goal of this workshop is to encourage Knowledge Discovery and Data mining (KDD) researchers to take on the numerous challenges that Bioinformatics offers. This year, the workshop featured the theme of "Knowledge discovery using big data in biological/biomedical systems".},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {726–728},
numpages = {3}
}

@inproceedings{10.1145/3105971.3105979,
author = {Keck, Mandy and Kammer, Dietrich and Gr\"{u}nder, Thomas and Thom, Thomas and Kleinsteuber, Martin and Maasch, Alexander and Groh, Rainer},
title = {Towards Glyph-Based Visualizations for Big Data Clustering},
year = {2017},
isbn = {9781450352925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105971.3105979},
doi = {10.1145/3105971.3105979},
abstract = {Data Analysts have to deal with an ever-growing amount of data resources. One way to make sense of this data is to extract features and use clustering algorithms to group items according to a similarity measure. Algorithm developers are challenged when evaluating the performance of the algorithm since it is hard to identify features that influence the clustering. Moreover, many algorithms can be trained using a semi-supervised approach, where human users provide ground truth samples by manually grouping single items. Hence, visualization techniques are needed that help data analysts achieve their goal in evaluating Big data clustering algorithms. In this context, Multidimensional Scaling (MDS) has become a prominent visualization tool. In this paper, we propose a combination with glyphs that can provide a detailed view of specific features involved in MDS. In consequence, human users can understand, adjust, and ultimately improve clustering algorithms. We present a thorough glyph design, which is founded in a comprehensive survey of related work and report the results of a controlled experiments, where participants solved data analysis tasks with both glyphs and a traditional textual display of data values.},
booktitle = {Proceedings of the 10th International Symposium on Visual Information Communication and Interaction},
pages = {129–136},
numpages = {8},
keywords = {big data, multidimensional scaling, visual cluster analysis, Glyph-based visualization techniques},
location = {Bangkok, Thailand},
series = {VINCI '17}
}

@inproceedings{10.1145/3022227.3022232,
author = {Saad, Amna and Amran, Ahmad Roshidi and Phillips, Iain William and Salagean, Ana M.},
title = {Big Data Analysis on Secure VoIP Services},
year = {2017},
isbn = {9781450348881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022227.3022232},
doi = {10.1145/3022227.3022232},
abstract = {VoIP users increase each day. However, the documentation on the behavior of VoIP applications is still lacking. The needs to understand and generalize the behavior of applications like Skype, GoogleTalk and SIP based applications grow each day. There are many factors that influenced the performance of a VoIP application such as bandwidth, packet loss rate, delay, jitter, codec type and CPU power of the end devices. The user experience of the service is important since, VoIP is a real time application running over the best effort internet. Since VoIP data co-exist with other data on the internet, extracting, transforming, loading and analyzing the selected VoIP application is a challenge. We design an instrument to do data collections, data massaging, data analysis and data interpretation of large amounts of network packet. The result shows that GoogleTalk, Skype and Express Talk are more sensitive to the impairments due to packet loss rate and jitter rather than to the impairment due to delay. Bandwidth and other resources like a de-jitter buffer and a gateway's CPU and memory are important in order to produce a good quality VoIP service. The lack of these resources would result in several packets lost before they reach a destination or the packets arrive too late to join the other packets in the de-jitter buffer at the destination gateway. The gateway would drop these packets if the de-jitter buffer is full or not enough memory or CPU powers to process the packets. A gateway closer to the receiver end decapsulates IPSec or TLS packets. The gateway also decodes the voice packets before the packets entering the receiving machine. High throughputs do not imply high Perceptual Evaluation of Speech Quality for Wideband (PESQ-WB) scores. The throughput size is determined by the codec type and the security features that are implemented on the infrastructure.},
booktitle = {Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication},
articleno = {5},
numpages = {8},
keywords = {resources, PESQ, CVSS, impairments, VoIP protocol, big data},
location = {Beppu, Japan},
series = {IMCOM '17}
}

@inproceedings{10.1145/3416921.3416943,
author = {Cuzzocrea, Alfredo},
title = {Uncertainty and Imprecision in Big Data Management: Models, Issues, Paradigms, and Future Research Directions},
year = {2020},
isbn = {9781450375382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416921.3416943},
doi = {10.1145/3416921.3416943},
abstract = {This paper provides an overview of state-of-the-art proposals and forefront research directions in the context of uncertainty and imprecision in big data management, an emerging topic in the actual research community.},
booktitle = {Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing},
pages = {6–9},
numpages = {4},
keywords = {Uncertainty in big data management, Imprecision in big data management, Big data management, Big data analytics},
location = {Virtual, United Kingdom},
series = {ICCBDC '20}
}

@inproceedings{10.1145/3372454.3372469,
author = {Garate-Escamilla, Anna Karen and El Hassani, Amir Hajjam and Andres, Emmanuel},
title = {Big Data Scalability Based on Spark Machine Learning Libraries},
year = {2020},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372469},
doi = {10.1145/3372454.3372469},
abstract = {The paper introduces the challenge of scalability in machine learning algorithms suitable for massive datasets. Today, big data has relevant applications in the industry due to improvements in the system performance and by turning information into knowledge. Big data challenges include the lack of strategies to process computational cost and the large amount of data when computing machine learning predictions. To overcome these scalability issues, it is convenient to work with distributed and parallelized architecture across multiple nodes. The approach is based on Apache Spark, an in-memory distributed application that offers extensive machine learning libraries. The main contribution of the study is to measure the scalability by calculating the execution time that a classifier achieves with larger workloads. We validate our classifier models with experiments on logistic regression and random forest by studying their adaptability to the Apache Spark framework. The present work expects to combine the areas of big data and machine learning on scalability, and the use of optimization methods, cache and persist. In addition, a comparison between the classifiers is provided. The evaluation experiments show that logistic regression performed the shortest execution time and best scalability.},
booktitle = {Proceedings of the 2019 3rd International Conference on Big Data Research},
pages = {166–171},
numpages = {6},
keywords = {Distributed Systems, Machine Learning, Apache Spark, Execution time prediction, scalability},
location = {Cergy-Pontoise, France},
series = {ICBDR 2019}
}

@inproceedings{10.1145/3175684.3175730,
author = {Fong, Simon},
title = {Big Data Mining Algorithms for Fog Computing},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175730},
doi = {10.1145/3175684.3175730},
abstract = {Fog computing is a contemporary distributed computing concept extending from Cloud computing, which pushes the data analytics to the edge of a sensor network as far as possible. It helps avoid performance bottleneck and data analytics latency at the central server of a Cloud. However, when Fog computing is deployed, the edge nodes are responsible in data analysis including learning and recognizing patterns from the incoming data streams. Hence it is crucial to find appropriate data mining algorithm(s) which is lightweight in operation and accurate in predictive performance. In this paper, the suitability of data mining and data stream mining algorithms are investigated in Fog computing environment. Specifically, non-black-box machine learning models such as decision trees are looked into, with a quick pre-processing function implemented by correlation-based feature selection algorithm coupled with traditional search methods and particle swarm optimization search method. The simulation is based on an IoT environment where emergency services are to be supported. The results of this paper sheds light into what/which algorithms should be designed and chosen for delivering edge intelligence under Fog computing environment.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {57–61},
numpages = {5},
keywords = {Computer network security, security threats, SME},
location = {London, United Kingdom},
series = {BDIOT2017}
}

@inproceedings{10.1145/2594538.2594551,
author = {Fan, Wenfei and Geerts, Floris and Libkin, Leonid},
title = {On Scale Independence for Querying Big Data},
year = {2014},
isbn = {9781450323758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594538.2594551},
doi = {10.1145/2594538.2594551},
abstract = {To make query answering feasible in big datasets, practitioners have been looking into the notion of scale independence of queries. Intuitively, such queries require only a relatively small subset of the data, whose size is determined by the query and access methods rather than the size of the dataset itself. This paper aims to formalize this notion and study its properties. We start by defining what it means to be scale-independent, and provide matching upper and lower bounds for checking scale independence, for queries in various languages, and for combined and data complexity. Since the complexity turns out to be rather high, and since scale-independent queries cannot be captured syntactically, we develop sufficient conditions for scale independence. We formulate them based on access schemas, which combine indexing and constraints together with bounds on the sizes of retrieved data sets. We then study two variations of scale-independent query answering, inspired by existing practical systems. One concerns incremental query answering: we check when query answers can be maintained in response to updates scale-independently. The other explores scale-independent query rewriting using views.},
booktitle = {Proceedings of the 33rd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
pages = {51–62},
numpages = {12},
keywords = {query answering, big data, scale independence},
location = {Snowbird, Utah, USA},
series = {PODS '14}
}

@inproceedings{10.1145/3027385.3029445,
author = {Oi, Misato and Yamada, Masanori and Okubo, Fumiya and Shimada, Atsushi and Ogata, Hiroaki},
title = {Reproducibility of Findings from Educational Big Data: A Preliminary Study},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029445},
doi = {10.1145/3027385.3029445},
abstract = {In this paper, we examined whether previous findings on educational big data consisting of e-book logs from a given academic course can be reproduced with different data from other academic courses. The previous findings showed that (1) students who attained consistently good achievement more frequently browsed different e-books and their pages than low achievers and that (2) this difference was found only for logs of preparation for course sessions (preview), not for reviewing material (review). Preliminarily, we analyzed e-book logs from four courses. The results were reproduced in only one course and only partially, that is, (1) high achievers more frequently changed e-books than low achievers (2) for preview. This finding suggests that to allow effective usage of learning and teaching analyses, we need to carefully construct an educational environment to ensure reproducibility.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {536–537},
numpages = {2},
keywords = {e-book, educational big data, reproducibility},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3274005.3274021,
author = {Fotache, Marin and Greavu-\c{S}erban, Valeric\u{a} and Hrubaru, Ionu\c{t} and Tic\u{a}, Alexandru},
title = {Big Data Technologies on Commodity Workstations: A Basic Setup for Apache Impala},
year = {2018},
isbn = {9781450364256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274005.3274021},
doi = {10.1145/3274005.3274021},
abstract = {Big Data technologies brought the idea of parallel processing on cheaper commodity servers. When dealing with huge amount of data, instead of migrating to more performant and costly hardware platforms, or buying resources in cloud, it is more affordable to add a number of cheaper servers as nodes for data processing and/or storage. NoSQL data stores, Hadoop ecosystems, NewSQL platforms have proved viable for Big Data storage and processing. In this paper we were concerned with setting up a platform for big data processing using commodity workstations. Many small and medium sized companies have limited resources and their workstations remain unused for more than 12 hours a day. Here Beowulf Cluster Computing could prove useful. Apache Impala was installed as part of a Hadoop distribution on a 9-node cluster. Three TPC-H database schema were loaded for the scale factors of 1, 2 and 10GB. A series of 100 SQL queries were randomly generated and executed for each scale factor. Results were collected and analyzed for determining if the cluster can provide a decent level of data processing performance.},
booktitle = {Proceedings of the 19th International Conference on Computer Systems and Technologies},
pages = {110–115},
numpages = {6},
keywords = {Distributed computing, Beowulf clustering, Impala, Hadoop, Query performance},
location = {Ruse, Bulgaria},
series = {CompSysTech'18}
}

@inproceedings{10.5555/3291291.3291361,
author = {Arruda, Darlan and Madhavji, Nazim H. and Taylor, Colin},
title = {CASCON Workshop on Developing Big Data Applications and Services},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Research from Gartner (2015) indicates that, in 2017, 60% of Big Data projects failed or did not provide the expected benefits [1]. However, in November 2017, Nick Heudecker, a Gartner analyst, posted in his twitter account that they were too conservative. The Big Data project failure rate is now close to 85%. The reasons are not only related to technology itself [2]. It is a mix of environmental, technological and managerial problems. Some of the reasons for Big Data projects failure are: At the project level [3], [4]: missing link to business objectives, lacking big data skills, relying too much on the data, failing to convince executives, and poor planning; At the technical level [5]: Rapid technology changes, difficulty in selecting Big Data technologies to address the systems and project requirements, complex integration between new and old systems, computation of intensive analytics, and the necessity of high scalability, availability and reliability, to name a few. Further, a previous study [6] has shown that there is approximately a 80:20 split in the industry focus in favor of algorithms for analytics and infrastructure, thereby shortchanging the aspects of creating and evolving applications and services concerned with Big Data.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {407–409},
numpages = {3},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/2783258.2783372,
author = {Bifet, Albert and de Francisci Morales, Gianmarco and Read, Jesse and Holmes, Geoff and Pfahringer, Bernhard},
title = {Efficient Online Evaluation of Big Data Stream Classifiers},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783372},
doi = {10.1145/2783258.2783372},
abstract = {The evaluation of classifiers in data streams is fundamental so that poorly-performing models can be identified, and either improved or replaced by better-performing models. This is an increasingly relevant and important task as stream data is generated from more sources, in real-time, in large quantities, and is now considered the largest source of big data. Both researchers and practitioners need to be able to effectively evaluate the performance of the methods they employ. However, there are major challenges for evaluation in a stream. Instances arriving in a data stream are usually time-dependent, and the underlying concept that they represent may evolve over time. Furthermore, the massive quantity of data also tends to exacerbate issues such as class imbalance. Current frameworks for evaluating streaming and online algorithms are able to give predictions in real-time, but as they use a prequential setting, they build only one model, and are thus not able to compute the statistical significance of results in real-time. In this paper we propose a new evaluation methodology for big data streams. This methodology addresses unbalanced data streams, data where change occurs on different time scales, and the question of how to split the data between training and testing, over multiple models.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {59–68},
numpages = {10},
keywords = {classification, evaluation, data streams, online learning},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@article{10.1145/269012.269023,
author = {Orr, Ken},
title = {Data Quality and Systems Theory},
year = {1998},
issue_date = {Feb. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/269012.269023},
doi = {10.1145/269012.269023},
journal = {Commun. ACM},
month = {feb},
pages = {66–71},
numpages = {6}
}

@inproceedings{10.1145/3289402.3289525,
author = {Aziz, Khadija and Zaidouni, Dounia and Bellafkih, Mostafa},
title = {Big Data Processing Using Machine Learning Algorithms: MLlib and Mahout Use Case},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289525},
doi = {10.1145/3289402.3289525},
abstract = {Machine learning is a field within artificial intelligence that allows machines to learn on their own from existing information to make predictions or/and decisions. There are three main categories of machine learning techniques: Collaborative filtering (for making recommendations), Clustering (for discovering structure in collections of data) and Classification (form of supervised learning). Machine learning helps users to make better decisions, Machine learning algorithms create patterns based on previous information and use them to design predictive models, then, use this models to obtain predictions about future data. A huge amount of data from several sources need methods and techniques to be processed correctly, in order to exploit this data efficiently, machine learning is a great technology for exploiting the needs in big data analysis. This paper describes the implementation of Apache Spark MLlib and Apache Mahout in order to process Big Data using Machine Learning algorithms. Furthermore, we conduct experimental simulations to show the difference between this two Machine Learning frameworks. Subsequently, we discuss the most striking observations that emerge from the comparison of these technologies through several experimental studies.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {25},
numpages = {6},
keywords = {Big Data, Machine Learning, Collaborative Filtering, Clustering, MLlib, Spark, Classification, Mahout, Hadoop},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.5555/3026877.3026922,
author = {Papadimitriou, Antonis and Bhagwan, Ranjita and Chandran, Nishanth and Ramjee, Ramachandran and Haeberlen, Andreas and Singh, Harmeet and Modi, Abhishek and Badrinarayanan, Saikrishna},
title = {Big Data Analytics over Encrypted Datasets with Seabed},
year = {2016},
isbn = {9781931971331},
publisher = {USENIX Association},
address = {USA},
abstract = {Today, enterprises collect large amounts of data and leverage the cloud to perform analytics over this data. Since the data is often sensitive, enterprises would prefer to keep it confidential and to hide it even from the cloud operator. Systems such as CryptDB and Monomi can accomplish this by operating mostly on encrypted data; however, these systems rely on expensive cryptographic techniques that limit performance in true "big data" scenarios that involve terabytes of data or more.This paper presents Seabed, a system that enables efficient analytics over large encrypted datasets. In contrast to previous systems, which rely on asymmetric encryption schemes, Seabed uses a novel, additively symmetric homomorphic encryption scheme (ASHE) to perform large-scale aggregations efficiently. Additionally, Seabed introduces a novel randomized encryption scheme called Splayed ASHE, or SPLASHE, that can, in certain cases, prevent frequency attacks based on auxiliary data.},
booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
pages = {587–602},
numpages = {16},
location = {Savannah, GA, USA},
series = {OSDI'16}
}

@article{10.1145/2935882,
author = {Metcalf, Jacob},
title = {Big Data Analytics and Revision of the Common Rule},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2935882},
doi = {10.1145/2935882},
abstract = {Reconsidering traditional research ethics given the emergence of big data analytics.},
journal = {Commun. ACM},
month = {jun},
pages = {31–33},
numpages = {3}
}

@inproceedings{10.1145/2539150.2539154,
author = {Pokorny, Jaroslav},
title = {NoSQL Databases - No Panacea for Big Data Processing},
year = {2013},
isbn = {9781450321136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2539150.2539154},
doi = {10.1145/2539150.2539154},
abstract = {Big Data, big number of users, and cloud computing are driving the adoption of new database architectures, particularly NoSQL databases. Both research and practice indicate that traditional universal DBMS architecture hardly satisfies new trends in data processing in such environment. NoSQL databases enable better application development productivity through a more flexible data model, greater ability to scale dynamically to support more users and data, an ability to develop highly responsive applications and more complex processing of data.On the other hand, NoSQL databases support solving data problems only partially. We will describe their basic features like horizontal scalability and concurrency model, which offer mostly weaker tools for querying and transactions processing than relational SQL-like database systems do. We will also present some data models and querying capabilities of NoSQL databases in more detail as well as an overview of some their representatives.The NoSQL system properties mentioned imply that most of them are unsuitable, e.g., for the DW and BI querying or, in general, for the enterprise data processing. Consequently, new database architectures and various hybrid solutions are developed. We will point out on these actual problems and present some of the current approaches in detail.},
booktitle = {Proceedings of International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {4},
numpages = {1},
location = {Vienna, Austria},
series = {IIWAS '13}
}

@inproceedings{10.1145/3399205.3399225,
author = {Elhassan, Jamal and Aniss, Moumen and Jamal, Chao},
title = {Big Data Analytic Architecture for Water Resources Management: A Systematic Review},
year = {2020},
isbn = {9781450375788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399205.3399225},
doi = {10.1145/3399205.3399225},
abstract = {The management of hydraulic resources and especially water resources has become a major priority for decision-makers and planners at the international level. The advent of new technologies such as big data analytics and IoT has led to exponential changes in the volume of data generated daily in real-time, this monitoring information has potential significance if it is collected and aggregated effectively. The data collected are increasingly complex and represent a central government issue, developing water resource management strategies. The exploitation of this type of data requires new methods of analysis and knowledge discovery. This work presents a literature review of articles related to big data and water resources. Also, we present our proposition of a new architecture to conduct a big data analytic. In conclusion, we discuss the impact of using this technology in water resource management.},
booktitle = {Proceedings of the 4th Edition of International Conference on Geo-IT and Water Resources 2020, Geo-IT and Water Resources 2020},
articleno = {19},
numpages = {5},
keywords = {IoT, real-time, water resources, big data analytics},
location = {Al-Hoceima, Morocco},
series = {GEOIT4W-2020}
}

@inproceedings{10.1145/2970276.2970325,
author = {Li, Nan and Lei, Yu and Khan, Haider Riaz and Liu, Jingshu and Guo, Yun},
title = {Applying Combinatorial Test Data Generation to Big Data Applications},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970325},
doi = {10.1145/2970276.2970325},
abstract = {Big data applications (e.g., Extract, Transform, and Load (ETL) applications) are designed to handle great volumes of data. However, processing such great volumes of data is time-consuming. There is a need to construct small yet effective test data sets during agile development of big data applications. In this paper, we apply a combinatorial test data generation approach to two real-world ETL applications at Medidata. In our approach, we first create Input Domain Models (IDMs) automatically by analyzing the original data source and incorporating constraints manually derived from requirements. Next, the IDMs are used to create test data sets that achieve t-way coverage, which has shown to be very effective in detecting software faults. The generated test data sets also satisfy all the constraints identified in the first step. To avoid creating IDMs from scratch when there is a change to the original data source or constraints, our approach extends the original IDMs with additional information. The new IDMs, which we refer to as Adaptive IDMs (AIDMs), are updated by comparing the changes against the additional information, and are then used to generate new test data sets. We implement our approach in a tool, called comBinatorial bIg daTa Test dAta Generator (BIT-TAG). Our experience shows that combinatorial testing can be effectively applied to big data applications. In particular, the test data sets created using our approach for the two ETL applications are only a small fraction of the original data source, but we were able to detect all the faults found with the original data source.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {637–647},
numpages = {11},
keywords = {Combinatorial Testing, Big Data Testing, Adaptive Input Domain Model, Input Domain Model, Test Data Generation},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@article{10.1145/2107536.2107538,
author = {M\"{u}ller, Heiko and Freytag, Johann-Christoph and Leser, Ulf},
title = {Improving Data Quality by Source Analysis},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2107536.2107538},
doi = {10.1145/2107536.2107538},
abstract = {In many domains, data cleaning is hampered by our limited ability to specify a comprehensive set of integrity constraints to assist in identification of erroneous data. An alternative approach to improve data quality is to exploit different data sources that contain information about the same set of objects. Such overlapping sources highlight hot-spots of poor data quality through conflicting data values and immediately provide alternative values for conflict resolution. In order to derive a dataset of high quality, we can merge the overlapping sources based on a quality assessment of the conflicting values. The quality of the resulting dataset, however, is highly dependent on our ability to asses the quality of conflicting values effectively.The main objective of this article is to introduce methods that aid the developer of an integrated system over overlapping, but contradicting sources in the task of improving the quality of data. Value conflicts between contradicting sources are often systematic, caused by some characteristic of the different sources. Our goal is to identify such systematic differences and outline data patterns that occur in conjunction with them. Evaluated by an expert user, the regularities discovered provide insights into possible conflict reasons and help to assess the quality of inconsistent values. The contributions of this article are two concepts of systematic conflicts: contradiction patterns and minimal update sequences. Contradiction patterns resemble a special form of association rules that summarize characteristic data properties for conflict occurrence. We adapt existing association rule mining algorithms for mining contradiction patterns. Contradiction patterns, however, view each class of conflicts in isolation, sometimes leading to largely overlapping patterns. Sequences of set-oriented update operations that transform one data source into the other are compact descriptions for all regular differences among the sources. We consider minimal update sequences as the most likely explanation for observed differences between overlapping data sources. Furthermore, the order of operations within the sequences point out potential dependencies between systematic differences. Finding minimal update sequences, however, is beyond reach in practice. We show that the problem already is NP-complete for a restricted set of operations. In the light of this intractability result, we present heuristics that lead to convincing results for all examples we considered.},
journal = {J. Data and Information Quality},
month = {mar},
articleno = {15},
numpages = {38},
keywords = {data cleaning, Conflict resolution, quality assessment, semantic distance measure}
}

@inproceedings{10.1145/3378936.3378957,
author = {Hujran, Omar and Alikaj, Ahmad and Durrani, Usman Khan and Al-Dmour, Nidal},
title = {Big Data and Its Effect on the Music Industry},
year = {2020},
isbn = {9781450376907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378936.3378957},
doi = {10.1145/3378936.3378957},
abstract = {This research discusses the effect of big data and Internet technologies on the music industry. Specifically, this paper addresses two research questions; (1) how do modern businesses in the music industry implement the use of Internet technologies and big data to ensure their success in the market, and (2) what are the advantages and drawbacks of implementing digital business models in the music industry? To answer the research questions, two real-life cases (i.e. Shazam and Spotify) were analyzed to show how modern businesses in the music industry implement big data and Internet technologies to ensure their success in the market. Furthermore, previous literature and secondary resources were used to explain the development of traditional business models into digital business models in the music industry. In addition to discussing the benefits and drawbacks of implementing the modern digital business model.},
booktitle = {Proceedings of the 3rd International Conference on Software Engineering and Information Management},
pages = {5–9},
numpages = {5},
keywords = {Music, Business Analytics, Spotify, Shazam, Big Data},
location = {Sydney, NSW, Australia},
series = {ICSIM '20}
}

@inproceedings{10.1145/3404687.3404707,
author = {Zhai, Chenggong and Xiong, Liang and Li, Yan},
title = {Optimization of Clothing Supply Standard Based on Big Data},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404707},
doi = {10.1145/3404687.3404707},
abstract = {By collecting the data of the process of the clothing support, a big data analysis and mining tool set based on the business entity of the clothing support is built, which supports the typical data mining methods such as classification analysis, association analysis, clustering analysis, etc. the analysis and mining results provide support for the business competent department to control the development trend, technical trends and grass-roots officers and soldiers' feedback, and research and establish a system Unified, scientific, structure optimized, level clear and easy to operate clothing supply standard system, timely adjust the existing supply support standards to meet the needs of Logistics Command and information management, which is conducive to the improvement of the clothing supply support ability and the improvement of the clothing support scientific decision-making ability.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {18–22},
numpages = {5},
keywords = {Supply Standard, Big Data, Clothing Support Demand},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1109/CCGrid.2016.61,
author = {Krish, K. R. and Wadhwa, Bharti and Iqbal, M. Safdar and Rafique, M. Mustafa and Butt, Ali R.},
title = {On Efficient Hierarchical Storage for Big Data Processing},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.61},
doi = {10.1109/CCGrid.2016.61},
abstract = {A promising trend in storage management for big data frameworks, such as Hadoop and Spark, is the emergence of heterogeneous and hybrid storage systems that employ different types of storage devices, e.g. SSDs, RAMDisks, etc., alongside traditional HDDs. However, scheduling data accesses or requests to an appropriate storage device is non-trivial and depends on several factors such as data locality, device performance, and application compute and storage resources utilization. To this end, we present Dux, an application-attuned dynamic data management system for data processing frameworks, which aims to improve overall application I/O throughput by efficiently using SSDs only for workloads that are expected to benefit from them rather than the extant approach of storing a fraction of the overall workloads in SSDs. The novelty of Dux lies in profiling application performance on SSDs and HDDs, analyzing the resulting I/O behavior, and considering the available SSDs at runtime to dynamically place data in an appropriate storage tier. Evaluation of Dux with trace-driven simulations using synthetic Facebook workloads shows that even when using 5.5\texttimes{} fewer SSDs compared to a SSD-only solution, Dux incurs only a small (5%) performance overhead, and thus offers an affordable and efficient storage tier management.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {403–408},
numpages = {6},
keywords = {heterogeneous storage, tiered storage, MapReduce, data-intensive computing, performance prediction},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/2339530.2339534,
author = {Jordan, Michael I.},
title = {Divide-and-Conquer and Statistical Inference for Big Data},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339534},
doi = {10.1145/2339530.2339534},
abstract = {I present some recent work on statistical inference for Big Data. Divide-and-conquer is a natural computational paradigm for approaching Big Data problems, particularly given recent developments in distributed and parallel computing, but some interesting challenges arise when applying divide-and-conquer algorithms to statistical inference problems. One interesting issue is that of obtaining confidence intervals in massive datasets.The bootstrap principle suggests resampling data to obtain fluctuations in the values of estimators, and thereby confidence intervals, but this is infeasible with massive data. Subsampling the data yields fluctuations on the wrong scale, which have to be corrected to provide calibrated statistical inferences. I present a new procedure, the "bag of little bootstraps," which circumvents this problem, inheriting the favorable theoretical properties of the bootstrap but also having a much more favorable computational profile. Another issue that I discuss is the problem of large-scale matrix completion. Here divide-and-conquer is a natural heuristic that works well in practice, but new theoretical problems arise when attempting to characterize the statistical performance of divide-and-conquer algorithms. Here the theoretical support is provided by concentration theorems for random matrices, and I present a new approach to this problem based on Stein's method1.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {4},
numpages = {1},
keywords = {big data, confidence intervals, subsampling, divide-and-conquer},
location = {Beijing, China},
series = {KDD '12}
}

@article{10.5555/3204979.3205017,
author = {Singh, Anshuman and Singh, Sumi and Yousef, Mahmoud},
title = {A Conceptual Framework for Designing a Big Data Course},
year = {2018},
issue_date = {May 2018},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {33},
number = {5},
issn = {1937-4771},
abstract = {Big data is a fast changing area, and it is challenging to design a college course in big data that is up to date and is grounded in the fundamentals of the discipline. Some themes have emerged in the last few years that can be considered candidates for fundamentals of big data. In this paper, we identify and organize these fundamental concepts into a framework that can be used to design a big data course. We also present our course designs based on the framework and student feedback from our offerings in the last two years.},
journal = {J. Comput. Sci. Coll.},
month = {may},
pages = {192–198},
numpages = {7}
}

@inproceedings{10.5555/2740769.2740789,
author = {Wu, Zhaohui and Wu, Jian and Khabsa, Madian and Williams, Kyle and Chen, Hung-Hsuan and Huang, Wenyi and Tuarob, Suppawong and Choudhury, Sagnik Ray and Ororbia, Alexander and Mitra, Prasenjit and Giles, C. Lee},
title = {Towards Building a Scholarly Big Data Platform: Challenges, Lessons and Opportunities},
year = {2014},
isbn = {9781479955695},
publisher = {IEEE Press},
abstract = {We introduce a big data platform that provides various services for harvesting scholarly information and enabling efficient scholarly applications. The core architecture of the platform is built on a secured private cloud, crawls data using a scholarly focused crawler that leverages a dynamic scheduler, processes by utilizing a map reduce based crawl-extraction-ingestion (CEI) workflow, and is stored in distributed repositories and databases. Services such as scholarly data harvesting, information extraction, and user information and log data analytics are integrated into the platform and provided by an OAI and RESTful API. We also introduce a set of scholarly applications built on top of this platform including citation recommendation and collaborator discovery.},
booktitle = {Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {117–126},
numpages = {10},
keywords = {scholarly big data, big data, information extraction},
location = {London, United Kingdom},
series = {JCDL '14}
}

@article{10.5555/2627435.2638579,
author = {Tan, Mingkui and Tsang, Ivor W. and Wang, Li},
title = {Towards Ultrahigh Dimensional Feature Selection for Big Data},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {In this paper, we present a new adaptive feature scaling scheme for ultrahigh-dimensional feature selection on Big Data, and then reformulate it as a convex semi-infinite programming (SIP) problem. To address the SIP, we propose an efficient feature generating paradigm. Different from traditional gradient-based approaches that conduct optimization on all input features, the proposed paradigm iteratively activates a group of features, and solves a sequence of multiple kernel learning (MKL) subproblems. To further speed up the training, we propose to solve the MKL subproblems in their primal forms through a modified accelerated proximal gradient approach. Due to such optimization scheme, some efficient cache techniques are also developed. The feature generating paradigm is guaranteed to converge globally under mild conditions, and can achieve lower feature selection bias. Moreover, the proposed method can tackle two challenging tasks in feature selection: 1) group-based feature selection with complex structures, and 2) nonlinear feature selection with explicit feature mappings. Comprehensive experiments on a wide range of synthetic and real-world data sets of tens of million data points with O(1014) features demonstrate the competitive performance of the proposed method over state-of-the-art feature selection methods in terms of generalization performance and training effciency.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1371–1429},
numpages = {59},
keywords = {feature generation, feature selection, ultrahigh dimensionality, big data, multiple kernel learning, nonlinear feature selection}
}

@inproceedings{10.1145/3357223.3366029,
author = {Patel, Hiren and Jindal, Alekh and Szyperski, Clemens},
title = {Big Data Processing at Microsoft: Hyper Scale, Massive Complexity, and Minimal Cost},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3366029},
doi = {10.1145/3357223.3366029},
abstract = {The past decade has seen a tremendous interest in large-scale data processing at Microsoft. Typical scenarios include building business-critical pipelines such as advertiser feedback loop, index builder, and relevance/ranking algorithms for Bing; analyzing user experience telemetry for Office, Windows or Xbox; and gathering recommendations for products like Windows and Xbox. To address these needs a first-party big data analytics platform, referred to as Cosmos, was developed in the early 2010s at Microsoft. Cosmos makes it possible to store data at exabyte scale and process in a serverless form factor, with SCOPE [4] being the query processing workhorse. Over time, however, several newer challenges have emerged, requiring major technical innovations in Cosmos to meet these newer demands. In this abstract, we describe three such challenges from the query processing viewpoint, and our approaches to handling them.Hyper Scale. Cosmos has witnessed a significant growth in usage from its early days, from the number of customers (starting from Bing to almost every single business unit at Microsoft today), to the volume of data processed (from petabytes to exabytes today), to the amount of processing done (from tens of thousands of SCOPE jobs to hundreds of thousands of jobs today, across hundreds of thousands of machines). Even a single job can consume tens of petabytes of data and produce similar volumes of data by running millions of tasks in parallel. Our approach to handle this unprecedented scale is two fold. First, we decoupled and disaggregated the query processor from storage and resource management components, thereby allowing different components in the Cosmos stack to scale independently. Second, we scaled the data movement in the SCOPE query processor with quasilinear complexity [2]. This is crucial since data movement is often the most expensive step, and hence the bottleneck, in massive-scale data processing.Massive Complexity. Cosmos workloads are also highly complex. Thanks to adoption across the whole of Microsoft, Cosmos needs to support workloads that are representative of multiple industry segments, including search engine (Bing), operating system (Windows), workplace productivity (Office), personal computing (Surface), gaming (XBox), etc. To handle such diverse workloads, our approach has been to provide a one-size-fits-all experience. First of all, to make it easy for the customers to express their computations, SCOPE supports different types of queries, from batch to interactive to streaming and machine learning. Second, SCOPE supports both structured and unstructured data processing. Likewise, multiple data formats, including both propriety and open source source such as Parquet, are supported. Third, users can write business logic using a mix of declarative and imperative languages, over even different imperative languages such as C# and Python, in the same job. Furthermore, users can express all of the above in simple data flow style computation for better readability and maintainability. Finally, considering the diverse workload mix inside Microsoft, we have come to realization that it is not possible to fits all scenarios using SCOPE. Therefore, we also support the popular Spark query processing engine. Overall, the one-size-fits-all query processing experience in Cosmos covers very diverse workloads, including data formats, programming languages, and the backend engines.Minimal Cost. While scale and complexity are hard by themselves, the biggest challenge is to achieve all of that at minimal cost. In fact, there is a pressing need to improve Cosmos efficiency and reduce operational costs. This is challenging due to several reasons. First, optimizing a SCOPE job is hard considering that the SCOPE DAGs are super large (up to 1000s of operators in single job!), and the optimization estimates (cardinality, cost, etc.) are often way off from the actuals. Second, SCOPE optimizes a given query, while the operational costs depend on the overall workload. Therefore workload optimization becomes very important. And finally, SCOPE jobs are typically interlinked in data pipelines, i.e., the output of one job is consumed by other jobs. This means that workload optimization needs to be aware of these dependencies. Our approach is to develop a feedback loop to learn from past workloads in order to optimize the future ones. Specifically, we leverage machine learning to learn models for optimizing individual jobs [3], apply multi-query optimizations to optimize the costs of overall workload [1], and build dependency graphs to identify and optimize for the data pipelines.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {490},
numpages = {1},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings{10.1145/3322134.3322137,
author = {Koberidze, Aleksandr Z.},
title = {Implementation Practice of Big Data Technology in a Sharing Economy},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322137},
doi = {10.1145/3322134.3322137},
abstract = {This article attempts to uncover the main reasons for the need to implement and use Big Data for the effective functioning of an enterprise in the conditions of the sharing economy. As a key evidence that Big Data is a necessary element of the sharing economy, a comparative analysis of traditional and large databases are used. As a research task, an attempt was made to assess the scope, level of use and results of the implementation of Big Data in companies belonging to various market areas. In conclusion, all the advantages are revealed with which, in the context of the new economic paradigm, Big Data allows companies to respond quickly to user requests in the virtual space.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {14–18},
numpages = {5},
keywords = {Crowd Funding, Big Data, Peer-To-Peer accommodation, Crowd Lending, sharing economy, Peer-To-Peer, Car Sharing},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@inproceedings{10.1145/2642769.2642802,
author = {Cheptsov, Alexey},
title = {HPC in Big Data Age: An Evaluation Report for Java-Based Data-Intensive Applications Implemented with Hadoop and OpenMPI},
year = {2014},
isbn = {9781450328753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642769.2642802},
doi = {10.1145/2642769.2642802},
abstract = {The current IT technologies have a strong need for scaling up the high-performance analysis to large-scale datasets. Tremendously increased over the last few years volume and complexity of data gathered in both public (such as on the web) and enterprise (e.g. digitalized internal document base) domains have posed new challenges to providers of high performance computing (HPC) infrastructures, which is recognised in the community as Big Data problem. On contrast to the typical HPC applications, the Big Data ones are not oriented on reaching the peak performance of the infrastructure and thus offer more opportunities for the "capacity" infrastructure model rather than for the "capability" one, making the use of Cloud infrastructures preferable over the HPC. However, considering the more and more vanishing difference between these two infrastructure types, i.e. Cloud and HPC, it makes a lot of sense to investigate the abilities of traditional HPC infrastructure to execute Big Data applications as well, despite their relatively poor efficiency as compared with the traditional, very optimized HPC ones. This paper discusses the main state-of-the-art parallelisation techniques utilised in both Cloud and HPC domains and evaluates them on an exemplary text processing application on a testbed HPC cluster.},
booktitle = {Proceedings of the 21st European MPI Users' Group Meeting},
pages = {175–180},
numpages = {6},
keywords = {MPI, HPC, Hadoop, Cloud, MapReduce},
location = {Kyoto, Japan},
series = {EuroMPI/ASIA '14}
}

@inproceedings{10.1145/3030207.3053670,
author = {Singhal, Rekha and Kunde, Shruti},
title = {Technology Migration Challenges in a Big Data Architecture Stack},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3053670},
doi = {10.1145/3030207.3053670},
abstract = {Application and/or data migration is a result of limitations in existing system architecture to handle new requirements and the availability of newer, more efficient technology. In any big data architecture, technology migration is staggered across multiple levels and poses functional (related to components of the architecture and underlying infrastructure) and non-functional (QoS) challenges such as availability, reliability and performance guarantees in the target architecture. In this paper, (1) we outline a big data architecture stack and identify research problems arising out of the technology migration in this scenario (2) we propose a smart rule engine system which facilitates the decision making process for the technology to be used at different layers in the architecture during migration.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {159–160},
numpages = {2},
keywords = {performance, migration, big data},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/3323503.3345030,
author = {Goularte, Rudinei and Trojahn, Tiago H. and Kishi, Rodrigo M.},
title = {Multimedia Information Retrieval in Big Data Using OpenCV Python},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3345030},
doi = {10.1145/3323503.3345030},
abstract = {The popularization of systems, applications and devices to produce, view and share multimedia, saw the need to treat a large volume of data arise. In related areas (such as Multimedia Big Data, Data Science and Multimedia Information Retrieval) a key step is commonly referred as Multimedia Indexing or Multimedia Big Data Analysis, where the aim is to represent multimedia content into smaller, more manageable units, allowing the extraction of data features and information essential to the proper performance of the associated services. This mini-course discusses current tools and techniques for indexing, extracting and processing of multimodal multimedia content. The techniques are exemplified in Python OpenCV over different content (like images, audio, text and video), leading to the interest of services like Netflix, Google and YouTube on this subject, attracting the interest of researchers and developers.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {25–27},
numpages = {3},
keywords = {multimedia, big data, multimedia information retrieval},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.4108/icst.valuetools.2013.254398,
author = {Barbierato, Enrico and Gribaudo, Marco and Iacono, Mauro},
title = {Modeling Apache Hive Based Applications in Big Data Architectures},
year = {2013},
isbn = {9781936968480},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.valuetools.2013.254398},
doi = {10.4108/icst.valuetools.2013.254398},
abstract = {Performance prediction for Big Data applications is a powerful tool supporting designers and administrators in achieving a better exploitation of their computing resources. Big Data architectures are complex, continuously evolving and adaptive, thus a rapid design and verification modeling approach can be fit to the needs. As a result, a minimal semantic gap between models and applications would enable a wider number of designers to directly benefit from the results. The paper presents a multiformalism modeling approach based on a one-to-one mapping of Apache Hive querying primitives to modeling primitives. This approach exploits a combination of proper Big Data specific submodels and Petri nets to enable modeling of conventional application logic.},
booktitle = {Proceedings of the 7th International Conference on Performance Evaluation Methodologies and Tools},
pages = {30–38},
numpages = {9},
location = {Torino, Italy},
series = {ValueTools '13}
}

@inproceedings{10.1145/3148055.3148057,
author = {Hall, Logan and Harris, Bryan and Tomes, Erica and Altiparmak, Nihat},
title = {Big Data Aware Virtual Machine Placement in Cloud Data Centers},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148057},
doi = {10.1145/3148055.3148057},
abstract = {While society continues to be transformed by insights from processing big data, the increasing rate at which this data is gathered is making processing in private clusters obsolete. A vast amount of big data already resides in the cloud, and cloud infrastructures provide a scalable platform for both the computational and I/O needs of big data processing applications. Virtualization is used as a base technology in the cloud; however, existing virtual machine placement techniques do not consider data replication and I/O bottlenecks of the infrastructure, yielding sub-optimal data retrieval times. This paper targets efficient big data processing in the cloud and proposes novel virtual machine placement techniques, which minimize data retrieval time by considering data replication, storage performance, and network bandwidth. We first present an integer-programming based optimal virtual machine placement algorithm and then propose two low cost data- and energy-aware virtual machine placement heuristics. Our proposed heuristics are compared with optimal and existing algorithms through extensive evaluation. Experimental results provide strong indications for the superiority of our proposed solutions in both performance and energy, and clearly outline the importance of big data aware virtual machine placement for efficient processing of large datasets in the cloud.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {209–218},
numpages = {10},
keywords = {big data, virtualization, storage systems, cloud computing},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.1145/3075564.3078885,
author = {Vermij, Erik and Fiorin, Leandro and Hagleitner, Christoph and Bertels, Koen},
title = {Sorting Big Data on Heterogeneous Near-Data Processing Systems},
year = {2017},
isbn = {9781450344876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3075564.3078885},
doi = {10.1145/3075564.3078885},
abstract = {Big data workloads assumed recently a relevant importance in many business and scientific applications. Sorting elements efficiently in big data workloads is a key operation. In this work, we analyze the implementation of the mergesort algorithm on heterogeneous systems composed of CPUs and near-data processors located on the system memory channels. For configurations with equal number of active CPU cores and near-data processors, our experiments show a performance speedup of up to 2.5, as well as up to 2.5x energy-per-solution reduction.},
booktitle = {Proceedings of the Computing Frontiers Conference},
pages = {349–354},
numpages = {6},
location = {Siena, Italy},
series = {CF'17}
}

@inproceedings{10.1145/2642937.2643006,
author = {James Stephen, Julian and Savvides, Savvas and Seidel, Russell and Eugster, Patrick},
title = {Program Analysis for Secure Big Data Processing},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2643006},
doi = {10.1145/2642937.2643006},
abstract = {The ubiquitous nature of computers is driving a massive increase in the amount of data generated by humans and machines. Two natural consequences of this are the increased efforts to (a) derive meaningful information from accumulated data and (b) ensure that data is not used for unintended purposes. In the direction of analyzing massive amounts of data (a.), tools like MapReduce, Spark, Dryad and higher level scripting languages like Pig Latin and DryadLINQ have significantly improved corresponding tasks for software developers. The second, but equally important aspect of ensuring confidentiality (b.), has seen little support emerge for programmers: while advances in cryptographic techniques allow us to process directly on encrypted data, programmer-friendly and efficient ways of programming such data analysis jobs are still missing. This paper presents novel data flow analyses and program transformations for Pig Latin, that automatically enable the execution of corresponding scripts on encrypted data. We avoid fully homomorphic encryption because of its prohibitively high cost; instead, in some cases, we rely on a minimal set of operations performed by the client. We present the algorithms used for this translation, and empirically demonstrate the practical performance of our approach as well as improvements for programmers in terms of the effort required to preserve data confidentiality.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {277–288},
numpages = {12},
keywords = {privacy, big data, cloud computing},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1145/2963143,
author = {Khalifa, Shadi and Elshater, Yehia and Sundaravarathan, Kiran and Bhat, Aparna and Martin, Patrick and Imam, Fahim and Rope, Dan and Mcroberts, Mike and Statchuk, Craig},
title = {The Six Pillars for Building Big Data Analytics Ecosystems},
year = {2016},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/2963143},
doi = {10.1145/2963143},
abstract = {With almost everything now online, organizations look at the Big Data collected to gain insights for improving their services. In the analytics process, derivation of such insights requires experimenting-with and integrating different analytics techniques, while handling the Big Data high arrival velocity and large volumes. Existing solutions cover bits-and-pieces of the analytics process, leaving it to organizations to assemble their own ecosystem or buy an off-the-shelf ecosystem that can have unnecessary components to them. We build on this point by dividing the Big Data Analytics problem into six main pillars. We characterize and show examples of solutions designed for each of these pillars. We then integrate these six pillars into a taxonomy to provide an overview of the possible state-of-the-art analytics ecosystems. In the process, we highlight a number of ecosystems to meet organizations different needs. Finally, we identify possible areas of research for building future Big Data Analytics Ecosystems.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {33},
numpages = {36},
keywords = {Orchestration, analytics talent gap, consumable analytics}
}

@inproceedings{10.1145/3156346.3156347,
author = {Kwoh, Chee Keong},
title = {AI and Big Data Analytics for Health and Bioinformatics},
year = {2017},
isbn = {9781450353502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3156346.3156347},
doi = {10.1145/3156346.3156347},
abstract = {With the technological advances that allow for high throughput profiling of biological systems at a low cost. The low cost of data generation is leading us to the "big data" era. The availability of big data provides unprecedented opportunities but also raises new challenges for data mining and analysis. In this talk, I will start will the concepts in the analysis of big data, specifically the AI algorithms.My group has in The Biomedical Informatics Lab (BIL) is a research Centre is the focus of the education, research and development, and human-resource training in heath informatics and bioinformatics at NTU. The mission of BIL is to provide the interdisciplinary environment and training for students and researchers to engage in leading and cutting edge research in bioinformatics, and thereby become a part of the life sciences workforce in Singapore and elsewhere.This talk, by presenting selected research activities, will provide an overview of some of the innovative and creative approaches with the application of AI in big data analytics to address the challenges and solutions in both health and bioinformatics.},
booktitle = {Proceedings of the 8th International Conference on Computational Systems-Biology and Bioinformatics},
pages = {1},
numpages = {1},
location = {Nha Trang City, Viet Nam},
series = {CSBio '17}
}

@inproceedings{10.1145/2608020.2612731,
author = {Parashar, Manish},
title = {Big Data Challenges in Simulation-Based Science},
year = {2014},
isbn = {9781450329132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2608020.2612731},
doi = {10.1145/2608020.2612731},
abstract = {Data-related challenges are quickly dominating computational and data-enabled sciences, and are limiting the potential impact of scientific applications enabled by current and emerging high-performance distributed computing environments. These data-intensive application workflows involve dynamic coordination, interactions and data coupling between multiple application process that run at scale on different resources, and with services for monitoring, analysis and visualization and archiving. In this talk I will explore data grand challenges in simulation-based science and investigate how solutions based on data sharing abstractions, managed data pipelines, in-memory data-staging, in-situ placement and execution, and in-transit data processing can be used to address these data challenges at extreme scales.},
booktitle = {Proceedings of the Sixth International Workshop on Data Intensive Distributed Computing},
pages = {1–2},
numpages = {2},
keywords = {extreme-scale computing, dataspaces, data-intensive computing, in-situ data processing, data staging},
location = {Vancouver, BC, Canada},
series = {DIDC '14}
}

@inproceedings{10.1145/3386723.3387886,
author = {Tantaoui, Mouad and Laanaoui, My Driss and Kabil, Mustapha},
title = {Real-Time Prediction of Accident Using Big Data System},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387886},
doi = {10.1145/3386723.3387886},
abstract = {In this paper, we propose a new prediction system in real time using Big Data to improve the VANET network. Firstly, The Traffic density and average speed are calculated in each section of road, and then the risk of vehicle accident is predicted in instantaneous manner with parallel data processing, which makes execution faster.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {67},
numpages = {5},
keywords = {Lambda architecture, Traffic density, VANET, Big Data},
location = {Marrakech, Morocco},
series = {NISS2020}
}

@inproceedings{10.1145/1951365.1951432,
author = {Agrawal, Divyakant and Das, Sudipto and El Abbadi, Amr},
title = {Big Data and Cloud Computing: Current State and Future Opportunities},
year = {2011},
isbn = {9781450305280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1951365.1951432},
doi = {10.1145/1951365.1951432},
abstract = {Scalable database management systems (DBMS)---both for update intensive application workloads as well as decision support systems for descriptive and deep analytics---are a critical part of the cloud infrastructure and play an important role in ensuring the smooth transition of applications from the traditional enterprise infrastructures to next generation cloud infrastructures. Though scalable data management has been a vision for more than three decades and much research has focussed on large scale data management in traditional enterprise setting, cloud computing brings its own set of novel challenges that must be addressed to ensure the success of data management solutions in the cloud environment. This tutorial presents an organized picture of the challenges faced by application developers and DBMS designers in developing and deploying internet scale applications. Our background study encompasses both classes of systems: (i) for supporting update heavy applications, and (ii) for ad-hoc analytics and decision support. We then focus on providing an in-depth analysis of systems for supporting update intensive web-applications and provide a survey of the state-of-the-art in this domain. We crystallize the design choices made by some successful systems large scale database management systems, analyze the application demands and access patterns, and enumerate the desiderata for a cloud-bound DBMS.},
booktitle = {Proceedings of the 14th International Conference on Extending Database Technology},
pages = {530–533},
numpages = {4},
location = {Uppsala, Sweden},
series = {EDBT/ICDT '11}
}

@article{10.14778/3137765.3137829,
author = {Giatrakos, Nikos and Artikis, Alexander and Deligiannakis, Antonios and Garofalakis, Minos},
title = {Complex Event Recognition in the Big Data Era},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137829},
doi = {10.14778/3137765.3137829},
abstract = {The concept of event processing is established as a generic computational paradigm in various application fields, ranging from data processing in Web environments, over maritime and transport, to finance and medicine. Events report on state changes of a system and its environment. Complex Event Recognition (CER) in turn, refers to the identification of complex/composite events of interest, which are collections of simple events that satisfy some pattern, thereby providing the opportunity for reactive and proactive measures. Examples include the recognition of attacks in computer network nodes, human activities on video content, emerging stories and trends on the Social Web, traffic and transport incidents in smart cities, fraud in electronic marketplaces, cardiac arrhythmias, and epidemic spread. In each scenario, CER allows to make sense of Big event Data streams and react accordingly. The goal of this tutorial is to provide a step-by-step guide for realizing CER in the Big Data era. To do so, it elaborates on major challenges and describes algorithmic toolkits for optimized manipulation of event streams characterized by high volume, velocity and/or lack of veracity, placing emphasis on distributed CER over potentially heterogeneous (data variety) event sources. Finally, we highlight future research directions in the field.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1996–1999},
numpages = {4}
}

@inproceedings{10.1145/3155133.3155138,
author = {Onizuka, Makoto},
title = {Trend and Applications of Big Data and IoT Techniques},
year = {2017},
isbn = {9781450353281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3155133.3155138},
doi = {10.1145/3155133.3155138},
abstract = {As people say "Data is the new oil," Big data is expected to make a large impact on our society and economics by mining hidden knowledge and rules from the data. In particular, the structure of the real world data is changing from traditional relational data model to more generalized graph data model, as the web and social media are getting popular in the world. One of the most important technical challenges here is to efficiently analyze large graph data that express various types of relationship between people, items, and places. In this talk, we overview the trend of Big Data and IoT and then explain our research on distributed query optimization on cloud environment and efficient graph mining algorithms. Finally, we introduce some of our interesting applications of Big Data: 1) social network analysis by employing graph mining algorithms, 2) business data analysis by exploratory data analysis techniques, and 3) Smart route recommendation system empowered by IoT.},
booktitle = {Proceedings of the Eighth International Symposium on Information and Communication Technology},
pages = {5},
numpages = {1},
location = {Nha Trang City, Viet Nam},
series = {SoICT 2017}
}

@proceedings{10.1145/3226116,
title = {ICBDT '18: Proceedings of 2018 International Conference on Big Data Technologies},
year = {2018},
isbn = {9781450364270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The aim of this conference was to influence the spawning of new areas of Big Data Technologies, to provide a great platform for researchers, academicians and industry persons of these fields to exchange new ideas and application experiences face to face, to establish business or research relations and to find global cooperation. It covered a broad range of topics in the field, such as Data Visualization and Visual Analytics, Big Data Algorithms, Applications and Services, Big Data Mining and Analytics, Big Data Processing and Querying, Natural Language Processing in Big Texts, Scalable computational intelligence tools, Evolutionary and Bio-inspired approaches for Big Data analysis, etc.},
location = {Hangzhou, China}
}

@article{10.1109/TCBB.2014.2351800,
author = {Wang, Chao and Li, Xi and Chen, Peng and Wang, Aili and Zhou, Xuehai and Yu, Hong},
title = {Heterogeneous Cloud Framework for Big Data Genome Sequencing},
year = {2015},
issue_date = {January/February 2015},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {12},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2014.2351800},
doi = {10.1109/TCBB.2014.2351800},
abstract = {The next generation genome sequencing problem with short (long) reads is an emerging field in numerous scientific and big data research domains. However, data sizes and ease of access for scientific researchers are growing and most current methodologies rely on one acceleration approach and so cannot meet the requirements imposed by explosive data scales and complexities. In this paper, we propose a novel FPGA-based acceleration solution with MapReduce framework on multiple hardware accelerators. The combination of hardware acceleration and MapReduce execution flow could greatly accelerate the task of aligning short length reads to a known reference genome. To evaluate the performance and other metrics, we conducted a theoretical speedup analysis on a MapReduce programming platform, which demonstrates that our proposed architecture have efficient potential to improve the speedup for large scale genome sequencing applications. Also, as a practical study, we have built a hardware prototype on the real Xilinx FPGA chip. Significant metrics on speedup, sensitivity, mapping quality, error rate, and hardware cost are evaluated, respectively. Experimental results demonstrate that the proposed platform could efficiently accelerate the next generation sequencing problem with satisfactory accuracy and acceptable hardware cost.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jan},
pages = {166–178},
numpages = {13},
keywords = {reconfigurable hardware, FPGA, short reads, mapping, genome sequencing}
}

@inproceedings{10.1145/3423603.3424049,
author = {Aissa, Mohamed Mehdi Ben and Sfaxi, Lilia and Robbana, Riadh},
title = {Decisional Architectures from Business Intelligence to Big Data: Challenges and Opportunities},
year = {2020},
isbn = {9781405377539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423603.3424049},
doi = {10.1145/3423603.3424049},
abstract = {Information is one of the most important factors in business success, hence the importance of the Business Intelligence (BI) domain in order to simplify the decision making and make it more relevant. Decisional systems have been used for several years to help decision-makers access, analyze and extract value from the data that their organisation accumulated through the years. The success gained by these types of systems caused the establishment of a well-known architecture and development chain, and the proliferation of tools and methodologies that have proven their value. Nonetheless, in some use cases, the classical decisional architecture shows some shortcomings. In fact, the traditional storage and processing models in Business Intelligence systems are not sufficient anymore when confronted with data that becomes more and more massive, varied and with a high velocity. This is where Big Data solutions can be of great use. In fact, these solutions have proven their efficiency when dealing with enormous constantly increasing amounts of data with a changing schema. Our goal in this article is to show the various manners to integrate big data solutions into the decisional world, and to help architects choose which architecture corresponds better to their needs, by taking into consideration the environmental, technical and functional constraints they are faced with.},
booktitle = {Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress},
articleno = {12},
numpages = {9},
keywords = {business intelligence, decisional systems, decision tree for decisional architectures, big data, data lake, data warehouse, OLAP, software architecture},
location = {Virtual Event, Tunisia},
series = {DTUC '20}
}

@inproceedings{10.1145/2818869.2818884,
author = {Wu, Chun-Hsin and Hsu, Pu},
title = {Cost-Effective and Reliable Cloud Storage for Big Data},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818884},
doi = {10.1145/2818869.2818884},
abstract = {Efficiency and reliability are critical to the performance of big data computing. Hadoop is one of the most popular cloud platforms to support big data analysis. To improve data availability, the Hadoop distributed file system usually stores blocks of files in triplicate, but it incurs high storage costs and high information-leakage risks. In this paper we propose a new cost-effective approach in xHDFS: paired blocks from two machines are erasure-coded to generate redundant blocks that are intelligently managed. We evaluate that xHDFS can effectively improve the reliability of cloud storage and tolerate network or site failures with lower storage costs.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {2},
numpages = {6},
keywords = {Big Data Computing, Hadoop, Reliability, Cloud File Systems, Remote Backup, Cloud Storage, Erasure Codes},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@inproceedings{10.1145/3374587.3374650,
author = {Shen, Shaoyi and Li, Bin and Li, Situo},
title = {Construction and Application of Big Data Analysis Platform for Enterprise},
year = {2020},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374650},
doi = {10.1145/3374587.3374650},
abstract = {A data revolution has been leading by big data, which have the extremely profound influence on the economic, social development and public life. This paper introduces the meaning of big data, and discusses the innovation and opportunity of enterprise under the perspective of big data. According to the information architecture, this paper supplies the basic construction of enterprise big data analysis platform, and suggests the strategy of application, which have certain realistic directive significance.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {54–58},
numpages = {5},
keywords = {Distributed, Shared, Data asset, Analysis of big data, Construction},
location = {Normal, IL, USA},
series = {CSAI2019}
}

@inproceedings{10.5555/3213032.3213044,
author = {Kavak, Hamdi and Padilla, Jose J. and Lynch, Christopher J. and Diallo, Saikou Y.},
title = {Big Data, Agents, and Machine Learning: Towards a Data-Driven Agent-Based Modeling Approach},
year = {2018},
isbn = {9781510860148},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {We have recently witnessed the proliferation of large-scale behavioral data that can be used to empirically develop agent-based models (ABMs). Despite this opportunity, the literature has neglected to offer a structured agent-based modeling approach to produce agents or its parts directly from data. In this paper, we present initial steps towards an agent-based modeling approach that focuses on individual-level data to generate agent behavioral rules and initialize agent attribute values. We present a structured way to integrate Big Data and machine learning techniques at the individual agent-level. We also describe a conceptual use-case study of an urban mobility simulation driven by millions of geo-tagged Twitter social media messages. We believe our approach will advance the-state-of-the-art in developing empirical ABMs and conducting their validation. Further work is needed to assess data suitability, to compare with other approaches, to standardize data collection, and to serve all these features in near-real time.},
booktitle = {Proceedings of the Annual Simulation Symposium},
articleno = {12},
numpages = {12},
keywords = {agent-based simulation, big data, data-driven modeling, machine learning},
location = {Baltimore, Maryland},
series = {ANSS '18}
}

