@article{10.1145/3092944,
author = {Kulkarni, Amey and Shea, Colin and Abtahi, Tahmid and Homayoun, Houman and Mohsenin, Tinoosh},
title = {Low Overhead CS-Based Heterogeneous Framework for Big Data Acceleration},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3092944},
doi = {10.1145/3092944},
abstract = {Big data processing on hardware gained immense interest among the hardware research community to take advantage of fast processing and reconfigurability. Though the computation latency can be reduced using hardware, big data processing cost is dominated by data transfers. In this article, we propose a low overhead framework based on compressive sensing (CS) to reduce data transfers up to 67% without affecting signal quality. CS has two important kernels: “sensing” and “reconstruction.” In this article, we focus on CS reconstruction is using orthogonal matching pursuit (OMP) algorithm. We implement the OMP CS reconstruction algorithm on a domain-specific PENC many-core platform and a low-power Jetson TK1 platform consisting of an ARM CPU and a K1 GPU. Detailed performance analysis of OMP algorithm on each platform suggests that the PENC many-core platform has 15\texttimes{} and 18\texttimes{} less energy consumption and 16\texttimes{} and 8\texttimes{} faster reconstruction time as compared to the low-power ARM CPU and K1 GPU, respectively. Furthermore, we implement the proposed CS-based framework on heterogeneous architecture, in which the PENC many-core architecture is used as an “accelerator” and processing is performed on the ARM CPU platform. For demonstration, we integrate the proposed CS-based framework with a hadoop MapReduce platform for a face detection application. The results show that the proposed CS-based framework with the PENC many-core as an accelerator achieves a 26.15% data storage/transfer reduction, with an execution time and energy consumption overhead of 3.7% and 0.002%, respectively, for 5,000 image transfers. Compared to the CS-based framework implementation on the low-power Jetson TK1 ARM CPU+GPU platform, the PENC many-core implementation is 2.3\texttimes{} faster for the image reconstruction part, while achieving 29% higher performance and 34% better energy efficiency for the complete face detection application on the Hadoop MapReduce platform.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {dec},
articleno = {25},
numpages = {25},
keywords = {Compressive sensing, many-core, heterogeneous architecture}
}

@inproceedings{10.5555/3382225.3382440,
author = {Day, Min-Yuh and Cheng, Tun-Kung and Li, Jheng-Gang},
title = {AI Robo-Advisor with Big Data Analytics for Financial Services},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Robo-Advisors has been growing attraction from the financial industry for offering financial services by using algorithms and acting as like human advisors to support investors making investment decisions. During the investment planning stage, portfolio optimization plays a crucial role, especially for the medium and long-term investors, in determining the allocation weight of assets to achieve the balance between investors expectation return and risk tolerance. The literature on the topic of portfolio optimization has been offering plenty of theoretical and practical guidance for implementing the theory; however, there is a paucity of studies focusing on the applications which are designed for Robo-Advisors. In this research, we proposed a modular system and focused on integrating big data analysis, deep learning method and the Black-Litterman model to generate asset allocation weight. We developed a portfolio optimization module which takes the information from a variety of sources, such as stocks prices, investor profile and the other alternative data, and used them as input to calculate optimal weights of assets in the portfolio. The module we developed could be used as a sub-system for Robo-Advisors, which offers a customized optimal portfolio based on investors preference.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1027–1031},
numpages = {5},
keywords = {portfolio optimization, big data analysis, robo-advisors, deep learning, black-litterman, investment management, financial technology},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/2815782.2815793,
author = {Malaka, Iman and Brown, Irwin},
title = {Challenges to the Organisational Adoption of Big Data Analytics: A Case Study in the South African Telecommunications Industry},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815793},
doi = {10.1145/2815782.2815793},
abstract = {The purpose of this interpretive study was to explore the challenges to the adoption of Big Data Analytics (BDA) in organisations. The Technology-Organisation-Environment (TOE) model was used to guide the study. Data was collected from a large telecommunication organization in South Africa. Seven participants, from both Information Technology (IT) and business were interviewed to gain a holistic overview of challenges towards the adoption of BDA. An inductive approach was used for analysis. Findings revealed technological challenges to the adoption of BDA as being Data Integration; Data Privacy; Return on Investment; Data Quality; Cost; Data Integrity; and Performance and Scalability. From the organizational perspective, the major challenges were Ownership and Control; Skills Shortages; Business Focus and Prioritisation; Training and Exposure; Silos; and Unclear Processes. From the environmental context there were no major challenges highlighted. Organisational challenges were deemed to be the major inhibitors to adoption of BDA},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {27},
numpages = {9},
keywords = {Big Data, Technology Adoption, Big Data Analytics, South Africa},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3250292,
author = {Bacardit, Jaume and Arnaldo, Ignacio and Veeramachaneni, Kalyan and O'Reilly, Una-May},
title = {Session Details: Workshop: Evolutionary Computation for Big Data and Big Learning},
year = {2014},
isbn = {9781450328814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250292},
doi = {10.1145/3250292},
abstract = {It is our great pleasure to welcome you to the first Workshop on Evolutionary Computation for Big Data and Big Learning ECBDL'14We live in a time of unprecedented access to cheap and vast amounts of computational resources, which is producing a big leap forward in the fields of machine learning and data mining. We can tackle datasets of a scale (be it instances, attributes, classes, etc.) that was unimaginable some years ago, in what is well known as big data. On the other hand we can also use all these vast computational resources with the aim of understanding better our machine learning methods, by performing large scale evaluations, parameter sweeps, etc. We refer to the overall use massive on-demand computation (cloud or GPUs) for machine learning as Big Learning. Evolutionary Machine Learning techniques are perfect candidates for big learning tasks due to their flexibility in knowledge representations, learning paradigms and their innate parallelism.In this workshop we have accepted two papers representing very different scenarios of big data and big learning. The first paper, "Evolving Relational Hierarchical Classification Rules for Predicting Gene Ontology-Based Protein Functions" by Ricardo Cerri, Rodrigo C. Barros, Alex A. Freitas and Andr\'{e} C. P. L. F. Carvalho, focuses on an extremely complex and heterogeneous classification task, where instances have multiple classes organized hierarchically. The method is tested on real-world biological data, and explores the use of new rule representations to enhance knowledge discovery.The second paper, "On the Application of GP to Streaming Data Classification Tasks with Label Budgets" by Ali Vahdat, Aaron Atwater, Andrew R. McIntyre, Malcolm I. Heywood, focuses on a very important topic within big data/learning, streaming data classification, in the particular scenario where access to the real annotation (classes) of data is costly, and budgets need to be specified.},
booktitle = {Proceedings of the Companion Publication of the 2014 Annual Conference on Genetic and Evolutionary Computation},
location = {Vancouver, BC, Canada},
series = {GECCO Comp '14}
}

@inproceedings{10.1145/2684822.2685326,
author = {Franklin, Michael},
title = {Making Sense of Big Data with the Berkeley Data Analytics Stack},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684822.2685326},
doi = {10.1145/2684822.2685326},
abstract = {The Berkeley AMPLab is creating a new approach to data analytics. Launching in early 2011, the lab aims to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and in crowds). The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the four years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark in-memory computation framework, and the Shark query processing system. BDAS features prominently in many industry discussions of the future of the Big Data analytics ecosystem -- a rare degree of impact for an ongoing academic project. Given this initial success, the lab is continuing on its research path, moving "up the stack" to better integrate and support advanced analytics and to make people a full-fledged resource for making sense of data. In this talk, I'll first outline the motivation and insights behind our research approach and describe how we have organized to address the cross-disciplinary nature of Big Data challenges. I will then describe the current state of BDAS with an emphasis on our newest efforts, including some or all of: the GraphX graph processing system, the Velox and MLBase machine learning platforms, and the SampleClean framework for hybrid human/computer data cleaning. Finally I will present our current views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
pages = {1–2},
numpages = {2},
keywords = {big data},
location = {Shanghai, China},
series = {WSDM '15}
}

@inproceedings{10.1145/3293614.3293626,
author = {Garc\'{\i}a-Ojeda, J. C. and Ort\'{\i}z, Meleny Luna and Garc\'{\i}a, Rodolfo S\'{a}nchez and C\'{a}ceres, Javier Hern\'{a}ndez and Argoti, Andr\'{e}s},
title = {A Computer Cluster for Big Data and Data Analytics Management: Design, Implementation, and Assessment},
year = {2018},
isbn = {9781450365727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293614.3293626},
doi = {10.1145/3293614.3293626},
abstract = {This paper deals with the implementation of a computer cluster based on Apache Hadoop technology to provide Big Data and Data Analytics services. The cluster is an innovative proposal of the TIC consulting office (CTIC) of the Universidad Santo Tom\'{a}s - Bucaramanga, which aims at satisfying the needs regarding Big Data and Data Analytics of the Bucaramanga Metropolitan Area. The cluster implementation includes the description of the architecture and configuration of the equipment involved as well as the series of experiments carried out to assess the cluster's performance in terms of computing time, energy consumption, cost, and carbon footprint.},
booktitle = {Proceedings of the Euro American Conference on Telematics and Information Systems},
articleno = {45},
numpages = {8},
keywords = {IT services, Apache Hadoop, Big Data, Data Analytics},
location = {Fortaleza, Brazil},
series = {EATIS '18}
}

@inproceedings{10.1145/3424978.3425000,
author = {Yuan, Lufeng and Gao, Xiaoxin and Wang, Sining and Wang, Jun},
title = {Research on Multi-Source Heterogeneous Big Data in Extra-Large Enterprises},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425000},
doi = {10.1145/3424978.3425000},
abstract = {Extra-large enterprises, due to their huge scale and complex businesses, face serious challenges in the big data time. This paper introduces the Operating and Monitoring Information System (OMIS) in the State Grid Corporation of China to try to use big data in the extra-large enterprises. OMIS consists of full coverage data flow path, compound general library of model and algorithm, multi-mode computing platform and interface components. It solves a series of key problems that are data barrier, transmission, analysis, computing, usability in extra large enterprises. OMIS has connected headquarters, provinces and cities, covered 27 provinces. Benefited from interface components, a programme for data extraction, model train and parallel computing can be implemented by several codes conveniently in OMIS. In the experiments, OMIS can extract 1.73TB line loss data in about 93 hours from 27 provinces, provide multiple algorithms to detect abnormal low-voltage substation areas, support high performance computing by parallelization. Finally, OMIS reaches 3.71 speedup ratio on five nodes.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {22},
numpages = {5},
keywords = {Multi-source, Parallelization, Big data, Monitoring, Extra-large enterprise, Heterogeneous, Operating, Data extraction},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.1145/3379247.3379294,
author = {Feng, Xiaoling and Zhang, Tianming and Zhao, Yue and Yu, Zhi and Yan, Long and Chen, Bo and Song, Yulun},
title = {Malicious SIM Cards Identification Method Based on Telecom Big Data Analytics},
year = {2020},
isbn = {9781450376730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379247.3379294},
doi = {10.1145/3379247.3379294},
abstract = {This paper presents a method that helps e-commercial enterprises enhance risk control capabilities and operate anti-fraud during online marketing events. By definition, Malicious SIM Cards (MSCs) identification in this paper aims to identify SIM cards which are utilized by malicious purposes on specified terminals at scale (e.g., SIM modem pool or bulk SMS machine), and behave significantly different from normal SIM cards, especially towards e-commercial companies' marketing events, e.g., releases of coupons. The proposed MSC identification method is implemented as a label-function-based factor matrix framework. Particularly, a set of label functions is constructed from six dimensions via telecom big data analytics, including frequency of replacement, location, card type, Call Detail Records (CDRs), ID type, in-network-period in terms of SIM cards. Subsequently, MSC labels are provided to the identified SIM cards among the entire China Unicom SIM cards, which realize an effective way for large scale commercial use. Finally, selected features' distribution of MSCs identified and normal SIM cards are illustrated for comparison purposes.},
booktitle = {Proceedings of 2020 the 6th International Conference on Computing and Data Engineering},
pages = {136–141},
numpages = {6},
keywords = {Malicious SIM cards, risk control, factor matrix, online-marketing, telecom big data analytics, label function},
location = {Sanya, China},
series = {ICCDE 2020}
}

@inproceedings{10.1145/2640087.2644159,
author = {Guo, Yike and Johnson, David},
title = {Creating a Chemistry of Sciences with Big Data: Building the Data Science Institute at Imperial College London},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644159},
doi = {10.1145/2640087.2644159},
abstract = {The Data Science Institute at Imperial College London launched in April 2014, and will provide a hub for data-driven research and education. Its mission is to provide a focal point for the College's capabilities in multidisciplinary data-driven research by coordinating advanced data science research for college scientists and partners, and educating the next generation of data scientists. We surveyed the data-driven research needs at Imperial College London to gain an understanding across all disciplines offered by the College, and analysed the responses to gain insights into scientific and engineering needs for data science research.A clear message is that multidisciplinarity is essential for Big Data and data science research to enable a "chemistry of sciences": connecting all disciplines by integrating data. This paper presents our efforts to best understand data-driven research needs in a highly multidisciplinary research-intensive institution and describes our vision for the future of the Data Science Institute at Imperial College London.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {13},
numpages = {7},
location = {Beijing, China},
series = {BigDataScience '14}
}

@proceedings{10.1145/3404687,
title = {ICBDC '20: Proceedings of the 5th International Conference on Big Data and Computing},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the proceedings of the 2020 5th International Conference on Big Data and Computing (ICBDC 2020), held from May 28 to 30, 2020. The keynote speakers who also further explored this topic that is so significant for this field include: Prof. Rajkumar Buyya, University of Melbourne, Australia, on speech title "New Frontiers in Cloud and Edge/Fog Computing for Big Data &amp; Internet-of- Things Applications", Prof. Changsheng Xu, Chinese Academy of Sciences, China, on speech title "Connecting Isolated Social Multimedia Big Data", Prof. Hong Shen, Sun Yat-sen University, China, on speech title "Job Scheduling in Cloud Data Centers", and Prof. Ruidong Li, National Institute of Information and Communications Technology, Japan, on speech Title "Blockchain Meets In-Network Big Data Sharing.},
location = {Chengdu, China}
}

@inproceedings{10.1145/2980258.2980319,
author = {Kamaruddin, Sk. and Ravi, Vadlamani},
title = {Credit Card Fraud Detection Using Big Data Analytics: Use of PSOAANN Based One-Class Classification},
year = {2016},
isbn = {9781450347563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2980258.2980319},
doi = {10.1145/2980258.2980319},
abstract = {Banking and financial industries are facing severe challenges in the form of fraudulent transactions. Credit card fraud is one example of them. In order to detect credit card fraud, we employed one-class classification approach in big data paradigm. We implemented a hybrid architecture of Particle Swarm Optimization and Auto-Associative Neural Network for one-class classification in Spark computational framework. In this paper, we implemented parallelization of the auto-associative neural network in the hybrid architecture.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics},
articleno = {33},
numpages = {8},
keywords = {Particle swarm optimization, One-class classification, Single class classification, Auto-associative neural network, Auto-encoder},
location = {Pondicherry, India},
series = {ICIA-16}
}

@proceedings{10.1145/2896825,
title = {BIGDSE '16: Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Big Data is about extracting valuable information from data in order to use it in intelligent ways such as to revolutionize decision-making in businesses, science and society. Big Data may open up radical new ways and unprecedented opportunities of attacking software engineering problems. Already now forums, forges, blogs, Q&amp;A sites, and social networks, provide a wealth of data that may be analysed to uncover new requirements, provide evidence on usage and development trends of application frameworks, or to perform empirical studies involving real-world software developers. In addition, real-time data collected from mobile and cloud applications may be analysed to detect user trends, preferences, and optimization opportunities.BIGDSE 2016 features contributions and discussions that explore opportunities that Big Data technology offers to software engineering, both in research and practice ("big data for software engineering"). BIGDSE also looks at the software engineering challenges imposed by building Big Data software systems ("software engineering for big data").},
location = {Austin, Texas}
}

@inproceedings{10.1145/3344948.3344987,
author = {D\'{\i}az-de-Arcaya, Josu and Mi\~{n}on, Ra\"{u}l and Torre-Bastida, Ana I.},
title = {Towards an Architecture for Big Data Analytics Leveraging Edge/Fog Paradigms},
year = {2019},
isbn = {9781450371421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344948.3344987},
doi = {10.1145/3344948.3344987},
abstract = {An industry transformation is being boosted by Big Data and Cloud technologies. We present a Big Data architecture, which expands the life cycle of data processing through the Edge, Fog and Cloud computing layers. The proposed architecture takes advantage of the strengths of each: the Cloud layer executes heavy analytical processes, the Fog is responsible for the ingestion and performing aggregations, and the Edge manages devices and actuators. The proposed architecture tackles two main goals, 1) latencies and response times can be reduced by bringing the analytics closer to where the data is generated and 2) the use of computing resources is optimised. In order to conceptualise this architecture, an orchestration module is proposed with the goal of optimising the deployment of analytical workloads across the three layers, by evaluating their computing resources. In addition to this, another module is designed to monitor the performance of such workloads allowing the redistribution of tasks assigned to each node. These modules will be implemented in a real case scenario in the train domain.},
booktitle = {Proceedings of the 13th European Conference on Software Architecture - Volume 2},
pages = {173–176},
numpages = {4},
keywords = {IoT, fog computing, predictive maintenance, cloud computing, data analytics, big data},
location = {Paris, France},
series = {ECSA '19}
}

@inproceedings{10.1145/3341069.3341079,
author = {Weigang, Gao and Guiming, Chen and Xiang, Zheng},
title = {Analysis on the Application of Big Data in Military Equipment Expense Management},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3341079},
doi = {10.1145/3341069.3341079},
abstract = {With the development of computer and information technology, the era of big data has arrived, and entered the military field comprehensively. To improve the ability to develop and use big data has become a strategic project that affecting the victory of war. This paper takes the elaboration of the relevant connotation of big data as the traction, analyzes the urgency of the application of big data to the management of military equipment costs, points out the key problems of big data in the management of military equipment costs and proposes corresponding solutions.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {177–180},
numpages = {4},
keywords = {equipment, cost, big data},
location = {Guangzhou, China},
series = {HPCCT '19}
}

@inproceedings{10.1145/3148453.3306250,
author = {Wang, Pengpeng and Liu, Hongri and Wang, Bailing and Dong, Kaikun and Wang, Lianhai and Xu, Shujiang},
title = {Simulation of Dark Network Scene Based on the Big Data Environment},
year = {2018},
isbn = {9781450363525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148453.3306250},
doi = {10.1145/3148453.3306250},
abstract = {In the era of big data, the amount of information on dark network resources has exploded. Massive dark network data contain abundant information. To detect dark network resources and obtain dark network information, in-depth understanding of the dark network is a prerequisite. However, due to the high anonymity of dark network, it is usually difficult to be found by traditional search engines. Users need to register strictly and use specific tools to log in dynamically. In this paper, we explore the simulation of dark network scene in the big data environment. The Tor network is built on the openstack platform, which simulate the dark network scene. By using wireshark software to analyze network traffic, and using nmon tool to analyze network performance, the results show that the dark network scene can be simulated realistically.},
booktitle = {Proceedings of the International Conference on Information Technology and Electrical Engineering 2018},
articleno = {11},
numpages = {6},
keywords = {Dark network, Scene simulation, Big data, Tor},
location = {Xiamen, Fujian, China},
series = {ICITEE '18}
}

@proceedings{10.5555/2819289,
title = {BIGDSE '15: Proceedings of the First International Workshop on BIG Data Software Engineering},
year = {2015},
publisher = {IEEE Press},
abstract = {Big Data is about extracting valuable information from data in order to use it in intelligent ways such as to revolutionize decision-making in businesses, science and society. The continuous and tremendous growth of data volume and velocity combined with easier access to data and the availability of powerful IT systems have led to intensified activities around Big Data.BIGDSE 2015 aims to explore opportunities that Big Data technology offers to software engineering, both in research and practice. In addition, BIGDSE will look at the software engineering challenges imposed by building Big Data software systems. The workshop brings together researchers and practitioners working in the areas of Big Data, software engineering and software analytics to discuss research challenges, recent developments, novel applications and scenarios, as well as methods, techniques, experiences, and tools to leverage and exploit the opportunities offered by Big Data.},
location = {Florence, Italy}
}

@inproceedings{10.5555/2591305.2591323,
author = {Birke, Robert and Bj\"{o}rkqvist, Mathias and Chen, Lydia Y. and Smirni, Evgenia and Engbersen, Ton},
title = {(Big)Data in a Virtualized World: Volume, Velocity, and Variety in Cloud Datacenters},
year = {2014},
isbn = {9781931971089},
publisher = {USENIX Association},
address = {USA},
abstract = {Virtualization is the ubiquitous way to provide computation and storage services to datacenter end-users. Guaranteeing sufficient data storage and efficient data access is central to all datacenter operations, yet little is known of the effects of virtualization on storage workloads. In this study, we collect and analyze field data from production datacenters that operate within the private cloud paradigm, during a period of three years. The datacenters of our study consist of 8,000 physical boxes, hosting over 90,000 VMs, which in turn use over 22 PB of storage. Storage data is analyzed from the perspectives of volume, velocity, and variety of storage demands on virtual machines and of their dependency on other resources. In addition to the growth rate and churn rate of allocated and used storage volume, the trace data illustrates the impact of virtualization and consolidation on the velocity of IO reads and writes, including IO deduplication ratios and peak load analysis of co-located VMs. We focus on a variety of applications which are roughly classified as app, web, database, file, mail, and print, and correlate their storage and IO demands with CPU, memory, and network usage. This study provides critical storage workload characterization by showing usage trends and how application types create storage traffic in large datacenters.},
booktitle = {Proceedings of the 12th USENIX Conference on File and Storage Technologies},
pages = {177–189},
numpages = {13},
location = {Santa Clara, CA},
series = {FAST'14}
}

@inproceedings{10.1145/3123024.3124439,
author = {Ferreira, Eija and Ferreira, Denzil},
title = {Towards Altruistic Data Quality Assessment for Mobile Sensing},
year = {2017},
isbn = {9781450351904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123024.3124439},
doi = {10.1145/3123024.3124439},
abstract = {High-quality data is a necessity for successful research and development endeavors. In this article, we review relevant literature for data quality (DQ) assessment methods in different domains and discuss the possibilities, challenges and constraints of applying them to mobile sensing. We identify DQ dimensions directly applicable to sensor data: believability (comparison with the correct operating bounds), completeness (missing values), free-of-error (erroneous values), consistency (over time), timeliness (delay), accuracy (deviation from true value) and precision (granularity of readings) are core aspects of high-quality sensor data. We also emphasize that sensor data must be representative of the originating type of sensor. We propose an altruistic approach to DQ assessment for sensor data that facilitates aggregating and sharing of domain knowledge through a community-contributed library of DQ assessment methods organized by sensor type.},
booktitle = {Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers},
pages = {464–469},
numpages = {6},
keywords = {library of domain knowledge, sensors, metrics, mobile instrumentation, data quality, dimensions},
location = {Maui, Hawaii},
series = {UbiComp '17}
}

@inproceedings{10.1145/3269206.3274270,
author = {Bereta, Konstantina and Koubarakis, Manolis and Manegold, Stefan and Stamoulis, George and Demir, Beg\"{u}m},
title = {From Big Data to Big Information and Big Knowledge: The Case of Earth Observation Data},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3274270},
doi = {10.1145/3269206.3274270},
abstract = {Some particularly important rich sources of open and free big geospatial data are the Earth observation (EO) programs of various countries such as the Landsat program of the US and the Copernicus programme of the European Union. EO data is a paradigmatic case of big data and the same is true for the big information and big knowledge extracted from it. EO data (satellite images and in-situ data), and the information and knowledge extracted from it, can be utilized in many applications with financial and environmental impact in areas such as emergency management, climate change, agriculture and security.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {2293–2294},
numpages = {2},
keywords = {earth observation data, copernicus program, linked geospatial data, semantic web},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1145/2990473,
author = {Wang, Kun and Mi, Jun and Xu, Chenhan and Zhu, Qingquan and Shu, Lei and Deng, Der-Jiunn},
title = {Real-Time Load Reduction in Multimedia Big Data for Mobile Internet},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2990473},
doi = {10.1145/2990473},
abstract = {In the age of multimedia big data, the popularity of mobile devices has been in an unprecedented growth, the speed of data increasing is faster than ever before, and Internet traffic is rapidly increasing, not only in volume but also in heterogeneity. Therefore, data processing and network overload have become two urgent problems. To address these problems, extensive papers have been published on image analysis using deep learning, but only a few works have exploited this approach for video analysis. In this article, a hybrid-stream model is proposed to solve these problems for video analysis. Functionality of this model covers Data Preprocessing, Data Classification, and Data-Load-Reduction Processing. Specifically, an improved Convolutional Neural Networks (CNN) classification algorithm is designed to evaluate the importance of each video frame and video clip to enhance classification precision. Then, a reliable keyframe extraction mechanism will recognize the importance of each frame or clip, and decide whether to abandon it automatically by a series of correlation operations. The model will reduce data load to a dynamic threshold changed by σ, control the input size of the video in mobile Internet, and thus reduce network overload. Through experimental simulations, we find that the size of processed video has been effectively reduced and the quality of experience (QoE) has not been lowered due to a suitably selected parameter η. The simulation also shows that the model has a steady performance and is powerful enough for continuously growing multimedia big data.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {oct},
articleno = {76},
numpages = {20},
keywords = {networking, big data, mobile Internet, Multimedia, load reduction, real-time}
}

@inproceedings{10.1145/3006299.3006310,
author = {Glauner, Patrick and Meira, Jorge Augusto and Dolberg, Lautaro and State, Radu and Bettinger, Franck and Rangoni, Yves},
title = {Neighborhood Features Help Detecting Non-Technical Losses in Big Data Sets},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006310},
doi = {10.1145/3006299.3006310},
abstract = {Electricity theft occurs around the world in both developed and developing countries and may range up to 40% of the total electricity distributed. More generally, electricity theft belongs to non-technical losses (NTL), which occur during the distribution of electricity in power grids. In this paper, we build features from the neighborhood of customers. We first split the area in which the customers are located into grids of different sizes. For each grid cell we then compute the proportion of inspected customers and the proportion of NTL found among the inspected customers. We then analyze the distributions of features generated and show why they are useful to predict NTL. In addition, we compute features from the consumption time series of customers. We also use master data features of customers, such as their customer class and voltage of their connection. We compute these features for a Big Data base of 31M meter readings, 700K customers and 400K inspection results. We then use these features to train four machine learning algorithms that are particularly suitable for Big Data sets because of their parallelizable structure: logistic regression, k-nearest neighbors, linear support vector machine and random forest. Using the neighborhood features instead of only analyzing the time series has resulted in appreciable results for Big Data sets for varying NTL proportions of 1%-90%. This work can therefore be deployed to a wide range of different regions.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {253–261},
numpages = {9},
keywords = {feature selection, electricity theft detection, machine learning, time series classification, feature engineering, data mining, non-technical losses},
location = {Shanghai, China},
series = {BDCAT '16}
}

@inproceedings{10.1145/3318464.3389770,
author = {Yang, Zongheng and Chandramouli, Badrish and Wang, Chi and Gehrke, Johannes and Li, Yinan and Minhas, Umar Farooq and Larson, Per-\r{A}ke and Kossmann, Donald and Acharya, Rajeev},
title = {Qd-Tree: Learning Data Layouts for Big Data Analytics},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389770},
doi = {10.1145/3318464.3389770},
abstract = {Corporations today collect data at an unprecedented and accelerating scale, making the need to run queries on large datasets increasingly important. Technologies such as columnar block-based data organization and compression have become standard practice in most commercial database systems. However, the problem of best assigning records to data blocks on storage is still open. For example, today's systems usually partition data by arrival time into row groups, or range/hash partition the data based on selected fields. For a given workload, however, such techniques are unable to optimize for the important metric of the number of blocks accessed by a query. This metric directly relates to the I/O cost, and therefore performance, of most analytical queries. Further, they are unable to exploit additional available storage to drive this metric down further. In this paper, we propose a new framework called a query-data routing tree, or qd-tree, to address this problem, and propose two algorithms for their construction based on greedy and deep reinforcement learning techniques. Experiments over benchmark and real workloads show that a qd-tree can provide physical speedups of more than an order of magnitude compared to current blocking schemes, and can reach within 2X of the lower bound for data skipping based on selectivity, while providing complete semantic descriptions of created blocks.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {193–208},
numpages = {16},
keywords = {big data, deep reinforcement learning, query processing, data layout, deep learning, OLAP, data analytics, storage, indexing, data partitioning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/2287076.2287078,
author = {Budiu, Mihai},
title = {Putting a "Big-Data" Platform to Good Use: Training Kinect},
year = {2012},
isbn = {9781450308052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2287076.2287078},
doi = {10.1145/2287076.2287078},
booktitle = {Proceedings of the 21st International Symposium on High-Performance Parallel and Distributed Computing},
pages = {1–2},
numpages = {2},
keywords = {big data, linq, dryadlinq, parallel computing, kinect},
location = {Delft, The Netherlands},
series = {HPDC '12}
}

@inproceedings{10.1145/3057148.3057149,
author = {Mesbah, Sepideh and Bozzon, Alessandro and Lofi, Christoph and Houben, Geert-Jan},
title = {Describing Data Processing Pipelines in Scientific Publications for Big Data Injection},
year = {2017},
isbn = {9781450352406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3057148.3057149},
doi = {10.1145/3057148.3057149},
abstract = {The rise of Big Data analytics has been a disruptive game changer for many application domains, allowing the integration into domain-specific applications and systems of insights and knowledge extracted from external big data sets. The effective "injection" of external Big Data demands an understanding of the properties of available data sets, and expertise on the available and most suitable methods for data collection, enrichment and analysis. A prominent knowledge source is scientific literature, where data processing pipelines are described, discussed, and evaluated. Such knowledge is however not readily accessible, due to its distributed and unstructured nature. In this paper, we propose a novel ontology aimed at modeling properties of data processing pipelines, and their related artifacts, as described in scientific publications. The ontology is the result of a requirement analysis that involved experts from both academia and industry. We showcase the effectiveness of our ontology by manually applying it to a collection of publications describing data processing methods.},
booktitle = {Proceedings of the 1st Workshop on Scholarly Web Mining},
pages = {1–8},
numpages = {8},
location = {Cambridge, United Kingdom},
series = {SWM '17}
}

@inproceedings{10.1145/3107514.3107518,
author = {Wu, Jinrong and Sinnott, Richard O. and Effendy, Jemie and Gl\"{o}ckner, Stephan and Hu, William and Li, Jiajie},
title = {Usage Patterns and Data Quality: A Case Study of a National Type-1 Diabetes Study},
year = {2017},
isbn = {9781450352246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107514.3107518},
doi = {10.1145/3107514.3107518},
abstract = {The Environmental Determinants of Islet Auto- immunity (ENDIA) project is Australia's largest study into the causes of Type-1 Diabetes (T1D). The ENDIA study is supported by a Cloud-based software platform including a clinical registry comprising extensive longitudinal information on families at risk of having a child that might go on to develop T1D. This registry includes both demographic and clinical information on families and children as well as the environmental factors that might influence the onset of T1D. A multitude of samples are obtained through the study and used to support a diverse portfolio of bioinformatics data analytics. The quality of the data in the registry is essential to the overall success of the project. This paper presents a Cloud-based log-analytics platform that supports the detailed analysis of patterns of usage of the registry by the clinical centres and collaborators involved. We explore the impact that the usage patterns have on the overall data quality. We also consider ways of improving data quality by mothers entering their own data through targeted mobile applications that have been developed for dietary data collection.},
booktitle = {Proceedings of the 1st International Conference on Medical and Health Informatics 2017},
pages = {18–27},
numpages = {10},
keywords = {Type-1 diabetes, auditing, Cloud, log analysis},
location = {Taichung City, Taiwan},
series = {ICMHI '17}
}

@inproceedings{10.1145/3284103.3284123,
author = {Li, Tao},
title = {Using Big Data Analytics to Build Prosperity Index of Transportation Market},
year = {2018},
isbn = {9781450360449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284103.3284123},
doi = {10.1145/3284103.3284123},
abstract = {As the transportation services represented by DiDi have entered the mobile Internet, the data volume of transportation services on various network platforms and social media has increased dramatically, which indicates that the era of big data of transportation system has come. This paper proposes a transportation market prosperity index system based on big data analytics. First, the construction principle of the prosperity index is established; second, the quintessential prosperity indexes are selected; third, we formulate the index data acquisition method, data processing and index calculation method; fourth, the calculation of transportation diffusion index and traffic composite index is developed. The contribution of this paper is that we can grasp the degree of economic development of the transportation market from both qualitative and quantitative perspectives and show its development trend based on the mining and analysis of big data on the Internet. Then, we can provide foundation for the government to formulate relative policies and decision-making for relevant enterprises.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience},
articleno = {17},
numpages = {6},
keywords = {Prosperity index, Transportation market, Big data analytics},
location = {Seattle, WA, USA},
series = {Safety and Resilience'18}
}

@inproceedings{10.1145/3320326.3320356,
author = {Daki, Houda and El Hannani, Asmaa and Ouahmane, Hassan},
title = {Big-Data Architecture for Electrical Consumption Forecasting in Educational Institutions Buildings},
year = {2019},
isbn = {9781450366458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320326.3320356},
doi = {10.1145/3320326.3320356},
abstract = {Recently, educational institutions suffer from high electrical consumption due to their new practices and activities. One of the promising solutions to overcome this challenge is to improve their energy management strategies using smart grids which ensure efficiency, reliability and energy saving. For this same reason, the National School of Applied Sciences of El Jadida -- Morocco has decided to install a private smart grid based on photovoltaic panels that will cover 40% of its electricity needs. But the problem that arises when using this new approach is the high level of complexity in term of data management due to the variety, veracity and the volume of the data. So, to meet these needs the use of Big Data technologies is required. In this paper, we propose a Big Data solution based on Lambda architecture to handle electrical consumption data in the National School of Applied Sciences of El Jadida -- Morocco. This system collects all parameters that might influence electrical consumption with Kafka, then it applies Spark libraries to analyze it. The solution allows also electrical energy forecasting using Spark machine learning library and the data persistence using HBase storage system.},
booktitle = {Proceedings of the 2nd International Conference on Networking, Information Systems &amp; Security},
articleno = {24},
numpages = {6},
keywords = {Lambda architecture, Spark, HBase storage system, Kafka, Big Data, Machine learning, Smart grid, Electrical forecasting},
location = {Rabat, Morocco},
series = {NISS19}
}

@inproceedings{10.1145/3252678,
author = {Mishne, Gilad},
title = {Session Details: SIRIP I: Big Companies, Big Data},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3252678},
doi = {10.1145/3252678},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{10.1145/3357384.3358124,
author = {Salloum, Salman and Wu, Yinxu and Huang, Joshua Zhexue},
title = {A Sampling-Based System for Approximate Big Data Analysis on Computing Clusters},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358124},
doi = {10.1145/3357384.3358124},
abstract = {To break the in-memory bottleneck and facilitate online sampling in cluster computing frameworks, we propose a new sampling-based system for approximate big data analysis on computing clusters. We address both computational and statistical aspects of big data across the main layers of cluster computing frameworks: big data storage, big data management, big data online sampling, big data processing, and big data exploration and analysis. We use the new Random Sample Partition (RSP) distributed data model to store a big data set as a set of ready-to-use random sample data blocks in Hadoop Distributed File System (HDFS), called RSP blocks. With this system, only a few RSP blocks are selected and processed using a sequential algorithm in a distributed data-parallel manner to produce approximate results for the entire data set. In this paper, we present a prototype RSP-based system and demonstrate its advantages. Our experiments show that RSP blocks can be used to get approximate models and summary statistics as well as estimate the proportions of inconsistent values without computing the entire data or running expensive online sampling operations. This new system enables big data exploration and analysis where the entire data set cannot be computed.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2481–2484},
numpages = {4},
keywords = {random sample partition, cluster computing, approximate computing, block-level sampling, big data},
location = {Beijing, China},
series = {CIKM '19}
}

@article{10.1145/2989214,
author = {Dong, Mianxiong and Piuri, Vincenzo and Chan, Shueng-Han Gary and Jain, Ramesh},
title = {Introduction to Special Issue on Multimedia Big Data: Networking},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2989214},
doi = {10.1145/2989214},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {sep},
articleno = {70},
numpages = {3}
}

@inproceedings{10.1145/3206157.3206160,
author = {Alshammari, Sultanah M. and Mikler, Armin M.},
title = {Big Data Opportunities for Disease Outbreaks Detection in Global Mass Gatherings},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206160},
doi = {10.1145/3206157.3206160},
abstract = {The different mass gatherings occurring all over the world, such as sports and religious events, pose public health concerns due to the increased risk of transmitting infectious diseases in these settings. When these events are concluded, the travel patterns of the returning international participants could further contribute to a rapid spread of infectious diseases causing global epidemics. The need to establish real-time disease outbreak surveillance at global mass gatherings motivates new technologies and advanced computational methods. The rapid expansion of digital devices and access to internet applications among participants in these gatherings generate a massive amount of data. Once being collected and processed, these data along with other health-related data can make significant contributions to improve disease surveillance systems at global mass gatherings. In this paper, we present an overview of the main existing approaches for monitoring outbreaks of infectious diseases in these events and illustrate the perspectives and opportunities of Big Data in these application areas.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {16–21},
numpages = {6},
keywords = {Computational Modeling, Disease Surveillance, Wireless Sensors, Internet Data, Mass Gatherings, Big Data, Outbreak, Syndromic Surveillance System, Epidemic, Infectious Disease},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/3268891.3268892,
author = {Liu, Zhao-ge and Li, Xiang-yang},
title = {Full View Scenario Model of Big Data Governance in Community Safety Service},
year = {2018},
isbn = {9781450365024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268891.3268892},
doi = {10.1145/3268891.3268892},
abstract = {In community safety service, big data governance is the prime mode to achieve community safety big data sharing and service value increasing. Although existing researches have preliminarily established the general big data governance framework, identification and governance of specific sharing problems lack comprehensive and systematic scenario description. Applying software engineering method, this paper proposes a kind of scenario expression model of big data governance in community safety service. Considering the common features of big data governance scenarios, construct the meta-scenario model of big data governance in community safety services. Considering the scenario expression difference under different levels, scales and particle sizes, construct the full view scenario model of big data governance in community safety services by meta-models nesting to complete the scenario expression under different applying situation. Finally, a use case is proposed to verify the rationality and effectiveness of the scenario expression models.},
booktitle = {Proceedings of the 8th International Conference on Information Communication and Management},
pages = {44–49},
numpages = {6},
keywords = {safety service, big data governance, scenario model, community safety},
location = {Edinburgh, United Kingdom},
series = {ICICM '18}
}

@inproceedings{10.1145/3219788.3219809,
author = {Ren, Da Qi and Xia, Bing},
title = {File System Performance Tuning for Standard Big Data Benchmarks},
year = {2018},
isbn = {9781450363938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219788.3219809},
doi = {10.1145/3219788.3219809},
abstract = {Modern file system manages super large data sets to perform data intensive and cost-effective analytical processing. Performance of a file system relies on storages, software, workload characteristic and configurations. Complex techniques have to be used in analysis because the data are often hybrid mix of different formats and different structured datasets. Performance study helps to optimize these factors and improve the design of a file system to meet the requirements of a specific application. A promising approach is to allocate the diverse data of various applications on different file systems according to their individual properties, in order to support the best possible performance to every particular application. Some basis that simulate the characters and scenarios of each step of data execution procedures are addressed in this paper. Based on workload characteristic analysis, administrator can implement tuning methods in the large and high-dimensional configuration parameter settings provided by the platform accordingly. Preliminary results are provided by running standard benchmark TPCx-HS, TPCx-BB, TPC-H and HiBench K-means on Ext4 and Btrfs file systems, and the impactions of workload characteristics to the benchmark performance have been analysed.},
booktitle = {Proceedings of the 2018 International Conference on Computing and Data Engineering},
pages = {22–26},
numpages = {5},
keywords = {File system, Big data, Performance model, Benchmarks},
location = {Shanghai, China},
series = {ICCDE 2018}
}

@inproceedings{10.1145/3357419.3357441,
author = {Chaveesuk, Singha and Khalid, Bilal and Chaiyasoonthorn, Wornchanok},
title = {Emergence of New Business Environment with Big Data and Artificial Intelligence},
year = {2019},
isbn = {9781450371889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357419.3357441},
doi = {10.1145/3357419.3357441},
abstract = {The research paper aims to study the factors related to incorporation of the artificial intelligence and big data in the business environment leading to rapid transformation leading to businesses expansion while riding the market trends, taking over of the competitions and other dynamics of the business environment. The need of conducting this research evolved from the recent events which have put many data mining companies under fire with the uncovering of the Big-Data scandals, manipulating and directing consumers' mindsets among masses to affect the favorable impressions. Furthermore, it has been also observed that business tends to seek assistance through its professionals, corporate and social networks to have price parity to achieve its strategic business goals and objectives.},
booktitle = {Proceedings of the 9th International Conference on Information Communication and Management},
pages = {181–185},
numpages = {5},
keywords = {Big-Data, Business Environment, Anti-Free Market Competition, Collusion, Anti-Trust Laws, Artificial Intelligence},
location = {Prague, Czech Republic},
series = {ICICM 2019}
}

@inproceedings{10.1145/3361525.3361547,
author = {Birke, Robert and Rocha, Isabelly and Perez, Juan and Schiavoni, Valerio and Felber, Pascal and Chen, Lydia Y.},
title = {Differential Approximation and Sprinting for Multi-Priority Big Data Engines},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361547},
doi = {10.1145/3361525.3361547},
abstract = {Today's big data clusters based on the MapReduce paradigm are capable of executing analysis jobs with multiple priorities, providing differential latency guarantees. Traces from production systems show that the latency advantage of high-priority jobs comes at the cost of severe latency degradation of low-priority jobs as well as daunting resource waste caused by repetitive eviction and re-execution of low-priority jobs. We advocate a new resource management design that exploits the idea of differential approximation and sprinting. The unique combination of approximation and sprinting avoids the eviction of low-priority jobs and its consequent latency degradation and resource waste. To this end, we designed, implemented and evaluated DiAS, an extension of the Spark processing engine to support deflate jobs by dropping tasks and to sprint jobs. Our experiments on scenarios with two and three priority classes indicate that DiAS achieves up to 90% and 60% latency reduction for low- and high-priority jobs, respectively. DiAS not only eliminates resource waste but also (surprisingly) lowers energy consumption up to 30% at only a marginal accuracy loss for low-priority jobs.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {202–214},
numpages = {13},
keywords = {priorities, differential approximation, energy savings, sprinting, Spark},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1145/3419635.3419675,
author = {Liu, Yan},
title = {Research on Humanistic Value of Labor Practice Education under Big Data Technology},
year = {2020},
isbn = {9781450387729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419635.3419675},
doi = {10.1145/3419635.3419675},
abstract = {The goal of humanistic value of big data technology is to promote the development of individual freedom, inner balance, comprehensive education and cultural harmony. According to the present situation of big data technology, this paper makes a preliminary exploration on the way to realize the humanistic value in the big data labor practice education. In other words, the humanistic value of big data technology can be realized through three ways: persisting in putting people first, optimizing the technological environment of big data, and integrating the technological resources of big data. Through the above research on the humanistic value of big data technology, this paper applies the application of big data algorithm in labor practice to make it better serve human beings.},
booktitle = {Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education},
pages = {359–362},
numpages = {4},
keywords = {Labor practice, Big data technology, Humanistic value},
location = {Ottawa, ON, Canada},
series = {CIPAE 2020}
}

@proceedings{10.1145/3277104,
title = {ICCBD '18: Proceedings of the 2018 International Conference on Computing and Big Data},
year = {2018},
isbn = {9781450365406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {2018 International Conference on Computing and Big Data (ICCBD 2018) has been successfully held in College of Charleston, Charleston, South Carolina, USA during September 08-10, 2018. The objective of the conference is to provide a forum for the discussion of new developments, recent progress, and innovations in the Computing and Big Data.},
location = {Charleston, SC, USA}
}

@inproceedings{10.1145/3339186.3339207,
author = {Xia, Qiufen and Bai, Luyao and Liang, Weifa and Xu, Zichuan and Yao, Lin and Wang, Lei},
title = {QoS-Aware Proactive Data Replication for Big Data Analytics in Edge Clouds},
year = {2019},
isbn = {9781450371964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339186.3339207},
doi = {10.1145/3339186.3339207},
abstract = {We are in the era of big data and cloud computing, large quantity of computing resource is desperately needed to detect invaluable information hidden in the coarse big data through query evaluation. Users demand big data analytic services with various Quality of Service (QoS) requirements. However, cloud computing is facing new challenges in meeting stringent QoS requirements of users due to the remoteness from its users. Edge computing has emerged as a new paradigm to address such shortcomings by bringing cloud services to the edge of the operation network in proximity of users for performance improvement. To satisfy the QoS requirements of users for big data analytics in edge computing, the data replication and placement problem must be properly dealt with such that user requests can be efficiently and promptly responded. In this paper, we consider data replication and placement for big data analytic query evaluation. We first cast a novel proactive data replication and placement problem of big data analytics in a two-tier edge cloud environment, we then devise an approximation algorithm with an approximation ratio for it, we finally evaluate the proposed algorithm against existing benchmarks, using both simulation and experiment in a testbed based on real datasets, the evaluation results show that the proposed algorithm is promising.},
booktitle = {Proceedings of the 48th International Conference on Parallel Processing: Workshops},
articleno = {26},
numpages = {10},
keywords = {query evaluation, Data replication and placement, edge clouds, big data analytics},
location = {Kyoto, Japan},
series = {ICPP 2019}
}

@article{10.1145/3403951,
author = {Khader, Mariam and Al-Naymat, Ghazi},
title = {Density-Based Algorithms for Big Data Clustering Using MapReduce Framework: A Comprehensive Study},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3403951},
doi = {10.1145/3403951},
abstract = {Clustering is used to extract hidden patterns and similar groups from data. Therefore, clustering as a method of unsupervised learning is a crucial technique for big data analysis owing to the massive number of unlabeled objects involved. Density-based algorithms have attracted research interest, because they help to better understand complex patterns in spatial datasets that contain information about data related to co-located objects. Big data clustering is a challenging task, because the volume of data increases exponentially. However, clustering using MapReduce can help answer this challenge. In this context, density-based algorithms in MapReduce have been largely investigated in the past decade to eliminate the problem of big data clustering. Despite the diversity of the algorithms proposed, the field lacks a structured review of the available algorithms and techniques for desirable partitioning, local clustering, and merging. This study formalizes the problem of density-based clustering using MapReduce, proposes a taxonomy to categorize the proposed algorithms, and provides a systematic and comprehensive comparison of these algorithms according to the partitioning technique, type of local clustering, merging technique, and exactness of their implementations. Finally, the study highlights outstanding challenges and opportunities to contribute to the field of density-based clustering using MapReduce.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {93},
numpages = {38},
keywords = {clustering, density clustering, Big data, mapreduce framework}
}

@inproceedings{10.1145/3134271.3134296,
author = {Peng, Michael Yao-Ping and Tuan, Sheng-Hwa and Liu, Feng-Chi},
title = {Establishment of Business Intelligence and Big Data Analysis for Higher Education},
year = {2017},
isbn = {9781450352765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134271.3134296},
doi = {10.1145/3134271.3134296},
abstract = {The applications on business intelligence and big data analysis to extract useful information are getting more mature, but the development and operations in higher education institutions are still be lack. This study aims to explore how HEIs employ business intelligence to analysis and mining student learning and HEIs' operation data from database. The outcomes are benefit for universities to support the management of decision-making.},
booktitle = {Proceedings of the International Conference on Business and Information Management},
pages = {121–125},
numpages = {5},
keywords = {Database, Institutional Research, Big data, Business Intelligence},
location = {Bei Jing, China},
series = {ICBIM 2017}
}

@article{10.1145/291469.291471,
author = {Ballou, Donald P. and Tayi, Giri Kumar},
title = {Enhancing Data Quality in Data Warehouse Environments},
year = {1999},
issue_date = {Jan. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/291469.291471},
doi = {10.1145/291469.291471},
journal = {Commun. ACM},
month = {jan},
pages = {73–78},
numpages = {6}
}

@inproceedings{10.1145/3175628.3175647,
author = {Abbad, Hicham and Bouchaib, Radi},
title = {Towards a Big Data Analytics Framework for Smart Cities},
year = {2017},
isbn = {9781450352116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175628.3175647},
doi = {10.1145/3175628.3175647},
abstract = {Due to the emergence of the Internet of Things, social network and the mobile applications, the data produced is very important and diversified. These data are very useful to design a smart city based on machine learning and statistical algorithms used by real time analytics or predictive analytics. This paper presents a framework headlines that helps cities to take profit from data to improve the life quality of citizens in several areas: mobility and transportation, economy, environment and energy, public safety, health, education and others.This framework describe different data sources (IoT, software applications, sensors,..) used to generate the information. It also describes, the processing steps that make data usable, the different machine learning statistical algorithms and data mining techniques used to extract the insights. This framework, combine two complementary approaches: Techno-centric approach based on a city populated by sensors that collect a massive data to drive all the urban services, and the collaborative approach that addresses citizens more directly.},
booktitle = {Proceedings of the Mediterranean Symposium on Smart City Application},
articleno = {17},
numpages = {5},
keywords = {smart cities, internet of things (IoT), machine learning, analytics, big data},
location = {Tangier, Morocco},
series = {SCAMS '17}
}

@inproceedings{10.1145/3421537.3421542,
author = {Lam, Daphne Lai Fong},
title = {Study of IPhone's Big Data, Market Share, Usage and Their Relationships},
year = {2020},
isbn = {9781450375504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421537.3421542},
doi = {10.1145/3421537.3421542},
abstract = {In this paper, we will discuss different brands of iPhone of different market share, the relationships between iPhone's Big Data, market share [8] and their usage. We will have a look at their market share statistics, difference and comparison in graphs and tables mainly in HuaWei, Apple and Samsung [9]. Huawei is the first time to go ahead of Apple. We will also find that more and more people use iPhone nowadays because of its convenience. iPhone like our friends and assistant, we can use it anytime, anywhere and for many purposes. Research will be focused on how the iPhone changed the people's traditional behaviors such as sending emails, taking pictures, playing games, accessing the internet...etc, and inspire how the future iPhone can be developed by producers and used by people. Results and analysis of this study would be useful and could be extended for other kinds of data analysis, models, businesses used and commercial areas.},
booktitle = {Proceedings of the 2020 4th International Conference on Big Data and Internet of Things},
pages = {7–10},
numpages = {4},
keywords = {market share, Flow, Computer, iPhone, usage, Facebook, Big Data, eCommerce},
location = {Singapore, Singapore},
series = {BDIOT '20}
}

@inproceedings{10.1145/3377812.3390811,
author = {Khalajzadeh, Hourieh and Simmons, Andrew and Abdelrazek, Mohamed and Grundy, John and Hosking, John and He, Qiang and Ratnakanthan, Prasanna and Zia, Adil and Law, Meng},
title = {A Practical, Collaborative Approach for Modeling Big Data Analytics Application Requirements},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3390811},
doi = {10.1145/3377812.3390811},
abstract = {Data analytics application development introduces many challenges including: new roles not in traditional software engineering practices - e.g. data scientists and data engineers; use of sophisticated machine learning (ML) model-based approaches; uncertainty inherent in the models; interfacing with models to fulfill software functionalities; deploying models at scale and rapid evolution of business goals and data sources. We describe our Big Data Analytics Modeling Languages (BiDaML) toolset to bring all stakeholders around one tool to specify, model and document big data applications. We report on our experience applying BiDaML to three real-world large-scale applications. Our approach successfully supports complex data analytics application development in industrial settings.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {256–257},
numpages = {2},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3260511,
author = {Newton, Glen},
title = {Session Details: Session 3 - Big Data, Big Resources},
year = {2015},
isbn = {9781450335942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3260511},
doi = {10.1145/3260511},
booktitle = {Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries},
location = {Knoxville, Tennessee, USA},
series = {JCDL '15}
}

@inproceedings{10.1145/2670386.2670388,
author = {Goergen, David and Mendiratta, Veena and State, Radu and Engel, Thomas},
title = {Analysis of Large Call Data Records with Big Data},
year = {2014},
isbn = {9781450321242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2670386.2670388},
doi = {10.1145/2670386.2670388},
abstract = {Mobile communication flows describe the on-going traffic on the network and are therefore a good indication of what is happening. Analysing these flows can improve the overall quality offered to the users and it can enable operators to detect abnormal patterns and react.This paper will focus on the analysis of cellular communications records. By using the collected call and message exchanges we present a method based on the PageRank algorithm that detects abnormal communications events. Taking the number of calls and the total call duration as parameters we use a weighted version of the PageRank algorithm to further investigate the influence of these parameters on the connected network graph. We proceed by correlating the results obtained with events happening in the respective region and at that time.},
booktitle = {Proceedings of the Conference on Principles, Systems and Applications of IP Telecommunications},
articleno = {8},
numpages = {6},
keywords = {big data processing, call data record analysis, call pattern detection, page rank},
location = {Chicago, Illinois},
series = {IPTComm '14}
}

@article{10.1145/3210752,
author = {Orenga-Rogl\'{a}, Sergio and Chalmeta, Ricardo},
title = {Framework for Implementing a Big Data Ecosystem in Organizations},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3210752},
doi = {10.1145/3210752},
abstract = {Featuring the various dimensions of data management, it guides organizations through implementation fundamentals.},
journal = {Commun. ACM},
month = {dec},
pages = {58–65},
numpages = {8}
}

@inproceedings{10.1145/2910019.2910033,
author = {Netten, Niels and van den Braak, Susan and Choenni, Sunil and van Someren, Maarten},
title = {A Big Data Approach to Support Information Distribution in Crisis Response},
year = {2016},
isbn = {9781450336406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910019.2910033},
doi = {10.1145/2910019.2910033},
abstract = {Crisis response organizations operate in very dynamic environments, in which it is essential for responders to acquire all information critical to their task execution in time. In reality, the responders are often faced with information overload, incomplete information, or a combination of both. This hampers their decision-making process, workflow, situational awareness and, consequently, effective execution of collaborative crisis response. Therefore, getting the right information to the right person at the right time is of crucial importance.The task of processing all data during crisis response situations and determining for whom at a particular moment the information is relevant is not straightforward. When developing an information system to support this task, some important challenges have to be taken into account. These challenges relate to the structure and truthfulness of the used data, the assessment of information relevance, and the dissemination of relevant information in time. While methods and techniques from big data can be used to collect and integrate data, machine learning can be used to build a model for relevance assessments. An example implementation of such a framework of big data is the TAID software system that collects and integrates data communicated between first responders and may send information to crisis responders that were not addressed in the initial communication. As an example of the impact of TAID on crisis response, we show its effect in a simulated crisis response scenario.},
booktitle = {Proceedings of the 9th International Conference on Theory and Practice of Electronic Governance},
pages = {266–275},
numpages = {10},
keywords = {Relevance Assessments, Information Distribution, Crisis Response for Public Safety, Big Data, Machine Learning},
location = {Montevideo, Uruguay},
series = {ICEGOV '15-16}
}

@inproceedings{10.1145/3297156.3297249,
author = {Samoylov, Alexey and Sergeev, Nikolay and Kucherova, Margarita and Denisov, Boris},
title = {Methodology of Big Data Integration from A Priori Unknown Heterogeneous Data Sources},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297249},
doi = {10.1145/3297156.3297249},
abstract = {The success of data preparation for Big Data analytics directly depends on the quality of data integration from heterogeneous data sources. Extract, Transform and Load (ETL) systems have proved to be an efficient solution for this task. But to the moment, in the stages of data selection, definition of extraction rules and transformation, the decision is usually made exclusively by a data specialist. This, in turn, causes such problems as redundancy and inconsistency of imported data, narrow specialization of rules (up to uniqueness) with a limited number of analytical models and known requirements for the data mart. This paper presents the concept of solving the problem by providing methodological support for Big Data preparation procedure to efficiently collect data from a priory unknown heterogeneous data sources.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {131–135},
numpages = {5},
keywords = {semantics, modeling, knowledge extraction, heterogeneous data sources, data integration, Big Data, ETL},
location = {Shenzhen, China},
series = {CSAI '18}
}

@inproceedings{10.1145/3422713.3422738,
author = {Yu, Bai and Chuncheng, Wei and Dongping, Zhou and Yang, Yu},
title = {Design and Application of Digital Platform for Big Data Eco-System},
year = {2020},
isbn = {9781450387859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422713.3422738},
doi = {10.1145/3422713.3422738},
abstract = {Ecological assets are an integral part of natural resources assets and the foundation for human survival and development. To achieve the goal of a unified management of "mountains, rivers, forests, farmlands, lakes and grasslands", aiming at the current online value evaluation issue on the green development of ecological assets in urban areas. This paper probes into the various information and data resources from the ecological factors, completes a demand analysis of the basic platform of urban ecological factors, establishes a monitoring analysis system for urban green development ecological system, and constructs a green development ecological system digital monitoring platform with high digital integrity, strong analysis ability and accurate decision support. The construction of platform layer, cleaning, storage and operation modeling of data as well as data service, sharing and exchange of the platform, all help to solve the isolated data situation of various information systems, and lay a foundation for subsequent data mining value, and establishing a data-driven city. Besides, it also provides decision support for urban green development, and helps to further build a model green development city, and promote urban ecological civilization. Meanwhile, this Big Data platform could reflect real situation of ecological assets in a more scientific and rational way, and provide certain basis and reference for the off-office auditing of leading cadres in relevant posts of natural assets.},
booktitle = {Proceedings of the 2020 3rd International Conference on Big Data Technologies},
pages = {69–73},
numpages = {5},
keywords = {Design and Application, Big Data, Ecological Assets, Platform},
location = {Qingdao, China},
series = {ICBDT 2020}
}

