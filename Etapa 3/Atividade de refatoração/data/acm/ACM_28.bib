@inproceedings{10.1145/2808797.2808883,title = {Incorporating Big Data and Social Sensors in a Novel Early Warning System of Dengue Outbreaks}, author = {Lee Chung-Hong , Yang Hsin-Chang , Lin Shih-Jan },year = {2015}, isbn = {9781450338547}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2808797.2808883}, doi = {10.1145/2808797.2808883}, abstract = {In this work, an \"analytical data model of mosquito vector\" was developed to perform analytical computation to the character of the dengue vectors. Our goal is to investigate a way to understand how the temporal trend of collected dataset correlates with the incidence dengue as identified by national health authorities. Based upon the mosquito-vector big data collections, we investigate how changes in some specific variables such as rainfall, temperature, and humidity can dramatically affect the population of mosquito vectors, in order to provide early warnings of dengue outbreaks. Thus, our system will collectively collect online sensing data of the variables and store them in a database, in order to combine the historical big data as training datasets for analytical computation. Also, the developed model is able to merge the experimental datasets with current hot-topic information which is relevant to mosquito vectors obtained from data of social sensors (i.e. social messages). The experimental data show that our system is of great potentials in providing early warnings of dengue outbreaks.}, location = {Paris, France}, series = {ASONAM '15}, pages = {1428\u20131433}, numpages = {6}, keywords = {Big Data, Data Mining, Dengue Outbreaks, Machine Learning, Social Sensors}}
@inproceedings{10.1145/2513549,title = {Proceedings of the 2013 international workshop on Mining unstructured big data using natural language processing},year = {2013}, isbn = {9781450324151}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {It is our great pleasure to welcome you to the 2013 ACM International Workshop on Mining Unstructured Big Data using Natural Language Processing, which will be held at ACM International Conference on Information and Knowledge Management, CIKM 2013.Unstructured text data is heterogeneous and available in different formats, such as text document, scientific publication, web page, and customer comment. The availability of many big unstructured text datasets enables, while also challenges researchers to discover and explore valuable information/knowledge via different techniques.Mining semantics by using Natural Language Processing (NLP) methodologies is an important approach to uncover the \"latent knowledge/semantic\" of the unstructured text data. In the past decade, while a number of NLP based features already successfully used to enhance the performance of the text mining or information retrieval systems, we are also facing some challenges. For instance, most NLP algorithms' computational cost is high, and we can hardly employ them directly to large-scale text data for online systems.In this workshop, we aggregate different but highly related research communities, i.e., \"NLP\", \"Text Mining\" and \"IR\" researchers, to investigate the possible opportunities and challenges in semantic mining problem. Nine very interesting papers, covering semantic analysis, social media mining, real-time information extraction, and etc., will be presented in this workshop. For this workshop, an opportunity is offered to both NLP and text mining research communities to better clarify the opportunities and challenges in NLP based semantic mining for big unstructured text data with their research experience.We also encourage attendees to attend the keynote presentation - \"HathiTrust Data, Opportunities and Challenges for Text Mining and NLP\" by Dr. Beth A. Plale, Director of Data to Insight Center, and Professor at School of Informatics and Computing, Indiana University. HathiTrust is a partnership of academic & research institutions, offering a collection of millions of digitized from libraries around the world plus effective API access.We hope that you will find this program interesting and thought-provoking and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.}, location = {San Francisco, California, USA}}
@inproceedings{10.1145/2500489,title = {Addressing Big Data Time Series: Mining Trillions of Time Series Subsequences Under Dynamic Time Warping}, author = {Rakthanmanon Thanawin , Campana Bilson , Mueen Abdullah , Batista Gustavo , Westover Brandon , Zhu Qiang , Zakaria Jesin , Keogh Eamonn },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2500489}, doi = {10.1145/2500489}, abstract = {Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms, including classification, clustering, motif discovery, anomaly detection, and so on. The difficulty of scaling a search to large datasets explains to a great extent why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine massive time series for the first time. We demonstrate the following unintuitive fact: in large datasets we can exactly search under Dynamic Time Warping (DTW) much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. In particular, the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining papers ever published. We explain how our ideas allow us to solve higher-level time series data mining problems such as motif discovery and clustering at scales that would otherwise be untenable. Moreover, we show how our ideas allow us to efficiently support the uniform scaling distance measure, a measure whose utility seems to be underappreciated, but which we demonstrate here. In addition to mining massive datasets with up to one trillion datapoints, we will show that our ideas also have implications for real-time monitoring of data streams, allowing us to handle much faster arrival rates and/or use cheaper and lower powered devices than are currently possible.}, pages = {1\u201331}, numpages = {31}, keywords = {lower bounds, Time series, similarity search}}
@inproceedings{10.1145/3098593,title = {Proceedings of the Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},year = {2017}, isbn = {9781450350549}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Los Angeles, CA, USA}}
@inproceedings{10.1145/2818869.2818935,title = {An incremental learning technique for detecting driving behaviors using collected EV big data}, author = {Lee Chung-Hong , Wu Chih-Hung },year = {2015}, isbn = {9781450337359}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2818869.2818935}, doi = {10.1145/2818869.2818935}, abstract = {This paper presents an expansible machine learning approach applying the EV big data as the human sensor to extract driving behaviors and driving modes. A pattern recognition approach is proposed to model the driving pattern according to the energy consumption of an EV. The growing hierarchical self-organizing maps (GHSOM) is applied to learn driver's behaviors gradually in the offline process, and the clustered neurons are used as the training sets for implementing online classifiers based on support vector machine (SVM). This proposed framework would facilitate the understanding of driver's behaviors and help drivers overcome range anxiety.}, location = {Kaohsiung, Taiwan}, series = {ASE BD&amp;SI '15}, pages = {1\u20135}, numpages = {5}, keywords = {Machine learning, Driving pattern recognition, EV big data, Range anxiety, Electric vehicle, Driving behavior}}
@inproceedings{10.1145/3363459,title = {Proceedings of the 1st ACM International Workshop on Urban Building Energy Sensing, Controls, Big Data Analysis, and Visualization},year = {2019}, isbn = {9781450370141}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The advancements and availability of low-cost, low-energy sensors have improved energy and environmental sensing exponentially. Besides the millions of sensors used for monitoring, the improved accuracy of these sensors offer greater resolution for modeling 'what-if' scenarios in near real-time harnessing the vast computational power. Similarly, big data analysis has enabled city-scale modeling of energy and environmental impact using, among others, energy-efficient 'smaller' machine learning algorithms and/or physics-based modeling approaches. Coupled with interactive data visualization including Virtual Reality (VR), urban-scale energy and environmental systems modeling has become an exciting niche at the intersection of computer science and urban / architecture / mechanical engineering disciplines. The 1st International Urban Building Energy Sensing, Controls, Big Data Analysis, and Visualization (UrbSys) Workshop intends to capture recent exciting work by research experts, from U.S. universities and U.S. national laboratories, at this nexus that supports sustainable urban systems' design and engineering through state-of-the-art sensing, controls, modeling, and visualization.}, location = {New York, NY, USA}}
@inproceedings{10.1145/2938503.2938517,title = {An Innovative Framework for Effectively and Efficiently Supporting Big Data Analytics over Geo-Located Mobile Social Media}, author = {Cuzzocrea Alfredo , Psaila Giuseppe , Toccu Maurizio },year = {2016}, isbn = {9781450341189}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2938503.2938517}, doi = {10.1145/2938503.2938517}, abstract = {Mobile Social Media are gaining momentum in the broader context of Big Data Analytics, where the main issue is represented by the problem of extracting interesting and actionable knowledge from big data repositories. Mobile social media sources like Twitter and Instagram are indeed producing massive amounts of data (namely, posts) that represent a very rich source of knowledge for predictive analytics. In line with this emerging trend, this paper proposes an innovative approach for effectively and efficiently supporting big data analytics over geo-localized mobile social media, with particular emphasis with the context of modern tourist information systems. In this context, the innovative FollowMe suite, which implements the proposed methodology, is also described in details. We complement our analytical contribution with a real-life case study focusing on the EXPO 2015 event in Milan, Italy which clearly shows benefits and potentialities of our proposed big data analytics framework.}, location = {Montreal, QC, Canada}, series = {IDEAS '16}, pages = {62\u201369}, numpages = {8}, keywords = {Mobile Social Media, Big Data Frameworks, Big Data Analytics}}
@inproceedings{10.1145/3306500.3306552,title = {Big data for loyalty program management in hypermarket}, author = {Lee Kuan-Yin , Hsu Yin-Chiech },year = {2019}, isbn = {9781450366021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3306500.3306552}, doi = {10.1145/3306500.3306552}, abstract = {Loyalty programs lead to a natural split of a firm's customer base into members and nonmembers. To manage both groups effectively, it is essential to know how they concern about, such as services or promotions. This article, set in context of the second hypermarket density in Asia, examines the impact of satisfaction on store patronage and explores moderator roles of employee interaction and price sensitivity between members and nonmembers. Therefore, a survey was performed among 317 hypermarket members and nonmembers from top three settings in Taiwan. The study demonstrates that the satisfaction and store patronage behavior relationship of members stronger than nonmembers. And moderator of employee interaction and price sensitivity of members has stronger effect between satisfaction and store patronage than nonmembers. According to inconsistent relation between satisfaction and store patronage in past studies, the study extend existing theories of retention to incorporate contingency relationships, especially among members and nonmembers to manage retailer-both customer relationship better.}, location = {Tokyo, Japan}, series = {IC4E '19}, pages = {363\u2013367}, numpages = {5}, keywords = {loyalty program, store patronage, shopping characteristics, satisfaction}}
@inproceedings{10.1145/2882903.2915229,title = {Big Data Analytics with Datalog Queries on Spark}, author = {Shkapsky Alexander , Yang Mohan , Interlandi Matteo , Chiu Hsuan , Condie Tyson , Zaniolo Carlo },year = {2016}, isbn = {9781450335317}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2882903.2915229}, doi = {10.1145/2882903.2915229}, abstract = {There is great interest in exploiting the opportunity provided by cloud computing platforms for large-scale analytics. Among these platforms, Apache Spark is growing in popularity for machine learning and graph analytics. Developing efficient complex analytics in Spark requires deep understanding of both the algorithm at hand and the Spark API or subsystem APIs (e.g., Spark SQL, GraphX). Our BigDatalog system addresses the problem by providing concise declarative specification of complex queries amenable to efficient evaluation. Towards this goal, we propose compilation and optimization techniques that tackle the important problem of efficiently supporting recursion in Spark. We perform an experimental comparison with other state-of-the-art large-scale Datalog systems and verify the efficacy of our techniques and effectiveness of Spark in supporting Datalog-based analytics.}, location = {San Francisco, California, USA}, series = {SIGMOD '16}, pages = {1135\u20131149}, numpages = {15}, keywords = {spark, recursive queries, monotonic aggregates, datalog}}
@inproceedings{10.1145/3482632.3487482,title = {Application of big data technology and clustering algorithm in business activities}, author = {Luan Shaohong , Wu Yunze },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3487482}, doi = {10.1145/3482632.3487482}, abstract = {As big data technology penetrates all aspects of our lives; more and more industries are beginning to apply big data technology. Its robust data analysis, resource acquisition, and predictive capabilities have brought tremendous business activities as a brand-new data management technology. This article analyzes the big data technology application in business activities and introduces the principles of two essential algorithms in big data technology, clustering algorithm, and principal component analysis.\u00a0Moreover, it introduces applying the mean shift clustering and principal component analysis in clustering algorithm to analyze the commercial data. Finally, a comprehensive analysis of the advantages and disadvantages of big data technology is made.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {2614\u20132618}, numpages = {5}}
@inproceedings{10.1145/3203217.3205863,title = {The D.A.V.I.D.E. big-data-powered fine-grain power and performance monitoring support}, author = {Bartolini Andrea , Borghesi Andrea , Libri Antonio , Beneventi Francesco , Gregori Daniele , Tinti Simone , Gianfreda Cosimo , Alto\u00e8 Piero },year = {2018}, isbn = {9781450357616}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3203217.3205863}, doi = {10.1145/3203217.3205863}, abstract = {On the race toward exascale supercomputing systems are facing important challenges which limit the efficiency of the system. Among all, power and energy consumption fueled by the end of Dennard's scaling start to show their impact on limiting supercomputers peak performance and cost effectiveness.In this paper we present and describe a new methodology based on a set of HW and SW extensions for fine-grain monitoring of power and aggregation of them for fast analysis and visualization. We propose a turn-key system which uses MQTT communication layer, NoSQL database, fine grain monitoring and in future AI technology to measure and control power and performance. This methodology is shown as an integrated feature of the D.A.V.I.D.E. supercomputing machine.}, location = {Ischia, Italy}, series = {CF '18}, pages = {303\u2013308}, numpages = {6}, keywords = {fine-grain power and performance monitoring, AMESTER, big data, high performance computing, beaglebone black}}
@inproceedings{10.1145/3396452.3396465,title = {Research on the Influencing Factors and Management Countermeasures of College Students' Sense of Security under the Environment of Big Data-an Empirical Analysis based on the Event of COVID-19}, author = {Xu Wei , Chen Chongyang },year = {2020}, isbn = {9781450374989}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3396452.3396465}, doi = {10.1145/3396452.3396465}, abstract = {Exposed in environment of big data, college students come easily into contact with massive data presentation. When crisis events occur, college students will be affected not only by crisis events but also by human psychological crisis. The greater psychological threat, faced by college students in the crisis environment, is the loss of sense of security. Through literature review, the hypotheses in crisis events are as followed: crisis events, government and media response, university coping measures, group coping behavior are the four main factors that affect college students' sense of security in crisis events. The outbreak of COVID-19 in Wuhan, Hubei affecting all the people, all colleges and universities across the country delayed the opening time. Among the affected universities, take the University of Electronic Science and Technology as an example, 600 samples were randomly selected to collect data. Through the exploratory factor analysis test, the influence hypotheses are verified. Through the structural equation model test, the four kinds of factors can explain the loss of college students' sense of security in the crisis, but show differences in explanatory power. Based on the elements of college students' sense of security, this paper puts forward an further explanation on the action path of the four factors on the public sense of security. According to the conclusion, we come to the conclusion that improving the coping ability of colleges and universities, enhancing the sense of crisis determination and the efficiency of control are the key to improve college students' sense of security and ensure the effectiveness of crisis management in colleges and universities.}, location = {London, United Kingdom}, series = {ICBDE '20}, pages = {21\u201325}, numpages = {5}, keywords = {big data, structural equation model, coping measures of colleges, dealing with emergency in groups, government and media's response, college students' sense of security, crisis events}}
@inproceedings{10.1145/2757384.2757390,title = {A Mobile Cloud Computing Middleware for Low Latency Offloading of Big Data}, author = {Yin Bo , Shen Wenlong , Cai Lin X. , Cheng Yu },year = {2015}, isbn = {9781450335249}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2757384.2757390}, doi = {10.1145/2757384.2757390}, abstract = {Recent years have witnessed an explosive growth of mobile applications. Thanks to improved network connectivity, it becomes a promising enabling solution to offload computation-intensive applications to the resource abundant public cloud to further augment the capacity of resource-constrained devices. As mobile applications usually have QoS requirements, it is critical to provide low latency services to the mobile users while maintain low leasing cost of cloud resources. However, the resources offered by cloud vendors are usually charged based on a time quanta while the offloading demand for heavy-lifting computation may occur infrequently on mobile devices. This mismatch would demotivate users to resort to public cloud for computation offloading. In this paper, we design a computation offloading middleware which bridges the aforementioned gap between cloud vendors and mobile clients, providing offloading service to multiple users with low cost and delay. The proposed middleware has two key components: Task Scheduler and Instance Manager. The Task Scheduler dispatches the received offloading tasks to execute in the instances reserved by the Instance Manager. Based on the arrival pattern of offloading tasks, the Instance Manager dynamically changes the number of instances to ensure certain service grade of mobile users. Our proposed mechanisms are validated through numerical results. It is shown that a lower average delay can be achieved through proposed scheduling heuristic, and the number of reserved instances well adapts to the offloading demands.}, location = {Hangzhou, China}, series = {Mobidata '15}, pages = {31\u201335}, numpages = {5}, keywords = {mobile cloud computing, computation offloading}}
@inproceedings{10.1145/3076113.3076115,title = {Big data causing big (TLB) problems: taming random memory accesses on the GPU}, author = {Karnagel Tomas , Ben-Nun Tal , Werner Matthias , Habich Dirk , Lehner Wolfgang },year = {2017}, isbn = {9781450350259}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3076113.3076115}, doi = {10.1145/3076113.3076115}, abstract = {GPUs are increasingly adopted for large-scale database processing, where data accesses represent the major part of the computation. If the data accesses are irregular, like hash table accesses or random sampling, the GPU performance can suffer. Especially when scaling such accesses beyond 2GB of data, a performance decrease of an order of magnitude is encountered. This paper analyzes the source of the slowdown through extensive micro-benchmarking, attributing the root cause to the Translation Lookaside Buffer (TLB). Using the micro-benchmarks, the TLB hierarchy and structure are fully analyzed on two different GPU architectures, identifying never-before-published TLB sizes that can be used for efficient large-scale application tuning. Based on the gained knowledge, we propose a TLB-conscious approach to mitigate the slowdown for algorithms with irregular memory access. The proposed approach is applied to two fundamental database operations - random sampling and hash-based grouping - showing that the slowdown can be dramatically reduced, and resulting in a performance increase of up to 13\u00d7.}, location = {Chicago, Illinois}, series = {DAMON '17}, pages = {1\u201310}, numpages = {10}, keywords = {virtual memory, grouping, TLB, GPU, random memory access}}
@inproceedings{10.1145/2766196.2766199,title = {High performance spatial queries for spatial big data: from medical imaging to GIS}, author = {Wang Fusheng , Aji Ablimit , Vo Hoang },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2766196.2766199}, doi = {10.1145/2766196.2766199}, abstract = {Support of high performance queries on large volumes of spatial data has become increasingly important in many application domains, including geospatial problems in numerous disciplines, location based services, and emerging medical imaging applications. There are two major challenges for managing massive spatial data to support spatial queries: the explosion of spatial data, and the high computational complexity of spatial queries. Our goal is to develop a general framework to support high performance spatial queries and analytics for spatial big data on MapReduce and CPU-GPU hybrid platforms. In this paper, we introduce Hadoop-GIS -- a scalable and high performance spatial data warehousing system for running large scale spatial queries on Hadoop. Hadoop-GIS supports multiple types of spatial queries on MapReduce through skew-aware spatial partitioning, on-demand indexing, customizable spatial query engine RESQUE, implicit parallel spatial query execution on MapReduce, and effective methods for amending query results through handling boundary objects. To accelerate compute-intensive geometric operations, GPU based geometric computation algorithms are integrated into MapReduce pipelines. Our experiments have demonstrated that Hadoop-GIS is highly efficient and scalable, and outperforms parallel spatial DBMS for compute-intensive spatial queries.}, pages = {11\u201318}, numpages = {8}}
@inproceedings{10.1145/3497749,title = {Locality Sensitive Hash Aggregated Nonlinear Neighborhood Matrix Factorization for Online Sparse Big Data Analysis}, author = {Li Zixuan , Li Hao , Li Kenli , Wu Fan , Chen Lydia , Li Keqin },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3497749}, doi = {10.1145/3497749}, abstract = {Matrix factorization (MF) can extract the low-rank features and integrate the information of the data manifold distribution from high-dimensional data, which can consider the nonlinear neighborhood information. Thus, MF has drawn wide attention for low-rank analysis of sparse big data, e.g., Collaborative Filtering (CF) Recommender Systems, Social Networks, and Quality of Service. However, the following two problems exist: (1) huge computational overhead for the construction of the Graph Similarity Matrix (GSM) and (2) huge memory overhead for the intermediate GSM. Therefore, GSM-based MF, e.g., kernel MF, graph regularized MF, and so on, cannot be directly applied to the low-rank analysis of sparse big data on cloud and edge platforms. To solve this intractable problem for sparse big data analysis, we propose Locality Sensitive Hashing (LSH) aggregated MF (LSH-MF), which can solve the following problems: (1) The proposed probabilistic projection strategy of LSH-MF can avoid the construction of the GSM. Furthermore, LSH-MF can satisfy the requirement for the accurate projection of sparse big data. (2) To run LSH-MF for fine-grained parallelization and online learning on GPUs, we also propose CULSH-MF, which works on CUDA parallelization. Experimental results show that CULSH-MF can not only reduce the computational time and memory overhead but also obtain higher accuracy. Compared with deep learning models, CULSH-MF can not only save training time but also achieve the same accuracy performance.}, pages = {1\u201327}, numpages = {27}, keywords = {CUDA parallelization on gpu and multiple GPUs, online learning for sparse big data, Locality Sensitive Hash (LSH), Graph Similarity Matrix (GSM), Top-K nearest neighboors., Matrix Factorization (MF)}}
@inproceedings{10.1145/3209415.3209427,title = {A framework for evidence based policy making combining big data, dynamic modelling and machine intelligence}, author = {Androutsopoulou Aggeliki , Charalabidis Yannis },year = {2018}, isbn = {9781450354219}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3209415.3209427}, doi = {10.1145/3209415.3209427}, abstract = {Governments and policy makers are striving to respond to contemporary socio-economic challenges, however, often neglecting the human factor and the multidimensionality of policy implications. In this chapter, a framework for evidence based policy making is proposed, which integrates the usage of open big data coming from a multiplicity of sources with policy simulations. It encompasses the application of dynamic modelling methodologies and data mining techniques to extract knowledge from two types of data. On the one hand, objective data such as governmental and statistical data, are used to capture the interlinked policy domains and their underlying casual mechanisms. On the other hand, behavioural patterns and citizens' opinions are extracted from Web 2.0 sources, social media posts, polls and statistical surveys. To combine this multimodal information, our approach suggests a modelling methodology that bases on big data acquisition and processing for the identification of significant factors and counterintuitive interrelations between them, which can be applied in any policy domain. Then, to allow the practical application of the framework an ICT architecture is designed, with the aim to overcome challenges related with big data management and processing. Finally, validation of the approach for driving policy design and implementation in the future in diverse policy domains, is suggested.}, location = {Galway, Ireland}, series = {ICEGOV '18}, pages = {575\u2013583}, numpages = {9}, keywords = {data mining, behavioural patterns, policy Modelling, Big data, impact assessment, evidence based policy making, dynamic simulation}}
@inproceedings{10.1145/2481528.2481537,title = {Intel \"big data\" science and technology center vision and execution plan}, author = {Stonebraker Michael , Madden Sam , Dubey Pradeep },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2481528.2481537}, doi = {10.1145/2481528.2481537}, abstract = {Intel has moved to a collaboration model with universities consisting of \"Science and Technology Centers\" (ISTCs). These are located at a \"hub\" university with participation from other universities, contain embedded Intel personnel, and are focused on some research theme. Intel held a national competition for a 5th Science and Technology center in 2012 and selected a proposal from M.I.T. with a theme of \"Big Data\". This paper presents the big data vision of this technology center and the execution plan for the first few years.}, pages = {44\u201349}, numpages = {6}}
@inproceedings{10.5555/2888619.2888700,title = {Weaving multi-agent modeling and big data for stochastic process inference}, author = {Dong Wen },year = {2015}, isbn = {9781467397414}, publisher = {IEEE Press}, abstract = {In this paper, we develop a stochastic process tool to tell the stories behind big data with agent-based models. Specifically, we identify an agent-based model as a stochastic process that generates the big data, and make inferences by solving the agent-based model under the constraint of the data. We hope to use this tool to create a bridge between those who have access to big data and those who use agent-based simulators to convey their insight about these data.}, location = {Huntington Beach, California}, series = {WSC '15}, pages = {713\u2013724}, numpages = {12}}
@inproceedings{10.1145/3327962.3331455,title = {Can We Securely Outsource Big Data Analytics with Lightweight Cryptography?}, author = {Chow Sherman S. M. },year = {2019}, isbn = {9781450367882}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3327962.3331455}, doi = {10.1145/3327962.3331455}, abstract = {Advances in cryptography such as secure multiparty computation (SMC) and fully-/somewhat-homomorphic encryption (FHE/SHE) have already provided a generic solution to the problem of processing encrypted data; however, they are still not that efficient if one directly applies them for big data analytics.Many cryptographers have recently designed specialized privacy-preserving frameworks for neural networks. While promising, they are still not entirely satisfactory. Gazelle (Usenix Security 2018) supports inference but not training. SecureNN (PoPETS 2019), with the help of non-colluding servers, is still orders of magnitudes slower than plaintext training/inferencing.To narrow the gap between theory and practice, we put forward a new paradigm for privacy-preserving big data analytics which leverages both trusted processor such as Intel SGX (Software Guard Extensions) and (untrusted) GPU (Graphics Processing Unit). Note that SGX is not a silver bullet in this scenario. In general, SGX is subject to a memory constraint which can be easily exceeded by a single layer of the (evergrowing) neural networks. Relying on the generic solution such as paging mechanism is, again, inefficient. GPU is an ideal platform for deep learning, yet, we do not want to assume it is trusted. We thus still need cryptographic techniques.In this keynote, we will briefly survey the research landscape of privacy-preserving machine learning, point out the obstacles brought by seemingly slight changes of requirements (e.g., a single query from different data sources, multiple model owners, outsourcing a trained model to an untrusted cloud), and highlight a number of settings which aids in ensuring privacy without heavyweight cryptography. We will also discuss two notable recent works, Graviton (OSDI 2018) and Slalom (ICLR 2019), and our ongoing research.}, location = {Auckland, New Zealand}, series = {SCC '19}, pages = {1}, numpages = {1}, keywords = {applied cryptography, homomorphic encryption, neural networks}}
@inproceedings{10.1145/1595808.1595830,title = {Software process data quality and characteristics: a historical view on open and closed source projects}, author = {Bachmann Adrian , Bernstein Abraham },year = {2009}, isbn = {9781605586786}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1595808.1595830}, doi = {10.1145/1595808.1595830}, abstract = {Software process data gathered from bug tracking databases and version control system log files are a very valuable source to analyze the evolution and history of a project or predict its future. These data are used for instance to predict defects, gather insight into a project's life-cycle, and additional tasks. In this paper we survey five open source projects and one closed source project in order to provide a deeper insight into the quality and characteristics of these often-used process data. Specifically, we first define quality and characteristics measures, which allow us to compare the quality and characteristics of the data gathered for different projects. We then compute the measures and discuss the issues arising from these observation. We show that there are vast differences between the projects, particularly with respect to the quality in the link rate between bugs and commits.}, location = {Amsterdam, The Netherlands}, series = {IWPSE-Evol '09}, pages = {119\u2013128}, numpages = {10}, keywords = {open source, version control system, closed source, data quality, data characteristics, bug tracker, case study}}
@inproceedings{10.1145/3265639.3265680,title = {Research on Key Problems of Data Quality in Large Industrial Data Environment}, author = {Guo Aizhang , Liu Xiuyuan , Sun Tao },year = {2018}, isbn = {9781450365307}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3265639.3265680}, doi = {10.1145/3265639.3265680}, abstract = {At present, the modern manufacturing and management concepts such as digitalization, networking and intellectualization have been popularized in the industry, and the degree of industrial automation and information has been improved unprecedentedly. Industrial products are everywhere in the world. They are involved in design, manufacture, operation, maintenance and recycling. The whole life cycle involves huge amounts of data. Improving data quality is very important for data mining and data analysis. To solve the problem of data inconsistency is a very important part of improving data quality.}, location = {Chengdu, China}, series = {ICRCA '18}, pages = {245\u2013248}, numpages = {4}, keywords = {Data Cleaning, Data Inconsistency, Data Quality}}
@inproceedings{10.1145/2808719.2816981,title = {Senior health management through internet of things and real-time big data analytics}, author = {Deng Xin , Wu Donghui },year = {2015}, isbn = {9781450338530}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2808719.2816981}, doi = {10.1145/2808719.2816981}, abstract = {In 10 years, one third of the population in developed countries will be 60-years or older. 90% of seniors want to age in their own homes, not a facility. Families want to know that their aging parents are healthy and safe. It is extremely challenging for seniors with chronic conditions to manage their day-to-day medical needs and prevent medical emergencies, e.g. stroke, fall, etc. With the rapid growth of Internet of Things, increasing popularity of wearable medical devices or monitor devices, and other sensors at home, the opportunity for senior health management and prevention of medical emergency at home has never been better with big data platform, integration of massive and diverse data sources, e.g. medical charts from doctor's office, prescription information from pharmacies, claims data from insurance companies, more importantly, real-time data stream from Internet-of-things, wearable devices, and other vital and sensor data at home; and even more critically, real-time alerts of various risks from predictive analytics connected with providers, care givers and family members. In this paper, we will analyze the scope of the problem, and present current status and challenges in this area. Secondly we will propose a prototype schema to address this problem through Internet of things and real-time big data predictive analytics. Finally we will discuss some technical and non-technical challenges observed.}, location = {Atlanta, Georgia}, series = {BCB '15}, pages = {674}, numpages = {1}, keywords = {real-time data collection, health informatics, real-time alerts, data integration, big data platform, predictive analytics}}
@inproceedings{10.1145/3488377,title = {Hierarchical Satellite System Graph for Approximate Nearest Neighbor Search on Big Data}, author = {Zhang Jiaru , Ma Ruhui , Song Tao , Hua Yang , Xue Zhengui , Guan Chenyang , Guan Haibing },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3488377}, doi = {10.1145/3488377}, abstract = {Approximate nearest neighbor search is a classical problem in data science, which is widely applied in many fields. With the rapid growth of data in the real world, it becomes more and more important to speed up the nearest neighbor search process. Satellite System Graph (SSG) is one of the state-of-the-art methods to solve the problem. However, with the further increase of the data scale of problems, SSG still needs a considerable amount of time to finish the search due to the limitation of step length and start point locations. To solve the problem, we propose Hierarchical Satellite System Graph (HSSG) and present its index algorithm and search algorithm. The index process can be distributed deployed due to the good parallelism of our designed hierarchical structure. The theoretical analysis reveals that HSSG decreases the search steps and reduces the computational cost and reduces the search time by searching on the hierarchical structure with a similar indexing time compared with SSG, hence reaches a better search efficiency. The experiments on multiple datasets present that HSSG reduces the distance computations, accelerates the search process, and increases the search precision in the real tasks, especially under the tasks with large scale and crowded distributions, which presents a good application prospect of HSSG.}, pages = {1\u201315}, numpages = {15}, keywords = {big data, hierarchical structure, Nearest neighbor search, approximate nearest neighbor search, data science}}
@inproceedings{10.1145/1839379.1839396,title = {Discovering dynamic integrity rules with a rules-based tool for data quality analyzing}, author = {Pham Thi Thanh Thoa , Helfert Markus },year = {2010}, isbn = {9781450302432}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1839379.1839396}, doi = {10.1145/1839379.1839396}, abstract = {Rules based approaches for data quality solutions often use business rules or integrity rules for data monitoring purpose. Integrity rules are constraints on data derived from business rules into a formal form in order to allow computerization. One of challenges of these approaches is rules discovering, which is usually manually made by business experts or system analysts based on experiences. In this paper, we present our rule-based approach for data quality analyzing, in which we discuss a comprehensive method for discovering dynamic integrity rules.}, location = {Sofia, Bulgaria}, series = {CompSysTech '10}, pages = {89\u201394}, numpages = {6}, keywords = {data quality, data quality analyzing, business rules, integrity rules}}
@inproceedings{10.1145/3510858.3511373,title = {Credit Anti Fraud Identification Method Based on Power Big Data}, author = {Zhan Shaohui , Tang Keqian , Chang Kaixuan , Yuan Liang , Liu Shuo , Li Zhaoming },year = {2021}, isbn = {9781450390422}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510858.3511373}, doi = {10.1145/3510858.3511373}, abstract = {With the development of Internet finance industry, there are more and more fraud means in credit business. How to effectively prevent credit fraud and reduce credit risk has become a key research topic for financial institutions. Based on the big data of electric power, the evaluation index of enterprise production and operation is established, and the real and objective enterprise production and operation situation is provided for financial units according to the scoring standard. This can assist the decision-making financial units to evaluate the risk of enterprises in the pre loan link, effectively reduce the financing risk of enterprises, and avoid bad debts and non-performing assets.}, location = {Changsha, China}, series = {ICASIT 2021}, pages = {735\u2013741}, numpages = {7}}
@inproceedings{10.1145/3377672.3378052,title = {Research on Evaluation of Innovation and Entrepreneurship Education in Local Universities under the Background of Big Data}, author = {Quanli Wang , Chu-jian Guo },year = {2019}, isbn = {9781450362481}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377672.3378052}, doi = {10.1145/3377672.3378052}, abstract = {The arrival of the era of big data has had a profound impact on the education system. As far as local university entrepreneurship education is concerned, in the context of big data, the evaluation model can be better applied to analyze the basic features of entrepreneurship education. Based on the profound influence of big data on entrepreneurship education, this paper analyzes the combination of entrepreneurship education and big data with the sample of entrepreneurship education in local colleges and universities, and analyzes the construction of scientific and reasonable evaluation system of entrepreneurship education.}, location = {Kuala Lumpur, Malaysia}, series = {AMME 2019}, pages = {169\u2013173}, numpages = {5}, keywords = {Local universities, Evaluation of entrepreneurship education, Big data}}
@inproceedings{10.5555/2591305.2591323,title = {(Big)data in a virtualized world: volume, velocity, and variety in cloud datacenters}, author = {Birke Robert , Bj\u00f6rkqvist Mathias , Chen Lydia Y. , Smirni Evgenia , Engbersen Ton },year = {2014}, isbn = {9781931971089}, publisher = {USENIX Association}, address = {USA}, abstract = {Virtualization is the ubiquitous way to provide computation and storage services to datacenter end-users. Guaranteeing sufficient data storage and efficient data access is central to all datacenter operations, yet little is known of the effects of virtualization on storage workloads. In this study, we collect and analyze field data from production datacenters that operate within the private cloud paradigm, during a period of three years. The datacenters of our study consist of 8,000 physical boxes, hosting over 90,000 VMs, which in turn use over 22 PB of storage. Storage data is analyzed from the perspectives of volume, velocity, and variety of storage demands on virtual machines and of their dependency on other resources. In addition to the growth rate and churn rate of allocated and used storage volume, the trace data illustrates the impact of virtualization and consolidation on the velocity of IO reads and writes, including IO deduplication ratios and peak load analysis of co-located VMs. We focus on a variety of applications which are roughly classified as app, web, database, file, mail, and print, and correlate their storage and IO demands with CPU, memory, and network usage. This study provides critical storage workload characterization by showing usage trends and how application types create storage traffic in large datacenters.}, location = {Santa Clara, CA}, series = {FAST'14}, pages = {177\u2013189}, numpages = {13}}
@inproceedings{10.1145/2980258.2980319,title = {Credit Card Fraud Detection using Big Data Analytics: Use of PSOAANN based One-Class Classification}, author = {Kamaruddin Sk. , Ravi Vadlamani },year = {2016}, isbn = {9781450347563}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2980258.2980319}, doi = {10.1145/2980258.2980319}, abstract = {Banking and financial industries are facing severe challenges in the form of fraudulent transactions. Credit card fraud is one example of them. In order to detect credit card fraud, we employed one-class classification approach in big data paradigm. We implemented a hybrid architecture of Particle Swarm Optimization and Auto-Associative Neural Network for one-class classification in Spark computational framework. In this paper, we implemented parallelization of the auto-associative neural network in the hybrid architecture.}, location = {Pondicherry, India}, series = {ICIA-16}, pages = {1\u20138}, numpages = {8}, keywords = {Auto-associative neural network, Particle swarm optimization, One-class classification, Auto-encoder, Single class classification}}
@inproceedings{10.1145/2538542.2538565,title = {Asynchronous object storage with QoS for scientific and commercial big data}, author = {Brim Michael J. , Dillow David A. , Oral Sarp , Settlemyer Bradley W. , Wang Feiyi },year = {2013}, isbn = {9781450325059}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2538542.2538565}, doi = {10.1145/2538542.2538565}, abstract = {This paper presents our design for an asynchronous object storage system intended for use in scientific and commercial big data workloads. Use cases from the target workload domains are used to motivate the key abstractions used in the application programming interface (API). The architecture of the Scalable Object Store (SOS), a prototype object storage system that supports the API's facilities, is presented. The SOS serves as a vehicle for future research into scalable and resilient big data object storage. We briefly review our research into providing efficient storage servers capable of providing quality of service (QoS) contracts relevant for big data use cases.}, location = {Denver, Colorado}, series = {PDSW '13}, pages = {7\u201313}, numpages = {7}, keywords = {HPC storage, storage QoS, object storage, cloud storage}}
@inproceedings{10.1145/3310205.3310211,title = {Data quality rule definition and discovery},year = {2019}, isbn = {9781450371520}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3310205.3310211}, doi = {10.1145/3310205.3310211}, abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.}, pages = {}}
@inproceedings{10.1145/3092944,title = {Low Overhead CS-Based Heterogeneous Framework for Big Data Acceleration}, author = {Kulkarni Amey , Shea Colin , Abtahi Tahmid , Homayoun Houman , Mohsenin Tinoosh },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3092944}, doi = {10.1145/3092944}, abstract = {Big data processing on hardware gained immense interest among the hardware research community to take advantage of fast processing and reconfigurability. Though the computation latency can be reduced using hardware, big data processing cost is dominated by data transfers. In this article, we propose a low overhead framework based on compressive sensing (CS) to reduce data transfers up to 67% without affecting signal quality. CS has two important kernels: \u201csensing\u201d and \u201creconstruction.\u201d In this article, we focus on CS reconstruction is using orthogonal matching pursuit (OMP) algorithm. We implement the OMP CS reconstruction algorithm on a domain-specific PENC many-core platform and a low-power Jetson TK1 platform consisting of an ARM CPU and a K1 GPU. Detailed performance analysis of OMP algorithm on each platform suggests that the PENC many-core platform has 15\u00d7 and 18\u00d7 less energy consumption and 16\u00d7 and 8\u00d7 faster reconstruction time as compared to the low-power ARM CPU and K1 GPU, respectively. Furthermore, we implement the proposed CS-based framework on heterogeneous architecture, in which the PENC many-core architecture is used as an \u201caccelerator\u201d and processing is performed on the ARM CPU platform. For demonstration, we integrate the proposed CS-based framework with a hadoop MapReduce platform for a face detection application. The results show that the proposed CS-based framework with the PENC many-core as an accelerator achieves a 26.15% data storage/transfer reduction, with an execution time and energy consumption overhead of 3.7% and 0.002%, respectively, for 5,000 image transfers. Compared to the CS-based framework implementation on the low-power Jetson TK1 ARM CPU+GPU platform, the PENC many-core implementation is 2.3\u00d7 faster for the image reconstruction part, while achieving 29% higher performance and 34% better energy efficiency for the complete face detection application on the Hadoop MapReduce platform.}, pages = {1\u201325}, numpages = {25}, keywords = {Compressive sensing, heterogeneous architecture, many-core}}
@inproceedings{10.1145/3368756.3369046,title = {Automated management of maritime container terminals using internet of things and big data technologies}, author = {Kaderi Farah Al , Koulali Rim , Rida Mohamed },year = {2019}, isbn = {9781450362894}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3368756.3369046}, doi = {10.1145/3368756.3369046}, abstract = {Global trade is growing steadily and the management of the transport and logistics fields is facing many challenges day after day. Container shipping is one of those areas whose management has become a crucial task because of its major role in ensuring the quality of freight transport service. In this paper, we present and describe a real-time monitoring and management system dedicated to the automatization of the maritime terminal containers management. Our proposed solution is based on using both Internet of Things (IoT) and Big Data technologies in order to build a system that provides advanced features to cover different aspects of container logistics management. As the implementation of the proposed system implies using various tools and technologies, we conducted a comparative study on several technologies and tools related to IoT and Big Data in order to choose the appropriate ones. We also adopted a multi-layer architecture for the proposed system and through this paper, we will present the scope, role and tools set for every tier.}, location = {Casablanca, Morocco}, series = {SCA '19}, pages = {1\u20136}, numpages = {6}, keywords = {MapReduce, internet of things, Arduino Uno, maritime container terminal, HDFS, multi-layer architecture, RFID, RaspBerry pi 3 b+, big data, Apache hadoop, MQTT, Apache spark}}
@inproceedings{10.1145/3331453.3360976,title = {Research on Big Data Open Intelligent Platform of Guizhou Province E-government Service}, author = {Zhou Guixian , Chen Kaijian , Tu Jingmei },year = {2019}, isbn = {9781450362948}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3331453.3360976}, doi = {10.1145/3331453.3360976}, abstract = {Government affairs open service platform to integrate distributed and heterogeneous system of information resources, eliminate the \"information island\" phenomenon, through the open service platform, service background, e-government service management platform, monitoring platform four subsystems to achieve between different institutions, different application system and database based on the different transmission protocol of data exchange, information sharing and business collaboration, thus to provide good data to support the government and the enterprise information construction environment.}, location = {Sanya, China}, series = {CSAE 2019}, pages = {1\u20135}, numpages = {5}, keywords = {E-government service, Open intelligent platform, Big data}}
@inproceedings{10.1145/2463676.2465290,title = {Fast data in the era of big data: Twitter's real-time related query suggestion architecture}, author = {Mishne Gilad , Dalton Jeff , Li Zhenghua , Sharma Aneesh , Lin Jimmy },year = {2013}, isbn = {9781450320375}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2463676.2465290}, doi = {10.1145/2463676.2465290}, abstract = {We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time \"twist\": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of \"big data\". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a \"big data\" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle \"big\" as well as \"fast\" data.}, location = {New York, New York, USA}, series = {SIGMOD '13}, pages = {1147\u20131158}, numpages = {12}, keywords = {mapreduce, hadoop, log analysis}}
@inproceedings{10.1145/3482632.3483101,title = {Quantitative Research on Psychological Education Based on Big Data Analysis Technology}, author = {Qi Xiaoying },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483101}, doi = {10.1145/3482632.3483101}, abstract = {Under the background of the current big data era, it is particularly important to establish and apply the big data guarantee mechanism for quantitative research to carry out psychological education, Based on the results of Freshmen's psychological survey in the era of big data, this paper discusses the theory and practice methods of psychological education in the era of big data, aiming to put forward constructive suggestions for the development of psychological education in Colleges and universities, give full play to the full participation of the whole staff in student management, the maintenance and improvement of students' psychological quality, and strengthen the positive work of the whole staff in major crisis events In order to promote the healthy development of psychological education in Colleges and universities and the happy growth of students, it is necessary to improve the enthusiasm, initiative and sense of acquisition of all staff in Colleges and universities.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1139\u20131143}, numpages = {5}}
@inproceedings{10.1145/3510249.3510262,title = {Applications of Big Data within Finance: Fraud Detection and Risk Management within the Real Estate Industry}, author = {Eltweri Ahmed , Faccia Alessio , KHASSAWNEH OSAMA },year = {2021}, isbn = {9781450387392}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510249.3510262}, doi = {10.1145/3510249.3510262}, abstract = {Big Data within the world of finance is about large, complex, and diverse (unstructured and structured) data sets that may be employed in providing solutions across the world for business challenges within banking companies and financial services that are long-standing. Big Data helps in enhancing the significance of FinTech in offering numerous financial services for users, facilitating the distribution of new payment, financing, and exchange services within an increasingly large proportion of the population. Technological developments have changed our lives profoundly, particularly over the last two decades. All fields of the economy have been changed, so it is hardly surprising that the world of real estate has been impacted by technological advances. Indeed, great technological strides have been made within the financial world that has allowed both professionals and amateurs to employ technical, innovative solutions that may lead to improved performance both within personally used commercial activity and for the purposes of commerce. Various applications of Big Data have been very beneficial for the world of finance because of new innovations in various technologies. The focus of this research has been upon the undertaking of a systematic analysis related to the technologies that are considered most important and that currently allow great progress to be made in fraud detection and risk management within the real estate industry by analysing the data collected. The particular focus of the research has been to highlight 3 particular interest areas, namely: i) FinTech and Big Data, ii) risk management, iii) fraud detection. A recent case study related to scandals that have arisen in the FinTech industry has provided further help in support of the research hypotheses and the conclusions are drawn.}, location = {Sanya, China}, series = {EBEE 2021}, pages = {67\u201373}, numpages = {7}, keywords = {PropTech, Real Estate Industry, Fintech, Fraud Detection, Big Data, Risk management}}
@inproceedings{10.1145/3495018.3495292,title = {Analysis on Regional Social Organization Standardization Based on Big Data}, author = {Niu Nana , Che Di },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495292}, doi = {10.1145/3495018.3495292}, abstract = {Since the issuance of the Plan for Furthering the Standardization Reforms in March 2015, in the more than five years of cultivating and developing social organization standards, government departments and social organizations across the country have actively promoted social organization standardization work, and social organization standards have developed rapidly in China. In order to better study the standardization work of various local organizations, this paper selected Beijing for research based on big data analysis methods. In the process of actively implementing the standardization reform policy, cultivating and developing social organization standards, Beijing's social organization standardization work has achieved remarkable results, but there are also some problems. Based on the data disclosed by the National Social Organization Standard Information Platform, this paper sorts out and summarizes the situation of Beijing's social organizations carrying out social organization standardization work, analyzes existing problems and makes suggestions.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {862\u2013867}, numpages = {6}}
@inproceedings{10.1145/3549843.3549851,title = {Research on the Learning Situation Analysis and Academic Prediction based on Education Big Data}, author = {Yang Yun },year = {2022}, isbn = {9781450397216}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3549843.3549851}, doi = {10.1145/3549843.3549851}, abstract = {Base on literature research, this study defined the concepts of learner portrait and academic prediction and constructed the label system of learner portrait. By applying portrait technology to realize learning situation analysis, it proposed learning situation analysis based on five dimensions of portrait, to grasp learners' learning situation information. Base on the results of learning situation analysis of five dimensions, association rules, sequence analysis model and other technical means were used to construct four-dimensional academic prediction, and provide different academic prediction strategies according to learning situation analysis. Based on the design of academic prediction mechanism based on big data of learning situation, a learning situation analysis system with Hadoop as the core was designed, and an implementation scheme of data mining parallel algorithm processing platform was proposed to mine more valuable data information and recommend more scientific, reasonable and useful learning schemes for learners. It also helped teachers to selectively provide suitable personalized teaching for individual and group learners of different types of learners}, location = {Beijing, China}, series = {ICEBT '22}, pages = {52\u201358}, numpages = {7}, keywords = {Education Big Data, learning Situation Analysis, Academic Prediction}}
@inproceedings{10.1145/2818869.2818934,title = {A Hybrid Big Data Analytics Method for Yield Improvement in Semiconductor Manufacturing}, author = {Lee Chung-Hong , Yang Hsin-Chang , Cheng Shou-Chen , Tsai Sheng-Wen },year = {2015}, isbn = {9781450337359}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2818869.2818934}, doi = {10.1145/2818869.2818934}, abstract = {In the manufacturing of semiconductor encapsulation, the production yield is one of critical issues concerned by all foundries. It is because that yield rate can directly affects the quality of the final product and the profitability. In this work we take the defect-products as samples and use machine learning techniques to classify the samples and verify the accuracy and feasibility of the experiment. We use Support Vector Machines (SVM) model to perform classification and compare the resulting accuracy with the results of Back Propagation Neural Network (BPN) model. Furthermore, we employ a statistical method namely Pearson product-moment correlation coefficient to identify the influential factors for production quality. The experimental result demonstrates that our hybrid method has great potentials for yield improvement in semiconductor manufacturing.}, location = {Kaohsiung, Taiwan}, series = {ASE BD&amp;SI '15}, pages = {1\u20134}, numpages = {4}, keywords = {Support Vector Machines, Machine Learning, Big Data Analysis}}
@inproceedings{10.1145/3255772,title = {Session details: Industry session 4: big data systems}, author = {Finkelstein Shel },year = {2014}, isbn = {9781450323765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3255772}, doi = {10.1145/3255772}, location = {Snowbird, Utah, USA}, series = {SIGMOD '14}, pages = {}}
@inproceedings{10.1145/966389.966395,title = {Assessing data quality with control matrices}, author = {Pierce Elizabeth M. },year = {2004}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/966389.966395}, doi = {10.1145/966389.966395}, abstract = {The control matrix, long used by IS auditors to evaluate information integrity, can be modified to assess the reliability of information products.}, pages = {82\u201386}, numpages = {5}}
@inproceedings{10.1145/3494885.3494939,title = {Overview of information visualization for business under the background of big data: Overview of information visualization}, author = {Wang Yan , Zhao Zi-hao , Li Hang },year = {2021}, isbn = {9781450390675}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3494885.3494939}, doi = {10.1145/3494885.3494939}, abstract = {In the era of data explosion, the data is growing exponentially. Information visualization maps abstract data into visual effects, and helps people to understand the market and make efficient decisions in an intuitive way, which has attracted more and more attention.This paper focuses on the field of information visualization, studies the theoretical basis of visualization models, research methods and design principles, expounds commercial application cases of information visualization, and presents the challenges and opportunities of the information visualization.}, location = {Singapore, Singapore}, series = {CSSE 2021}, pages = {295\u2013299}, numpages = {5}, keywords = {Model, Business Applications, Information Visualization, Big Data}}
@inproceedings{10.1145/3480571.3480580,title = {Research on language dynamics development based on rule-based methods in the context of big data}, author = {Zeli Cen },year = {2021}, isbn = {9781450390637}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3480571.3480580}, doi = {10.1145/3480571.3480580}, abstract = {Based on the background of big data in the Internet age, and ecological linguistics as the theoretical foundation, this article uses the annual \"Top Ten Media Buzzwords\" and the annual \"Top Ten Internet Terms\" as the research objects to explore the synchronicity of the internal semantics of these annual buzzwords The diachronic development of change and its life trend and the study of its communication mechanism. The study found that both the media's annual buzzwords and the Internet's annual buzzwords have the characteristics of time, high frequency and history, but they are different in style, communication path, semantic expression connotation and production path, grammatical level, etc. They are constantly in the process of dynamic and balanced development. The annual Internet buzzwords are mainly spread dynamically in the way of semantic generalization and form derivation. Taking the media annual buzzwords as an example to conduct a dynamic investigation, it is found that their life trends are mainly \"protruding\". \"Trends\", \"Growth\" trends, \"Recession\" trends, \"Stable\" trends and other categories.}, location = {Bucharest, Romania}, series = {ICIIP 2021}, pages = {53\u201357}, numpages = {5}, keywords = {Dynamics, Big data, Regular algorithms, Buzzwords}}
@inproceedings{10.1145/3361525.3361547,title = {Differential Approximation and Sprinting for Multi-Priority Big Data Engines}, author = {Birke Robert , Rocha Isabelly , Perez Juan , Schiavoni Valerio , Felber Pascal , Chen Lydia Y. },year = {2019}, isbn = {9781450370097}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3361525.3361547}, doi = {10.1145/3361525.3361547}, abstract = {Today's big data clusters based on the MapReduce paradigm are capable of executing analysis jobs with multiple priorities, providing differential latency guarantees. Traces from production systems show that the latency advantage of high-priority jobs comes at the cost of severe latency degradation of low-priority jobs as well as daunting resource waste caused by repetitive eviction and re-execution of low-priority jobs. We advocate a new resource management design that exploits the idea of differential approximation and sprinting. The unique combination of approximation and sprinting avoids the eviction of low-priority jobs and its consequent latency degradation and resource waste. To this end, we designed, implemented and evaluated DiAS, an extension of the Spark processing engine to support deflate jobs by dropping tasks and to sprint jobs. Our experiments on scenarios with two and three priority classes indicate that DiAS achieves up to 90% and 60% latency reduction for low- and high-priority jobs, respectively. DiAS not only eliminates resource waste but also (surprisingly) lowers energy consumption up to 30% at only a marginal accuracy loss for low-priority jobs.}, location = {Davis, CA, USA}, series = {Middleware '19}, pages = {202\u2013214}, numpages = {13}, keywords = {energy savings, sprinting, Spark, differential approximation, priorities}}
@inproceedings{10.1145/3219819.3220015,title = {Robust Bayesian Kernel Machine via Stein Variational Gradient Descent for Big Data}, author = {Nguyen Khanh , Le Trung , Nguyen Tu Dinh , Phung Dinh , Webb Geoffrey I. },year = {2018}, isbn = {9781450355520}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3219819.3220015}, doi = {10.1145/3219819.3220015}, abstract = {Kernel methods are powerful supervised machine learning models for their strong generalization ability, especially on limited data to effectively generalize on unseen data. However, most kernel methods, including the state-of-the-art LIBSVM, are vulnerable to the curse of kernelization, making them infeasible to apply to large-scale datasets. This issue is exacerbated when kernel methods are used in conjunction with a grid search to tune their kernel parameters and hyperparameters which brings in the question of model robustness when applied to real datasets. In this paper, we propose a robust Bayesian Kernel Machine (BKM) - a Bayesian kernel machine that exploits the strengths of both the Bayesian modelling and kernel methods. A key challenge for such a formulation is the need for an efficient learning algorithm. To this end, we successfully extended the recent Stein variational theory for Bayesian inference for our proposed model, resulting in fast and efficient learning and prediction algorithms. Importantly our proposed BKM is resilient to the curse of kernelization, hence making it applicable to large-scale datasets and robust to parameter tuning, avoiding the associated expense and potential pitfalls with current practice of parameter tuning. Our extensive experimental results on 12 benchmark datasets show that our BKM without tuning any parameter can achieve comparable predictive performance with the state-of-the-art LIBSVM and significantly outperforms other baselines, while obtaining significantly speedup in terms of the total training time compared with its rivals}, location = {London, United Kingdom}, series = {KDD '18}, pages = {2003\u20132011}, numpages = {9}, keywords = {big data, stein divergence, random feature, variational method, multiclass supervised learning, kernel methods, bayesian inference}}
@inproceedings{10.1145/2847263.2847294,title = {A Scalable Heterogeneous Dataflow Architecture For Big Data Analytics Using FPGAs (Abstract Only)}, author = {Ghasemi Ehsan , Chow Paul },year = {2016}, isbn = {9781450338561}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2847263.2847294}, doi = {10.1145/2847263.2847294}, abstract = {Due to rapidly expanding data size, there is increasing need for scalable, high-performance, and low-energy frameworks for large- scale data computation. We build a dataflow architecture that harnesses FPGA resources within a distributed analytics platform creating a heterogeneous data analytics framework. This approach leverages the scalability of existing distributed processing environments and provides easy access to custom hardware accelerators for large-scale data analysis. We prototype our framework within the Apache Spark analytics tool running on a CPU-FPGA heterogeneous cluster. As a specific application case study, we have chosen the MapReduce paradigm to implement a multi-purpose, scalable, and customizable RTL accelerator inside the FPGA, capable of incorporating custom High-Level Synthesis (HLS) MapReduce kernels. We demonstrate how a typical MapReduce application can be simply adapted to our distributed framework while retaining the scalability of the Spark platform.}, location = {Monterey, California, USA}, series = {FPGA '16}, pages = {274}, numpages = {1}, keywords = {apache spark, big data, fpga, mapreduce}}
@inproceedings{10.1145/3252678,title = {Session details: SIRIP I: Big companies, big data}, author = {Mishne Gilad },year = {2016}, isbn = {9781450340694}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3252678}, doi = {10.1145/3252678}, location = {Pisa, Italy}, series = {SIGIR '16}, pages = {}}
@inproceedings{10.1145/3495018.3495374,title = {Clothing Material Design Concept Based on Big Data and Information Technology}, author = {Hu Na },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495374}, doi = {10.1145/3495018.3495374}, abstract = {Fashion design belongs to the category of art and craft, which is an art form combining practicality and artistry. With the development of science and technology, network technology has been fully integrated into social life. The emergence of big data has changed the traditional economic and social form, and put forward new requirements for fashion design profession in the field of education. Based on big data, this paper studies the concept of Chinese clothing material design through hash algorithm of big data. Therefore, in order to adapt to the requirements of the era of big data, this paper believes that the fashion design major of higher professional universities should start with the education reform, strengthen the education mode in line with the times, improve the market demand, and provide professional fashion design talents for the society. Recently, China's garment industry needs a lot of experts. Many enterprises can't find talents suitable for many positions every year. Graduates are still facing employment problems. The imbalance of demand and supply is reflected in the talent training methods can not meet the requirements of the clothing industry. In the context of big data today, the innovation of fashion design should be the main task to develop, massive information can be easily presented in front of people. Therefore, we need to cater to the pace of fashion elements to meet the existing needs. In the source of fashion elements, the innovation of fashion design and the pursuit of fashion is no longer a lot of designers. Massive data resources use traditional fashion prediction methods to predict the results of its development trend, but the actual embodiment of life and culture. The trend of fashion design has been unable to achieve. In the context of the new era, the development of the Internet has led to information innovation, which is not limited to the structure of style and color, but in the development of clothing data.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {1244\u20131247}, numpages = {4}}
@inproceedings{10.5555/352925.352970,title = {Assessing data quality for information products}, author = {Parssian Amir , Sarkar Sumit , Jacob Varghese S. },year = {1999}, publisher = {Association for Information Systems}, address = {USA}, location = {Charlotte, North Carolina, USA}, series = {ICIS '99}, pages = {428\u2013433}, numpages = {6}}