@article{10.14778/2536222.2536235,
author = {Bellamkonda, Srikanth and Li, Hua-Gang and Jagtap, Unmesh and Zhu, Yali and Liang, Vince and Cruanes, Thierry},
title = {Adaptive and Big Data Scale Parallel Execution in Oracle},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536222.2536235},
doi = {10.14778/2536222.2536235},
abstract = {This paper showcases some of the newly introduced parallel execution methods in Oracle RDBMS. These methods provide highly scalable and adaptive evaluation for the most commonly used SQL operations - joins, group-by, rollup/cube, grouping sets, and window functions. The novelty of these techniques is their use of multi-stage parallelization models, accommodation of optimizer mistakes, and the runtime parallelization and data distribution decisions. These parallel plans adapt based on the statistics gathered on the real data at query execution time. We realized enormous performance gains from these adaptive parallelization techniques. The paper also discusses our approach to parallelize queries with operations that are inherently serial. We believe all these techniques will make their way into big data analytics and other massively parallel database systems.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1102–1113},
numpages = {12}
}

@inproceedings{10.1145/3254147,
author = {Antoniu, Gabriel},
title = {Session Details: HPC and Big Data Convergence},
year = {2016},
isbn = {9781450343534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254147},
doi = {10.1145/3254147},
booktitle = {Proceedings of the ACM 7th Workshop on Scientific Cloud Computing},
location = {Kyoto, Japan},
series = {ScienceCloud '16}
}

@inproceedings{10.1145/3299815.3314439,
author = {Faker, Osama and Dogdu, Erdogan},
title = {Intrusion Detection Using Big Data and Deep Learning Techniques},
year = {2019},
isbn = {9781450362511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299815.3314439},
doi = {10.1145/3299815.3314439},
abstract = {In this paper, Big Data and Deep Learning Techniques are integrated to improve the performance of intrusion detection systems. Three classifiers are used to classify network traffic datasets, and these are Deep Feed-Forward Neural Network (DNN) and two ensemble techniques, Random Forest and Gradient Boosting Tree (GBT). To select the most relevant attributes from the datasets, we use a homogeneity metric to evaluate features. Two recently published datasets UNSW NB15 and CICIDS2017 are used to evaluate the proposed method. 5-fold cross validation is used in this work to evaluate the machine learning models. We implemented the method using the distributed computing environment Apache Spark, integrated with Keras Deep Learning Library to implement the deep learning technique while the ensemble techniques are implemented using Apache Spark Machine Learning Library. The results show a high accuracy with DNN for binary and multiclass classification on UNSW NB15 dataset with accuracies at 99.16% for binary classification and 97.01% for multiclass classification. While GBT classifier achieved the best accuracy for binary classification with the CICIDS2017 dataset at 99.99%, for multiclass classification DNN has the highest accuracy with 99.56%.},
booktitle = {Proceedings of the 2019 ACM Southeast Conference},
pages = {86–93},
numpages = {8},
keywords = {feature selection, big data, deep learning, ensemble techniques, artificial neural networks, Intrusion detection system, machine learning},
location = {Kennesaw, GA, USA},
series = {ACM SE '19}
}

@inproceedings{10.1145/3127479.3129253,
author = {Hwang, Eunji and Kim, Hyungoo and Nam, Beomseok and Choi, Young-ri},
title = {Exploring Memory Locality for Big Data Analytics in Virtualized Clusters},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3129253},
doi = {10.1145/3127479.3129253},
abstract = {In this work, we investigate techniques to improve the performance of big data analytics in virtualized clusters by effectively increasing the utilization of cached data and efficiently using scarce memory resources.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {657},
numpages = {1},
keywords = {memory locality, big data analytics frameworks, hadoop, cloud computing},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/2628194.2628231,
author = {Ceci, Michelangelo and Cassavia, Nunziato and Corizzo, Roberto and Dicosta, Pietro and Malerba, Donato and Maria, Gaspare and Masciari, Elio and Pastura, Camillo},
title = {Innovative Power Operating Center Management Exploiting Big Data Techniques},
year = {2014},
isbn = {9781450326278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628194.2628231},
doi = {10.1145/2628194.2628231},
abstract = {The problem of accurately predicting the energy production from renewable sources has recently received an increasing attention from both the industrial and the research communities. It presents several challenges, such as facing with the rate data are provided by sensors, the heterogeneity of the data collected, power plants efficiency, as well as uncontrollable factors, such as weather conditions and user consumption profiles. In this paper we describe Vi-POC (Virtual Power Operating Center), a project conceived to assist energy producers and decision makers in the energy market. In this paper we present the Vi-POC project and how we face with challenges posed by the specific application. The solutions we propose have roots both in big data management and in stream data mining.},
booktitle = {Proceedings of the 18th International Database Engineering &amp; Applications Symposium},
pages = {326–329},
numpages = {4},
location = {Porto, Portugal},
series = {IDEAS '14}
}

@inproceedings{10.1145/3224207.3224220,
author = {Bhuiyan, Md Zakirul Alam and Zaman, Aliuz and Wang, Tian and Wang, Guojun and Tao, Hai and Hassan, Mohammad Mehedi},
title = {Blockchain and Big Data to Transform the Healthcare},
year = {2018},
isbn = {9781450364188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3224207.3224220},
doi = {10.1145/3224207.3224220},
abstract = {The increase in reported incidents of security breaches that compromise privacy of individuals requires us to question the current model used to collect patient information. What we have learned from bitcoin and the underlying blockchain technology is that there are ways for us to protect this information by using a distributed ledger. In this paper, we review and propose a solution that can be used to manage individual health data as well as cross-institutional sharing of this information. The solution will increase clinical effectiveness and an increase in research when the data is shared with researchers. The proposed system solution based on blockchain technology that includes providers, hospitals and clinic, insurance companies, and patients. All along the ownership of the data would belong to the individual or the patient. In the solution, we suggest to adopt a private blockchain solution where all participants are known and trusted, which allows for privacy and security of the data.},
booktitle = {Proceedings of the International Conference on Data Processing and Applications},
pages = {62–68},
numpages = {7},
keywords = {healthcare, Blockchain, patent data, big data, privacy},
location = {Guangdong, China},
series = {ICDPA 2018}
}

@article{10.1145/27544.27546,
author = {Ballou, Donald P. and Pazer, Harold L. and Belardo, Salvatore and Klein, Barbara},
title = {Implications of Data Quality for Spreadsheet Analysis},
year = {1987},
issue_date = {March 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/27544.27546},
doi = {10.1145/27544.27546},
abstract = {This paper examines the impact of deficiencies in data quality on the results generated for spreadsheet applications. The purpose is to describe a framework which can be systematically used to determine the relative importance of potential errors in operational and judgmental data. Special emphasis is placed on analyzing the implications of deficiencies in data quality on projected spreadsheet results.},
journal = {SIGMIS Database},
month = {mar},
pages = {13–19},
numpages = {7}
}

@inproceedings{10.1145/2465848.2480346,
author = {Sion, Radu},
title = {To Cloud My Big Data or Not to? Musings at the Intersection of Big Data, Intense Computing and Clouds},
year = {2013},
isbn = {9781450319799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465848.2480346},
doi = {10.1145/2465848.2480346},
booktitle = {Proceedings of the 4th ACM Workshop on Scientific Cloud Computing},
pages = {3–4},
numpages = {2},
keywords = {cloud computing, big data, cloud economics},
location = {New York, New York, USA},
series = {Science Cloud '13}
}

@inproceedings{10.1109/CCGrid.2015.170,
author = {Cuzzocrea, Alfredo and Mumolo, Enzo and Corona, Pietro},
title = {Cloud-Based Machine Learning Tools for Enhanced Big Data Applications},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.170},
doi = {10.1109/CCGrid.2015.170},
abstract = {We propose Cloud-based machine learning tools for enhanced Big Data applications, where the main idea is that of predicting the "next" workload occurring against the target Cloud infrastructure via an innovative ensemble-based approach that combine the effectiveness of different well-known classifiers in order to enhance the whole accuracy of the final classification, which is very relevant at now in the specific context of Big Data. So-called workload categorization problem plays a critical role towards improving the efficiency and the reliability of Cloud-based big data applications. Implementation-wise, our method proposes deploying Cloud entities that participate to the distributed classification approach on top of virtual machines, which represent classical "commodity" settings for Cloud-based big data applications. Preliminary experimental assessment and analysis clearly confirm the benefits deriving from our classification framework.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {908–914},
numpages = {7},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3141128.3141138,
author = {Zafar, Muhammad Nouman and Azam, Farooque and Rehman, Saad and Anwar, Muhammad Waseem},
title = {A Systematic Review of Big Data Analytics Using Model Driven Engineering},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141138},
doi = {10.1145/3141128.3141138},
abstract = {In this era of information technology, there is a huge and excessive amount of fully distributed, structured and unstructured data which is usually referred as 'Big Data'. This data cannot be easily and directly used for business purposes due to its excessiveness nature. Therefore, it is required to intelligently process this large amount of data to extract desired information and examine pattern to make decisions and predictions for certain business objectives. In this context, Model Driven Engineering (MDE) techniques are frequently applied for Big Data analytics. This paper investigates the latest models, approaches and tools for Big Data analytics using model driven approaches. Particularly, a Systematic Literature Review (SLR) is performed to select and analyze 24 researches published during 2010 to 2017. This leads to identify 18 models, 13 tools, and 10 approaches for big data analytics using model driven approaches. The findings of this SLR are highly valuable for the researchers, students and practitioners of the domain.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {1–5},
numpages = {5},
keywords = {MDE, Big data, Model driven big data analytics, Big data predictive models},
location = {London, United Kingdom},
series = {ICCBDC 2017}
}

@inproceedings{10.1145/2554688.2554789,
author = {Jun, Sang-Woo and Liu, Ming and Fleming, Kermin Elliott and Arvind},
title = {Scalable Multi-Access Flash Store for Big Data Analytics},
year = {2014},
isbn = {9781450326711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554688.2554789},
doi = {10.1145/2554688.2554789},
abstract = {For many "Big Data" applications, the limiting factor in performance is often the transportation of large amount of data from hard disks to where it can be processed, i.e. DRAM. In this paper we examine an architecture for a scalable distributed flash store which aims to overcome this limitation in two ways. First, the architecture provides a high-performance, high-capacity, scalable random-access storage. It achieves high-throughput by sharing large numbers of flash chips across a low-latency, chip-to-chip backplane network managed by the flash controllers. The additional latency for remote data access via this network is negligible as compared to flash access time. Second, it permits some computation near the data via a FPGA-based programmable flash controller. The controller is located in the datapath between the storage and the host, and provides hardware acceleration for applications without any additional latency. We have constructed a small-scale prototype whose network bandwidth scales directly with the number of nodes, and where average latency for user software to access flash store is less than 70mus, including 3.5mus of network overhead.},
booktitle = {Proceedings of the 2014 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {55–64},
numpages = {10},
keywords = {flash, storage system, big data, fpga networks, ssd},
location = {Monterey, California, USA},
series = {FPGA '14}
}

@inproceedings{10.1145/3041021.3054141,
author = {Chen, Kuan-Ting and Luo, Jiebo},
title = {When Fashion Meets Big Data: Discriminative Mining of Best Selling Clothing Features},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054141},
doi = {10.1145/3041021.3054141},
abstract = {With the prevalence of e-commence websites and the ease of online shopping, consumers are embracing huge amounts of various options in products. Undeniably, shopping is one of the most essential activities in our society and studying consumer's shopping behavior is important for the industry as well as sociology and psychology. Indisputable, one of the most popular e-commerce categories is clothing business. There arises the needs for analysis of popular and attractive clothing features which could further boost many emerging applications, such as clothing recommendation and advertising. In this work, we design a novel system that consists of three major components: 1) exploring and organizing a large-scale clothing dataset from a online shopping website, 2) pruning and extracting images of best-selling products in clothing item data and user transaction history, and 3) utilizing a machine learning based approach to discovering fine-grained clothing attributes as the representative and discriminative characteristics of popular clothing style elements. Through the experiments over a large-scale online clothing shopping dataset, we demonstrate the effectiveness of our proposed system, and obtain useful insights on clothing consumption trends and profitable clothing features.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {15–22},
numpages = {8},
keywords = {online shopping, big data, clothing features, data mining, image analysis},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@article{10.14778/2367502.2367512,
author = {Rabl, Tilmann and G\'{o}mez-Villamor, Sergio and Sadoghi, Mohammad and Munt\'{e}s-Mulero, Victor and Jacobsen, Hans-Arno and Mankovskii, Serge},
title = {Solving Big Data Challenges for Enterprise Application Performance Management},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367512},
doi = {10.14778/2367502.2367512},
abstract = {As the complexity of enterprise systems increases, the need for monitoring and analyzing such systems also grows. A number of companies have built sophisticated monitoring tools that go far beyond simple resource utilization reports. For example, based on instrumentation and specialized APIs, it is now possible to monitor single method invocations and trace individual transactions across geographically distributed systems. This high-level of detail enables more precise forms of analysis and prediction but comes at the price of high data rates (i.e., big data). To maximize the benefit of data monitoring, the data has to be stored for an extended period of time for ulterior analysis. This new wave of big data analytics imposes new challenges especially for the application performance monitoring systems. The monitoring data has to be stored in a system that can sustain the high data rates and at the same time enable an up-to-date view of the underlying infrastructure. With the advent of modern key-value stores, a variety of data storage systems have emerged that are built with a focus on scalability and high data rates as predominant in this monitoring use case.In this work, we present our experience and a comprehensive performance evaluation of six modern (open-source) data stores in the context of application performance monitoring as part of CA Technologies initiative. We evaluated these systems with data and workloads that can be found in application performance monitoring, as well as, on-line advertisement, power monitoring, and many other use cases. We present our insights not only as performance results but also as lessons learned and our experience relating to the setup and configuration complexity of these data stores in an industry setting.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1724–1735},
numpages = {12}
}

@inproceedings{10.1145/3150919.3150925,
author = {Ramirez, Andres and Rahnemoonfar, Maryam},
title = {Improved Locally Linear Embedding for Big-Data Classification},
year = {2017},
isbn = {9781450354943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3150919.3150925},
doi = {10.1145/3150919.3150925},
abstract = {A hyperspectral image provides a multidimensional data consisting of hundreds of spectral dimensions. Even though having an abundance of spectral might seem favorable, classification of hyperspectral data tends to collide with the curse of dimensionality. Therefore, reducing the number of dimensions before classification is always favorable. For this research, the feature extraction method will consist of a nonlinear manifold learning technique named locally linear embedding (LLE). Additionally, another problem that we attempt to overcome is the high computational time required to run manifold learning methods. In order to help overcome this problem, this research compares one implementation of LLE against an improved version that runs much quicker than the original version.},
booktitle = {Proceedings of the 6th ACM SIGSPATIAL Workshop on Analytics for Big Geospatial Data},
pages = {37–41},
numpages = {5},
keywords = {Big-data, Hyperspectral, Locally Linear Embedding},
location = {Redondo Beach, CA, USA},
series = {BigSpatial'17}
}

@inproceedings{10.1145/3205977.3205998,
author = {Colombo, Pietro and Ferrari, Elena},
title = {Access Control in the Era of Big Data: State of the Art and Research Directions},
year = {2018},
isbn = {9781450356664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205977.3205998},
doi = {10.1145/3205977.3205998},
abstract = {Data security and privacy issues are magnified by the volume, the variety, and the velocity of Big Data and by the lack, up to now, of a standard data model and related data manipulation language. In this paper, we focus on one of the key data security services, that is, access control, by highlighting the differences with traditional data management systems and describing a set of requirements that any access control solution for Big Data platforms may fulfill. We then describe the state of the art and discuss open research issues.},
booktitle = {Proceedings of the 23nd ACM on Symposium on Access Control Models and Technologies},
pages = {185–192},
numpages = {8},
keywords = {access control, big data, privacy, NOSQL data management systems},
location = {Indianapolis, Indiana, USA},
series = {SACMAT '18}
}

@inproceedings{10.1145/3148055.3148060,
author = {Boufea, Aikaterini and Finkers, Richard and van Kaauwen, Martijn and Kramer, Mark and Athanasiadis, Ioannis N.},
title = {Managing Variant Calling Files the Big Data Way: Using HDFS and Apache Parquet},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148060},
doi = {10.1145/3148055.3148060},
abstract = {Big Data has been seen as a remedy for the efficient management of the ever-increasing genomic data. In this paper, we investigate the use of Apache Spark to store and process Variant Calling Files (VCF) on a Hadoop cluster. We demonstrate Tomatula, a software tool for converting VCF files to Apache Parquet storage format, and an application to query variant calling datasets. We evaluate how the wall time (i.e. time until the query answer is returned to the user) scales out on a Hadoop cluster storing VCF files, either in the original flat-file format, or using the Apache Parquet columnar storage format. Apache Parquet can compress the VCF data by around a factor of 10, and supports easier querying of VCF files as it exposes the field structure. We discuss advantages and disadvantages in terms of storage capacity and querying performance with both flat VCF files and Apache Parquet using an open plant breeding dataset. We conclude that Apache Parquet offers benefits for reducing storage size and wall time, and scales out with larger datasets.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {219–226},
numpages = {8},
keywords = {variant calling, apache parquet, apache spark, tomatula, hdfs, bioinformatics, big data, hadoop},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.1145/3341620.3341639,
author = {Jia, Haitian and Jia, Chun},
title = {Construction and Application of Data Standard in Big Data Environment},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341639},
doi = {10.1145/3341620.3341639},
abstract = {School informatization Construction has gone through 10 years, Multiple systems operate independently. Meanwhile, the role of unstructured data resources about security monitoring, smart card system, online course resources has become more and more important in the construction of intelligent campus. According to the present condition of the school, this paper give a data model for the information construction of colleges and universities. Solving the System Fusion put to use Hadoop distributed system Architecture between structured data and unstructured data, Providing basis for data analysis and decision-making. Big data will become the evolutive direction of Intelligent Campus in the next few years, It will promote the construction of school informatization about Construction and implementation.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {121–124},
numpages = {4},
keywords = {Spark, Hadoop, Data Standard, Unstructure data},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/2351316.2351320,
author = {Zhang, Junbo and Li, Tianrui and Pan, Yi},
title = {Parallel Rough Set Based Knowledge Acquisition Using MapReduce from Big Data},
year = {2012},
isbn = {9781450315470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351316.2351320},
doi = {10.1145/2351316.2351320},
abstract = {Nowadays, with the volume of data growing at an unprecedented rate, big data mining and knowledge discovery have become a new challenge. Rough set theory for knowledge acquisition has been successfully applied in data mining. The recently introduced MapReduce technique has received much attention from both scientific community and industry for its applicability in big data analysis. To mine knowledge from big data, we present parallel rough set based methods for knowledge acquisition using MapReduce in this paper. Comprehensive experimental evaluation on large data sets shows that the proposed parallel methods can effectively process big data.},
booktitle = {Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
pages = {20–27},
numpages = {8},
keywords = {knowledge acquisition, rough sets, big data, MapReduce},
location = {Beijing, China},
series = {BigMine '12}
}

@inproceedings{10.1145/2808797.2808841,
author = {O'Halloran, Sharyn and Maskey, Sameer and McAllister, Geraldine and Park, David K. and Chen, Kaiping},
title = {Big Data and the Regulation of Financial Markets},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2808841},
doi = {10.1145/2808797.2808841},
abstract = {The development of computational data science techniques in natural language processing (NLP) and machine learning (ML) algorithms to analyze large and complex textual information opens new avenues to study intricate processes, such as government regulation of financial markets, at a scale unimaginable even a few years ago. This paper develops scalable NLP and ML algorithms (classification, clustering and ranking methods) that automatically classify laws into various codes/labels, rank feature sets based on use case, and induce best structured representation of sentences for various types of computational analysis. The results provide standardized coding labels of policies to assist regulators to better understand how key policy features impact financial markets.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {1118–1124},
numpages = {7},
keywords = {machine learning, natural language processing, big data, political economics, financial regulation},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/3220228.3220238,
author = {Galletta, Antonino and Allam, Salma and Carnevale, Lorenzo and Bekri, Moulay Ali and Ouahbi, Rachid El and Villari, Massimo},
title = {An Innovative Methodology for Big Data Visualization in Oceanographic Domain},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220238},
doi = {10.1145/3220228.3220238},
abstract = {Nowadays, thanks to new technologies, we are observing an explosion of data in different fields such as clinical, environmental and so on. In this context, a typical example of the well-known Big Data problem is represented by visualization. In this work, we propose an innovative platform for managing the oceanographic acquisitions. More specifically, we present two innovative visualization techniques: general overview and site specific observation. Experiments prove the goodness of the proposed system in terms both of performance and user experience.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {103–107},
numpages = {5},
keywords = {microservices, big data visualization, IoT, acidification, big data, oceanography, geolocation},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.1145/3211890.3211914,
author = {Qui\~{n}ones, Eduardo and Bertogna, Marko and Hadad, Erez and Ferrer, Ana Juan and Chiantore, Luca and Reboa, Alfredo},
title = {Big Data Analytics for Smart Cities: The H2020 CLASS Project},
year = {2018},
isbn = {9781450358491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211890.3211914},
doi = {10.1145/3211890.3211914},
booktitle = {Proceedings of the 11th ACM International Systems and Storage Conference},
pages = {130},
numpages = {1},
location = {Haifa, Israel},
series = {SYSTOR '18}
}

@inproceedings{10.1145/3219104.3229288,
author = {Cheng, Yanzhe and Liu, Fang Cherry and Jing, Shan and Xu, Weijia and Chau, Duen Horng},
title = {Building Big Data Processing and Visualization Pipeline through Apache Zeppelin},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3229288},
doi = {10.1145/3219104.3229288},
abstract = {Big data analytics pipeline becomes popular for large volume data processing, Apache Zeppelin provides an integrated environment for data ingestion, data discovery, data analytics and data visualization and collaboration with an extended framework which allows different programming languages and data processing back ends to be plugged in. The supported languages include Scala, Python, SQL, and Shell script as well as big data processing back ends including Hadoop, Spark and Hive. With the necessary tool sets, an interactive and dynamic data analysis can be done on the fly with heterogeneous programming interfaces. Although Zeppelin is great for code development and interactive analysis with small scale data set for proof-of-concept or use-case presentations, running the data processing pipeline in the batch mode is still needed for performance, robustness to fit in an automated workflow in some cases. We are developing a tool to convert Zeppelin notebook into a workflow with a set of codes that can run in a batch mode through command line interface without requiring running Zeppelin, so that the prototype code can be seamlessly deployed on the production cluster after demo stage. The entire workflow can be preserved, configured manually and run automatically. Zeppelin also provides a flexible way to integrate the visualization functionality, another contribution of this paper is to extend the Zeppelin's existing built-in visualization component for D3Network. With two added features described above, Zeppelin can help users to develop big data pipeline and visualizing graph data quickly and efficiently.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {57},
numpages = {7},
keywords = {Scala, Batch processing, Spark, Big Data, Visualization},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@inproceedings{10.5555/2390524.2390640,
author = {Zhang, Ce and Niu, Feng and R\'{e}, Christopher and Shavlik, Jude},
title = {Big Data versus the Crowd: Looking for Relationships in All the Right Places},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.},
booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1},
pages = {825–834},
numpages = {10},
location = {Jeju Island, Korea},
series = {ACL '12}
}

@article{10.1145/3383464,
author = {Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv},
title = {SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3383464},
doi = {10.1145/3383464},
abstract = {Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {46},
numpages = {40},
keywords = {Big data, big data analytics application, service level agreement, service layer, SLA, SLA metrics}
}

@inproceedings{10.1145/2627770.2627774,
author = {Loebman, Sarah and Ortiz, Jennifer and Choo, Lee Lee and Orr, Laurel and Anderson, Lauren and Halperin, Daniel and Balazinska, Magdalena and Quinn, Thomas and Governato, Fabio},
title = {Big-Data Management Use-Case: A Cloud Service for Creating and Analyzing Galactic Merger Trees},
year = {2014},
isbn = {9781450329972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627770.2627774},
doi = {10.1145/2627770.2627774},
abstract = {We present the motivation, design, implementation, and preliminary evaluation for a service that enables astronomers to study the growth history of galaxies by following their `merger trees' in large-scale astrophysical simulations. The service uses the Myria parallel data management system as back-end and the D3 data visualization library within its graphical front-end. We demonstrate the service at the workshop on a ~5TB dataset.},
booktitle = {Proceedings of Workshop on Data Analytics in the Cloud},
pages = {1–4},
numpages = {4},
keywords = {Cloud service, Myria, parallel data management, astronomy},
location = {Snowbird, UT, USA},
series = {DanaC'14}
}

@inproceedings{10.5555/3320516.3321154,
author = {Uhrmacher, Adelinde and Sanchez, Susan M.},
title = {Session Details: Advanced Tutorials: Inferential Big Data},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/3220199.3220200,
author = {Zhengjun, Pan and Lianfen, Zhao},
title = {Application Research of Big Data Experimental Teaching Platform Based on OpenStack},
year = {2018},
isbn = {9781450364263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220199.3220200},
doi = {10.1145/3220199.3220200},
abstract = {In view of the traditional cloud computing and big data experimental teaching platform in Colleges and universities can not meet the requirements of practical teaching, based on open source OpenStack, we designed and implemented an experimental teaching platform for cloud computing and big data.The platform is based on the effective use of existing laboratory soft hardware resources, through virtualization technology, can realize the flexible allocation of resources and effective management, realize the effective use of resources to maximize the sharing and meet the problem of cloud computing and big data management of experiment teaching and development and testing, and can also provide some support for cloud computing and big data scientific research.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Computing},
pages = {1–4},
numpages = {4},
keywords = {OpenStack, Cloud computing, experimental teaching platform, big data},
location = {Shenzhen, China},
series = {ICBDC '18}
}

@inproceedings{10.5555/3213069.3213074,
author = {Abidi, Faiz and Polys, Nicholas and Rajamohan, Srijith and Arsenault, Lance and Mohammed, Ayat},
title = {Remote High Performance Visualization of Big Data for Immersive Science},
year = {2018},
isbn = {9781510860162},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Remote visualization has emerged as a necessary tool in the analysis of big data. High-performance computing clusters can provide several benefits in scaling to larger data sizes, from parallel file systems to larger RAM profiles to parallel computation among many CPUs and GPUs. For scalable data visualization, remote visualization tools and infrastructure is critical where only pixels and interaction events are sent over the network instead of the data. In this paper, we present our pipeline using VirtualGL, TurboVNC, and ParaView to render over 40 million points using remote HPC clusters and project over 26 million pixels in a CAVE-style system. We benchmark the system by varying the video stream compression parameters supported by TurboVNC and establish some best practices for typical usage scenarios. This work will help research scientists and academicians in scaling their big data visualizations for remote, real-time interaction.},
booktitle = {Proceedings of the High Performance Computing Symposium},
articleno = {5},
numpages = {12},
keywords = {HPC, paraview, remote rendering, big data, CAVE},
location = {Baltimore, Maryland},
series = {HPC '18}
}

@inproceedings{10.1145/3240117.3240139,
author = {Ben Amor, Fatma and Mkadmi, Abderrazak},
title = {Les Archives \`{a} l'\`{E}re Des Big Data: Les Enjeux de l'Archivage Des Donn\'{e}es Num\'{e}riques Massives},
year = {2018},
isbn = {9781450364515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240117.3240139},
doi = {10.1145/3240117.3240139},
abstract = {Big Data is now a cross-cutting research topic in all disciplines related to digital as content and as technology too. They lie in the intersection between all the massive data captured, obtained, created by different means and of various origins. They represent an advanced step in the re-development of information, particularly concerning data management and data retention issues. This upheaval due to these massive data has touched all sectors, particularly the archives. Indeed, given their volume, their speed of creation and their importance to the social, economic, scientific and cultural actors, the sorting, the treatment and the conservation of these data Massive e orts require memory capacity, new methods and techniques for processing, analyzing and managing particular flows. We will try in this article to bring some elements of primary answers on the modalities of generative multiplication. exponential of numerical data and archive in a mass of numeric data, data that are in flux and that occur in a sup speed.},
booktitle = {Proceedings of the 1st International Conference on Digital Tools &amp; Uses Congress},
articleno = {18},
numpages = {6},
keywords = {long-term conservation, big data access, Digital archiving, Big Data},
location = {Paris, France},
series = {DTUC '18}
}

@inproceedings{10.1145/3090354.3090388,
author = {Sara, Amghar and Yassine, Tabaa and Abdellatif, Medouri},
title = {Secure Confidential Big Data Sharing in Cloud Computing Using KP-ABE},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090388},
doi = {10.1145/3090354.3090388},
abstract = {In recent year, cloud computing and big data have become the hottest research topics which attract the attention of researchers due to its potential to provide major benefits to the industry and the community. Currently, the Organizations have a new option to outsource their massive data in the cloud without having to worry about the size of data or the capacity of memory. However, moving confidential and sensitive data from trusted domain of the data owners to public cloud will cause various security and privacy risks. Furthermore, the increasing amount of big data outsourced in the cloud increases the chance of breaching the privacy and security of these data. Despite all the research that has been done in this area, big data storage security and privacy remains one of the major concerns of organizations that adopt the cloud computing and big data technologies. Thus to ensure better data security we need for focus on two major problems which are the access control and encryption policies.In this paper, we propose a new hybrid model that enhances the security and privacy of big data shared in cloud environment using an access control scheme based on KP-ABE and authentication system. Our approach provides a flexible fine-grained access control of big data stored and shared in the cloud computing such that the encrypted data can only be accessed by authorized users.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {33},
numpages = {4},
keywords = {Access Control, KP-ABE, Attributed Based Encryption, Big Data Security, Authentication, Data Encryption, Cloud Computing},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3254069,
author = {McCoy, Kathleen},
title = {Session Details: Big Data and Blind Users},
year = {2016},
isbn = {9781450341240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254069},
doi = {10.1145/3254069},
booktitle = {Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility},
location = {Reno, Nevada, USA},
series = {ASSETS '16}
}

@inproceedings{10.1145/2537148.2537159,
author = {Soran, Ahmet and Akdemir, Furkan Mustafa and Yuksel, Murat},
title = {Parallel Routing on Multi-Core Routers for Big Data Transfers},
year = {2013},
isbn = {9781450325752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2537148.2537159},
doi = {10.1145/2537148.2537159},
abstract = {Over the last several years, the deployment of multi-core routers has grown rapidly. However, big data transfers are not leveraging the powerful multi-core routers to the extent possible, particularly in the key function of routing. Our main goal is to find a way to use these cores more effectively and efficiently in routing the big data transfers. We propose a novel approach to parallelize data transfers by using each core in the routers to calculate a separate shortest path. For each core, we generate a different "substrate" topology in order to allow shortest path calculations to find a different end-to-end (e2e) path. By abstracting a different topology for each core, we indirectly steer each core to calculate a different e2e path in parallel to each other. The e2e big data transfers can use these shortest paths obtained from each substrate topology to increase the total throughput. We present an initial evaluation of the concept.},
booktitle = {Proceedings of the 2013 Workshop on Student Workhop},
pages = {35–38},
numpages = {4},
keywords = {load balancing, multi-core routers, multi-path routing},
location = {Santa Barbara, California, USA},
series = {CoNEXT Student Workhop '13}
}

@inproceedings{10.1145/3325773.3325779,
author = {Li, Zhiyong and Li, Tao and Zhu, Fangdong},
title = {An Online Password Guessing Method Based on Big Data},
year = {2019},
isbn = {9781450372114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325773.3325779},
doi = {10.1145/3325773.3325779},
abstract = {Password authentication is the most widely used authentication method in information systems. The traditional proactive password detection method is generally implemented by counting password length, character class number and computing password information entropy to improve password security. However, passwords that pass proactive password detection do not represent that they are secure. In this paper, based on the research of the characteristics of password distribution under big data, we propose an online password guessing method, which collects a dataset of guessing passwords composed of weak passwords, high frequency passwords and personal information related passwords. It is used to guess the 13k password dataset leaked in China's largest ticketing website, China Railways 12306 website. The experimental results show that even if our guess object has passed the strict proactive password detection, we can construct a guessing password dataset contain only 100 passwords, and effectively guess 4.84% of the passwords.},
booktitle = {Proceedings of the 2019 3rd International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence},
pages = {59–62},
numpages = {4},
keywords = {password guessing attack, proactive password check, Password security},
location = {Male, Maldives},
series = {ISMSI 2019}
}

@article{10.1145/1577840.1577845,
author = {Klein, A. and Lehner, W.},
title = {Representing Data Quality in Sensor Data Streaming Environments},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1577840.1577845},
doi = {10.1145/1577840.1577845},
abstract = {Sensors in smart-item environments capture data about product conditions and usage to support business decisions as well as production automation processes. A challenging issue in this application area is the restricted quality of sensor data due to limited sensor precision and sensor failures. Moreover, data stream processing to meet resource constraints in streaming environments introduces additional noise and decreases the data quality. In order to avoid wrong business decisions due to dirty data, quality characteristics have to be captured, processed, and provided to the respective business task. However, the issue of how to efficiently provide applications with information about data quality is still an open research problem.In this article, we address this problem by presenting a flexible model for the propagation and processing of data quality. The comprehensive analysis of common data stream processing operators and their impact on data quality allows a fruitful data evaluation and diminishes incorrect business decisions. Further, we propose the data quality model control to adapt the data quality granularity to the data stream interestingness.},
journal = {J. Data and Information Quality},
month = {sep},
articleno = {10},
numpages = {28},
keywords = {Data stream processing, smart items, data quality}
}

@article{10.1145/2968332,
author = {Geisler, Sandra and Quix, Christoph and Weber, Sven and Jarke, Matthias},
title = {Ontology-Based Data Quality Management for Data Streams},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2968332},
doi = {10.1145/2968332},
abstract = {Data Stream Management Systems (DSMS) provide real-time data processing in an effective way, but there is always a tradeoff between data quality (DQ) and performance. We propose an ontology-based data quality framework for relational DSMS that includes DQ measurement and monitoring in a transparent, modular, and flexible way. We follow a threefold approach that takes the characteristics of relational data stream management for DQ metrics into account. While (1) Query Metrics respect changes in data quality due to query operations, (2) Content Metrics allow the semantic evaluation of data in the streams. Finally, (3) Application Metrics allow easy user-defined computation of data quality values to account for application specifics. Additionally, a quality monitor allows us to observe data quality values and take counteractions to balance data quality and performance. The framework has been designed along a DQ management methodology suited for data streams. It has been evaluated in the domains of transportation systems and health monitoring.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {18},
numpages = {34},
keywords = {Data streams, data quality control, ontologies, data quality assessment}
}

@inproceedings{10.1145/3368691.3368713,
author = {Mouchili, Mama Nsangou and Atwood, John William and Aljawarneh, Shadi},
title = {Call Data Record Based Big Data Analytics for Smart Cities},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368713},
doi = {10.1145/3368691.3368713},
abstract = {A Call Data Record (CDR) is produced for each call (or other interaction) handled by a telephone company. CDRs have traditionally been used for billing and network engineering purposes. Given how mobile phones have become an integral part of - and have undoubtedly transformed - the everyday life of a great part of the earth's population, and given that 90% or more of phone subscriptions are registered in any city, if CDRs are collected from the mobile phone and cellular networks, and combined with other data from the organization or from elsewhere, this will allow managers to identify trends, detect patterns, and glean other valuable findings from the data.This paper highlights the applicability of CDR-based big data, to gathering such insights. By stitching events together into clusters of related events across runtime environments and/or geographies, raw data becomes business insight to make decisions either to understand customer needs, to mitigate problems, or to ultimately gain a competitive advantage.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {22},
numpages = {7},
keywords = {big data, CDR, Hadoop, traffic congestion, insights, traffic management, mining algorithms, data analytics, random forest, city management, smart city, JSON},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/2609876.2609878,
author = {Gersh, John R. and Bos, Nathan},
title = {Cognitive and Organizational Challenges of Big Data in Cyber Defense},
year = {2014},
isbn = {9781450329385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609876.2609878},
doi = {10.1145/2609876.2609878},
abstract = {The cognitive and organizational challenges associated with `Big Data' have not received much research attention. We have begun an interview study of analysts who work in the computer network (cyber) defense (CND) area and have experienced changes in data scale affecting their analytical work. Our goal is to understand any changes in analysts' mental models of their data and their domain. We used a qualitative inquiry method, starting with relatively open-ended questions. Our interview protocol also asked analysts to describe critical incidents related to data use, and probed for previously-identified cognitive biases that may affect analysis in this domain.},
booktitle = {Proceedings of the 2014 Workshop on Human Centered Big Data Research},
pages = {4–8},
numpages = {5},
keywords = {big data, computer network defense, organizations, Mental models, cognition, data analysis},
location = {Raleigh, NC, USA},
series = {HCBDR '14}
}

@inproceedings{10.1145/3428363.3428369,
author = {Meem, Jannat Ara and Ahmad, Farzana Yasmin and Adnan, Muhammad Abdullah},
title = {Distributed Principal Component Analysis for Real-Time Big Data Processing},
year = {2020},
isbn = {9781450389051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428363.3428369},
doi = {10.1145/3428363.3428369},
abstract = {Real-time big data analytics, which is the combination of real-time analytics and big data, works on processing large scale data as it arrives and strives to obtain insights from it without exceeding a limited time period. Massive amount of data is being generated every moment through web sites, social networks, scientific experiments, etc. which is stored in the cloud. When decision making requires an insight of the raw data in real-time (often by applying machine learning algorithms such as dimensionality reduction), these algorithms fail to analyze these incessantly flowing high volume data i.e dynamic big data. Our work proposes a variant of scalable principal component analysis (PCA) which is suited for real-time big data applications. We maintain a sliding window (representing incoming data in real-time) over the most recent data and project every incoming data into lower dimensional subspace. Our goal is to minimize the reconstruction error of the output from the input and keep updating the principal components depending on it. We have implemented our scalable algorithm on popular Spark framework for distributed platform and performed extensive experiments on datasets from a variety of real-time applications e.g. activity recognition, customer expenditure, etc. Furthermore, we have demonstrated that our algorithm can capture the changing distributions of real-life datasets, thus enabling real-time PCA. We have also compared the performances of distributed and non-distributed versions of our algorithm over a variety of window sizes and showed that our distributed algorithm is scalable and performs better when window size and target dimension increase.},
booktitle = {7th International Conference on Networking, Systems and Security},
pages = {89–99},
numpages = {11},
keywords = {Big Data Analytics, Real-time Processing, Distributed Algorithm.},
location = {Dhaka, Bangladesh},
series = {7th NSysS 2020}
}

@article{10.5555/3417639.3417658,
author = {DePratti, Roland},
title = {Jupyter Notebooks versus a Textbook in a Big Data Course},
year = {2020},
issue_date = {April 2020},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {35},
number = {8},
issn = {1937-4771},
abstract = {In building curriculum in new areas of computer science, often the tools introduced in the course are an important component. This is especially true in the area of big data, where the complexity of the problems the area tackles is high. In the 4 years since its inception, my big data course has gone through two major redesigns and has settled on a tool set including: the Hadoop platform, Spark processing engine, the Python programming language, Eclipse IDE, and Jupyter Notebooks. Many of the changes were driven by input from professional peers on big data teams, who were struggling with the complexity resulting from the low-level programming model used by MapReduce. Jupyter Notebook, a type of computational notebook, was added to the course to introduce students to the Python programming language. Data scientists and researchers have found computational notebooks an effective tool to manage their work by providing a way to track their thinking process, their code, and conclusions in one web document. To assess the effectiveness of using Jupyter Notebook in a big data course, students' views on the use of computational notebooks and traditional textbooks were captured and statistically analyzed.},
journal = {J. Comput. Sci. Coll.},
month = {aug},
pages = {208–220},
numpages = {13}
}

@inproceedings{10.1145/3010089.3010100,
author = {Seref, Berna and Bostanci, Erkan},
title = {Opportunities, Threats and Future Directions in Big Data for Medical Wearables},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010100},
doi = {10.1145/3010089.3010100},
abstract = {Big data term has started to take part in our lives as a result of advanced technologies that produce unprecedented amount of data. Big data management has a critical role on smart decision making, new product developments and optimized offerings. Wearable devices and Internet of Things (IoT) devices are constantly adding more to this data, creating a crucial need for efficient storage, processing and analysis tools. These devices are used in many different areas including health, fitness, and entertainment. If we focus on health area, there are some medical wearable devices for tracking of cardiovascular, glaucoma, diabetes diseases etc. Literature presents a huge number of methods, algorithms and platforms which are carried on big data to manage and analyze it effectively. This paper aims to present a review of the developments in big data and wearable devices, and provide the state-of-the art approaches and developments on them. We summarize current challenges as well as defining new ones and then suggest solutions for these challenges.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {15},
numpages = {5},
keywords = {solutions, wearable devices, medical wearables, challenges, Big data},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/3053600.3053621,
author = {Apte, Varsha},
title = {Recent Trends in Performance Modeling of Big Data Systems},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053621},
doi = {10.1145/3053600.3053621},
abstract = {With the advent of big data through social media and continuous creation of digital footprints through various mobile devices, special-purpose programming models were developed that would make it easy to write programs to process such data. MapReduce and its Hadoop implementation is one of the most popular platforms for writing such programs. The MapReduce framework involves a "map" phase where various tasks work in parallel for intermediate processing of data and a "reduce" phase where again various tasks work in parallel to extract information from this processed data. Performance modeling of such systems will need different approaches than are used for traditional multi-threaded multi-core systems supporting Web applications, primarily because the dependencies and synchronization required between various tasks is not easily expressible using standard queuing network models. In this talk we will review work done by researchers to address this modeling problem. The work done encompasses first-principles calculations of execution time completion, queuing network models, and finally, simulation. We will review these efforts as well as highlight opportunities for further work in this area.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {105},
numpages = {1},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/3434581.3434720,
author = {Zhang, Renjie and Ren, Chuanrong and Yang, Weishu and Wang, Yan and Ding, Qian},
title = {Research on Big Data Classification Technology Based on Deep Learning},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434720},
doi = {10.1145/3434581.3434720},
abstract = {Based on the current digital industry background, the production processes become centralized and closely related to each other. If industrial production fails, it will bring great damage to it, which makes it more and more important to accurately identify the status of industrial equipment and repair it in real time by collecting time series data with industrial sensors. However, due to the large number, variety and high-frequency sampling of sensors, industrial big data has certain complexity, i.e. high spatial dimension, complex logical relationship, changeable rules, large amount of data and so on. At present, end-to-end algorithms of deep learning are widely used in many fields. From the perspective of application of deep learning, this paper studies its application in industrial big data time series classification, and analyzes the characteristics of industrial data and the challenges of industrial time series classification from three aspects: accurate classification, efficient classification and incremental learning.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {718–721},
numpages = {4},
keywords = {Data classification, Big data, Deep learning},
location = {Weihai City, China},
series = {ICASIT 2020}
}

@inproceedings{10.1145/3175603.3175613,
author = {Cao, Mengmeng and Guo, Chaoyou},
title = {Key Technologies of Big Data and Its Development in Intelligent Ship},
year = {2017},
isbn = {9781450353588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175603.3175613},
doi = {10.1145/3175603.3175613},
abstract = {The application of big data techniques will contribute to the transforming and upgrading in shipbuilding industry and produce a profound influence in the development of intelligent ship. An overall framework of shipbuilding big data platform is established in this paper, which is based on the relationships among the intelligent ship, cloud computing and big data. Then key techniques of big data are discussed in four aspects, including data generation, data acquisition, data storage, and data analysis. Finally, an overview of the architecture of big data and relevant techniques are demonstrated.},
booktitle = {Proceedings of the 2017 International Conference on Robotics and Artificial Intelligence},
pages = {61–65},
numpages = {5},
keywords = {data acquisition, big data, intelligent ship, data mining, data storage, data visualization},
location = {Shanghai, China},
series = {ICRAI 2017}
}

@inproceedings{10.1145/3322134.3322150,
author = {Li, Jian and Yang, Qiang and Zou, Xiaohui},
title = {Big Data and Higher Vocational and Technical Education: Green Food and Its Industry Orientation},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322150},
doi = {10.1145/3322134.3322150},
abstract = {This paper aims to focus on a series of issues related to green food and its industry orientation through big data and higher vocational and technical education technologies. The method consists of three steps the first step is to lock in the research objectives, namely: a series of problems in green food and its industry orientation; the second step is to distinguish three types of problems, namely: the first category is a large probability event, the predictable problem is directly solved by machine learning; the second category is a small probability event, which relies solely on machine automatic processing or batch processing can't solve problems, using a variety of human-computer interaction methods to deal with; the third category is cases and very special exceptions, usually only rely on the corresponding human experts to find the ways, and then, through the knowledge acquisition path to develop a dedicated artificial intelligence system. The third step is to incorporate them into the actual classroom teaching practice, or find the special daily life circle, it can be artificially set, even virtual, and tested in various application environments. The result is: through the teachers and students to continue to explore a series of issues related to green food and its industry forming the characteristics of big data and higher vocational and technical education technology. The significance is that is, a series of results of the research of the smart system can be directly used to study a series of problems focusing on green food and its industrial orientation.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {118–123},
numpages = {6},
keywords = {Data Management, Big Data Applications, E-education},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@article{10.14778/2994509.2994519,
author = {Liu, Hai and Xiao, Dongqing and Didwania, Pankaj and Eltabakh, Mohamed Y.},
title = {Exploiting Soft and Hard Correlations in Big Data Query Optimization},
year = {2016},
issue_date = {August 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2994509.2994519},
doi = {10.14778/2994509.2994519},
abstract = {Big data infrastructures are increasingly supporting datasets that are relatively structured. These datasets are full of correlations among their attributes, which if managed in systematic ways would enable optimization opportunities that otherwise will be missed. Unlike relational databases in which discovering and exploiting the correlations in query optimization have been extensively studied, in big data infrastructures, such important data properties and their utilization have been mostly abandoned. The key reason is that domain experts may know many correlations but with a degree of uncertainty (fuzziness or softness). Since the data is big, it is very challenging to validate such correlations, judge their worthiness, and put strategies for utilizing them in query optimization. Existing techniques for exploiting soft correlations in RDBMSs, e.g., BHUNT, CORDS, and CM, are heavily tailored towards optimizing factors inherent in relational databases, e.g., predicate selectivity and random I/O accesses of secondary indexes, which are issues not applicable to big data infrastructures, e.g., Hadoop.In this paper, we propose the EXORD system to fill in this gap by exploiting the data's correlations in big data query optimization. EXORD supports two types of correlations; hard correlations---which are guaranteed to hold for all data records, and soft correlations---which are expected to hold for most, but not all, data records. We introduce a new three-phase approach for (1) Validating and judging the worthiness of soft correlations, (2) Selecting and preparing the soft correlations for deployment by specially handling the violating data records, and (3) Deploying and exploiting the correlations in query optimization. We propose a novel cost-benefit model for adaptively selecting the most beneficial soft correlations w.r.t a given query workload while minimizing the introduced overhead. We show the complexity of this problem (NP-Hard), and propose a heuristic to efficiently solve it in a polynomial time. EXORD can be integrated with various state-of-art big data query optimization techniques, e.g., indexing and partitioning. EXORD prototype is implemented as an extension to the Hive engine on top of Hadoop. The experimental evaluation shows the potential of EXORD in achieving more than 10x speedup while introducing minimal storage overheads.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1005–1016},
numpages = {12}
}

@inproceedings{10.1145/2769458.2769485,
author = {Kacsuk, Peter},
title = {Enabling Distributed Simulations Using Big Data and Clouds},
year = {2015},
isbn = {9781450335836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2769458.2769485},
doi = {10.1145/2769458.2769485},
abstract = {Large scientific simulation projects should enable the collaboration of large scientific consortia where members are located in different countries and even continents storing their usually very large data set in different kind of storages. Therefore state-of-the-art simulations should process very large set of data stored in a distributed way in different kind of storages located in all over the world. As the data is big its processing time can be intolerably long. To reduce processing time we have to use large infrastructure that enables the exploitation of parallel processing wherever it is possible in the simulation process. Clouds provide the required large set of computing resources and hence we need simulation environments that enable the easy exploitation of cloud resources. This keynote speech introduces a cloud-oriented simulation platform that enables the exploitation of large cloud resources as well as accessing all the major data storage types. This platform called as WS-PGRADE/gUSE is intensively used in many EU FP7 projects among them in CloudSME where the main target is to enable particularly small and medium-sized manufacturing and engineering companies (SMEs), to use state of the art simulation technology as a Service (SaaS, one-stop-shop, pay-per-use) in the cloud.In this talk we will show the main features of WS-PGRADE/gUSE that enable the use of cloud and large data resources to conduct distributed simulations. First, the workflow creation and execution mechanism will be explained. Then the DCI Bridge service will be shown that enables the exploitation of many independent cloud resources in parallel. Finally, the Data Avenue service that enables the access and transfer of large data among various types of data storages will be described. These services together enable the creation of simulation workflows that are easily portable among different distributed computing and data infrastructures including various types of clouds and cloud storages. At the end of the talk some concrete examples from the CloudSME project (www.cloudsme.eu) will highlight the main advantages of using the platform.},
booktitle = {Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {125–126},
numpages = {2},
keywords = {distributed simulation, simulation, big data},
location = {London, United Kingdom},
series = {SIGSIM PADS '15}
}

@inproceedings{10.1145/3307681.3325410,
author = {Fox, Geoffrey C.},
title = {Perspectives on High-Performance Computing in a Big Data World},
year = {2019},
isbn = {9781450366700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307681.3325410},
doi = {10.1145/3307681.3325410},
abstract = {High-Performance Computing (HPC) and Cyberinfrastructure have played a leadership role in computational science even since the start of the NSF computing centers program. Thirty years ago parallel computing was a centerpiece of computer science research. Naively Big Data surely requires HPC to be processed, and transformational Big Data technology such as Hadoop and Spark exploit parallelism to success. Nevertheless, the HPC community does not appear to be thriving as a leader in Data Science while parallel computing is no longer a centerpiece. Some reasons for this are the dominant presence of Industry in technology futures and the universal fascination with Artificial Intelligence and Machine Learning. Maybe the pendulum will swing back a bit, but I expect the "AI first" philosophy to dominate in the foreseeable future. Thus I describe a future where HPC thrives in collaboration with Industry and AI. In particular, I discuss the promise of MLforHPC (AI for systems) and HPCforML (systems for AI).},
booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {145},
numpages = {1},
keywords = {computational science, HPC, data science, big data},
location = {Phoenix, AZ, USA},
series = {HPDC '19}
}

@inproceedings{10.1145/3407947.3407977,
author = {Wang, Bo and Chen, Zhiguang and Xiao, Nong},
title = {A Survey of System Scheduling for HPC and Big Data},
year = {2020},
isbn = {9781450376914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407947.3407977},
doi = {10.1145/3407947.3407977},
abstract = {In the rapidly expanding field of parallel processing, job schedulers act as the "operating systems" of the clusters, including modern big data architectures and supercomputing systems. Job schedulers manage and allocate system resources, dispatch the queued jobs, and control the execution of processes on the allocated resources. In this paper, we firstly make an introduction to the cluster schedulers. Then according to the scenarios, we make a comprehensive survey of schedulers for HPC and Big Data. We can conclude that most of these current schedulers are centralized, which means master assigns jobs to the slaves. We call this mode Push, which is different from our new idea that introduces Pull to the schedulers. We proposed a novel scheduling model that allow slaves to actively pull jobs from master to execute. By analyzing the execution time and resource requests of jobs in "Tianhe-II", we will clarify that scheduling based on Push &amp; Pull is a direction worthy of in-depth study in the future.},
booktitle = {Proceedings of the 2020 4th International Conference on High Performance Compilation, Computing and Communications},
pages = {178–183},
numpages = {6},
keywords = {Job scheduler, decentralized scheduling, big data cluster, high performance computing},
location = {Guangzhou, China},
series = {HP3C 2020}
}

@inproceedings{10.1145/2508859.2516701,
author = {Dong, Changyu and Chen, Liqun and Wen, Zikai},
title = {When Private Set Intersection Meets Big Data: An Efficient and Scalable Protocol},
year = {2013},
isbn = {9781450324779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508859.2516701},
doi = {10.1145/2508859.2516701},
abstract = {Large scale data processing brings new challenges to the design of privacy-preserving protocols: how to meet the increasing requirements of speed and throughput of modern applications, and how to scale up smoothly when data being protected is big. Efficiency and scalability become critical criteria for privacy preserving protocols in the age of Big Data. In this paper, we present a new Private Set Intersection (PSI) protocol that is extremely efficient and highly scalable compared with existing protocols. The protocol is based on a novel approach that we call oblivious Bloom intersection. It has linear complexity and relies mostly on efficient symmetric key operations. It has high scalability due to the fact that most operations can be parallelized easily. The protocol has two versions: a basic protocol and an enhanced protocol, the security of the two variants is analyzed and proved in the semi-honest model and the malicious model respectively. A prototype of the basic protocol has been built. We report the result of performance evaluation and compare it against the two previously fastest PSI protocols. Our protocol is orders of magnitude faster than these two protocols. To compute the intersection of two million-element sets, our protocol needs only 41 seconds (80-bit security) and 339 seconds (256-bit security) on moderate hardware in parallel mode.},
booktitle = {Proceedings of the 2013 ACM SIGSAC Conference on Computer &amp; Communications Security},
pages = {789–800},
numpages = {12},
keywords = {bloom filters, private set intersection},
location = {Berlin, Germany},
series = {CCS '13}
}

@inproceedings{10.1145/3321454.3321474,
author = {Yu, Bangbo and Zhao, Haijun},
title = {Research on the Construction of Big Data Trading Platform in China},
year = {2019},
isbn = {9781450366335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321454.3321474},
doi = {10.1145/3321454.3321474},
abstract = {As a new type of asset, the value of big data resources can only be realized in the transaction circulation. Establishing and improving the big data trading platform market system is a systematic project to transform data from resources into assets. Through the comparative analysis of several typical big data trading platform construction practices in China, this research finds some problems, such as unclear positioning of some platforms leading to overlapping functions, extensive data transaction, lack of unified data pricing methods, unclear data ownership. In addition, there is no difference between the data escrow transaction mode and the aggregate transaction mode, and the rights of the data supply parties cannot be guaranteed. And it also discusses how to promote the construction and improvement of China's big data trading market more systematically, normally and institutionally. Finally, it proposes to build local big data trading platforms according to local conditions, establish a data transaction system based on blockchain, establish a big data transaction pricing index system, and establish a big data standard system.},
booktitle = {Proceedings of the 2019 4th International Conference on Intelligent Information Technology},
pages = {107–112},
numpages = {6},
keywords = {Data assets, regulatory construction, big data trading platform},
location = {Da, Nang, Viet Nam},
series = {ICIIT '19}
}

