@inproceedings{10.1145/3404649.3404656,
author = {Yao, Xiaolin and Wei, Qi and Zhang, Qisong},
title = {Innovation of Undergraduate Education Mode of the Financial Management Major in Big Data Era},
year = {2020},
isbn = {9781450387781},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404649.3404656},
doi = {10.1145/3404649.3404656},
abstract = {The fast development of a new generation of information technology represented by artificial intelligence has brought a far-reaching impact to the financial management activities in enterprises. In the future, big data, artificial intelligence and robot process automation will be widely applied, these promoted the transformation of traditional financial management into intelligent financial management [1]. How to meet the demand of financial management transformation in the big data era is an important issue that all universities and colleges should consider. By integrating OBE educational concept and CDIO engineering education mode, this paper reforms the curriculum system and teaching contents of financial management major of undergraduate education in order to improve students' ability of big data analysis. With the help of school-enterprise cooperation resources and technological advantages, the undergraduate education can cultivate compound and intelligent financial management talents to meet the needs of enterprises in the era of big data},
booktitle = {Proceedings of the 2020 4th International Conference on E-Education, E-Business and E-Technology},
pages = {43–49},
numpages = {7},
keywords = {Big data, Artificial Intelligence, Robot Process Automation, OBE-CDIO Mode},
location = {Shanghai, China},
series = {ICEBT '20}
}

@inproceedings{10.1145/3378936.3378981,
author = {Ding, Jianwei and Guo, Xiaoyu and Chen, Zhouguo},
title = {Big Data Analyses of ZeroNet Sites for Exploring the New Generation DarkWeb},
year = {2020},
isbn = {9781450376907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378936.3378981},
doi = {10.1145/3378936.3378981},
abstract = {ZeroNet is a new generation typical dark web, which uses the Bitcoin encryption algorithm and BitTorrent technology to build a distributed and censored-resistant communication network. Based on our cumulative studies on the onion router, we present a big data analyses framework for automated multi-categorization of ZeroNet websites to facilitate analyst situational awareness of new content that emerges from this dynamic landscape. Over the last two years, our team has developed a distributed crawling infrastructure called ZeroCrawler that automatically crawls and updates ZeroNet websites in realtime. It stores data into a research repository designed to help better understand ZeroNet's hidden service ecosystem. The analysis component of our framework is called Automated Multi-Categorization Labeling (AMCL), which introduces a three-stage thematic labeling strategy: (1) it learns descriptive and discriminative keywords for different categories, and (2) get a probability distribution of the keywords for different categories, and then (3) uses these terms to map ZeroNet website content to several labels. We also present empirical results of AMCL and our ongoing experimentation with it, as we have gained experience applying it to the entirety of our ZeroNet repository, now over 3000 indexed websites. The experimental results show that AMCL can discover categories on previously unlabeled websites, and we discuss applications of AMCL in supporting various analyses and investigations of the ZeroNet websites.},
booktitle = {Proceedings of the 3rd International Conference on Software Engineering and Information Management},
pages = {46–52},
numpages = {7},
keywords = {Multi-categarization labels, Dark web conent analysis, ZeroNet},
location = {Sydney, NSW, Australia},
series = {ICSIM '20}
}

@inproceedings{10.1145/3356998.3365776,
author = {Zhang, Mingke and Guo, Danhuai and Hu, Jinyong and Jin, Wei},
title = {Risk Prediction and Assessment of Foodborne Disease Based on Big Data},
year = {2020},
isbn = {9781450369657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356998.3365776},
doi = {10.1145/3356998.3365776},
abstract = {In recent years, the outbreak of foodborne diseases has been on an upward trend clearly. It is of great significance for us to predict the outbreak of foodborne diseases accurately and conduct quantitative risk assessment timely. Traditional prediction methods based on a single data source have drawbacks such as complex prediction processes and inaccurate prediction results. In this article, we figure out the scientific issues of how to improve the temporal and spatial accuracy of foodborne disease outbreak risk prediction in Beijing. Firstly, we analyze the different foodborne disease risk factors caused by the spread of water pollution in Beijing, and study the methods of collecting and preprocessing multi-source data. Then, through the comparison of different regression models and parameters tuning, we propose the use of Gradient Boosting Regression (GBR) model as a multi-source data fusion model to predict the outbreak of foodborne disease. Finally, we use the risk map to detect and predict foodborne disease outbreak in different business districts of Beijing based on visualization techniques, aiming to provide prevention and control assessment for decision-makers quickly and precisely.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management},
articleno = {8},
numpages = {6},
keywords = {risk assessment, machine learning, big data, foodborne disease},
location = {Chicago, Illinois},
series = {EM-GIS '19}
}

@inproceedings{10.1145/2684200.2684333,
author = {Hassan, Sabri and Pernul, G\"{u}nther},
title = {Efficiently Managing the Security and Costs of Big Data Storage Using Visual Analytics},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684333},
doi = {10.1145/2684200.2684333},
abstract = {We are currently living in the age of big data with ever growing volumes of heterogeneous and fast moving data. Whether they are mobile devices, internal or external systems or cloud-based systems data is generated, stored, processed and distributed in many different systems. This leads to various information security and privacy risks. To address these issues, especially from the viewpoint of data management and data governance we propose a conceptual analysis model. Thereby, our model takes into account the dimension of data storage location together with their respective risks and costs while considering the strategic value and sensitivity of data assets. For demonstrating our approach we developed a visual analytics web application which is based on parallel sets visualizations. By being able to interactively explore the analysis dimensions users are supported in developing enhanced situational awareness for making decisions in the context of secure and economical data storage.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {180–184},
numpages = {5},
keywords = {Visual Analytics, Information Security, Distributed Systems, Big Data, Data Management, Data Governance, Cloud Storage},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1145/3372454.3372480,
author = {Wang, Zhuo and Zhou, Yanghui and Li, Gangmin},
title = {Anomaly Detection for Machinery by Using Big Data Real-Time Processing and Clustering Technique},
year = {2020},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372480},
doi = {10.1145/3372454.3372480},
abstract = {This paper aims to apply techniques of Big Data Analytics including K-Means Clustering to diagnose potential problems for offshore rotating machinery. The innovative methods are attempted in both Batch K-Means and Streaming K-Means. Their performances are compared with the conventional signal analysis method. Both K-Means models have a better performance on detecting significant mechanical faults as anomalies for offshore rotating machinery which can be considered as appropriate method for machine operational maintenance.},
booktitle = {Proceedings of the 2019 3rd International Conference on Big Data Research},
pages = {30–36},
numpages = {7},
keywords = {K-Means Clustering, Real-Time processing, Anomaly detection, Streaming K-Means Clustering, Big Data, Trouble Diagnose},
location = {Cergy-Pontoise, France},
series = {ICBDR 2019}
}

@inproceedings{10.1145/1595808.1595830,
author = {Bachmann, Adrian and Bernstein, Abraham},
title = {Software Process Data Quality and Characteristics: A Historical View on Open and Closed Source Projects},
year = {2009},
isbn = {9781605586786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595808.1595830},
doi = {10.1145/1595808.1595830},
abstract = {Software process data gathered from bug tracking databases and version control system log files are a very valuable source to analyze the evolution and history of a project or predict its future. These data are used for instance to predict defects, gather insight into a project's life-cycle, and additional tasks. In this paper we survey five open source projects and one closed source project in order to provide a deeper insight into the quality and characteristics of these often-used process data. Specifically, we first define quality and characteristics measures, which allow us to compare the quality and characteristics of the data gathered for different projects. We then compute the measures and discuss the issues arising from these observation. We show that there are vast differences between the projects, particularly with respect to the quality in the link rate between bugs and commits.},
booktitle = {Proceedings of the Joint International and Annual ERCIM Workshops on Principles of Software Evolution (IWPSE) and Software Evolution (Evol) Workshops},
pages = {119–128},
numpages = {10},
keywords = {data quality, closed source, version control system, case study, data characteristics, open source, bug tracker},
location = {Amsterdam, The Netherlands},
series = {IWPSE-Evol '09}
}

@inproceedings{10.1109/ISCA45697.2020.00036,
author = {Jang, Jaeyoung and Jung, Sung Jun and Jeong, Sunmin and Heo, Jun and Shin, Hoon and Ham, Tae Jun and Lee, Jae W.},
title = {A Specialized Architecture for Object Serialization with Applications to Big Data Analytics},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00036},
doi = {10.1109/ISCA45697.2020.00036},
abstract = {Object serialization and deserialization (S/D) is an essential feature for efficient communication between distributed computing nodes with potentially non-uniform execution environments. S/D operations are widely used in big data analytics frameworks for remote procedure calls and massive data transfers like shuffles. However, frequent S/D operations incur significant performance and energy overheads as they must traverse and process a large object graph. Prior approaches improve S/D throughput by effectively hiding disk or network I/O latency with computation, increasing compression ratio, and/or application-specific customization. However, inherent dependencies in the existing (de)serialization formats and algorithms eventually become the major performance bottleneck. Thus, we propose Cereal, a specialized hardware accelerator for memory object serialization. By co-designing the serialization format with hardware architecture, Cereal effectively utilizes abundant parallelism in the S/D process to deliver high throughput. Cereal also employs an efficient object packing scheme to compress metadata such as object reference offsets and a space-efficient bitmap representation for the object layout. Our evaluation of Cereal using both a cycle-level simulator and synthesizable Chisel RTL demonstrates that Cereal delivers 43.4x higher average S/D throughput than 88 other S/D libraries on Java Serialization Benchmark Suite. For six Spark applications Cereal achieves 7.97x and 4.81x speedups on average for S/D operations over Java built-in serializer and Kryo, respectively, while saving S/D energy by 227.75x and 136.28x.},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {322–334},
numpages = {13},
keywords = {hardware-software co-design, domain-specific architecture, object serialization, data analytics, apache spark},
location = {Virtual Event},
series = {ISCA '20}
}

@article{10.1145/2627534.2627558,
author = {Sharma, Abhishek B. and Ivan\v{c}i\'{c}, Franjo and Niculescu-Mizil, Alexandru and Chen, Haifeng and Jiang, Guofei},
title = {Modeling and Analytics for Cyber-Physical Systems in the Age of Big Data},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627558},
doi = {10.1145/2627534.2627558},
abstract = {In this position paper we argue that the availability of "big" monitoring data on Cyber-Physical Systems (CPS) is challenging the traditional CPS modeling approaches by violating their fundamental assumptions. However, big data alsobrings unique opportunities in its wake by enabling new modeling and analytics approaches as well as facilitating novel applications. We highlight a few key challenges andopportunities, and outline research directions for addressing them. To provide a proper context, we also summarize CPS modeling approaches, and discuss how modeling and analytics for CPS differs from general purpose IT systems.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {74–77},
numpages = {4}
}

@inproceedings{10.1145/2797433.2797467,
author = {Collins, Graham and Varilly, Hugh and Yoshinori, Tanabe},
title = {Pedagogical Lessons from an International Collaborative Big Data Undergraduate Research Project},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797467},
doi = {10.1145/2797433.2797467},
abstract = {This experience report covers the collaboration between UCL and NII Tokyo students in development of data analytics research projects: the challenges, contributing student pedagogy and changes to teaching. Students are often taught technology management separate from other computing modules. The teaching team designed a more coherent learning experience linking the technology management teaching more closely to engineering processes, specifically to engage students whose interest lies more with computing. This project has given rise to a re-evaluation of how technology management is taught to undergraduate students, adoption of architecture as a key aspect and inclusion of students with different levels of academic attainment within a class.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {32},
numpages = {6},
keywords = {Data analytics, software architecture, peer assessment, contributing student pedagogy, retrospectives},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/3175628.3175653,
author = {Abdelhakim, Boudhir Anouar and Mohamed, Ben Ahmed and Fellaji, Soumaya},
title = {Big Data Architecture for Decision Making in Protocols and Medications Assignment},
year = {2017},
isbn = {9781450352116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175628.3175653},
doi = {10.1145/3175628.3175653},
abstract = {The work on health research in general, and cancer in particular, is dual-purpose; on the one hand because it is a great subject and of essential importance given the difficulty of reaching a miraculous and rapid solution. On the other hand, because this subject is a humanitarian field and his success is of immense social size. This pulse pushed us a lot to get involved in this research in order to contribute massively to research collaboration in this area by creating multidisciplinary groups to make technology available to the medical sciences and fight against pathological scourge. In this paper, we present a new architecture to serve doctors and nurses in the assignment of adequate treatment for patients based on common digital repository of decision support. This repository is a result of given treatment by doctors dispatched in time and place. The System of proposed architecture will be able to analyze treatments, symptoms, historical medical history of patients, and results on order to help other doctors to prescribe the correct prescription according to similarity of cases and successful results.},
booktitle = {Proceedings of the Mediterranean Symposium on Smart City Application},
articleno = {5},
numpages = {4},
keywords = {analytics, hadoop, healthcare, big data},
location = {Tangier, Morocco},
series = {SCAMS '17}
}

@article{10.1145/3012004,
author = {Becker, Christoph and Duretec, Kresimir and Rauber, Andreas},
title = {The Challenge of Test Data Quality in Data Processing},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3012004},
doi = {10.1145/3012004},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {7},
numpages = {4},
keywords = {data formats, data quality, digital preservation, Benchmarking, data processing, model-based testing, digital curation, test data, ground truth, quality model, test oracle}
}

@inproceedings{10.1145/3297663.3309676,
author = {Vemulapati, Jayanti and Khastgir, Anuruddha S. and Savalgi, Chethana},
title = {AI Based Performance Benchmarking &amp; Analysis of Big Data and Cloud Powered Applications: An in Depth View},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3309676},
doi = {10.1145/3297663.3309676},
abstract = {Big data analytics platforms on cloud are becoming mainstream technology enabling cost-effective rapid deployment of customer's Big Data applications delivering quicker insights from their data. It is, therefore, even more imperative that we have high performant platform infrastructure and application at a reasonable cost. This is only possible if we make a transition from traditional approach to execute and measure performance by adopting new AI techniques such as Machine Learning (ML) &amp; predictive approach to performance benchmarking for every application domain.This paper proposes a high-level conceptual model for automated performance benchmarking which includes execution engine that has been designed to support a self-service model covering automated benchmarking in every application domain. The automated engine is supported by performance scaling recommendations via prescriptive analytics from real performance data set.We furthermore extended the recommendation capabilities of our self-service automated engine by introducing predictive analytics for making it more flexible in addressing 'what-if' scenarios to predict 'Right Scale' with measurement of "Performance Cost Ratio" (PCR). Finally, we also present some real-world industry examples which have seen the performance benefits in their applications with the recommendations given by our proposed model.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {103–109},
numpages = {7},
keywords = {performance tuning, benchmarking, ai, right scale, complex deployments, performance metrics, predictive analytics, automation, big data, scale factor, performance cost ratio, ml, auto scale},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/2668260.2668264,
author = {Al-Quzlan, Tuqya and Hamdi-Cherif, Aboubekeur and Kara-Mohamed, Chafia},
title = {Big Data Fuzzy Management Methods in Gene Regulatory Networks Inference: A Review},
year = {2014},
isbn = {9781450327671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668260.2668264},
doi = {10.1145/2668260.2668264},
abstract = {To address one of the most challenging ecosystems issues at the cellular level, this paper surveys the fuzzy methods used in gene regulatory networks (GRNs) inference. GRNs represent causal relationships between genes that have a direct influence on the life and the development of living organisms, and provide a useful contribution to the understanding of the cellular functions as well as the mechanisms of diseases. The ecosystems impacted by GRN inference span various levels from cell to society -- globally.},
booktitle = {Proceedings of the 6th International Conference on Management of Emergent Digital EcoSystems},
pages = {201–203},
numpages = {3},
keywords = {GRN inference, Fuzzy systems, Soft computing, Gene regulatory networks (GRNs), Link and graph mining, Big data},
location = {Buraidah, Al Qassim, Saudi Arabia},
series = {MEDES '14}
}

@inproceedings{10.1145/3286606.3286788,
author = {Bibri, Simon Elias and Krogstie, John},
title = {The Big Data Deluge for Transforming the Knowledge of Smart Sustainable Cities: A Data Mining Framework for Urban Analytics},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286788},
doi = {10.1145/3286606.3286788},
abstract = {There has recently been much enthusiasm about the possibilities created by the big data deluge to better understand, monitor, analyze, and plan modern cities to improve their contribution to the goals of sustainable development. Indeed, much of our knowledge of urban sustainability has been gleaned from studies that are characterized by data scarcity. Therefore, this paper endeavors to develop a systematic framework for urban sustainability analytics based on a cross-industry standard process for data mining. The intention is to enable well-informed decision-making and enhanced insights in relation to diverse urban domains. We argue that there is tremendous potential to transform and advance the knowledge of smart sustainable cities through the creation of a big data deluge that seeks to provide much more sophisticated, wider-scale, finer-grained, real-time understanding, and control of various aspects of urbanity in the undoubtedly upcoming Exabyte Age.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {11},
numpages = {10},
keywords = {data mining, big data analytics, Smart sustainable cities},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@article{10.1145/2978571,
author = {Ye, Conghuan and Ling, Hefei and Xiong, Zenggang and Zou, Fuhao and Liu, Cong and Xu, Fang},
title = {Secure Social Multimedia Big Data Sharing Using Scalable JFE in the TSHWT Domain},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2978571},
doi = {10.1145/2978571},
abstract = {With the advent of social networks and cloud computing, the amount of multimedia data produced and communicated within social networks is rapidly increasing. In the meantime, social networking platforms based on cloud computing have made multimedia big data sharing in social networks easier and more efficient. The growth of social multimedia, as demonstrated by social networking sites such as Facebook and YouTube, combined with advances in multimedia content analysis, underscores potential risks for malicious use, such as illegal copying, piracy, plagiarism, and misappropriation. Therefore, secure multimedia sharing and traitor tracing issues have become critical and urgent in social networks. In this article, a joint fingerprinting and encryption (JFE) scheme based on tree-structured Haar wavelet transform (TSHWT) is proposed with the purpose of protecting media distribution in social network environments. The motivation is to map hierarchical community structure of social networks into a tree structure of Haar wavelet transform for fingerprinting and encryption. First, fingerprint code is produced using social network analysis (SNA). Second, the content is decomposed based on the structure of fingerprint code by the TSHWT. Then, the content is fingerprinted and encrypted in the TSHWT domain. Finally, the encrypted contents are delivered to users via hybrid multicast-unicast. The proposed method, to the best of our knowledge, is the first scalable JFE method for fingerprinting and encryption in the TSHWT domain using SNA. The use of fingerprinting along with encryption using SNA not only provides a double layer of protection for social multimedia sharing in social network environment but also avoids big data superposition effect. Theory analysis and experimental results show the effectiveness of the proposed JFE scheme.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {sep},
articleno = {61},
numpages = {23},
keywords = {joint fingerprinting and encryption, Tree-structured Haar wavelet transform (TSHWT), collusion attack, social network, social multimedia big data sharing}
}

@inproceedings{10.1145/2685553.2685558,
author = {Fiesler, Casey and Young, Alyson and Peyton, Tamara and Bruckman, Amy S. and Gray, Mary and Hancock, Jeff and Lutters, Wayne},
title = {Ethics for Studying Online Sociotechnical Systems in a Big Data World},
year = {2015},
isbn = {9781450329460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2685553.2685558},
doi = {10.1145/2685553.2685558},
abstract = {The evolution of social technology and research methods present ongoing challenges to studying people online. Recent high-profile cases have prompted discussion among both the research community and the general public about the ethical implications of researching humans, their information, and their activities in large-scale digital contexts. Examples of scientific and market research involving Facebook users and OKCupid clients exemplify the ethical complexities of both studying and manipulating online user behavior. When does data science become human subjects research, and what are our obligations to these subjects as researchers' Drawing from previous work around the ethics of digital research, one goal of this workshop is to work towards a set of guiding principles for CSCW scholars doing research online.},
booktitle = {Proceedings of the 18th ACM Conference Companion on Computer Supported Cooperative Work &amp; Social Computing},
pages = {289–292},
numpages = {4},
keywords = {sociotechnical systems, online communities, research ethics, big data},
location = {Vancouver, BC, Canada},
series = {CSCW'15 Companion}
}

@article{10.1145/2782759.2782765,
author = {Fuchs, Georg and Stange, Hendrik and Hecker, Dirk and Andrienko, Natalia and Andrienko, Gennady},
title = {Constructing Semantic Interpretation of Routine and Anomalous Mobility Behaviors from Big Data},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/2782759.2782765},
doi = {10.1145/2782759.2782765},
abstract = {Annually organized VAST Challenges provide a unique opportunity to analyze complex data with available ground truth. In 2014, one of the tasks was to interpret routine and anomalous patterns of human mobility based on big data: trajectories of cars and credit card transactions.We describe a scalable visual analytics approach to solving this problem. Repeatedly visited personal and public places were extracted from trajectories by finding spatial clusters of stop points. Temporal patterns of people's presence in the places resulted from spatio-temporal aggregation of the data by the places and hourly intervals within the weekly cycle. Based on these patterns, we identified the meanings or purposes of the places: home, work, breakfast, lunch and dinner, etc. Meanings of some places could be refined using the credit card transaction data. By representing the place meanings as points on a 2D plane, we built an abstract semantic space and transformed the original trajectories to trajectories in the semantic space, i.e., performed semantic abstraction of the data. Spatio-temporal aggregation of the transformed trajectories into flows between the semantic places and subsequent clustering of time intervals by the similarity of the flow situations allowed us to reveal and analyze the routine movement behaviors. To detect anomalies, we (a) investigated the visits to the places with unknown meanings, and (b) looked for unusual presence times or visit durations at different semantic places.The analysis is scalable since all tools and methods can be applied to much larger data. Moreover, the semantic data abstraction can serve as a tool for protecting the personal privacy.},
journal = {SIGSPATIAL Special},
month = {may},
pages = {27–34},
numpages = {8}
}

@inproceedings{10.1145/2820468.2820476,
author = {Prabhakar, Balaji},
title = {A Big Data System for the Internet of Moving Things},
year = {2015},
isbn = {9781450339667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2820468.2820476},
doi = {10.1145/2820468.2820476},
abstract = {The world consists of many interesting things that move: people go to work, home, school, and shop in public transit buses and trains or in cars and taxis; goods move on these networks and by trucks or by air each day; and food items travel a very large distance to meet their eater. Thus, massive movement processes are underway in the world every day and it is critical to ensure their safe, timely and efficient operation. Towards this end, low-cost sensing and acquisition of the movement data is being achieved: from GPS devices, RFID and barcode scanners, to smart commuter cards and smartphones, snapshots of the movement process are becoming available.In this talk, I will present a system for stitching together these snapshots and reconstructing urban mobility at a very fine-grained level. The system, which we call the Space-Time Engine, provides an interactive dashboard and a querying engine for answering questions such as: what is the crowding at a train station? where're packages held up and how can their delivery be sped up? how can the available supply of transport capacity be better used to address daily demand as well as the demand on exceptional days (such as rallies and severe weather events). I will describe the STE's capabilities for operational and planning purposes, and as a learning system.},
booktitle = {Federated Computing Research Conference},
pages = {7},
location = {Portland, Oregon, USA},
series = {FCRC '15}
}

@inproceedings{10.1145/2818869.2818935,
author = {Lee, Chung-Hong and Wu, Chih-Hung},
title = {An Incremental Learning Technique for Detecting Driving Behaviors Using Collected EV Big Data},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818935},
doi = {10.1145/2818869.2818935},
abstract = {This paper presents an expansible machine learning approach applying the EV big data as the human sensor to extract driving behaviors and driving modes. A pattern recognition approach is proposed to model the driving pattern according to the energy consumption of an EV. The growing hierarchical self-organizing maps (GHSOM) is applied to learn driver's behaviors gradually in the offline process, and the clustered neurons are used as the training sets for implementing online classifiers based on support vector machine (SVM). This proposed framework would facilitate the understanding of driver's behaviors and help drivers overcome range anxiety.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {10},
numpages = {5},
keywords = {Machine learning, Driving pattern recognition, Driving behavior, Electric vehicle, Range anxiety, EV big data},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@inproceedings{10.1145/3257763,
author = {Mejova, Yelena},
title = {Session Details: Big Data and Social Media Studies on Nutrition},
year = {2016},
isbn = {9781450342247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257763},
doi = {10.1145/3257763},
booktitle = {Proceedings of the 6th International Conference on Digital Health Conference},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {DH '16}
}

@inproceedings{10.1145/3341069.3341086,
author = {Pengxi, Li},
title = {The Construction Study of College Informationization Teaching Service System under the Background of Big Data},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3341086},
doi = {10.1145/3341069.3341086},
abstract = {Under the background of the rapid development of big data technology, the construction of college informationization teaching service system is the basis of the informationization teaching in colleges and universities. That is very important for the success of the informatization in Universities what is service realization model, business logic, architecture and platform conform to the whole development strategy of universities. Information management organization supports the planning, implementation, operation, maintenance and management of business information system. This paper analyzes the reform mode of college education information service system supported by big data technology. Based on the analysis of the reform mode of college informationization teaching service system supported by big data technology, this paper puts forward the design idea of post system based on big data. At the same time, with the case of "big data assisted employment", the post design and adjustment were carried out. The results show that big data assisted employment has greatly improved the efficiency and quality of the school's employment department, providing students with better employment security. Finally, the problems that need to be solved in the informatization teaching service are sorted out.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {185–189},
numpages = {5},
keywords = {Service system, Teaching informatization, Big data},
location = {Guangzhou, China},
series = {HPCCT '19}
}

@inproceedings{10.1145/3380688.3380702,
author = {Amaechi, Eloanyi Samson and Van Pham, Hai},
title = {Enhancement of Convolutional Neural Networks Classifier Performance in the Classification of IoT Big Data},
year = {2020},
isbn = {9781450376310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380688.3380702},
doi = {10.1145/3380688.3380702},
abstract = {Current developments in technologies occupy a central role in weather forecasting and the Internet-of-Things for both organizations and the IT sector. Big-data analytics and the classification of data (derived from many sources including importantly the Internet-of-Things) provides significant information on which organizations can optimize their current and future business planning. This paper considers convolutional neural networks and data classification as it relates to big-data and presents a novel approach to weather forecasting. The proposed approach targets the enhancement of convolutional neural networks and data classification to enable improved classification performance for big-data classifiers. Our contribution combines the positive benefits of convolutional neural networks with expert knowledge represented by fuzzy rules for prepared data sets in time series, the aim being to achieve improvements in the predictive quality of weather forecasting. Experimental testing demonstrates that the proposed enhanced convolutional network approach achieves a high level of accuracy in weather forecasting when compared to alternative methods evaluated.},
booktitle = {Proceedings of the 4th International Conference on Machine Learning and Soft Computing},
pages = {25–29},
numpages = {5},
keywords = {fuzzy rules, Convolutional Neural Network, Big data, Neural Network, Internet of Things (loT)},
location = {Haiphong City, Viet Nam},
series = {ICMLSC 2020}
}

@inproceedings{10.1145/3399205.3399228,
author = {Moumen, Aniss},
title = {Adoption of Big Data, Cloud Computing &amp; IoT in Morocco Perception of Public Administrations Collaborators},
year = {2020},
isbn = {9781450375788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399205.3399228},
doi = {10.1145/3399205.3399228},
abstract = {The integration of new technologies such as big data, cloud computing, and the Internet of Things (IoT), constitute a new challenge for existing information systems of local administrations.In this work, we present the results of interviews conducted with the public administrations in the South-eastern region of Morocco, and as conclusion we expose two proposed models induced by this study.},
booktitle = {Proceedings of the 4th Edition of International Conference on Geo-IT and Water Resources 2020, Geo-IT and Water Resources 2020},
articleno = {21},
numpages = {4},
keywords = {Cloud Computing, IoT, Big data, Information System},
location = {Al-Hoceima, Morocco},
series = {GEOIT4W-2020}
}

@inproceedings{10.1145/1864708.1864767,
author = {De Pessemier, Toon and Dooms, Simon and Deryckere, Tom and Martens, Luc},
title = {Time Dependency of Data Quality for Collaborative Filtering Algorithms},
year = {2010},
isbn = {9781605589060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1864708.1864767},
doi = {10.1145/1864708.1864767},
abstract = {The efficiency of personal suggestions generated by collaborative filtering techniques is highly dependent on the quality and quantity of the available consumption data. Extending data sets with additional consumption data (from the past) might enrich the user profiles and generally leads to more accurate recommendations. Although if a considerable amount of profile information is already available and detailed personal preferences can be derived, supplementary consumption data may not have any (or a very limited) added value for the recommendation algorithm. These additional consumption data increase the required storage capacity and the computational load to generate the personal recommendations. Moreover, since personal preferences and the relevance of content items may vary over time, older consumption data might be outdated and lead to inaccurate recommendations. Therefore, we investigate which consumption data are (the most) relevant to feed the conventional collaborative filtering algorithms. For provider-generated content systems, we demonstrate that the accuracy of collaborative filtering algorithms increases by extending user profiles with additional older consumption data. In contrast, we witness the opposite effect for user-generated content systems: involving older consumption data has a negative influence on the recommender accuracy. These results are important for website owners who intend to employ a recommendation system at a minimum storage and computation cost.},
booktitle = {Proceedings of the Fourth ACM Conference on Recommender Systems},
pages = {281–284},
numpages = {4},
keywords = {data quality, collaborative filtering, recommender systems},
location = {Barcelona, Spain},
series = {RecSys '10}
}

@inproceedings{10.1145/2983323.2983841,
author = {Zhang, Xuyun and Leckie, Christopher and Dou, Wanchun and Chen, Jinjun and Kotagiri, Ramamohanarao and Salcic, Zoran},
title = {Scalable Local-Recoding Anonymization Using Locality Sensitive Hashing for Big Data Privacy Preservation},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983841},
doi = {10.1145/2983323.2983841},
abstract = {While cloud computing has become an attractive platform for supporting data intensive applications, a major obstacle to the adoption of cloud computing in sectors such as health and defense is the privacy risk associated with releasing datasets to third-parties in the cloud for analysis. A widely-adopted technique for data privacy preservation is to anonymize data via local recoding. However, most existing local-recoding techniques are either serial or distributed without directly optimizing scalability, thus rendering them unsuitable for big data applications. In this paper, we propose a highly scalable approach to local-recoding anonymization in cloud computing, based on Locality Sensitive Hashing (LSH). Specifically, a novel semantic distance metric is presented for use with LSH to measure the similarity between two data records. Then, LSH with the MinHash function family can be employed to divide datasets into multiple partitions for use with MapReduce to parallelize computation while preserving similarity. By using our efficient LSH-based scheme, we can anonymize each partition through the use of a recursive agglomerative $k$-member clustering algorithm. Extensive experiments on real-life datasets show that our approach significantly improves the scalability and time-efficiency of local-recoding anonymization by orders of magnitude over existing approaches.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {1793–1802},
numpages = {10},
keywords = {big data, privacy preservation, cloud, mapreduce, LSH},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1145/3291801.3291816,
author = {Sidib\'{e}, Abdoulaye and Shu, Gao and Ma, Yunzhao and Wanqi, Wei},
title = {Big Data Framework for Abnormal Vessel Trajectories Detection Using Adaptive Kernel Density Estimation},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291816},
doi = {10.1145/3291801.3291816},
abstract = {In the maintaining of inland navigational safety and security, the automatic detection of anomalous vessels trajectory behavior from a large amount of vessels traffic datasets produced by automatic identification systems (AIS) is an interesting task, and in another hand constitutes a challenge because of the size of the AIS data. In this paper we propose a new big data framework based on the Adaptive Kernel Density Estimation (AKDE) method, for abnormal vessel trajectory detection using Apache Spark. In the proposed framework, first, the water area is divided into space partitions. Second, on the Apache Spark distributed computing platform, the AKDE method is used to build in a parallel manner local models of normal vessels trajectory data for space partitions, as probability density functions (PDFs). Finally, the detection of abnormal trajectory data point is performed based on the corresponding built local models, by sequentially checking the real incoming trajectory data point. And a trajectory with a user-defined number of abnormal data points is considered to be an abnormal trajectory. In addition, we discuss the main features and some limitations of the proposed framework.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {43–46},
numpages = {4},
keywords = {Kernel Density Estimation, AIS, Vessel Trajectory, Anomaly detection, Apache Spark},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/3025453.3025456,
author = {Moritz, Dominik and Fisher, Danyel and Ding, Bolin and Wang, Chi},
title = {Trust, but Verify: Optimistic Visualizations of Approximate Queries for Exploring Big Data},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025456},
doi = {10.1145/3025453.3025456},
abstract = {Analysts need interactive speed for exploratory analysis, but big data systems are often slow. With sampling, data systems can produce approximate answers fast enough for exploratory visualization, at the cost of accuracy and trust. We propose optimistic visualization, which approaches these issues from a user experience perspective. This method lets analysts explore approximate results interactively, and provides a way to detect and recover from errors later. Pangloss implements these ideas. We discuss design issues raised by optimistic visualization systems. We test this concept with five expert visualizers in a laboratory study and three case studies at Microsoft. Analysts reported that they felt more confident in their results, and used optimistic visualization to check that their preliminary results were correct.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {2904–2915},
numpages = {12},
keywords = {approximation, data visualization, exploratory analysis, uncertainty, optimistic visualization},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/2882903.2912573,
author = {Papakonstantinou, Yannis},
title = {Semistructured Models, Queries and Algebras in the Big Data Era: Tutorial Summary},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2912573},
doi = {10.1145/2882903.2912573},
abstract = {Numerous databases promoted as SQL-on-Hadoop, NewSQL and NoSQL support semi-structured, schemaless and heterogeneous data, typically in the form of enriched JSON. They also provide corresponding query languages. In addition to these genuine JSON databases, relational databases also provide special functions and language features for the support of JSON columns, typically piggybacking on non-1NF (non first normal form) features that SQL acquired over the years. We refer to SQL databases with JSON support as SQL/JSON databases.The evolving query languages present multiple variations: Some are superficial syntactic ones, while other ones are genuine differences in modeling, language capabilities and semantics. Incompatibility with SQL presents a learning challenge for genuine JSON databases, while the table orientation of SQL/JSON databases often leads to cumbersome syntactic/semantic structures that are contrary to the semistructured nature of JSON. Furthermore, the query languages often fall short of full-fledged semistructured query language capabilities, when compared to the yardstick set by XQuery and prior works on semistructured data (even after superficial model differences are abstracted out).We survey features, the designers' options and differences in the approaches taken by actual systems. In particular, we first present a SQL backwards-compatible language, named SQL++, which can access both SQL and JSON data. SQL++ is expected to be supported by Couchbase's CouchDB and UCI's AsterixDB semistructured databases. Then we expand SQL++ into the Configurable SQL++, whereas multiple possible (and different) semantics are formally captured by the multiple options that the language's semantic configuration options can take. We show how appropriate setting of the configuration options morphs the Configurable SQL++ semantics into the semantics of 10 surveyed languages, hence providing a compact and formal tool to understand the essential semantic differences between different systems. We briefly comment on the utility of formally capturing semantic variations in polystore systems.Finally we discuss the comparison with prior nested and semistructured query languages (notably OQL and XQuery) and describe a key aspect of query processor implementation: set-oriented semistructured query algebras. In particular, we transfer into the JSON era lessons from the semistructured query processing research of the 90s and 00s and combine them with insights on current JSON databases. Again, the tutorial presents the algebras' fundamentals while it abstracts away modeling differences that are not applicable.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2229–2233},
numpages = {5},
keywords = {SQL, noSQL, semistructured query languages, newSQL, polystores, JSON query languages, query algebras},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/2983323.2983357,
author = {Zheng, Zimu and Wang, Dan and Pei, Jian and Yuan, Yi and Fan, Cheng and Xiao, Fu},
title = {Urban Traffic Prediction through the Second Use of Inexpensive Big Data from Buildings},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983357},
doi = {10.1145/2983323.2983357},
abstract = {Traffic prediction, particularly in urban regions, is an important application of tremendous practical value. In this paper, we report a novel and interesting case study of urban traffic prediction in Central, Hong Kong, one of the densest urban areas in the world. The novelty of our study is that we make good second use of inexpensive big data collected from the Hong Kong International Commerce Centre (ICC), a 118-story building in Hong Kong where more than 10,000 people work. As building environment data are much cheaper to obtain than traffic data, we demonstrate that it is highly effective to estimate building occupancy information using building environment data, and then to further use the information on occupancy to provide traffic predictions in the proximate area. Scientifically, we investigate how and to what extent building data can complement traffic data in predicting traffic. In general, this study sheds new light on the development of accurate data mining applications through the second use of inexpensive big data.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {1363–1372},
numpages = {10},
keywords = {building occupancy, traffic prediction},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1145/1322263.1322324,
author = {Reddy, Sasank and Burke, Jeff and Estrin, Deborah and Hansen, Mark and Srivastava, Mani},
title = {A Framework for Data Quality and Feedback in Participatory Sensing},
year = {2007},
isbn = {9781595937636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1322263.1322324},
doi = {10.1145/1322263.1322324},
abstract = {The rapid adoption of mobile phones by society over the last decade and the increasing ability to capture, classifying, and transmit a wide variety of data (image, audio, and location) have enabled a new sensing paradigm - where humans carrying mobile phones can act as sensor systems. Human-in-the-loop sensor systems raise many new challenges in areas of sensor data quality assessment, mobility and sampling coordination, and user interaction procedures.},
booktitle = {Proceedings of the 5th International Conference on Embedded Networked Sensor Systems},
pages = {417–418},
numpages = {2},
keywords = {human sensor networks, mobile systems},
location = {Sydney, Australia},
series = {SenSys '07}
}

@inproceedings{10.1145/3246871,
author = {Papatheodorou, Christos and Gergatsoulis, Manolis},
title = {Session Details: Special Session in Big Data, Metadata and Knowledge},
year = {2014},
isbn = {9781450328975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246871},
doi = {10.1145/3246871},
booktitle = {Proceedings of the 18th Panhellenic Conference on Informatics},
location = {Athens, Greece},
series = {PCI '14}
}

@inproceedings{10.1145/2588555.2612174,
author = {Istvan, Zsolt and Woods, Louis and Alonso, Gustavo},
title = {Histograms as a Side Effect of Data Movement for Big Data},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2612174},
doi = {10.1145/2588555.2612174},
abstract = {Histograms are a crucial part of database query planning but their computation is resource-intensive. As a consequence, generating histograms on database tables is typically performed as a batch job, separately from query processing. In this paper, we show how to calculate statistics as a side effect of data movement within a DBMS using a hardware accelerator in the data path. This accelerator analyzes tables as they are transmitted from storage to the processing unit, and provides histograms on the data retrieved for queries at virtually no extra performance cost. To evaluate our approach, we implemented this accelerator on an FPGA. This prototype calculates histograms faster and with similar or better accuracy than commercial databases. Moreover, the FPGA can provide various types of histograms such as Equi-depth, Compressed, or Max-diff on the same input data in parallel, without additional overhead.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {1567–1578},
numpages = {12},
keywords = {histogram, statistics, query optimization, FPGA},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/3152723.3152728,
author = {Xiao, Yan and Fan, Xixuan and Dong, Li},
title = {Quantitative Study on Commercial Street Vibrancy Based on Big Data: A Case of Dalian},
year = {2017},
isbn = {9781450353564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152723.3152728},
doi = {10.1145/3152723.3152728},
abstract = {Street as traffic carrier and public space of a city are playing increasingly important role in daily city life. The paper quantitatively explores vibrancy of Commercial Street, while referring to urban space syntax analysis method, taking Dalian as an example. The paper also explores the factors influencing the dynamic commercial block space based on poi, function density and function diversity, so as to provide theoretical guidance for the planning and design of the commercial block.},
booktitle = {Proceedings of the 2017 International Conference on Big Data Research},
pages = {7–11},
numpages = {5},
keywords = {Commercial Street, Function Diversity, Point of Interest, Data Augmented Design, Function Density},
location = {Osaka, Japan},
series = {ICBDR 2017}
}

@inproceedings{10.1145/3404687.3404692,
author = {Fikri, Noussair and Rida, Mohamed and Abghour, Noureddine and Moussaid, Khalid and El Omri, Amina},
title = {A Near Metal Platform for Intensive Big Data Processing Using A Novel Approach: Persistent Distributed Channels},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404692},
doi = {10.1145/3404687.3404692},
abstract = {In this paper we present a new Golang based framework for distributed intensive data processing and also micro batching. It uses a novel approach, the persistent distributed channels, based on the concept of Share memory by communicating, and inspired from Resilient distributed datasets of Apache Spark. The architecture of our proposed system is considered as near-metal platform for Big Data operations in order to enhance the speed of massive data processing.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {1–5},
numpages = {5},
keywords = {near-metal, persistent, batching, Big data, channels, Golang, datasets, distributed, Resilient, massive},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1109/CCGrid.2013.100,
author = {Tracey, David and Sreenan, Cormac},
title = {A Holistic Architecture for the Internet of Things, Sensing Services and Big Data},
year = {2013},
isbn = {9780768549965},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2013.100},
doi = {10.1109/CCGrid.2013.100},
abstract = {Wireless Sensor Networks (WSNs) increasingly enable applications and services to interact with the physical world. Such services may be located across the Internet from the sensing network. Cloud services and big data approaches may be used to store and analyse this data to improve scalability and availability, which will be required for the billions of devices envisaged in the Internet of Things (IoT). The potential of WSNs is limited by the relatively low number deployed and the difficulties imposed by their heterogeneous nature and limited (or proprietary) development environments and interfaces. This paper proposes a set of requirements for achieving a pervasive, integrated information system of WSNs and associated services. It also presents an architecture which is termed holistic as it considers the flow of the data from sensors through to services. The architecture provides a set of abstractions for the different types of sensors and services. It has been designed for implementation on a resource constrained node and to be extensible to server environments. This paper presents a 'C' implementation of the core architecture, including services on Linux and Contiki (using the Constrained Application Protocol (CoAP)) and a Linux service to integrate with the Hadoop HBase datastore.},
booktitle = {Proceedings of the 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {546–553},
numpages = {8},
keywords = {wireless sensor networks, information model, cloud computing, big data, protocols, tuple space},
location = {Delft, Netherlands},
series = {CCGRID '13}
}

@inproceedings{10.1145/3358528.3358589,
author = {Yin, Lizi and Yu, Kaiwen},
title = {A Research of Readers' Borrowing Library Books Management and Optimization Based On Big Data Technology},
year = {2019},
isbn = {9781450371926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358528.3358589},
doi = {10.1145/3358528.3358589},
abstract = {In the era of Internet, massive data are coming. The big data mining and analysis is vital to guide people in their work. In this paper, we introduce properties of data of the library borrowing big data and distribution of library books. By data mining of the library borrowing big data, we bring forward some opinions in three aspects. Firstly, according to the number of books borrowed from different stacks, we come up with some ideas to optimize the library floor set and make reasonable use of resources. Secondly, Based on the analysis of borrowing time, we make some suggestions to optimize the management of borrowing books. Finally, according to the types of books of the most borrowed, we put forward some suggestions to optimized collection books of library.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
pages = {87–91},
numpages = {5},
keywords = {Data mining, Big data, Books management, Optimization, Book borrowing},
location = {Jinan, China},
series = {ICBDT2019}
}

@inproceedings{10.1145/3322134.3322146,
author = {Yawei, Liu and Shiming, Zhu},
title = {The Role and Task of Innovation and Entrepreneurship Teachers under the Background of Big Data},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322146},
doi = {10.1145/3322134.3322146},
abstract = {In the process of promoting innovation and entrepreneurship education reform under the background of big data, the role and task of innovation and entrepreneurship teachers play an important role. However, there are some problems in innovation and entrepreneurship teachers, such as weak professional competence, lack of educational synergy, weak information literacy and innovation ability, and emphasizing knowledge over methods. Innovation and entrepreneurship teachers should play the roles of "big" maker, "small" maker and guide. They should encourage every student to solve practical problems by means of big data and other information technology innovation, and provide targeted guidance for students.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {98–102},
numpages = {5},
keywords = {Innovation and entrepreneurship teachers, Innovation and entrepreneurship education, Big data},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@inproceedings{10.1109/BDC.2014.11,
author = {Ahnn, Jong Hoon},
title = {A Practical Approach to Scalable Big Data Computing for the Personalization of Services at Samsung},
year = {2014},
isbn = {9781479918973},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BDC.2014.11},
doi = {10.1109/BDC.2014.11},
abstract = {We observe that the recent advances in big data computing have empowered the personalization of service including model-based services such as speech recognition, face recognition, and context-aware service. Various sources of user's logs can be utilized in remodeling, adapting, and personalizing pretrained models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner on top of Samsung' big data infrastructure. An automatic speech recognition (ASR) service such as Samsung's S-Voice, Apple's SIRI is one of the representative examples. Recently advances in ASR married with big data technologies drive more personalized services in many areas of services. A speaker adaptation is now a well-accepted technology that requires huge computation cost in creating a personalized acoustic model and corresponding language model over several billions of Samsung product users. We implement a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. An optimal set of performance optimization is suggested to have the best performance such as workflow compaction, file compression, best file system selection among several distributed file systems.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing},
pages = {64–73},
numpages = {10},
keywords = {Speech Recognition, Cloud Computing, Scalability, Personalization, Big Data, Hadoop},
series = {BDC '14}
}

@inproceedings{10.1145/3352740.3352757,
author = {Xiaomei, Yang and Songyang, Jiang},
title = {The Construction of Big Data Credit Evaluation System in the Management of Indemnificatory Housing Access},
year = {2019},
isbn = {9781450372053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352740.3352757},
doi = {10.1145/3352740.3352757},
abstract = {There are numbers of issues in the management of indemnificatory housing access. The quantity of access cannot meet demand, the standard of access is inaccurate and the evaluation of post-access is passive. This leads to mismatch between supply and demand, as well as lack of liquidity in indemnificatory housing. It should introduce the big data credit evaluation system into the management of indemnificatory housing access, so as to create a binding access credit environment, balance supply and demand, and promote the utilization of housing resources. Specifically, big data platform of national social security credit should be constructed. The "funnel type" information sharing mechanism and "dynamic assessment+ active early warning" mechanism also should be constructed. In addition, the multi-dimensional big data credit evaluation measure index should be formed.},
booktitle = {Proceedings of the 2019 3rd International Workshop on Education, Big Data and Information Technology},
pages = {98–101},
numpages = {4},
keywords = {Evaluation System, Management of Indemnificatory Housing Access, Mechanism, Big Data Credit},
location = {Guilin, China},
series = {EBDIT 2019}
}

@inproceedings{10.1145/3322134.3322135,
author = {Geng, Han and Guang-yu, Li},
title = {The Connotation and Strategy of College Students' Behavior Analysis under the Background of Big Data},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322135},
doi = {10.1145/3322134.3322135},
abstract = {Big data, as a derivative of Internet technology, is now changing the way people living and knowing things. Using data mining technology to analyze the data of college students' learning and living behavior can get conclusions with rich guiding significance and educational connotation. This paper expounds the connotation and significance of student behavior analysis under the background of big data from three aspects of precise subsidy, academic assistance and mental health supervision, and discusses the strategies of student behavior analysis and student data mining. At last, taking Beihang Shoue College as an example, this paper studies the correlation between students' behavior and academic level, which is helpful to improve the framework of talent training system and promote the development of students' personalized talents.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {48–51},
numpages = {4},
keywords = {Strategy, Student behavior analysis, Big data, Connotation},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@inproceedings{10.1145/3368756.3369046,
author = {Kaderi, Farah Al and Koulali, Rim and Rida, Mohamed},
title = {Automated Management of Maritime Container Terminals Using Internet of Things and Big Data Technologies},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369046},
doi = {10.1145/3368756.3369046},
abstract = {Global trade is growing steadily and the management of the transport and logistics fields is facing many challenges day after day. Container shipping is one of those areas whose management has become a crucial task because of its major role in ensuring the quality of freight transport service. In this paper, we present and describe a real-time monitoring and management system dedicated to the automatization of the maritime terminal containers management. Our proposed solution is based on using both Internet of Things (IoT) and Big Data technologies in order to build a system that provides advanced features to cover different aspects of container logistics management. As the implementation of the proposed system implies using various tools and technologies, we conducted a comparative study on several technologies and tools related to IoT and Big Data in order to choose the appropriate ones. We also adopted a multi-layer architecture for the proposed system and through this paper, we will present the scope, role and tools set for every tier.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {65},
numpages = {6},
keywords = {Arduino Uno, multi-layer architecture, MQTT, maritime container terminal, HDFS, Apache spark, internet of things, big data, Apache hadoop, RaspBerry pi 3 b+, RFID, MapReduce},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@inproceedings{10.1145/3429351.3431745,
author = {Tavakolisomeh, Sanaz},
title = {Selecting a JVM Garbage Collector for Big Data and Cloud Services},
year = {2020},
isbn = {9781450382007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429351.3431745},
doi = {10.1145/3429351.3431745},
abstract = {Memory management is responsible for allocating and releasing the memory used by applications (e.g., in the Java Virtual Machine-JVM). There are several garbage collectors (GCs) each designed to target different performance metrics, making it very hard for developers to decide which GC to use for a particular application. We start with a review of existing GC algorithms. Then, we intend to evaluate throughput, pause time, and memory usage in existing JVM GCs using benchmark suites like DaCapo and Renaissance. The goal is to find the trade-offs between the above mentioned performance metrics to have a better understanding of which GC helps fulfilling certain application requirements.},
booktitle = {Proceedings of the 21st International Middleware Conference Doctoral Symposium},
pages = {22–25},
numpages = {4},
keywords = {Garbage Collection Algorithm, Garbage Collector, Memory Management, Big Data, Cloud, JVM},
location = {Delft, Netherlands},
series = {Middleware'20 Doctoral Symposium}
}

@inproceedings{10.1145/3152723.3152741,
author = {Barakbah, Ali Ridho and Harsono, Tri and Sudarsono, Amang and Aliefyan, Roy Advandy},
title = {Big Data Analysis for Spatio-Temporal Earthquake Risk-Mapping System in Indonesia with Automatic Clustering},
year = {2017},
isbn = {9781450353564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152723.3152741},
doi = {10.1145/3152723.3152741},
abstract = {Earthquake is one of nature disaster types. Almost all regions in Indonesia are often earthquakes ranging from small magnitude to large. Anticipation and mitigation of earthquake victims is one of the important points in preventing the occurrence of earthquake victims in large numbers. One of the important information in anticipating and handling earthquake victims is to provide information about earthquake risk mapping in a region (province). This information is given by big data processing of the earthquake distribution and big data analytics of spatio-temporal earthquake data. This paperpresented a big data analysis for earthquake risk mapping system based on earthquake density projected to provinces in Indonesia. This system has 4 main features: (1) Data acquisation and preprocessing, (2) Automatic clustering using our Valley Tracing algorithm, (3) Density measurement of earthquake data distribution, and (4) Risk-mapping visualization projected to provinces. For experimental study, earthquake data is obtained from Advanced National Seismic System(ANSS) year 1963-2016 in location of Indonesia. We made a series of experiments in the places hit by big earthquake in Andaman (Banca Aceh), West Sumatra, and Papua. Based on the big data processing of the earthquake distribution and big data analytics of spatio-temporal earthquake data, it performed that the high seismic density value affected the risk of earthquake occurrence in the next year in the area concerned.},
booktitle = {Proceedings of the 2017 International Conference on Big Data Research},
pages = {33–37},
numpages = {5},
keywords = {Earthquake Data Distribution, Risk-Mapping Visualization, Automatic Clustering, Spatio-Temporal Earthquake Data Distribution},
location = {Osaka, Japan},
series = {ICBDR 2017}
}

@inproceedings{10.1145/3341069.3342980,
author = {Islam, M. D. Samiul and Liu, Daizong and Wang, Kewei and Zhou, Pan and Yu, Li and Wu, Dapeng},
title = {A Case Study of HealthCare Platform Using Big Data Analytics and Machine Learning},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3342980},
doi = {10.1145/3341069.3342980},
abstract = {The medical services in Bangladesh are shortage nowadays; people are suffering from getting the correct treatment from the hospital. With the low proportion of the doctors and the low per capita salary in Bangladesh, patients need to spend more money to get the appropriate treatments. Therefore, it is necessary to apply modern information technologies by which the scaffold between the patients and specialists can be reduced, and the patients can take proper treatment at a lower cost. Fortunately, we can solve this critical problem by utilizing interaction among electrical devices. With the big data collected from these devices, machine learning is a powerful tool for the data analytics because of its high accuracy, lower computational costs, and lower power consumption. This research is based on a case of study by the incorporation of the database, mobile application, web application and develops a novel platform through which the patients and the doctors can interact. In addition, the platform helps to store the patients' health data to make the final prediction using machine learning methods to get the proper healthcare treatment with the help of the machines and the doctors. The experiment result shows the high accuracy over 95% of the disease detection using machine learning methods, with the cost 90% lower than the local hospital in Bangladesh, which provides the strong support to implement of our platform in the remote area of the country.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {139–146},
numpages = {8},
keywords = {Disease Prediction, Data Mining, Machine Learning, Big Data, Healthcare},
location = {Guangzhou, China},
series = {HPCCT '19}
}

@inproceedings{10.1145/3026480.3026491,
author = {Ying, Yang and Xialing, Tang and Wei, Tang},
title = {Study on Governmental Cultural Resources Purchase Management Based on Public Information Behavior Big Data},
year = {2017},
isbn = {9781450348218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3026480.3026491},
doi = {10.1145/3026480.3026491},
abstract = {With the rapid development of computer and network technology, big data analytics has been more and more widely used in public management. In the process of people obtaining and making use of cultural resources, the data of information behavior which closely related to the cultural resources is produced. These data reflect the cultural needs of people. So if the government takes the analysis of these data into account while purchasing cultural resources, the time efficiency and efficiency of fund will be improved. The governmental culture resources purchase management based on the big data analysis of public information behavior is put forward after research the public information behavior model and the book resources purchase way based on people's reading behavior -PDA(Patron - driven - acquisition).},
booktitle = {Proceedings of the 8th International Conference on E-Education, E-Business, E-Management and E-Learning},
pages = {72–75},
numpages = {4},
keywords = {information behavior, PDA, big data, culture resources purchase management, public cultural service},
location = {Kuala Lumpur, Malaysia},
series = {IC4E '17}
}

@inproceedings{10.5555/3398761.3398924,
author = {Trafton, J. Gregory and Hiatt, Laura M. and Brumback, Benjamin and McCurry, J. Malcolm},
title = {Using Cognitive Models to Train Big Data Models with Small Data},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Modeling and predicting human behavior pose a difficult challenge for AI and other related fields. Some current techniques (e.g., cognitive architectures) are able to model people's goals and actions from little data, but have poor predictive capabilities. Other methods (e.g., deep networks) have strong predictive capabilities but require large amounts of data to train the model; such abundant empirical data on human performance is not available for many human-based tasks. We show a novel and general method of generating copious synthetic data of human behavior using a cognitive architecture, and then use the data to train a deep network classifier to predict ensuing human actions. We test our approach by predicting human actions on a supervisory control task; the results show that our approach provides superior prediction when compared to training a classifier with only (limited) empirical data.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1413–1421},
numpages = {9},
keywords = {cognitive models, act-r, agents for improving human cooperative activities, agent-based analysis of human interactions},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@proceedings{10.1145/3264560,
title = {ICCBDC'18: Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing},
year = {2018},
isbn = {9781450364744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICCBDC 2018 and ICIIP 2018 provide a scientific platform for both local and international scientists, engineers and technologists who work in all aspects of cloud and big data computing, and intelligent information processing. In addition to the contributed papers, Prof. Hong Zhu from Oxford Brookes University, UK, Assoc. Prof. Huseyin Seker form the University of Northumbria at Newcastle, UK and Prof. Alfredo Cuzzocrea form University of Trieste, Italy were invited to deliver keynote speeches at ICCBDC 2018 2018 and ICIIP 2018.},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/3090354.3090369,
author = {Hourrane, Oumaima and Benlahmar, El Habib},
title = {Survey of Plagiarism Detection Approaches and Big Data Techniques Related to Plagiarism Candidate Retrieval},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090369},
doi = {10.1145/3090354.3090369},
abstract = {The easy and fast access to the web and the massive existence of databases of information systems today have led to an agile increase in the phenomenon of plagiarism as a serious problem for publishers and researchers. Fact, a number of researchers have discussed this problem by adopting several techniques that can detect plagiarism. Nevertheless, most of these techniques are still insufficient for the detection of intelligent plagiarism, which still needs to be improved. In this paper, we give an overview of the best-known methods of detection of plagiarism that exist. We start with defining the concept of plagiarism and its various forms most used by plagiarists. A thorough study of these approaches is then carried out, by establishing a comparative table of these approaches according to several criteria. Moreover, we finish by defining the concept of big data as well as one of these techniques that called Text mining, which applied in the phase of extraction of documents sources for plagiarism detection.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {15},
numpages = {6},
keywords = {Big data, Text-mining, Plagiarism Detection, Plagiarism, Information Retrieval},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3345252.3345276,
author = {Stefanova, Svetlana and Draganov, Iliya},
title = {Comparison between Advanced Software Technologies for Creating Web Systems Working with Big Data},
year = {2019},
isbn = {9781450371490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345252.3345276},
doi = {10.1145/3345252.3345276},
abstract = {This article reviews and compares modern and up to date software tools for creating web systems, including those that work with Big Data. The software tools are examined by four processing stages of the data lifecycle: input, process and preparation, storage, sharing. Programming languages and software frameworks are compared separately regarding the stages of input, preparation and sharing, and Database Management System (DBMS) are compared separately regarding the storage stage. Comparison criteria for the software tools are introduced as well. Statistical information is provided in order to prove that the considered software tools are used widely.},
booktitle = {Proceedings of the 20th International Conference on Computer Systems and Technologies},
pages = {269–272},
numpages = {4},
keywords = {Software frameworks, DBMS, Web programming languages, Web systems},
location = {Ruse, Bulgaria},
series = {CompSysTech '19}
}

@inproceedings{10.1145/3436209.3436888,
author = {Yumin, Gui},
title = {Analysis of Financial Risk Management of E-Commerce Enterprises Based on Big Data},
year = {2021},
isbn = {9781450388573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436209.3436888},
doi = {10.1145/3436209.3436888},
abstract = {As a new type of transaction model in the Internet + era, e-commerce has been integrated into people's lives and plays an important role. E-commerce business transactions will inevitably be affected by payment behavior, taxation and other factors. With the rapid development of e-commerce companies, the current financial risk management model of e-commerce companies is no longer able to meet the development requirements of the company. Their financial management and financial status are facing major tests, which directly affect the normal operation of the company. It is increasingly important to strengthen the financial risk management of e-commerce companies. This article takes e-commerce companies as the research object and analyzes financial risk management on the basis of big data. It mainly explores how e-commerce companies do a good job in security management, how to establish and improve corporate financial risk management systems, and then improve corporate financial risk identification and management and control capabilities to ensure the healthy and sustainable development of the company. With the emergence of big data, e-commerce companies have risen sharply, so the competition between them has become increasingly fierce. Therefore, their own financial risks are more strictly controlled. Only by strengthening corporate financial risk management can the company be able to promote stable and long-term development. In the context of Internet+, strengthening the research on financial risk management of e-commerce companies has become a key concern of all sectors of society. This is of great significance for preventing corporate financial risks, improving financial management capabilities and comprehensive competitiveness.},
booktitle = {2020 The 4th International Conference on E-Business and Internet},
pages = {37–41},
numpages = {5},
location = {Singapore, Singapore},
series = {ICEBI 2020}
}

