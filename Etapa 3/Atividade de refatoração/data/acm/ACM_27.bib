@inproceedings{10.1109/UCC.2014.46,title = {Priority-Based Resource Scheduling in Distributed Stream Processing Systems for Big Data Applications}, author = {Bellavista Paolo , Corradi Antonio , Reale Andrea , Ticca Nicola },year = {2014}, isbn = {9781479978816}, publisher = {IEEE Computer Society}, address = {USA}, url = {https://doi.org/10.1109/UCC.2014.46}, doi = {10.1109/UCC.2014.46}, abstract = {Distributed Stream Processing Systems (DSPSs) are attracting increasing industrial and academic interest as flexible tools to implement scalable and cost-effective on-line analytics applications over Big Data streams. Often hosted in private/public cloud deployment environments, DSPSs offer data stream processing services that transparently exploit the distributed computing resources made available to them at runtime. Given the volume of data of interest, possible (hard/soft) real-time processing requirements, and the time-variable characteristics of input data streams, it is very important for DSPSs to use smart and innovative scheduling techniques that allocate computing resources properly and avoid static over-provisioning. In this paper, we originally investigate the suitability of exploiting application-level indications about differentiated priorities of different stream processing tasks to enable application-specific DSPS resource scheduling, e.g., Capable of re-shaping processing resources in order to dynamically follow input data peaks of prioritized tasks, with no static over-provisioning. We originally propose a general and simple technique to design and implement priority-based resource scheduling in flow-graph-based DSPSs, by allowing application developers to augment DSPS graphs with priority metadata and by introducing an extensible set of priority schemas to be automatically handled by the extended DSPS. In addition, we show the effectiveness of our approach via its implementation and integration in our Quasit DSPS and through experimental evaluation of this prototype on a real-world stream processing application of Big Data vehicular traffic analysis.}, series = {UCC '14}, pages = {363\u2013370}, numpages = {8}, keywords = {Big Data, Cloud Computing Optimization, Vehicular Traffic Analysis, Application-level and Application-specific Scheduling, Distributed Stream Processing, Priority-based Resource Scheduling}}
@inproceedings{10.1145/3037697.3037699,title = {Big Data Analytics and Intelligence at Alibaba Cloud}, author = {Zhou Jingren },year = {2017}, isbn = {9781450344654}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3037697.3037699}, doi = {10.1145/3037697.3037699}, location = {Xi&apos;an, China}, series = {ASPLOS '17}, pages = {}}
@inproceedings{10.1145/3037699,title = {Big Data Analytics and Intelligence at Alibaba Cloud}, author = {Zhou Jingren },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3037699}, doi = {10.1145/3037699}, pages = {}, numpages = {1}}
@inproceedings{10.1145/3490395,title = {Identifying the Big Shots\u2014A Quantile-Matching Way in the Big Data Context}, author = {Li Guangrui (Kayla) , So Mike K. P. , Tam Kar Yan },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3490395}, doi = {10.1145/3490395}, abstract = {The prevalence of big data has raised significant epistemological concerns in information systems research. This study addresses two of them\u2014the deflated p-value problem and the role of explanation and prediction. To address the deflated p-value problem, we propose a multivariate effect size method that uses the log-likelihood ratio test. This method measures the joint effect of all variables used to operationalize one factor, thus overcoming the drawback of the traditional effect size method (\u03b8), which can only be applied at the single variable level. However, because factors can be operationalized as different numbers of variables, direct comparison of multivariate effect size is not possible. A quantile-matching method is proposed to address this issue. This method provides consistent comparison results with the classic quantile method. But it is more flexible and can be applied to scenarios where the quantile method fails. Furthermore, an absolute multivariate effect size statistic is developed to facilitate concluding without comparison. We have tested our method using three different datasets and have found that it can effectively differentiate factors with various effect sizes. We have also compared it with prediction analysis and found consistent results: explanatorily influential factors are usually also predictively influential in a large sample scenario.}, pages = {1\u201330}, numpages = {30}, keywords = {Big data, quantile matching, prediction analysis, deflated p-value, multivariate effect size}}
@inproceedings{10.1145/3400934.3400996,title = {Process Safety Management (PSM) and Reliability for Compressor Inspection Using Big Data Analytics: A Conceptual Study}, author = {Amalia Sarah Sholihatul , Sommeng Andy Noorsaman },year = {2020}, isbn = {9781450376006}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3400934.3400996}, doi = {10.1145/3400934.3400996}, abstract = {Process safety is related to leak prevention, oil spills, monitoring of equipment damage, overpressure, excess temperature, corrosion, metal fatigue, and other similar conditions. Besides, operations are related to productivity and risk management, so it is essential to monitor the process in depth. This paper is focusing on risk management of the downstream segment on the priority element of Process Safety Management (PSM). Based on research, Mechanical Integrity is the most critical element in PSM that have to be focused. The aspect of essential reliability of equipment, in this case, the compressor becomes vital to prevent shutdown/trip and unplanned maintenance, which will have an impact on oil and gas production. Historical failure data and support that include structured and unstructured data from the reciprocating compressor approximately from 2014 until 2019 will be collected. It will use to identify the damage patterns and reliability rates of the equipment. The regression value will be calculated by R as Big Data Analytics Software to determine whether Weibull distribution is sufficient. By using Weibull analysis, we can conclude that it will be more useful to use preventive maintenance as the first barrier from getting fail.}, location = {Depok, Indonesia}, series = {APCORISE 2020}, pages = {339\u2013343}, numpages = {5}, keywords = {Risk Management, Maintenance, Compressor, Big Data, Reliability, R Software}}
@inproceedings{10.1145/2663876.2663885,title = {Privacy Preserving Data Quality Assessment for High-Fidelity Data Sharing}, author = {Freudiger Julien , Rane Shantanu , Brito Alejandro E. , Uzun Ersin },year = {2014}, isbn = {9781450331517}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2663876.2663885}, doi = {10.1145/2663876.2663885}, abstract = {In a data-driven economy that struggles to cope with the volume and diversity of information, data quality assessment has become a necessary precursor to data analytics. Real-world data often contains inconsistencies, conflicts and errors. Such dirty data increases processing costs and has a negative impact on analytics. Assessing the quality of a dataset is especially important when a party is considering acquisition of data held by an untrusted entity. In this scenario, it is necessary to consider privacy risks of the stakeholders.This paper examines challenges in privacy-preserving data quality assessment. A two-party scenario is considered, consisting of a client that wishes to test data quality and a server that holds the dataset. Privacy-preserving protocols are presented for testing important data quality metrics: completeness, consistency, uniqueness, timeliness and validity. For semi-honest parties, the protocols ensure that the client does not discover any information about the data other than the value of the quality metric. The server does not discover the parameters of the client's query, the specific attributes being tested and the computed value of the data quality metric. The proposed protocols employ additively homomorphic encryption in conjunction with condensed data representations such as counting hash tables and histograms, serving as efficient alternatives to solutions based on private set intersection.}, location = {Scottsdale, Arizona, USA}, series = {WISCS '14}, pages = {21\u201329}, numpages = {9}, keywords = {privacy and confidentiality, data quality assessment, cryptographic protocols}}
@inproceedings{10.1109/CCGrid.2015.139,title = {Predicting and mitigating jobs failures in big data clusters}, author = {Ros\u00e0 Andrea , Chen Lydia Y. , Binder Walter },year = {2015}, isbn = {9781479980062}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2015.139}, doi = {10.1109/CCGrid.2015.139}, abstract = {In large-scale datacenters, software and hardware failures are frequent, resulting in failures of job executions that may cause significant resource waste and performance deterioration. To proactively minimize the resource inefficiency due to job failures, it is important to identify them in advance using key job attributes. However, so far, prevailing research on datacenter workload characterization has overlooked job failures, including their patterns, root causes, and impact. In this paper, we aim to develop prediction models and mitigation policies for unsuccessful jobs, so as to reduce the resource waste in big datacenters. In particular, we base our analysis on Google cluster traces, consisting of a large number of big-data jobs with a high task fanout. We first identify the time-varying patterns of failed jobs and the contributing system features. Based on our characterization study, we develop an on-line predictive model for job failures by applying various statistical learning techniques, namely Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Logistic Regression (LR). Furthermore, we propose a delay-based mitigation policy which, after a certain grace period, proactively terminates the execution of jobs that are predicted to fail. The particular objective of postponing job terminations is to strike a good tradeoff between resource waste and false prediction of successful jobs. Our evaluation results show that the proposed method is able to significantly reduce the resource waste by 41.9% on average, and keep false terminations of jobs low, i.e., only 1%.}, location = {Shenzhen, China}, series = {CCGRID '15}, pages = {221\u2013230}, numpages = {10}}
@inproceedings{10.1145/2788516,title = {Big data comes in tiny packages: single-cell driven science and health}, author = {Sachs Karen , Chen Tiffany },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2788516}, doi = {10.1145/2788516}, abstract = {Single-cell data creates computational opportunities for discovery in disease and human health.}, pages = {54\u201359}, numpages = {6}}
@inproceedings{10.1145/3349614.3356026,title = {Networked Cameras Are the New Big Data Clusters}, author = {Jiang Junchen , Zhou Yuhao , Ananthanarayanan Ganesh , Shu Yuanchao , Chien Andrew A. },year = {2019}, isbn = {9781450369282}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3349614.3356026}, doi = {10.1145/3349614.3356026}, abstract = {The increasing complexity of deep learning and massive deployment of cameras at the edge have drastically increased the resource demand of edge data analytics. Compared to traditional Internet web applications, such resource demand (in computing, storage and networking) is not limited by millions of human users, but rather the continuous activities of billions of sensors. This paper presents the abstraction of camera cluster as an attempt to address this challenge in the context of video analytics. We envision a novel analytics stack that orchestrates the computing resource of massive networked cameras to enable efficient edge video analytics.}, location = {Los Cabos, Mexico}, series = {HotEdgeVideo'19}, pages = {1\u20137}, numpages = {7}, keywords = {video analytics, edge, camera cluster}}
@inproceedings{10.1145/3286606.3286788,title = {The Big Data Deluge for Transforming the Knowledge of Smart Sustainable Cities: A Data Mining Framework for Urban Analytics}, author = {Bibri Simon Elias , Krogstie John },year = {2018}, isbn = {9781450365628}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3286606.3286788}, doi = {10.1145/3286606.3286788}, abstract = {There has recently been much enthusiasm about the possibilities created by the big data deluge to better understand, monitor, analyze, and plan modern cities to improve their contribution to the goals of sustainable development. Indeed, much of our knowledge of urban sustainability has been gleaned from studies that are characterized by data scarcity. Therefore, this paper endeavors to develop a systematic framework for urban sustainability analytics based on a cross-industry standard process for data mining. The intention is to enable well-informed decision-making and enhanced insights in relation to diverse urban domains. We argue that there is tremendous potential to transform and advance the knowledge of smart sustainable cities through the creation of a big data deluge that seeks to provide much more sophisticated, wider-scale, finer-grained, real-time understanding, and control of various aspects of urbanity in the undoubtedly upcoming Exabyte Age.}, location = {Tetouan, Morocco}, series = {SCA '18}, pages = {1\u201310}, numpages = {10}, keywords = {Smart sustainable cities, data mining, big data analytics}}
@inproceedings{10.1145/3540200,title = {Heterogeneous Big Data Parallel Computing Optimization Model using MPI/OpenMP Hybrid and Sensor Networks}, author = {Yin Fei , Shi Feng },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3540200}, doi = {10.1145/3540200}, abstract = {For the heterogeneous big data parallel computing model, two levels of parallelism between nodes are not considered, resulting in low efficiency of heterogeneous big data parallel computing and bandwidth to send and receive information, high communication overhead, long model running time and small computational volume. In the paper, we propose an optimization model of heterogeneous big data parallel computing based on a hybrid Multi Point Interface (MPI)/Open Multi-Processing (OpenMP) and Sensor Networks. First, the processor characteristics of heterogeneous big data architecture is analyzed, the parallel tasks among processors are divided, collect the heterogeneous big data to be computed and cluster them, and use the processing results as the input items of the model. Then, a parallel load balancing mechanism is established to optimally divide the parallel computing load of heterogeneous big data, and a parallel computing optimization program is written by combining the hybrid programming mode of MPI and OpenMP and using the hybrid MPI/OpenMP, and finally, the parallel computing optimization of heterogeneous big data is realized by optimizing the parallel communication and determining the model parameters. The results show that the proposed model has a communication bandwidth of 510Mbps, a computational volume of 1.16GB, a model runtime of 24s, and an improved network bandwidth utilization of 93%, which can effectively reduce the communication overhead, and improve the efficiency of parallel computing and bandwidth sending and receiving information in sensor networks, and shorten the model running time.}, keywords = {Machine Learning, Sensor Networks, Heterogeneous big data, Parallel computing, Reservoir computing, MPI/OpenMP hybrid}}
@inproceedings{10.1145/27544.27546,title = {Implications of data quality for spreadsheet analysis}, author = {Ballou Donald P. , Pazer Harold L. , Belardo Salvatore , Klein Barbara },year = {1987}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/27544.27546}, doi = {10.1145/27544.27546}, abstract = {This paper examines the impact of deficiencies in data quality on the results generated for spreadsheet applications. The purpose is to describe a framework which can be systematically used to determine the relative importance of potential errors in operational and judgmental data. Special emphasis is placed on analyzing the implications of deficiencies in data quality on projected spreadsheet results.}, pages = {13\u201319}, numpages = {7}}
@inproceedings{10.5555/2814058.2814137,title = {Open Source Tools Applied to Text Data Recovery in Big Data Environments}, author = {Attorre Brunno F. M. , Silva Leandro A. },year = {2015}, publisher = {Brazilian Computer Society}, address = {Porto Alegre, BRA}, abstract = {As the volume of data on the web continue to increase, it is getting more challenging for the search mechanism to find with a high precision rate what the users want to find. As a solution to improve these results, the development of a recommender engine, based on the content of the documents, would prove itself very useful. In this context, this research has the objective to show how the current search and indexing tools could be improved with recommendation, Machine Learning and textual analysis algorithms. The idea behind these project would be to, based on the content of the documents recovered in the search, find similar documents using most of the Open Source technology we have available right now.}, location = {Goiania, Goias, Brazil}, series = {SBSI 2015}, pages = {487\u2013492}, numpages = {6}, keywords = {Machine Learning, Machine Learning Tools, big data, Index tools}}
@inproceedings{10.1145/3473714.3473825,title = {Research on Key Technologies of Intelligent Recommendation Based Online Education Platform in Big Data Environment}, author = {Meng Hainie , Cheng Yunli },year = {2021}, isbn = {9781450390231}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3473714.3473825}, doi = {10.1145/3473714.3473825}, abstract = {The information education mode supported by emerging technologies such as big data technology, cloud computing, communication and the Internet of Things is called intelligent education. The purpose of promoting intelligent education is to make use of developed countries and emerging technological means to create intelligent, effective and accurate education methods and adopt correct talent training methods based on the results of big data calculation, thus laying a good foundation for the cultivation of high quality technology and technical talents. Intelligent education platform is a new way of education communication which is constantly improved and developed along with the Internet and education digitization and information. On the one hand, it brings students great convenience, but also provides a new way of learning; On the other hand, it also proposes a solution to the phenomenon of \"information overload\" caused by the rapid increment of learning resources. Secondly, students who have no basic knowledge of courses will have more choices in choosing courses and learning paths.}, location = {Guangzhou, China}, series = {ICCIR '21}, pages = {638\u2013645}, numpages = {8}, keywords = {collaborative filtering, Big data, Intelligent education system, course recommendation algorithm}}
@inproceedings{10.1145/3510858.3510971,title = {Development of Multimedia Communication Terminal Technology under Big Data Technology}, author = {Zhang Guoming },year = {2021}, isbn = {9781450390422}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510858.3510971}, doi = {10.1145/3510858.3510971}, abstract = {With the rapid development of big data and multimedia communication technology, modern multimedia communication terminal technology has made human life more and more convenient, enabling distance learning, TV program broadcasting, video conferencing, etc. The long-term development of multimedia communication terminal technology must conform to the corresponding system specifications in order to promote the sound development of multimedia communication. This article aims to study the development of multimedia communication terminal technology under big data technology. Taking the representative video conference in multimedia communication terminal technology as an example, combined with echo cancellation algorithms, a video conference system is designed and implemented. Performance was tested. The test results show that the system can meet the basic needs of video conferencing and can provide reliable real-time communication.}, location = {Changsha, China}, series = {ICASIT 2021}, pages = {384\u2013388}, numpages = {5}}
@inproceedings{10.1145/3512353.3512376,title = {Strategic Analysis and Research of Network Communication Security in the Age of Big Data}, author = {Ji Wenjun },year = {2022}, isbn = {9781450395571}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3512353.3512376}, doi = {10.1145/3512353.3512376}, abstract = {With the continuous development of the world economy, the information communication industry is developing extremely rapidly. Under the condition of the rapid development of the information and communication industry, the information and communication face serious network security problems because the relevant systems and equipment cannot meet the needs of the Internet information development. At the same time, with the continuous development of Internet technology, the current society has entered the era of big data. In the era of big data, information communication and network security have become extremely important. In order to scientifically avoid the security risks of network communication, the article deeply analyzes the communication network hardware equipment, data storage, data communication and communication network system, and proposes the strategy for relevant security management.}, location = {Virtual Event, Thailand}, series = {APIT 2022}, pages = {155\u2013159}, numpages = {5}, keywords = {Big data area, Strategic Analysis and research, Network Communication Security}}
@inproceedings{10.1145/1563821.1563874,title = {The Pathologies of Big Data: Scale up your datasets enough and all your apps will come undone. What are the typical problems and where do the bottlenecks generally surface?}, author = {Jacobs Adam },year = {2009}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1563821.1563874}, doi = {10.1145/1563821.1563874}, abstract = {What is \"big data\" anyway? Gigabytes? Terabytes? Petabytes? A brief personal memory may provide some perspective. In the late 1980s at Columbia University I had the chance to play around with what at the time was a truly enormous \"disk\": the IBM 3850 MSS (Mass Storage System). The MSS was actually a fully automatic robotic tape library and associated staging disks to make random access, if not exactly instantaneous, at least fully transparent. In Columbia\u2019s configuration, it stored a total of around 100 GB. It was already on its way out by the time I got my hands on it, but in its heyday, the early to mid-1980s, it had been used to support access by social scientists to what was unquestionably \"big data\" at the time: the entire 1980 U.S. Census database.}, pages = {10\u201319}, numpages = {10}}
@inproceedings{10.14778/3352063.3352130,title = {Experiences with approximating queries in Microsoft's production big-data clusters}, author = {Kandula Srikanth , Lee Kukjin , Chaudhuri Surajit , Friedman Marc },year = {2019}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3352063.3352130}, doi = {10.14778/3352063.3352130}, abstract = {With the rapidly growing volume of data, it is more attractive than ever to leverage approximations to answer analytic queries. Sampling is a powerful technique which has been studied extensively from the point of view of facilitating approximation. Yet, there has been no large-scale study of effectiveness of sampling techniques in big data systems. In this paper, we describe an in-depth study of the sampling-based approximation techniques that we have deployed in Microsoft's big data clusters. We explain the choices we made to implement approximation, identify the usage cases, and study detailed data that sheds insight on the usefulness of doing sampling based approximation.}, pages = {2131\u20132142}, numpages = {12}}
@inproceedings{10.1109/TCBB.2019.2951555,title = {Data-Enabled Digestive Medicine: A New Big Data Analytics Platform}, author = {Yan Lu , Huang Weihong , Wang Liming , Feng Song , Peng Yonghong , Peng Jie },year = {2021}, publisher = {IEEE Computer Society Press}, address = {Washington, DC, USA}, url = {https://doi.org/10.1109/TCBB.2019.2951555}, doi = {10.1109/TCBB.2019.2951555}, abstract = {This paper presents a big data analystics platform for clinical research and practice in the Gastroenterology Department of Xiangya Hospital at Central South University in China. This platform features a comprehensive and systematic support of big data in digestive medicine including geneneral health management, clinical gastroenterology practice, and related genomics research, which is proven to be helpful in real world clinical practices. A typical use case of integrated analysis based on electronic medical records and colonoscopy data was presented and discussed, the analaystic report on risk factors of colorectal diseases shows a reasonable recommendation about the age when people should start to screen the colorectal cancer, which could be very useful to individual and group health management for the general population in China.}, pages = {922\u2013931}, numpages = {10}}
@inproceedings{10.1145/2483574.2483579,title = {Exploiting in-network processing for big data management}, author = {Rupprecht Lukas },year = {2013}, isbn = {9781450321556}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2483574.2483579}, doi = {10.1145/2483574.2483579}, abstract = {Data processing systems face the task of efficiently storing and processing data at petabyte scale, with the amount set to increase in the future. To meet such a requirement, highly scalable, shared-nothing systems, e.g. Google's BigTable [6] or Facebook's Cassandra [14], are built to partition data and process it in parallel on distributed nodes in a cluster. This allows the handling of data at scale but introduces new challenges due to the distribution of data. Running queries involves a high network overhead because data has to be exchanged between cluster nodes and hence, the network becomes a critical part of the system. To avoid the network bottleneck, it is essential for distributed data processing systems (DDPS) to be aware of the network rather than treating it as a black box.We propose in-network processing as a way of achieving network-awareness to decrease bandwidth usage by custom routing, redundancy elimination, and on-path data reduction. Thereby, we can increase the query throughput of a DDPS. The challenges of an in-network processing system range from design issues, such as performance and transparency, to the integration with query optimisation and deployment in data centres. We formulate these challenges as possible research directions and provide a prototype implementation. Our preliminary results suggest that we can significantly improve query throughput in a DDPS by performing partial data reduction within the network.}, location = {New York, New York, USA}, series = {SIGMOD'13 PhD Symposium}, pages = {1\u20136}, numpages = {6}, keywords = {scale-out, nosql, network-awareness, date centres}}
@inproceedings{10.1145/2925686.2925691,title = {New chapters focus on big data and resource-constrained environments},year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2925686.2925691}, doi = {10.1145/2925686.2925691}, abstract = {SIGHPC is expanding its \"virtual chapter\" offerings through two new chapters: one focused on topics at the intersection of HPC and Big Data (SIGHPC-BigData), and the other on developing cyberinfrastructure and workforce development in resourceconstrained environments (SIGHPC-RCE). These join SIGHPC's first virtual chapter on Education in HPC (SIGHPC-Edu).}, pages = {7\u20138}, numpages = {2}}
@inproceedings{10.1145/1882291.1882293,title = {Big data, global development, and complex social systems}, author = {Eagle Nathan },year = {2010}, isbn = {9781605587912}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1882291.1882293}, doi = {10.1145/1882291.1882293}, abstract = {Petabytes of data about human movements, transactions, and communication patterns are continuously being generated by everyday technologies such as mobile phones and credit cards. In collaboration with the mobile phone, internet, and credit card industries, Eagle and colleagues are aggregating and analyzing behavioral data from over 250 million people from North and South America, Europe, Asia and Africa. Eagle discusses projects arising from these collaborations that involve inferring behavioral dynamics on a broad spectrum of scales from risky behavior in a group of MIT freshman to population-level behavioral signatures, including cholera outbreaks in Rwanda and wealth in the UK. The research group is developing a range of large-scale network analysis and machine learning algorithms that will provide deeper insight into human behavior.}, location = {Santa Fe, New Mexico, USA}, series = {FSE '10}, pages = {3\u20134}, numpages = {2}, keywords = {machine learning, behavioral dynamics, network analysis, data analysis}}
@inproceedings{10.1145/3167486.3167565,title = {Machine learning algorithms for oncology big data treatment}, author = {Mohammed Zouiten },year = {2017}, isbn = {9781450353069}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3167486.3167565}, doi = {10.1145/3167486.3167565}, abstract = {Two-dimensional arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 m and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements. Our work is part of user-centered healthcare decision-making systems based on a process of predicting cancer distribution. This process should lead to a set of knowledge in Datamining, Ontologies and Geographical Information Systems. It is in the same time iterative and interactive. Therefore, it seems essential to take into account principles and methods of Human-Machine Interaction in the development of such systems. In this respect, development of interactive decision-making systems is currently being approached using two opposing approaches. In the first one, technology is fundamental; the second one is user centered placing the human actors in a central position. Although the first approach is still present in healthcare organizations, the current trend is definitely the user centric. In our framework we propose an approach that aims to integrate the steps of the predicting future from data process into a development model enriched from human-machine interactions. Our application context is the fight against breast cancer in hospitals. We demonstrate that medical decision can be based on a spatial analysis of the geographical distribution of many cancers. Several factors explain our choice of datamining for assistance of health decision-makers for learning in the CART algorithm about patients who are future actors of suspicion.}, location = {Larache, Morocco}, series = {ICCWCS'17}, pages = {1\u20136}, numpages = {6}, keywords = {CART, GIS health, Redundancy, datamining, Machine learning}}
@inproceedings{10.1145/2382416.2382425,title = {Big data technology and implications for security research}, author = {Mell Peter },year = {2012}, isbn = {9781450316613}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2382416.2382425}, doi = {10.1145/2382416.2382425}, location = {Raleigh, North Carolina, USA}, series = {BADGERS '12}, pages = {15\u201316}, numpages = {2}}
@inproceedings{10.1145/3340531.3412182,title = {Automatic Gaussian Process Model Retrieval for Big Data}, author = {Berns Fabian , Beecks Christian },year = {2020}, isbn = {9781450368599}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3340531.3412182}, doi = {10.1145/3340531.3412182}, abstract = {Gaussian Process Models (GPMs) are widely regarded as a prominent tool for capturing the inherent characteristics of data. These bayesian machine learning models allow for data analysis tasks such as regression and classification. Usually a process of automatic GPM retrieval is needed to find an optimal model for a given dataset, despite prevailing default instantiations and existing prior knowledge in some scenarios, which both shortcut the way to an optimal GPM. Since non-approximative Gaussian Processes only allow for processing small datasets with low statistical versatility, we propose a new approach that allows to efficiently and automatically retrieve GPMs for large-scale data. The resulting model is composed of independent statistical representations for non-overlapping segments of the given data. Our performance evaluation of the new approach demonstrates the quality of resulting models, which clearly outperform default GPM instantiations, while maintaining reasonable model training time.}, location = {Virtual Event, Ireland}, series = {CIKM '20}, pages = {1965\u20131968}, numpages = {4}, keywords = {bayesian machine learning, performance evaluation, gaussian processes, regression, information retrieval}}
@inproceedings{10.1145/2809793,title = {Expanding minds to big data and data sciences}, author = {Dopplick Renee },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2809793}, doi = {10.1145/2809793}, pages = {88}, numpages = {1}}
@inproceedings{10.1145/2534921.2534924,title = {When big data meets big smog: a big spatio-temporal data framework for China severe smog analysis}, author = {Chen Jiaoyan , Chen Huajun , Pan Jeff Z. , Wu Ming , Zhang Ningyu , Zheng Guozhou },year = {2013}, isbn = {9781450325349}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2534921.2534924}, doi = {10.1145/2534921.2534924}, abstract = {Recently, the appearing disaster of severe smog has been attacking many cities in China such as the capital Beijing. The chief culprit of China smog, namely PM2.5, is affected by various factors including air pollutants, weather, climate, geographical location, urbanization, etc. To analyze the factors, we collect about 35,000,000 air quality records and about 30,000,000 weather records from the sensors in 77 China's cities in 2013. Moreover, two big data sets named Geoname and DBPedia are also combined for the data of climate, geographical location and urbanization. To deal with big spatio-temporal data for big smog analysis, we propose a MapReduce-based framework named BigSmog. It mainly conducts parallel correlation analysis of the factors and scalable training of artificial neural networks for spatio-temporal approximation of the concentration of PM2.5. In the experiments, BigSmog displays high scalability for big smog analysis with big spatio-temporal data. The analysis result shows that the air pollutants influence the short-term concentration of PM2.5 more than the weather and the factors of geographical location and climate rather than urbanization play a major role in determining a city's long-term pollution level of PM2.5. Moreover, the trained ANNs can accurately approximate the concentration of PM2.5.}, location = {Orlando, Florida}, series = {BigSpatial '13}, pages = {13\u201322}, numpages = {10}, keywords = {artificial neural network, correlation analysis, spatio-temporal, China smog, MapReduce, PM2.5}}
@inproceedings{10.1145/3277139.3277144,title = {The effect of big data analytics on firms sustainable competitive advantage of quality: a theory framework}, author = {Pu Guoli , Li Yuanyuan , Bai Ju },year = {2018}, isbn = {9781450364867}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3277139.3277144}, doi = {10.1145/3277139.3277144}, abstract = {How can firms achieve sustainable competitive advantage of quality (SCAQ)? Some typical cases show that big data analytics is a possible approach. Based on the theory of dynamic capability, this paper constructs a theoretical framework and possible hypotheses of big data analytics (BDA) in the supply chain field that influences the firms SCAQ. The theoretical framework includes: (1) the definition and the dimensions of big data supply chain analytic (BDSCA) and SCAQ; (2) the internal and external factors that influence the use of BDSCA based on the TOE framework; (3) the path and effect of BDSCA on SCQR; (4) moderating effects of industry characters and environmental uncertainty. The research contributes to define the connotation and characteristics of BDSCA from the perspective of management, clarify the impact mechanism of BDSCA on SCAQ, and seek ways to improve the firms SCAQ in the area of big data.}, location = {Chengdu, China}, series = {IMMS '18}, pages = {33\u201337}, numpages = {5}, keywords = {environmental uncertainty, industry characters, sustainable competitive advantage of quality, TOE framework, big data analytics}}
@inproceedings{10.1145/3425709,title = {Government Big Data Ecosystem: Definitions, Types of Data, Actors, and Roles and the Impact in Public Administrations}, author = {Shah Syed Iftikhar Hussain , Peristeras Vassilios , Magnisalis Ioannis },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3425709}, doi = {10.1145/3425709}, abstract = {The public sector, private firms, business community, and civil society are generating data that are high in volume, veracity, and velocity and come from a diversity of sources. This type of data is today known as big data. Public administrations pursue big data as \u201cnew oil\u201d and implement data-centric policies to collect, generate, process, share, exploit, and protect data for promoting good governance, transparency, innovative digital services, and citizens\u2019 engagement in public policy. All of the above constitute the Government Big Data Ecosystem (GBDE). Despite the great interest in this ecosystem, there is a lack of clear definitions, the various important types of government data remain vague, the different actors and their roles are not well defined, while the impact in key public administration sectors is not yet deeply understood and assessed. Such research and literature gaps impose a crucial obstacle for a better understanding of the prospects and nascent issues in exploiting GBDE. With this study, we aim to start filling the above-mentioned gaps by organizing our findings from an extended Systematic Literature Review into a framework to organise and address the above-mentioned challenges. Our goal is to contribute in this fast-evolving area by bringing some clarity and establishing common understanding around key elements of the emerging GBDE.}, pages = {1\u201325}, numpages = {25}, keywords = {data-driven government, Big data, government big data ecosystem, data and information, big data actors and roles}}
@inproceedings{10.14778/2556549.2556557,title = {Scalable progressive analytics on big data in the cloud}, author = {Chandramouli Badrish , Goldstein Jonathan , Quamar Abdul },year = {2013}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2556549.2556557}, doi = {10.14778/2556549.2556557}, abstract = {Analytics over the increasing quantity of data stored in the Cloud has become very expensive, particularly due to the pay-as-you-go Cloud computation model. Data scientists typically manually extract samples of increasing data size (progressive samples) using domain-specific sampling strategies for exploratory querying. This provides them with user-control, repeatable semantics, and result provenance. However, such solutions result in tedious workflows that preclude the reuse of work across samples. On the other hand, existing approximate query processing systems report early results, but do not offer the above benefits for complex ad-hoc queries. We propose a new progressive analytics system based on a progress model called Prism that (1) allows users to communicate progressive samples to the system; (2) allows efficient and deterministic query processing over samples; and (3) provides repeatable semantics and provenance to data scientists. We show that one can realize this model for atemporal relational queries using an unmodified temporal streaming engine, by re-interpreting temporal event fields to denote progress. Based on Prism, we build Now!, a progressive data-parallel computation framework for Windows Azure, where progress is understood as a first-class citizen in the framework. Now! works with \"progress-aware reducers\"- in particular, it works with streaming engines to support progressive SQL over big data. Extensive experiments on Windows Azure with real and synthetic workloads validate the scalability and benefits of Now! and its optimizations, over current solutions for progressive analytics.}, pages = {1726\u20131737}, numpages = {12}}
@inproceedings{10.1145/3297663.3309676,title = {AI Based Performance Benchmarking & Analysis of Big Data and Cloud Powered Applications: An in Depth View}, author = {Vemulapati Jayanti , Khastgir Anuruddha S. , Savalgi Chethana },year = {2019}, isbn = {9781450362399}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297663.3309676}, doi = {10.1145/3297663.3309676}, abstract = {Big data analytics platforms on cloud are becoming mainstream technology enabling cost-effective rapid deployment of customer's Big Data applications delivering quicker insights from their data. It is, therefore, even more imperative that we have high performant platform infrastructure and application at a reasonable cost. This is only possible if we make a transition from traditional approach to execute and measure performance by adopting new AI techniques such as Machine Learning (ML) & predictive approach to performance benchmarking for every application domain.This paper proposes a high-level conceptual model for automated performance benchmarking which includes execution engine that has been designed to support a self-service model covering automated benchmarking in every application domain. The automated engine is supported by performance scaling recommendations via prescriptive analytics from real performance data set.We furthermore extended the recommendation capabilities of our self-service automated engine by introducing predictive analytics for making it more flexible in addressing 'what-if' scenarios to predict 'Right Scale' with measurement of \"Performance Cost Ratio\" (PCR). Finally, we also present some real-world industry examples which have seen the performance benefits in their applications with the recommendations given by our proposed model.}, location = {Mumbai, India}, series = {ICPE '19}, pages = {103\u2013109}, numpages = {7}, keywords = {auto scale, complex deployments, predictive analytics, performance cost ratio, benchmarking, scale factor, performance tuning, big data, ai, ml, automation, performance metrics, right scale}}
@inproceedings{10.1145/3554726,title = {Behavior-driven testing of big data exploration tools}, author = {Battle Leilani },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3554726}, doi = {10.1145/3554726}, pages = {9\u201310}, numpages = {2}}
@inproceedings{10.1145/3443467.3443468,title = {Design of Infectious Disease Prevention and Control Platform Based on Big Data Analysis of Location Information}, author = {Li Xiong , Wang ShiYun },year = {2020}, isbn = {9781450387811}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3443467.3443468}, doi = {10.1145/3443467.3443468}, abstract = {In the case of large-scale transmission of Novel Coronavirus pneumonia through air and contact and a long incubation period, it is particularly important to control the further spread of potential infections. By designing a big data analysis platform for individual positioning information, this paper confirms suspected cases, isolated cases and confirmed cases, which is conducive to the prevention and control of the epidemic and improves the work efficiency.}, location = {Xiamen, China}, series = {EITCE 2020}, pages = {1\u20135}, numpages = {5}, keywords = {Big data, Epidemic detection, Differential GPS}}
@inproceedings{10.1145/3289600.3291372,title = {The 1st International Workshop on Context-Aware Recommendation Systems with Big Data Analytics (CARS-BDA)}, author = {Zhou Xiangmin , Zhang Ji , Zhang Yanchun },year = {2019}, isbn = {9781450359405}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3289600.3291372}, doi = {10.1145/3289600.3291372}, abstract = {With the explosive growth of online service platforms, increasing number of people and enterprises are doing everything online. In order for organizations, governments, and individuals to understand their users, and promote their products or services, it is necessary for them to analyse big data and recommend the media or online services in real time. Effective recommendation of items of interest to consumers has become critical for enterprises in domains such as retail, e-commerce, and online media. Driven by the business successes, academic research in this field has also been active for many years. Through many scientific breakthroughs have been achieved, there are still tremendous challenges in developing effective and scalable recommendation systems for real-world industrial applications. Existing solutions focus on recommending items based on pre-set contexts, such as time, location, weather etc. The big data sizes and complex contextual information add further challenges to the deployment of advanced recommender systems. This workshop aims to bring together researchers with wide-ranging backgrounds to identify important research questions, to exchange ideas from different research disciplines, and, more generally, to facilitate discussion and innovation in the area of context-aware recommender systems and big data analytics.}, location = {Melbourne VIC, Australia}, series = {WSDM '19}, pages = {842\u2013843}, numpages = {2}, keywords = {big data analysis, context-aware recommendation}}
@inproceedings{10.5555/3374138.3374194,title = {Research and implementation of efficient parallel processing of big data at TELBE user facility}, author = {Bawatna Mohammed , Green Bertram , Kovalev Sergey , Deinert Jan-Christoph , Knodel Oliver , Spallek Rainer G. },year = {2019}, publisher = {Society for Computer Simulation International}, address = {San Diego, CA, USA}, abstract = {In recent years, improvements in high-speed Analog-to-Digital Converters (ADC) and sensor technology has encouraged researchers to improve the performance of Data Acquisition (DAQ) systems for scientific experiments which require high speed and continuous data measurements --- in particular, measuring the electronic and magnetic properties of materials using pump-probe experiments at high repetition rates. Experiments at TELBE are capable of acquiring almost 100 Gigabytes of raw data every ten minutes. The DAQ system used at TELBE partitions the raw data into various subdirectories for further parallel processing utilizing the multicore structure of modern CPUs.Furthermore, several other types of processors that accelerate data processing like the GPU and FPGA have emerged to solve the challenges of processing the massive amount of raw data. However, the memory and network bottlenecks become a significant challenge in big data processing, and new scalable programming techniques are needed to solve these challenges. In this contribution, we will outline the design and implementation of our practical software approach for efficient parallel processing of our large data sets at the TELBE user facility.}, location = {Berlin, Germany}, series = {SummerSim '19}, pages = {1\u20136}, numpages = {6}, keywords = {signal processing, data processing pipeline, data acquisition systems, big data, data analytics}}
@inproceedings{10.1145/3190578,title = {Visual Interactive Creation, Customization, and Analysis of Data Quality Metrics}, author = {Bors Christian , Gschwandtner Theresia , Kriglstein Simone , Miksch Silvia , Pohl Margit },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3190578}, doi = {10.1145/3190578}, abstract = {During data preprocessing, analysts spend a significant part of their time and effort profiling the quality of the data along with cleansing and transforming the data for further analysis. While quality metrics\u2014ranging from general to domain-specific measures\u2014support assessment of the quality of a dataset, there are hardly any approaches to visually support the analyst in customizing and applying such metrics. Yet, visual approaches could facilitate users\u2019 involvement in data quality assessment. We present MetricDoc, an interactive environment for assessing data quality that provides customizable, reusable quality metrics in combination with immediate visual feedback. Moreover, we provide an overview visualization of these quality metrics along with error visualizations that facilitate interactive navigation of the data to determine the causes of quality issues present in the data. In this article, we describe the architecture, design, and evaluation of MetricDoc, which underwent several design cycles, including heuristic evaluation and expert reviews as well as a focus group with data quality, human-computer interaction, and visual analytics experts.}, pages = {1\u201326}, numpages = {26}, keywords = {data quality metrics, Data profiling, visual exploration}}
@inproceedings{10.1145/2668897,title = {Big data's end run around procedural privacy protections}, author = {Barocas Solon , Nissenbaum Helen },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2668897}, doi = {10.1145/2668897}, abstract = {Recognizing the inherent limitations of consent and anonymity.}, pages = {31\u201333}, numpages = {3}}
@inproceedings{10.1145/3464385.3468146,title = {Fashion Within the Big Data Society: How can data enable fashion transition towards a more meaningful and sustainable paradigm?}, author = {Bertola Paola },year = {2021}, isbn = {9781450389778}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3464385.3468146}, doi = {10.1145/3464385.3468146}, abstract = {The availability of big data is creating a paradigm shift in understanding and driving public opinions, informing individual behaviors and expectations. Therefore, future decision-making processes within companies and institutions will be driven by envisioning capacities based on data analytics that can provide meaningful representations of social behaviors. The paper addresses the current and potential impacts that digital transformation and data analytics are producing in the fashion industry. In particular, it shows two dynamics. On the one hand, the advent of the Internet, particularly social media, has transformed the interaction between brands and their customers\u2019 communities, pushing the fashion industry to embrace more sustainable models better reflecting their contemporary values, expectations, and behaviors. On the other hand, it explores how a new systemic approach to data analytics can empower the design process within the fashion industry to promote a radical sustainable transformation.}, location = {Bolzano, Italy}, series = {CHItaly '21}, pages = {1\u20138}, numpages = {8}, keywords = {fashion data analytics, design driven innovation, sustainable development}}
@inproceedings{10.5555/2888619.2888709,title = {Big data-driven service level analysis for a retail store}, author = {Gaku Rie , Takakuwa Soemon },year = {2015}, isbn = {9781467397414}, publisher = {IEEE Press}, abstract = {Using simulation technology, a procedure is proposed for a big data-driven service-level analysis for a real retail store. First, a data generator is designed to randomly select a sample of an expected number of customers or sampling data on a certain day from a large-scale dataset of sales predefined. Second, the clerk schedules are inputted into a data table created using Excel. Finally, simulation modeling mimics the service process of the retail store to examine and analyze the customer service level based on the selected data and the inputted clerk schedules. The proposed procedure for big data-driven service-level analysis shows the relations between the influencing service-level elements between the number of customers coming into stores, the frequency of customers, and the average customer service time. The procedure is generic and can easily be used to examine the service level in the remote past or to analyze and forecast the future.}, location = {Huntington Beach, California}, series = {WSC '15}, pages = {791\u2013799}, numpages = {9}}
@inproceedings{10.1145/3444370.3444614,title = {Human resource data quality management based on multiple regression analysis}, author = {Zhang Yong },year = {2020}, isbn = {9781450387828}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3444370.3444614}, doi = {10.1145/3444370.3444614}, abstract = {The essence of human resource management informatization is data. Firstly, human information is transformed into data, and then the data can be used. This requires that the comprehensive, complete, timely and effective human resource data is entered into the system, and then the data is analyzed by using the human resource management system to provide decision support for the management. Due to the complexity of the internal law of objective things and the limitation of people's cognition, it is impossible to analyze the internal causality of the actual object and establish a mathematical model in accordance with the mechanism law. Therefore, when some mathematical models cannot be established by mechanism analysis, we usually adopt the method of collecting a large amount of data, and establish the model based on the statistical analysis of the data. Among them, the most widely used random model is statistical regression model.}, location = {Guangzhou, China}, series = {CIAT 2020}, pages = {465\u2013470}, numpages = {6}, keywords = {Multiple regression analysis, human resources, data quality}}
@inproceedings{10.1145/2631775.2631812,title = {Big data visualization engines for understanding the development of countries, social networks, culture and cities}, author = {Hidalgo Cesar },year = {2014}, isbn = {9781450329545}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2631775.2631812}, doi = {10.1145/2631775.2631812}, abstract = {Big data can be used for more than improving the targeting of marketing campaigns. In this talk I will present five big data visualization engines we have created at the MIT Media Lab's Macro Connections group and will show how we can use big data and visualizations to improve our understanding of the development of economies, cultures and cities. The data visualization engines I will demo include (i) the Observatory of Economic Complexity (atlas.media.mit.edu), which is the most comprehensive tool for exploring international trade data created to date; (ii) DataViva (dataviva.info), which is a tool we created to open up data for the entire formal sector economy of Brazil, including data on all of the working force, municipalities, industries, and occupations of Brazil; (iii) Pantheon (pantheon.media.mit.edu), a dataset and visualization engine we created to explore global patterns of cultural production; (iv) Immersion (immersion.media.mit.edu), which is a tool that inverts the email interface, by focusing it on people rather than messages; and (v) Place Pulse and StreetScore (pulse.media.mit.edu & streetscore.media.mit.edu), which are crowd-sourcing and machine learning tools we have developed to help understand the aesthetic aspects of cities and their evolution.}, location = {Santiago, Chile}, series = {HT '14}, pages = {3}, numpages = {1}, keywords = {urban computing, big data, economic complexity, cultural production, information visualization, data visualization}}
@inproceedings{10.1145/2588555.2610498,title = {Indexing for interactive exploration of big data series}, author = {Zoumpatianos Kostas , Idreos Stratos , Palpanas Themis },year = {2014}, isbn = {9781450323765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2588555.2610498}, doi = {10.1145/2588555.2610498}, abstract = {Numerous applications continuously produce big amounts of data series, and in several time critical scenarios analysts need to be able to query these data as soon as they become available, which is not currently possible with the state-of-the-art indexing methods and for very large data series collections. In this paper, we present the first adaptive indexing mechanism, specifically tailored to solve the problem of indexing and querying very large data series collections. The main idea is that instead of building the complete index over the complete data set up-front and querying only later, we interactively and adaptively build parts of the index, only for the parts of the data on which the users pose queries. The net effect is that instead of waiting for extended periods of time for the index creation, users can immediately start exploring the data series. We present a detailed design and evaluation of adaptive data series indexing over both synthetic data and real-world workloads. The results show that our approach can gracefully handle large data series collections, while drastically reducing the data to query delay: by the time state-of-the-art indexing techniques finish indexing 1 billion data series (and before answering even a single query), adaptive data series indexing has already answered $3*10^5$ queries.}, location = {Snowbird, Utah, USA}, series = {SIGMOD '14}, pages = {1555\u20131566}, numpages = {12}, keywords = {adaptive indexing, data-series, adaptive data-series index, nearest neighbor, similarity search}}
@inproceedings{10.1145/2808492.2808532,title = {Centroid location algorithm in three dimensions based on big data}, author = {Chen Boyang , Ge Shiming , Xie Kaixuan },year = {2015}, isbn = {9781450335287}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2808492.2808532}, doi = {10.1145/2808492.2808532}, abstract = {Conventional centroid location algorithms are all in two dimensions. In order to solve the problem that the conventional centroid location algorithms are useless when the point spread function is smaller than the size of the detector, the research is about the centroid location algorithm in three dimensions based on big data. By using the time parameter to link the big data of energy received by the detector at different time, not only the single image but the time sequence images are used in the algorithm, based on the geometric theorem, the exact position at the special time is calculated out. It is sure that, the algorithm is very steady when the sample number is enough, that means the phase of the sample point is nothing, and the error of the position got by the algorithm is less than 0.06 pixel when the non-uniformity of the detectors is smaller than 5%, that is usually the upper limit of the non-uniformity of the detector.}, location = {Zhangjiajie, Hunan, China}, series = {ICIMCS '15}, pages = {1\u20136}, numpages = {6}, keywords = {centroid location, point target, three dimensions}}
@inproceedings{10.1145/3405962.3405970,title = {Data Management in Japanese Planetary Explorations for Big Data Era}, author = {Yamamoto Yukio , Ishikawa Hiroshi },year = {2020}, isbn = {9781450375429}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3405962.3405970}, doi = {10.1145/3405962.3405970}, abstract = {The data obtained by planetary explorations has various aspects such as decision making during an ongoing mission, anomaly detection for spacecraft safety, data archives for scientific analysis, and attractive snapshots for outreach. Each aspect requires each data formats and processing techniques. In this paper, we discuss changes in the environment surrounding planetary explorations and the handling of big data on computers. As a result, for the long-term preservation of scientific data, there must be standards and a community to endorse the standards. After standards, each community prepares the analysis tools. Furthermore, scientists need to make efforts not only in standardization but also in ensuring the quality of science. For highly informative data in recent years, the processing of data archives requires information science experts. Also, data providers or distributors should define data policies to clarify data usages to users. Finally, scientific analysis of cloud-based architecture due to big data and computer resources.}, location = {Biarritz, France}, series = {WIMS 2020}, pages = {88\u201390}, numpages = {3}, keywords = {Planetary Data System, SPICE, planetary exploration}}
@inproceedings{10.1145/2076450.2076453,title = {Researchers' big data crisis; understanding design and functionality}, author = {Stonebraker Michael , Hong Jason },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2076450.2076453}, doi = {10.1145/2076450.2076453}, abstract = {The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.twitterFollow us on Twitter at http://twitter.com/blogCACMhttp://cacm.acm.org/blogs/blog-cacmMichael Stonebraker issues a call to arms about research groups' data-management problems. Jason Hong discusses the nature of functionality with respect to design.}, pages = {10\u201311}, numpages = {2}}
@inproceedings{10.1145/3007818.3007838,title = {Spammer detection for real-time big data graphs}, author = {Eom Chris Soo-Hyun , Lee Wookey , Lee James Jung-Hun },year = {2016}, isbn = {9781450347549}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3007818.3007838}, doi = {10.1145/3007818.3007838}, abstract = {In recent years, prodigious explosion of social network services may trigger new business models. However, it has negative aspects such as personal information spill or spamming, as well. Amongst conventional spam detection approaches, the studies which are based on vertex degrees or Local Clustering Coefficient have been caused false positive results so that normal vertices can be specified as spammers. In this paper, we propose a novel approach by employing the circuit structure in the social networks, which demonstrates the advantages of our work through the experiment.}, location = {Jeju, Republic of Korea}, series = {EDB '16}, pages = {51\u201360}, numpages = {10}, keywords = {spammer, shortest path, local clustering coefficient, graph, circuit}}
@inproceedings{10.5555/3069658.3069683,title = {Development of an introductory big data programming and concepts course}, author = {DePratti Roland , Dancik Garrett M. , Lucci Fred , Sampson Russell D. },year = {2017}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {Computer scientists have been developing techniques to glean useful information from datasets for decades. The nascent disciplines of Big Data and Data Science have evolved over the last 10 years due to the rapid explosion in the amount of data collected by scientists, businesses, and other organizations. It is imperative that the next generation of workers is educated with the necessary knowledge to confront Big Data problems. It is the role of higher education institutions to train future data scientists and Big Data practitioners to fill those positions that the marketplace needs. This paper describes the choices and decisions made by one higher education institution to develop a course in Big Data Programming and Concepts that will be part of a future concentration in Data Science.}, pages = {175\u2013182}, numpages = {8}}
@inproceedings{10.1145/3289402.3289536,title = {A Text Classification Approach using Parallel Naive Bayes in Big Data Context}, author = {Amazal Houda , Ramdani Mohammed , Kissi Mohamed },year = {2018}, isbn = {9781450364621}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3289402.3289536}, doi = {10.1145/3289402.3289536}, abstract = {Text classification is a domain that has been inspiring researchers since many years. Indeed, several approaches have been developed in order to find methods that improve the performance of text classification. But in last decades, because of the technological evolution, textual data becomes more and more abundant on the web. So that classical classification methods are unable to process this huge amount of data and consequently cannot produce satisfied results. Thus, new ways have been explored; to overcome the big dimensions of data, it was necessary to reduce the size of the features of documents and use parallel processing. For this, in our work, we developed a Term Frequency- Inverse Document Frequency (TF-IDF) parallel model to save only the most relevant words in documents. Then, we feed the dataset to a parallel Naive Bayes classifier. Both, the TF-IDF parallel model and parallel Na\u00efve Bayes classifier were implemented on Hadoop system using the MapReduce architecture. The experimental results demonstrate the efficiency of the proposed method to improve the classification accuracy.}, location = {Rabat, Morocco}, series = {SITA'18}, pages = {1\u20136}, numpages = {6}, keywords = {TF-IDF, Big Data, Text classification, Machine Learning, MapReduce, Na\u00efve Bayes}}
@inproceedings{10.1145/2640087.2644169,title = {Symposium: CAP-Plus for Big Data}, author = {Shi Justin Y. },year = {2014}, isbn = {9781450328913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2640087.2644169}, doi = {10.1145/2640087.2644169}, abstract = {Data intensive parallel applications are harder to protect against transient software and hardware failures compared to traditional parallel applications. Due to the need for distributed data replication, the CAP Conjecture and Theorem define the ultimate limits for data intensive application's reliability, availability and overall scalability. This paper examines the two assumptions in the proof of CAP Theorem and proposes a statistic multiplexing paradigm for eliminating the reliability, availability and scalability limits of data intensive parallel applications.}, location = {Beijing, China}, series = {BigDataScience '14}, pages = {1\u20135}, numpages = {5}, keywords = {Statistic Multiplexed Computing, CAP Conjectur, Unlimited Scalability of Extreme Scale Data Intensive Application, CAP Theorem}}
@inproceedings{10.5555/2753024.2753046,title = {Challenges in designing an introductory course in big data programming: lightning talk}, author = {DePratti Roland A. },year = {2015}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {We live in a world where massive amounts of data are being generated, leading to advances in disciplines including physics, astronomy, biology, sociology, and business. This so-called Big Data cannot be stored and analyzed using traditional data storage and processing applications, yet its successful storage, mining, and analyses are critical for advances in the fields mentioned above and others. Since April 2014, four Computer Science professors from Eastern Connecticut State University have been participating in Big Data training exercises and developing an Introductory Course in Big Data Programming. However, the broad scope of Big Data and its relative newness pose key challenges to course development.}, pages = {104\u2013105}, numpages = {2}}
@inproceedings{10.1145/3297067.3297093,title = {Expressway Crash Prediction based on Traffic Big Data}, author = {Meng Hailang , Wang Xinhong , Wang Xuesong },year = {2018}, isbn = {9781450366052}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297067.3297093}, doi = {10.1145/3297067.3297093}, abstract = {With the development of society, the number of vehicles increases rapidly. The vehicle plays an important role in people's life, however the problem of traffic safety caused by vehicles has also become increasingly prominent. In China, the high crash rate and casualty rate on expressways have always troubled traffic management department. So crash prediction on expressway becomes vital. Conventionally, crash prediction is based on traffic flow data. These data do not contain all the necessary factors. In this paper, we propose a method of prediction using real-world data, including historical accident data, road geometry data, vehicle speed data, and weather data. We treat the crash prediction problem as a binary classification problem. For classification, sample imbalanced is a great challenge in practice. Modifying sample weights is applied to handle this challenge. Three machine learning classification techniques, namely Random Forest (RF), Gradient Boosting Decision Tree (GBDT) and Xgboost, are considered to carry out the crash prediction task respectively. The best recall and precision rate of these models are respectively 0.764253 and 0.01062. The proposed method can be integrated into urban traffic control systems toward police dispatch and crash prevention.}, location = {Shanghai, China}, series = {SPML '18}, pages = {11\u201316}, numpages = {6}, keywords = {feature extraction and selection, sample imbalance, Crash prediction, machine learning}}