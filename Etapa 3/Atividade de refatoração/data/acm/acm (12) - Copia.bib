@inproceedings{10.1145/3297662.3365787,
author = {Troiano, Ernesto and Soldatos, John and Polyviou, Ariana and Polyviou, Andreas and Mamelli, Alessandro and Drakoulis, Dimitris},
title = {Big Data Platform for Integrated Cyber and Physical Security of Critical Infrastructures for the Financial Sector: Critical Infrastructures as Cyber-Physical Systems},
year = {2020},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365787},
doi = {10.1145/3297662.3365787},
abstract = {As critical infrastructures become more complex, sophisticated and digitally interconnected they are also more susceptible to cyber and physical security attacks. In order to mitigate the risks of such attacks, there is a need for securing them in an integrated way, which considers the simultaneous protection of their cyber and physical assets. In this paper we introduce a BigData platform that implements an integrated approach to securing and protecting critical infrastructures for the financial sector, by treating them as large scale cyber-physical systems. The main building blocks of the platform include an integrated security model that covers cyber and physical assets, an architecture for security monitoring and control based on appropriate probes, as well as a range of data analytics algorithms for detecting risks, vulnerabilities and threats. These building blocks are outlined in the paper, along with their deployment and use in a number of representative critical infrastructure protection use cases for the financial sector. One of the merits of our work is its reference character i.e. it can serve as a blueprint for developing and deploying systems for integrated cyber/physical security in various application areas.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {262–269},
numpages = {8},
keywords = {Cyber Physical Systems, Finance, STIX, FINSTIX, Physical Security, Cybersecurity},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1145/3318396.3318427,
author = {Ho, P. C. W. and Fok, W. W. T. and Chan, C. K. K. and Yeung, H. H. Au and Ng, H. W. and Wong, S. L. and Ngai, S. Y. and Kwok, P. H. and Ho, Y. S. and Chan, K. H.},
title = {Flipping the Learning and Teaching of Reading Strategies and Comprehension through a Cloud-Based Interactive Big Data Reading Platform},
year = {2019},
isbn = {9781450362672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318396.3318427},
doi = {10.1145/3318396.3318427},
abstract = {This study investigates the learning approach of the designed Flipped Reading Platform (FRP) and its effects on primary school students' general Chinese reading and comprehension capabilities. This study was undertaken as part of the Quality Education Fund project in Hong Kong, titled "Flipped Reading: Enhancing the Learning and Teaching of Reading Strategies and Comprehension in Chinese via an Interactive Cloud Platform."This paper presents the design of the Interactive Cloud Platform FRP, which incorporates elements of both reading strategies and learning activities, and investigates the changes in students' reading performance, applied strategies, and active learning level with the application of FRP. The results show the experimental students using the FRP in the pilot scheme generally gained more in three stages of reading comprehension, and that low-achieving students learned reading strategies better. Analysis of FRP log activities shows students' active engagement in reading and perceived competence. Different learning outcomes were also found within the experimental group, categorized by BYOD and non-BYOD classes. Implications of the study show the effectiveness of FRP, and the design demonstrates how the reading measures integrated the assessment indicators of both international and local standards in the domain of Chinese Language reading. Further research can be developed to examine individual online reading performance and learning behaviour on FRP.},
booktitle = {Proceedings of the 2019 8th International Conference on Educational and Information Technology},
pages = {185–191},
numpages = {7},
keywords = {Big data, reading strategy, Chinese Language, Flipped reading, Cloud Platform, e-Learning},
location = {Cambridge, United Kingdom},
series = {ICEIT 2019}
}

@inproceedings{10.1145/3293663.3293683,
author = {Salman, Muhammad and Husna, Diyanatul and Apriliani, Stella Gabriella and Pinem, Josua Geovani},
title = {Anomaly Based Detection Analysis for Intrusion Detection System Using Big Data Technique with Learning Vector Quantization (LVQ) and Principal Component Analysis (PCA)},
year = {2018},
isbn = {9781450366410},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293663.3293683},
doi = {10.1145/3293663.3293683},
abstract = {Data security has become a very serious parf of any organizational information system. More and more threats across the Internet has evolved and capable to deceive firewall as well as antivirus software. In addition, the number of attacks become larger and become more dificult to be processed by the firewall or antivirus software. To improve the security of the system is usually done by adding Intrusion Detection System(IDS), which divided into anomaly-based detection and signature-based detection. In this research to process a huge amount of data, Big Data technique is used. Anomaly-based detection is proposed using Learning Vector Quantization Algorithm to detect the attacks. Learning Vector Quantization is a neural network technique that learn the input itself and then give the appropriate output according to the input. Modifications were made to improve test accuracy by varying the test parameters that present in LVQ. Varying the learning rate, epoch and k-fold cross validation resulted in a more efficient output. The output is obtained by calculating the value of information retrieval from the confusion matrix table from each attack classes. Principal Component Analysis technique is used along with Learning Vector Quantization to improve system performance by reducing the data dimensionality. By using 18-Principal Component, dataset successfully reduced by 47.3%, with the best Recognition Rate of 96.52% and time efficiency improvement up to 43.16%.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Virtual Reality},
pages = {20–23},
numpages = {4},
keywords = {Network Security, Principal Component Analysis (key words), Big Data, IDS, Learning Vector Quantization},
location = {Nagoya, Japan},
series = {AIVR 2018}
}

@inproceedings{10.1145/2184751.2184861,
author = {Yamashita, Akika and Iwaki, Saeko and Oguchi, Masato},
title = {An Evaluation of Input Data Quality of Lifelog Analysis Application with a Framework Based on Quantitative Index},
year = {2012},
isbn = {9781450311724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2184751.2184861},
doi = {10.1145/2184751.2184861},
abstract = {In recent years, by the improvement of the data acquisition technology and the development of storage, it has become greatly easier than before to collect lifelog that is to record the person's behavior as digital data. As a result, various lifelog analysis applications have been developed that offer the user profitable information such as person's action histories with an analysis of collected data by sensor terminals, video cameras, and so on.However, in these lifelog analysis applications, the quality of the data that was collected from the sensor terminals and inputted to the application was not discussed in detail. Therefore, in this paper, we have focused on the quality of video image data and the acceleration data of objects. As a representative lifelog analysis application, we have chosen an application which verbalizes person's behavior from the data, and shown the influence of the quality of input data on the execution result of the application by a quantitative index.An evaluation framework is proposed for the discussion of a correlation between input data and execution results of the application. As data processing methods, Bayesian Classifier and HMM are employed in his paper. With various conditions, it has been clarified how the quality of input data affects the result of the lifelog analysis application.},
booktitle = {Proceedings of the 6th International Conference on Ubiquitous Information Management and Communication},
articleno = {94},
numpages = {8},
keywords = {Bayesian classifier, data quality, hidden Markov model, lifelog analysis, quantitative index},
location = {Kuala Lumpur, Malaysia},
series = {ICUIMC '12}
}

@proceedings{10.1145/3409501,
title = {HPCCT &amp; BDAI '20: Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference &amp; 2020 3rd International Conference on Big Data and Artificial Intelligence},
year = {2020},
isbn = {9781450375603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 4th High Performance Computing and Cluster Technologies Conference (HPCCT 2020) and the 3rd International Conference on Big Data and Artificial Intelligence (BDAI 2020), to be held at Qingdao, China online during July 3-6, 2020. It is indeed a great privilege for us to present the proceedings of HPCCT 2020 and BDAI2020 to the authors and delegates of the event. We hope that you will find it educative and inspiring.},
location = {Qingdao, China}
}

@inproceedings{10.1145/3341069.3342974,
author = {Shiling, Zhang},
title = {Research on CF4 Gas Molecular Simulation of High Voltage GIS Based on MATLAB Big Data Analysis and Mining Technology},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3342974},
doi = {10.1145/3341069.3342974},
abstract = {Large-scale and high-tech energy computing system is used to simulate CF4 gas generated in high-voltage GIL/GIS. The numerical simulation software is used to simulate the experimental platform and the related modeling technology for the optical system. The simulation experiments of the CF4 gas absorption capacity, spectrum broadening and the gas cross-interference performance under high pressure were carried out. The non-spectroscopic infrared (NDIR) gas absorption spectroscopy experimental system was built. CF4 gas was used for the quantitative analysis experiments and compared with simulation experiments. According to the absorption spectrum of CF4 gas, the variation law of CF4 gas absorption spectrum with temperature, pressure, vibration and other factors was analyzed quantitatively. Under pressure condition, the ability to suppress the interference effect of the optical path of the on-line detection device is evaluated, and the optical path loss experiment is carried out. On the basis of experimental data, large data analysis and mining are carried out on the platform of MATLAB software, including the application of clustering algorithm to cluster the large data of CF4 gas, and to predict the time series of CF4 gas under fault conditions. The research results of this paper form the sample database of CF4 gas concentration to some certain extent, and put forward the CF4 gas molecular simulation of high-voltage combinations based on the large data analysis and mining technology of MATLAB, which provides theoretical and practical basis for the operation and maintenance of high-voltage combinations GIS/GIL.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {34–38},
numpages = {5},
keywords = {fault algorithm clustering, non-spectral infrared (NDIR) technology, MATLAB software platform, large data mining, CF4 gas molecular simulation},
location = {Guangzhou, China},
series = {HPCCT '19}
}

@proceedings{10.1145/3359992,
title = {Big-DAMA '19: Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks},
year = {2019},
isbn = {9781450369992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, FL, USA}
}

@inproceedings{10.1145/3352740.3352748,
author = {Min, Chen and Jinfen, Ye and Haoyu, Zhu},
title = {An Empirical Analysis of the Performance Management System of Private College Teachers under the Background of Big Data: Taking A College as an Example},
year = {2019},
isbn = {9781450372053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352740.3352748},
doi = {10.1145/3352740.3352748},
abstract = {In modern enterprise management, more and more companies value employee performance. Performance is equivalent to the external behavior of the company for employees. Effective performance management can stimulate all aspects of employees' work and enhance corporate image. Therefore, performance management runs through the whole work of employees. It is the key to improve employees' behavior and quality.With the acceleration of China's reform, opening up and modernization drive, more and more students need to receive higher education, which is followed by the emergence of private colleges. The number of private colleges is growing and the scale is expanding, which has gradually become an important part of higher education in China. Therefore, the management of private college teachers is crucial and inevitable. In fact, the performance management of private college teachers has become a widely studied issue. How to conduct an objective, fair and effective performance appraisal of teachers in private colleges has been paid more and more attention. Based on the reflection on the performance management of private college teachers, this paper takes a typical private college A College as an example, through the form of investigation report, to analyze the status quo and problems of teachers' performance. On this basis, the current performance management of teachers in private colleges is analyzed and suggested about improvement are put forward. Finally, through the analysis of the performance management of teachers in A College, this paper summarizes the problems that should be focused on in sustainable development of China's private colleges.},
booktitle = {Proceedings of the 2019 3rd International Workshop on Education, Big Data and Information Technology},
pages = {40–47},
numpages = {8},
keywords = {performance, private colleges, current situation, analysis, teachers},
location = {Guilin, China},
series = {EBDIT 2019}
}

@inproceedings{10.1145/3132847.3133187,
author = {Wang, Hongzhi and Ding, Xiaoou and Chen, Xiangying and Li, Jianzhong and Gao, Hong},
title = {CleanCloud: Cleaning Big Data on Cloud},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133187},
doi = {10.1145/3132847.3133187},
abstract = {We describe CleanCloud, a system for cleaning big data based on Map-Reduce paradigm in cloud. Using Map-Reduce paradigm, the system detects and repairs various data quality problems in big data. We demonstrate the following features of CleanCloud: (a) the support for cleaning multiple data quality problems in big data; (b) a visual tool for watching the status of big data cleaning process and tuning the parameters for data cleaning; (c) the friendly interface for data input and setting as well as cleaned data collection for big data. CleanCloud is a promising system that provides scalable and effect data cleaning mechanism for big data in either files or databases.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2543–2546},
numpages = {4},
keywords = {entity resolution, parallel computing, data cleaning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@proceedings{10.1145/2351316,
title = {BigMine '12: Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
year = {2012},
isbn = {9781450315470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Recent years have witnessed a dramatic increase in our ability to collect data from various sensors, devices, in different formats, from independent or connected applications. This data flood has outpaced our capability to process, analyze, store and understand these datasets. Consider the Internet data. The web pages indexed by Google were around one million in 1998, but quickly reached 1 billion in 2000 and have already exceeded 1 trillion in 2008. This rapid expansion is accelerated by the dramatic increase in acceptance of social networking applications, such as Facebook, Twitter, Weibo, etc., that allow users to create contents freely and amplify the already huge Web volume. Furthermore, with mobile phones becoming the sensory gateway to get real-time data on people from different aspects, the vast amount of data that mobile carrier can potentially process to improve our daily life has significantly outpaced our past CDR (call data record)- based processing for billing purposes only. It can be foreseen that Internet of things (IoT) applications will raise the scale of data to an unprecedented level. People and devices (from home coffee machines to cars, to buses, railway stations and airports) are all loosely connected. Trillions of such connected components will generate a huge data ocean, and valuable information must be discovered from the data to help improve quality of life and make our world a better place. For example, after we get up every morning, in order to optimize our commute time to work and complete the optimization before we arrive at office, the system needs to process information from traffic, weather, construction, police activities to our calendar schedules, and perform deep optimization under the tight time constraints. In all these applications, we are facing significant challenges in leveraging the vast amount of data, including challenges in (1) system capabilities (2) algorithmic design (3) business models.},
location = {Beijing, China}
}

@inproceedings{10.1145/3335656.3335699,
author = {Deng, Jianzhi and Cheng, Xiaohui and Zhou, Yuehan},
title = {Gene Clustering, Enrichment and Survival Analysis of Differentially Expressed Genes in Low Grade Glioma between Different Genders by Big Data Analysis},
year = {2019},
isbn = {9781450360906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335656.3335699},
doi = {10.1145/3335656.3335699},
abstract = {In this study, we aim to reveal the relationship between differentially expressed mRNAs and genders in low grade glioma (LGG) patients. Based on the downloaded RNA-seq files of LGG patients from The Cancer Genome Atlas database (TCGA), 89 differentially expressed mRNAs between male and female were screened out by clustering analysis. The differentially expressed mRNAs were analyzed by DAVID and KOBAS online tools. The differentially expressed mRNAs were enriched in 67 gene ontology terms, including cellular components, biological processes and molecular functions group and 7 signaling pathways. Then, the differentially expressed mRNAs were divided into two parts according to the expression level. The high-expressed mRNAs and low-expressed mRNAs were all analyzed with the clinical survival data by Kaplan-meier method and kmplot survival curves. In comparison with the LGG female patients, male patients with a differential expression mRNAs were closely related to the higher risk of LGG.},
booktitle = {Proceedings of the 2019 International Conference on Data Mining and Machine Learning},
pages = {136–139},
numpages = {4},
keywords = {data mining, Computational biology, medical big data, mRNA, TCGA, gender, Low Grade Glioma},
location = {Hong Kong, Hong Kong},
series = {ICDMML 2019}
}

@inproceedings{10.1145/3184558.3186334,
author = {Safder, Iqra and Hassan, Saeed-Ul and Aljohani, Naif Radi},
title = {AI Cognition in Searching for Relevant Knowledge from Scholarly Big Data, Using a Multi-Layer Perceptron and Recurrent Convolutional Neural Network Model},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186334},
doi = {10.1145/3184558.3186334},
abstract = {Although, over the years, information retrieval systems have shown tremendous improvements in searching for relevant scientific literature, human cognition is still required to search for specific document elements in full text publications. For instance, pseudocodes pertaining to algorithms published in scientific publications cannot be correctly matched against user queries, hence the process requires human involvement. AlgorithmSeer, a state-of-the-art technique, claims to replace humans in this task, but one of the limitations of such an algorithm search engine is that the metadata is simply a textual description of each pseudocode, without any algorithm-specific information. Hence, the search is performed merely by matching the user query to the textual metadata and ranking the results using conventional textual similarity techniques. The ability to automatically identify algorithm-specific metadata such as precision, recall, or f-measure would be useful when searching for algorithms. In this article, we propose a set of algorithms to extract further information pertaining to the performance of each algorithm. Specifically, sentences in an article that convey information about the efficiency of the corresponding algorithm are identified and extracted using a recurrent convolutional neural network (RCNN). Furthermore, we propose improving the efficacy of the pseudocode detection task by using a multi-layer perceptron (MLP) classification trained with 15 features, which improves the classification performance of the state-of-the-art pseudocode detection methods used in AlgorithmSeer by 27%. Finally, we show the advantages of the AI-enabled search engine (based on RCNN and MLP models) over conventional text-retrieval models.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {251–258},
numpages = {8},
keywords = {algorithm search, multi-layer perceptron (mlp), ai-enabled search engine, cognitive computing, scholarly big data, recurrent convolutional neural network (rcnn)},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.1007/s00779-019-01228-x,
author = {Xu, Caie and Xu, Lisha and Lu, Yingying and Xu, Huan and Zhu, Zhongliang},
title = {E-Government Recommendation Algorithm Based on Probabilistic Semantic Cluster Analysis in Combination of Improved Collaborative Filtering in Big-Data Environment of Government Affairs},
year = {2019},
issue_date = {Jul 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3–4},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-019-01228-x},
doi = {10.1007/s00779-019-01228-x},
abstract = {In order to solve the problem of information overload and satisfy their individual needs in the use of e-government system, a personalized e-government recommendation algorithm combining probabilistic semantic cluster analysis and collaborative filtering is proposed to recommend the items most associated with users. Firstly, the static basic attributes and dynamic behavior attributes of the users and items are modeled by probabilistic semantic cluster analysis separately. Secondly, based on the user's historical record and attribute similarity for user community discovery, the user set most similar to the target user is pre-filtered by collaborative filtering algorithm to improve the diversity of the recommended results and reduce the computational amount of the core recommendation process. Finally, the associated sequence mining of the items was taken full account of the business characteristic of e-government, and the item sequence mining with time dimension was added to further improve the accuracy of the recommended results. The simulation experiments were carried out with the information after desensitization of users on the Spark platform. The experimental results show that our proposed e-government recommendation algorithm is suitable for the recommendation of items with sequence or process characteristic and has higher recommendation accuracy compared with traditional recommendation algorithms. The multi-community attribution factor of the user increases the diversity of the recommended results.},
journal = {Personal Ubiquitous Comput.},
month = {jul},
pages = {475–485},
numpages = {11},
keywords = {Multi-community attribution, E-government, Big-data environment, Collaborative filtering, Personalized recommendation, Probabilistic semantic cluster}
}

@inproceedings{10.1145/3185768.3186300,
author = {Ivanov, Todor and Singhal, Rekha},
title = {ABench: Big Data Architecture Stack Benchmark},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3186300},
doi = {10.1145/3185768.3186300},
abstract = {Distributed big data processing and analytics applications demand a comprehensive end-to-end architecture stack consisting of big data technologies. However, there are many possible architecture patterns (e.g. Lambda, Kappa or Pipeline architectures) to choose from when implementing the application requirements. A big data technology in isolation may be best performing for a particular application, but its performance in connection with other technologies depends on the connectors and the environment. Similarly, existing big data benchmarks evaluate the performance of different technologies in isolation, but no work has been done on benchmarking big data architecture stacks as a whole. For example, BigBench (TPCx-BB) may be used to evaluate the performance of Spark, but is it applicable to PySpark or to Spark with Kafka stack as well? What is the impact of having different programming environments and/or any other technology like Spark? This vision paper proposes a new category of benchmark, called ABench, to fill this gap and discusses key aspects necessary for the performance evaluation of different big data architecture stacks.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {13–16},
numpages = {4},
keywords = {bigbench, ABench, big data, big data benchmarking},
location = {Berlin, Germany},
series = {ICPE '18}
}

@article{10.1145/2935694.2935702,
author = {Wang, Hongzhi and Li, Mingda and Bu, Yingyi and Li, Jianzhong and Gao, Hong and Zhang, Jiacheng},
title = {Cleanix: A Parallel Big Data Cleaning System},
year = {2016},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2935694.2935702},
doi = {10.1145/2935694.2935702},
abstract = {For big data, data quality problem is more serious. Big data cleaning system requires scalability and the abilityof handling mixed errors. Motivated by this, we develop Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. In this paper, we show the organization, data cleaning algorithms as well as the design of Cleanix.},
journal = {SIGMOD Rec.},
month = {may},
pages = {35–40},
numpages = {6}
}

@inproceedings{10.1145/3396452.3396465,
author = {Xu, Wei and Chen, Chongyang},
title = {Research on the Influencing Factors and Management Countermeasures of College Students' Sense of Security under the Environment of Big Data-an Empirical Analysis Based on the Event of COVID-19},
year = {2020},
isbn = {9781450374989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396452.3396465},
doi = {10.1145/3396452.3396465},
abstract = {Exposed in environment of big data, college students come easily into contact with massive data presentation. When crisis events occur, college students will be affected not only by crisis events but also by human psychological crisis. The greater psychological threat, faced by college students in the crisis environment, is the loss of sense of security. Through literature review, the hypotheses in crisis events are as followed: crisis events, government and media response, university coping measures, group coping behavior are the four main factors that affect college students' sense of security in crisis events. The outbreak of COVID-19 in Wuhan, Hubei affecting all the people, all colleges and universities across the country delayed the opening time. Among the affected universities, take the University of Electronic Science and Technology as an example, 600 samples were randomly selected to collect data. Through the exploratory factor analysis test, the influence hypotheses are verified. Through the structural equation model test, the four kinds of factors can explain the loss of college students' sense of security in the crisis, but show differences in explanatory power. Based on the elements of college students' sense of security, this paper puts forward an further explanation on the action path of the four factors on the public sense of security. According to the conclusion, we come to the conclusion that improving the coping ability of colleges and universities, enhancing the sense of crisis determination and the efficiency of control are the key to improve college students' sense of security and ensure the effectiveness of crisis management in colleges and universities.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Big Data and Education},
pages = {21–25},
numpages = {5},
keywords = {coping measures of colleges, government and media's response, dealing with emergency in groups, college students' sense of security, crisis events, structural equation model, big data},
location = {London, United Kingdom},
series = {ICBDE '20}
}

@inproceedings{10.1145/2640087.2644149,
author = {Mohammadian, Esmaeil and Noferesti, Morteza and Jalili, Rasool},
title = {FAST: Fast Anonymization of Big Data Streams},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644149},
doi = {10.1145/2640087.2644149},
abstract = {This paper proposes an anonymization algorithm (FAST) to speed up anonymization of big data streams. The proposed parallel algorithm provides an efficient big data anonymization by a multithread technique. A proactive time-expiration heuristic is applied to publish data before they are being expired. Our simulation results indicate significant improvement in big data stream anonymization in terms of information loss and cost metric.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {23},
numpages = {8},
keywords = {Big data, Anonymization, Stream, Privacy},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/2640087.2644187,
author = {Mohammadian, Esmaeil and Noferesti, Morteza and Jalili, Rasool},
title = {FAST: Fast Anonymization of Big Data Streams},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644187},
doi = {10.1145/2640087.2644187},
abstract = {This paper proposes an anonymization algorithm (FAST) to speed up anonymization of big data streams. The proposed parallel algorithm provides an efficient big data anonymization by a multithread technique. A proactive time-expiration heuristic is applied to publish data before they are being expired. Our simulation results indicate significant improvement in big data stream anonymization in terms of information loss and cost metric.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {31},
numpages = {2},
keywords = {Big data, Privacy, Anonymization, Stream},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/2661829.2661837,
author = {Wang, Hongzhi and Li, Mingda and Bu, Yingyi and Li, Jianzhong and Gao, Hong and Zhang, Jiacheng},
title = {Cleanix: A Big Data Cleaning Parfait},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661837},
doi = {10.1145/2661829.2661837},
abstract = {In this demo, we present Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate - the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. We demonstrate that Cleanix is a practical tool that supports effective and efficient data cleaning at the large scale.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {2024–2026},
numpages = {3},
keywords = {big data, data cleaning, data quality},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/2808797.2808839,
author = {Mart\'{\i}nez-Pel\'{a}ez, Jos\'{e} Juan and Buenabad-Ch\'{a}vez, Jorge and Rangel-Garc\'{\i}a, Jos\'{e} and Ram\'{\i}rez-Mel\'{e}ndez, Rafael},
title = {BDSP: A Big Data Start Platform},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2808839},
doi = {10.1145/2808797.2808839},
abstract = {We present the design and functionality of Big Data Start Platform (BDSP), a web system wherein users can perform Big Data tasks anytime, anywhere through any device with a browser and access to Internet. BDSP integrates different data sources and data processing tasks through web services. Its purpose is to serve in three ways: as a tool for rapid-prototyping of Big Data projects; as a base platform to be tuned and extended according to need; and as a training vehicle both in data analysis and in developing software for Big Data tasks.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {1110–1117},
numpages = {8},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/3395032.3395324,
author = {Ivanov, Todor and Ghazal, Ahmad and Crolotte, Alain and Kostamaa, Pekka and Ghazal, Yoseph},
title = {CoreBigBench: Benchmarking Big Data Core Operations},
year = {2020},
isbn = {9781450380010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395032.3395324},
doi = {10.1145/3395032.3395324},
abstract = {Significant effort was put into big data benchmarking with focus on end-to-end applications. While covering basic functionalities implicitly, the details of the individual contributions to the overall performance are hidden. As a result, end-to-end benchmarks could be biased toward certain basic functions. Micro-benchmarks are more explicit at covering basic functionalities but they are usually targeted at some highly specialized functions. In this paper we present CoreBigBench, a benchmark that focuses on the most common big data engines/platforms functionalities like scans, two way joins, common UDF execution and more. These common functionalities are benchmarked over relational and key-value data models which covers majority of data models. The benchmark consists of 22 queries applied to sales data and key-value web logs covering the basic functionalities. We ran CoreBigBench on Hive as a proof of concept and verified that the benchmark is easy to deploy and collected performance data. Finally, we believe that CoreBigBench is a good fit for commercial big data engines performance testing focused on basic engine functionalities not covered in end-to-end benchmarks.},
booktitle = {Proceedings of the Workshop on Testing Database Systems},
articleno = {4},
numpages = {6},
keywords = {big data benchmarking, BigBench, benchmark operations},
location = {Portland, Oregon},
series = {DBTest '20}
}

@inproceedings{10.1145/3357223.3362738,
author = {Contreras-Rojas, Bertty and Quian\'{e}-Ruiz, Jorge-Arnulfo and Kaoudi, Zoi and Thirumuruganathan, Saravanan},
title = {TagSniff: Simplified Big Data Debugging for Dataflow Jobs},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3362738},
doi = {10.1145/3357223.3362738},
abstract = {Although big data processing has become dramatically easier over the last decade, there has not been matching progress over big data debugging. It is estimated that users spend more than 50% of their time debugging their big data applications, wasting machine resources and taking longer to reach valuable insights. One cannot simply transplant traditional debugging techniques to big data. In this paper, we propose the TagSniff model, which can dramatically simplify data debugging for dataflows (the de-facto programming model for big data). It is based on two primitives -- tag and sniff -- that are flexible and expressive enough to model all common big data debugging scenarios. We then present Snoopy -- a general purpose monitoring and debugging system based on the TagSniff model. It supports both online and post-hoc debugging modes. Our experimental evaluation shows that Snoopy incurs a very low overhead on the main dataflow, 6% on average, as well as it is highly responsive to system events and users instructions.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {453–464},
numpages = {12},
keywords = {distributed systems, dataflow systems, data debugging, big data},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings{10.1145/2907294.2907319,
author = {Xu, Yiqi and Zhao, Ming},
title = {IBIS: Interposed Big-Data I/O Scheduler},
year = {2016},
isbn = {9781450343145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2907294.2907319},
doi = {10.1145/2907294.2907319},
abstract = {Big-data systems are increasingly shared by diverse, data-intensive applications from different domains. However, existing systems lack the support for I/O management, and the performance of big-data applications degrades in unpredictable ways when they contend for I/Os. To address this challenge, this paper proposes IBIS, an Interposed Big-data I/O Scheduler, to provide I/O performance differentiation for competing applications in a shared big-data system. IBIS transparently intercepts, isolates, and schedules an application's different phases of I/Os via an I/O interposition layer on every datanode of the big-data system. It provides a new proportional-share I/O scheduler, SFQ(D2), to allow applications to share the I/O service of each datanode with good fairness and resource utilization. It enables the distributed I/O schedulers to coordinate with one another and to achieve proportional sharing of the big-data system's total I/O service in a scalable manner. Finally, it supports the shared use of big-data resources by diverse frameworks and manages the I/Os from different types of big-data workloads (e.g., batch jobs vs. queries) across these frameworks. The prototype of IBIS is implemented in Hadoop/YARN, a widely used big-data system. Experiments based on a variety of representative applications (WordCount, TeraSort, Facebook, TPC-H) show that IBIS achieves good total-service proportional sharing with low overhead in both application performance and resource usages. IBIS is also shown to support various performance policies: it can deliver stronger performance isolation than native Hadoop/YARN (99% better for WordCount and 15% better for TPC-H queries) with good resource utilization; and it can also achieve perfect proportional slowdown with better application performance (30% better than native Hadoop).},
booktitle = {Proceedings of the 25th ACM International Symposium on High-Performance Parallel and Distributed Computing},
pages = {111–122},
numpages = {12},
keywords = {big data, storage management, i/o scheduler},
location = {Kyoto, Japan},
series = {HPDC '16}
}

@inproceedings{10.1145/3105831.3105847,
author = {Falt\'{\i}n, Tom\'{a}\v{s} and Hanzeli, Michal and \v{S}\'{\i}pek, Vojt\v{e}ch and \v{S}kva\v{r}il, Jan and Vari\v{s}, Du\v{s}an and Ml\'{y}nkov\'{a}, Irena Holubov\'{a}},
title = {BDgen: A Universal Big Data Generator},
year = {2017},
isbn = {9781450352208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105831.3105847},
doi = {10.1145/3105831.3105847},
abstract = {This paper introduces BDgen, a generator of Big Data targeting various types of users, implemented as a general and easily extensible framework. It is divided into a scalable backend designed to generate Big Data on clusters and a frontend for user-friendly definition of the structure of the required data, or its automatic inference from a sample data set. In the first release we have implemented generators of two commonly used formats (JSON and CSV) and the support for general grammars. We have also performed preliminary experimental comparisons confirming the advantages and competitiveness of the solution.},
booktitle = {Proceedings of the 21st International Database Engineering &amp; Applications Symposium},
pages = {200–208},
numpages = {9},
keywords = {Big Data generator, benchmarking, SW testing, synthetic data},
location = {Bristol, United Kingdom},
series = {IDEAS '17}
}

@inproceedings{10.1145/2723372.2747646,
author = {Khayyat, Zuhair and Ilyas, Ihab F. and Jindal, Alekh and Madden, Samuel and Ouzzani, Mourad and Papotti, Paolo and Quian\'{e}-Ruiz, Jorge-Arnulfo and Tang, Nan and Yin, Si},
title = {BigDansing: A System for Big Data Cleansing},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2747646},
doi = {10.1145/2723372.2747646},
abstract = {Data cleansing approaches have usually focused on detecting and fixing errors with little attention to scaling to big datasets. This presents a serious impediment since data cleansing often involves costly computations such as enumerating pairs of tuples, handling inequality joins, and dealing with user-defined functions. In this paper, we present BigDansing, a Big Data Cleansing system to tackle efficiency, scalability, and ease-of-use issues in data cleansing. The system can run on top of most common general purpose data processing platforms, ranging from DBMSs to MapReduce-like frameworks. A user-friendly programming interface allows users to express data quality rules both declaratively and procedurally, with no requirement of being aware of the underlying distributed platform. BigDansing takes these rules into a series of transformations that enable distributed computations and several optimizations, such as shared scans and specialized joins operators. Experimental results on both synthetic and real datasets show that BigDansing outperforms existing baseline systems up to more than two orders of magnitude without sacrificing the quality provided by the repair algorithms.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1215–1230},
numpages = {16},
keywords = {schema constraints, distributed data cleansing, distributed data repair, cleansing abstraction},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@article{10.1145/3068457,
author = {Chen, Bo-Wei and Ji, Wen and Li, Zhu},
title = {Guest Editorial for ACM TECS Special Issue on Effective Divide-and-Conquer, Incremental, or Distributed Mechanisms of Embedded Designs for Extremely Big Data in Large-Scale Devices},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3068457},
doi = {10.1145/3068457},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {apr},
articleno = {72},
numpages = {2}
}

@inproceedings{10.1145/3184558.3186205,
author = {Giannotti, Fosca and Trasarti, Roberto and Bontcheva, Kalina and Grossi, Valerio},
title = {SoBigData: Social Mining &amp; Big Data Ecosystem},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186205},
doi = {10.1145/3184558.3186205},
abstract = {One of the most pressing and fascinating challenges scientists face today, is understanding the complexity of our globally interconnected society. The big data arising from the digital breadcrumbs of human activities has the potential of providing a powerful social microscope, which can help us understand many complex and hidden socio-economic phenomena. Such challenge requires high-level analytics, modeling and reasoning across all the social dimensions above. There is a need to harness these opportunities for scientific advancement and for the social good, compared to the currently prevalent exploitation of big data for commercial purposes or, worse, social control and surveillance. The main obstacle to this accomplishment, besides the scarcity of data scientists, is the lack of a large-scale open ecosystem where big data and social mining research can be carried out. The SoBigData Research Infrastructure (RI) provides an integrated ecosystem for ethic-sensitive scientific discoveries and advanced applications of social data mining on the various dimensions of social life as recorded by "big data". The research community uses the SoBigData facilities as a "secure digital wind-tunnel" for large-scale social data analysis and simulation experiments. SoBigData promotes repeatable and open science and supports data science research projects by providing: (i) an ever-growing, distributed data ecosystem for procurement, access and curation and management of big social data, to underpin social data mining research within an ethic-sensitive context; (ii) an ever-growing, distributed platform of interoperable, social data mining methods and associated skills: tools, methodologies and services for mining, analysing, and visualising complex and massive datasets, harnessing the techno-legal barriers to the ethically safe deployment of big data for social mining; (iii) an ecosystem where protection of personal information and the respect for fundamental human rights can coexist with a safe use of the same information for scientific purposes of broad and central societal interest. SoBigData has a dedicated ethical and legal board, which is implementing a legal and ethical framework.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {437–438},
numpages = {2},
keywords = {e-infrastructure, European project},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.5555/3280489.3280492,
author = {Talaga, Paul G},
title = {Course-in-a-Box: Big Data},
year = {2018},
issue_date = {October 2018},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {34},
number = {1},
issn = {1937-4771},
abstract = {Massive amounts of data are being generated faster than ever before, with more resolution, and with varying formats. Unfortunately, traditional analysis tools are not able to handle these datasets in an efficient manner, if at all. In this course-in-a-box workshop, we will discuss the computational issues involved, with an emphasis on using modern big data analysis systems such as Hadoop, Hive, and Spark, for large datasets. In order for participants to bring this material into their classrooms, exercises, slides, exam questions, and curriculum recommendations based on teaching Big Data for three semesters will be discussed and provided for your own use.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {7},
numpages = {1}
}

@inproceedings{10.1109/CCGrid.2015.109,
author = {Fern\'{a}ndez, Victor and M\'{e}ndez, Victor and Pena, Tom\'{a}s F.},
title = {BigDataDIRAC: Deploying Distributed Big Data Applications},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.109},
doi = {10.1109/CCGrid.2015.109},
abstract = {The Distributed Infrastructure with Remote Agent Control (DIRAC) software framework allows a user community to manage computing activities in different grid and cloud environments. Many communities from several fields (LHCb, Belle II, Creatis, DIRAC4EGI multiple community portal, etc.) use DIRAC to run jobs in distributed environments. Google created the MapReduce programming model offering an efficient way of performing distributed computation over large data sets. Several enterprises are providing Hadoop cloud based resources to their users, and are trying to simplify the usage of Hadoop in the cloud.Based in these two robust technologies, we have created BigDataDIRAC, a solution which gives users the opportunity to access multiple Big Data resources scattered in different geographical areas, such as access to grid resources. This approach opens the possibility of offering not only grid and cloud to the users, but also Big Data resources from the same DIRAC environment. Proof of concept is shown using three computing centers in two countries, and with four Hadoop clusters. Our results demonstrate the ability of BigDataDIRAC to manage jobs driven by dataset location stored in the Hadoop File System (HDFS) of the Hadoop distributed clusters. DIRAC is used to monitor the execution, collect the necessary statistical data, and upload the results from the remote HDFS to the SandBox Storage machine. The tests produced the equivalent of 5 days continuous processing.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {1177–1180},
numpages = {4},
keywords = {big data, MapReduce, hive, DIRAC, hadoop, cloud computing, multi-cloud environment},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3090354.3090373,
author = {Mdarbi, Fatima Ezzahra and Afifi, Nadia and Hilal, Imane},
title = {Comparative Study: Dependability of Big Data in the Cloud},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090373},
doi = {10.1145/3090354.3090373},
abstract = {Cloud Computing provides computing resources remotely and on demand. These resources can be infrastructures, platforms, or application software. Whereas Big Data is well known for needing high capacity of both storage and computation's data streams. Thus, dependability must be studied in order to determine the attitude of Cloud systems to complete the features required by Big Data. The objective of this article is to deal with the general context of the Dependability and Cloud environment for Big Data. Thus, we carried out a statistic study of the research work that have been conducted in this context. The state of the art will allow us to approach the problematic of the dependability of Big Data in the Cloud. Our first objective is to know if there are many papers that deal at the same time a three thematics: the Cloud, the Big Data and the Dependability},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {19},
numpages = {6},
keywords = {IaaS, Velocity, Cloud Computing, Big Data, Value, PaaS, Variety, Veracity, Dependability, Volume, SaaS},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/2462902.2462922,
author = {Xu, Yiqi and Suarez, Adrian and Zhao, Ming},
title = {IBIS: Interposed Big-Data I/O Scheduler},
year = {2018},
isbn = {9781450319102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462902.2462922},
doi = {10.1145/2462902.2462922},
abstract = {Existing big-data systems (e.g., Hadoop/MapReduce) do not expose management of shared storage I/O resources. As such, application's performance may degrade in unpredictable ways under I/O contention, even with fair sharing of computing resources. This paper proposes emph{IBIS}, a new Interposed Big-data I/O Scheduler, to provide performance differentiation for competing applications' I/Os in a shared MapReduce-type big-data system. IBIS is implemented in Hadoop by interposing HDFS I/Os and use an SFQ-based proportional-sharing algorithm. Experiments show that the IBIS provides strong performance isolation for one application against another highly I/O-intensive application. IBIS also enforces good proportional sharing of the global bandwidth among competing parallel applications, by coordinating distributed IBIS schedulers to deal with the uneven distribution of local services in big-data systems.},
booktitle = {Proceedings of the 22nd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {109–110},
numpages = {2},
keywords = {proportional sharing},
location = {New York, New York, USA},
series = {HPDC '13}
}

@inproceedings{10.1145/2493123.2462922,
author = {Xu, Yiqi and Suarez, Adrian and Zhao, Ming},
title = {IBIS: Interposed Big-Data I/O Scheduler},
year = {2013},
isbn = {9781450319102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493123.2462922},
doi = {10.1145/2493123.2462922},
abstract = {Existing big-data systems (e.g., Hadoop/MapReduce) do not expose management of shared storage I/O resources. As such, application's performance may degrade in unpredictable ways under I/O contention, even with fair sharing of computing resources. This paper proposes emph{IBIS}, a new Interposed Big-data I/O Scheduler, to provide performance differentiation for competing applications' I/Os in a shared MapReduce-type big-data system. IBIS is implemented in Hadoop by interposing HDFS I/Os and use an SFQ-based proportional-sharing algorithm. Experiments show that the IBIS provides strong performance isolation for one application against another highly I/O-intensive application. IBIS also enforces good proportional sharing of the global bandwidth among competing parallel applications, by coordinating distributed IBIS schedulers to deal with the uneven distribution of local services in big-data systems.},
booktitle = {Proceedings of the 22nd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {109–110},
numpages = {2},
keywords = {proportional sharing},
location = {New York, New York, USA},
series = {HPDC '13}
}

@inproceedings{10.1145/2430475.2430501,
author = {Guigang, Zhang and Chao, Li and Chunxiao, Xing and Yong, Zhang},
title = {SemanMR: Big Data Processing Framework Based on Semantics},
year = {2012},
isbn = {9781450318884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430475.2430501},
doi = {10.1145/2430475.2430501},
abstract = {In this paper, we design a kind of big data processing framework SemanMR (Semantic MapReduce). SemanMR is a programming framework based on the Hadoop MapReduce programming model. SemanMR provide a kind of bid data processing mechanism based on the metadata cluster of distributed file systems or cloud databases. In addition, we add some semantic index on the big data, and so it will improve our processing efficiency in SemanMR. SemanMR is a kind of big data processing internetware in the cloud environment.},
booktitle = {Proceedings of the Fourth Asia-Pacific Symposium on Internetware},
articleno = {26},
numpages = {8},
keywords = {cloud data processing internetware, SemanMR, MapReduce, semantics, big data},
location = {Qingdao, China},
series = {Internetware '12}
}

@inproceedings{10.5555/3320516.3320577,
author = {Sanchez, Susan M.},
title = {Data Farming: Better Data, Not Just Big Data},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Data mining tools have been around for several decades, but the term "big data" has only recently captured widespread attention. Numerous success stories have been promulgated as organizations have sifted through massive volumes of data to find interesting patterns that are, in turn, transformed into actionable information. Yet a key drawback to the big data paradigm is that it relies on observational data, limiting the types of insights that can be gained. The simulation world is different. A "data farming" metaphor captures the notion of purposeful data generation from simulation models. Large-scale experiments let us grow the simulation output efficiently and effectively. We can use modern statistical and visual analytic methods to explore massive input spaces, uncover interesting features of complex simulation response surfaces, and explicitly identify cause-and-effect relationships. With this new mindset, we can achieve tremendous leaps in the breadth, depth, and timeliness of the insights yielded by simulation.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {425–439},
numpages = {15},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/2976749.2976753,
author = {Kantarcioglu, Murat and Xi, Bowei},
title = {Adversarial Data Mining: Big Data Meets Cyber Security},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2976753},
doi = {10.1145/2976749.2976753},
abstract = {As more and more cyber security incident data ranging from systems logs to vulnerability scan results are collected, manually analyzing these collected data to detect important cyber security events become impossible. Hence, data mining techniques are becoming an essential tool for real-world cyber security applications. For example, a report from Gartner [gartner12] claims that "Information security is becoming a big data analytics problem, where massive amounts of data will be correlated, analyzed and mined for meaningful patterns". Of course, data mining/analytics is a means to an end where the ultimate goal is to provide cyber security analysts with prioritized actionable insights derived from big data. This raises the question, can we directly apply existing techniques to cyber security applications? One of the most important differences between data mining for cyber security and many other data mining applications is the existence of malicious adversaries that continuously adapt their behavior to hide their actions and to make the data mining models ineffective. Unfortunately, traditional data mining techniques are insufficient to handle such adversarial problems directly. The adversaries adapt to the data miner's reactions, and data mining algorithms constructed based on a training dataset degrades quickly. To address these concerns, over the last couple of years new and novel data mining techniques which is more resilient to such adversarial behavior are being developed in machine learning and data mining community. We believe that lessons learned as a part of this research direction would be beneficial for cyber security researchers who are increasingly applying machine learning and data mining techniques in practice.To give an overview of recent developments in adversarial data mining, in this three hour long tutorial, we introduce the foundations, the techniques, and the applications of adversarial data mining to cyber security applications. We first introduce various approaches proposed in the past to defend against active adversaries, such as a minimax approach to minimize the worst case error through a zero-sum game. We then discuss a game theoretic framework to model the sequential actions of the adversary and the data miner, while both parties try to maximize their utilities. We also introduce a modified support vector machine method and a relevance vector machine method to defend against active adversaries. Intrusion detection and malware detection are two important application areas for adversarial data mining models that will be discussed in details during the tutorial. Finally, we discuss some practical guidelines on how to use adversarial data mining ideas in generic cyber security applications and how to leverage existing big data management tools for building data mining algorithms for cyber security.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1866–1867},
numpages = {2},
keywords = {big data analytics for cyber security, adversarial data mining},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/2839509.2844560,
author = {Silva, Yasin N. and Almeida, Isadora and Queiroz, Michell},
title = {SQL: From Traditional Databases to Big Data},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2844560},
doi = {10.1145/2839509.2844560},
abstract = {The Structured Query Language (SQL) is the main programing language designed to manage data stored in database systems. While SQL was initially used only with relational database management systems (RDBMS), its use has been significantly extended with the advent of new types of database systems. Specifically, SQL has been found to be a powerful query language in highly distributed and scalable systems that process Big Data, i.e., datasets with high volume, velocity and variety. While traditional relational databases represent now only a small fraction of the database systems landscape, most database courses that cover SQL consider only the use of SQL in the context of traditional relational systems. In this paper, we propose teaching SQL as a general language that can be used in a broad range of database systems from traditional RDBMSs to Big Data systems. This paper presents well-structured guidelines to introduce SQL in the context of new types of database systems including MapReduce, NoSQL and NewSQL. A key contribution of this paper is the description of an array of course resources, e.g., virtual machines, sample projects, and in-class exercises, to enable a hands-on experience with SQL across a broad set of modern database systems.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {413–418},
numpages = {6},
keywords = {databases curricula, sql, big data, structured query language},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/3102304.3102307,
author = {Han, Liangxiu},
title = {Towards Sustainable Smart Society: Big Data Driven Approaches},
year = {2017},
isbn = {9781450348447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102304.3102307},
doi = {10.1145/3102304.3102307},
abstract = {By 2020, the total size of digital data generated by social networks, sensors, biomedical imaging and simulation devices, will reach an estimated 44 Zettabytes (e.g. 44 trillion gigabytes) according to IDC report. This type of 'big data', together with the advances in information and communication technologies such as Internet of Things (IoT), connected smart objects, wearable technology, ubiquitous computing, is transforming every aspect of modern life and bringing great challenges and spectacular opportunities to fulfill our dream of a sustainable smart society.This talk will focus on new developments and methods based on big data driven approaches to address society challenges. The talk will also present real case studies to demonstrate how we apply big data approaches in various application domains such as Health, Food, Smart Cities, etc. to realize the smart society.},
booktitle = {Proceedings of the International Conference on Future Networks and Distributed Systems},
articleno = {3},
keywords = {Big data, Internet of Things, Smart cities, Societal challenges},
location = {Cambridge, United Kingdom},
series = {ICFNDS '17}
}

@inproceedings{10.1145/2463676.2463683,
author = {Barnett, Mike and Chandramouli, Badrish and DeLine, Robert and Drucker, Steven and Fisher, Danyel and Goldstein, Jonathan and Morrison, Patrick and Platt, John},
title = {Stat! An Interactive Analytics Environment for Big Data},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2463683},
doi = {10.1145/2463676.2463683},
abstract = {Exploratory analysis on big data requires us to rethink data management across the entire stack -- from the underlying data processing techniques to the user experience. We demonstrate Stat! -- a visualization and analytics environment that allows users to rapidly experiment with exploratory queries over big data. Data scientists can use Stat! to quickly refine to the correct query, while getting immediate feedback after processing a fraction of the data. Stat! can work with multiple processing engines in the backend; in this demo, we use Stat! with the Microsoft StreamInsight streaming engine. StreamInsight is used to generate incremental early results to queries and refine these results as more data is processed. Stat! allows data scientists to explore data, dynamically compose multiple queries to generate streams of partial results, and display partial results in both textual and visual form.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1013–1016},
numpages = {4},
keywords = {visualization, interactive, big data, analytics, tool},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/3416921.3416944,
author = {Cuzzocrea, Alfredo},
title = {OLAPing Big Social Data: Multidimensional Big Data Analytics over Big Social Data Repositories},
year = {2020},
isbn = {9781450375382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416921.3416944},
doi = {10.1145/3416921.3416944},
abstract = {Nowadays, a great deal of attention is devoted to the relevant problem of supporting big data analytics from social systems (e.g., social networks, smart city applications, skill management platforms, and so forth). Following this innovative trend, the opportunity of adopting advanced OLAP-based tools for supporting the knowledge extraction phase from big social data represents the new frontiers for big social data computing. Indeed, the well-known features of multidimensional data analysis are able to support a "rich" extraction of actionable knowledge, beyond actual limitations of alternative procedural approaches. In line with this emerging research challenge, this paper explores benefits, limitations and challenges of OLAP-based big data analytics tools over (big) social data.},
booktitle = {Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing},
pages = {15–19},
numpages = {5},
keywords = {OLAPing big social data, Big social data, Big data analytics, Big social data computing},
location = {Virtual, United Kingdom},
series = {ICCBDC '20}
}

@article{10.14778/2824032.2824123,
author = {S., Ashoke and Haritsa, Jayant R.},
title = {CODD: A Dataless Approach to Big Data Testing},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824123},
doi = {10.14778/2824032.2824123},
abstract = {The construction and development of the so-called Big Data systems has occupied centerstage in the data management community in recent years. However, there has been comparatively little attention paid to the testing of such systems, an essential pre-requisite for successful deployment. This is surprising given that traditional testing techniques, which typically involve construction of representative databases and regression query suites, are completely impractical at Big Data scale -- simply due to the time and space overheads involved in their execution. For instance, consider the situation where a database engineer wishes to evaluate the query optimizer's behavior on a futuristic Big Data setup featuring "yottabyte" (1024 bytes) sized relational tables. Obviously, just generating this data, let alone storing it, is practically infeasible even on the best of systems.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2008–2011},
numpages = {4}
}

@inproceedings{10.1145/2600428.2617559,
author = {Williams, Hugh E.},
title = {The Data Revolution: How Companies Are Transforming with Big Data},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2617559},
doi = {10.1145/2600428.2617559},
abstract = {Spelling correction in the 1990s was all about algorithms and small dictionaries. This century, it is about mining vast data sets of past user behaviors, simple algorithms, and using those to correct mistakes. The large Internet giants are data-driven enterprises that use data to transform and continually improve user experiences. In this talk, Hugh Williams shares stories about data and how it is used to build Internet products, and explains why he believes data will transform businesses as we know them. Every major company is becoming a data-driven company, and Hugh shares examples of transformations occurring in health, aviation, farming, and telecommunications. He recently joined Pivotal, a company that is assembling the toolkit that exists in only a few consumer Internet companies, and making that toolkit open and available to every industry, including big data platforms, development frameworks, and an open, cloud-independent Platform-as-a-Service. He will conclude by sharing details about Pivotal, the Pivotal vision, and roadmap.Hugh E. Williams has been Senior Vice President of Research &amp; Development at Pivotal since January 2014. His teams build big data technologies, and development frameworks and services, including Pivotal's Hadoop, Spring Java framework, and Greenplum database offerings. Most recently, he spent four and a half years as an executive with eBay where he was responsible for the team that conceived, designed, and built eBay's user experiences, search engine, big data technologies and platforms. Prior to joining eBay, he managed an R&amp;D team at Microsoft's Bing for four and a half years, spent over ten years researching and developing search technologies, and ran his own startup and consultancy for several years. He has published over 100 works, mostly in the field of Information Retrieval, including two books for O'Reilly Media Inc. He holds 19 U.S. patents, with many more pending. He has a PhD from RMIT University in Australia.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {525–526},
numpages = {2},
keywords = {big data, hadoop, data-driven applications, consumer internet},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@article{10.14778/2733004.2733083,
author = {Wu, Sai and Chen, Chun and Chen, Gang and Chen, Ke and Shou, Lidan and Cao, Hui and Bai, He},
title = {YZStack: Provisioning Customizable Solution for Big Data},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733083},
doi = {10.14778/2733004.2733083},
abstract = {YZStack is our developing solution which implements many well-established big data techniques as selectable modules and allows users to customize their systems as a process of module selection. In particular, it includes an openstack based IaaS (Infrastructure as a Service) layer, a distributed file system based DaaS (Data as a Service) layer, a PaaS (Platform as a Service) layer equipped with parallel processing techniques and a SaaS (Software as a Service) layer with popular data analytic algorithms. Layers of YZStack are loosely connected, so that customization of one layer does not affect the other layers and their interactions. In this paper, we use a smart financial system developed for the Zhejiang Provincial Department of Finance to demonstrate how to leverage YZStack to speed up the implementation of big data system. We also introduce two popular applications of the financial system, economic prediction and detection of improper payment.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1778–1783},
numpages = {6}
}

@inproceedings{10.1145/2487788.2488042,
author = {De Francisci Morales, Gianmarco},
title = {SAMOA: A Platform for Mining Big Data Streams},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488042},
doi = {10.1145/2487788.2488042},
abstract = {Social media and user generated content are causing an ever growing data deluge. The rate at which we produce data is growing steadily, thus creating larger and larger streams of continuously evolving data. Online news, micro-blogs, search queries are just a few examples of these continuous streams of user activities. The value of these streams relies in their freshness and relatedness to ongoing events. However, current (de-facto standard) solutions for big data analysis are not designed to deal with evolving streams.In this talk, we offer a sneak preview of SAMOA, an upcoming platform for mining dig data streams. SAMOA is a platform for online mining in a cluster/cloud environment. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as S4 and Storm. SAMOA includes algorithms for the most common machine learning tasks such as classification and clustering. Finally, SAMOA will soon be open sourced in order to foster collaboration and research on big data stream mining.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {777–778},
numpages = {2},
keywords = {big data, machine learning, stream mining, data streams, open source, distributed computing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2949550.2949556,
author = {Chiu, Chui-hui and Lewis, Nathan and Singh, Dipak Kumar and Das, Arghya Kusum and Jalazai, Mohammad M. and Platania, Richard and Goswami, Sayan and Lee, Kisung and Park, Seung-Jong},
title = {BIC-LSU: Big Data Research Integration with Cyberinfrastructure for LSU},
year = {2016},
isbn = {9781450347556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2949550.2949556},
doi = {10.1145/2949550.2949556},
abstract = {In recent years, big data analysis has been widely applied to many research fields including biology, physics, transportation, and material science. Even though the demands for big data migration and big data analysis are dramatically increasing in campus IT infrastructures, there are several technical challenges that need to be addressed. First of all, frequent big data transmission between storage systems in different research groups imposes heavy burdens on a regular campus network. Second, the current campus IT infrastructure is not designed to fully utilize the hardware capacity for big data migration and analysis. Last but not the least, running big data applications on top of large-scale high-performance computing facilities is not straightforward, especially for researchers and engineers in non-IT disciplines.We develop a campus IT cyberinfrastructure for big data migration and analysis, called BIC-LSU, which consists of a task-aware Clos OpenFlow network, high-performance cache storage servers, customized high-performance transfer applications, a light-weight control framework to manipulate existing big data storage systems and job scheduling systems, and a comprehensive social networking-enabled web portal. BIC-LSU achieves 40Gb/s disk-to-disk big data transmission, maintains short average transmission task completion time, enables the convergence of control on commonly deployed storage and job scheduling systems, and enhances easiness of big data analysis with a universal user-friendly interface. BIC-LSU software requires minimum dependencies and has high extensibility. Other research institutes can easily customize and deploy BIC-LSU as an augmented service on their existing IT infrastructures.},
booktitle = {Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale},
articleno = {28},
numpages = {8},
keywords = {science gateway, software-defined networking, task-aware network scheduling, solid-state drive storage server, Big data},
location = {Miami, USA},
series = {XSEDE16}
}

@inproceedings{10.1145/3243176.3243190,
author = {Gao, Wanling and Zhan, Jianfeng and Wang, Lei and Luo, Chunjie and Zheng, Daoyi and Tang, Fei and Xie, Biwei and Zheng, Chen and Wen, Xu and He, Xiwen and Ye, Hainan and Ren, Rui},
title = {Data Motifs: A Lens towards Fully Understanding Big Data and AI Workloads},
year = {2018},
isbn = {9781450359863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243176.3243190},
doi = {10.1145/3243176.3243190},
abstract = {The complexity and diversity of big data and AI workloads make understanding them difficult and challenging. This paper proposes a new approachto modelling and characterizing big data and AI workloads. We consider each big data and AI workload as a pipeline of one or more classes of units of computation performed on different initial or intermediate data inputs. Each class of unit of computation captures the common requirements while being reasonably divorced from individual implementations, and hence we call it a data motif. For the first time, among a wide variety of big data and AI workloads, we identify eight data motifs that take up most of the run time of those workloads, including Matrix, Sampling, Logic, Transform, Set, Graph, Sort and Statistic. We implement the eight data motifs on different software stacks as the micro benchmarks of an open-source big data and AI benchmark suite --- BigDataBench 4.0 (publicly available from http://prof.ict.ac.cn/BigDataBench), and perform comprehensive characterization of those data motifs from perspective of data sizes, types, sources, and patterns as a lens towards fully understanding big data and AI workloads. We believe the eight data motifs are promising abstractions and tools for not only big data and AI benchmarking, but also domain-specific hardware and software co-design.},
booktitle = {Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques},
articleno = {2},
numpages = {14},
keywords = {AI, data motif, big data, workload characterization},
location = {Limassol, Cyprus},
series = {PACT '18}
}

@inproceedings{10.1145/2064676.2064695,
author = {Cuzzocrea, Alfredo and Song, Il-Yeol and Davis, Karen C.},
title = {Analytics over Large-Scale Multidimensional Data: The Big Data Revolution!},
year = {2011},
isbn = {9781450309639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064676.2064695},
doi = {10.1145/2064676.2064695},
abstract = {In this paper, we provide an overview of state-of-the-art research issues and achievements in the field of analytics over big data, and we extend the discussion to analytics over big multidimensional data as well, by highlighting open problems and actual research trends. Our analytical contribution is finally completed by several novel research directions arising in this field, which plays a leading role in next-generation Data Warehousing and OLAP research.},
booktitle = {Proceedings of the ACM 14th International Workshop on Data Warehousing and OLAP},
pages = {101–104},
numpages = {4},
keywords = {analytics over big data, olap, analytics over big multidimensional data, data warehousing},
location = {Glasgow, Scotland, UK},
series = {DOLAP '11}
}

@inproceedings{10.1145/2723372.2735378,
author = {Rabl, Tilmann and Danisch, Manuel and Frank, Michael and Schindler, Sebastian and Jacobsen, Hans-Arno},
title = {Just Can't Get Enough: Synthesizing Big Data},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2735378},
doi = {10.1145/2723372.2735378},
abstract = {With the rapidly decreasing prices for storage and storage systems ever larger data sets become economical. While only few years ago only successful transactions would be recorded in sales systems, today every user interaction will be stored for ever deeper analysis and richer user modeling. This has led to the development of big data systems, which offer high scalability and novel forms of analysis. Due to the rapid development and ever increasing variety of the big data landscape, there is a pressing need for tools for testing and benchmarking. Vendors have little options to showcase the performance of their systems but to use trivial data sets like TeraSort or WordCount. Since customers' real data is typically subject to privacy regulations and rarely can be utilized, simplistic proof-of-concepts have to be used, leaving both, customers and vendors, unclear of the target use-case performance. As a solution, we present an automatic approach to data synthetization from existing data sources. Our system enables a fully automatic generation of large amounts of complex, realistic, synthetic data.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1457–1462},
numpages = {6},
keywords = {pdgf, dbsynth, data generator},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/2627770.2627776,
author = {Smith, Ken and Seligman, Len and Rosenthal, Arnon and Kurcz, Chris and Greer, Mary and Macheret, Catherine and Sexton, Michael and Eckstein, Adric},
title = {"Big Metadata": The Need for Principled Metadata Management in Big Data Ecosystems},
year = {2014},
isbn = {9781450329972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627770.2627776},
doi = {10.1145/2627770.2627776},
abstract = {Current big data ecosystems lack a principled approach to metadata management. This impedes large organizations' ability to share data and data preparation and analysis code, to integrate data, and to ensure that analytic code makes compatible assumptions with the data it uses. This use-case paper describes the challenges and an in-progress effort to address them. We present a real application example, discuss requirements for "big metadata" drawn from that example as well as other U.S. government analytic applications, and briefly describe an effort to adapt an existing open source metadata manager to support the needs of big data ecosystems.},
booktitle = {Proceedings of Workshop on Data Analytics in the Cloud},
pages = {1–4},
numpages = {4},
keywords = {data discovery, metadata, data integration, Big data analytics},
location = {Snowbird, UT, USA},
series = {DanaC'14}
}

@inproceedings{10.1145/2463676.2463712,
author = {Ghazal, Ahmad and Rabl, Tilmann and Hu, Minqing and Raab, Francois and Poess, Meikel and Crolotte, Alain and Jacobsen, Hans-Arno},
title = {BigBench: Towards an Industry Standard Benchmark for Big Data Analytics},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2463712},
doi = {10.1145/2463676.2463712},
abstract = {There is a tremendous interest in big data by academia, industry and a large user base. Several commercial and open source providers unleashed a variety of products to support big data storage and processing. As these products mature, there is a need to evaluate and compare the performance of these systems.In this paper, we present BigBench, an end-to-end big data benchmark proposal. The underlying business model of BigBench is a product retailer. The proposal covers a data model and synthetic data generator that addresses the variety, velocity and volume aspects of big data systems containing structured, semi-structured and unstructured data. The structured part of the BigBench data model is adopted from the TPC-DS benchmark, which is enriched with semi-structured and unstructured data components. The semi-structured part captures registered and guest user clicks on the retailer's website. The unstructured data captures product reviews submitted online. The data generator designed for BigBench provides scalable volumes of raw data based on a scale factor. The BigBench workload is designed around a set of queries against the data model. From a business prospective, the queries cover the different categories of big data analytics proposed by McKinsey. From a technical prospective, the queries are designed to span three different dimensions based on data sources, query processing types and analytic techniques.We illustrate the feasibility of BigBench by implementing it on the Teradata Aster Database. The test includes generating and loading a 200 Gigabyte BigBench data set and testing the workload by executing the BigBench queries (written using Teradata Aster SQL-MR) and reporting their response times.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1197–1208},
numpages = {12},
keywords = {benchmarking, map reduce, big data},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@article{10.1145/2674026.2674029,
author = {Goonetilleke, Oshini and Sellis, Timos and Zhang, Xiuzhen and Sathe, Saket},
title = {Twitter Analytics: A Big Data Management Perspective},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/2674026.2674029},
doi = {10.1145/2674026.2674029},
abstract = {With the inception of the Twitter microblogging platform in 2006, a myriad of research efforts have emerged studying different aspects of the Twittersphere. Each study exploits its own tools and mechanisms to capture, store, query and analyze Twitter data. Inevitably, platforms have been developed to replace this ad-hoc exploration with a more structured and methodological form of analysis. Another body of literature focuses on developing languages for querying Tweets. This paper addresses issues around the big data nature of Twitter and emphasizes the need for new data management and query language frameworks that address limitations of existing systems. We review existing approaches that were developed to facilitate twitter analytics followed by a discussion on research issues and technical challenges in developing integrated solutions.},
journal = {SIGKDD Explor. Newsl.},
month = {sep},
pages = {11–20},
numpages = {10}
}

