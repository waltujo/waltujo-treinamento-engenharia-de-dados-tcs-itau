@inproceedings{10.1145/2811411.2811481,title = {A real-time knowledge extracting system from social big data using distributed architecture}, author = {Han Youngsub , Lee Hyeoncheol , Kim Yanggon },year = {2015}, isbn = {9781450337380}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2811411.2811481}, doi = {10.1145/2811411.2811481}, abstract = {A huge amount of data is being generated by social media in real time. Accordingly, demands for extracting meaningful information from the social data have been dramatically increased. However, most of the previous research encompasses potential problems with data processing, management and analysis in real time. In this paper, we propose a distributed system architecture for generating meaningful information from text-based social data. The system collects data from multi-source channels, such as Twitter, YouTube, and The New York Times. Also, the system extracts terms and sentiment from each document using data mining technologies. In addition, the system uses HDFS, Map-reduce, and message service to handle the huge data. By analyzing keywords in texts and user account information, the system generates a summary of results including terms, sentiments and data variations for further analysis, including reputation, social trends, and customer reactions. The experiment results show that our approach is able to effectively process the social data in real time.}, location = {Prague, Czech Republic}, series = {RACS}, pages = {74\u201379}, numpages = {6}, keywords = {distributed computing, natural language processing, data mining, message driven processing, Hadoop, sentiment analysis, crawling, big data}}
@inproceedings{10.1145/2791405.2791549,title = {A Theoretical Model for Big Data Analytics using Machine Learning Algorithms}, author = {Sheshasaayee Ananthi , Lakshmi J. V. N. },year = {2015}, isbn = {9781450333610}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2791405.2791549}, doi = {10.1145/2791405.2791549}, abstract = {Big Data processing is currently becoming increasingly important in modern era due to continuous growth of the amount of data generated in various fields. Architecture for Big Data usually ranges across multiple machines and clusters consisting of various sub systems. To potentially speed up the processing, a unified way of machine learning is applied on MapReduce frame work. A broadly applicable programming model MapReduce is applied on different learning algorithms belonging to machine learning family for all business decisions. This paper presents parallel implementation of various machine learning algorithms, includes K-Means, Logistic Regression implemented on top of MapReduce model.}, location = {Kochi, India}, series = {WCI '15}, pages = {635\u2013639}, numpages = {5}, keywords = {Logistic Regression, MapReduce, Parallel Implementation, Serial Implementation, Hadoop}}
@inproceedings{10.1145/3018896.3066908,title = {From smart-city and IoT simulation to big data generation}, author = {Bounceur Ahc\u00e8ne },year = {2017}, isbn = {9781450347747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3018896.3066908}, doi = {10.1145/3018896.3066908}, abstract = {Our world is digitized everyday and increasingly. In 2020, it is expected that over 70% of the population will live in or around cities. To guarantee a good quality of life, it is necessary to ensure fast and reliable services in all areas, in particular those which are mainly based on the use of connected objects. This is one of the cornerstones of a smart city project. It will make possible to provide close to real-time the remote monitoring of sick patients, the monitoring of the environment in order to know its evolution over time and to anticipate developments that can be harmful to health and the environment itself, and to accurately analyze the signals transmitted by the on-board sensors.To further develop domains such as eHealth or the monitoring of other networks in the context of Smart Cities, fast and reliable design tools are needed. Their objectives are to study the realizability of such networks, their behavior in terms of energy consumption, safety, cost and other reliability parameters.This keynote aims to present a new platform called CupCarbon that allows to design systems of connected objects mainly representing sensors and to prepare future deployments of large-scale IoT infrastructures for Smart cities in optimal conditions. This kind of platforms will be a part of systems in the world that will participate in the generation of Big Data.1}, location = {Cambridge, United Kingdom}, series = {ICC '17}, pages = {1}, numpages = {1}, keywords = {interference, cupcarbon simulation, radio propagation channel, visibility tree, alpha-stable distribution}}
@inproceedings{10.1145/3507454.3507459,title = {Review of \"Composition and Big Data edited by Amanda Licastro and Benjamin Miller,\" Licastro, A. & Miller, B. (Eds.). (2021). Composition and big data. University of Pittsburg Press.}, author = {Hope Lacy },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3507454.3507459}, doi = {10.1145/3507454.3507459}, abstract = {The evolution of digital tools and platforms has ushered in new possibilities for researchers, scholars, and practitioners of rhetoric and composition and adjacent fields like technical communication. These technologies change the ways we can gather, store, and use larger datasets, prompting new discussions on what big data methods look like in the field. The chapters housed in Amanda Licastro and Benjamin Miller's edited collection Composition and Big Data investigate the promises, concerns, and areas for further conversation regarding the applications of big data methods in composition-focused research.}, pages = {51\u201353}, numpages = {3}}
@inproceedings{10.1145/3498851.3498977,title = {Research on Tourism Prosperity Index Based on the Power Big Data}, author = {Li Aihua , Zhan Qiyuan , Xu Weijia , Zhang Yuejin },year = {2021}, isbn = {9781450391870}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3498851.3498977}, doi = {10.1145/3498851.3498977}, abstract = {With the development of computer technology, large amounts of data are stored and analyzed, which provides a new perspective for analyzing social and economic problems and assisting scientific decision-making. Tourism is the main source of revenues for many cities in China, and research on the prosperity of the tourism industry is very important. Based on the electric power big data, this paper analyzed the internal connection between electricity, economy, and tourism prosperity index, and chose the electricity sales data of several industry sectors as the analysis indicators to build the new tourism prosperity index system. We made an empirical analysis using data from Chengde City, Hebei Province, and put forward relevant policy suggestions.}, location = {Melbourne, VIC, Australia}, series = {WI-IAT '21}, pages = {347\u2013352}, numpages = {6}, keywords = {Tourist City, Composite Index, Power Data, Tourism Prosperity Index}}
@inproceedings{10.1145/2994539.2994546,title = {Data Quality Challenges and Future Research Directions in Threat Intelligence Sharing Practice}, author = {Sillaber Christian , Sauerwein Clemens , Mussmann Andrea , Breu Ruth },year = {2016}, isbn = {9781450345651}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2994539.2994546}, doi = {10.1145/2994539.2994546}, abstract = {In the last couple of years, organizations have demonstrated an increased willingness to participate in threat intelligence sharing platforms. The open exchange of information and knowledge regarding threats, vulnerabilities, incidents and mitigation strategies results from the organizations' growing need to protect against today's sophisticated cyber attacks. To investigate data quality challenges that might arise in threat intelligence sharing, we conducted focus group discussions with ten expert stakeholders from security operations centers of various globally operating organizations. The study addresses several factors affecting shared threat intelligence data quality at multiple levels, including collecting, processing, sharing and storing data. As expected, the study finds that the main factors that affect shared threat intelligence data stem from the limitations and complexities associated with integrating and consolidating shared threat intelligence from different sources while ensuring the data's usefulness for an inhomogeneous group of participants.Data quality is extremely important for shared threat intelligence. As our study has shown, there are no fundamentally new data quality issues in threat intelligence sharing. However, as threat intelligence sharing is an emerging domain and a large number of threat intelligence sharing tools are currently being rushed to market, several data quality issues -- particularly related to scalability and data source integration -- deserve particular attention.}, location = {Vienna, Austria}, series = {WISCS '16}, pages = {65\u201370}, numpages = {6}, keywords = {threat intelligence data, data quality challenges, cyber security operations center, threat intelligence sharing data quality}}
@inproceedings{10.1145/2967878.2967919,title = {Query-oriented Unsupervised Multi-document Summarization on Big Data}, author = {Sunaina , S. Sowmya Kamath },year = {2016}, isbn = {9781450341790}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2967878.2967919}, doi = {10.1145/2967878.2967919}, abstract = {Real time document summarization is a critical need nowadays, owing to the large volume of information available for our reading, and our inability to deal with this entirely due to limitations of time and resources. Oftentimes, information is available in multiple sources, offering multiple contexts and viewpoints on a single topic of interest. Automated multi-document summarization (MDS) techniques aim to address this problem. However, current techniques for automated MDS suffer from low precision and accuracy with reference to a given subject matter, when compared to those summaries prepared by humans and takes large time to create the summary when the input given is too huge. In this paper, we propose a hybrid MDS technique combining feature based algorithms and dynamic programming for generating a summary from multiple documents based on user provided query. Further, in real-world scenarios, Web search serves up a large number of URLs to users, and the work of making sense of these with reference to a particular query is left to the user. In this context, an efficient parallelized MDS technique based on Hadoop is also presented, for serving a concise summary of multiple Webpage contents for a given user query in reduced time duration.}, location = {Dallas, TX, USA}, series = {ICCCNT '16}, pages = {1\u20136}, numpages = {6}, keywords = {natural language processing, map-reduce, multi-document summarization, dynamic programming}}
@inproceedings{10.1145/2593728.2593733,title = {Utilization of synergetic human-machine clouds: a big data cleaning case}, author = {Iren Deniz , Kul Gokhan , Bilgen Semih },year = {2014}, isbn = {9781450328579}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2593728.2593733}, doi = {10.1145/2593728.2593733}, abstract = {Cloud computing and crowdsourcing are growing trends in IT. Combining the strengths of both machine and human clouds within a hybrid design enables us to overcome certain problems and achieve efficiencies. In this paper we present a case in which we developed a hybrid, throw-away prototype software system to solve a big data cleaning problem in which we corrected and normalized a data set of 53,822 academic publication records. The first step in our solution consists of utilization of external DOI query web services to label the records with matching DOIs. Then we used customized string similarity calculation algorithms based on Levensthein Distance and Jaccard Index to grade the similarity between records. Finally we used crowdsourcing to identify duplicates among the residual record set consisting of similar yet not identical records. We consider this proof of concept to be successful and report that we achieved certain results that we could not have achieved by using either human or machine clouds alone.}, location = {Hyderabad, India}, series = {CSI-SE 2014}, pages = {15\u201318}, numpages = {4}, keywords = {Crowdsourcing, Crowdservice, Cloud Computing}}
@inproceedings{10.1145/3297662.3365807,title = {Facilitating and Managing Machine Learning and Data Analysis Tasks in Big Data Environments using Web and Microservice Technologies}, author = {Shahoud Shadi , Gunnarsdottir Sonja , Khalloof Hatem , Duepmeier Clemens , Hagenmeyer Veit },year = {2019}, isbn = {9781450362382}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297662.3365807}, doi = {10.1145/3297662.3365807}, abstract = {Driven by the great advance of machine learning in a wide range of application areas, the need for developing machine learning frameworks effectively as well as easily usable by novices increased dramatically. Furthermore, building machine learning models in the context of big data environments still represents a great challenge. In the present paper, we tackle these challenges by introducing a new generic framework for efficiently facilitating the training, testing, managing, storing, and retrieving of machine learning models in the context of big data. The framework makes use of a powerful big data software stack and a microservice architecture for a fully manageable and highly scalable solution. A highly configurable user interface is introduced giving the user the ability to easily train, test, and manage machine learning models. Moreover, it automatically indexes models and allows flexible exploration of them in the visual interface. The performance of the new framework is evaluated on state-of-the-arts machine learning algorithms: it is shown that storing and retrieving machine learning models as well as a respective acceptable low overhead demonstrate an efficient approach to facilitate machine learning in big data environments.}, location = {Limassol, Cyprus}, series = {MEDES '19}, pages = {80\u201387}, numpages = {8}, keywords = {Data Analytic, Machine Learning, Big Data, Web-based Applications, Microservice}}
@inproceedings{10.1145/3482632.3483079,title = {Big Data Forecasting Methods and Technology Curriculum Teaching Reform under TOPCARES-CDIO}, author = {Zhang Xiaoyu },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483079}, doi = {10.1145/3482632.3483079}, abstract = {As a theoretical and practical subject, \"Big Data Prediction Methods and Technologies\" needs teaching reform. Based on the integrated talent training model of TOPCARES-CDIO, the teaching reform of the curriculum is probed from three aspects: teaching concept, teaching goal and teaching implementation. Finally, it will contribute to the cultivation of compound talents with excellent big data prediction methods and technical theories, strong practical ability and strong comprehensive quality.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1043\u20131045}, numpages = {3}}
@inproceedings{10.1145/3344948.3344988,title = {Big data from the cloud to the edge: the aggregate computing solution}, author = {Ali Shaukat , Damiani Ferruccio , Dustdar Schahram , Sanseverino Marialuisa , Viroli Mirko , Weyns Danny },year = {2019}, isbn = {9781450371421}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3344948.3344988}, doi = {10.1145/3344948.3344988}, abstract = {We advocate a novel concept of dependable intelligent edge systems (DIES) i.e., the edge systems ensuring a high degree of dependability (e.g., security, safety, and robustness) and autonomy because of their applications in critical domains. Building DIES entail a paradigm shift in architectures for acquiring, storing, and processing potentially large amounts of complex data: data management is placed at the edge between the data sources and local processing entities, with loose coupling to storage and processing services located in the cloud. As such, the literal definition of edge and intelligence is adopted, i.e., the ability to acquire and apply knowledge and skills is shifted towards the edge of the network, outside the cloud infrastructure. This paradigm shift offers flexibility, auto configuration, and auto diagnosis, but also introduces novel challenges.}, location = {Paris, France}, series = {ECSA '19}, pages = {177\u2013180}, numpages = {4}, keywords = {dependability, formal methods, adaptation}}
@inproceedings{10.1145/2949550.2949556,title = {BIC-LSU: Big Data Research Integration with Cyberinfrastructure for LSU}, author = {Chiu Chui-hui , Lewis Nathan , Singh Dipak Kumar , Das Arghya Kusum , Jalazai Mohammad M. , Platania Richard , Goswami Sayan , Lee Kisung , Park Seung-Jong },year = {2016}, isbn = {9781450347556}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2949550.2949556}, doi = {10.1145/2949550.2949556}, abstract = {In recent years, big data analysis has been widely applied to many research fields including biology, physics, transportation, and material science. Even though the demands for big data migration and big data analysis are dramatically increasing in campus IT infrastructures, there are several technical challenges that need to be addressed. First of all, frequent big data transmission between storage systems in different research groups imposes heavy burdens on a regular campus network. Second, the current campus IT infrastructure is not designed to fully utilize the hardware capacity for big data migration and analysis. Last but not the least, running big data applications on top of large-scale high-performance computing facilities is not straightforward, especially for researchers and engineers in non-IT disciplines.We develop a campus IT cyberinfrastructure for big data migration and analysis, called BIC-LSU, which consists of a task-aware Clos OpenFlow network, high-performance cache storage servers, customized high-performance transfer applications, a light-weight control framework to manipulate existing big data storage systems and job scheduling systems, and a comprehensive social networking-enabled web portal. BIC-LSU achieves 40Gb/s disk-to-disk big data transmission, maintains short average transmission task completion time, enables the convergence of control on commonly deployed storage and job scheduling systems, and enhances easiness of big data analysis with a universal user-friendly interface. BIC-LSU software requires minimum dependencies and has high extensibility. Other research institutes can easily customize and deploy BIC-LSU as an augmented service on their existing IT infrastructures.}, location = {Miami, USA}, series = {XSEDE16}, pages = {1\u20138}, numpages = {8}, keywords = {science gateway, task-aware network scheduling, software-defined networking, solid-state drive storage server, Big data}}
@inproceedings{10.1145/3561202,title = {Editorial: Special Issue on Data Quality and Ethics}, author = {Firmani Donatella , Tanca Letizia , Torlone Riccardo },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3561202}, doi = {10.1145/3561202}, abstract = {This editorial summarizes the content of the Special Issue on Data Quality and Ethics of the Journal of Data and Information Quality (JDIQ). The issue accepted submissions from June 1 to July 30, 2021.}}
@inproceedings{10.1145/3495018.3495386,title = {Application of Financial Big Data Analysis Based on Computer Data Mining Technology}, author = {Zhong Xiaoqing , Fu Dandan , Zhong Chongjie },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495386}, doi = {10.1145/3495018.3495386}, abstract = {In today's information technology era, computers are still a necessity of modern society. Computer data mining technology is a necessary means in the big data environment. Nowadays, in our daily life and work, Massive data and information need to be processed, especially for the financial industry, all kinds of data need to be processed by financial practitioners. How to process the data quickly and grasp the key information more quickly and accurately is the concern of many financial elites. The idea of applying computer data mining technology to financial big data has aroused the interest of many relevant people. On this basis, this paper selects a famous commercial bank as the research object to explore the influence of computer data mining technology on financial big data analysis. And from the experimental data, the data processing method optimized by computer data mining technology is more efficient, and the time spent on data processing is greatly reduced, even relatively less than 60%, up to 69%. In addition, 61% of the employees are satisfied with the optimization method, much more than half of them.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {1300\u20131303}, numpages = {4}}
@inproceedings{10.1145/2909132.2927471,title = {Road Mapping Infrastructures for Advanced Visual Interfaces Supporting Big Data Applications in Virtual Research Environments}, author = {Bornschlegl Marco X. , Manieri Andrea , Walsh Paul , Catarci Tiziana , Hemmje Matthias L. },year = {2016}, isbn = {9781450341318}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2909132.2927471}, doi = {10.1145/2909132.2927471}, abstract = {Handling the complexity of relevant data requires new techniques about data access, visualization, perception, and interaction for innovative and successful strategies. In order to address human-computer interaction, cognitive eficiency, and interoperability problems, a generic information visualization, user empowerment, as well as service integration and mediation approach based on the existing state-of-the-art in the relevant areas of computer science has to be achieved.This workshop will address these issues with a special focus on supporting distributed Big Data analysis in Virtual Research Environments (VREs). In this way, the overall scope and goal of the workshop is to bring together researchers in these areas to achieve a road map, which can support the acceleration in research activities by means of transforming, enriching, and deploying advanced visual user interfaces for managing and using e-Science infrastructures. Advancements in this fields of research can i.e. support the, creation, configuration, management, and usage of distributed Big Data analysis in VREs.}, location = {Bari, Italy}, series = {AVI '16}, pages = {363\u2013367}, numpages = {5}, keywords = {Information Visualization, Advanced Visual User Interfaces, User Empowerment, Distributed Big Data Analysis, Virtual Research Environments}}
@inproceedings{10.1145/2695664.2695753,title = {A data quality-aware cloud service based on metaheuristic and machine learning provisioning algorithms}, author = {Nascimento Dimas C. , Pires Carlos Eduardo , Mestre Demetrio Gomes },year = {2015}, isbn = {9781450331968}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2695664.2695753}, doi = {10.1145/2695664.2695753}, abstract = {Cloud Computing as a service has become a topic of increasing interest. The outsourcing of duties and infrastructure to external parties became a crucial concept for many business models. In this paper we discuss the design and experimental evaluation of provisioning algorithms, in a Data Quality-aware Service (DQaS) context, that enables dynamic Data Quality Service Level Agreements (DQSLA) management and optimization of cloud resources. The DQaS has been designed to respond effectively to the DQSLA requirements of the service customers, by minimizing SLA penalties and provisioning the cloud infrastructure for the execution of data quality algorithms. An experimental evaluation of the proposed provisioning algorithms, carried out through simulation, has provided very encouraging results that confirm the adequacy of these algorithms in the DQaS context.}, location = {Salamanca, Spain}, series = {SAC '15}, pages = {1696\u20131703}, numpages = {8}, keywords = {provisioning, cloud computing, machine learning, metaheuristic, data quality}}
@inproceedings{10.1145/2554688.2554694,title = {Big data genome sequencing on Zynq based clusters (abstract only)}, author = {Wang Chao , Li Xi , Zhou Xuehai , Chen Yunji , Cheung Ray C.C. },year = {2014}, isbn = {9781450326711}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2554688.2554694}, doi = {10.1145/2554688.2554694}, abstract = {Next-generation sequencing (NGS) problems have attracted many attentions of researchers in biological and medical computing domains. The current state-of-the-art NGS computing machines are dramatically lowering the cost and increasing the throughput of DNA sequencing. In this paper, we propose a practical study that uses Xilinx Zynq board to summarize acceleration engines using FPGA accelerators and ARM processors for the state-of-the-art short read mapping approaches. The heterogeneous processors and accelerators are coupled with each other using a general Hadoop distributed processing framework. First the reads are collected by the central server, and then distributed to multiple accelerators on the Zynq for hardware acceleration. Therefore, the combination of hardware acceleration and Map-Reduce execution flow could greatly accelerate the task of aligning short length reads to a known reference genome. Our approach is based on preprocessing the reference genomes and iterative jobs for aligning the continuous incoming reads. The hardware acceleration is based on the creditable read-mapping algorithm RMAP software approach. Furthermore, the speedup analysis on a Hadoop cluster, which concludes 8 development boards, is evaluated. Experimental results demonstrate that our proposed architecture and methods has the speedup of more than 112X, and is scalable with the number of accelerators. Finally, the Zynq based cluster has efficient potential to accelerate even general large scale big data applications.This work was supported by the NSFC grants No. 61379040, No. 61272131 and No. 61202053.}, location = {Monterey, California, USA}, series = {FPGA '14}, pages = {247}, numpages = {1}, keywords = {rmap, hardware acceleration., fpga, genome sequencing, bioinformatics}}
@inproceedings{10.1145/3349341.3349502,title = {Factors Affecting the Box Office of Chinese Main-Melody Films Based on Big Data}, author = {Dai Jianhua , Jin Libo , Wang Xinyang },year = {2019}, isbn = {9781450371506}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3349341.3349502}, doi = {10.1145/3349341.3349502}, abstract = {With the popularity of the Main-melody films such as \"Wolf 2\" and \"Mekong River Action\" in 2017, \"Red Sea Action\" in 2018, a small wave of development climax of Chinese main-melody films has been set off. Film box office is an important index to measure the success of a film. The analysis and study of the factors affecting the box office of the film provides an indispensable theoretical basis for the development of the film industry. This survey selected 100 main-melody films which were the top box office films in 2013-2018 as the research data. Scores, number of commentaries, actor influence and director influence were taken as independent variables, while box office was taken as a dependent variable for correlation test and regression analysis.}, location = {Wuhan, Hubei, China}, series = {AICS 2019}, pages = {741\u2013744}, numpages = {4}, keywords = {main-melody film, film box office, regression analysis, big data}}
@inproceedings{10.1145/2378016.2378018,title = {Creating a General (Family) Practice Epidemiological Database in Ireland - Data Quality Issue Management}, author = {Collins Claire , Janssens Kelly },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2378016.2378018}, doi = {10.1145/2378016.2378018}, abstract = {In Ireland, while detailed information is available regarding hospital attendance, little is known regarding general (family) practice attendance. However, it is conservatively estimated that there are almost nine times as many general practice encounters than there are hospital encounters each year in Ireland. This represents a very significant gap in health information. Indeed, general practice has been shown in other countries to be an important and rich source of information about the health of the population, their behaviors and their utilization of health services. Funded by the Health Information and Quality Authority (HIQA), the Irish College of General Practitioners (ICGP) undertook a feasibility study of diagnostic coding of routinely entered patient data and the creation of a national general practice morbidity and epidemiological database (GPMED project). This article outlines the process of data quality issue management undertaken.The study\u2019s findings suggest that the quality of data collection and reporting structures available in general practice throughout Ireland at the outset of this project were not adequate to permit the creation of a database of sufficient quality for service planning and policy or epidemiological research. Challenges include the dearth of a minimum standard of data recorded in consultations by GPs and the absence of the digital data recording and exporting infrastructure within Irish patient management software systems. In addition, there is at present a lack of recognition regarding the value of such data for patient management and service planning---including importantly, data collectors who do not fully accept the merit of maintaining data, which has a direct consequence for data quality. The work of this project has substantial implications for the data available to the health sector in Ireland and contributes to the knowledge base internationally regarding general practice morbidity data.}, pages = {1\u20139}, numpages = {9}, keywords = {Data quality, family practice, epidemiology}}
@inproceedings{10.1145/1966901.1966903,title = {Towards a vocabulary for data quality management in semantic web architectures}, author = {F\u00fcrber Christian , Hepp Martin },year = {2011}, isbn = {9781450306089}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1966901.1966903}, doi = {10.1145/1966901.1966903}, abstract = {Reliable decision-making and reliable information based on Semantic Web data requires methodologies and techniques for managing the quality of the published data. To make things more complicated, the judgment of what is \"good\" data will often depend on the task at hand or the subjective requirements of data owners or data consumers. Some data quality requirements can be modeled using data quality rules, i.e. executable definitions that allow the identification and measurement of data quality problems. In this paper, we provide a conceptual model that allows the representation of such rules and other quality-related knowledge using the Resource Description Framework (RDF) and the Web Ontology Language (OWL). Based on our model, it is possible to monitor and assess the quality of data sources and to automate data cleansing tasks. The use of a generic conceptual model based on Semantic Web formalisms supports the definition of reusable, broadly applicable SPARQL queries and portable applications for data quality management (DQM). Furthermore, the explicit representation of rules in RDF/OWL facilitates rule management tasks, e.g. for analyzing consistency among the rules, and allows to collaborate and create a shared understanding.}, location = {Uppsala, Sweden}, series = {LWDM '11}, pages = {1\u20138}, numpages = {8}, keywords = {information quality, SPARQL, data quality management, trust, ontology, semantic web, knowledge representation, linked data management}}
@inproceedings{10.1145/3428502.3428515,title = {Macao government fights the COVID-19 epidemics with the help of e-Government and big data}, author = {Li Lue },year = {2020}, isbn = {9781450376747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3428502.3428515}, doi = {10.1145/3428502.3428515}, abstract = {Macao is a tourist city. When the COVID-19 epidemic occurred, Macao's risks and pressures were enormous. However, the response policies of the Government are timely and effective, making Macao one of the lightest epidemic regions in the world. This article reviewed the SAR government's anti-epidemic policies, and found four most important policies all based on the E-government technology and the implementation of big data: 1. Quickly screen, restrict and isolate tourists from the affected areas; 2. Supply masks; 3. Accurate collection, in-depth analysis and rapid release of information; 4. Electronic consumer card.}, location = {Athens, Greece}, series = {ICEGOV 2020}, pages = {112\u2013115}, numpages = {4}, keywords = {E-government, Special Webpage against Epidemics, COVID-19 Epidemic, Big data, Public Health Crisis}}
@inproceedings{10.1145/3167132.3167376,title = {An adaptive neuro-fuzzy inference system for improving data quality in disease registries}, author = {Bellaaj Hatem , Mdhaffar Afef , Jmaiel Mohamed , Mseddi Sondes Hdiji , Freisleben Bernd },year = {2018}, isbn = {9781450351911}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3167132.3167376}, doi = {10.1145/3167132.3167376}, abstract = {The purpose of disease registries is to collect and analyze data related to specific diseases in terms of incidence and prevalence. Since the data is typically entered by wearable sensors and/or human caregivers, errors in the data fields are often inevitable. In this paper, we propose a new approach to improve data quality in disease registries based on (a) a semi-random combination of parameters and (b) a learning algorithm for detecting and signaling the loss of quality of the entered data. To implement the approach, we have developed a novel adaptive neuro-fuzzy inference system. It is applied to specific sections of the Tunisian Fanconi Anemia Registry with the aims of reducing false alarms and automatically adjusting the parameters of coefficients of the disease. Our experimental results indicate that both aims can be achieved and effectively lead to improved data quality in disease registries.}, location = {Pau, France}, series = {SAC '18}, pages = {30\u201333}, numpages = {4}, keywords = {ANFIS, data quality, disease registry, fuzzy logic}}
@inproceedings{10.1145/3472456.3472488,title = {Best VM Selection for Big Data Applications across Multiple Frameworks by Transfer Learning}, author = {Wu Yuewen , Wu Heng , Xu Yuanjia , Hu Yi , Zhang Wenbo , Zhong Hua , Huang Tao },year = {2021}, isbn = {9781450390682}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472456.3472488}, doi = {10.1145/3472456.3472488}, abstract = {Cloud providers\u00a0are\u00a0presented\u00a0with\u00a0a\u00a0bewildering choice\u00a0of\u00a0VM\u00a0types\u00a0for\u00a0a\u00a0range\u00a0of\u00a0contemporary data\u00a0processing\u00a0frameworks\u00a0today. However, existing\u00a0performance modeling\u00a0and\u00a0machine\u00a0learning\u00a0efforts cannot pick optimal VM types for multiple frameworks simultaneously, since they are difficult to balance model accuracy\u00a0and model training\u00a0cost. We\u00a0propose\u00a0Vesta, a\u00a0novel\u00a0transfer\u00a0learning\u00a0approach,\u00a0to\u00a0address\u00a0this\u00a0challenge: (1) it\u00a0abstracts knowledge of VM type selection\u00a0through\u00a0offline\u00a0benchmarking on\u00a0multiple\u00a0frameworks; (2) it\u00a0employs\u00a0a\u00a0two-layer\u00a0bipartite\u00a0graph\u00a0to represent\u00a0knowledge\u00a0across\u00a0frameworks; (3) it minimizes training overhead by\u00a0reus-ing\u00a0the knowledge to select the best VM type for given applications. Comparing\u00a0with\u00a0state-of-the-art\u00a0efforts, our\u00a0experiments\u00a0on\u00a030\u00a0applications\u00a0of\u00a0Hadoop, Hive\u00a0and\u00a0Spark\u00a0show\u00a0that\u00a0 Vesta\u00a0can\u00a0improve application performance up to 51%\u00a0while\u00a0reducing 85% training overhead.}, location = {Lemont, IL, USA}, series = {ICPP 2021}, pages = {1\u201311}, numpages = {11}, keywords = {big data application, virtual machine, multiple frameworks, transfer learning}}
@inproceedings{10.5555/2755753.2757169,title = {Big-data streaming applications scheduling with online learning and concept drift detection}, author = {Kanoun Karim , van der Schaar Mihaela },year = {2015}, isbn = {9783981537048}, publisher = {EDA Consortium}, address = {San Jose, CA, USA}, abstract = {Several techniques have been proposed to adapt Big-Data streaming applications to resource constraints. These techniques are mostly implemented at the application layer and make simplistic assumptions about the system resources and they are often agnostic to the system capabilities. Moreover, they often assume that the data streams characteristics and their processing needs are stationary, which is not true in practice. In fact, data streams are highly dynamic and may also experience concept drift, thereby requiring continuous online adaptation of the throughput and quality to each processing task. Hence, existing solutions for Big-Data streaming applications are often too conservative or too aggressive. To address these limitations, we propose an online energy-efficient scheduler which maximizes the QoS (i.e., throughput and output quality) of Big-Data streaming applications under energy and resources constraints. Our scheduler uses online adaptive reinforcement learning techniques and requires no offline information. Moreover, our scheduler is able to detect concept drifts and to smoothly adapt the scheduling strategy. Our experiments realized on a chain of tasks modeling real-life streaming application demonstrate that our scheduler is able to learn the scheduling policy and to adapt it such that it maximizes the targeted QoS given energy constraint as the Big-Data characteristics are dynamically changing.}, location = {Grenoble, France}, series = {DATE '15}, pages = {1547\u20131550}, numpages = {4}}
@inproceedings{10.1145/2967938.2967944,title = {Bridging the Semantic Gaps of GPU Acceleration for Scale-out CNN-based Big Data Processing: Think Big, See Small}, author = {Song Mingcong , Hu Yang , Xu Yunlong , Li Chao , Chen Huixiang , Yuan Jingling , Li Tao },year = {2016}, isbn = {9781450341219}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2967938.2967944}, doi = {10.1145/2967938.2967944}, abstract = {Convolutional Neural Networks (CNNs) have substantially advanced the state-of-the-art accuracies of object recognition, which is the core function of a myriad of modern multimedia processing techniques such as image/video processing, speech recognition, and natural language processing. GPU-based accelerators gained increasing attention because a large amount of highly parallel neurons in CNN naturally matches the GPU computation pattern. In this work, we perform comprehensive experiments to investigate the performance bottlenecks and overheads of current GPU acceleration platform for scale-out CNN-based big data processing.In our characterization, we observe two significant semantic gaps: framework gap that lies between CNN-based data processing workflow and data processing manner in distributed framework; and the standalone gap that lies between the uneven computation loads at different CNN layers and fixed computing capacity provisioning of current GPU acceleration library. To bridge these gaps, we propose D3NN, a Distributed, Decoupled, and Dynamically tuned GPU acceleration framework for modern CNN architectures. In particular, D3NN features a novel analytical model that enables accurate time estimation of GPU accelerated CNN processing with only 5-10% error. Our evaluation results show the throughput of standalone processing node using D3NN gains up to 3.7X performance improvement over current standalone GPU acceleration platform. Our CNN-oriented GPU acceleration library with built-in dynamic batching scheme achieves up to 1.5X performance improvement over the non-batching scheme and outperforms the state-of-the-art deep learning library by up to 28% (performance mode) ~ 67% (memory-efficient mode).}, location = {Haifa, Israel}, series = {PACT '16}, pages = {315\u2013326}, numpages = {12}, keywords = {gpu, big data, distributed system, deep learning}}
@inproceedings{10.1145/3467707.3467730,title = {Dynamic Load Balancing Method for Urban Surveillance Video Big Data Storage Based on HDFS}, author = {Li Yue },year = {2021}, isbn = {9781450389501}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3467707.3467730}, doi = {10.1145/3467707.3467730}, abstract = {HDFS has been widely used by many video service websites, but its load balancing tool does not consider the bandwidth consumption characteristics of video file online playback and the heterogeneous performance difference of NameNode in metadata allocation problem. The dynamic load imbalance of cluster makes the utilization of bandwidth resources low. In this paper, a HDFS NameNode dynamic load balancing tool (NDLBT) for city monitoring video in urban surveillance video big data storage in cloud storage environment is proposed. method. Firstly, it analyzes the relationship between the bandwidth consumption and the bit rate, data block size and access heat of the video file when the video file is played online, and a new load evaluation model is established. On this basis, it adds consideration to the bandwidth consumption factor in the load scheme generation and load scheduling, and through the dynamic adaptive backup of multi-replica heterogeneous nodes of metadata. The dynamic distribution of metadata is realized under the consideration of node performance and load, and the performance of metadata server cluster is guaranteed. Finally, combined with cache strategy and automatic recovery mechanism, the reading and writing of metadata is improved. The simulation results show that compared with the proposed method, we can effectively avoid the aggregation of high bandwidth consumption data blocks. In the experimental scenario where high bandwidth consumption video files are used as service access hotspots, the proposed method is superior to the original load balancing method in 90% scenarios, and can reduce the bandwidth peak value of bottleneck nodes in data node clusters by 20%.}, location = {Tianjin, China}, series = {ICCAI 2021}, pages = {160\u2013167}, numpages = {8}, keywords = {metadata management, HDFS, dynamic load balancing, Cloud storage, urban surveillance video, big data storage}}
@inproceedings{10.5555/2486788.2486842,title = {Assisting developers of big data analytics applications when deploying on hadoop clouds}, author = {Shang Weiyi , Jiang Zhen Ming , Hemmati Hadi , Adams Bram , Hassan Ahmed E. , Martin Patrick },year = {2013}, isbn = {9781467330763}, publisher = {IEEE Press}, abstract = {Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures.}, location = {San Francisco, CA, USA}, series = {ICSE '13}, pages = {402\u2013411}, numpages = {10}}
@inproceedings{10.1145/3482632.3483176,title = {Competency Model of College PHYSICAL Education Teachers based on Big Data}, author = {Zhou Ming , Ye Liaokun , He Chaohu },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483176}, doi = {10.1145/3482632.3483176}, abstract = {This paper aims to construct a competency model based on the sum of a series of competency characteristics required by PEICAU (physical education in collage and universities) under the background of big data, and use this model to predict the performance of the competency model of PEICAU. After one semester of training, the work performance of the experimental group and the control group was investigated. The results of statistical analysis showed that the average score of the experimental group was higher than that of the control group in terms of overall work performance, and there was a significant difference at the level of 0.01. In terms of learning performance, the mean score of the experimental group was higher than that of the control group, and the difference was significant at the level of 0.001. Experimental results show that, by setting the different training contents, physical education teachers through training, there are obvious differences in enhancing performance, based on PE teachers' competence characteristics model of training the effect is higher than that of traditional physical education teacher training effect of these measures, the PE teachers' work characteristic model of performance predictive power got a further test by experiment.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1472\u20131475}, numpages = {4}}
@inproceedings{10.1145/3486611.3491139,title = {The 1st ACM international workshop on big data and machine learning for smart buildings and cities}, author = {Dong Bing , Markovic Romana , Carlucci Salvatore },year = {2021}, isbn = {9781450391146}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3486611.3491139}, doi = {10.1145/3486611.3491139}, abstract = {The proliferation of urban sensing, IoT, and big data in buildings, cities, and urban areas provides unprecedented opportunities for a deeper understanding of occupant behavior, transportation, and energy and water usage patterns. However, utilizing the existing data sources and modeling methods in building science to model urban scale occupant behaviors can be pretty challenging. Therefore, technological progress is needed to unlock its full potential. In order to fulfill the latter task, this workshop focuses on the methodologies for big urban and building data collection, analytics, modeling, and real-world technology deployment. The workshop aims to open discussion on the current challenges of big data in smart buildings and cities.}, location = {Coimbra, Portugal}, series = {BuildSys '21}, pages = {338\u2013340}, numpages = {3}, keywords = {big data analysis, digital cities, machine learning, occupant behavior, smart buildings, modeling and prediction}}
@inproceedings{10.1145/3482632.3483009,title = {Information Transformation of Teaching Management System Based on Big Data Investigation Technology}, author = {Yuan Meijuan , Yan Fei },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483009}, doi = {10.1145/3482632.3483009}, abstract = {Informatization is an important step in the development of colleges and universities and an important part of the modernization of education management. In the process of promoting education management informatization, the regional differences in Chinese universities are obvious. Under the background of big data era, informatization has become the inevitable trend of teaching management development. Under this trend, in order to meet the learning needs of students, this paper uses big data technology as an advanced tool to build the information teaching management system, analyze the development situation of universities and abroad, review the literature, investigate 5 universities in a province, analyze the development situation, understand the problems, summarize the development, and conclude the transformation of education management is very necessary.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {749\u2013753}, numpages = {5}}
@inproceedings{10.1145/3348445.3348479,title = {WebGIS for Managing Household Data within a Provincial Big Data Project}, author = {Puarungroj Wichai , Phromkhot Suchada , Boonsirisumpun Narong , Pongpatrakant Pathapong , Sangpradid Satith },year = {2019}, isbn = {9781450371957}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3348445.3348479}, doi = {10.1145/3348445.3348479}, abstract = {The government agencies require decision support information before commencing their community development projects in rural areas. However, such information is not always available or does not meet their requirements. This research presents the design and development of the WebGIS, which is intended to store and provide spatially related household information for government agencies. This research has been conducted as a part of a provincial big data project. In this research, the spatial database system and the data visualization of the database were designed and developed by focusing on the details of each house in the targeted villages. The data were collected by the researchers from the study areas, which comprised 5 villages in Loei and Khonkaen Provinces in Thailand. The important household and location data were collected and combined with the community data from the Community Development Office. The GIS was developed using QGIS where the geolocation of each house in the villages was applied on the map derived from Google map. The data were analyzed and visualized in different formats such as color, table, and graph in order to establish the data classification and summarization. The system and data were finally evaluated by the Community Development Office and community leaders in terms of system performance and data accuracy.}, location = {Bangkok, Thailand}, series = {ICCCM '19}, pages = {115\u2013118}, numpages = {4}, keywords = {WebGIS, Village improvement, Spatial database, Geographic information systems, Community development}}
@inproceedings{10.1145/3495018.3501140,title = {Application of Recommendation Algorithm in Electronic Commerce through Computer Big Data Technology}, author = {Li Bin , Zhou Zhisheng },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3501140}, doi = {10.1145/3495018.3501140}, abstract = {In the development of e-commerce, Haiquan data gradually exerts its commercial value. Whether this value can be fully mined depends on the way of data mining and data utilization. Aiming at the background of massive big data demand, a personalized recommendation engine model based on collaborative filtering and content-based combined recommendation algorithms is proposed, and hot recommendation based on text similarity is tentatively incorporated. This model is proposed for the problems faced by big data recommendation. It includes two main modules: offline data calculation and online recommendation. Finally, a simulation experiment was carried out through the real data set of a certain domestic e-commerce and the Movie Lens data set to demonstrate the rationality of the improvement.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {2564\u20132567}, numpages = {4}}
@inproceedings{10.1145/3318464.3380584,title = {Cost Models for Big Data Query Processing: Learning, Retrofitting, and Our Findings}, author = {Siddiqui Tarique , Jindal Alekh , Qiao Shi , Patel Hiren , Le Wangchao },year = {2020}, isbn = {9781450367356}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3318464.3380584}, doi = {10.1145/3318464.3380584}, abstract = {Query processing over big data is ubiquitous in modern clouds, where the system takes care of picking both the physical query execution plans and the resources needed to run those plans, using a cost-based query optimizer. A good cost model, therefore, is akin to better resource efficiency and lower operational costs. Unfortunately, the production workloads at Microsoft show that costs are very complex to model for big data systems. In this work, we investigate two key questions: (i) can we learn accurate cost models for big data systems, and (ii) can we integrate the learned models within the query optimizer. To answer these, we make three core contributions. First, we exploit workload patterns to learn a large number of individual cost models and combine them to achieve high accuracy and coverage over a long period. Second, we propose extensions to Cascades framework to pick optimal resources, i.e, number of containers, during query planning. And third, we integrate the learned cost models within the Cascade-style query optimizer of SCOPE at Microsoft. We evaluate the resulting system, Cleo, in a production environment using both production and TPC-H workloads. Our results show that the learned cost models are 2 to 3 orders of magnitude more accurate, and 20X more correlated with the actual runtimes, with a large majority (70%) of the plan changes leading to substantial improvements in latency as well as resource usage.}, location = {Portland, OR, USA}, series = {SIGMOD '20}, pages = {99\u2013113}, numpages = {15}, keywords = {machine learning, query optimization, resource optimization, cost models}}
@inproceedings{10.1145/3363542.3363548,title = {Big data and optical lightpaths driven data center network}, author = {Ng T. S. Eugene },year = {2019}, isbn = {9781450368780}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3363542.3363548}, doi = {10.1145/3363542.3363548}, location = {Beijing, China}, series = {OptSys '19}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/3254549,title = {Session details: Session 6: Big Data in Bioinformatics I}, author = {Kalyanaraman Ananth },year = {2017}, isbn = {9781450347228}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3254549}, doi = {10.1145/3254549}, location = {Boston, Massachusetts, USA}, series = {ACM-BCB '17}, pages = {}}
@inproceedings{10.1145/291469.291471,title = {Enhancing data quality in data warehouse environments}, author = {Ballou Donald P. , Tayi Giri Kumar },year = {1999}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/291469.291471}, doi = {10.1145/291469.291471}, pages = {73\u201378}, numpages = {6}}
@inproceedings{10.1145/3494583.3494608,title = {Analysis of the Auditor's Perspective on the Use of Big Data in Financial Statements: UTAUT Model Approach}, author = {Deniswara Kevin , Kartono Rahim Rano , Hamsal Mohammad , Furinto Asnan , Anthony Alexander },year = {2021}, isbn = {9781450390644}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3494583.3494608}, doi = {10.1145/3494583.3494608}, abstract = {The purpose of this study is to determine the perception of the existence of big data on auditors supported by the use of the UTAUT Model. Empirical evidence will be tested on performance expectations, effort expectations, social influences, facilitating conditions, and trust that are predicted to influence behavioral intentions. This research was conducted using quantitative methods on 70 samples of auditor respondents who work in Public Accounting Firms in Indonesia. The analytical method used in this study is PLS-SEM with behavioral intention as the dependent variable. This study found that the variables of independent performance expectations, business expectations, social influences, and facilitation conditions had no significant effect on behavioral intentions. While the trust variable has a significant positive effect. The abstract needs to summarize the content of the paper.}, location = {Macau, China}, series = {ICIBE 2021}, pages = {43\u201350}, numpages = {8}, keywords = {UTAUT Model, Auditor, Behavioral Intention, Big Data, Public Accounting Firm}}
@inproceedings{10.1145/269012.269024,title = {Assessing data quality in accounting information systems}, author = {Kaplan David , Krishnan Ramayya , Padman Rema , Peters James },year = {1998}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/269012.269024}, doi = {10.1145/269012.269024}, pages = {72\u201378}, numpages = {7}}
@inproceedings{10.1145/240455.240479,title = {Anchoring data quality dimensions in ontological foundations}, author = {Wand Yair , Wang Richard Y. },year = {1996}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/240455.240479}, doi = {10.1145/240455.240479}, pages = {86\u201395}, numpages = {10}}
@inproceedings{10.1145/3446999.3447629,title = {Alloy Content on Mechanical Properties Research Based on Industrial Big Data Analysis in Microalloyed Steel}, author = {LIU YANCHAO , Liu HongLiang , NIAN GUANHUA },year = {2020}, isbn = {9781450388559}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3446999.3447629}, doi = {10.1145/3446999.3447629}, abstract = {The pipeline steel for the third-line engineering of West-East was successful produced in Benxi Steel, based on their hot strip mills equipment advantage. However, the impact toughness was unstable during the production. In this research, optical microscope, scanning electron microscopy (SEM) and energy dispersive analysis (EDS) test were used to study on the relationships between different alloy content and microstructure in pipeline steel. What's more, the effect of alloy content on strength and impact toughness in pipeline steel was explored based on the statistics of alloy composition and the mechanical properties in production. The results show that, the alloying elements in pipeline steel can be divided into three categories in this condition. First, Ni, Cu and Mo alloy elements promote the MA constituents transformation, and leading the impact toughness decreases. So, it should be appropriate to reduce the content of these alloy elements, or improvement the cooling process. Second, Cr and Mn alloy, which have no significant effect on both strength and impact toughness, should be appropriately control its content. And third, Nb and Ti micro-alloying, which is particularly improving both strength and impact toughness in pipeline steel. This study is not only provided a basis to improve pipeline steel impact toughness, but also provided the necessary theoretical support for the research and development of higher grade pipeline steel.}, location = {Xi&apos;an, China}, series = {ICIT 2020}, pages = {183\u2013188}, numpages = {6}, keywords = {alloying element, impact toughness, big data, steel, strength}}
@inproceedings{10.1145/3047273.3047274,title = {A Big Data Analytics Framework for Enterprise Service Ecosystems in an e-Governance Scenario}, author = {Shrivastava Swapnil , Pal Supriya N. },year = {2017}, isbn = {9781450348256}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3047273.3047274}, doi = {10.1145/3047273.3047274}, abstract = {In the recent times we have been seeing a fundamental shift from Enterprise Applications towards large scale Enterprise Service Ecosystems. Enterprise Service Ecosystems are developed by modularizing and bundling of individual business rules and functions in the form of services. These services are loosely coupled, distributed and heterogeneous components which orchestrate amongst themselves in a seamless manner. Ecosystem components record the events that are related to the activities performed by them. These components could span across Data Centre, Cloud Infrastructure and Internet of Things. Aadhaar Authentication Ecosystem and e-Governance Service Exchange are examples of Enterprise Service Ecosystems which recently emerged in national e-Governance scenario. A Big Data Analytics Framework for comprehensive mining and analyzing event data of Enterprise Service Ecosystems is proposed in this paper. The offered framework facilitates interesting real time analytics (e.g. Process Conformance Checking, Bottleneck Detection) as well as performing offline analytics (e.g. Process Discovery). The application of the proposed framework for real time analytics is explained using Aadhaar (Unique Identity) Authentication Ecosystem case study.}, location = {New Delhi AA, India}, series = {ICEGOV '17}, pages = {5\u201311}, numpages = {7}, keywords = {Enterprise Service Ecosystem, Event Data, e-Governance, Graph Analytics, Process Mining, Aadhaar Authentication Ecosystem, Complex Event Processing, Big Data Analytics}}
@inproceedings{10.1145/3090057,title = {Data Quality Challenges in Social Spam Research}, author = {El-Mawass Nour , Alaboodi Saad },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3090057}, doi = {10.1145/3090057}, pages = {1\u20134}, numpages = {4}, keywords = {Reproducibility, machine learning, supervised learning, online social networks, social spam detection}}
@inproceedings{10.5555/2648668.2648699,title = {Breaking the boundary for whole-system performance optimization of big data}, author = {Li Yan , Wang Kun , Guo Qi , Li Xin , Zhang Xiaochen , Chen Guancheng , Liu Tao , Li Jian },year = {2013}, isbn = {9781479912353}, publisher = {IEEE Press}, abstract = {MapReduce plays an critical role in finding insights in Big Data. The performance optimization of MapReduce programs is challenging because it requires a comprehensive understanding of the whole system including both hardware layers (processors, storages, networks and etc), and software stacks (operating systems, JVM, runtime, applications and etc). However, most of the existing performance tuning and optimization are based on empirical and heuristic attempts. It remains a blank on how to build a systematical framework which breaks the boundary of multiple layers for performance optimization.In this paper, we propose a performance evaluation framework by correlating performance metrics from different layers, which provides insights to efficiently pinpoint the performance issue. This framework is composed of a series of predefined patterns. Each pattern indicates one or more potential issues. The behavior of a MapReduce program is mapped to the corresponding resource utilization. The framework provides a holistic approach which allows users at different levels of experience to conduct MapReduce program performance optimization.We use Terasort benchmark running on a 10-node Power7R2 cluster as a real case to show how this framework improves the performance. By this framework, we finally get the Terasort result improved from 47 mins to less than 8 mins. In addition to the best practice on performance tuning, several key findings are summarized as valuable workload analysis for JVM, MapReduce runtime and application design.}, location = {Beijing, China}, series = {ISLPED '13}, pages = {126\u2013131}, numpages = {6}}
@inproceedings{10.1145/3254563,title = {Session details: Session 20: Big Data in Bioinformatics II}, author = {Pollard Tom },year = {2017}, isbn = {9781450347228}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3254563}, doi = {10.1145/3254563}, location = {Boston, Massachusetts, USA}, series = {ACM-BCB '17}, pages = {}}
@inproceedings{10.1145/3055219.3055234,title = {Validating the Tele-diagnostic Potential of Affordable Thermography in a Big-data Data-enabled ICU}, author = {Sethi Tavpritesh , Nagori Aditya , Bhatnagar Ambika , Gupta Priyanka , Fletcher Richard , Lodha Rakesh },year = {2017}, isbn = {9781450349307}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3055219.3055234}, doi = {10.1145/3055219.3055234}, abstract = {The potential for whole body thermal patterns in diagnosis of hemodynamic perfusion disturbances in critical care as well as community settings is unexplored. In this study we have combined an in-house digitized Big-data resource from ICU settings with Infra-red thermography to derive novel inferences about the tele-diagnostic potential of IR thermography in diagnosis of shock and perfusion disturbances. While Data-science and Big-data are expected to revolutionize the next generation medicine and healthcare, the scientific efforts towards building Big-data resources for enhancing patient safety and healthcare governance are missing, especially in developing countries. We addressed this challenge and describe our experience on deployment of Big-data warehousing and data-analytics software using lean pipelines developed using open-source technologies and their utility in deriving knowledge and high utility patterns from Affordable Infrared Thermography. These knowledge frameworks and potentially translatable technology were developed in the Pediatric Intensive Care environment through extensive cross-talk between expert clinicians and data-scientists. In this work, we first demonstrate the successful creation of a unique Pediatric ICU resource of over 60,000 hours of continuous multivariate monitoring data followed by validation of the potential of whole body IR thermography in diagnosis of hemodynamic compromise. These patterns were validated through linear mixed models, a state-of-the-art statistical method for longitudinal data. The validated technology is affordable, and can be coupled to smartphones thus providing a huge potential in tele-medicine and electronic governance in healthcare and has the potential to be deployed in a tele-medicine setting with capturing of whole body temperature patterns by Accredited Social Health Activist (ASHA) workers. Therefore, this can enable early diagnosis of critical conditions such as sepsis and shock that are commonly associated with epidemics such as Dengue hemorrhagic fever in developing countries such as India. These images can be remotely shared with expert physicians and data-analysts via telemedicine thus aiding decisions in the Critical Care as well as Community settings.}, location = {New Delhi AA, India}, series = {ICEGOV '17}, pages = {64\u201369}, numpages = {6}, keywords = {Clinical Decision Support, Open Source Technologies, Thermal Imaging, Big-data Pipelines, Community Care, Smartphones, Child health, Telemedicine, Affordable, Intensive Care Units}}
@inproceedings{10.1145/3410566.3410591,title = {Empowering big data analytics with polystore and strongly typed functional queries}, author = {Gillet Annabelle , Leclercq \u00c9ric , Savonnet Marinette , Cullot Nadine },year = {2020}, isbn = {9781450375030}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3410566.3410591}, doi = {10.1145/3410566.3410591}, abstract = {Polystores are of primary importance to tackle the diversity and the volume of Big Data, as they propose to store data according to specific use cases. Nevertheless, analytics frameworks often lack a uniform interface allowing to fully access and take advantage of the various models offered by the polystore. It also should be ensured that the typing of the algebraic expressions built with data manipulation operators can be checked and that schema can be inferred before starting to execute the operators (type-safe).Tensors are good candidates for supporting a pivot data model. They are powerful abstract mathematical objects which can embed complex relationships between entities and that are used in major analytics frameworks. However, they are far away from data models, and lack high level operators to manipulate their content, resulting in bad coding habits and less maintainability, and sometimes poor performances.With TDM (Tensor Data Model), we propose to join the best of both worlds, to take advantage of modeling capabilities of tensors by adding schema and data manipulation operators to them. We developed an implementation in Scala using Spark, providing users with a type-safe and schema inference mechanism that guarantees the technical and functional correctness of composed expressions on tensors at compile time. We show that this extension does not induce overhead and allows to outperform Spark query optimizer using bind join.}, location = {Seoul, Republic of Korea}, series = {IDEAS '20}, pages = {1\u201310}, numpages = {10}, keywords = {query language, tensor, high performance data analytics, polystore}}
@inproceedings{10.1145/3456887.3459688,title = {Research on financial management mode of e-commerce enterprises based on multi influencing factors from the perspective of big data}, author = {Hui Zhu , Zhaoming Li , Xuecong Cao , Sisi Chen },year = {2021}, isbn = {9781450389969}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456887.3459688}, doi = {10.1145/3456887.3459688}, abstract = {In the era of big data, the centralization and subdivision of data provide abundant data resources for the financial management analysis of enterprises, but how to stand out from the financial data analysis depends on the innovation consciousness of enterprises. As an important part of enterprise management, financial management should also make changes to meet the development needs of big data. The financial management of e-commerce enterprises often deals with financial and non-financial data. The rise of big data provides powerful information technology support for the financial management of e-commerce enterprises, which enables enterprises to fully explore, analyze and process the information needed by finance. Based on multiple influencing factors, this paper starts with the necessity of innovation in financial management of e-commerce enterprises, and expounds the influence of big data on traditional financial management of enterprises, and also brings ideas and countermeasures for innovation of financial management mode for some enterprises.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2021}, pages = {1407\u20131411}, numpages = {5}, keywords = {financial management, E-commerce enterprises, Big data}}
@inproceedings{10.1145/3465631.3465664,title = {Application Strategies of Medical Big Data in Health Economic Management}, author = {Yu Xiaomu , Yin Yuelin },year = {2021}, isbn = {9781450385015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3465631.3465664}, doi = {10.1145/3465631.3465664}, abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.}, location = {Jakarta, Indonesia}, series = {ICIMTECH 21}, pages = {1\u20135}, numpages = {5}}
@inproceedings{10.1145/3128572.3140452,title = {Beyond Big Data: What Can We Learn from AI Models? Invited Keynote}, author = {Caliskan Aylin },year = {2017}, isbn = {9781450352024}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3128572.3140452}, doi = {10.1145/3128572.3140452}, abstract = {My research involves the heavy use of machine learning and natural language processing in novel ways to interpret big data, develop privacy and security attacks, and gain insights about humans and society through these methods. I do not use machine learning only as a tool but I also analyze machine learning models? internal representations to investigate how the artificial intelligence perceives the world. This work [3] has been recently featured in Science where I showed that societal bias exists at the construct level of machine learning models, namely semantic space word embeddings which are dictionaries for machines to understand language. When I use machine learning as a tool to uncover privacy and security problems, I characterize and quantify human behavior in language, including programming languages, by coming up with a linguistic fingerprint for each individual. By extracting linguistic features from natural language or programming language texts of humans, I show that humans have unique linguistic fingerprints since they all learn language on an individual basis. Based on this finding, I can de-anonymize humans that have written certain text, source code, or even executable binaries of compiled code [2, 4, 5]. This is a serious privacy threat for individuals that would like to remain anonymous, such as activists, programmers in oppressed regimes, or malware authors. Nevertheless, being able to identify authors of malicious code enhances security. On the other hand, identifying authors can be used to resolve copyright disputes or detect plagiarism. The methods in this realm [1] have been used to identify so called doppelg\u00e4ngers to link the accounts that belong to the same identities across platforms, especially underground forums that are business platforms for cyber criminals. By analyzing machine learning models? internal representation and linguistic human fingerprints, I am able to uncover facts about the world, society, and the use of language, which have implications for privacy, security, and fairness in machine learning.}, location = {Dallas, Texas, USA}, series = {AISec '17}, pages = {1}, numpages = {1}, keywords = {invited keynote}}
@inproceedings{10.1145/3513135,title = {Editorial: Special Issue on Deep Learning for Data Quality}, author = {Santoro Donatello , Thirumuruganathan Saravanan , Papotti Paolo },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3513135}, doi = {10.1145/3513135}, abstract = {This editorial summarizes the content of the Special Issue on Deep Learning for Data Quality of the Journal of Data and Information Quality (JDIQ).}, pages = {1\u20133}, numpages = {3}, keywords = {data labeling, Deep learning, schema matching}}