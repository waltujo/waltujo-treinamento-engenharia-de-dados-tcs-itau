@inproceedings{10.1145/3291801.3291822,
author = {Sun, Zhaohao and Strang, Kenneth and Li, Rongping},
title = {Big Data with Ten Big Characteristics},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291822},
doi = {10.1145/3291801.3291822},
abstract = {This paper reveals ten big characteristics (10 Bigs) of big data and explores their non-linear interrelationships through presenting a unified framework of big data. The framework has three levels: fundamental level, technological level, and socio-economic level. The fundamental level has four big fundamental characteristics of big data. The technological level consists of three big technological characteristics of big data. The socioeconomic level has three big socioeconomic characteristics of big data. The paper looks at each level of the proposed framework from a service-oriented perspective. The proposed approach in this paper might facilitate the research and development of big data, big data analytics, business intelligence, and business analytics.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {56–61},
numpages = {6},
keywords = {big data analytics, business analytics, artificial intelligence, big data, business intelligence},
location = {Weihai, China},
series = {ICBDR 2018}
}

@article{10.1145/2331042.2331058,
author = {Heer, Jeffrey and Kandel, Sean},
title = {Interactive Analysis of Big Data},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331058},
doi = {10.1145/2331042.2331058},
abstract = {New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.},
journal = {XRDS},
month = {sep},
pages = {50–54},
numpages = {5}
}

@inproceedings{10.1145/3340531.3412173,
author = {Song, Shaoxu and Zhang, Aoqian},
title = {IoT Data Quality},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412173},
doi = {10.1145/3340531.3412173},
abstract = {Data quality issues have been widely recognized in IoT data, and prevent the downstream applications. In this tutorial, we review the state-of-the-art techniques for IoT data quality management. In particular, we discuss how the dedicated approaches improve various data quality dimensions, including validity, completeness and consistency. Among others, we further highlight the recent advances by deep learning techniques for IoT data quality. Finally, we indicate the open problems in IoT data quality management, such as benchmark or interpretation of data quality issues.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3517–3518},
numpages = {2},
keywords = {internet of things, data curation},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1145/3158335,
author = {Johnson, Jeffrey and Denning, Peter and Sousa-Rodrigues, David and Delic, Kemal A.},
title = {Big Data, Digitization, and Social Change: Big Data (Ubiquity Symposium)},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2017},
number = {December},
url = {https://doi.org/10.1145/3158335},
doi = {10.1145/3158335},
abstract = {We use the term "big data" with the understanding that the real game changer is the connection and digitization of everything. Every portfolio is affected: finance, transport, housing, food, environment, industry, health, welfare, defense, education, science, and more. The authors in this symposium will focus on a few of these areas to exemplify the main ideas and issues.},
journal = {Ubiquity},
month = {dec},
articleno = {1},
numpages = {8}
}

@inproceedings{10.1145/3341161.3343512,
author = {Leung, Carson and Jiang, Fan and Zhang, Yibin},
title = {Flexible Compression of Big Data},
year = {2020},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3343512},
doi = {10.1145/3341161.3343512},
abstract = {High volumes of valuable data and information can be easily collected in the current era of big data. As rich and constant sources of big data, an incredible amount of people from different social stratum take part in social networks. Hence, social networks are desired for many research topics. In social networks, users (or social entities) are often linked by some 'following' relationships. As the social networks growing, some famous users account (or social entities) might be followed by a large number of same other users. In this situation, we call those famous users as frequently followed groups, which some researchers (or businesses) may be interested in them for investigating. However, the discovery of those frequently followed groups might be difficult and challenging because the following data in social networks are usually very big but sparse (huge number of users lead to big 'following' data, but each user is likely only following a small number of other users). As a result, in this paper, we present a new compression model, which can be used during mining these very big but sparse social networks for discovering the frequently followed groups of users/social entities.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {741–748},
numpages = {8},
keywords = {big data, social network 'following' patterns mining, social networks},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@article{10.1145/2627534.2627561,
author = {Savas, Onur and Sagduyu, Yalin and Deng, Julia and Li, Jason},
title = {Tactical Big Data Analytics: Challenges, Use Cases, and Solutions},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627561},
doi = {10.1145/2627534.2627561},
abstract = {We discuss tactical challenges of the Big Data analytics regarding the underlying data, application space, and com- puting environment, and present a comprehensive solution framework motivated by the relevant tactical use cases. First, we summarize the unique characteristics of the Big Data problem in the Department of Defense (DoD) context and underline the main differences from the commercial Big Data problems. Then, we introduce two use cases, (i) Big Data analytics with multi-intelligence (multi-INT) sensor data and (ii) man-machine crowdsourcing using MapReduce framework. For these two use cases, we introduce Big Data analytics and cloud computing solutions in a coherent frame- work that supports tactical data, application, and computing needs.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {86–89},
numpages = {4},
keywords = {cloud computing, analytics, big data, tactical environment, algorithms}
}

@article{10.1145/505248.506010,
author = {Pipino, Leo L. and Lee, Yang W. and Wang, Richard Y.},
title = {Data Quality Assessment},
year = {2002},
issue_date = {April 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/505248.506010},
doi = {10.1145/505248.506010},
abstract = {How good is a company's data quality? Answering this question requires usable data quality metrics. Currently, most data quality measures are developed on an ad hoc basis to solve specific problems [6, 8], and fundamental principles necessary for developing usable metrics in practice are lacking. In this article, we describe principles that can help organizations develop usable data quality metrics.},
journal = {Commun. ACM},
month = {apr},
pages = {211–218},
numpages = {8}
}

@inproceedings{10.5555/2735522.2735573,
author = {Mohammadi, Mohammad Mahdi and Raahemi, Bijan and Cheraghchi, Fatemeh and Obidallah, Wael and Bigdeli, Elnaz},
title = {Big Data Analytics Using Hadoop},
year = {2014},
publisher = {IBM Corp.},
address = {USA},
abstract = {The exponential growth of data, especially over the internet; leads to the dramatic rise of unstructured and semi-structured data, in addition to the traditional (structured) data. Since relational databases and associated tools were designed to interact with structured data, companies such as Google and Yahoo were facing challenges dealing with the unstructured and semi-structured data. When the volume of data goes beyond the processing capacity of the existing algorithms, it is considered as Big Data. Hadoop is a popular technology for analyzing Big data. There are tools available on Hadoop platform to assist analysts create complex queries and run machine learning algorithms in a parallel and distributed fashion. The goal of this workshop is to provide the participants with hands-on experiences on analyzing Big data, installing Hadoop on Linux-based machines (PCs equipped with Ubuntu OS), and running examples on Hadoop framework.},
booktitle = {Proceedings of 24th Annual International Conference on Computer Science and Software Engineering},
pages = {323–325},
numpages = {3},
location = {Markham, Ontario, Canada},
series = {CASCON '14}
}

@article{10.14778/2824032.2824140,
author = {Balazinska, Magdalena},
title = {Big Data Research: Will Industry Solve All the Problems?},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824140},
doi = {10.14778/2824032.2824140},
abstract = {The need for effective tools for big data data management and analytics continues to grow. While the ecosystem of tools is expanding many research problems remain open: they include challenges around efficient processing, flexible analytics, ease of use, and operation as a service. Many new systems and much innovation, however, come from industry (or from academic projects that quickly became big players in industry). An important question for our community is whether industry will solve all the problems or whether there is a place for academic research in big data and what is that place. In this paper, we address this question by looking back at our research on the Nuage, CQMS, Myria, and Data Pricing projects, and the SciDB collaboration.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2053–2056},
numpages = {4}
}

@inproceedings{10.1145/3209978.3210213,
author = {Ye, Jieping},
title = {Big Data at Didi Chuxing},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210213},
doi = {10.1145/3209978.3210213},
abstract = {Didi Chuxing is the largest ride-sharing platform in China, providing transportation services for over 400 million users. Every day, Didi Chuxing's platform generates over 100 TB worth of data, processes more than 40 billion routing requests, and produces over 15 billion location points. In this talk, I will explain how Didi Chuxing applies big data and AI technologies to analyze big transportation data and improve the travel experience for millions of users.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {1341},
numpages = {1},
keywords = {ai, big data, transportation},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3274250.3275113,
author = {Xinhua, E. and Zhu, Binjie},
title = {Big Data Service Delivery Network},
year = {2018},
isbn = {9781450365383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274250.3275113},
doi = {10.1145/3274250.3275113},
abstract = {Big data service is a promising technology in Internet. Quality of service of big data services is a very important indicator. A service delivery network was presented in this paper to reduce service delays. The web services were distribution to the edge of the network to making it closer to users, so the network delay is small. A services distribution method with QoS guarantee was presented in this paper. Friendly degrees were measured in this method between the servers. According to the friendly degree determine the coverage areas of a copy. It takes up less resource under the premise of QoS guaranteeing.},
booktitle = {Proceedings of the 2018 1st International Conference on Mathematics and Statistics},
pages = {89–91},
numpages = {3},
keywords = {SDN, Web service, Distribution method},
location = {Porto, Portugal},
series = {ICoMS '18}
}

@inproceedings{10.1145/2859889.2883586,
author = {Zibitsker, Boris},
title = {Big Data Applications Performance Assurance},
year = {2016},
isbn = {9781450341479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2859889.2883586},
doi = {10.1145/2859889.2883586},
abstract = {Today's fast-paced businesses have to make business decisions in real-time. That creates pressure on IT leaders to develop near real-time Big Data and Data Warehouse applications that apply advance analytics against large volumes of data to deliver recommendations fast. Hardware and software used to build Big Data infrastructure is cheap, but management of complex environments is not easy In this presentation we will review role of Performance Assurance incorporating Descriptive, Diagnostic, Predictive, Prescriptive and Control Analytics during each phase of the Application and Data life cycle. We will review challenges and Performance Assurance solutions for Big Data Batch and Real Time applications based on YARN, Map/Reduce, Kafka, Spark/Storm and Cassandra Apache projects},
booktitle = {Companion Publication for ACM/SPEC on International Conference on Performance Engineering},
pages = {31},
numpages = {1},
location = {Delft, The Netherlands},
series = {ICPE '16 Companion}
}

@article{10.1145/2481244.2481246,
author = {Fan, Wei and Bifet, Albert},
title = {Mining Big Data: Current Status, and Forecast to the Future},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/2481244.2481246},
doi = {10.1145/2481244.2481246},
abstract = {Big Data is a new term used to identify datasets that we can not manage with current methodologies or data mining software tools due to their large size and complexity. Big Data mining is the capability of extracting useful information from these large datasets or streams of data. New mining techniques are necessary due to the volume, variability, and velocity, of such data. The Big Data challenge is becoming one of the most exciting opportunities for the years to come. We present in this issue, a broad overview of the topic, its current status, controversy, and a forecast to the future. We introduce four articles, written by influential scientists in the field, covering the most interesting and state-of-the-art topics on Big Data mining.},
journal = {SIGKDD Explor. Newsl.},
month = {apr},
pages = {1–5},
numpages = {5}
}

@article{10.5555/2602724.2602725,
author = {Hendler, Jim},
title = {Big Data Meets Computer Science},
year = {2014},
issue_date = {June 2014},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {29},
number = {6},
issn = {1937-4771},
abstract = {As "big data" moves from buzzword to practice, campus departments must increasingly figure out where data science fits into their curricula. Clearly such an interdisciplinary area crosses traditional boundaries ranging from statistics traditionally taught in mathematics or engineering departments, a new method of scientific discovery for biologists and chemists, a new challenge for ethicists and political scientists, and a new realm for design and electronic arts, etc. Within computer science departments, it currently seems to reside in the machine learning and knowledge discovery areas where the metaphor of big data as "the new oil" to be mined is pursued. In this talk, however, I opine that just as oil is important for the energy it generates, which powers the technologies of modern life, data is increasingly important for the information it generates, which will power the information applications of the future. We will explore some of these emerging trends, ranging from high performance modeling to the Watson AI system, looking at what we might want to be teaching our students if they are to be leaders in this emerging area.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {5–6},
numpages = {2}
}

@inproceedings{10.5555/3172795.3172852,
author = {Wong, Serene and Jurisica, Igor},
title = {Big Data Analytics: Challenges and Applications to Health Care},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {With recent technological advancement, biomedical data is growing rapidly, in terms of volume, quality and depth. This creates many challenges, and in this workshop we focused on how one can turn this data using "big data analytics" into knowledge that can be used effectively. One of the main building blocks of this process is diverse networks - typed graphs that provide detailed annotation of relationships among measured entities.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {338},
numpages = {1},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/3210284.3214344,
author = {Markl, Volker},
title = {Mosaics in Big Data: Stratosphere, Apache Flink, and Beyond},
year = {2018},
isbn = {9781450357821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210284.3214344},
doi = {10.1145/3210284.3214344},
abstract = {The global database research community has greatly impacted the functionality and performance of data storage and processing systems along the dimensions that define "big data", i.e., volume, velocity, variety, and veracity. Locally, over the past five years, we have also been working on varying fronts. Among our contributions are: (1) establishing a vision for a database-inspired big data analytics system, which unifies the best of database and distributed systems technologies, and augments it with concepts drawn from compilers (e.g., iterations) and data stream processing, as well as (2) forming a community of researchers and institutions to create the Stratosphere platform to realize our vision. One major result from these activities was Apache Flink, an open-source big data analytics platform and its thriving global community of developers and production users. Although much progress has been made, when looking at the overall big data stack, a major challenge for database research community still remains. That is, how to maintain the ease-of-use despite the increasing heterogeneity and complexity of data analytics, involving specialized engines for various aspects of an end-to-end data analytics pipeline, including, among others, graph-based, linear algebra-based, and relational-based algorithms, and the underlying, increasingly heterogeneous hardware and computing infrastructure. At TU Berlin, DFKI, and the Berlin Big Data Center (BBDC), we aim to advance research in this field via the Mosaics project. Our goal is to remedy some of the heterogeneity challenges that hamper developer productivity and limit the use of data science technologies to just the privileged few, who are coveted experts.},
booktitle = {Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems},
pages = {7–13},
numpages = {7},
keywords = {declarative languages, Apache Flink, big data, data science, federation, heterogeneous data management},
location = {Hamilton, New Zealand},
series = {DEBS '18}
}

@inproceedings{10.1145/2781562.2781574,
author = {See, S. L.},
title = {Big Data Applications: Adaptive User Interfaces to Enhance Managerial Decision Making},
year = {2015},
isbn = {9781450334617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2781562.2781574},
doi = {10.1145/2781562.2781574},
abstract = {Big data applications may present opportunity for business executives to make better informed decisions. However, how well such application can support and effect managerial decision making still remains a challenge. From a case study of a traditional business in adopting new technology, it was found that an underlying issue that impeded effective and efficient managerial decision making lied in the human computer interaction process, and the design of the system user interface can be the culprit. With the rise of big data revolution, it seems that this underlying issue has still not been resolved for the applications to best support the executive users. This paper therefore shares research findings and developments in the social media and human language technology, and suggests employing adaptive user interfaces for big data applications to better support managerial decision making.},
booktitle = {Proceedings of the 17th International Conference on Electronic Commerce 2015},
articleno = {11},
numpages = {3},
keywords = {personality, human computer interaction, big data, management, human behavior analysis, adaptive user interface design, information systems, Managerial decision making, decision support systems},
location = {Seoul, Republic of Korea},
series = {ICEC '15}
}

@article{10.1145/3158337,
author = {Huberman, Bernardo A.},
title = {Big Data and the Attention Economy: Big Data (Ubiquity Symposium)},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2017},
number = {December},
url = {https://doi.org/10.1145/3158337},
doi = {10.1145/3158337},
abstract = {While attention has always been prized above money, few people have had the means to attract it to themselves. But the new digital economy has provided everyone with a loudspeaker; thus efforts at getting noticed have rapidly escalated in global society. The attention economy focuses on the mechanisms that mediate the allocation of this scarce entity. Social networks and big data play a role in determining what is noticed and acted upon.},
journal = {Ubiquity},
month = {dec},
articleno = {2},
numpages = {7}
}

@inproceedings{10.1145/3289100.3289108,
author = {Bahadi, Jihane and El Asri, Bouchra and Courtine, M\'{e}lanie and Rhanoui, Maryem and Kergosien, Yannick},
title = {Towards Efficient Big Data: Hadoop Data Placing and Processing},
year = {2018},
isbn = {9781450365079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289100.3289108},
doi = {10.1145/3289100.3289108},
abstract = {Currently, the generated data flow is growing at a high rate resulting to the problem of data obesity and abundance, but yet a lack of pertinent information. To handle this Big Data, Hadoop is a distributed framework that facilitates data storage and processing. Although Hadoop is designed to deal with demands of storage and analysis of ever-growing Data, its performance characteristics are still to improve. In this regard, many approaches have been proposed to enhance Hadoop capabilities. Nevertheless, an overview of these approaches shows that several aspects need to be improved in terms of performance and data relevancy. The main challenge is how to extract efficiently value from the big data sources. For this purpose, we propose in this paper to discuss Hadoop architecture and intelligent data discovery, and propose an effective on-demand Big Data contribution enabling to process relevant data in efficient and effective way according to the stakeholder's needs, and aiming to boost Data appointment by integrating multidimensional approach.},
booktitle = {Proceedings of the 2nd International Conference on Smart Digital Environment},
pages = {42–47},
numpages = {6},
keywords = {MapReduce jobs, Hadoop, Multidimensional approach, Big Data, Data placing, Intelligent processing},
location = {Rabat, Morocco},
series = {ICSDE'18}
}

@article{10.1145/2699414,
author = {Reed, Daniel A. and Dongarra, Jack},
title = {Exascale Computing and Big Data},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2699414},
doi = {10.1145/2699414},
abstract = {Scientific discovery and engineering innovation requires unifying traditionally separated high-performance computing and big data analytics.},
journal = {Commun. ACM},
month = {jun},
pages = {56–68},
numpages = {13}
}

@inproceedings{10.1145/2382416.2382420,
author = {Manadhata, Pratyusa K.},
title = {Big Data for Security: Challenges, Opportunities, and Examples},
year = {2012},
isbn = {9781450316613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382416.2382420},
doi = {10.1145/2382416.2382420},
abstract = {This is the age of big data. Enterprises collect large amounts of data about their operations and analyze the data to improve all aspects of their businesses. Big data for security, i.e., the analysis of very large enterprise data sets to identify actionable security information and hence to improve enterprise security, however, is a relatively unexplored area. Enterprises routinely collect terabytes of security relevant data, e.g., network logs and application logs, for several reasons such as availability of cheap storage and need for regulatory compliance and post hoc forensic analysis. But we face a situation where more is less; the more data we collect, the less is our ability to derive actionable information from the data.Our research group is trying to move toward a scenario where more is more; we aim to design and implement algorithms and systems to identify security relevant information from large enterprise datasets. The more data we collect, the more value we derive from the data. Our approach opens up new opportunities by combining data from multiple sources in an enterprise and from multiple enterprises. We, however, face many challenges, e.g., legal, privacy, and technical issues regarding scalable data collection and storage and scalable analytics platforms for security.Our group is currently focusing on several big data problems. In this talk, we will briefly describe the problems and then focus on one example - scalable and reliable identification of infected hosts in an enterprise network and of malicious domains visited by the enterprise's hosts. We model the identification problem as an inference problem over very large graphs derived from enterprise datasets. We will describe our experience of applying the inference approach to datasets collected from multiple enterprises worldwide.},
booktitle = {Proceedings of the 2012 ACM Workshop on Building Analysis Datasets and Gathering Experience Returns for Security},
pages = {3–4},
numpages = {2},
keywords = {inference, big data analytics, malicious domain detection, data mining, malware detection, big data},
location = {Raleigh, North Carolina, USA},
series = {BADGERS '12}
}

@inproceedings{10.5555/2694443.2694450,
author = {Borkar, Vinayak and Carey, Michael J.},
title = {Big Data Technologies circa 2012},
year = {2012},
publisher = {Computer Society of India},
address = {Mumbai, Maharashtra, IND},
abstract = {The growth of the World Wide Web has led to an astronomical amount of data being generated. More recently, the amount of user-generated content has seen tremendous expansion thanks to social media like Facebook and Twitter. Enterprises, researchers, and even governments consider this data to be an invaluable source of insight into people's behavior, creating a race to analyze as much data as possible. This race has driven virtually everyone, ranging from Web companies to brick and mortar businesses, into a "Big Data" frenzy. On the systems side, traditional relational databases have proven to be un-scalable, too expensive, too rigid, and/or too heavy-weight for dealing with current Big Data problems. As a result, there has been an explosion in the number of systems being developed, both within industry as well as in academia, to manage massive amounts of data.},
booktitle = {Proceedings of the 18th International Conference on Management of Data},
pages = {12–14},
numpages = {3},
location = {Pune, India},
series = {COMAD '12}
}

@article{10.1145/3158339,
author = {Birkin, Mark},
title = {Big Data for Social Science Research: Big Data (Ubiquity Symposium)},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2018},
number = {January},
url = {https://doi.org/10.1145/3158339},
doi = {10.1145/3158339},
abstract = {Academic studies exploiting novel data sources are scarce. Typically, data is generated by commercial businesses or government organizations with no mandate and little motivation to share their assets with academic partners---partial exceptions include social messaging data and some sources of open data. The mobilization of citizen sensors at a massive scale has allowed for the development of impressive infrastructures. However, data availability is driving applications---problems are prioritized because data is available rather than because they are inherently important or interesting. The U.K. is addressing this through investments by the Economic and Social Research Council in its Big Data Network. A group of Administrative Data Research Centres are tasked with improving access to data sets in central government, while a group of Business and Local Government Centres are tasked with improving access to commercial and regional sources. This initiative is described. It is illustrated by examples from health care, transport, and infrastructure. In all of these cases, the integration of data is a key consideration. For social science problems relevant to policy or academic studies, it is unlikely all the answers will be found in a single novel data source, but rather a combination of sources is required. Through such synthesis great leaps are possible by exploiting models that have been constructed and refined over extended periods of time e.g., microsimulation, spatial interaction models, agents, discrete choice, and input-output models. Although interesting and valuable new methods are appearing, any suggestion that a new box of magic tricks labeled "Big Data Analytics" that sits easily on top of massive new datasets can radically and instantly transform our long-term understanding of society is na\"{\i}ve and dangerous. Furthermore, the privacy and confidentiality of personal data is a great concern to both the individuals concerned and the data owners.},
journal = {Ubiquity},
month = {jan},
articleno = {1},
numpages = {7}
}

@inproceedings{10.1145/2905055.2905099,
author = {Misra, Rachita and Panda, Bijayalaxmi and Tiwary, Mayank},
title = {Big Data and ICT Applications: A Study},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905099},
doi = {10.1145/2905055.2905099},
abstract = {Big Data is used to manage the data due to their large size and complexity, because it can't be handled with the traditional methods and the current technology or tools used for that. Big Data mining is populated with 5 V's volume, variability, velocity, variety, value which has the ability of retrieving important information from the huge data storage. Now the challenge of Big Data is becoming the opportunities of research for the next few years. Throughout the world researchers and developers are trying to make use of the Big Data technology to extend the ICT applications from the traditional LAN, WAN environment to Internet on cloud with Big Data. In this scenario this paper provides and an overview of some of the ICT applications which take advantage of data mining and analytics for big data. The paper tries to establish the wide range of applications of big data in ICT with the currently available data mining &amp; data analytics platforms, languages and tools. An effort has been made to analyze the challenges faced in the different application fields. Some of the advances in the Big Data technology research that can help solve some of these challenges in ICT applications have been discussed in brief.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {41},
numpages = {6},
keywords = {HDFS, ICT, Big data analytics, Hadoop, Big data},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/2247596.2247598,
author = {Borkar, Vinayak and Carey, Michael J. and Li, Chen},
title = {Inside "Big Data Management": Ogres, Onions, or Parfaits?},
year = {2012},
isbn = {9781450307901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2247596.2247598},
doi = {10.1145/2247596.2247598},
abstract = {In this paper we review the history of systems for managing "Big Data" as well as today's activities and architectures from the (perhaps biased) perspective of three "database guys" who have been watching this space for a number of years and are currently working together on "Big Data" problems. Our focus is on architectural issues, and particularly on the components and layers that have been developed recently (in open source and elsewhere) and on how they are being used (or abused) to tackle challenges posed by today's notion of "Big Data". Also covered is the approach we are taking in the ASTERIX project at UC Irvine, where we are developing our own set of answers to the questions of the "right" components and the "right" set of layers for taming the "Big Data" beast. We close by sharing our opinions on what some of the important open questions are in this area as well as our thoughts on how the dataintensive computing community might best seek out answers.},
booktitle = {Proceedings of the 15th International Conference on Extending Database Technology},
pages = {3–14},
numpages = {12},
location = {Berlin, Germany},
series = {EDBT '12}
}

@article{10.1145/2874239.2874256,
author = {Gumbus, Andra and Grodzinsky, Frances},
title = {Era of Big Data: Danger of Descrimination},
year = {2016},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0095-2737},
url = {https://doi.org/10.1145/2874239.2874256},
doi = {10.1145/2874239.2874256},
abstract = {We live in a world of data collection where organizations and marketers know our income, our credit rating and history, our love life, race, ethnicity, religion, interests, travel history and plans, hobbies, health concerns, spending habits and millions of other data points about our private lives. This data, mined for our behaviors, habits, likes and dislikes, is referred to as the "creep factor" of big data [1]. It is estimated that data generated worldwide will be 1.3 zettabytes (ZB) by 2016. The rise of computational power plus cheaper and faster devices to capture, collect, store and process data, translates into the "datafication" of society [4]. This paper will examine a side effect of datafication: discrimination.},
journal = {SIGCAS Comput. Soc.},
month = {jan},
pages = {118–125},
numpages = {8},
keywords = {discrimination, privacy, big data, human resources}
}

@inproceedings{10.1145/2345316.2345328,
author = {Xue, Zhiming},
title = {Cloud Computing &amp; Big Data Computing},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345328},
doi = {10.1145/2345316.2345328},
abstract = {The amount of data each organization deals with today has been rapidly growing. However, analyzing large datasets commonly referred to as "big data" has been a huge challenge due to lack of suitable tools and adequate computing resources. Why are organizations, both in public sector and private sector, so keen on unlocking business insights from all structured and unstructured data? What is the current state of big data solutions and service providers? How effective are some of the solutions that have been put into real world practices? What is the current state of cloud computing technologies? What impacts have cloud computing technologies available in public clouds and private clouds had on the way organizations addressing big data challenges? How to secure big data in the clouds? What are the future roadmaps for cloud-based big data solutions, especially for geospatial related applications?This panel discussion will include a short presentation or discussion related to big data and cloud computing by each panelist, followed by questions and questions from the audience and the panel.},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {9},
numpages = {1},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@inproceedings{10.1145/3363459.3363533,
author = {Elias, Rita and Issa, Raja R. A.},
title = {Big Data: A Decade of Energy Characteristics of Single-Family Homes in Florida},
year = {2019},
isbn = {9781450370141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3363459.3363533},
doi = {10.1145/3363459.3363533},
abstract = {Housing in Florida is mainly driven by population growth of between 300,000 and 400,000 people per year, which makes Florida the third largest homebuilding state in the U.S. This high rate of population growth sheds light on the importance of building healthy residential houses that are energy-efficient throughout all of Florida, in all its climate zones. The purpose of this study is to investigate the direction Florida's single-family homes constructed between the years 2009 and 2018 are taking with respect to energy efficiency by analyzing their energy characteristics through big data analysis. Therefore, this study (a) developed a comprehensive literature review of the existing energy-efficient design strategies adopted in Florida. In addition, (b) explained the process of collecting information about materials and design strategies used in single-family houses constructed between the years 2009 and 2018, from energy forms prepared in the framework of the permit application process at every local building permit department in different counties all over Florida. Finally, (c) analyzed the evolution of information collected throughout the years, including but not limited to wall types, ceiling types, ducts' location, windows' features, heating and cooling systems, in the different climate zones of Florida influencing the building performance. The results of this big data analysis indicated that, on the average, single-family homes in Florida tended to get more energy-efficient and sustainable throughout the last decade (2009-2018). This conclusion cannot be totally confirmed unless all economic, environmental, and social aspects of sustainability are also taken into consideration.},
booktitle = {Proceedings of the 1st ACM International Workshop on Urban Building Energy Sensing, Controls, Big Data Analysis, and Visualization},
pages = {101–111},
numpages = {11},
keywords = {Florida, design strategies, single-family houses, sustainability, Energy},
location = {New York, NY, USA},
series = {UrbSys'19}
}

@inproceedings{10.1145/3090354.3090401,
author = {Khtira, R. and Elasri, B. and Rhanoui, M.},
title = {From Data to Big Data: Moroccan Public Sector},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090401},
doi = {10.1145/3090354.3090401},
abstract = {Digitalization, interconnection, open data, and the use of internet and social media by governments and citizens have consequently leaded to an enormous growth of data in the public sector. As government data available increases, many big data initiatives have been launched by governments in order to derive insights and create new value in many areas. This paper details some concepts related to government data specifically open data and big data in order to draw the relationships between them and show its potential value. The article also brings into light the impact of digital progress, made so far in the Moroccan Administration, on the growth of data in the public sector, and presents first steps taken by the government toward big data era. In particular, we focus on social media, open government data and e-government initiatives. In addition, the paper showcases examples of applying advanced analytics in the public finances specifically in Tax Administration to uncover insights and make better decisions from large datasets. This article also overviews some challenges facing future big data initiatives in the Moroccan public sector and proposes recommendations to address them.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {46},
numpages = {6},
keywords = {Advanced Analytics, Public finances, Open Government data, Moroccan public sector, Machine Learning, E-Government, Big data},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@article{10.1145/2627534.2627557,
author = {Suthaharan, Shan},
title = {Big Data Classification: Problems and Challenges in Network Intrusion Prediction with Machine Learning},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627557},
doi = {10.1145/2627534.2627557},
abstract = {This paper focuses on the specific problem of Big Data classification of network intrusion traffic. It discusses the system challenges presented by the Big Data problems associated with network intrusion prediction. The prediction of a possible intrusion attack in a network requires continuous collection of traffic data and learning of their characteristics on the fly. The continuous collection of traffic data by the network leads to Big Data problems that are caused by the volume, variety and velocity properties of Big Data. The learning of the network characteristics require machine learning techniques that capture global knowledge of the traffic patterns. The Big Data properties will lead to significant system challenges to implement machine learning frameworks. This paper discusses the problems and challenges in handling Big Data classification using geometric representation-learning techniques and the modern Big Data networking technologies. In particular this paper discusses the issues related to combining supervised learning techniques, representation-learning techniques, machine lifelong learning techniques and Big Data technologies (e.g. Hadoop, Hive and Cloud) for solving network traffic classification problems.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {70–73},
numpages = {4},
keywords = {intrusion detection, machine learning, big data, hadoop distributed file systems}
}

@inproceedings{10.1145/2837060.2837116,
author = {Mohania, Mukesh},
title = {Big Data Processing Flow},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837116},
doi = {10.1145/2837060.2837116},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {9},
numpages = {1},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/3287324.3287551,
author = {Lewis, Mark C.},
title = {Big Data Analytics with Spark},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287551},
doi = {10.1145/3287324.3287551},
abstract = {Born from a Berkeley graduate project, the Apache Spark library has grown to be the most broadly used big data analytics platform. While Spark integrates with the older Hadoop ecosystem, it provides much more intuitive, faster, and powerful abstractions for manipulating distributed data than MapReduce. In this workshop, we will cover the basics of the Spark library with the goal of getting participants up to speed so that they can use the library or teach it in courses that involve big data or distributed processing. Participants will work with examples that range from calculating basic summary statistics to using the Spark Machine Learning library for performing sophisticated machine learning analyses on large datasets. Tasks during the session will be performed on smaller samples using the Spark local standalone implementation on participant's laptops. We will also discuss how Spark can be run on a local or cloud-based cluster and point participants toward resources for setting up those environments for their students.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1242},
numpages = {1},
keywords = {distributed computing, big data, data science, spark},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.5555/2886444.2886490,
author = {Badashian, Ali Sajedi and Shah, Vraj and Stroulia, Eleni},
title = {GitHub's Big Data Adaptor: An Eclipse Plugin},
year = {2015},
publisher = {IBM Corp.},
address = {USA},
abstract = {The data of GitHub, the most popular code-sharing platform, fits the characteristics of "big data" (Volume, Variety and Velocity). To facilitate studies on this huge GitHub data volume, the GHTorrent web-site publishes a MYSQL dump of (some) GitHub data quarterly. Unfortunately, developers using these published data dumps face challenges with respect to the time required to parse and ingest the data, the space required to store it, and the latency of their queries. To help address these challenges, we developed a data adaptor as an Eclipse plugin, which efficiently handles this dump. The plugin offers an interactive interface through which users can explore and select any field in any table. After extracting the data selected by the user, the parser exports it in easy-to-use spreadsheets. We hope that using this plugin will facilitate further studies on the GitHub data as a whole.},
booktitle = {Proceedings of the 25th Annual International Conference on Computer Science and Software Engineering},
pages = {265–268},
numpages = {4},
keywords = {big data, mining software repositories, eclipse plugin, software tools, GitHub},
location = {Markham, Canada},
series = {CASCON '15}
}

@article{10.1145/2788453,
author = {Kitner, Kathi R. and de Wet, Thea},
title = {Big City, Big Data},
year = {2015},
issue_date = {July - August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1072-5520},
url = {https://doi.org/10.1145/2788453},
doi = {10.1145/2788453},
abstract = {This forum addresses conceptual, methodological, and professional issues that arise in the UX field's continuing effort to contribute robust information about users to product planning and design. --- David Siegel and Susan Dray, Editors},
journal = {Interactions},
month = {jun},
pages = {70–73},
numpages = {4}
}

@inproceedings{10.1145/3373419.3373425,
author = {Bobulski, Janusz and Kubanek, Mariusz},
title = {Design Big Data Analysis System - Bigdeepexaminator},
year = {2020},
isbn = {9781450376754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373419.3373425},
doi = {10.1145/3373419.3373425},
abstract = {Big Data is a term used for such data sets, which at the same time are characterized by high volume, di-versity, real-time stream inflow, variability, complexity, as well as require the use of innovative technolo-gies, tools and methods in order to extracting new and useful knowledge from them. Big Data is a new challenge and information possibilities. Correct interpretation of data can play a key role in the global and local economy, social policy and enterprises. We present a data analysis system design with the use of ar-tificial intelligence that will help in obtaining valuable information from big data.},
booktitle = {Proceedings of the 2019 3rd International Conference on Advances in Image Processing},
pages = {110–113},
numpages = {4},
keywords = {intelligent systems, Component, data pre-processing multi-data processing, Big data},
location = {Chengdu, China},
series = {ICAIP 2019}
}

@inproceedings{10.1145/2905055.2905194,
author = {Tamane, Sharvari},
title = {Non-Relational Databases in Big Data},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905194},
doi = {10.1145/2905055.2905194},
abstract = {These days' Big data is becoming a very essential component for the industries where large volume of data at very high speed is used to solve particular data problems. Generally, big data is first analyzed and then used with other available data in the company to make it more effective. Therefore, big data is never operated in isolation. There are a variety of non-relational data stores (databases) available. These data stores and big data can be used in combination to work with. Attributes of these databases are available for companies where big data is used. In last few years it is the requirement of companies that these databases should operate very fast, it should be extended/contracted whenever required and should generate reports quickly. It also requires that the different means should be available to manage and organize these massive databases. This paper mainly focuses on some methods for data management like key-value databases, document databases, tabular databases, object data bases and graph databases. Use of RDBMS for big data implementation is not practical because of its performance, scale or even cost. Now a day's companies have adopted non-relational databases, known as NoSQL databases. Programmers and analysts may take benefit of non-relational databases as it has simple modeling constraints than the relational databases. Analysts can do various types of analysis by taking different types of non-relational databases every time. For example, key value databases, graph databases. The non-relational databases do not depend on the common traditional relational database management systems.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {134},
numpages = {4},
keywords = {Non-Relational Databases, Big Data, NoSQL},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/2658840.2658845,
author = {Neamtu, Rodica and Ahsan, Ramoza and Stokes, Jeff and Hoxha, Armend and Bao, Jialiang and Gvozdenovic, Stefan and Meyer, Ted and Patel, Nilesh and Rangan, Raghu and Wang, Yumou and Zhang, Dongyun and Rundensteiner, Elke A.},
title = {Taming Big Data: Integrating Diverse Public Data Sources for Economic Competitiveness Analytics},
year = {2014},
isbn = {9781450331869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658840.2658845},
doi = {10.1145/2658840.2658845},
abstract = {In an era where Big Data can greatly impact a broad population, many novel opportunities arise, chief among them the ability to integrate data from diverse sources and "wrangle" it to extract novel insights. Conceived as a tool that can help both expert and non-expert users better understand public data, MATTERS was collaboratively developed by the Massachusetts High Tech Council, WPI and other institutions as an analytic platform offering dynamic modeling capabilities. MATTERS is an integrative data source on high fidelity cost and talent competitiveness metrics. Its goal is to extract, integrate and model rich economic, financial, educational and technological information from renowned heterogeneous web data sources ranging from The US Census Bureau, The Bureau of Labor Statistics to the Institute of Education Sciences, all known to be critical factors influencing economic competitiveness of states. This demonstration of MATTERS illustrates how we tackle challenges of data acquisition, cleaning, integration and wrangling into appropriate representations, visualization and story-telling with data in the context of state competitiveness in the high-tech sector.},
booktitle = {Proceedings of the First International Workshop on Bringing the Value of "Big Data" to Users (Data4U 2014)},
pages = {25–28},
numpages = {4},
keywords = {Big data, data integration, diverse data sources},
location = {Hangzhou, China},
series = {Data4U '14}
}

@inproceedings{10.1145/3010089.3017606,
author = {Boubiche, Sabrina and Boubiche, Djallel Eddine and Azzedine, Bilami},
title = {Integrating Big Data Paradigm in WSNs},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3017606},
doi = {10.1145/3010089.3017606},
abstract = {WSNs consist of large number of small sensors densely deployed to monitor a phenomenon. Most of the data generated from the WSNs represent events happening at time intervals. Sometimes and according to the nature of the applications, this data stream is continuous and can reach high speeds. Therefore, adopting new techniques, platforms and tools to deal with this large amount of sensory data became necessary. Therefore, the Big Data paradigm can represent a good solution for the extraction, analysis, viewing, sharing, storage and transfer of such volume of data. This paper presents a survey on integrating Big Data tools for gathering, storing and analyzing the data generated by WSNs.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {56},
numpages = {4},
keywords = {Hadoop, wireless sensor networks, Big data, MapReduce},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/2660168.2660187,
author = {Weyde, Tillman and Cottrell, Stephen and Dykes, Jason and Benetos, Emmanouil and Wolff, Daniel and Tidhar, Dan and Kachkaev, Alexander and Plumbley, Mark and Dixon, Simon and Barthet, Mathieu and Gold, Nicolas and Abdallah, Samer and Alancar-Brayner, Aquiles and Mahey, Mahendra and Tovell, Adam},
title = {Big Data for Musicology},
year = {2014},
isbn = {9781450330022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660168.2660187},
doi = {10.1145/2660168.2660187},
abstract = {Digital music libraries and collections are growing quickly and are increasingly made available for research. We argue that the use of large data collections will enable a better understanding of music performance and music in general, which will benefit areas such as music search and recommendation, music archiving and indexing, music production and education. However, to achieve these goals it is necessary to develop new musicological research methods, to create and adapt the necessary technological infrastructure, and to find ways of working with legal limitations. Most of the necessary basic technologies exist, but they need to be brought together and applied to musicology. We aim to address these challenges in the Digital Music Lab project, and we feel that with suitable methods and technology Big Music Data can provide new opportunities to musicology.},
booktitle = {Proceedings of the 1st International Workshop on Digital Libraries for Musicology},
pages = {1–3},
numpages = {3},
location = {London, United Kingdom},
series = {DLfM '14}
}

@inproceedings{10.1145/2939672.2945369,
author = {Simoudis, Evangelos and Gorenberg, Mark and Guleri, Tim and Ocko, Matt and Sands, Greg},
title = {Big Data Needs Big Dreamers: Lessons from Successful Big Data Investors},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2945369},
doi = {10.1145/2939672.2945369},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {11–12},
numpages = {2},
keywords = {panel},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3410352.3410785,
author = {Moldagulova, Aiman and Satybaldiyeva, Ryskhan and Kuandykov, Abu},
title = {Application of Big Data in Logistics},
year = {2020},
isbn = {9781450377362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410352.3410785},
doi = {10.1145/3410352.3410785},
abstract = {This article discusses the prospect of using big data to optimize logistic processes. An analytical model for the collection and analysis of data from various sources was built. Firstly, the evolution and features of both logistics and big data were have been analyzed using the systematic review method. This was followed by discussions about the implementation of big data in logistics and the results of optimization. The article summarizes the main effects of the use of Big Data in logistics such as informatization; efficiency; quality of service; and promoting technical modernization.},
booktitle = {Proceedings of the 6th International Conference on Engineering &amp; MIS 2020},
articleno = {50},
numpages = {6},
keywords = {Big Data, Data sources, Analytical model, Logistics},
location = {Almaty, Kazakhstan},
series = {ICEMIS'20}
}

@inproceedings{10.1145/2837060.2837076,
author = {Leung, Carson K.},
title = {Big Data Mining Applications and Services},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837076},
doi = {10.1145/2837060.2837076},
abstract = {Data mining and analytics aims to analyze valuable data and extract implicit, previously unknown, and potentially useful information from the data. Due to advances in technology, high volumes of valuable data are generated at a high velocity in high varieties of data sources in various real-life business, scientific and engineering applications. Due to their high volumes, the quality and accuracy of these data depend on their veracity (uncertainty of data). This leads us into the new era of Big Data. This paper presents some works on big data mining and computing, especially on an important task of frequent pattern mining, which computes and mines from big data for interesting knowledge in the forms of frequently occurring sets of merchandise items in shopping markets, interesting co-located events, and/or popular individuals in social networks. The paper also shows how big data mining contributes to real-life applications and services.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {1–8},
numpages = {8},
keywords = {frequent patterns, Data mining},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/3368691.3368741,
author = {Shatnawi, Mohammed Q. and Yassein, Muneer Bani and Abuein, Qusai and Nsuir, Lujain},
title = {Big Data Analytics Tools and Applications: Survey},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368741},
doi = {10.1145/3368691.3368741},
abstract = {Big data term appeared when the data were generated in a huge size. big data is helpful and have many benefits in many applications, it is considered as the upcoming technology in the market. Therefore, many tools developed to analyze this data to benefit from it since it is hard to analyze big data using traditional tools. Because of this, Big Data analytics became one of the most up-to-date topics of research in the last decade. This paper defines big data and mentions its properties, types, and challenges. also, describes big data analytics tools and applications in business, security, health, education, and industry.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {50},
numpages = {4},
keywords = {business analytics, big data, data analysis, analysis tools},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/3386723.3387841,
author = {El Haourani, Lamia and El Kalam, Anas Abou and Ouahman, Abdellah Ait},
title = {Big Data Security and Privacy Techniques},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387841},
doi = {10.1145/3386723.3387841},
abstract = {Nowadays Information technologies have caused irreversible changes in many parts of our society. This development reached its climax with the advent of 'Big Data', whose economic and social benefits should not be neglected. However, 'Big Data' inherently threatens security as well as the founding pillars of personal data law, and for this reason many techniques are invented in parallel with the development of big data.in this article we will present what Big Data is, the challenges it faces in terms of privacy and security, as well as privacy and security techniques for Big Data at the level of anonymization encryption and Differential Privacy, by describing, analyzing, and comparing these methods},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {22},
numpages = {9},
keywords = {techniques, Big Data, Privacy, Security},
location = {Marrakech, Morocco},
series = {NISS2020}
}

@inproceedings{10.1145/2479724.2479730,
author = {Bertot, John Carlo and Choi, Heeyoon},
title = {Big Data and E-Government: Issues, Policies, and Recommendations},
year = {2013},
isbn = {9781450320573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479724.2479730},
doi = {10.1145/2479724.2479730},
abstract = {The promises and potential of Big Data in transforming digital government services, governments, and the interaction between governments, citizens, and the business sector, are substantial. From "smart" government to transformational government, Big Data can foster collaboration; create real-time solutions to challenges in agriculture, health, transportation, and more; and usher in a new era of policy- and decision-making. There are, however, a range of policy challenges to address regarding Big Data, including access and dissemination; digital asset management, archiving and preservation; privacy; and security. This paper selectively reviews and analyzes the U.S. policy context regarding Big Data and offers recommendations aimed at facilitating Big Data initiatives.},
booktitle = {Proceedings of the 14th Annual International Conference on Digital Government Research},
pages = {1–10},
numpages = {10},
keywords = {open government, big data},
location = {Quebec, Canada},
series = {dg.o '13}
}

@inproceedings{10.1145/1985374.1985376,
author = {Shepperd, Martin},
title = {Data Quality: Cinderella at the Software Metrics Ball?},
year = {2011},
isbn = {9781450305938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985374.1985376},
doi = {10.1145/1985374.1985376},
abstract = {In this keynote I explore what exactly do we mean by data quality, techniques to assess data quality and the very significant challenges that poor data quality can pose. I believe we neglect data quality at our peril since - whether we like it or not - our research results are founded upon data and our assumptions that data quality issues do not confound our results. A systematic review of the literature suggests that it is a minority practice to even explicitly discuss data quality. I therefore suggest that this topic should become a higher priority amongst empirical software engineering researchers.},
booktitle = {Proceedings of the 2nd International Workshop on Emerging Trends in Software Metrics},
pages = {1–4},
numpages = {4},
keywords = {data quality, empirical research, software metrics},
location = {Waikiki, Honolulu, HI, USA},
series = {WETSoM '11}
}

@article{10.1145/2592267,
author = {Bean, Jonathan and Rosner, Daniela},
title = {Big Data, Diminished Design?},
year = {2014},
issue_date = {May-June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1072-5520},
url = {https://doi.org/10.1145/2592267},
doi = {10.1145/2592267},
journal = {Interactions},
month = {may},
pages = {18–19},
numpages = {2}
}

@inproceedings{10.1145/3383583.3398507,
author = {Zavalina, Oksana L. and Kim, Jeonghyun and Wang, Xiaoguang and Cheng, Qikai},
title = {Organizing Big Data, Information, and Knowledge},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398507},
doi = {10.1145/3383583.3398507},
abstract = {This virtual workshop organized as part of JCDL 2020 conference serves as continuation of the workshop "Organizing Data, Information, and Knowledge in Big Data Environments" held at the JCDL 2019 conference. The workshop focuses on the challenges and opportunities provided by Big Data environment for information and computing professionals, and explores strategies and solutions for organizing data, information, and knowledge on a large scale.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {579–580},
numpages = {2},
keywords = {knowledge organization, data management, big data, information organization, data analytics},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3209281.3209372,
author = {Chatfield, Akemi Takeoka and Ojo, Adegboyega and Puron-Cid, Gabriel and Reddick, Christopher G.},
title = {Census Big Data Analytics Use: International Cross Case Analysis},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209372},
doi = {10.1145/3209281.3209372},
abstract = {Despite the growing practices in big data and big data analytics use, there is still the paucity of research on links between government big data analytics use and public value creation. This multi-case study of Australia, Ireland, Mexico, and U.S.A. examines the state of big data and big data analytics use in the national census context. The census agencies are at varying stages in digitally transforming their national census process, products and services through assimilating and using big data and big data analytics. The cross-case analysis of government websites and documents identified emerging agency challenges in creating public value in the national census context: (1) big data analytics capability development, (2) cross agency data access and data integration, and (3) data security, privacy &amp; trust. Based on the insights gained, a research model aims to postulate the possible links among challenges, big data/big data analytics use, and public value creation.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {10},
numpages = {10},
keywords = {use, cross case analysis, electronic census, big data challenges, census big data, big data analytics, public value creation},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@article{10.1145/3331651.3331659,
author = {Schilling, Lisa M. and Pena-Jackson, Griselda and Russell, Seth and Corral, Janet and Kwan, Bethany and Ressalam, Julie},
title = {Co-Designing Learning Materials to Empower Laypersons to Better Understand Big Data and Big Data Methods},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/3331651.3331659},
doi = {10.1145/3331651.3331659},
abstract = {University of Colorado Anschutz Medical Campus' Data Science to Patient Value Program and 2040 Partners for Health sought to create open learning materials for engaged citizens and community leaders regarding big data and big data methods to support their collaboration in patient-centered and participatorybased community research and evaluation. 2040 is a local nonprofit organization that cultivates partnerships in Aurora, Colorado neighborhoods to tackle critical health needs. Our goal was to co-design and co-create a series of big data learning modules accessible to community laypeople, so they might better understand big data topics and be empowered more actively engage in health research and evaluation that uses big data methods.},
journal = {SIGKDD Explor. Newsl.},
month = {may},
pages = {41–44},
numpages = {4},
keywords = {education, big data, co-design, community engagement}
}

