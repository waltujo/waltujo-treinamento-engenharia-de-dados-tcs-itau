@inproceedings{10.1145/3415958.3433072,title = {A Meta Learning Approach for Automating Model Selection in Big Data Environments using Microservice and Container Virtualization Technologies}, author = {Shahoud Shadi , Khalloof Hatem , Winter Moritz , Duepmeier Clemens , Hagenmeyer Veit },year = {2020}, isbn = {9781450381154}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3415958.3433072}, doi = {10.1145/3415958.3433072}, abstract = {For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming, since a variety of learning algorithms are proposed in literature and the non-expert users do not know which one to use in order to obtain good performance results. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new generic microservice-based framework for realizing the concept of meta learning in Big Data environments is introduced. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture for a fully manageable and highly scalable solution. In this demonstration and for evaluation purpose, time series model selection is taken into account. The performance and usability of the new framework is evaluated on state-of-the-art machine learning algorithms for time series forecasting: it is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, the recommendation of the most appropriate forecasting model results in a well acceptable low overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in context of Big Data.}, location = {Virtual Event, United Arab Emirates}, series = {MEDES '20}, pages = {84\u201391}, numpages = {8}, keywords = {Machine Learning, Meta Learning, Big Data, Microservice, Web-based Applications}}
@inproceedings{10.1145/2839509.2844631,title = {Combining Big Data and Thick Data Analyses for Understanding Youth Learning Trajectories in a Summer Coding Camp}, author = {Fields Deborah A. , Quirke Lisa , Amely Janell , Maughan Jason },year = {2016}, isbn = {9781450336857}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2839509.2844631}, doi = {10.1145/2839509.2844631}, abstract = {In this paper we explore how to assess novice youths' learning of programming in an open-ended, project-based learning environment. Our goal is to combine analysis of frequent, automated snapshots of programming (e.g., \"big\" data) within the \"thick\" social context of kids? learning for deeper insights into their programming trajectories. This paper focuses on the first stage of this endeavor: the development of exploratory quantitative measures of youths? learning of computer science concepts. Analyses focus on kids? learning in a series of three Scratch Camps where 64 campers aged 10-13 used Scratch 2.0 to make a series of creative projects over 30 hours in five days. In the discussion we consider the highlights of the insights-and blind spots-of each data source with regard to youths' learning.}, location = {Memphis, Tennessee, USA}, series = {SIGCSE '16}, pages = {150\u2013155}, numpages = {6}, keywords = {scratch, assessment, computer science education, constructionism, novice programmers, big data}}
@inproceedings{10.1145/3510858.3510965,title = {Voice Assistance and Big Data Financial Management Based on High-Resolution Imaging Algorithm}, author = {Yao Chunyun },year = {2021}, isbn = {9781450390422}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510858.3510965}, doi = {10.1145/3510858.3510965}, abstract = {With the combination of information technology and economic fields, the amount of data has been greatly increased, and big data has begun to be valued by modern enterprises. As a new IT technology, it has had a huge impact on enterprise management, financial and management models, and business processes. Big data will surely become the basis of enterprise competition and management, and the use of information will have a decisive impact on the operating efficiency of enterprises. Big data sets put forward new requirements for corporate financial management. This article is the research goal of voice assistance and big data financial management based on high-resolution imaging algorithms. This paper establishes the specific process of the speech recognition model and high-resolution imaging algorithm based on the genetic algorithm of big data, and compares the experimental data of this paper with the data obtained from the reference literature and the Internet. Big data puts forward new requirements for financial management. It integrates high-resolution imaging algorithms and voice assistance into financial management based on big data, and studies the academic value and practical application value of financial management based on big data. Combined with actual data practice, it proves the feasibility and practicability of the research direction of this article. According to the experimental research in this article, the voice assistance and big data financial management based on the high-resolution imaging algorithm proposed in this article, adding voice assistance to the financial management can make the financial management run better, and the customers can obtain better data. The changes to the management staff can get management errors in a more timely manner, so that they can be modified in a more timely manner. In the use of genetic algorithms based on big data to optimize speech acquisition and recognition, experimental data shows that the highest recognition rate of optimized speech assistance is 98% close to 100%.}, location = {Changsha, China}, series = {ICASIT 2021}, pages = {356\u2013360}, numpages = {5}}
@inproceedings{10.1145/3365871.3365900,title = {Embracing Opportunities of Livestock Big Data Integration with Privacy Constraints}, author = {Papst Franz , Saukh Olga , R\u00f6mer Kay , Grandl Florian , Jakovljevic Igor , Steininger Franz , Mayerhofer Martin , Duda J\u00fcrgen , Egger-Danner Christa },year = {2019}, isbn = {9781450372077}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3365871.3365900}, doi = {10.1145/3365871.3365900}, abstract = {Today's herd management undergoes a major transformation triggered by the penetration of cheap sensor solutions into cattle farms, and the promise of predictive analytics to detect animal health issues and product-related problems before they occur. The latter is particularly important to prevent disease spread, ensure animal health, animal welfare and product quality. Sensor businesses entering the market tend to build their solutions as end-to-end pipelines spanning sensors, proprietary algorithms, cloud services, and mobile apps. Since data privacy is an important issue in this industry, as a result, disconnected data silos, heterogeneity of APIs, and lack of common standards limit the value the sensor technologies could provide for herd management. In the last few years, researchers and communities proposed a number of data integration architectures to enable exchange between streams of sensor data. This paper surveys the existing efforts and outlines the opportunities they fail to address by treating sensor data as a black box. We discuss alternative solutions to the problem based on privacy-preserving collaborative learning, and provide a set of scenarios to show their benefits for both farmers and businesses.}, location = {Bilbao, Spain}, series = {IoT 2019}, pages = {1\u20134}, numpages = {4}, keywords = {data privacy, privacy-preserving data analysis, agriculture}}
@inproceedings{10.1145/3501409.3501637,title = {Distribution big data technology of active distribution Network based on edge computing}, author = {Zhang Wei , Wang Tianjun , Wang Hao },year = {2021}, isbn = {9781450384322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3501409.3501637}, doi = {10.1145/3501409.3501637}, abstract = {Aiming at the problems of cloud communication and storage congestion and computing delay caused by massive heterogeneous distribution data, a hierarchical architecture model of active distribution network based on edge computing was proposed. Firstly, the edge computing framework based on the functional architecture of industrial Internet is proposed, and the internal and external interaction modes of edge computing node data are specifically sorted out. According to the established data interaction modes, the interactive processing mechanism of cloud-edge collaboration is proposed. Then, according to the logical protocol and physical architecture of active distribution network, the hierarchical architecture model of active distribution network based on edge computing is established to collect, interact and monitor the operating status, operating environment and electricity quantity data of distribution equipment. Finally, based on big data technology, the typical application scenarios of edge computing technology in actual power distribution are analyzed, and the efficiency, real-time, security and accuracy of edge computation-based power distribution big data system for local data storage and processing are verified.}, location = {Xiamen, China}, series = {EITCE 2021}, pages = {1292\u20131296}, numpages = {5}, keywords = {Large data distribution data, Industrial Iot network, Information physical system, Edge calculation, Data interchange, Cloud edge coordination, Main power distribution network}}
@inproceedings{10.1145/2902961.2902984,title = {Low Energy Sketching Engines on Many-Core Platform for Big Data Acceleration}, author = {Kulkarni Amey , Abtahi Tahmid , Smith Emily , Mohsenin Tinoosh },year = {2016}, isbn = {9781450342742}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2902961.2902984}, doi = {10.1145/2902961.2902984}, abstract = {Almost 90% of the data available today was created within the last couple of years, thus Big Data set processing is of utmost importance. Many solutions have been investigated to increase processing speed and memory capacity, however I/O bottleneck is still a critical issue. To tackle this issue we adopt Sketching technique to reduce data communications. Reconstruction of the sketched matrix is performed using Orthogonal Matching Pursuit (OMP). Additionally we propose Gradient Descent OMP (GD-OMP) algorithm to reduce hardware complexity. Big data processing at real-time imposes rigid constraints on sketching kernel, hence to further reduce hardware overhead both algorithms are implemented on a low power domain specific many-core platform called Power Efficient Nano Clusters (PENC). GD-OMP algorithm is evaluated for image reconstruction accuracy and the PENC many-core architecture. Implementation results show that for large matrix sizes GD-OMP algorithm is 1.3x faster and consumes 1.4x less energy than OMP algorithm implementations. Compared to GPU and Quad-Core CPU implementations the PENC many-core reconstructs 5.4x and 9.8x faster respectively for large signal sizes with higher sparsity.}, location = {Boston, Massachusetts, USA}, series = {GLSVLSI '16}, pages = {57\u201362}, numpages = {6}, keywords = {OMP, compressive sensing, high performance and reconfigurable architecture, many-core}}
@inproceedings{10.1145/3433539,title = {Research On Pre-Training Method and Generalization Ability of Big Data Recognition Model of the Internet of Things}, author = {Tan Junyang , Xia Dan , Dong Shiyun , Zhu Honghao , Xu Binshi },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3433539}, doi = {10.1145/3433539}, abstract = {The Internet of Things and big data are currently hot concepts and research fields. The mining, classification, and recognition of big data in the Internet of Things system are the key links that are widely of concern at present. The artificial neural network is beneficial for multi-dimensional data classification and recognition because of its strong feature extraction and self-learning ability. Pre-training is an effective method to address the gradient diffusion problem in deep neural networks and could result in better generalization. This article focuses on the performance of supervised pre-training that uses labelled data. In particular, this pre-training procedure is a simulation that shows the changes in judgment patterns as they progress from primary to mature within the human brain. In this article, the state-of-the-art of neural network pre-training is reviewed. Then, the principles of the auto-encoder and supervised pre-training are introduced in detail. Furthermore, an extended structure of supervised pre-training is proposed. A set of experiments are carried out to compare the performances of different pre-training methods. These experiments include a comparison between the original and pre-trained networks as well as a comparison between the networks with two types of sub-network structures. In addition, a homemade database is established to analyze the influence of pre-training on the generalization ability of neural networks. Finally, an ordinary convolutional neural network is used to verify the applicability of supervised pre-training.}, pages = {1\u201315}, numpages = {15}, keywords = {Big data, generalization, neural network, pre-training procedure, convergence}}
@inproceedings{10.1145/2500468.2500471,title = {Big data is 'buzzword du jour;' CS academics 'have the best job'}, author = {Stonebraker Michael , Robertson Judy },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2500468.2500471}, doi = {10.1145/2500468.2500471}, abstract = {The Communications Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.twitterFollow us on Twitter at http://twitter.com/blogCACMhttp://cacm.acm.org/blogs/blog-cacmMichael Stonebraker analyzes the different varieties of Big Data, while Judy Robertson considers the rewards of teaching computer science.}, pages = {10\u201311}, numpages = {2}}
@inproceedings{10.1145/2669368,title = {A Case Study of Data Quality in Text Mining Clinical Progress Notes}, author = {Berndt Donald J. , McCart James A. , Finch Dezon K. , Luther Stephen L. },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2669368}, doi = {10.1145/2669368}, abstract = {Text analytic methods are often aimed at extracting useful information from the vast array of unstructured, free format text documents that are created by almost all organizational processes. The success of any text mining application rests on the quality of the underlying data being analyzed, including both predictive features and outcome labels. In this case study, some focused experiments regarding data quality are used to assess the robustness of Statistical Text Mining (STM) algorithms when applied to clinical progress notes. In particular, the experiments consider the impacts of task complexity (by removing signals), training set size, and target outcome quality. While this research is conducted using a dataset drawn from the medical domain, the data quality issues explored are of more general interest.}, pages = {1\u201321}, numpages = {21}, keywords = {feature selection, data quality, text mining, health informatics, predictive model quality, clinical progress notes, electronic health records, noisy text analysis, Machine learning}}
@inproceedings{10.1145/2600821.2600841,title = {Initial evaluation of data quality in a TSP software engineering project data repository}, author = {Shirai Yasutaka , Nichols William , Kasunic Mark },year = {2014}, isbn = {9781450327541}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2600821.2600841}, doi = {10.1145/2600821.2600841}, abstract = {To meet critical business challenges, software development teams need data to effectively manage product quality, cost, and schedule. The Team Software ProcessSM (TSPSM) provides a framework that teams use to collect software process data in real time, using a defined disciplined process. This data holds promise for use in software engineering research. We combined data from 109 industrial projects into a database to support performance benchmarking and model development. But is the data of sufficient quality to draw conclusions? We applied various tests and techniques to identify data anomalies that affect the quality of the data in several dimensions. In this paper, we report some initial results of our analysis, describing the amount and the rates of identified anomalies and suspect data, including incorrectness, inconsistency, and credibility. To illustrate the types of data available for analysis, we provide three examples. The preliminary results of this empirical study suggest that some aspects of the data quality are good and the data are generally credible, but size data are often missing.}, location = {Nanjing, China}, series = {ICSSP 2014}, pages = {25\u201329}, numpages = {5}, keywords = {Database, TSP, Team Software Process, Data Quality}}
@inproceedings{10.1145/3268808.3268832,title = {Tax Reduction and Corporate Investment - Applying Big Data to Tax Policy Formulation}, author = {Lu Ming-Che , Chen Yi-Xuan , Yang Yu-Ying , Sha Min-Xuan , Chen Yan-Wei , Lin Sih-Ling },year = {2018}, isbn = {9781450365284}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3268808.3268832}, doi = {10.1145/3268808.3268832}, abstract = {In this paper, we apply big data to the tax reform evaluation for the Ministry of Finance of the Republic of China. From empirical data, we find strong evidences that the reduction of profit-seeking enterprise income tax rate indeed improves corporate investment. Some sensitivity tests are performed to ensure our results robust.}, location = {Taipei, Taiwan}, series = {ICSET 2018}, pages = {103\u2013106}, numpages = {4}, keywords = {Government policy, Profit-seeking enterprise income tax, Corporate investment}}
@inproceedings{10.1145/3007818.3007824,title = {Big data compression paradigms for supporting efficient and scalable data-intensive IoT frameworks}, author = {Cuzzocrea Alfredo },year = {2016}, isbn = {9781450347549}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3007818.3007824}, doi = {10.1145/3007818.3007824}, abstract = {In this paper we focus on big data compression paradigms within reference data-intensive IoT frameworks, which are currently recognized as one of the emerging scientific in a rich interdisciplinary field that comprises service-oriented infrastructures, Cloud computing, big data management and analytics. Basically, big data compression techniques allow to tame the complexity of big data management tasks within such frameworks, hence beneficially influencing all the other activities, perhaps delivered as services in a reference Cloud architecture. Inspired by these considerations, in this paper we provide an overview on noticeable state-of-the-art big data compression techniques, and depict future research directions on the investigated scientific topic to be considered during future years.}, location = {Jeju, Republic of Korea}, series = {EDB '16}, pages = {67\u201371}, numpages = {5}}
@inproceedings{10.1145/2979779.2979844,title = {Identification and ranking of key persons in a Social Networking Website using Hadoop & Big Data Analytics}, author = {Agarwal Prerna , Ahmed Rafeeq , Ahmad Tanvir },year = {2016}, isbn = {9781450342131}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2979779.2979844}, doi = {10.1145/2979779.2979844}, abstract = {Big Data is a term which defines a vast amount of structured and unstructured data which is challenging to process because of its large size, using traditional algorithms and lack of high speed processing techniques. Now a days, vast amount of digital data is being gathered from many important areas, including social networking websites like Facebook and Twitter. It is important for us to mine this big data for analysis purpose. One important analysis in this domain is to find key nodes in a social graph which can be the major information spreader. Node centrality measures can be used in many graph applications such as searching and ranking of nodes. Traditional centrality algorithms fail on such huge graphs therefore it is difficult to use these algorithms on big graphs. Traditional centrality algorithms such as degree centrality, betweenness centrality and closeness centrality were not designed for such large data. In this paper, we calculate centrality measures for big graphs having huge number of edges and nodes by parallelizing traditional centrality algorithms so that they can be used in an efficient way when the size of graph grows. We use MapReduce and Hadoop to implement these algorithms for parallel and distributed data processing. We present results and anomalies of these algorithms and also show the comparative processing time taken on normal systems and on Hadoop systems.}, location = {Bikaner, India}, series = {AICTC '16}, pages = {1\u20136}, numpages = {6}, keywords = {key persons, Degree Centrality Big Data, Betweenness Centrality, MapReduce, Closeness Centrality, ranking}}
@inproceedings{10.1145/3482632.3483004,title = {Application of Computer Big Data Technology in the Teaching of Interpretation and Translation}, author = {Xiong Huiqin },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483004}, doi = {10.1145/3482632.3483004}, abstract = {With the era of progress and development of technology, and the popularization of computer technology, Internet users are also increasing day by day, resulting in massive amounts of data. Meanwhile, big data technology is gradually improving and achieving a wide range of applications. Big data technology can extract the required effective data from a large amount of messy data and perform statistical analysis. Thereby improving teaching efficiency. The purpose of this article is to study the application of big data technology (ABDT) in the teaching of interpretation and translation (IAT). This article secondly describes the ABDT in the teaching of IAT, and collects and analyzes the teaching data of IAT through big data technology. This article analyzes various difficulties encountered in practical application of oral and written translation teaching through practical research, including difficulties in translation skills, cognitive difficulties in the process of note-taking, and difficulties in lack of professional vocabulary. Finally, according to various application difficulties, relevant coping strategies are proposed to improve the quality of IAT teaching. The experimental research results show that before applying big data technology in teaching, the teaching effects of various indicators are 71.48%, 73.19% and 65.94% respectively. After application, the teaching effects of various indicators are 85.61%, 81.52% and 86.37 respectively. %. From the perspective of the whole day, after applying big data, the teaching effect has been significantly improved.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {726\u2013730}, numpages = {5}}
@inproceedings{10.1145/3510858.3511399,title = {Research on Financial Management Algorithm Based on Machine Learning in Big Data Era}, author = {Li Meifu },year = {2021}, isbn = {9781450390422}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510858.3511399}, doi = {10.1145/3510858.3511399}, abstract = {Based on the development of big data technology, the application of machine intelligence and intelligent computer has gradually changed the work and life style of traditional society, and brought great progress and changes to human beings and society. Effective use of cutting-edge big data and artificial intelligence related technologies can provide effective advice for future financial management and solve possible problems in financial management. By analyzing the influence of artificial intelligence and big data on financial management of companies, this paper puts forward a financial management algorithm based on machine learning, hoping to have a positive impact on financial management and financial reform of enterprises. So as to build a financial management information data management system, and better integrate big data, artificial intelligence technology and enterprise financial management. Under the background of the new era, the financial management of companies should improve the efficiency of financial management through the management and budget of funds related to education management and a series of information audits.}, location = {Changsha, China}, series = {ICASIT 2021}, pages = {825\u2013828}, numpages = {4}}
@inproceedings{10.1145/3482632.3487506,title = {Application and Research of Computer Big Data Based on Structure in Internet Learning}, author = {Zhang Huiru , Liu Ruixiao },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3487506}, doi = {10.1145/3482632.3487506}, abstract = {With the advent of the era of big data, many teachers began to adopt online learning teaching methods, and many enterprises and schools also developed a variety of learning systems, examination systems and live broadcast systems, which broke away from the traditional teaching mode and established a brand-new learning system. The application of big data in the education industry is very prominent. Adding Internet thinking in the education process can enable students to learn relevant knowledge online after class, consolidate knowledge and exercise students' self-learning ability. As the core component of the computer application field, online teaching has outstanding advantages. Learners can talk with famous teachers anytime and anywhere, work out their own learning plans, and eliminate knowledge blind spots. This paper mainly discusses the application of computer big data in Internet learning, and analyzes the application process of computer big data in Internet learning based on the structure level, aiming at using Internet technology to achieve efficient learning.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {2741\u20132745}, numpages = {5}}
@inproceedings{10.1145/3456887.3457115,title = {Party Building Innovation in Judicial Professional Colleges in the Context of Big Data}, author = {Li Jimin },year = {2021}, isbn = {9781450389969}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456887.3457115}, doi = {10.1145/3456887.3457115}, abstract = {In the context of big data, judicial vocational colleges must respond to the trends of society and conduct innovative party building research based on the context of big data. This paper looks at the problems of judicial professional schools in their party building and innovation efforts to analyze their Influencing factors and giving strategies for solving party building innovation work in the context of big data.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2021}, pages = {962\u2013964}, numpages = {3}}
@inproceedings{10.1145/3495018.3501162,title = {Analysis of blizzard process based on big data numerical simulation}, author = {Zhu Xiande , Wu Baoqin , Liu Yuhong , Wang Chuan , Wu Yunfan },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3501162}, doi = {10.1145/3495018.3501162}, abstract = {On 3 \u223c 4 Decree 2005 a Quasi-stationary confidential cloud band which was located in cost of Yantai and Weihai resulted in a severe cold-air outbreak snow storm and rough double prevention centers: Zhao yuan and Rong cheng. Using the radar datas and the routine observations as well as the results of simulation by mesoscale model-MM5, we study and research this severe c Old-air outbreak snow storm event. The results show that the snow band maintained approximately 12h and its height reached to 500hPa in vertical at yantai. This snow storm event generated owing to the cooperation of the fashionable synchronous circuit and both thermodynamic forging caused by the warm Bohai-sea surface and dynamic forging by the topography. The coast Al front generated owing to the different temperature between Bohai Sea and land and the ageostrophic wind, which brings an obvious increment of local snow fall and whose secondary circulation decide the distribution of snowfall. Meanwhile the weak land break which formatted in night is significant for the formation and maintenance of the selective cloud band.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {2674\u20132682}, numpages = {9}}
@inproceedings{10.1145/2875475.2875489,title = {How Can We Enable Privacy in an Age of Big Data Analytics?}, author = {Landwehr Carl E. },year = {2016}, isbn = {9781450340779}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2875475.2875489}, doi = {10.1145/2875475.2875489}, abstract = {Even though some seem to think privacy is dead, we are all still wearing clothes, as Bruce Schneier observed at a recent conference on surveillance[1]. Yet big data and big data analytics are leaving some of us feeling a bit more naked than before. This talk will provide some personal observations on privacy today and then outline some research areas where progress is needed to enable society to gain the benefits of analyzing large datasets without giving up more privacy than necessary. Not since the early 1970s, when computing pioneer Willis Ware chaired the committee that produced the initial Fair Information Practice Principles [2] has privacy been so much in the U.S. public eye. Snowden's revelations, as well as a growing awareness that merely living our lives seems to generate an expanding \"digital exhaust.\" Have triggered many workshops and meetings. A national strategy for privacy research is in preparation by a Federal interagency group. The ability to analyze large datasets rapidly and to extract commercially useful insights from them is spawning new industries. Must this industrial growth come at the cost of substantial privacy intrusions?}, location = {New Orleans, Louisiana, USA}, series = {IWSPA '16}, pages = {47}, numpages = {1}, keywords = {privacy, fair information practice principles, surveillance}}
@inproceedings{10.1145/3495018.3501058,title = {High Speed Digital Signal Processing System Based on Big Data}, author = {Guo Jiayuan },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3501058}, doi = {10.1145/3495018.3501058}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {2098\u20132101}, numpages = {4}}
@inproceedings{10.1145/3209914.3226157,title = {Study on Recommend Model of Online Shopping for Music and Dance majors under the Background of Big Data}, author = {Zou Mi },year = {2018}, isbn = {9781450364218}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3209914.3226157}, doi = {10.1145/3209914.3226157}, abstract = {With the rapid development of Internet technology and e-commerce, online shopping has become the main mode of consumption nowadays. Music and dance majors have become the main group of online shopping because of their particularity. Based on the theory of big data and Bayesian networks, this paper collects data of college students' online shopping apparel by means of questionnaire survey and builds the style recommendation model and the clothing's main color recommendation model of online shopping.}, location = {Jeju, Republic of Korea}, series = {ICISS '18}, pages = {98\u2013101}, numpages = {4}, keywords = {Music and dance majors, Online shopping, Big data, Bayesian networks}}
@inproceedings{10.1145/3195612.3195621,title = {Exploration of in-memory computing for big data analytics using queuing theory}, author = {Srivastava Riktesh },year = {2018}, isbn = {9781450363372}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3195612.3195621}, doi = {10.1145/3195612.3195621}, abstract = {Assigning suitable memory chunk for Big Data analysis is posing serious problems for Business Analysts. There are plentiful solutions that came along to solve the issue of memory management. The noteworthy solutions to the problems included JVM based and Container based solutions. However, both of these solutions suffered from disk I/O bottleneck. To reduce disk, I/O bottleneck, in-memory system was introduced, which supports interactive data analytics. Present study conducts request time processing for in-memory system using three types of queue models- MG1, GM1 and GG1.}, location = {Hong Kong, Hong Kong}, series = {HP3C}, pages = {11\u201316}, numpages = {6}, keywords = {in-memory computing (IMC), G/M/1 queue, M/M/1 queue, G/G/1 queue, M/G/1 queue}}
@inproceedings{10.1145/3131085.3131124,title = {Big data visualization as an auxiliary tool in designing a distribution network of a company}, author = {Kozlov Stanislav },year = {2017}, isbn = {9781450354264}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3131085.3131124}, doi = {10.1145/3131085.3131124}, abstract = {This paper offers an original way of improving the interpretation of the results of designing a distribution network via a set of various visualization techniques. First, using clustering algorithms allows us to automatically sort through a indeterminate number of warehouses and offer the recommended number of warehouses (clusters) depending on some incoming parameters. Visualization of the results of the algorithm can be illustrated on a geographical map as a model of distribution network. Second, using interpolation algorithms allows us to visualize the problem areas of a region in a model of previously designed network. Such visualizations provide a deep understanding of the properties of a distribution network.}, location = {Tampere, Finland}, series = {AcademicMindtrek '17}, pages = {247\u2013250}, numpages = {4}, keywords = {clustering, express delivery, visualization, distribution network, warehouse coordinates, big data}}
@inproceedings{10.1145/2933267.2933539,title = {Taming velocity and variety simultaneously in big data with stream reasoning: tutorial}, author = {Della Valle Emanuele , Dell'Aglio Daniele , Margara Alessandro },year = {2016}, isbn = {9781450340212}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2933267.2933539}, doi = {10.1145/2933267.2933539}, abstract = {Many \"big data\" applications must tame velocity (processing data in-motion) and variety (processing many different types of data) simultaneously.The research on knowledge representation and reasoning has focused on the variety of data, devising data representation and processing techniques that promote integration and reasoning on available data to extract implicit information. On the other hand, the event and stream processing community has focused on the velocity of data, producing systems that efficiently operate on streams of data on-the-fly according to pre-deployed processing rules or queries. Several recent works explore the synergy between stream processing and reasoning to fully capture the requirements of modern data intensive applications, thus giving birth to the research domain of stream reasoning.This tutorial paper offers an overview of the theoretical and technological achievements in stream reasoning, highlighting the key benefits and limitations of existing approaches, and discussing the open challenges and the opportunities for future research. The paper mainly targets researchers and practitioners in the area of event and stream processing. The paper aims to stimulate the discussion on stream reasoning and to further promote the integration of reasoning techniques within event and stream processing systems in three ways: (i) by presenting an active research domain, where researchers on event and stream processing can apply their expertise; (ii) by discussing techniques and technologies that can help advancing the state of the art in event and stream processing; (iii) by identifying the open problems in the field of stream reasoning, and drawing attention to promising research directions.}, location = {Irvine, California}, series = {DEBS '16}, pages = {394\u2013401}, numpages = {8}, keywords = {stream processing, event processing, reasoning, stream reasoning, complex event processing}}
@inproceedings{10.1145/3209914.3234639,title = {Application of Big Data and Intelligent Processing Technology in Modern Chinese Multi-category Words Part of Speech Tagging Corpus}, author = {Song Zhendong },year = {2018}, isbn = {9781450364218}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3209914.3234639}, doi = {10.1145/3209914.3234639}, abstract = {The application of modern Chinese multi-category words corpus is very wide. With the development of the Internet, data from the corpus is getting bigger and bigger during collection. The data gradually develops so big that the current relational database is difficult to deal with them. This article analyzes the important role of the big data technology in corpu}, location = {Jeju, Republic of Korea}, series = {ICISS '18}, pages = {107\u2013111}, numpages = {5}, keywords = {Multi-category words, Big data, Corpus, Tagging, Intelligent processing}}
@inproceedings{10.14778/2536222.2536243,title = {Designing query optimizers for big data problems of the future}, author = {Tran Nga , Bodagala Sreenath , Dave Jaimin },year = {2013}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2536222.2536243}, doi = {10.14778/2536222.2536243}, abstract = {The Vertica SQL Query Optimizer was written from the ground up for the Vertica Analytic Database. Its design, and the tradeoffs we encountered during implementation, support the case that the full power of novel database systems can be realized only with a custom Query Optimizer, carefully crafted exclusively for the system in which it operates.}, pages = {1168\u20131169}, numpages = {2}}
@inproceedings{10.5555/3195638.3195649,title = {Bridging the I/O performance gap for big data workloads: a new NVDIMM-based approach}, author = {Chen Renhai , Shao Zili , Li Tao },year = {2016}, publisher = {IEEE Press}, abstract = {The long I/O latency posts significant challenges for many data-intensive applications, such as the emerging big data workloads. Recently, the NVDIMM (Non-Volatile Dual In-line Memory Module) technologies provide a promising solution to this problem. By employing non-volatile NAND flash memory as storage media and connecting them via DIMM (Dual Inline Memory Module) slots, the NVDIMM devices are exposed to memory bus so the access latencies due to going through I/O controllers can be significantly mitigated. However, placing NVDIMM on the memory bus introduces new challenges. For instance, by mixing I/O and memory traffic, NVDIMM can cause severe performance degradation on memory-intensive applications. Besides, there exists a speed mismatch between fast memory access and slow flash read/write operations. Moreover, garbage collection (GC) in NAND flash may cause up to several millisecond latency.This paper presents novel, enabling mechanisms that allow NVDIMM to more effectively bridge the I/O performance gap for big data workloads. To address the workload heterogeneity challenge, we develop a scheduling scheme in memory controller to minimize the interference between the native and the I/O-derived memory traffic by exploiting both data access criticality and resource utilization. For NVDIMM controller, several mechanisms are designed to better orchestrate traffic between the memory controller and NAND flash to alleviate the speed discrepancy issue. To mitigate the lengthy GC period, we propose a proactive GC scheme for the NVDIMM controller and flash controller to intelligently synchronize and transfer data involving in forthcoming GC operations. We present detailed evaluation and analysis to quantify how well our techniques fit with the NVDIMM design. Our experimental results show that overall the proposed techniques yield 10%~35% performance improvements over the state-of-the-art baseline schemes.}, location = {Taipei, Taiwan}, series = {MICRO-49}, pages = {1\u201312}, numpages = {12}}
@inproceedings{10.1145/3167486.3167506,title = {Integrating Big Data technologies in a dynamic environment EIAH dedicated to e-learning systems based on cloud infrastructure}, author = {Dahdouh K. , Dakkak Ahmed , Oughdir Lahcen },year = {2017}, isbn = {9781450353069}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3167486.3167506}, doi = {10.1145/3167486.3167506}, abstract = {Online learning has experienced a lot of change, largely as a result of a number of technological Innovations. Newer e-learning (online learning) authoring tools have been developed, and more sophisticated Learning Management Systems (LMSs) have been deployed. Nowadays, e-learning systems have evolved exponentially. The emergence of new information and communication technologies, also with the development of new models of learning and new pedagogical concepts, which causes many problems such as the gigantic volume of data generated by e-learning platform, the large number of learners and the diversity of educational content. In this context, Big Data is a promising paradigm because of its permanent scalability and opportunities that offer to e-learning professionals in terms of data collection, storage, analysis, processing, optimization and representation of data. This article presents the Big Data concept, its characteristics, and focuses in particular on the integration of it in a dynamic environment dedicated to e-learning systems, and how Big Data impacts the future of e-learning. Moreover, it proposes a new approach for E-learning systems based on big data technologies in a cloud infrastructure. Furthermore this work explores the benefits and advantages of Big Data for e-learning professionals and how Big Data can improve the online learning experience.}, location = {Larache, Morocco}, series = {ICCWCS'17}, pages = {1\u20137}, numpages = {7}, keywords = {Cassandra, Online learning, E-learning, Big Data, NoSQL databases, mongoDB, Cloud computing, Learner, Learning Management Systems (LMS), Spark, Hadoop, MapReduce}}
@inproceedings{10.1145/3299869.3328524,title = {Formal Approaches to Querying Big Data in Shared-Nothing Systems}, author = {Ketsman Bas },year = {2019}, isbn = {9781450356435}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3299869.3328524}, doi = {10.1145/3299869.3328524}, abstract = {To meet today's data management needs, it is a widespread practice to use distributed data storage and processing systems. Since the publication of the MapReduce paradigm, a plethora of such systems arose, but although widespread, the capabilities of these systems are still poorly understood and putting them to effective use is often more of an art than a science. As one of the causes for this observation, we identify a lack of theoretical underpinnings for these systems, which makes it hard to understand what the advantages and disadvantages of the particular systems are and which, in addition, complicates the choice of a particular formalism for a particular task. In my PhD thesis, we zoom in on several important aspects of query evaluation using clusters of servers, including coordination and communication, data-skew, load balancing, and data partitioning, and propose a set of elegant and theoretically sound frameworks and theories that help to understand the applicable limitations and trade-offs.}, location = {Amsterdam, Netherlands}, series = {SIGMOD '19}, pages = {1115\u20131116}, numpages = {2}, keywords = {coordination, distributed query evaluation, shared-nothing systems, communication, worst-case optimality}}
@inproceedings{10.1145/3544109.3544145,title = {Research Review of Cloud Computing Technology Based on Big Data}, author = {Li Bo },year = {2022}, isbn = {9781450395786}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544109.3544145}, doi = {10.1145/3544109.3544145}, location = {Dalian, China}, series = {IPEC '22}, pages = {198\u2013201}, numpages = {4}}
@inproceedings{10.1145/2976749.2978378,title = {High Fidelity Data Reduction for Big Data Security Dependency Analyses}, author = {Xu Zhang , Wu Zhenyu , Li Zhichun , Jee Kangkook , Rhee Junghwan , Xiao Xusheng , Xu Fengyuan , Wang Haining , Jiang Guofei },year = {2016}, isbn = {9781450341394}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2976749.2978378}, doi = {10.1145/2976749.2978378}, abstract = {Intrusive multi-step attacks, such as Advanced Persistent Threat (APT) attacks, have plagued enterprises with significant financial losses and are the top reason for enterprises to increase their security budgets. Since these attacks are sophisticated and stealthy, they can remain undetected for years if individual steps are buried in background \"noise.\" Thus, enterprises are seeking solutions to \"connect the suspicious dots\" across multiple activities. This requires ubiquitous system auditing for long periods of time, which in turn causes overwhelmingly large amount of system audit events. Given a limited system budget, how to efficiently handle ever-increasing system audit logs is a great challenge. This paper proposes a new approach that exploits the dependency among system events to reduce the number of log entries while still supporting high-quality forensic analysis. In particular, we first propose an aggregation algorithm that preserves the dependency of events during data reduction to ensure the high quality of forensic analysis. Then we propose an aggressive reduction algorithm and exploit domain knowledge for further data reduction. To validate the efficacy of our proposed approach, we conduct a comprehensive evaluation on real-world auditing systems using log traces of more than one month. Our evaluation results demonstrate that our approach can significantly reduce the size of system logs and improve the efficiency of forensic analysis without losing accuracy.}, location = {Vienna, Austria}, series = {CCS '16}, pages = {504\u2013516}, numpages = {13}, keywords = {dependency analysis, data reduction, intrusion detection, forensics}}
@inproceedings{10.1145/3477314.3507234,title = {5D-IoT, a semantic web based framework for assessing IoT data quality}, author = {Mante Shubham , Hernandez Nathalie , Hussain Aftab M , Chaudhari Sachin , Gangadharan Deepak , Monteil Thierry },year = {2022}, isbn = {9781450387132}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3477314.3507234}, doi = {10.1145/3477314.3507234}, abstract = {Due to the increasing number of Internet of Things (IoT) devices, a large amount of data is being generated. However, factors such as hardware malfunctions, network failures, or cyber-attacks affect data quality and result in inaccurate data generation. Therefore, to facilitate the data usage, we propose a novel 5D-IoT framework for heterogeneous IoT systems that provides uniform data quality assessment with meaningful data descriptions. Based on the quality assessment result, a data consumer can directly access data from any IoT source, which ultimately speeds up the analysis process and helps gain important insights in less time. The framework relies on semantic descriptions of sensor observations and SHACL shapes assessing the quality of such data. Evaluations carried out on real-time data show the added value of such a framework.}, location = {Virtual Event}, series = {SAC '22}, pages = {1921\u20131924}, numpages = {4}, keywords = {IoT data quality assessment, semantic web of things, SPARQL, SHACL}}
@inproceedings{10.1145/3236644.3236649,title = {IEEE big data 2017 panel discussion on bias and transparency}, author = {Maurya Abhinav },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3236644.3236649}, doi = {10.1145/3236644.3236649}, abstract = {In January 2017, the ACM US Public Policy Council released a report on algorithmic transparency and accountability (ACM US Public Policy Council, 2017) which outlined several characteristics for algorithms to be considered transparent and accountable:}, pages = {13\u201320}, numpages = {8}}
@inproceedings{10.1145/2818869.2818906,title = {Mining Massive Web Log Data of an Official Tourism Web Site as a Step towards Big Data Analysis in Tourism}, author = {Yung Chung },year = {2015}, isbn = {9781450337359}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2818869.2818906}, doi = {10.1145/2818869.2818906}, abstract = {The focus of this paper is on the conceptual and technical solution design to the analysis of massive web log data of an official tourism web site when integrated with web mining and big data technology. With the rapid development of Internet and World Wide Web, web log becomes one the fastest growing user generated contents, and web log mining plays an important role in many fields, such as personalized information service, design and service improvement of web sites. The underlying technology for analyzing massive web log data includes web log mining and big data analysis. In this paper, we give a comprehensive overview at the underlying technology, and then we propose an open architecture of big data solution design in tourism with mining the massive web log data. We include the discussion on the difficulties in implementing the proposed architecture as a conclusion.}, location = {Kaohsiung, Taiwan}, series = {ASE BD&amp;SI '15}, pages = {1\u20134}, numpages = {4}, keywords = {big data analysis, big data in tourism, Web log mining}}
@inproceedings{10.1145/2909493,title = {Should you upload or ship big data to the cloud?}, author = {Date Sachin },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2909493}, doi = {10.1145/2909493}, abstract = {The accepted wisdom does not always hold true.}, pages = {44\u201351}, numpages = {8}}
@inproceedings{10.1145/3456887.3459716,title = {Construction of Small and Medium-sized Enterprises' Financial Strategy System Based on Big Data in Low Carbon Economy}, author = {Sun Meimei , Wei Congcong },year = {2021}, isbn = {9781450389969}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456887.3459716}, doi = {10.1145/3456887.3459716}, abstract = {The arrival of low-carbon economy has caused great changes in the production and operation environment of enterprises. With the changes of social, economic, financial and legal environment, the financial supervision measures of enterprises need to be adjusted. The arrival of big data era has forced the competition among SMEs to increase, and effective financial internal control can provide necessary guarantee for SMEs to achieve their business development goals. With the rapid economic renewal and social progress, financial management is very important for an enterprise, which holds the lifeblood of an enterprise. Based on this, in the demand of low-carbon development, enterprise operation needs scientific management. Enterprises affected by the coordinated development of economy and environment pay great attention to environmental protection, and improve the measures of resource and environmental protection, so as to complete the task of overall development of enterprises. This paper discusses the internal financial management problems of small and medium-sized enterprises in low-carbon development, effectively improves the internal financial control based on big data technology, and puts forward the development path to promote the healthy and stable progress of small and medium-sized enterprises.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2021}, pages = {1535\u20131538}, numpages = {4}, keywords = {Low carbon economy, Financial management, Big data, Small and medium-sized enterprises}}
@inproceedings{10.1145/3543106.3543107,title = {Problems and Countermeasures of the E-commerce for the Agricultural Product in the Conditions of the Big Data in China}, author = {Li Zhihong , Wu Dingbang },year = {2022}, isbn = {9781450397162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3543106.3543107}, doi = {10.1145/3543106.3543107}, abstract = {The agriculture industry is a quite important component of the national economy in China. The e-commerce of the agricultural product has been quickly developed with the internet, particularly the big data emerging. However, some problems appear while the fast growth of the agricultural e-commerce is flourishing the economy and improving the people's life. They are restricting the further growth of the agricultural e-commerce. How to deal with these problems is worthwhile to be considered. So, the paper first exposes the real state of the agricultural e-commerce in the conditions of the big data, then finds out some key problems constraining the further growth of the e-commerce of the agricultural item under the big data by comparing the existent suppliers and e-commerce platforms in the agricultural e-commerce industrial chain, finally proposes some countermeasures to solve the problems.}, location = {Seoul, Republic of Korea}, series = {ICEMC '22}, pages = {1\u20135}, numpages = {5}, keywords = {big data, countermeasures, agricultural products, E-commerce problems}}
@inproceedings{10.1145/2661020.2661028,title = {On the Locality of Java 8 Streams in Real-Time Big Data Applications}, author = {Chan Yu , Wellings Andy , Gray Ian , Audsley Neil },year = {2014}, isbn = {9781450328135}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2661020.2661028}, doi = {10.1145/2661020.2661028}, abstract = {Typical Big Data frameworks do not consider the architecture of the servers that make up the cluster. However, these computers are increasingly heterogeneous and are based on a ccNUMA architecture. In such architectures, main memory access times differ depending on the core on which access is requested. Hence, as well as locality of data access throughout a cluster of servers, locality of memory access within individual servers can have an impact on performance.Java is a commonly-used language for Big Data applications (through the popularity of Hadoop) and the newly-released Java 8 introduces streams to simplify data-parallel programming. However, this paper argues that there are no built-in parallel stream sources that can efficiently operate on very large datasets and take data locality into account. This paper details recent work from the JUNIPER project, an EU Framework 7 Project, which is investigating how the Java 8 platform (augmented by the Real-Time Specification for Java) can be used for real-time Big Data applications. JUNIPER introduces architecture-aware stream sources which are suitable for Big Data systems and which preserve locality of data. Our results show that when reading data from disk, thread affinity can seriously degrade the performance of standard Java streams, but JUNIPER's architecture-aware streams maintain their performance.}, location = {Niagara Falls, NY, USA}, series = {JTRES '14}, pages = {20\u201328}, numpages = {9}}
@inproceedings{10.1145/3097983.3097999,title = {A Minimal Variance Estimator for the Cardinality of Big Data Set Intersection}, author = {Cohen Reuven , Katzir Liran , Yehezkel Aviv },year = {2017}, isbn = {9781450348874}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3097983.3097999}, doi = {10.1145/3097983.3097999}, abstract = {In recent years there has been a growing interest in developing \"streaming algorithms\" for efficient processing and querying of continuous data streams. These algorithms seek to provide accurate results while minimizing the required storage and the processing time, at the price of a small inaccuracy in their output. A fundamental query of interest is the intersection size of two big data streams. This problem arises in many different application areas, such as network monitoring, database systems, data integration and information retrieval. In this paper we develop a new algorithm for this problem, based on the Maximum Likelihood (ML) method. We show that this algorithm outperforms all known schemes in terms of the estimation's quality (lower variance) and that it asymptotically achieves the optimal variance.}, location = {Halifax, NS, Canada}, series = {KDD '17}, pages = {95\u2013103}, numpages = {9}, keywords = {data mining, streaming algorithms, cardinality estimation, set intersection}}
@inproceedings{10.1145/3453187.3453357,title = {Analysis and Research on the Teaching Methods of Vocational College Teachers under the Background of Big Data}, author = {Zhen Ma },year = {2020}, isbn = {9781450389099}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3453187.3453357}, doi = {10.1145/3453187.3453357}, abstract = {The teaching methods of vocational college teachers can not only reflect the comprehensive quality and professional level of teachers, but also highlight the innovative ability of teachers. Good teaching methods can not only help students learn professional knowledge, but also affect students' professional employment development. This article analyzes the teaching methods of vocational college teachers under the background of big data.}, location = {Wuhan, China}, series = {EBIMCS 2020}, pages = {341\u2013345}, numpages = {5}, keywords = {teaching methods, analysis and research, Big data background, vocational colleges}}
@inproceedings{10.1145/3514221.3524072,title = {BiDEDE'22: Second International Workshop on Big Data in Emergent Distributed Environments}, author = {Groppe Sven , Gruenwald Le , Hsu Ching-Hsien },year = {2022}, isbn = {9781450392495}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3514221.3524072}, doi = {10.1145/3514221.3524072}, abstract = {The Second International Workshop on Big Data in Emergent Distributed Environments (BiDEDE) focuses on scalable data management issues in emergent computing environments like (post) cloud and fog/edge/dew computing. All these computing environments aim to smoothly integrate scalable data management and processing into distributed environments, such that communication and computational costs are reduced for higher throughput, lower latencies of applications and extending battery lifetimes of nodes in companion with robust approaches to overcome failures and crashes. While there has been research in these areas for already over one decade, still many open challenges exist because of technology triggers like lightweight virtualization, increasing capabilities of nodes and increasing massive parallelization. This workshop supports lively discussions in these and related areas.}, location = {Philadelphia, PA, USA}, series = {SIGMOD '22}, pages = {2542\u20132543}, numpages = {2}, keywords = {cloud computing, serverless computing, post-cloud computing, data management}}
@inproceedings{10.1145/3350546.3352531,title = {Cenote: A Big Data Management and Analytics Infrastructure for the Web of Things}, author = {Chatzidimitriou Kyriakos , Papamichail Michail , Oikonomou Napoleon-Christos , Lampoudis Dimitrios , Symeonidis Andreas },year = {2019}, isbn = {9781450369343}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3350546.3352531}, doi = {10.1145/3350546.3352531}, abstract = {In the era of Big Data, Cloud Computing and Internet of Things, most of the existing, integrated solutions that attempt to solve their challenges are either proprietary, limit functionality to a predefined set of requirements, or hide the way data are stored and accessed. In this work we propose Cenote, an open source Big Data management and analytics infrastructure for the Web of Things that overcomes the above limitations. Cenote is built on component-based software engineering principles and provides an all-inclusive solution based on components that work well individually.}, location = {Thessaloniki, Greece}, series = {WI '19}, pages = {282\u2013285}, numpages = {4}, keywords = {analytics, apache kafka, apache storm, cockroachdb, infrastructure, internet of things, restful api, web of things}}
@inproceedings{10.1145/2516775.2516782,title = {A methodology for analyzing and measuring semantic data quality in service oriented architectures}, author = {Petkov Plamen , Helfert Markus },year = {2013}, isbn = {9781450320214}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2516775.2516782}, doi = {10.1145/2516775.2516782}, abstract = {Nowadays, Service Oriented Architecture (SOA) has become a preferable way of building information systems because they enable enterprises to rapidly response to the business' changes. However, the more complex SOA develops, the more likely are data quality (DQ) issues to be encountered. Despite the huge number of studies that have been done on SOA, very little has been investigated about the DQ aspect. In this paper we address issues concerning the detection of data quality problems. Hence, we propose a DQ methodology in the SOA context which will assess semantic (business) data.}, location = {Ruse, Bulgaria}, series = {CompSysTech '13}, pages = {201\u2013208}, numpages = {8}, keywords = {data quality methodology in SOA, service oriented architectures, data issues in SOA semantic data quality, semantic inaccuracy}}
@inproceedings{10.1145/3167132.3167447,title = {Human behavior analysis based on big data analytics in cyber-physical system: student research abstract}, author = {Din Sadia },year = {2018}, isbn = {9781450351911}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3167132.3167447}, doi = {10.1145/3167132.3167447}, abstract = {The growing gap between users and the Big Data analytics requires innovative tools that address the challenges faced by big data volume, variety, and velocity. Therefore, it becomes computationally inefficient to analyze such massive volume of data. Moreover, advancements in the field of Big Data application and data science leads toward a new paradigm of human behavior, where various smart devices integrate with each other and establish a relationship. However, majority of the systems are either memoryless or computational inefficient, which are unable to define or predict human behavior. Therefore, keeping in view the aforementioned needs, there is a requirement for a system that can efficiently analyze a stream of Big Data within their requirements. Hence, this paper presents a system architecture that integrates social network with the technical network. We derive a novel notion of 'Smart Socio Network', where a friendship is made based on the geo-location information of the user, and trust index is used based on graphs theory. The proposed graph theory provides a better understanding of extraction knowledge from the data and finding relationship between different users.}, location = {Pau, France}, series = {SAC '18}, pages = {673\u2013674}, numpages = {2}}
@inproceedings{10.1145/3495018.3495344,title = {Application Research of Big Data Technology in Air Conditioning Energy-Saving Monitoring Management}, author = {Yang Changyao , Lin Xiaonao },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495344}, doi = {10.1145/3495018.3495344}, abstract = {Human production and life can no longer be separated from the big data. It has been applied in many areas, including water and electricity management and air-conditioning energy-saving monitoring and management. This paper uses big data technology to analyze the air-conditioning energy-saving monitoring and management, establishes a mathematical model, and evaluates the air-conditioning energy-saving monitoring management analysis status. Experimental results show that the analytical hierarchy process improves the efficiency of energy saving(EA) monitoring and management analysis by air conditioning with 43%, and reduces the wrong alarm rate and the wrong alarm rate. Finally, by comparing the analysis of the air-conditioning energy-saving monitoring management system and the analysis of the instantaneous energy consumption and the cumulative energy consumption of the system classification, the influence of big data technology on the air-conditioning energy-saving monitoring management analysis is explained.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {1102\u20131106}, numpages = {5}}
@inproceedings{10.1145/3110291,title = {Challenges of Open Data Quality: More Than Just License, Format, and Customer Support}, author = {Corsar David , Edwards Peter },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3110291}, doi = {10.1145/3110291}, pages = {1\u20134}, numpages = {4}, keywords = {Open data}}
@inproceedings{10.1145/3544109.3544398,title = {Research on fast extraction algorithm of big data information based on machine learning}, author = {Wu Wenchen , Yang Xuliang },year = {2022}, isbn = {9781450395786}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544109.3544398}, doi = {10.1145/3544109.3544398}, abstract = {With the development of the information industry, people are faced with an increasing amount of data. In order to obtain the intrinsic relationship and implicit information from these large-scale data, data mining as an important method has been paid more and more attention by people, and cluster analysis is an important research direction of data mining. In the era of big data, massive multi-modal data exists widely. How to mine the huge hidden value of data through complementary learning between modal data is the main problem of big data research at this stage, and it is also the task of big data and traditional data learning. Mixed attribute datasets are the most common type of datasets in the real world, especially in commercial financial databases, but there are very few clustering algorithms suitable for such datasets. The method used in this paper is machine learning, which enables computers to simulate human learning behavior, automatically acquire knowledge and skills through learning, continuously improve performance, and achieve self-improvement.}, location = {Dalian, China}, series = {IPEC '22}, pages = {1004\u20131007}, numpages = {4}}
@inproceedings{10.1145/2903150.2906141,title = {On the design of scalable and reusable accelerators for big data applications}, author = {Pilato Christian , Xu Qirui , Mantovani Paolo , Di Guglielmo Giuseppe , Carloni Luca P. },year = {2016}, isbn = {9781450341288}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2903150.2906141}, doi = {10.1145/2903150.2906141}, abstract = {Accelerators are becoming key elements of computing platforms for both data centers and mobile devices as they deliver energy-efficient high performance for key computational kernels. However, the design and integration of such components is complex, especially for Big Data applications where they have very large workloads to elaborate. Properly customizing the accelerators' private local memories (PLMs) is of critical importance. To analyze this problem we design an accelerator for Collaborative Filtering by applying a system-level design methodology that allows us to synthesize many alternative micro-architectures as we vary the PLM sizes. We then evaluate the resulting accelerators in terms of resource requirements for both embedded architectures and data centers as we vary the size and density of the workloads.}, location = {Como, Italy}, series = {CF '16}, pages = {406\u2013411}, numpages = {6}}
@inproceedings{10.1145/3404555.3404601,title = {mRNA Big Data Analysis of Hepatoma Carcinoma Between Different Genders}, author = {Deng Jianzhi , Zhou Yuehan , Cheng Xiaohui , Li Tianyu , Qin Chuling },year = {2020}, isbn = {9781450377089}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3404555.3404601}, doi = {10.1145/3404555.3404601}, abstract = {In this paper, we did the researches of the directly related differentially expression mRNAs (DEmRNAs) and their gene ontology (GO), Kyoto Encyclopedia of Genes and Genomes (KEGG) signal pathway, COX model and survival analysis. For the purpose, the 87 directly related DEmRNAs (DRmRNAs) to the hepatoma carcinoma illness were selected from the intersectional DEmRNAs of normal-tumor sample matrix and male-female tumor's sample matrix. By the analysis of online databases, DAVID, KOBAS and KEGG, DRmRNAs were enriched in 18 biological process (BP), 5 cellular component (CC), 9 molecular function (MF) and 3 signal pathways (hsa04974, hsa04972 and hsa04080). The co-expression DRmRNAs were analyzed by using the COX model. CHGA was regard as a potential biomarker of hepatoma carcinoma by the proof of survival kmplot analysis and ROC curve analysis.}, location = {Tianjin, China}, series = {ICCAI '20}, pages = {84\u201388}, numpages = {5}, keywords = {gender difference, hepatoma carcinoma, DEmRNA, TCGA, CHGA}}
@inproceedings{10.1145/3360901.3364438,title = {Searching for Evidence of Scientific News in Scholarly Big Data}, author = {Hoque Md Reshad Ul , Bradley Dash , Kwan Chiman , Chiatti Agnese , Li Jiang , Wu Jian },year = {2019}, isbn = {9781450370080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3360901.3364438}, doi = {10.1145/3360901.3364438}, abstract = {Public digital media can often mix factual information with fake scientific news, which is typically difficult to pinpoint, especially for non-professionals. These scientific news articles create illusions and misconceptions, thus ultimately influence the public opinion, with serious consequences at a broader social scale. Yet, existing solutions aiming at automatically verifying the credibility of news articles are still unsatisfactory. We propose to verify scientific news by retrieving and analyzing its most relevant source papers from an academic digital library (DL), e.g., arXiv. Instead of querying keywords or regular named entities extracted from news articles, we query domain knowledge entities (DKEs) extracted from the text. By querying each DKE, we retrieve a list of candidate scholarly papers. We then design a function to rank them and select the most relevant scholarly paper. After exploring various representations, experiments indicate that the term frequency-inverse document frequency (TF-IDF) representation with cosine similarity outperforms baseline models based on word embedding. This result demonstrates the efficacy of using DKEs to retrieve scientific papers which are relevant to a specific news article. It also indicates that word embedding may not be the best document representation for domain specific document retrieval tasks. Our method is fully automated and can be effectively applied to facilitating fake and misinformed news detection across many scientific domains.}, location = {Marina Del Rey, CA, USA}, series = {K-CAP '19}, pages = {251\u2013254}, numpages = {4}, keywords = {fake news, embedding, web api, domain knowledge entity}}