@article{10.1145/3331651.3331659,
author = {Schilling, Lisa M. and Pena-Jackson, Griselda and Russell, Seth and Corral, Janet and Kwan, Bethany and Ressalam, Julie},
title = {Co-Designing Learning Materials to Empower Laypersons to Better Understand Big Data and Big Data Methods},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/3331651.3331659},
doi = {10.1145/3331651.3331659},
abstract = {University of Colorado Anschutz Medical Campus' Data Science to Patient Value Program and 2040 Partners for Health sought to create open learning materials for engaged citizens and community leaders regarding big data and big data methods to support their collaboration in patient-centered and participatorybased community research and evaluation. 2040 is a local nonprofit organization that cultivates partnerships in Aurora, Colorado neighborhoods to tackle critical health needs. Our goal was to co-design and co-create a series of big data learning modules accessible to community laypeople, so they might better understand big data topics and be empowered more actively engage in health research and evaluation that uses big data methods.},
journal = {SIGKDD Explor. Newsl.},
month = {may},
pages = {41–44},
numpages = {4},
keywords = {education, big data, co-design, community engagement}
}

@article{10.1145/3158348,
author = {Saunier, Louisa and Delic, Kemal A.},
title = {Corporate Security is a Big Data Problem: Big Data (Ubiquity Symposium)},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2018},
number = {July},
url = {https://doi.org/10.1145/3158348},
doi = {10.1145/3158348},
abstract = {In modern times, we have seen a major shift toward hybrid cloud architectures, where corporations operate in a large, highly extended eco-system. Thus, the traditional enterprise security perimeter is disappearing and evolving into the concept of security intelligence where the volume, velocity/rate, and variety of data have dramatically changed. Today, to cope with the fast-changing security landscape, we need to be able to transform huge data lakes via security analytics and big data technologies into effective security intelligence presented through a security "cockpit" to achieve a better corporate security and compliance level, support sound risk management and informed decision making. We present a high-level architecture for efficient security intelligence and the concept of a security cockpit as a point of control for the corporate security and compliance state. Therefore, we could conclude nowadays corporate security can be perceived as a big-data problem.},
journal = {Ubiquity},
month = {jul},
articleno = {1},
numpages = {11}
}

@article{10.1145/2331042.2331057,
author = {Borkar, Vinayak R. and Carey, Michael J. and Li, Chen},
title = {Big Data Platforms: What's Next?},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331057},
doi = {10.1145/2331042.2331057},
abstract = {Three computer scientists from UC Irvine address the question "What's next for big data?" by summarizing the current state of the big data platform space and then describing ASTERIX, their next-generation big data management system.},
journal = {XRDS},
month = {sep},
pages = {44–49},
numpages = {6}
}

@article{10.1145/2486227.2486231,
author = {Grinter, Beki},
title = {A Big Data Confession},
year = {2013},
issue_date = {July + August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1072-5520},
url = {https://doi.org/10.1145/2486227.2486231},
doi = {10.1145/2486227.2486231},
journal = {Interactions},
month = {jul},
pages = {10–11},
numpages = {2}
}

@inproceedings{10.1145/2808580.2808667,
author = {Due, Beathe and Kristiansen, Monica and Colomo-Palacios, Ricardo and Hien, Dang Ha The},
title = {Introducing Big Data Topics: A Multicourse Experience Report from Norway},
year = {2015},
isbn = {9781450334426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808580.2808667},
doi = {10.1145/2808580.2808667},
abstract = {In the last few years we have witnessed an explosion of interest in Big Data in both academic and industry arenas. Big Data is about the capture, storage, analysis and visualization of huge volumes of data in both structured and unstructured forms generated from a myriad of applications and devices in a wide set of scenarios. Focusing on the need of academia to develop professional posing the competences that industry demands, the paper presents the approach adopted by the Faculty of Computer Sciences at \O{}stfold University College, Norway to deploy Big Data related contents throughout its studies in the computing field. This paper describes initiatives in bachelor and master programs along with continuous education courses with regards to Big Data topics. New master courses were implemented in the 2014-2015 academic year, while bachelor and continuous education courses will be deployed the year after. Initial results in terms of course assessments and students' acceptation unveil promising perspectives for the initiative.},
booktitle = {Proceedings of the 3rd International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {565–569},
numpages = {5},
keywords = {education, big data},
location = {Porto, Portugal},
series = {TEEM '15}
}

@inproceedings{10.1145/3366030.3366121,
author = {Safhi, Hicham Moad and Frikh, Bouchra and Ouhbi, Brahim},
title = {Data Source Selection in Big Data Context},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366121},
doi = {10.1145/3366030.3366121},
abstract = {Big Data presents promising technological and economical opportunities. In fact, it has become the raw material of production for many organizations. Data is available in large quantities, and it continues generating abundantly. However, not all the data will have valuable knowledge. Unreliable sources provide misleading and biased information, and even reliable sources could suffer from low data quality.In this paper, we propose a novel methodology for the selectability of data sources, by both considering the presence and the absence of users' preferences. The proposed model integrates multiple factors that affect the reliability of data sources, including their quality, gain, cost and coverage. Experimental results on real world data-sets, show its capability to find the subset of relevant and reliable sources with the lowest cost.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {611–616},
numpages = {6},
keywords = {Big Data integration, Big Data Source Selection, Data quality, Source reliability},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.5555/2819289.2819302,
author = {Chen, Hong-Mei and Kazman, Rick and Haziyev, Serge and Hrytsay, Olha},
title = {Big Data System Development: An Embedded Case Study with a Global Outsourcing Firm},
year = {2015},
publisher = {IEEE Press},
abstract = {Big data system development is dramatically different from small (traditional, structured) data system development. At the end of 2014, big data deployment is still scarce and failures abound. Outsourcing has become a main strategy for many enterprises. We therefore selected an outsourcing company who has successfully deployed big data projects for our study. Our research results from analyzing 10 outsourced big data projects provide a glimpse into early adopters of big data, illuminates the challenges for system development that stem from the 5Vs of big data and crystallizes the importance of architecture design choices and technology selection. We followed a collaborative practice research (CPR) method to develop and validate a new method, called BDD. BDD is the first attempt to systematically combine architecture design with data modeling approaches to address big data system development challenges. The use of reference architectures and a technology catalog are advancements to architecture design methods and are proving to be well-suited for big data system architecture design and system development.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {44–50},
numpages = {7},
keywords = {collaborative practice research, data system design methods, embedded case study methodology, software architecture, big data, system engineering},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@inproceedings{10.1145/2676536.2676538,
author = {Chen, Xin and Vo, Hoang and Aji, Ablimit and Wang, Fusheng},
title = {High Performance Integrated Spatial Big Data Analytics},
year = {2014},
isbn = {9781450331326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676536.2676538},
doi = {10.1145/2676536.2676538},
abstract = {The growth of spatial big data has been explosive thanks to cost-effective and ubiquitous positioning technologies, and the generation of data from multiple sources in multi-forms. Such emerging spatial data has high potential to create new insights and values for our life through spatial analytics. However, spatial data analytics faces two major challenges. First, spatial data is both data-and compute-intensive due to the massive amounts of data and the multi-dimensional nature, which requires high performance spatial computing infrastructure and methods. Second, spatial big data sources are often isolated, for example, OpenStreetMap, census data and Twitter tweets are independent data sources. This leads to incompleteness of information and sometimes limited data accuracy, thus limited values from the data. Integrating spatial big data analytics by consolidating multiple data sources provides significant potential for data quality improvement in terms of completeness and accuracy, and much increased values derived from the data. In this paper, we present our vision of a high performance integrated spatial big data analytics framework. We provide a scalable spatial query based data integration engine with MapReduce, and demonstrate integrated spatial data analytics through a few use cases in our preliminary work. We then present our future plan on integrated spatial big data analytics for improving public health research and applications.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {11–14},
numpages = {4},
keywords = {data warehouse, MapReduce, GIS, spatial analytics, database},
location = {Dallas, Texas},
series = {BigSpatial '14}
}

@inproceedings{10.1145/2487575.2506179,
author = {Getoor, Lise and Machanavajjhala, Ashwin},
title = {Entity Resolution for Big Data},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2506179},
doi = {10.1145/2487575.2506179},
abstract = {Entity resolution (ER), the problem of extracting, matching and resolving entity mentions in structured and unstructured data, is a long-standing challenge in database management, information retrieval, machine learning, natural language processing and statistics. Accurate and fast entity resolution has huge practical implications in a wide variety of commercial, scientific and security domains. Despite the long history of work on entity resolution, there is still a surprising diversity of approaches, and lack of guiding theory. Meanwhile, in the age of big data, the need for high quality entity resolution is growing, as we are inundated with more and more data, all of which needs to be integrated, aligned and matched, before further utility can be extracted. In this tutorial, we bring together perspectives on entity resolution from a variety of fields, including databases, information retrieval, natural language processing and machine learning, to provide, in one setting, a survey of a large body of work. We discuss both the practical aspects and theoretical underpinnings of ER. We describe existing solutions, current challenges and open research problems. In addition to giving attendees a thorough understanding of existing ER models, algorithms and evaluation methods, the tutorial will cover important research topics such as scalable ER, active and lightly supervised ER, and query-driven ER.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1527},
numpages = {1},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/3216122.3216124,
author = {Cappiello, Cinzia and Sam\'{a}, Walter and Vitali, Monica},
title = {Quality Awareness for a Successful Big Data Exploitation},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216124},
doi = {10.1145/3216122.3216124},
abstract = {The combination of data and technology is having a high impact on the way we live. The world is getting smarter thanks to the quantity of collected and analyzed data. However, it is necessary to consider that such amount of data is continuously increasing and it is necessary to deal with novel requirements related to variety, volume, velocity, and veracity issues. In this paper we focus on veracity that is related to the presence of uncertain or imprecise data: errors, missing or invalid data can compromise the usefulness of the collected values. In such a scenario, new methods and techniques able to evaluate the quality of the available data are needed. In fact, the literature provides many data quality assessment and improvement techniques, especially for structured data, but in the Big Data era new algorithms have to be designed. We aim to provide an overview of the issues and challenges related to Data Quality assessment in the Big Data scenario. We also propose a possible solution developed by considering a smart city case study and we describe the lessons learned in the design and implementation phases.},
booktitle = {Proceedings of the 22nd International Database Engineering &amp; Applications Symposium},
pages = {37–44},
numpages = {8},
keywords = {Big Data, Veracity, Data Quality Assessment},
location = {Villa San Giovanni, Italy},
series = {IDEAS '18}
}

@inproceedings{10.1145/2339530.2339567,
author = {Kitsuregawa, Masaru},
title = {Building an Engine for Big Data},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339567},
doi = {10.1145/2339530.2339567},
abstract = {IT program in Japan to build powerful engine for big data was launched. Quite recently the initial version is commercialized. This presentation will give a brief overview of the project. Also some of the potential applications will be introduced.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {223},
numpages = {1},
keywords = {search engine, big data},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.1145/3383972.3384075,
author = {Yuxuan, Yang and Xianyu, Zeng and Yuanjie, Jiao},
title = {Sociological Aspects of Big Data Privacy},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384075},
doi = {10.1145/3383972.3384075},
abstract = {Nowadays, information technology has entered the era of big data. It is regarded as a revolution that will transform how we live, work and think. In the network era, a large number of data will be generated every day. Most of these data have the participation of our individual users, and the left data footprints can be gathered together to discover a lot of personal privacy. Big data can bring better work and life experience to users, however, it also brings the risk of personal privacy disclosure. This paper will use some theories of sociology, including anomie theory, social control theory, cultural conflict theory, and social exchange theory, to understand the knowledge of big data privacy, and point out some technologies that could solve out the problem: anonymity technology, data encryption technology, different privacy.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {230–235},
numpages = {6},
keywords = {privacy, Sociology, big data},
location = {Shenzhen, China},
series = {ICMLC 2020}
}

@inproceedings{10.1145/3132498.3132510,
author = {Sena, Bruno and Allian, Ana Paula and Nakagawa, Elisa Yumi},
title = {Characterizing Big Data Software Architectures: A Systematic Mapping Study},
year = {2017},
isbn = {9781450353250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132498.3132510},
doi = {10.1145/3132498.3132510},
abstract = {Big data is a broad term for large, dynamic, and complex data sets that have brought great challenges to be addressed by traditional software systems. It has also demanded advanced software architectures (i.e., the big data software architectures) prepared to deal with the continuous expansion of the volume of data as well as to take advantage of new technologies for big data context. However, the main characteristics, basic requirements, and modules and organization of big data architectures are not still widely known. Besides that, no detailed overview about them is available. The main contribution of this paper is to present the state of the art related to big data software architectures; for this, we conducted a Systematic Mapping Study. As results, an essential set of eight requirements for big data architectures was identified, besides a collection of five modules that are fundamental to adequately enable the data flow. We also intend these results can guide architects in the development of software systems for this new challenging scenario of big data management.},
booktitle = {Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse},
articleno = {9},
numpages = {10},
keywords = {reference architecture, big data system, systematic mapping study, software architecture},
location = {Fortaleza, Cear\'{a}, Brazil},
series = {SBCARS '17}
}

@article{10.1145/3158350,
author = {Johnson, Jeffrey and Tesei, Luca and Piangerelli, Marco and Merelli, Emanuela and Paci, Riccardo and Stojanovic, Nenad and Leit\~{a}o, Paulo and Barbosa, Jos\'{e} and Amador, Marco},
title = {Big Data: Business, Technology, Education, and Science: Big Data (Ubiquity Symposium)},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2018},
number = {July},
url = {https://doi.org/10.1145/3158350},
doi = {10.1145/3158350},
abstract = {Transforming the latent value of big data into real value requires the great human intelligence and application of human-data scientists. Data scientists are expected to have a wide range of technical skills alongside being passionate self-directed people who are able to work easily with others and deliver high quality outputs under pressure. There are hundreds of university, commercial, and online courses in data science and related topics. Apart from people with breadth and depth of knowledge and experience in data science, we identify a new educational path to train "bridge persons" who combine knowledge of an organization's business with sufficient knowledge and understanding of data science to "bridge" between non-technical people in the business with highly skilled data scientists who add value to the business. The increasing proliferation of big data and the great advances made in data science do not herald in an era where all problems can be solved by deep learning and artificial intelligence. Although data science opens up many commercial and social opportunities, data science must complement other science in the search for new theory and methods to understand and manage our complex world.},
journal = {Ubiquity},
month = {jul},
articleno = {2},
numpages = {13}
}

@article{10.14778/2367502.2367572,
author = {Labrinidis, Alexandros and Jagadish, H. V.},
title = {Challenges and Opportunities with Big Data},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367572},
doi = {10.14778/2367502.2367572},
abstract = {The promise of data-driven decision-making is now being recognized broadly, and there is growing enthusiasm for the notion of "Big Data," including the recent announcement from the White House about new funding initiatives across different agencies, that target research for Big Data. While the promise of Big Data is real -- for example, it is estimated that Google alone contributed 54 billion dollars to the US economy in 2009 -- there is no clear consensus on what is Big Data. In fact, there have been many controversial statements about Big Data, such as "Size is the only thing that matters." In this panel we will try to explore the controversies and debunk the myths surrounding Big Data.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2032–2033},
numpages = {2}
}

@article{10.1145/2911975,
author = {Kugler, Logan},
title = {What Happens When Big Data Blunders?},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/2911975},
doi = {10.1145/2911975},
abstract = {Big data is touted as a cure-all for challenges in business, government, and healthcare, but as disease outbreak predictions show, big data often fails.},
journal = {Commun. ACM},
month = {may},
pages = {15–16},
numpages = {2}
}

@article{10.14778/2367502.2367563,
author = {Shim, Kyuseok},
title = {MapReduce Algorithms for Big Data Analysis},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367563},
doi = {10.14778/2367502.2367563},
abstract = {There is a growing trend of applications that should handle big data. However, analyzing big data is a very challenging problem today. For such applications, the MapReduce framework has recently attracted a lot of attention. Google's MapReduce or its open-source equivalent Hadoop is a powerful tool for building such applications. In this tutorial, we will introduce the MapReduce framework based on Hadoop, discuss how to design efficient MapReduce algorithms and present the state-of-the-art in MapReduce algorithms for data mining, machine learning and similarity joins. The intended audience of this tutorial is professionals who plan to design and develop MapReduce algorithms and researchers who should be aware of the state-of-the-art in MapReduce algorithms available today for big data analysis.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2016–2017},
numpages = {2}
}

@inproceedings{10.1145/2513190.2517827,
author = {Tsotras, Vassilis J.},
title = {Revisiting Aggregation Techniques for Big Data},
year = {2013},
isbn = {9781450324120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513190.2517827},
doi = {10.1145/2513190.2517827},
abstract = {In this talk we first present an introduction to AsterixDB [1], a parallel, semistructured platform to ingest, store, index, query, analyze, and publish "big data" (http://asterixdb.ics.uci.edu) and the various challenges we addressed while building it. AsterixDB combines ideas from semistructured data management, parallel database systems, and first-generation data-intensive computing platforms (MapReduce and Hadoop). The full AsterixDB software stack provides support for big data applications from the storage and processing engine (Hyracks [2] available at: http://hyracks.googlecode.com), to the exible query optimization layer (Algebricks), to the interfaces for user-level interaction (AQL, HiveQL, Pregelix, etc.) Hyracks is a partitioned-parallel engine for data intensive computing jobs in the form of DAGs. Algebricks is a model-agnostic, algebraic layer for compiling and optimizing parallel queries to be processed by Hyracks. Queries for AsterixDB can be expressed by either popular higher-level data analysis languages like Pig, Hive or Jaql, or by its native query language (AQL) and data model (ADM) with support for semi-structured information and fuzzy data.Fundamental data processing operations, like joins and aggregations, are natively supported in AsterixDB. The second part of the talk focuses on our experiences while designing efficient local (per node) aggregation algorithms for AsterixDB. In particular, there are two challenges for local aggregations in a big data system: first, if the aggregation is group-based (like the "group-by" in SQL), the aggregation result may not fit in main memory; second, in order to allow multiple operations being processed simultaneously, an aggregation operation should work within a strict memory budget provided by the platform. Despite its importance and challenges, the design and evaluation of local aggregation algorithms has not received the same level of attention that other basic operators, such as joins, have received in the literature. Facing a lack of "off the shelf" local aggregation algorithms for big data, we present low-level implementation details for engineering the aggregation operator, utilizing (i) sort-based, (ii) hash-based, and (iii) sort-hash-hybrid approaches. We present six algorithms all of which work within a strictly bounded memory budget, and can easily adapt between in-memory and external processing. Among them, two are novel and four are based on extending existing join algorithms.We deployed all algorithms as operators in the Hyracks platform and evaluated their performance through extensive experimentation. Our experiments cover many different performance factors, including input cardinality, memory, data distribution, and hash table structure. Our study guided our selection of the local aggregation algorithms supported in the recent release of AsterixDB, namely: the hybrid-hash. Pre-Partitioning algorithm for its tolerance on the estimation of the input grouping key cardinality, the Hash-Sort algorithm for its good performance when aggregating skewed data, and the Sort-Based algorithm when the input data is already sorted. This local aggregation work is the first part of a two-part big data aggregation study, as it addresses the "map" phase. Our findings provide the foundation for the global aggregation strategy we are currently investigating for the "reduce" phase. We hope our experience can help developers of other Big Data platforms to build a solid local aggregation operator.},
booktitle = {Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP},
pages = {1–2},
numpages = {2},
keywords = {big data management system, aggregation},
location = {San Francisco, California, USA},
series = {DOLAP '13}
}

@inproceedings{10.1145/2905055.2905215,
author = {Dubey, Arunima and Srivastava, Satyajee},
title = {A Major Threat to Big Data: Data Security},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905215},
doi = {10.1145/2905055.2905215},
abstract = {Big Data has nowadays become the most talked latest IT trends. The fact that it can handle all the forms of data which includes unstructured data, big data has now become the preferred choice for analysis of huge amount of data over the Relational Database Management System(RDBMS). Big Data is helpful for the analysis of petabytes of data which is not possible in case of normal database system. But as everything comes with pros and cons so does big data. There are certain challenges which big data analytics is facing. The various challenges include validating the end point input, handling enormous amount of data on a very large scale, ensuring the security of transactional data, ensuring the safe and secure storage of data, sharing of variants of data with third party and analyzing those data without skipping even a single piece of information in order to generate reports and draw a conclusion. In this paper, we will be considering the major challenge to the big data and that is data security. Even with enormous advantages, the industry is taking a backseat for the shift from normal database to big data because of the data privacy concern. Even many big organizations don't consider big data to be a safe option as the data can be accessed by anyone. Certain different methods like the encryption-decryption technique, anonymization based, etc have been suggested by the researchers who are working to overcome the major threat of Big Data i.e. Data security. But unfortunately because of the 3 V's of the Big Data as stated by Gartner i.e. Velocity, Volume and Variety these methods didn't prove to be advantageous. Basically in this paper we will be discussing the various concern issues of Big data, out of which our main focus will be on data security.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {9},
numpages = {6},
keywords = {anonymization, transactional data, Big data, unstructured data, encryption},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3361758.3361768,
author = {Venkatraman, Ramanathan and Venkatraman, Sitalakshmi},
title = {Big Data Infrastructure, Data Visualisation and Challenges},
year = {2019},
isbn = {9781450372466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361758.3361768},
doi = {10.1145/3361758.3361768},
abstract = {The importance of Big Data is being realised worldwide with the advancement of information technologies, leveraging the capabilities of virtualization and cloud computing. Big Data infrastructure and the use of its tools and applications will significantly transform the data centers of businesses in the next decade. Data analytics is evolving with the new real-time capability of Big Data solutions to provide business intelligence for timely and effective decision making. However, Big Data poses various challenges related to the infrastructure and resource constraints, and other issues including security and privacy. This paper takes an initial step in recognizing the value of creating Big Data infrastructure for delivering high performance and scalable business intelligence in an organization. It presents the state-of-the-art tools and technologies for Big Data infrastructure and NIST framework. The advantages of data visualisation are illustrated thorough industry case scenarios. The Big Data trends and challenges are also discussed. Overall, this paper contributes to providing valuable insights unto the Big Data journey of an organization to enable a scalable infrastructure for achieving mission critical decision-making through data visualisation.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Internet of Things},
pages = {13–17},
numpages = {5},
keywords = {cloud, Hadoop, Big Data infrastructure, Big Data, data visualisation, NoSQL database},
location = {Melbourn, VIC, Australia},
series = {BDIOT 2019}
}

@article{10.1145/2611567,
author = {Jagadish, H. V. and Gehrke, Johannes and Labrinidis, Alexandros and Papakonstantinou, Yannis and Patel, Jignesh M. and Ramakrishnan, Raghu and Shahabi, Cyrus},
title = {Big Data and Its Technical Challenges},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2611567},
doi = {10.1145/2611567},
abstract = {Exploring the inherent technical challenges in realizing the potential of Big Data.},
journal = {Commun. ACM},
month = {jul},
pages = {86–94},
numpages = {9}
}

@inproceedings{10.1145/2700171.2790380,
author = {Smyth, Barry},
title = {From Small Sensors to Big Data},
year = {2015},
isbn = {9781450333955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2700171.2790380},
doi = {10.1145/2700171.2790380},
abstract = {In our increasingly digitized world almost everything we do creates a record that is stored somewhere, whether we are purchasing a book, calling a friend, ordering a meal, or renting a movie. And in today's world of sensors and internet-enabled devices, smartphones and wearables, this is no longer just limited to our online activities. Exercising in the park, shopping for groceries, falling asleep, or even taking a shower, are just some of the everyday real-world activities that are likely to generate data. This is the big data world of the so-called Sensor Web. It is enabled by the widescale availability of high-performance computing, always-on communications, and mobile computing devices that come equipped with a variety of powerful sensors. This provides for a powerful computing and sensing ecosystem with important applications across all aspects of how we live, work, and play.The primary challenge for us now is to understand how we can (and whether we should) use this information. On the one hand, the promise of big data analytics is better decisions: better decisions about where we might live or where to send our kids to school; better decisions about the food we eat and the exercise we should take; and better decisions about some of the biggest choices facing modern societies when it comes to health, education, energy, and climate. On the other hand, this potential has a darker side, in the form of a gradual erosion of personal privacy as businesses and even governments seek to exploit our personal data for their own purposes, often without our informed consent.What is certain is that the combination of mobile computation, cheap but powerful sensors, and big data analytics points to new ways of thinking about some of society's toughest challenges. But to take advantage of these benefits we must reconcile the promise of big data with the pitfalls of privacy. Only then can these technologies can have a meaningful impact on how we can all benefit from the big data revolution as part of a healthier, safer, fairer world.},
booktitle = {Proceedings of the 26th ACM Conference on Hypertext &amp; Social Media},
pages = {101},
numpages = {1},
keywords = {big data, sensor web, mobile computing, data analytics},
location = {Guzelyurt, Northern Cyprus},
series = {HT '15}
}

@inproceedings{10.1145/2345316.2345347,
author = {Berkovich, Simon and Liao, Duoduo},
title = {On Clusterization of "Big Data" Streams},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345347},
doi = {10.1145/2345316.2345347},
abstract = {Current technology provides wonderful facilities for operating with extremely vast amounts of data. These facilities are expanding due to capabilities of "Cloud Computing." The developing situation gives rise to the "Big Data" concept posing specific engineering and organizational challenges. Big data refers to the rising flood of digital data from many sources, including the sensors, digitizers, scanners, software-based modeling, mobile phones, internet, videos, e-mails, and social network communications. The data type could be texts, geometries, images, videos, sounds, or their combination. Many of such data are directly or indirectly related to geospatial information. In this paper, we suggest to enhance the available information processing resources with a novel software/hardware technique for on-the-fly clusterization of amorphous data from diverse sources. The presented approach is based on the previously developed construction of FuzzyFind Dictionary utilizing the error-correction Golay Code. Realization of this technique requires processing of intensive continuous data streams, which can be effectively implemented using multi-core pipelining with forced interrupts. The objective of this paper is to bring forward a new simple and efficacious tool for one of the most demanding operations of this "Big Data" methodology --clustering of diverse information items in a data stream mode. Improving our ability to extract knowledge and insights from large and complex collections of digital data promises to solve some the Nation's most pressing challenges. Furthermore, the paper reveals a parallel between the computational model integrating "Big Data" streams and the organization of information processing in the brain. The uncertainties in relation to the considered method of clusterization are moderated due to the idea of the bounded rationality, an approach that does not require a complete exact knowledge for sensible decision-making.},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {26},
numpages = {6},
keywords = {"big data" concept, clustering algorithms, Golay code, meta knowledge" ontology, on-the-fly processing, heterogeneous computing, multi-core pipelining, stream processing},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@inproceedings{10.1145/2812428.2812441,
author = {Keka, Ilir and \c{C}i\c{c}o, Betim},
title = {Big Data in Electricity-Visualization Aspect},
year = {2015},
isbn = {9781450333573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2812428.2812441},
doi = {10.1145/2812428.2812441},
abstract = {Nowadays, the rapid development of technology has made it possible to store the data in memory with high capacity. There are various sources from where the data are collected or generated Moreover, the speed of updating and processing the data is very important to many organizations. Big Data is a concept the covers capacity, speed and different formats of data in many organizations.The aim of this paper is to analyze the Big Data in electricity using visualization aspect. Also, it gives an overview of features and sources of Big Data. The visualization on electricity data is made based on Revolution R Enterprise (RRE) tool. Writing scripts in RRE is found the relationship between electricity load and time. This relationship is illustrated using the line of best fit and the smoothing line. Also, some statistical parameters are computed and the relationship of data is evaluated using visualization.},
booktitle = {Proceedings of the 16th International Conference on Computer Systems and Technologies},
pages = {236–243},
numpages = {8},
keywords = {big data, regression, electric power, revolution r enterprise, time, visualization, load},
location = {Dublin, Ireland},
series = {CompSysTech '15}
}

@inproceedings{10.1145/3257818,
author = {Kelly, Terence},
title = {Session Details: Big Data},
year = {2015},
isbn = {9781450338349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257818},
doi = {10.1145/3257818},
booktitle = {Proceedings of the 25th Symposium on Operating Systems Principles},
location = {Monterey, California},
series = {SOSP '15}
}

@inproceedings{10.1145/3259177,
author = {\"{O}zsu, M. Tamer},
title = {Session Details: Big Data},
year = {2012},
isbn = {9781450313261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3259177},
doi = {10.1145/3259177},
booktitle = {Proceedings of the on SIGMOD/PODS 2012 PhD Symposium},
location = {Scottsdale, Arizona, USA},
series = {PhD '12}
}

@inproceedings{10.1145/2723372.2742794,
author = {Huang, Yiqing and Zhu, Fangzhou and Yuan, Mingxuan and Deng, Ke and Li, Yanhua and Ni, Bing and Dai, Wenyuan and Yang, Qiang and Zeng, Jia},
title = {Telco Churn Prediction with Big Data},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742794},
doi = {10.1145/2723372.2742794},
abstract = {We show that telco big data can make churn prediction much more easier from the $3$V's perspectives: Volume, Variety, Velocity. Experimental results confirm that the prediction performance has been significantly improved by using a large volume of training data, a large variety of features from both business support systems (BSS) and operations support systems (OSS), and a high velocity of processing new coming data. We have deployed this churn prediction system in one of the biggest mobile operators in China. From millions of active customers, this system can provide a list of prepaid customers who are most likely to churn in the next month, having $0.96$ precision for the top $50000$ predicted churners in the list. Automatic matching retention campaigns with the targeted potential churners significantly boost their recharge rates, leading to a big business value.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {607–618},
numpages = {12},
keywords = {customer retention, big data, telco churn prediction},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/3047273.3047377,
author = {Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil and Meijer, Ronald},
title = {Exploiting Big Data for Evaluation Studies},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047377},
doi = {10.1145/3047273.3047377},
abstract = {The collection and analysis of relevant data for evaluating public policies is not a straightforward task. An important type of such studies is the so-called ex-post evaluation. The main objective of ex-post evaluations is to determine to what extent a realized intervention is successful in tackling a societal challenge, e.g., youth unemployment. At a first glance an obvious method is to collect some baseline measurements for a set of relevant variables, apply the intervention for a while and collect the new measurement values for the same set of variables. Then, comparing the measurement values of the variables before and after the intervention provides an insight into the extent of successfulness of the intervention. This, however, is only true if the "ceteris paribus" condition holds. In practice it is infeasible to enforce this condition for societal challenges. Often, after having the baseline measurements, several phenomena emerge that may impact the new measurements without being taken into account. This makes it difficult to determine how much of the measured differences between the values of the variables before and after the intervention should be attributed to the emerging phenomena (or the so-called counterfactuals) and how much of the differences can be attributed to the applied intervention.This paper discusses how exploiting big data may contribute to the task of elucidating the influences of counterfactuals (and interventions) in ex-post evaluation studies. The paper proposes a framework to utilize big data for accounting for the impact of emerging phenomena in ex-post evaluation studies.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {228–231},
numpages = {4},
keywords = {counterfactuals, data linkage, Big data, ex-post policy evaluation},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3141128.3141139,
author = {Pti\v{c}ek, Marina and Vrdoljak, Boris},
title = {Big Data and New Data Warehousing Approaches},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141139},
doi = {10.1145/3141128.3141139},
abstract = {Big data are a data trend present around us mainly through Internet -- social networks and smart devices and meters -- mostly without us being aware of them. Also they are a fact that both industry and scientific research needs to deal with. They are interesting from analytical point of view, for they contain knowledge that cannot be ignored and left unused. Traditional system that supports the advanced analytics and knowledge extraction -- data warehouse -- is not able to cope with large amounts of fast incoming various and unstructured data, and may be facing a paradigm shift in terms of utilized concepts, technologies and methodologies, which have become a very active research area in the last few years. This paper provides an overview of research trends important for the big data warehousing, concepts and technologies used for data storage and (ETL) processing, and research approaches done in attempts to empower traditional data warehouses for handling big data.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {6–10},
numpages = {5},
keywords = {NoSQL, databases, data warehouse, NewSQL, big data, MapReduce},
location = {London, United Kingdom},
series = {ICCBDC 2017}
}

@inproceedings{10.1145/584792.584894,
author = {Loshin, David},
title = {Rule-Based Data Quality},
year = {2002},
isbn = {1581134924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584792.584894},
doi = {10.1145/584792.584894},
abstract = {In the business intelligence/data warehouse user community, there is a growing confusion as to the difference between data cleansing and data quality. While many data cleansing products can help in applying data edits to name and address data, or help in transforming data during an ETL process, there is usually no persistence in this cleansing. This paper describes how we have implemented a business rules approach to build a data validation engine, called GuardianIQ, that transforms declarative data quality rules into code that objectively measures and reports levels of data quality based on user expectations.},
booktitle = {Proceedings of the Eleventh International Conference on Information and Knowledge Management},
pages = {614–616},
numpages = {3},
keywords = {data validation, data quality, business rules},
location = {McLean, Virginia, USA},
series = {CIKM '02}
}

@inproceedings{10.1145/2790798.2790812,
author = {Kantere, Verena and Filatov, Maxim},
title = {A Framework for Big Data Analytics},
year = {2015},
isbn = {9781450334198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790798.2790812},
doi = {10.1145/2790798.2790812},
abstract = {Big Data may reside on multiple and dispersed sources and adhere to a variety of formats. Their analysis may include a range of tasks to be executed on a range of query engines. The tasks, as a whole, represent the rationale of a specific process. The users that create such a process may have various roles, like, business analysts, engineers, end-users etc. Each role may need or care for a different level of abstraction with respect to the execution of the individual tasks and overall process. Therefore, it is necessary to enable the expression of analytics tasks in an abstract manner, adaptable to the user role, interest and expertise. We propose a framework for the expression and preparation for execution of complex processes that perform analytics on Big Data. The framework enables the expression of such processes in the form of workflows and prepares such workflows for execution by determining and clarifying execution semantics of individual tasks, and by manipulating the workflow in order to create an equivalent workflow that will be optimally executed alone or together with other workflows.},
booktitle = {Proceedings of the Eighth International C* Conference on Computer Science &amp; Software Engineering},
pages = {125–132},
numpages = {8},
keywords = {Data Analytics, Workflow Management, Big Data},
location = {Yokohama, Japan},
series = {C3S2E '15}
}

@inproceedings{10.1145/2609876.2609887,
author = {Szymczak, Samantha and Zelik, Daniel J. and Elm, Wiliam},
title = {Support for Big Data's Limiting Resource: Human Attention},
year = {2014},
isbn = {9781450329385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609876.2609887},
doi = {10.1145/2609876.2609887},
abstract = {With an increase in data volume, variety, and velocity, Big Data advances tend to focus on technologies such as data gathering, processing, data storage, and analytics, all of which assume that technology is the limiting factor in leveraging Big Data to its fullest potential. The research framework proposed here takes a more holistic look at the Joint Cognitive System, identifying human attention as the limiting resource for employing Big Data for operational use. The framework leverages prior research in attention management, sensory perception, and joint cognitive systems to lay out a Human Centered Big Data Research agenda for designing attention direction support in Big Data environments.},
booktitle = {Proceedings of the 2014 Workshop on Human Centered Big Data Research},
pages = {45–48},
numpages = {4},
keywords = {human attention, mental models, Attention direction support, human-centered Big Data, cognitive systems engineering, joint cognitive system},
location = {Raleigh, NC, USA},
series = {HCBDR '14}
}

@inproceedings{10.1145/2361999.2362038,
author = {Miner, Donald},
title = {Unified Analytics Platform for Big Data},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362038},
doi = {10.1145/2361999.2362038},
abstract = {Greenplum is using Hadoop and several other open source tools in interesting ways as part of a big data architecture with their Greenplum Database (a scale-out MPP SQL database).},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {176},
numpages = {1},
keywords = {MPP, Hadoop, PostGIS, MADlib, R, Mahout, Solr, database, data science, Greenplum, EMC, geospatial},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@inproceedings{10.1145/1839379.1839397,
author = {Helfert, Markus and Hossain, Fakir},
title = {Certifying Data Quality Conformance},
year = {2010},
isbn = {9781450302432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1839379.1839397},
doi = {10.1145/1839379.1839397},
abstract = {Many researchers and practitioners have been attracted to improve data quality due to its monumental importance as a key success factor. Mathematical and statistical models have been deployed to information systems to introduce constrain and transaction based mechanisms to prevent data quality related problems. Entire management of the process and roles involved in data generation has also been scrutinized. Vast amount of knowledge base has been progressed in this area; however, most of the approaches are limited from practical perspective. System development process incorporating quality modelling is rarely integrated. Quality related meta data is absent from most information system. Neither process mapping nor data modelling provides sufficient provision to measure quality or certification of data in the information systems. Furthermore, ongoing monitoring of data for quality conformance through a separate process is expensive and time consuming. Recognising this limitation and aiming to provide a practical-orient comprehensive approach, we propose a process centric quality focused system design incorporating data product quality, conformance monitoring and certification. In this paper we focus on the self certification of data quality based on our earlier work on the process centric framework for ongoing data quality monitoring.},
booktitle = {Proceedings of the 11th International Conference on Computer Systems and Technologies and Workshop for PhD Students in Computing on International Conference on Computer Systems and Technologies},
pages = {95–100},
numpages = {6},
keywords = {ongoing data product monitoring, information manufacturing, quality monitoring, information quality, data quality certification},
location = {Sofia, Bulgaria},
series = {CompSysTech '10}
}

@inproceedings{10.1145/2656434.2657486,
author = {Villanustre, Flavio G.},
title = {Big Data Trends and Evolution: A Human Perspective},
year = {2014},
isbn = {9781450327114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656434.2657486},
doi = {10.1145/2656434.2657486},
abstract = {The Big Data revolution has already happened and, through it, organizations started realizing the potential of using data to take better informed decisions, mitigate risks and overall better control their destiny. With all the benefits that Big Data brings, it also creates new challenges; the growing talent gap possibly being the most representative of them all. In order to effectively leverage Big Data, a new profession is emerging: the data scientist. Tasked with understanding the methodologies to process and analyze vast and complex data, this professional must possess knowledge in a broad spectrum of domains, including mathematics (calculus, linear algebra, statistics, probabilities and even possibly category theory), programming languages (Python and R being frequently cited), data processing and analysis expertise (profiling, parsing, cleansing, linking), machine learning techniques (supervised and unsupervised learning, dimensionality reduction, feature selection, etc.) and business domain knowledge. While it is conceivable to identify individuals that can achieve this breadth of knowledge with significant depth, it is unreasonable to expect this to be the norm, so these individuals fall usually far into the upper tail of the population distribution. To make things worse, the current toolsets available to the data scientist tend to be very involved and require considerable amounts of time to develop applications, reducing the overall effectiveness of these experts. The solution to this talent gap is certainly not to try and breed a new step up the evolutionary ladder that can cope with this vast knowledge, but to create radically different abstractions as part of the toolsets that data scientists use, to increase efficiency and reduce the scope of the basic knowledge required to build Big Data applications. During this presentation we will explore this challenge and provide a new perspective on more efficient toolsets for Big Data applications.},
booktitle = {Proceedings of the 3rd Annual Conference on Research in Information Technology},
pages = {1–2},
numpages = {2},
keywords = {KEL, data science, HPCC, ECL, declarative programming, data analysis, dataflow programming},
location = {Atlanta, Georgia, USA},
series = {RIIT '14}
}

@article{10.1145/2448917.2448925,
author = {Salvo, Michael J.},
title = {Visual Rhetoric and Big Data: Design of Future Communication},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/2448917.2448925},
doi = {10.1145/2448917.2448925},
abstract = {The hype machine---media, corporate communications, and futurist prognosticators---are hard at work promoting Big Data. There are computing and storage resources that, like the "dark fiber" installed at the turn of the millennium that now carries streaming video, are looking for huge data sets that require the powerful processing and tremendous storage capacity of the new infrastructure. And there is no better confluence than that provided by the impetus to rearticulate Communication Design Quarterly in an age of Big Data. The New York Times has been running articles about Big Data for some time:"Big data is all about exploration without preconceived notions."},
journal = {Commun. Des. Q. Rev},
month = {sep},
pages = {37–40},
numpages = {4}
}

@inproceedings{10.1145/2928294.2928297,
author = {Bortoli, Stefano and Bouquet, Paolo and Pompermaier, Flavio and Molinari, Andrea},
title = {Semantic Big Data for Tax Assessment},
year = {2016},
isbn = {9781450342995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2928294.2928297},
doi = {10.1145/2928294.2928297},
abstract = {Semantic Big Data is about the creation of new applications exploiting the richness and flexibility of declarative semantics combined with scalable and highly distributed data management systems. In this work, we present an application scenario in which a domain ontology, Open Refine and the Okkam Entity Name System enable a frictionless and scalable data integration process leading to a knowledge base for tax assessment. Further, we introduce the concept of Entiton as a flexible and efficient data model suitable for large scale data inference and analytic tasks. We successfully tested our data processing pipeline on a real world dataset, supporting ACI Informatica in the investigation for Vehicle Excise Duty (VED) evasion in Aosta Valley region (Italy). Besides useful business intelligence indicators, we implemented a distributed temporal inference engine to unveil VED evasion and circulation ban violations. The results of the integration are presented to the tax agents in a powerful Siren Solution KiBi dashboard, enabling seamless data exploration and business intelligence.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {5},
numpages = {6},
keywords = {tax assessment, semantic big data, entity name system, inference},
location = {San Francisco, California},
series = {SBD '16}
}

@inproceedings{10.1145/1077501.1077519,
author = {Pon, Raymond K. and C\'{a}rdenas, Alfonso F.},
title = {Data Quality Inference},
year = {2005},
isbn = {1595931600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1077501.1077519},
doi = {10.1145/1077501.1077519},
abstract = {In the field of sensor networks, data integration and collaboration, and intelligence gathering efforts, information on the quality of data sources are important but are often not available. We describe a technique to rank data sources by observing and comparing their behavior (i.e., the data produced by data sources) to rank. Intuitively, our measure characterizes data sources that agree with accurate or high-quality data sources as likely accurate. Furthermore, our measure includes a temporal component that takes into account a data source's past accuracy in evaluating its current accuracy. Initial experimental results based on simulation data to support our hypothesis demonstrate high precision and recall on identifying the most accurate data sources.},
booktitle = {Proceedings of the 2nd International Workshop on Information Quality in Information Systems},
pages = {105–111},
numpages = {7},
location = {Baltimore, Maryland},
series = {IQIS '05}
}

@article{10.1145/269012.269021,
author = {Tayi, Giri Kumar and Ballou, Donald P.},
title = {Examining Data Quality},
year = {1998},
issue_date = {Feb. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/269012.269021},
doi = {10.1145/269012.269021},
journal = {Commun. ACM},
month = {feb},
pages = {54–57},
numpages = {4}
}

@inproceedings{10.1145/2948992.2949000,
author = {Lopes, Cl\'{a}udio and Cabral, Bruno and Bernardino, Jorge},
title = {Personalization Using Big Data Analytics Platforms},
year = {2016},
isbn = {9781450340755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948992.2949000},
doi = {10.1145/2948992.2949000},
abstract = {Personalization can be defined as the customization of the outputs of a system based on the collected personal information of its users. Personalization techniques rely on user information, such as interests, preferences, geographic location, etc. The data being collected is used to create a profile, and improve the relevance of the outputs presented to the user. Google's search engine, or Facebook's suggestions are examples of personalization. This paper intent to provide an overview of the concept, and pretend to answer the question: Which level of personalization can Big Data Analytics Platforms support?},
booktitle = {Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering},
pages = {131–132},
numpages = {2},
keywords = {Big Data Analytics Platforms, Personalization, Big Data},
location = {Porto, Portugal},
series = {C3S2E '16}
}

@inproceedings{10.1145/2463676.2486082,
author = {Nazaruk, Alex and Rauchman, Michael},
title = {Big Data in Capital Markets},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2486082},
doi = {10.1145/2463676.2486082},
abstract = {Over the past decade global securities markets have dramatically changed. Evolution of market structure in combination with advances in computer technologies led to emergence of electronic securities trading. Securities transactions that used to be conducted in person and over the phone are now predominantly executed by automated trading systems. This resulted in significant fragmentation of the markets, vast increase in the exchange volumes and even greater increase in the number of orders.In this talk we present and analyze forces behind the wide proliferation of electronic securities trading in US stocks and options markets. We also make a high-level introduction into electronic securities market structure. We discuss trading objectives of different classes of market participants and analyze how their activity affects data volumes. We also present typical securities trading firm data flow and analyze various types of data it uses in its trading operations.We close with the implications this "sea change" has on DBMS requirements in capital markets.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {917–918},
numpages = {2},
keywords = {transactions, big data, capital markets, securities},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/3255001,
author = {Narayanan, Krish},
title = {Session Details: Big Data},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255001},
doi = {10.1145/3255001},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1145/2463676.2465338,
author = {Condie, Tyson and Mineiro, Paul and Polyzotis, Neoklis and Weimer, Markus},
title = {Machine Learning for Big Data},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465338},
doi = {10.1145/2463676.2465338},
abstract = {Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities.The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {939–942},
numpages = {4},
keywords = {big data, machine learning, databases},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2786451.2786482,
author = {Weber, Matthew S. and Nguyen, Hai},
title = {Big Data? Big Issues Degradation in Longitudinal Data and Implications for Social Sciences},
year = {2015},
isbn = {9781450336727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786451.2786482},
doi = {10.1145/2786451.2786482},
abstract = {This article analyzes the issue of degradation of data accuracy in large-scale longitudinal data sets. Recent research points to a number of issues with large-scale data, including problems of reliability, accuracy and quality over time. Simultaneously, large-scale data is increasingly being utilized in the social sciences. As scholars work to produce theoretically grounded research utilized "small-scale" methods, it is important for researchers to better understand the critical issues associated with the analysis of large-scale data. In order to illustrate the issues associated with this type of research, a case study analysis of archival Internet data is presented focusing on the issues of degradation of data accuracy over time. Suggestions for future studies are given.},
booktitle = {Proceedings of the ACM Web Science Conference},
articleno = {6},
numpages = {5},
keywords = {Keywords are your own designated keywords},
location = {Oxford, United Kingdom},
series = {WebSci '15}
}

@inproceedings{10.1145/2623330.2630811,
author = {Cormode, Graham and Duffield, Nick},
title = {Sampling for Big Data: A Tutorial},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2630811},
doi = {10.1145/2623330.2630811},
abstract = {One response to the proliferation of large datasets has been to develop ingenious ways to throw resources at the problem, using massive fault tolerant storage architectures, parallel and graphical computation models such as MapReduce, Pregel and Giraph. However, not all environments can support this scale of resources, and not all queries need an exact response. This motivates the use of sampling to generate summary datasets that support rapid queries, and prolong the useful life of the data in storage. To be effective, sampling must mediate the tensions between resource constraints, data characteristics, and the required query accuracy. The state-of-the-art in sampling goes far beyond simple uniform selection of elements, to maximize the usefulness of the resulting sample. This tutorial reviews progress in sample design for large datasets, including streaming and graph-structured data. Applications are discussed to sampling network traffic and social networks.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1975},
numpages = {1},
keywords = {random sampling},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{10.1145/2896825.2896837,
author = {Chen, Hong-Mei and Kazman, Rick and Garbajosa, Juan and Gonzalez, Eloy},
title = {Toward Big Data Value Engineering for Innovation},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896837},
doi = {10.1145/2896825.2896837},
abstract = {This article articulates the requirements for an effective big data value engineering method. It then presents a value discovery method, called Eco-ARCH (Eco-ARCHitecture), tightly integrated with the BDD (Big Data Design) method for addressing these requirements, filling a methodological void. Eco-ARCH promotes a fundamental shift in design thinking for big data system design -- from "bounded rationality" for problem solving to "expandable rationality" for design for innovation. The Eco-ARCH approach is most suitable for big data value engineering when system boundaries are fluid, requirements are ill-defined, many stakeholders are unknown and design goals are not provided, where no architecture pre-exists, where system behavior is non-deterministic and continuously evolving, and where co-creation with consumers and prosumers is essential to achieving innovation goals. The method was augmented and empirically validated in collaboration with an IT service company in the energy industry to generate a new business model that we call "eBay in the Grid".},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {44–50},
numpages = {7},
keywords = {architecture landscape, value discovery, value engineering, ecosystem, big data, energy industry, innovation},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@article{10.1145/2331042.2331054,
author = {Chen, Yanpei and Ferguson, Andrew and Martin, Brian and Wang, Andrew and Wendell, Patrick},
title = {Big Data and Internships at Cloudera},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331054},
doi = {10.1145/2331042.2331054},
abstract = {Students working in the big data space get uniquely valuable experiences and perspectives by taking industrial internships, which can help further their research agendas.},
journal = {XRDS},
month = {sep},
pages = {35–37},
numpages = {3}
}

@article{10.1145/2481244.2481247,
author = {Lin, Jimmy and Ryaboy, Dmitriy},
title = {Scaling Big Data Mining Infrastructure: The Twitter Experience},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/2481244.2481247},
doi = {10.1145/2481244.2481247},
abstract = {The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on "big data". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life "in the trenches" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall "big picture" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as "plumbing". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.},
journal = {SIGKDD Explor. Newsl.},
month = {apr},
pages = {6–19},
numpages = {14}
}

@inproceedings{10.5555/3191835.3191979,
author = {Onyejekwe, Egondu Rosemary and Onyejekwe, Egondu Rosemary},
title = {Big Data in Health Informatics Architecture: Health Informatics},
year = {2014},
isbn = {9781479958764},
publisher = {IEEE Press},
abstract = {this paper narrates the current status of Big Data in the healthcare industry and how the industry could derive big benefits from Big Data. The role of Big Data (and perhaps its analytics) in scripting a Health Informatics (HI) text book is, therefore, the focus of this paper. For starters, HI Architecture can be conceptualized through data collected by the survey instrument provided at the end.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {728–736},
numpages = {9},
keywords = {architecture, health informatics, big data, information technology, healthcare, big data analytics},
location = {Beijing, China},
series = {ASONAM '14}
}

@article{10.1145/3148238,
author = {Heinrich, Bernd and Hristova, Diana and Klier, Mathias and Schiller, Alexander and Szubartowicz, Michael},
title = {Requirements for Data Quality Metrics},
year = {2018},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3148238},
doi = {10.1145/3148238},
abstract = {Data quality and especially the assessment of data quality have been intensively discussed in research and practice alike. To support an economically oriented management of data quality and decision making under uncertainty, it is essential to assess the data quality level by means of well-founded metrics. However, if not adequately defined, these metrics can lead to wrong decisions and economic losses. Therefore, based on a decision-oriented framework, we present a set of five requirements for data quality metrics. These requirements are relevant for a metric that aims to support an economically oriented management of data quality and decision making under uncertainty. We further demonstrate the applicability and efficacy of these requirements by evaluating five data quality metrics for different data quality dimensions. Moreover, we discuss practical implications when applying the presented requirements.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {12},
numpages = {32},
keywords = {data quality assessment, requirements for metrics, Data quality, data quality metrics}
}

