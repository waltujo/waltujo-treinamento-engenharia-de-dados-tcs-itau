@inproceedings{10.1145/3412497,title = {The Transnational Happiness Study with Big Data Technology}, author = {Peng Lingxi , Liu Haohuai , Nie Yangang , Xie Ying , Tang Xuan , Luo Ping },year = {2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3412497}, doi = {10.1145/3412497}, abstract = {Happiness is a hot topic in academic circles. The study of happiness involves many disciplines, such as philosophy, psychology, sociology, and economics. However, there are few studies on the quantitative analysis of the factors affecting happiness. In this article, we used the well-known World Values Survey Wave 6 (WV6) dataset to quantitatively analyze the happiness of 57 countries with Big Data techniques. First, we obtained the seven most important factors by constructing happiness decision trees for each country. Calculating the frequencies of these factors, we obtained the 17 most important indicators for the prediction of happiness in the world. Then, we selected five representative countries, namely, Sweden, Japan, India, China, and the USA, and analyzed the indicators with the random forest method. We identified different patterns of factors that influence happiness in different countries. This study is a successful attempt to apply data mining technology in the social sciences, and the results are of practical significance.}, pages = {1\u201312}, numpages = {12}, keywords = {Big Data, happiness, feature selection, decision tree}}
@inproceedings{10.1145/2612669.2612702,title = {Deadline-aware scheduling of big-data processing jobs}, author = {Bodik Peter , Menache Ishai , Naor Joseph (Seffi) , Yaniv Jonathan },year = {2014}, isbn = {9781450328210}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2612669.2612702}, doi = {10.1145/2612669.2612702}, abstract = {This paper presents a novel algorithm for scheduling big data jobs on large compute clusters. In our model, each job is represented by a DAG consisting of several stages linked by precedence constraints. The resource allocation per stage is malleable, in the sense that the processing time of a stage depends on the resources allocated to it (the dependency can be arbitrary in general).The goal of the scheduler is to maximize the total value of completed jobs, where the value for each job depends on its completion time. We design an algorithm for the problem which guarantees an expected constant approximation factor when the cluster capacity is sufficiently high. To the best of our knowledge, this is the first constant-factor approximation algorithm for the problem. The algorithm is based on formulating the problem as a linear program and then rounding an optimal (fractional) solution into a feasible (integral) schedule using randomized rounding.}, location = {Prague, Czech Republic}, series = {SPAA '14}, pages = {211\u2013213}, numpages = {3}, keywords = {deadline-aware scheduling, scheduling algorithms, big data}}
@inproceedings{10.1145/2628194.2628251,title = {Survey of real-time processing systems for big data}, author = {Liu Xiufeng , Iftikhar Nadeem , Xie Xike },year = {2014}, isbn = {9781450326278}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2628194.2628251}, doi = {10.1145/2628194.2628251}, abstract = {In recent years, real-time processing and analytics systems for big data--in the context of Business Intelligence (BI)--have received a growing attention. The traditional BI platforms that perform regular updates on daily, weekly or monthly basis are no longer adequate to satisfy the fast-changing business environments. However, due to the nature of big data, it has become a challenge to achieve the real-time capability using the traditional technologies. The recent distributed computing technology, MapReduce, provides off-the-shelf high scalability that can significantly shorten the processing time for big data; Its open-source implementation such as Hadoop has become the de-facto standard for processing big data, however, Hadoop has the limitation of supporting real-time updates. The improvements in Hadoop for the real-time capability, and the other alternative real-time frameworks have been emerging in recent years. This paper presents a survey of the open source technologies that support big data processing in a real-time/near real-time fashion, including their system architectures and platforms.}, location = {Porto, Portugal}, series = {IDEAS '14}, pages = {356\u2013361}, numpages = {6}, keywords = {survey, big data, systems, real-time, architectures}}
@inproceedings{10.1145/2905055.2905202,title = {Agent based Security Model for Cloud Big Data}, author = {Kumar Sunil , Shekhar Jayant , Gupta Himanshu },year = {2016}, isbn = {9781450339629}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2905055.2905202}, doi = {10.1145/2905055.2905202}, abstract = {As we know that digitization is one of boon of 21st century technologies. With the massstorage of digital information and development of internet based technologies like cloud computing, researcher interest has been increased in Big Data and its security. The term Big Data refers to the huge amount of digital information. Actually, Big Data is not a fully new technology; but it is the expansion of data mining technique. In this paper, we propose an agent based security model for cloud big data. The main objective of this security model is to facilitate the IT companies in term of data protection; those are using Cloud Big Data for the analyzing purpose.}, location = {Udaipur, India}, series = {ICTCS '16}, pages = {1\u20135}, numpages = {5}, keywords = {Cloud Security, Big Data Security, Agent Based Security, NoSQL Security}}
@inproceedings{10.1145/956750.956844,title = {Data quality through knowledge engineering}, author = {Dasu Tamraparni , Vesonder Gregg T. , Wright Jon R. },year = {2003}, isbn = {1581137370}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/956750.956844}, doi = {10.1145/956750.956844}, abstract = {Traditionally, data quality programs have acted as a preprocessing stage to make data suitable for a data mining or analysis operation. Recently, data quality concepts have been applied to databases that support business operations such as provisioning and billing. Incorporating business rules that drive operations and their associated data processes is critically important to the success of such projects. However, there are many practical complications. For example, documentation on business rules is often meager. Rules change frequently. Domain knowledge is often fragmented across experts, and those experts do not always agree. Typically, rules have to be gathered from subject matter experts iteratively, and are discovered out of logical or procedural sequence, like a jigsaw puzzle. Our approach is to impement business rules as constraints on data in a classical expert system formalism sometimes called production rules. Our system works by allowing good data to pass through a system of constraints unchecked. Bad data violate constraints and are flagged, and then fed back after correction. Constraints are added incrementally as better understanding of the business rules is gained. We include a real-life case study.}, location = {Washington, D.C.}, series = {KDD '03}, pages = {705\u2013710}, numpages = {6}, keywords = {static and dynamic constraints, business operations databases, data quality}}
@inproceedings{10.1145/3379247.3379298,title = {Application Analysis of Big Data Technology in Feeding Industry}, author = {Zhai Chenggong , Liu Nan , Li Jianxiang },year = {2020}, isbn = {9781450376730}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3379247.3379298}, doi = {10.1145/3379247.3379298}, abstract = {In recent years, our army attaches great importance to the application of big data technology in the field of military logistics. With the concern of the head of the CMC, relevant departments actively deploy the application of big data technology. At present, big data technology is only applied in some units of the military feeding Industry, which is in the initial stage and has a huge development space. This paper mainly introduces the application status of big data technology, the application ideas and related supporting measures in the feeding Industry.}, location = {Sanya, China}, series = {ICCDE 2020}, pages = {142\u2013146}, numpages = {5}, keywords = {Business Process Reengineering, big data, Feeding Industry}}
@inproceedings{10.1145/3545801,title = {Proceedings of the 7th International Conference on Big Data and Computing},year = {2022}, isbn = {9781450396097}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Shenzhen, China}}
@inproceedings{10.1145/3006299.3006310,title = {Neighborhood features help detecting non-technical losses in big data sets}, author = {Glauner Patrick , Meira Jorge Augusto , Dolberg Lautaro , State Radu , Bettinger Franck , Rangoni Yves },year = {2016}, isbn = {9781450346177}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3006299.3006310}, doi = {10.1145/3006299.3006310}, abstract = {Electricity theft occurs around the world in both developed and developing countries and may range up to 40% of the total electricity distributed. More generally, electricity theft belongs to non-technical losses (NTL), which occur during the distribution of electricity in power grids. In this paper, we build features from the neighborhood of customers. We first split the area in which the customers are located into grids of different sizes. For each grid cell we then compute the proportion of inspected customers and the proportion of NTL found among the inspected customers. We then analyze the distributions of features generated and show why they are useful to predict NTL. In addition, we compute features from the consumption time series of customers. We also use master data features of customers, such as their customer class and voltage of their connection. We compute these features for a Big Data base of 31M meter readings, 700K customers and 400K inspection results. We then use these features to train four machine learning algorithms that are particularly suitable for Big Data sets because of their parallelizable structure: logistic regression, k-nearest neighbors, linear support vector machine and random forest. Using the neighborhood features instead of only analyzing the time series has resulted in appreciable results for Big Data sets for varying NTL proportions of 1%-90%. This work can therefore be deployed to a wide range of different regions.}, location = {Shanghai, China}, series = {BDCAT '16}, pages = {253\u2013261}, numpages = {9}, keywords = {electricity theft detection, machine learning, feature selection, non-technical losses, data mining, time series classification, feature engineering}}
@inproceedings{10.1145/3524383.3524393,title = {Revision of the child and adolescent COVID-19 stressors scale and Big Data-Based Analysis of Disparities in Urban and Rural Areas}, author = {Liang Mingming , Zhang Yuqing },year = {2022}, isbn = {9781450395793}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3524383.3524393}, doi = {10.1145/3524383.3524393}, abstract = {The COVID-19 pandemic imposes a tremendous burden upon society. Several studies have documented stressors and fears of COVID-19 for adult populations, but few studies pay attention to the COVID-19 stressors on children and adolescents. Assessing the stressors of COVID-19 on children and adolescents can provide the basis for interventions to bring children and adolescents' mental health \"out of the shadows.\" Entering the Era of \"Big Data,\" the psychological state can be assessed through integrative analysis of data. This study adopted a whole-group sampling method. After a new round of the COVID-19 epidemic caused by imported cases in Jiangsu and Fujian provinces of China, self-report questionnaires were sent to children and adolescents aged 10\u201318 years. 1815 valid questionnaires were collected. Data analysis was performed using SPSS and AMOS software (version 26). To revise and test the reliability and validity of the COVID-19 stressors scale for children and adolescents, as well as to investigate the differences in stressors between rural and urban based\u00a0on\u00a0Big-Data\u00a0Mining. The results of this study indicate that the revised COVID-19 stressors scale, which includes a four-factor model of disease stressors, information stressors, measure stressors, and environmental stressors, has good reliability and validity for children and adolescents aged 10\u201318 years in a Chinese context. Big data-based demographic analysis showed that children and adolescents living in urban areas were generally less stressed about the COVID-19 epidemic than in rural areas.}, location = {Shanghai, China}, series = {ICBDE '22}, pages = {387\u2013392}, numpages = {6}, keywords = {children, COVID-19 stressors, adolescents, big data}}
@inproceedings{10.1145/3195106.3195172,title = {Big Data and Open Government Data in Public Services}, author = {Anshari Muhammad , Almunawar Mohammad Nabil , Lim Syamimi Ariff },year = {2018}, isbn = {9781450363532}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3195106.3195172}, doi = {10.1145/3195106.3195172}, abstract = {Big data is a relatively new approach in managing and analyzing a huge amount of dynamic data to discover useful information and knowledge. Even though big data is still in its infancy, it has been benefiting private and public organizations in large scale. Since public sector organizations have to maintain large amount of data from several domains, to utilize big data it is crucial that any challenges and issues faced in adapting big data to be addressed. This paper discusses the benefits and risks for accommodating big data and open government data for public services.}, location = {Macau, China}, series = {ICMLC 2018}, pages = {140\u2013144}, numpages = {5}, keywords = {Big data, Open Data, Open Government Data, Public Organization}}
@inproceedings{10.1145/3436829.3436838,title = {Performance Analysis of Twofish Cryptography Algorithm in Big Data}, author = {Maata Rolou Lyn R. , Cordova Ronald S. , Halibas Alrence },year = {2020}, isbn = {9781450377218}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3436829.3436838}, doi = {10.1145/3436829.3436838}, abstract = {Information security in big data plays a vital role in today's modern era of computing. It has become significant issue due to the popularity of Internet, free access of internet and data, online businesses, and communication technologies that have been emerged tremendously, making them a potential computer security threats. In order to overcome these threats, modern data communication uses cryptography as a technique to secure big data transmission efficiently and effectively. This paper aims to demonstrate the process of encryption and decryption of different big datasets and compare its results in terms of message size and time. There are seven (7) different big data files that have been loaded in a Java Netbeans twosifh algorithm program that includes app store, interactions train, border crossing, PP users, PP recipes, raw recipes and raw interactions for simulation purposes. The main purpose of simulation is to record the accuracy and efficiency of big data files used. The results of the simulation were recorded, compared, and analyzed to create valuable contribution to information security.}, location = {Cairo, Egypt}, series = {ICSIE 2020}, pages = {56\u201360}, numpages = {5}, keywords = {Big data, twofish algorithm, computer security}}
@inproceedings{10.1145/3209415.3209479,title = {Bridging Big Data and Policy Making: A case study of failure}, author = {Kudo Hiroko },year = {2018}, isbn = {9781450354219}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3209415.3209479}, doi = {10.1145/3209415.3209479}, abstract = {This paper investigates into a question through a failure case. The research question is; why we often fail to use the existing information and knowledge, including big data to design and/or implement public policies? The case is; the ticketing data, which was collected during the London Olympic Games and its so far under usage to design public policy related to health, well-being, and physical activities of the citizens. The research adopts qualitative analysis, including analysis of primary documents and semi-directive interviews. There is a limitation of single case study: however the case well represents the research question to provide preliminary investigation and to generate hypotheses for further studies.}, location = {Galway, Ireland}, series = {ICEGOV '18}, pages = {609\u2013615}, numpages = {7}, keywords = {Big Data, policy design, government policy, government failure, evidence-based policy making}}
@inproceedings{10.1145/3524383.3524401,title = {Research on the Long-term Mechanism Construction of College Students' \u201cHeart-to-heart Talk Education\u201d in the Era of Big Data}, author = {Zhang Xiaoling , Ren Weixin },year = {2022}, isbn = {9781450395793}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3524383.3524401}, doi = {10.1145/3524383.3524401}, abstract = {Heart-to-heart talk education evolved from the dialogue teaching method of Confucius and heuristic teaching method (\u201cmidwifery\u201d) of Socrates. With the increase of academic and psychological pressure of college students, heart-to-heart talk education has become an important way for colleges and universities to carry out ideological education and emotional counseling. And with the in-depth development of big data, the effectiveness and scientific nature of heart-to-heart talk education have been significantly improved. On the other hand, conducting heart-to-heart talk education with the help of big data technology also faces more technical and ethical challenges. Investigation and analysis shows that there are still problems such as lag, singleness, and randomness in heart-to-heart talks in the context of big data. To solve this problem, this paper proposes countermeasures from four aspects: platform construction, team building, application process, and data supervision to build a long-term mechanism for college students' heart-to-heart talks, which is of great significance for promoting the in-depth application of big data in the field of education and the innovative development of heart-to-heart talk education method.}, location = {Shanghai, China}, series = {ICBDE '22}, pages = {189\u2013193}, numpages = {5}, keywords = {Big data, College students;Heart-to-heart talk education, Long-term mechanism}}
@inproceedings{10.1145/3312714.3312728,title = {IoT Big Data Value Map: How to Generate Value from IoT Data}, author = {Hajiheydari Nastaran , Talafidaryani Mojtaba , Khabiri SeyedHossein },year = {2019}, isbn = {9781450362351}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3312714.3312728}, doi = {10.1145/3312714.3312728}, abstract = {Huge sources of business value are emerging due to big data generated by the Internet of Things (IoT) technologies paired with Machine Learning (ML) and Data Mining (DM) techniques' ability to harness and extract hidden knowledge from data and consequently learning and improving spontaneously. This paper reviews different examples of analyzing big data generated through IoT in previous studies and in various domains; then claims their business Value Proposition Map deploying Value Proposition Canvas as a novel conceptual tool. As a result, the proposed unprecedented framework of this paper entitled \"IoT Big Data Value Map\" shows a roadmap from raw data to real-world business value creation, blossomed out of a kind of three-pillar structure: IoT, Data Mining, and Value Proposition Map. The result of this study paves the way for prototyping business models in this field based on value invention from huge data analysis generated by IoT devices in different industries. Furthermore, researchers may complete this map by associating proposed framework with potential customers' profile and their expectations.}, location = {Vienna, Austria}, series = {ICSLT '19}, pages = {98\u2013103}, numpages = {6}, keywords = {Internet of Things (IoT), IoT Big Data Value Map, Data Mining, Value Proposition Map, Value Proposition Canvas}}
@inproceedings{10.1145/2623330.2623615,title = {Scaling out big data missing value imputations: pythia vs. godzilla}, author = {Anagnostopoulos Christos , Triantafillou Peter },year = {2014}, isbn = {9781450329569}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2623330.2623615}, doi = {10.1145/2623330.2623615}, abstract = {Solving the missing-value (MV) problem with small estimation errors in big data environments is a notoriously resource-demanding task. As datasets and their user community continuously grow, the problem can only be exacerbated. Assume that it is possible to have a single machine (`Godzilla'), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which accesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to access only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla's imputation estimation errors? In this paper we derive answers to these fundamentals questions and develop principled methods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pillars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive experimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.}, location = {New York, New York, USA}, series = {KDD '14}, pages = {651\u2013660}, numpages = {10}, keywords = {clustering, missing value, big data}}
@inproceedings{10.1145/3440054,title = {2020 2nd International Conference on Big-data Service and Intelligent Computation},year = {2020}, isbn = {9781450388399}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Xiamen, China}}
@inproceedings{10.1145/3546632.3546889,title = {Research on Security Mechanism of Hadoop Big Data Platform}, author = {Gu Wenjie , Jia Shuangying },year = {2022}, isbn = {9781450396363}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3546632.3546889}, doi = {10.1145/3546632.3546889}, abstract = {As a virtualized resource realization mode, Hadoop cloud platform has become an open-source cloud computing architecture and big data analysis platform. The platform plays a pivotal role in the information field, but the security mechanism of the Hadoop cloud computing platform is relatively fragile. Users are concerned about privacy leakage using which severely restricts the application rate of Hadoop cloud computing platform. This paper conducts research and analysis on big data security and privacy protection technology under cloud computing, and proposes a strategy based on optimizing Hadoop security mechanism, from Hadoop platform identity authentication, Hadoop platform access authorization, Hadoop platform big data encryption security integrated mechanism to architecture, and implement the Hadoop platform, A big data security strategy of Hadoop provides technical support for enterprise applications with security requirements on the Hadoop platform.}, location = {Nanchang, China}, series = {CIUP '22}, pages = {115\u2013118}, numpages = {4}, keywords = {Big data, security mechanism, cloud computing, Hadoop platform}}
@inproceedings{10.1145/3502300,title = {2021 3rd International Conference on Big-data Service and Intelligent Computation},year = {2021}, isbn = {9781450390552}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Xiamen, China}}
@inproceedings{10.1145/2642937.2643006,title = {Program analysis for secure big data processing}, author = {James Stephen Julian , Savvides Savvas , Seidel Russell , Eugster Patrick },year = {2014}, isbn = {9781450330138}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2642937.2643006}, doi = {10.1145/2642937.2643006}, abstract = {The ubiquitous nature of computers is driving a massive increase in the amount of data generated by humans and machines. Two natural consequences of this are the increased efforts to (a) derive meaningful information from accumulated data and (b) ensure that data is not used for unintended purposes. In the direction of analyzing massive amounts of data (a.), tools like MapReduce, Spark, Dryad and higher level scripting languages like Pig Latin and DryadLINQ have significantly improved corresponding tasks for software developers. The second, but equally important aspect of ensuring confidentiality (b.), has seen little support emerge for programmers: while advances in cryptographic techniques allow us to process directly on encrypted data, programmer-friendly and efficient ways of programming such data analysis jobs are still missing. This paper presents novel data flow analyses and program transformations for Pig Latin, that automatically enable the execution of corresponding scripts on encrypted data. We avoid fully homomorphic encryption because of its prohibitively high cost; instead, in some cases, we rely on a minimal set of operations performed by the client. We present the algorithms used for this translation, and empirically demonstrate the practical performance of our approach as well as improvements for programmers in terms of the effort required to preserve data confidentiality.}, location = {Vasteras, Sweden}, series = {ASE '14}, pages = {277\u2013288}, numpages = {12}, keywords = {privacy, big data, cloud computing}}
@inproceedings{10.1145/3337065,title = {Big Data Analytics for Large-scale Wireless Networks: Challenges and Opportunities}, author = {Dai Hong-Ning , Wong Raymond Chi-Wing , Wang Hao , Zheng Zibin , Vasilakos Athanasios V. },year = {2019}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3337065}, doi = {10.1145/3337065}, abstract = {The wide proliferation of various wireless communication systems and wireless devices has led to the arrival of big data era in large-scale wireless networks. Big data of large-scale wireless networks has the key features of wide variety, high volume, real-time velocity, and huge value leading to the unique research challenges that are different from existing computing systems. In this article, we present a survey of the state-of-art big data analytics (BDA) approaches for large-scale wireless networks. In particular, we categorize the life cycle of BDA into four consecutive stages: Data Acquisition, Data Preprocessing, Data Storage, and Data Analytics. We then present a detailed survey of the technical solutions to the challenges in BDA for large-scale wireless networks according to each stage in the life cycle of BDA. Moreover, we discuss the open research issues and outline the future directions in this promising area.}, pages = {1\u201336}, numpages = {36}, keywords = {wireless networks, machine learning, Big data}}
@inproceedings{10.1145/2660168.2660187,title = {Big Data for Musicology}, author = {Weyde Tillman , Cottrell Stephen , Dykes Jason , Benetos Emmanouil , Wolff Daniel , Tidhar Dan , Kachkaev Alexander , Plumbley Mark , Dixon Simon , Barthet Mathieu , Gold Nicolas , Abdallah Samer , Alancar-Brayner Aquiles , Mahey Mahendra , Tovell Adam },year = {2014}, isbn = {9781450330022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2660168.2660187}, doi = {10.1145/2660168.2660187}, abstract = {Digital music libraries and collections are growing quickly and are increasingly made available for research. We argue that the use of large data collections will enable a better understanding of music performance and music in general, which will benefit areas such as music search and recommendation, music archiving and indexing, music production and education. However, to achieve these goals it is necessary to develop new musicological research methods, to create and adapt the necessary technological infrastructure, and to find ways of working with legal limitations. Most of the necessary basic technologies exist, but they need to be brought together and applied to musicology. We aim to address these challenges in the Digital Music Lab project, and we feel that with suitable methods and technology Big Music Data can provide new opportunities to musicology.}, location = {London, United Kingdom}, series = {DLfM '14}, pages = {1\u20133}, numpages = {3}}
@inproceedings{10.1145/2592267,title = {Big data, diminished design?}, author = {Bean Jonathan , Rosner Daniela },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2592267}, doi = {10.1145/2592267}, pages = {18\u201319}, numpages = {2}}
@inproceedings{10.1145/3185768.3186300,title = {ABench: Big Data Architecture Stack Benchmark}, author = {Ivanov Todor , Singhal Rekha },year = {2018}, isbn = {9781450356299}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3185768.3186300}, doi = {10.1145/3185768.3186300}, abstract = {Distributed big data processing and analytics applications demand a comprehensive end-to-end architecture stack consisting of big data technologies. However, there are many possible architecture patterns (e.g. Lambda, Kappa or Pipeline architectures) to choose from when implementing the application requirements. A big data technology in isolation may be best performing for a particular application, but its performance in connection with other technologies depends on the connectors and the environment. Similarly, existing big data benchmarks evaluate the performance of different technologies in isolation, but no work has been done on benchmarking big data architecture stacks as a whole. For example, BigBench (TPCx-BB) may be used to evaluate the performance of Spark, but is it applicable to PySpark or to Spark with Kafka stack as well? What is the impact of having different programming environments and/or any other technology like Spark? This vision paper proposes a new category of benchmark, called ABench, to fill this gap and discusses key aspects necessary for the performance evaluation of different big data architecture stacks.}, location = {Berlin, Germany}, series = {ICPE '18}, pages = {13\u201316}, numpages = {4}, keywords = {big data, ABench, big data benchmarking, bigbench}}
@inproceedings{10.1145/3205651.3205701,title = {A distributed dendritic cell algorithm for big data}, author = {Dagdia Zaineb Chelly },year = {2018}, isbn = {9781450357647}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3205651.3205701}, doi = {10.1145/3205651.3205701}, abstract = {In this work, we focus on the Dendritic Cell Algorithm (DCA), a bio-inspired classifier, and its limitation when coping with very large datasets. To overcome this limitation, we propose a novel distributed DCA version for data classification based on the MapReduce framework to distribute the functioning of this algorithm through a cluster of computing elements. Our experimental results show that our proposed distributed solution is suitable to enhance the performance of the DCA enabling the algorithm to be applied over big data classification problems.}, location = {Kyoto, Japan}, series = {GECCO '18}, pages = {103\u2013104}, numpages = {2}, keywords = {dendritic cell algorithm, scalability, classification, big data, distributed processing}}
@inproceedings{10.1145/3264560,title = {Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing},year = {2018}, isbn = {9781450364744}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {ICCBDC 2018 and ICIIP 2018 provide a scientific platform for both local and international scientists, engineers and technologists who work in all aspects of cloud and big data computing, and intelligent information processing. In addition to the contributed papers, Prof. Hong Zhu from Oxford Brookes University, UK, Assoc. Prof. Huseyin Seker form the University of Northumbria at Newcastle, UK and Prof. Alfredo Cuzzocrea form University of Trieste, Italy were invited to deliver keynote speeches at ICCBDC 2018 2018 and ICIIP 2018.}, location = {Barcelona, Spain}}
@inproceedings{10.1145/3340531.3412173,title = {IoT Data Quality}, author = {Song Shaoxu , Zhang Aoqian },year = {2020}, isbn = {9781450368599}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3340531.3412173}, doi = {10.1145/3340531.3412173}, abstract = {Data quality issues have been widely recognized in IoT data, and prevent the downstream applications. In this tutorial, we review the state-of-the-art techniques for IoT data quality management. In particular, we discuss how the dedicated approaches improve various data quality dimensions, including validity, completeness and consistency. Among others, we further highlight the recent advances by deep learning techniques for IoT data quality. Finally, we indicate the open problems in IoT data quality management, such as benchmark or interpretation of data quality issues.}, location = {Virtual Event, Ireland}, series = {CIKM '20}, pages = {3517\u20133518}, numpages = {2}, keywords = {internet of things, data curation}}
@inproceedings{10.1145/3524383,title = {Proceedings of the 5th International Conference on Big Data and Education},year = {2022}, isbn = {9781450395793}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Shanghai, China}}
@inproceedings{10.1145/3507485.3507487,title = {The Importance of Big Data Visualisations for Auditors\u2019 Decisions}, author = {Eltweri Ahmed , Faccia Alessio , Sawan Nedal },year = {2021}, isbn = {9781450385831}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3507485.3507487}, doi = {10.1145/3507485.3507487}, abstract = {Many achievements using big data are recorded in the vast majority of industrial sectors, accounting is no different, so big data is ubiquitous. The presence of big data within the practice of auditing is still at an early stage. A reliance upon technological tools, however, has resulted in the implementation of computer-assisted auditing techniques. This study, then, based on qualitative analyses, highlights how big data visualisations may assist the evaluation of auditors\u2019 evidence so that data can be retrieved in ways that help produce professional judgments.}, location = {Osaka, Japan}, series = {ICSEB 2021}, pages = {7\u201312}, numpages = {6}, keywords = {Data Visualizations, Auditing Profession, Big Data, IT, Auditor Judgment}}
@inproceedings{10.1145/3158341,title = {Technology and Business Challenges of Big Data in the Digital Economy: Big Data (Ubiquity symposium)}, author = {Penkler Dave },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3158341}, doi = {10.1145/3158341}, abstract = {The early digital economy during the dot-com days of internet commerce successfully faced its first big data challenges of click-stream analysis with map-reduce technology. Since then the digital economy has been becoming much more pervasive. As the digital economy evolves, looking to benefit from its burgeoning big data assets, an important technical-business challenge is emerging: How to acquire, store, access, and exploit the data at a cost that is lower than the incremental revenue or GDP that its exploitation generates. Especially now that efficiency increases, which lasted for 50 years thanks to improvements in semiconductor manufacturing, is slowing and coming to an end.}, pages = {1\u20139}, numpages = {9}}
@inproceedings{10.1145/3152723.3152728,title = {Quantitative Study on Commercial Street Vibrancy Based on Big Data: A Case of Dalian}, author = {Xiao Yan , Fan Xixuan , Dong Li },year = {2017}, isbn = {9781450353564}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3152723.3152728}, doi = {10.1145/3152723.3152728}, abstract = {Street as traffic carrier and public space of a city are playing increasingly important role in daily city life. The paper quantitatively explores vibrancy of Commercial Street, while referring to urban space syntax analysis method, taking Dalian as an example. The paper also explores the factors influencing the dynamic commercial block space based on poi, function density and function diversity, so as to provide theoretical guidance for the planning and design of the commercial block.}, location = {Osaka, Japan}, series = {ICBDR 2017}, pages = {7\u201311}, numpages = {5}, keywords = {Function Density, Point of Interest, Data Augmented Design, Function Diversity, Commercial Street}}
@inproceedings{10.1145/3178315.3178323,title = {Requirements Engineering in the Context of Big Data Applications}, author = {Arruda Darlan },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3178315.3178323}, doi = {10.1145/3178315.3178323}, abstract = {Requirements Engineering (RE) plays an essential role in the software engineering process, being considered as one of the most critical phases of the software development life-cycle. As we might expect, then, the Requirements Engineering would play a similar role in the context of Big Data applications. However, practicing Requirements Engineering is a challenging and complex task. It involves (i) stakeholders with diverse backgrounds and levels of knowledge, (ii) different application domains, (iii) it is expensive and error-prone, (iii) it is important to be aligned with business goals, to name a few. Because it involves such complex activities, a lot has to be understood in order to properly address Requirements Engineering. Especially, when the technology domain (e.g., Big Data) is not yet well explored. In this context, this paper describes a research plan on Requirements Engineering involving the development of Big Data applications. The high-level goal is to investigate: (i) On the technical front, the Requirements Engineering activities with respect to the analysis and specification of Big Data requirements and, (ii) on the management side, the relationship between RE and Business Goals in the development of Big Data Software applications.}, pages = {1\u20136}, numpages = {6}, keywords = {big data applications, big data requirements engineering, business goals, empirical software engineering., empirical studies}}
@inproceedings{10.1145/3305275,title = {Proceedings of the International Symposium on Big Data and Artificial Intelligence},year = {2018}, isbn = {9781450365703}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Hong Kong, Hong Kong}}
@inproceedings{10.1145/2756406.2756924,title = {Towards Use And Reuse Driven Big Data Management}, author = {Xie Zhiwu , Chen Yinlin , Speer Julie , Walters Tyler , Tarazaga Pablo A. , Kasarda Mary },year = {2015}, isbn = {9781450335942}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2756406.2756924}, doi = {10.1145/2756406.2756924}, abstract = {We propose a use and reuse driven big data management approach that fuses the data repository and data processing capabilities in a co-located, public cloud. It answers to the urgent data management needs from the growing number of researchers who don't fit in the big science/small science dichotomy. This approach will allow researchers to more easily use, manage, and collaborate around big data sets, as well as give librarians the opportunity to work alongside the researchers to preserve and curate data while it is still fresh and being actively used. This also provides the technological foundation to foster a sharing culture more aligned with the open source software development paradigm than the lone-wolf, gift-exchanging small science sharing or the top-down, highly structured big science sharing. To materialize this vision, we provide a system architecture consisting of a scalable digital repository system coupled with the co-located cloud storage and cloud computing, as well as a job scheduler and a deployment management system. Motivated by Virginia Tech's Goodwin Hall instrumentation project, we implemented and evaluated a prototype. The results show not only sufficient capacities for this particular case, but also near perfect linear storage and data processing scalabilities under moderately high workload.}, location = {Knoxville, Tennessee, USA}, series = {JCDL '15}, pages = {65\u201374}, numpages = {10}, keywords = {sensor data, cloud computing, digital repository, smart infrastructure, big data, digital library}}
@inproceedings{10.1145/3291801.3291816,title = {Big Data Framework for Abnormal Vessel Trajectories Detection using Adaptive Kernel Density Estimation}, author = {Sidib\u00e9 Abdoulaye , Shu Gao , Ma Yunzhao , Wanqi Wei },year = {2018}, isbn = {9781450364768}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3291801.3291816}, doi = {10.1145/3291801.3291816}, abstract = {In the maintaining of inland navigational safety and security, the automatic detection of anomalous vessels trajectory behavior from a large amount of vessels traffic datasets produced by automatic identification systems (AIS) is an interesting task, and in another hand constitutes a challenge because of the size of the AIS data. In this paper we propose a new big data framework based on the Adaptive Kernel Density Estimation (AKDE) method, for abnormal vessel trajectory detection using Apache Spark. In the proposed framework, first, the water area is divided into space partitions. Second, on the Apache Spark distributed computing platform, the AKDE method is used to build in a parallel manner local models of normal vessels trajectory data for space partitions, as probability density functions (PDFs). Finally, the detection of abnormal trajectory data point is performed based on the corresponding built local models, by sequentially checking the real incoming trajectory data point. And a trajectory with a user-defined number of abnormal data points is considered to be an abnormal trajectory. In addition, we discuss the main features and some limitations of the proposed framework.}, location = {Weihai, China}, series = {ICBDR 2018}, pages = {43\u201346}, numpages = {4}, keywords = {Kernel Density Estimation, Apache Spark, AIS, Anomaly detection, Vessel Trajectory}}
@inproceedings{10.1145/2611567,title = {Big data and its technical challenges}, author = {Jagadish H. V. , Gehrke Johannes , Labrinidis Alexandros , Papakonstantinou Yannis , Patel Jignesh M. , Ramakrishnan Raghu , Shahabi Cyrus },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2611567}, doi = {10.1145/2611567}, abstract = {Exploring the inherent technical challenges in realizing the potential of Big Data.}, pages = {86\u201394}, numpages = {9}}
@inproceedings{10.1145/2627770.2627773,title = {Introducing Data Connectivity in a Big Data Web}, author = {Chatziantoniou Damianos , Tselai Florents },year = {2014}, isbn = {9781450329972}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2627770.2627773}, doi = {10.1145/2627770.2627773}, abstract = {Until recently, when relational systems was the main data management option and SQL the de facto language for querying/analyzing data, ODBC was an excellent API for applications to interact with the data provider. Standardization of data retrieval has helped innovation and productivity, allowing application developers to focus on the core of their ideas. However, the big data era added variety to all aspects of data facilitation: variety in data management options, variety in data formats, variety in querying/analyzing tasks. In this chaotic situation, standardizing data connectivity is more important than ever. What should be the replacement of ODBC? In this paper, we propose ODMC (Open Data Management Connectivity), a client-server protocol between data management entities (DMEs). A DME is anything that manages/manipulates data. In that respect, spreadsheets, java programs, Hadoop, RDBMs, stream engines, NoSQL, etc., all act as DMEs. In addition, there is no distinction between applications and data management servers, as in ODBC. A DME can be a data consumer in an ODMC instance and a data producer in another. This composability principle allows for the definition of analysis workflows. We present a preliminary implementation of ODMC for python-based DMEs. We argue that ODMC is simple, intuitive, scalable and suitable for both persistent and stream data.}, location = {Snowbird, UT, USA}, series = {DanaC'14}, pages = {1\u20134}, numpages = {4}, keywords = {ODMC, data connectivity, big data integration, big data interoperability, open data management connectivity, big data web}}
@inproceedings{10.1145/2756406.2756943,title = {Big Data Text Summarization for Events: A Problem Based Learning Course}, author = {Kanan Tarek , Zhang Xuan , Magdy Mohamed , Fox Edward },year = {2015}, isbn = {9781450335942}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2756406.2756943}, doi = {10.1145/2756406.2756943}, abstract = {Problem/project Based Learning (PBL) is a highly effective student-centered teaching method, where student teams learn by solving problems. This paper describes an instance of PBL applied to digital library education. We show the design, implementation, results, and partial evaluation of a Computational Linguistics course that provides students an opportunity to engage in active learning about adding value to digital libraries with large collections of text, i.e., one aspect of \"big data.\" Students are engaging in PBL with the semester long challenge of generating good English summaries of an event, given a large collection from our webpage archives. Six teams, each working with a different type of event, and applying three different summarization methods, learned how to generate good summaries; these have fair precision relative to the Wikipedia page that describes their event.}, location = {Knoxville, Tennessee, USA}, series = {JCDL '15}, pages = {87\u201390}, numpages = {4}, keywords = {text summarization, big data, digital libraries, computational linguistics, problem based learning}}
@inproceedings{10.1145/3097983.3098179,title = {Distributed Local Outlier Detection in Big Data}, author = {Yan Yizhou , Cao Lei , Kulhman Caitlin , Rundensteiner Elke },year = {2017}, isbn = {9781450348874}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3097983.3098179}, doi = {10.1145/3097983.3098179}, abstract = {In this work, we present the first distributed solution for the Local Outlier Factor (LOF) method -- a popular outlier detection technique shown to be very effective for datasets with skewed distributions. As datasets increase radically in size, highly scalable LOF algorithms leveraging modern distributed infrastructures are required. This poses significant challenges due to the complexity of the LOF definition, and a lack of access to the entire dataset at any individual compute machine. Our solution features a distributed LOF pipeline framework, called DLOF. Each stage of the LOF computation is conducted in a fully distributed fashion by leveraging our invariant observation for intermediate value management. Furthermore, we propose a data assignment strategy which ensures that each machine is self-sufficient in all stages of the LOF pipeline, while minimizing the number of data replicas. Based on the convergence property derived from analyzing this strategy in the context of real world datasets, we introduce a number of data-driven optimization strategies. These strategies not only minimize the computation costs within each stage, but also eliminate unnecessary communication costs by aggressively pushing the LOF computation into the early stages of the DLOF pipeline. Our comprehensive experimental study using both real and synthetic datasets confirms the efficiency and scalability of our approach to terabyte level data.}, location = {Halifax, NS, Canada}, series = {KDD '17}, pages = {1225\u20131234}, numpages = {10}, keywords = {local outlier, big data, distributed processing}}
@inproceedings{10.1109/BDC.2014.18,title = {Big Data Staging with MPI-IO for Interactive X-ray Science}, author = {Wozniak Justin M. , Sharma Hemant , Armstrong Timothy G. , Wilde Michael , Almer Jonathan D. , Foster Ian },year = {2014}, isbn = {9781479918973}, publisher = {IEEE Computer Society}, address = {USA}, url = {https://doi.org/10.1109/BDC.2014.18}, doi = {10.1109/BDC.2014.18}, abstract = {New techniques in X-ray scattering science experiments produce large data sets that can require millions of high-performance processing hours per week of computation for analysis. In such applications, data is typically moved from X-ray detectors to a large parallel file system shared by all nodes of a peta scale supercomputer and then is read repeatedly as different science application tasks proceed. However, this straightforward implementation causes significant contention in the file system. We propose an alternative approach in which data is instead staged into and cached in compute node memory for extended periods, during which time various processing tasks may efficiently access it. We describe here such a big data staging framework, based on MPI-IO and the Swift parallel scripting language. We discuss a range of large-scale data management issues involved in X-ray scattering science and measure the performance benefits of the new staging framework for high-energy diffraction microscopy, an important emerging application in data-intensive X-ray scattering. We show that our framework accelerates scientific processing turnaround from three months to under 10 minutes, and that our I/O technique reduces input overheads by a factor of 5 on 8K Blue Gene/Q nodes.}, series = {BDC '14}, pages = {26\u201334}, numpages = {9}, keywords = {MPI-IO, Swift, X-ray, Blue Gene, Broadcast, scattering, MPI}}
@inproceedings{10.1145/3154943.3154946,title = {Big data analysis of local business and reviews}, author = {Singh Ruchi , Ananth Yashaswi , Woo Dr. Jongwook },year = {2017}, isbn = {9781450353120}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3154943.3154946}, doi = {10.1145/3154943.3154946}, abstract = {In this paper, we have analyzed the local business data and reviews to get insights on the popularity of a business and factors responsible for it. We have also analyzed the sentiments of the customer reviews for better understanding of business popularity. The total size of the dataset is 180 MB and we have adopted cloud computing service like Big Data Hadoop using HiveQL and Pig for query. Our preferred choice of framework for this project was Big Data Hadoop primarily because it is an open source software and its scalability and flexibility best suited our requirements.This project has helped us in understanding various aspects of the Local Businesses; factors driving their popularity, customer review patterns and regions that favor certain businesses the most, are a few of the many aspects to name. Analyzing the customer sentiments based on their reviews has helped us in realizing the importance of customer satisfaction, also it is possible to derive action plans to improve business performance to keep up with the competition.}, location = {Pangyo, Seongnam, Republic of Korea}, series = {ICEC '17}, pages = {1\u20135}, numpages = {5}, keywords = {big data, reviews, azure, local business, IBM bluemix, hadoop, data analysis, hive, visualization}}
@inproceedings{10.1145/505248.506010,title = {Data quality assessment}, author = {Pipino Leo L. , Lee Yang W. , Wang Richard Y. },year = {2002}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/505248.506010}, doi = {10.1145/505248.506010}, abstract = {How good is a company's data quality? Answering this question requires usable data quality metrics. Currently, most data quality measures are developed on an ad hoc basis to solve specific problems [6, 8], and fundamental principles necessary for developing usable metrics in practice are lacking. In this article, we describe principles that can help organizations develop usable data quality metrics.}, pages = {211\u2013218}, numpages = {8}}
@inproceedings{10.1145/2788453,title = {Big city, big data}, author = {Kitner Kathi R. , de Wet Thea },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2788453}, doi = {10.1145/2788453}, abstract = {This forum addresses conceptual, methodological, and professional issues that arise in the UX field's continuing effort to contribute robust information about users to product planning and design. --- David Siegel and Susan Dray, Editors}, pages = {70\u201373}, numpages = {4}}
@inproceedings{10.1145/3447568.3448553,title = {Exploitation of ontological approaches in Big Data: A State of the Art}, author = {Djebouri Djamila , Keskes Nabil },year = {2020}, isbn = {9781450376556}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3447568.3448553}, doi = {10.1145/3447568.3448553}, abstract = {The emergence of web technologies is generating a data deluge called Big Data. All this data is in fact a gold mine to be exploited. However, we are confronted with huge volumes of heterogeneous data (various formats) and varied data (various sources) and in continuous expansion. To deal with this, some research works have introduced ontologies: this is the purpose of this paper. We present the Big Data concept on the one hand and the ontology concept on the other. We first recalled the definitions of Big Data, its main dimensions known by the 3 V (volume, velocity, variety), the fields of application as well as the various problems related to it. We reviewed the different solutions proposed as well as the existing tools by using the NoSQL and the Map-Reduce paradigm implemented in Hadoop and Spark.We then looked at the concept of ontology, starting by recalling the definition of ontology, so an ontology is a conceptual model to represent reality and on which it is possible to develop systems that can be shared and reused. Ontologies are used to represent a domain and reason about its entities.Finally, we presented and discussed some research works that combined ontologies and Big Data. We have found that there is a very abundant literature that deals with big data and ontologies separately, but few studies combine the two concepts together. We will therefore focus on the latter in order to enrich the scientific literature in the domain.}, location = {Lecce, Italy}, series = {ICIST '20}, pages = {1\u20136}, numpages = {6}, keywords = {ontology, Map-Reduce, HADOOP, Knowledge Base, Semantic Web, Big Data, Spark}}
@inproceedings{10.1145/2486227.2486231,title = {A big data confession}, author = {Grinter Beki },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2486227.2486231}, doi = {10.1145/2486227.2486231}, pages = {10\u201311}, numpages = {2}}
@inproceedings{10.1145/3323503.3345030,title = {Multimedia information retrieval in big data using OpenCV python}, author = {Goularte Rudinei , Trojahn Tiago H. , Kishi Rodrigo M. },year = {2019}, isbn = {9781450367639}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3323503.3345030}, doi = {10.1145/3323503.3345030}, abstract = {The popularization of systems, applications and devices to produce, view and share multimedia, saw the need to treat a large volume of data arise. In related areas (such as Multimedia Big Data, Data Science and Multimedia Information Retrieval) a key step is commonly referred as Multimedia Indexing or Multimedia Big Data Analysis, where the aim is to represent multimedia content into smaller, more manageable units, allowing the extraction of data features and information essential to the proper performance of the associated services. This mini-course discusses current tools and techniques for indexing, extracting and processing of multimodal multimedia content. The techniques are exemplified in Python OpenCV over different content (like images, audio, text and video), leading to the interest of services like Netflix, Google and YouTube on this subject, attracting the interest of researchers and developers.}, location = {Rio de Janeiro, Brazil}, series = {WebMedia '19}, pages = {25\u201327}, numpages = {3}, keywords = {big data, multimedia information retrieval, multimedia}}
@inproceedings{10.1145/3289430,title = {Proceedings of the 2018 2nd International Conference on Big Data and Internet of Things},year = {2018}, isbn = {9781450365192}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {With the sustainable development on information technology, and the widespread use of emerging technology, such as: cloud computing, internet of things and social network, the variety of Big Data is increasing day by day, and the scale of big date have expanded dramatically in recent years, which means the era of big data is coming silently. In order to keep up with the development of network technology, BDIOT2018 provide a platform to all scholars on relevant research area gather together on North China University of Technology, Beijing, China to share and discuss their experimental fruits or learn the most advanced innovative technologies from the outstanding experts on big data and internet of things. This conference is technically sponsored by North China University of Technology, China; University of Macau, Macau SAR; Dongguk University, The Republic of South Korea.}, location = {Beijing, China}}
@inproceedings{10.1145/2680821.2680824,title = {Performance and Fairness Issues in Big Data Transfers}, author = {Yu Se-young , Brownlee Nevil , Mahanti Aniket },year = {2014}, isbn = {9781450332828}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2680821.2680824}, doi = {10.1145/2680821.2680824}, abstract = {We present performance and fairness analysis of two TCP- based (GridFTP and FDT) and one UDP-based (UDT) big data transfer protocols. We perform long-haul performance experiments using a 10 Gb/s national network, and conduct fairness tests in our 10 Gb/s local network. Our results show that GridFTP with jumbo frames provides fast data transfers. GridFTP is also fair in sharing bandwidth with competing background TCP flows.}, location = {Sydney, Australia}, series = {CoNEXT Student Workshop '14}, pages = {9\u201311}, numpages = {3}, keywords = {fairness, big data transfer protocols, performance}}
@inproceedings{10.1145/3493700.3493772,title = {Green Computing for Big Data and Machine Learning}, author = {Barua Hrishav Bakul , Mondal Kartick Chandra , Khatua Sunirmal },year = {2022}, isbn = {9781450385824}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3493700.3493772}, doi = {10.1145/3493700.3493772}, abstract = {The current decade has beheld a tremendous spike in data volume, velocity, variety and many other such aspects which we call as Big Data and which gave birth to a new kind of science commonly known as \u201dData Science\u201d. With the \u201dData Apocalypse\u201d in progress, it is evident that the conventional methods to handle these data would not suffice. We need distributed and parallel architectures like Cloud services (IaaS, PaaS, SaaS, STaaS, etc.). But is that enough to satisfy our needs? Here, we propose a tutorial in a very different direction when we are talking about Data Science, that is, bringing greenness in Big Data and Machine Learning (ML). We divide the tutorial into two parts primarily assuming that we are using cloud backbone for analytic and prediction tasks. The first part speaks about the techniques and tools to bring energy efficiency/greenness in the algorithmic and code level for Big Data and ML using Approximate Computing. The second part talks about the green techniques and power models at the infrastructural level for the cloud.}, location = {Bangalore, India}, series = {CODS-COMAD 2022}, pages = {348\u2013351}, numpages = {4}, keywords = {Heuristics, Cloud Computing, Machine Learning, Data Science, Approximate Computing, Big Data, Power Modelling, Resource Scheduling, Green Computing}}
@inproceedings{10.1145/3208352,title = {Proceedings of the International Workshop on Semantic Big Data},year = {2018}, isbn = {9781450357791}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Houston, TX, USA}}
@inproceedings{10.1145/2911451.2911550,title = {Big Data in Climate: Opportunities and Challenges for Machine Learning}, author = {Kumar Vipin },year = {2016}, isbn = {9781450340694}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2911451.2911550}, doi = {10.1145/2911451.2911550}, abstract = {This talk will present an overview of research being done in a large interdisciplinary project on the development of novel data mining and machine learning approaches for analyzing massive amount of climate and ecosystem data now available from satellite and ground-based sensors, and physics-based climate model simulations. These information-rich data sets offer huge potential for monitoring, understanding, and predicting the behavior of the Earth's ecosystem and for advancing the science of global change. This talk will discuss challenges in analyzing such data sets and some of our research results in mapping the dynamics of surface water globally as well as detecting deforestation and fires in tropical forests using data from Earth observing satellites.}, location = {Pisa, Italy}, series = {SIGIR '16}, pages = {3}, numpages = {1}, keywords = {monitoring global water dynamics, and heterogeneous data, rare class detection, predictive models for imperfect, identifying tropical forest fires., spatio-temporal data, incomplete, big data}}