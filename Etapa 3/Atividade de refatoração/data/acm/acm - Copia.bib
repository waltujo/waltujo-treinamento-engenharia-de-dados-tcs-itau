@inproceedings{10.1145/2934466.2934476,
author = {Eichelberger, Holger and Qin, Cui and Sizonenko, Roman and Schmid, Klaus},
title = {Using IVML to Model the Topology of Big Data Processing Pipelines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934476},
doi = {10.1145/2934466.2934476},
abstract = {Creating product lines of Big Data stream processing applications introduces a number of novel challenges to variability modeling. In this paper, we discuss these challenges and demonstrate how advanced variability modeling capabilities can be used to directly model the topology of processing pipelines as well as their variability. We also show how such processing pipelines can be modeled, configured and validated using the Integrated Variability Modeling Language (IVML).},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {204–208},
numpages = {5},
keywords = {variability modeling, topologies, software product lines},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/1363686.1363915,
author = {Cutt, Bryce and Lawrence, Ramon},
title = {Managing Data Quality in a Terabyte-Scale Sensor Archive},
year = {2008},
isbn = {9781595937537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1363686.1363915},
doi = {10.1145/1363686.1363915},
abstract = {Sensor networks collect vast amounts of real-time information about the environment, business processes, and systems. Archived sensor data is valuable for long-term analysis and decision making, which requires it be suitably archived, indexed, and validated. In this paper, we describe a general approach to managing and improving data quality by the generation and validation of metadata and the logging of workflow events. The approach has been implemented within a system archiving terabytes of U.S. weather radar data. The data quality system has resulted in the detection of data errors while simplifying the administration of the complex archive system.},
booktitle = {Proceedings of the 2008 ACM Symposium on Applied Computing},
pages = {982–986},
numpages = {5},
keywords = {sensor network, real-time warehouse, data quality, hydrology, scientific data, archive},
location = {Fortaleza, Ceara, Brazil},
series = {SAC '08}
}

@inproceedings{10.1145/3361758.3361759,
author = {Saleh, Mohammed and Khdour, Thair and Qasaymeh, Mahmoud},
title = {Analysis of AMI, Smart Metering Deployment and Big Data Management Challenges},
year = {2019},
isbn = {9781450372466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361758.3361759},
doi = {10.1145/3361758.3361759},
abstract = {Sustainability, rise of consumer power, IOT, Cloud Computing and digitalization era are major factors driving the fourth industrial revolution. Electric energy stakeholders are automating technological and business processes to the new requirements of the energy sector. Deploying the right combination of AI, Advanced Metering Infrastructure and Big Data Analysis, will increase efficiency and self-optimize operations. The emerging markets including the Middle East will deploy nearly 250 million meters, representing an investment of almost $35bn. The GCC increasing appetite power generating capacity is 157 GW; which is almost half the total Middle East and North Africa power generating capacity Smart Grid market in Gulf Council Countries will top US$ 1.68 billion by 2026. In addition, GCC is projected to spend US$137 Billion to increase its power generating capacity, Transmission and Distribution by 69 MW within five years. This research focus on the analysis of AMI and smart metering deployment and challenges in GCC region.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Internet of Things},
pages = {3–7},
numpages = {5},
keywords = {Smart Grid, Digital Utility, AMI},
location = {Melbourn, VIC, Australia},
series = {BDIOT 2019}
}

@inproceedings{10.1145/3176653.3176657,
author = {Sahoh, Bukhoree and Choksuriwong, Anant},
title = {Smart Emergency Management Based on Social Big Data Analytics: Research Trends and Future Directions},
year = {2017},
isbn = {9781450363518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176653.3176657},
doi = {10.1145/3176653.3176657},
abstract = {Emergency management needs to be self-operating and utilize an autonomous processing framework to discover knowledge from various data sources. The aim of Smart Emergency Management (SEM) is to help Executive Management Teams (EMTs) make better decisions and deal with drastic events effectively; SEM needs reliable, accurate, and timely information and knowledge. The exponential growth of social networks, cloud computing, and the Internet of Things means that during emergency events human can share, reuse, and generate vast amounts of data through the application of Social Big Data (SBD). Currently, research tends to focus on the usage of SBD as a knowledge source in SEM, in particular, as a real-time data source for tracking disaster situations in order to manage them more effectively. This paper discusses potential research areas for state-of-the-art SBD analytics in support of SEM, examining fundamental frameworks, methodologies, and technologies for SEM; future trends for SEM using SBD analytics are also considered.},
booktitle = {Proceedings of the 2017 International Conference on Information Technology},
pages = {1–6},
numpages = {6},
keywords = {Social Big Data Analytics, Machine Learning, Smart Emergency Management, Bayesian Deep learning},
location = {Singapore, Singapore},
series = {ICIT 2017}
}

@proceedings{10.1145/2378356,
title = {MBDS '12: Proceedings of the 2012 Workshop on Management of Big Data Systems},
year = {2012},
isbn = {9781450317528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Data is growing at an exponential rate and several systems have emerged to store and analyze such large amounts of data. These systems, termed “Big data systems” are fast evolving Examples include the NoSQL storage systems, Hadoop Map-Reduce, data analytics platforms, search and indexing platforms, and messaging infrastructures. These systems address needs for structured and unstructured data across a wide spectrum of domains such as web, social networks, enterprise, cloud, mobile, sensor networks, multimedia/streaming, cyberphysical and high performance systems, and for multiple application verticals such as biosciences, healthcare, transportation, public sector, energy utilities, oil &amp; gas, and scientific computing.With increasing scale and complexity, managing these big data systems to cope with failures and performance problems is becoming non-trivial. New resource management and scheduling mechanisms are also needed for such systems, and so are mechanisms for tuning and support from platform layers. Several open source and proprietary solutions have been proposed to address these requirements, with extensive contributions from industry and academia. However, there remain substantial challenges, including those that pertain to such systems' autonomic and self-management capabilities.The objective of the MBDS workshop is to bring together researchers, practitioners, system administrators, system programmers, and others interested in sharing and presenting their perspectives on the effective management of big data systems. The focus of the workshop is on novel and practical, systems-oriented work. MBDS offers an opportunity for researchers and practitioners from industry, academia, and national labs to showcase the latest advances in this area and to also discuss and identify future directions and challenges in all aspects on autonomic management of big data systems.Papers are solicited on all aspects of big data management. Specific topics of interest include, but are not limited, to the following: Autonomic and self-managing techniques Application-level resource management and scheduling mechanisms System tuning/auto-tuning and configuration management Performance management, fault management, and power management Scalability challenges Complexity challenges, as for composite, cross-tier systems with multiple control loops Unified management of 'data in motion' and 'data at rest' Dealing with both structured and unstructured data Monitoring, diagnosis, and automated behaviour detection System-level principles and support for resource management Holistic management across hardware and software Implications of emerging hardware technologies such as non-volatile memory Domain specific challenges in web, cloud, social networks, mobile, sensor networks, streaming analytics, cyber-physical systems System building and experience papers for specific industry verticals},
location = {San Jose, California, USA}
}

@article{10.1145/2614512.2614518,
author = {Topi, Heikki},
title = {Learning to Think about Broader Implications of Big Data},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2153-2184},
url = {https://doi.org/10.1145/2614512.2614518},
doi = {10.1145/2614512.2614518},
journal = {ACM Inroads},
month = {jun},
pages = {24–25},
numpages = {2}
}

@inproceedings{10.1145/3063955.3063995,
author = {Zhang, Shuzhuang and Luo, Hao and Wu, Zhigang and Wang, Yi},
title = {Composed Sketch Framework for Quantiles and Cardinality Queries over Big Data Streams},
year = {2017},
isbn = {9781450348737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3063955.3063995},
doi = {10.1145/3063955.3063995},
abstract = {Quantiles and Cardinality queries are important tools to analyze statistical information from big data streams. Due to the features of the streams, such as huge volume and high velocity, it is a challenging problem to quickly provide responses for the two types of queries using constrained space over big data streams. In this paper, we propose a composed sketch framework, which can support both quantiles queries and cardinality queries over the data streams. We introduce cardinality estimators into a baseline q-digest structure and propose unified sketch merging and query processing operations. Our approach can support these two types of queries simultaneously. We conduct detailed theoretical and experimental analysis in terms of query accuracy and query response time. The analytical and experimental results show that our approach can obtain accurate estimates quicker than traditional method and system in big data streams environments, and it just produces less than 0.8‰ storage overhead in TB-scale real-world data sets.},
booktitle = {Proceedings of the ACM Turing 50th Celebration Conference - China},
articleno = {39},
numpages = {10},
keywords = {big data, data stream, sketch, cardinality query, quantiles query},
location = {Shanghai, China},
series = {ACM TUR-C '17}
}

@inproceedings{10.1145/2972958.2972967,
author = {Liebchen, Gernot and Shepperd, Martin},
title = {Data Sets and Data Quality in Software Engineering: Eight Years On},
year = {2016},
isbn = {9781450347723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2972958.2972967},
doi = {10.1145/2972958.2972967},
abstract = {Context: We revisit our review of data quality within the context of empirical software engineering eight years on from our PROMISE 2008 article.Objective: To assess the extent and types of techniques used to manage quality within data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets.Method: We update the 2008 mapping study through four subsequently published reviews and a snowballing exercise.Results: The original study located only 23 articles explicitly considering data quality. This picture has changed substantially as our updated review now finds 283 articles, however, our estimate is that this still represents perhaps 1% of the total empirical software engineering literature.Conclusions: It appears the community is now taking the issue of data quality more seriously and there is more work exploring techniques to automatically detect (and sometimes repair) noise problems. However, there is still little systematic work to evaluate the various data sets that are widely used for secondary analysis; addressing this would be of considerable benefit. It should also be a priority to work collab-oratively with practitioners to add new, higher quality data to the existing corpora.},
booktitle = {Proceedings of the The 12th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {7},
numpages = {4},
keywords = {data quality, mapping study, empirical software engineering},
location = {Ciudad Real, Spain},
series = {PROMISE 2016}
}

@proceedings{10.1145/2609876,
title = {HCBDR '14: Proceedings of the 2014 Workshop on Human Centered Big Data Research},
year = {2014},
isbn = {9781450329385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the proceedings of the first workshop on Human-Centered Big Data Research sponsored by the Laboratory for Analytic Sciences and hosted at the North Carolina State University.},
location = {Raleigh, NC, USA}
}

@inproceedings{10.1145/3206157.3206163,
author = {Li, Guanzhi and Yang, Xuan and Jun, Wang and Tao, Yang},
title = {A Theoretical Credit Reporting System Based on Big Data Concept: A Case Study of Humen Textile Garment Enterprises},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206163},
doi = {10.1145/3206157.3206163},
abstract = {Combining the characteristics of the capital demand scale and cycle volatility caused by the unique seasonal and production organization complexity of the textile and garment industry in Humen- China, the root reasons of difficult financing and high financing costs of the textile and garment enterprises are analyzed, and the fundamental solutions are proposed in this paper. Meanwhile, the guiding and directing role of government in the field of big data application, especially the big data credit collection, is emphasized. The big data technology is well utilized to build big data collection, storage and processing platform based on the big data credit collection. In order to fundamentally solve the problem of financing difficulty in local small and medium-sized enterprise due to long-term data circulation and information asymmetry, a theoretical credit rating model is established and continuously optimized according to the features of textile and garment industry in this area. This report also contributes to the innovation-driven development of local textile and garment enterprises, promotes the management level of enterprises, and improves the innovation ability of business models.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {22–26},
numpages = {5},
keywords = {textile and garment, Big data credit, SMEs, credit model},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/2630729.2630736,
author = {Parboteeah, Paul and Milne, Alistair},
title = {Lessons from the Medical Industry for Big Data in Finance: Ontologies and Financial Stress Diagnostics},
year = {2014},
isbn = {9781450330121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2630729.2630736},
doi = {10.1145/2630729.2630736},
abstract = {We examine the lessons from the experience of the SNOMED CT ontology in medicine for the monitoring of systemic financial risk. Using big data to create financial stress diagnostics that are informative about the causes of systemic risk, requires ontological understanding of financial data at the granular level. The history of SNOMED indicates that developing such an understanding is a long term project, requiring co-operation of many participants and a well developed structure of governance.},
booktitle = {Proceedings of the International Workshop on Data Science for Macro-Modeling},
pages = {1–2},
numpages = {2},
keywords = {FIBO, financial stress diagnostics, SNOMED CT, information governance ontologies, financial stress indicators, Big data},
location = {Snowbird, UT, USA},
series = {DSMM'14}
}

@inbook{10.5555/3042094.3042253,
author = {Bowman, Casey N. and Miller, John A.},
title = {Modeling Traffic Flow Using Simulation and Big Data Analytics},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {Improving the efficiency, safety, and cost of road systems is an essential social problem that must be solved as the number of drivers, and the size of mass transit systems increase. Methodologies used for the construction of traffic simulations need to be examined in the context of real world big traffic data. This data can be used to create models for vehicle arrivals, turning behavior, and traffic flow. Our work focuses mainly on generating models for these concepts and using them to drive microscopic traffic simulations built upon real world data. Strengths and weaknesses of various simulation optimization techniques are also considered as a methodology issue, since the nature of traffic systems weakens the effectiveness of some optimization techniques.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {1206–1217},
numpages = {12}
}

@inproceedings{10.1145/3232116.3232143,
author = {Xie, Xiaolan and Li, Xinrong},
title = {Research on Personalized Exercises and Teaching Feedback Based on Big Data},
year = {2018},
isbn = {9781450364966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3232116.3232143},
doi = {10.1145/3232116.3232143},
abstract = {With the rapid development of information technology, big data plays an increasingly important role in the research and practice of education and teaching. Online education has also become a research hotspot. To solve the problem of lack of personalized exercises and accurate teaching feedback in online education, a content-based recommendation model in big data and a clustering model based on EM algorithm is proposed in this paper. First of all, the students' answer of questions is recorded. Then the characteristic information is extracted, so recommends of the exercises are provided by the model according to the personal characteristic information. Then, all the students' recommendation information is stored in the feature library, in which the information of students are clustered, and the teaching effect is fed back according to the characteristic parameters of each category. On the one hand, the status of students' learning is fed back; On the other hand, the level of teachers' teaching level is also fed back. Finally, the model works well through experiments, with the good performance that it can improve the efficiency of online learning.},
booktitle = {Proceedings of the 3rd International Conference on Intelligent Information Processing},
pages = {166–171},
numpages = {6},
keywords = {personalized exercises, teaching feedback, online learning, recommendation algorithm, EM algorithm},
location = {Guilin, China},
series = {ICIIP '18}
}

@inproceedings{10.1145/2345316.2345327,
author = {Percivall, George},
title = {Realizing the Geospatial Potential of Mobile, IoT and Big Data},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345327},
doi = {10.1145/2345316.2345327},
abstract = {What happens when you have connected sensors in everyone's pockets, homes, vehicles, workspaces, street corners, shopping areas, and more? With the convergence of Mobile Computing, the Internet of Things (IoT), and the ability to gather and analyze this Big Data, the availability of massive amounts of information will continue to be gathered and you can expect the unexpected to happen.The themes of this panel are driving development in information technology, but what is the intersection with geospatial? Location determination and use of location for context are core capabilities of Mobile and IoT. Knowing your location along with nearby Points of Interest (PoIs) and Indoor maps provide a new level of spatial awareness and decision making. This information will be used and viewed in new ways including Augmented Reality (AR). Social computing with geospatial checkins provides a rich picture of the social environment. With embedded computing becoming even more ubiquitous, Sensor Webs will provide opportunistic sensing of the physical environment. Geospatial filtering is one of the most effective methods to extracting information from these big data streams. These streams will continue to grow, e.g., mobile 3D video at incredibly high resolution. Data Fusion to combine multiple data sources will create new capabilities many based on geospatial processing.How can we realize the full potential of these technological capabilities in regards to geospatial? We can envision a lot of upside with the technology, but at what cost to privacy and rights? How should policy, privacy and rights be included in the conversations and deployments of these technologies and the resultant data? What role will ambient and participatory crowdsourcing play? A goal of our technology development must be to reduce the apparent tradeoff between surveillance for public safety vs. interests and rights of people. Technology development will continue to be a social activity based on geospatial APIs and standards for mobile platforms from organizations like W3C, OGC, IETF, and OMA. Development of these technologies are a basis for the critical outcomes, e.g, in creating Smart Cities including Smart Energy. Crowdsourcing from mobile platforms and M2M-based sensors webs will provide a basis for humanity to better understand our world and make critical decisions about the livability of our future..},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {8},
numpages = {1},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@article{10.1145/2428616.2431055,
author = {Kumar, Arun and Niu, Feng and R\'{e}, Christopher},
title = {Hazy: Making It Easier to Build and Maintain Big-Data Analytics: Racing to Unleash the Full Potential of Big Data with the Latest Statistical and Machine-Learning Techniques.},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1542-7730},
url = {https://doi.org/10.1145/2428616.2431055},
doi = {10.1145/2428616.2431055},
abstract = {The rise of big data presents both big opportunities and big challenges in domains ranging from enterprises to sciences. The opportunities include better-informed business decisions, more efficient supply-chain management and resource allocation, more effective targeting of products and advertisements, better ways to "organize the world’s information," faster turnaround of scientific discoveries, etc.},
journal = {Queue},
month = {jan},
pages = {30–46},
numpages = {17}
}

@article{10.1007/s00778-011-0219-9,
author = {Denev, Dimitar and Mazeika, Arturas and Spaniol, Marc and Weikum, Gerhard},
title = {The SHARC Framework for Data Quality in Web Archiving},
year = {2011},
issue_date = {April     2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {2},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-011-0219-9},
doi = {10.1007/s00778-011-0219-9},
abstract = {Web archives preserve the history of born-digital content and offer great potential for sociologists, business analysts, and legal experts on intellectual property and compliance issues. Data quality is crucial for these purposes. Ideally, crawlers should gather coherent captures of entire Web sites, but the politeness etiquette and completeness requirement mandate very slow, long-duration crawling while Web sites undergo changes. This paper presents the SHARC framework for assessing the data quality in Web archives and for tuning capturing strategies toward better quality with given resources. We define data quality measures, characterize their properties, and develop a suite of quality-conscious scheduling strategies for archive crawling. Our framework includes single-visit and visit---revisit crawls. Single-visit crawls download every page of a site exactly once in an order that aims to minimize the "blur" in capturing the site. Visit---revisit strategies revisit pages after their initial downloads to check for intermediate changes. The revisiting order aims to maximize the "coherence" of the site capture(number pages that did not change during the capture). The quality notions of blur and coherence are formalized in the paper. Blur is a stochastic notion that reflects the expected number of page changes that a time-travel access to a site capture would accidentally see, instead of the ideal view of a instantaneously captured, "sharp" site. Coherence is a deterministic quality measure that counts the number of unchanged and thus coherently captured pages in a site snapshot. Strategies that aim to either minimize blur or maximize coherence are based on prior knowledge of or predictions for the change rates of individual pages. Our framework includes fairly accurate classifiers for change predictions. All strategies are fully implemented in a testbed and shown to be effective by experiments with both synthetically generated sites and a periodic crawl series for different Web sites.},
journal = {The VLDB Journal},
month = {apr},
pages = {183–207},
numpages = {25},
keywords = {Web archiving, Data quality, Crawls strategies, Coherence, Blur}
}

@inproceedings{10.1145/3291801.3291820,
author = {Maohong, Zhang and Aihua, Yang and Hui, Liu},
title = {Research on Security and Privacy of Big Data under Cloud Computing Environment},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291820},
doi = {10.1145/3291801.3291820},
abstract = {With the rapid development of computer science, Internet and information technology, the application scale of network is expanding constantly, and the data volume is increasing day by day. Therefore, the demand for data processing needs to be improved urgently, and Cloud computing and big data technology as the product of the development of computer networks came into being. However, the following data collection, storage, and the security and privacy issues in the process of use are faced with many risks. How to protect the security and privacy of cloud data has become one of the urgent problems to be solved. Aiming at the problem of security and privacy of data in cloud computing environment, the security of the data is ensured from two aspects: the storage scheme and the encryption mode of the cloud data.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {52–55},
numpages = {4},
keywords = {cloud computing, big data, security and privacy, encryption, storage scheme, cloud data},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/3220199.3220205,
author = {Seo, Sanghyun and Jeon, Yongjin and Kim, Juntae},
title = {Meta Learning for Imbalanced Big Data Analysis by Using Generative Adversarial Networks},
year = {2018},
isbn = {9781450364263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220199.3220205},
doi = {10.1145/3220199.3220205},
abstract = {Imbalanced big data means big data where the ratio of a certain class is relatively small compared to other classes. When the machine learning model is trained by using imbalanced big data, the problem with performance drops for the minority class occurs. For this reason, various oversampling methodologies have been proposed, but simple oversampling leads to problem of the overfitting. In this paper, we propose a meta learning methodology for efficient analysis of imbalanced big data. The proposed meta learning methodology uses the meta information of the data generated by the generative model based on Generative Adversarial Networks. It prevents the generative model from becoming too similar to the real data in minority class. Compared to the simple oversampling methodology for analyzing imbalanced big data, it is less likely to cause overfitting. Experimental results show that the proposed method can efficiently analyze imbalanced big data.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Computing},
pages = {5–9},
numpages = {5},
keywords = {Meta Learning, Generative Adversarial Network, Imbalanced Big Data Analysis, Oversampling},
location = {Shenzhen, China},
series = {ICBDC '18}
}

@inproceedings{10.1145/3020078.3021787,
author = {Cong, Jason and Fang, Zhenman and Huang, Muhuan and Wang, Libo and Wu, Di},
title = {CPU-FPGA Co-Optimization for Big Data Applications: A Case Study of In-Memory Samtool Sorting (Abstract Only)},
year = {2017},
isbn = {9781450343541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3020078.3021787},
doi = {10.1145/3020078.3021787},
abstract = {To efficiently process a tremendous amount of data, today's big data applications tend to distribute the datasets into multiple partitions, such that each partition can be fit into memory and be processed by a separate core/server in parallel. Meanwhile, due to the limited scaling of general-purpose CPUs, FPGAs have emerged as an attractive alternative to accelerate big data applications due to their low power, high performance and energy efficiency. In this paper we aim to answer one key question: How should the multicore CPU and FPGA coordinate together to optimize the performance of big data applications? To address the above question, we conduct a step-by-step case study to perform CPU and FPGA co-optimization for in-memory Samtool sorting in genomic data processing, which is one of the most important big data applications for personalized healthcare. First, to accelerate the time-consuming compression algorithm and its associated cyclic redundancy check (CRC) in Samtool sorting, we implement a portable and maintainable FPGA accelerator using high-level synthesis (HLS). Although FPGAs are traditionally well-known to be suitable for compression and CRC, we find that a straightforward integration of this FPGA accelerator into the multi-threaded Samtool sorting only achieves marginal system throughput improvement over the software baseline running on a 12-core CPU. To improve system performance, we propose a dataflow execution model to effectively orchestrate the computation between the multi-threaded CPU and FPGA. Experimental results show that our co-optimized CPU-FPGA system achieves a 2.6x speedup for in-memory Samtool sorting.},
booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {291},
numpages = {1},
keywords = {compression and CRC, genome data sorting, dataflow execution},
location = {Monterey, California, USA},
series = {FPGA '17}
}

@inproceedings{10.1109/UCC.2014.101,
author = {Self, Richard J.},
title = {Governance Strategies for the Cloud, Big Data, and Other Technologies in Education},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.101},
doi = {10.1109/UCC.2014.101},
abstract = {The Cloud, Big Data and many emerging technologies are now being considered by many educational establishments as candidates for deriving benefits for both students in their learning and also for the organisation in terms of more effective and efficient operation. This paper considers the governance strategies which need to be developed and implemented in order to ensure that the technologies can be safely incorporated into the technical and operational infrastructure. It demonstrates that synthesising ISO 27002 with a new framework of 12 Vs of Big Data provides an effective approach to identifying some important aspects of new technologies that do not naturally arise from traditional frameworks.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {630–635},
numpages = {6},
keywords = {education, emerging technology, strategy, cloud, big data, governance},
series = {UCC '14}
}

@inproceedings{10.1145/2987386.2987429,
author = {Son, Jae Gi and Kang, Ji-Woo and An, Jae-Hoon and Ahn, Hyung-Joo and Chun, Hyo-Jung and Kim, Jung-Guk},
title = {Parallel Job Processing Technique for Real-Time Big-Data Processing Framework},
year = {2016},
isbn = {9781450344555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987386.2987429},
doi = {10.1145/2987386.2987429},
abstract = {Since the introduction of big data, numerous researches aiming to improve the accuracy and speed of data processing has been conducted. Many platforms that can process real-time data were developed for this purpose. Most standard data processing platforms used Spark Streaming as data analysis layer. However, its limitation in performance calls for a better alternative. This paper introduces a new data processing framework, Squall. Squall utilizes parallel processing and allows real-time data processing using streaming modules. Go was used for development. Through various experiments, the performance of our newly developed framework on processing real-time data was compared to the performance of the previously existing framework completing the same task. Results show quantitative evidence that Squall excel the platforms that use Spark Streaming. Our future work includes making modifications that will improve Squall's performance.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {226–229},
numpages = {4},
keywords = {Real-time Big-Data Processing Framework, Apache Spark, Realtime Packet Analysis, Parallel Job Processing, Squall},
location = {Odense, Denmark},
series = {RACS '16}
}

@inproceedings{10.1145/3012258.3012268,
author = {Qian, Xuesheng and Xu, Yifeng and Zhang, Jing and Zhao, Wei},
title = {Novel Method for Organizational Evaluation and Practice Based on Big Data Analysis},
year = {2016},
isbn = {9781450347617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012258.3012268},
doi = {10.1145/3012258.3012268},
abstract = {Organizational evaluation plays an essential role in the decision-making and administrative management. Although there are numbers of conventional evaluation methods, the assessing result is subjective and limited by the application fields of data utilization, let along other drawbacks. With the perspective of big data, this thesis employs lower granularity and promotes the big-data based novel organizational evaluation pattern, in order to overcome the shortage of the conventional evaluation system. Then the thesis applies the big data method to elucidate an arduous assessing analysis case - the typical school evaluation problem with the process of standardizing and normalizing the 200,000 students' activity information in hundreds of k-12 schools of a district in Shanghai. Accordingly, the thesis accomplishes the assessment diagnosis on the school management and shares the effective supplements and risk warnings compared with the conventional ones.},
booktitle = {Proceedings of the 2016 8th International Conference on Information Management and Engineering},
pages = {30–35},
numpages = {6},
keywords = {Data Acquisition, Behavior Record, Big-Data Assessment, Organizational Evaluation, Chinese Short Texts De-duplication},
location = {Istanbul, Turkey},
series = {ICIME 2016}
}

@article{10.1145/2331042.2331047,
author = {Bhambhri, Anjul},
title = {Six Tips for Students Interested in Big Data Analytics},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331047},
doi = {10.1145/2331042.2331047},
journal = {XRDS},
month = {sep},
pages = {9},
numpages = {1}
}

@inproceedings{10.1145/3206157.3206184,
author = {Shaosen, Cao},
title = {The Study of Learner Autonomy in Foreign Language Learning in Big Data Era},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206184},
doi = {10.1145/3206157.3206184},
abstract = {Learner autonomy has been considered as the ultimate goal of education, and it has become a hot topic in education field since its birth. In Big Data era, the learning pattern is changing dramatically and learning activities can be carried out through an interactive process between learners and the online learning systems. This article first introduces Big Data and learner autonomy, then discusses the reforms big data brings to learner autonomy, finally analyzes the learning mode of learner autonomy in foreign language learning in Big Data era.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {101–105},
numpages = {5},
keywords = {Big Data, interactive process, learner autonomy, foreign language learning},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/2939502.2939518,
author = {Fisher, Danyel},
title = {Big Data Exploration Requires Collaboration between Visualization and Data Infrastructures},
year = {2016},
isbn = {9781450342070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939502.2939518},
doi = {10.1145/2939502.2939518},
abstract = {As datasets grow to tera- and petabyte sizes, exploratory data visualization becomes very difficult: a screen is limited to a few million pixels, and main memory to a few tens of millions of data points. Yet these very large scale analyses are of tremendous interest to industry and academia. This paper discusses some of the major challenges involved in data analytics at scale, including issues of computation, communication, and rendering. It identifies techniques for handling large scale data, grouped into "look at less of it," and "look at it faster." Using these techniques involves a number of difficult design tradeoffs for both the ways that data can be represented, and the ways that users can interact with the visualizations.},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {16},
numpages = {5},
keywords = {data analysis, big data, data visualization},
location = {San Francisco, California},
series = {HILDA '16}
}

@inproceedings{10.1145/3207677.3277994,
author = {Zhang, Han and Wang, Kuisheng},
title = {Research on Customer Credit Demand Forecasting Based on Big Data Analysis},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3277994},
doi = {10.1145/3207677.3277994},
abstract = {With1 the rapid development of the Internet, Internet credit business has emerged and the process is now booming. As a result, there is a problem of predicting credit demand of users. Therefore, we propose a method of using big data analysis to forecast the credit demand of users in this paper, which is used to reduce the risk of credit business and improve the utilization of funds.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {28},
numpages = {5},
keywords = {credit demand, K-means clustering algorithm, Data analysis, GBDT (Gradient Boosting Decision Tree) model, ReliefF feature extraction algorithm},
location = {Hohhot, China},
series = {CSAE '18}
}

@inproceedings{10.1145/2345316.2345324,
author = {Tossavainen, Olli-Pekka},
title = {Big Data Computing for Traffic Information by GPS Sensing},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345324},
doi = {10.1145/2345316.2345324},
abstract = {Real time traffic monitoring systems perform spatial and time dependent analysis of measurement data of different types such as traditional inductive loop detector data, microwave radar data, and GPS data. The goal of these systems is to provide information such as average speeds, volumes and densities on a given segment of a roadway. One of the fastest growing data source for traffic monitoring systems is GPS data collected from mobile devices. To some extent, in the industry GPS data is already replacing the traditional traffic sensing technologies.There is a large demand in industry and transportation agencies to have access to high resolution state of traffic on highways and arterial roads globally. This means that traffic information providers have to provide traffic information on a resolution that goes beyond the widely used TMC code based representation of the roadway.In order to obtain the high resolution state of traffic, noisy observations need to be fused into a mathematical model that represents the evolution of the system either based on physics or statistics. Common frameworks for fusing the data into physical models include for example Kalman filtering and particle filtering.Prior to the data fusion stage in the real time system, offline geospatial modelling has already been done. For example, construction and calibration of an accurate physics based traffic model includes tasks such as building a directed graph of the road network, detection of road geometry at lane level and speed limit detection. In all these tasks, GPS data is vital.Real time systems that use GPS data include geospatial pre-processing components such as map matching and path inference. The rapidly growing volume of GPS data cannot be handled using traditional methods but instead parallel computing systems are needed to handle the future volumes. Also, the new high resolution algorithms are developed to leverage the parallel processing frameworks.In this talk I will discuss directions taken to respond to the demand of providing high resolution information about the state of the traffic both in the context of modeling and implementation of a large scale system.},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {6},
numpages = {1},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@inproceedings{10.1145/2859889.2883587,
author = {Singhal, Rekha},
title = {Tutorial on Challenges for Big Data Application Performance Tuning and Prediction},
year = {2016},
isbn = {9781450341479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2859889.2883587},
doi = {10.1145/2859889.2883587},
abstract = {Digitization of user services and cheap access to the internet has led to two critical problems- quick response to end-user queries and faster analysis of large accumulated data to serve users better. This has also led to the advent of various big data processing technologies, each of them has architecture specific parameters to tune for optimal execution of the application. There are also challenges in optimal scheduling of analytic queries for faster analysis, which lead to the problem of estimating analytic queries execution time for large data sizes on the production system. A production system may be an enterprise database system or a cluster of machines with Hadoop etc, where each machine may be of different hardware configuration (known as heterogeneous environment). In the first part of this tutorial, we shall present need and challenges for tuning big data applications on various platforms. This is followed by discussion on various existing solutions for application tuning. The second part of the tutorial presents the challenges and state of the art for estimating application execution time.},
booktitle = {Companion Publication for ACM/SPEC on International Conference on Performance Engineering},
pages = {29},
numpages = {1},
location = {Delft, The Netherlands},
series = {ICPE '16 Companion}
}

@article{10.1145/2643132,
author = {Daries, Jon P. and Reich, Justin and Waldo, Jim and Young, Elise M. and Whittinghill, Jonathan and Ho, Andrew Dean and Seaton, Daniel Thomas and Chuang, Isaac},
title = {Privacy, Anonymity, and Big Data in the Social Sciences},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/2643132},
doi = {10.1145/2643132},
abstract = {Quality social science research and the privacy of human subjects require trust.},
journal = {Commun. ACM},
month = {sep},
pages = {56–63},
numpages = {8}
}

@inproceedings{10.1145/3234698.3234758,
author = {Chehbi-Gamoura, Samia and Derrouiche, Ridha and Malhotra, Manisha and Koruca, Halil-Ibrahim},
title = {Adaptive Management Approach for More Availability of Big Data Business Analytics},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234758},
doi = {10.1145/3234698.3234758},
abstract = {With the Big Data management, and the propagation of business lines into complex networks, activities are ever more subject to disasters than ever. It is nearly impossible to forecast their happening and degree of related costs. Accordingly, organizations try to collaborate in risk management. This paper outlines and discusses a generic approach based on Fuzzy Cognitive Maps (FCM) for cross-management of Disaster Recovery Plans (DRP). A set of basics of disaster planning is also provided. The proposed approach is focused on risk assessment methodology. The method is able to aggregate all assessment variables of the whole stakeholders involved in the business network. The main outcomes of this study aim to support networked enterprises in improving risk readiness capability and disaster recovery. Finally, we indicate the open challenges for further researches and an outlook on our future research.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {60},
numpages = {8},
keywords = {networked enterprises, Disaster Recovery Plan, adaptive management, Big Data, Fuzzy Cognitive Map},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

@article{10.1145/2674026.2674031,
author = {Tran, Dang-Hoan and Gaber, Mohamed Medhat and Sattler, Kai-Uwe},
title = {Change Detection in Streaming Data in the Era of Big Data: Models and Issues},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/2674026.2674031},
doi = {10.1145/2674026.2674031},
abstract = {Big Data is identified by its three Vs, namely velocity, volume, and variety. The area of data stream processing has long dealt with the former two Vs velocity and volume. Over a decade of intensive research, the community has provided many important research discoveries in the area. The third V of Big Data has been the result of social media and the large unstructured data it generates. Streaming techniques have also been proposed recently addressing this emerging need. However, a hidden factor can represent an important fourth V, that is variability or change. Our world is changing rapidly, and accounting to variability is a crucial success factor. This paper provides a survey of change detection techniques as applied to streaming data. The review is timely with the rise of Big Data technologies, and the need to have this important aspect highlighted and its techniques categorized and detailed.},
journal = {SIGKDD Explor. Newsl.},
month = {sep},
pages = {30–38},
numpages = {9}
}

@inproceedings{10.1145/3409501.3409523,
author = {Bo, Yu and Yongke, Chen},
title = {Strengthen the Establishment of Enterprise Logistics Management System by Means of Big Data},
year = {2020},
isbn = {9781450375603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409501.3409523},
doi = {10.1145/3409501.3409523},
abstract = {Big data plays an irreplaceable role in China's logistics industry. Enterprises need to reconstruct their logistics management system with the help of big data technology. This paper expounds the application advantages of big data technology in enterprise logistics management, puts forward the contradictions in enterprise logistics management under the background of big data, and explores the application of big data technology in enterprise logistics management from four aspects of safety management system, inventory management mode, logistics distribution management and talent team construction.},
booktitle = {Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference &amp; 2020 3rd International Conference on Big Data and Artificial Intelligence},
pages = {209–211},
numpages = {3},
keywords = {information, logistics, big data},
location = {Qingdao, China},
series = {HPCCT &amp; BDAI '20}
}

@inproceedings{10.1145/3414274.3414280,
author = {Heng, Li and Longfu, Zhou and Kaiyou, Yuan},
title = {Statistic Analysis of Safety Accidents in Filling Stations Based on Big Data},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414280},
doi = {10.1145/3414274.3414280},
abstract = {The big data technology which taking data analysis and mining as core can efficiently collect, store and process data of filling station safety accident cases, and further realize the statistical analysis, knowledge mining, principle summary and early warning forecast of filling station safety accidents. The article collected 461 cases of filling station safety accidents from 1981 to 2019 domestically and internationally. Given that those electronic text data are not highly standardized and cannot be directly analyzed and utilized, keyword extraction and structured storage were adopted by this article, in order to implement structured processing of electronic text data. Through the multi-dimensional statistical analysis of the structured data, high-quality, reliable, and practical analytical results were achieved. The results demonstrated that big data technology would become the main development stream and one of the most vital weapons of statistical analysis in various industries in the future.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {36–41},
numpages = {6},
keywords = {Structured storage, Big data, Safety accident case, Filling station, Statistic analysis, Keyword extraction},
location = {Xiamen, China},
series = {DSIT 2020}
}

@inproceedings{10.1145/3331453.3360973,
author = {Dong, Wei and Xiao, Litian and Niu, Shengfen and Niu, Jianjun and Wang, Fei},
title = {Application Research of Big Data for Launch Support System at Space Launch Site},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3360973},
doi = {10.1145/3331453.3360973},
abstract = {At the space launch site, the big data of the launch support system comes from the construction of the launch site, the ground service, the comprehensive support process, and launch mission organization and command. The big data is extensive sources, various types, large scale, and rapid growth. The big data application can improve the data processing and management efficiency for the launch support system. Then the application can enhance the support capability of flight mission and success rate. This paper analyzes the existing data application of launch support system. The challenges and requirements of big data application are studied by the construction of intelligent launch site. The application pattern and target are put forward from four aspects of launch mission organization and command, mission application, comprehensive support, and information security. The classification of big data is proposed for a launch support system. The architecture of big data application system is designed, which meets the application pattern and target. It lays a foundation for the future big data project at the launch site.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {23},
numpages = {6},
keywords = {Space launch site, Big data, Launch support system, Application research},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/3341620.3341637,
author = {Zhang, Baolei and Zhao, Fuquan and Liu, Zongwei},
title = {Application Strategy of Big Data in the Development of Complex Industrial Products (CIPs)},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341637},
doi = {10.1145/3341620.3341637},
abstract = {The product development is the key business of manufacturing and determines the competitive advantage of manufacturing enterprises, and has high difficulty in implementation. The product development of Complex Industrial Products (CIPs) is a great practical challenge for most enterprises. The demand for mass customization products makes enterprises to face more complicated product development situation. The deep integration of information technology and manufacturing technology makes big data an important value source for enterprises. Full application of big data to promote product development of CIPs has become a feasible approach for product development of enterprises. The value of big data needs to be applied through the knowledge-based application of data. The core work is to develop the functional data model. The application of big data in product development will eventually move towards knowledge-based intelligent. The case study provides the mechanism verification for the application strategy of big data in the development of CIPs.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {114–120},
numpages = {7},
keywords = {Big data, Complex Industrial Products (CIPs), Data processing, Smart manufacturing},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3078468.3078500,
author = {Garion, Shelly and Kolodner, Hillel and Adir, Allon and Aharoni, Ehud and Greenberg, Lev},
title = {Big Data Analysis of Cloud Storage Logs Using Spark},
year = {2017},
isbn = {9781450350358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078468.3078500},
doi = {10.1145/3078468.3078500},
abstract = {We use Apache Spark analytics to investigate the logs of an operational cloud object store service to understand how it is being used. This investigation involves going over very large amounts of historical data (PBs of records in some cases) collected over long periods of time retroactively. Existing tools, such as Elasticsearch-Logstash-Kibana (ELK), are mainly used for presenting short-term metrics and can-not perform advanced analytics such as machine learning. A possible solution is to save for long periods only certain aggregations or calculations produced from the raw log data, such as averages or histograms, however these must be decided in advance, and cannot be changed retroactively since the raw data has already been discarded. Spark allows us to gain insights going over historical data collected over long periods of time and to apply the historical models on online data in a simple and efficient way.},
booktitle = {Proceedings of the 10th ACM International Systems and Storage Conference},
articleno = {30},
numpages = {1},
location = {Haifa, Israel},
series = {SYSTOR '17}
}

@inproceedings{10.1145/3377170.3377278,
author = {Zhai, Chenggong and Fei, Xiande and Yang, Zhiwei},
title = {Design of Integrated Platform for Clothing and Accouterment Support Based on Big Data},
year = {2020},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377170.3377278},
doi = {10.1145/3377170.3377278},
abstract = {Based on the in-depth analysis of the necessity and feasibility of the optimization of big data-based quilt support, this paper puts forward the overall framework and implementation concept of the optimization of big data-based quilt support, and describes how to build the basic matching of the optimization of big data-based quilt support and the concept of big data information system system. Design of integrated platform for clothing and accouterment support based on big data that collects, stores, processes, analyzes and mines the supply data of conscription bedding by using big data, cloud computing and other information technologies, which plays an important role in deepening the reform of military bedding and strengthening the scientific management of bedding.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {54–58},
numpages = {5},
keywords = {Clothing and Accouterment, Big Data, Integrated Platform},
location = {Shanghai, China},
series = {ICIT 2019}
}

@article{10.14778/3090163.3090168,
author = {Anderson, Michael and Smith, Shaden and Sundaram, Narayanan and Capot\u{a}, Mihai and Zhao, Zheguang and Dulloor, Subramanya and Satish, Nadathur and Willke, Theodore L.},
title = {Bridging the Gap between HPC and Big Data Frameworks},
year = {2017},
issue_date = {April 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3090163.3090168},
doi = {10.14778/3090163.3090168},
abstract = {Apache Spark is a popular framework for data analytics with attractive features such as fault tolerance and interoperability with the Hadoop ecosystem. Unfortunately, many analytics operations in Spark are an order of magnitude or more slower compared to native implementations written with high performance computing tools such as MPI. There is a need to bridge the performance gap while retaining the benefits of the Spark ecosystem such as availability, productivity, and fault tolerance. In this paper, we propose a system for integrating MPI with Spark and analyze the costs and benefits of doing so for four distributed graph and machine learning applications. We show that offloading computation to an MPI environment from within Spark provides 3.1−17.7\texttimes{} speedups on the four sparse applications, including all of the overheads. This opens up an avenue to reuse existing MPI libraries in Spark with little effort.},
journal = {Proc. VLDB Endow.},
month = {apr},
pages = {901–912},
numpages = {12}
}

@inproceedings{10.1145/3093338.3093351,
author = {Liu, Fang Cherry and Xu, Weijia and Belgin, Mehmet and Huang, Ruizhu and Fleischer, Blake C.},
title = {Insights into Research Computing Operations Using Big Data-Powered Log Analysis},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3093351},
doi = {10.1145/3093338.3093351},
abstract = {Research computing centers provide a wide variety of services including large-scale computing resources, data storage, high-speed interconnect and scientific software repositories to facilitate continuous competitive research. Efficient management of these complex resources and services, as well as ensuring their fair use by a large number of researchers from different scientific domains are key to a center's success. Almost all research centers use monitoring services based on real time data gathered from systems and services, but often lack tools to perform a deeper analysis on large volumes of historical logs for identifying insightful trends from recurring events. The size of collected data can be massive, posing significant challenges for the use of conventional tools for this kind of analysis. This paper describes a big data pipeline based on Hadoop and Spark technologies, developed in close collaboration between TACC and Georgia Tech. This data pipeline is capable of processing large volumes of data collected from schedulers using PBSTools, making it possible to run a deep analysis in minutes as opposed to hours with conventional tools. Our component-based pipeline design adds the flexibility of plugging in different components, as well as promotes data reuse. Using this data pipeline, we demonstrate the process of formulating several critical operational questions around researcher behavior, systems health, operational aspects and software usage trends, all of which are critical factors in determining solutions and strategies for efficient management of research computing centers.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {31},
numpages = {8},
keywords = {log analysis, hadoop, spark, big data},
location = {New Orleans, LA, USA},
series = {PEARC17}
}

@inproceedings{10.1145/3018896.3066908,
author = {Bounceur, Ahc\`{e}ne},
title = {From Smart-City and IoT Simulation to Big Data Generation},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3066908},
doi = {10.1145/3018896.3066908},
abstract = {Our world is digitized everyday and increasingly. In 2020, it is expected that over 70% of the population will live in or around cities. To guarantee a good quality of life, it is necessary to ensure fast and reliable services in all areas, in particular those which are mainly based on the use of connected objects. This is one of the cornerstones of a smart city project. It will make possible to provide close to real-time the remote monitoring of sick patients, the monitoring of the environment in order to know its evolution over time and to anticipate developments that can be harmful to health and the environment itself, and to accurately analyze the signals transmitted by the on-board sensors.To further develop domains such as eHealth or the monitoring of other networks in the context of Smart Cities, fast and reliable design tools are needed. Their objectives are to study the realizability of such networks, their behavior in terms of energy consumption, safety, cost and other reliability parameters.This keynote aims to present a new platform called CupCarbon that allows to design systems of connected objects mainly representing sensors and to prepare future deployments of large-scale IoT infrastructures for Smart cities in optimal conditions. This kind of platforms will be a part of systems in the world that will participate in the generation of Big Data.1},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {3},
numpages = {1},
keywords = {interference, visibility tree, radio propagation channel, cupcarbon simulation, alpha-stable distribution},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3230905.3230944,
author = {Tajmouati, Samya and Abarda, Abdallah and El Moudden, Mustapha and Dakkon, Mohamed and Esghir, Mustapha},
title = {A Study of the Application of Statistical Methods for Big Data},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230944},
doi = {10.1145/3230905.3230944},
abstract = {The use of analysis and classification methods for big data is difficult. Several proposals consist in dividing randomly the population into b sub-samples and aggregating the parameters using an estimator based on the average parameters of these selected sub-samples. This paper aims to find a solution that minimizes calculations by selecting a small number b* sub-samples and keeping the same precision. We can apply this approach to the several method to measure its relevance.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {16},
numpages = {6},
keywords = {classification method, massive data, Latent class analysis},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@article{10.1145/2432596.2432601,
author = {Topi, Heikki},
title = {Where is Big Data in Your Information Systems Curriculum?},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2153-2184},
url = {https://doi.org/10.1145/2432596.2432601},
doi = {10.1145/2432596.2432601},
journal = {ACM Inroads},
month = {mar},
pages = {12–13},
numpages = {2}
}

@inproceedings{10.1109/CCGrid.2014.123,
author = {Gunarathne, Thilina and Qiu, Judy and Gannon, Dennis},
title = {Towards a Collective Layer in the Big Data Stack},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.123},
doi = {10.1109/CCGrid.2014.123},
abstract = {We generalize MapReduce, Iterative MapReduce and data intensive MPI runtime as a layered Map-Collective architecture with Map-AllGather, Map-AllReduce, MapReduceMergeBroadcast and Map-ReduceScatter patterns as the initial focus. Map-collectives improve the performance and efficiency of the computations while at the same time facilitating ease of use for the users. These collective primitives can be applied to multiple runtimes and we propose building high performance robust implementations that cross cluster and cloud systems. Here we present results for two collectives shared between Hadoop (where we term our extension H-Collectives) on clusters and the Twister4Azure Iterative MapReduce for the Azure Cloud. Our prototype implementations of Map-AllGather and Map-AllReduce primitives achieved up to 33% performance improvement for K-means Clustering and up to 50% improvement for Multi-Dimensional Scaling, while also improving the user friendliness. In some cases, use of Map-collectives virtually eliminated almost all the overheads of the computations.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {236–245},
numpages = {10},
keywords = {twister, cloud, HPC, collectives, K-means, MDS, performance, MapReduce},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/3297730.3297743,
author = {Liu, Yi and Peng, Jiawen and Yu, Zhihao},
title = {Big Data Platform Architecture under The Background of Financial Technology: In The Insurance Industry As An Example},
year = {2018},
isbn = {9781450365826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297730.3297743},
doi = {10.1145/3297730.3297743},
abstract = {With the rise of the concept of financial technology, financial and technology gradually in-depth integration, scientific and technological means to become financial product innovation, improve financial efficiency and reduce financial transaction costs an important driving force. In this context, the new technology platform is from the business philosophy, business model, technical means, sales, internal management and other dimensions to re-shape the financial industry. In this paper, the existing big data platform architecture technology innovation, adding space-time data elements, combined with the insurance industry for practical analysis, put forward a meaningful product circle and customer circle.},
booktitle = {Proceedings of the 2018 International Conference on Big Data Engineering and Technology},
pages = {31–35},
numpages = {5},
keywords = {insurance industry, platform architecture, time and space data, Financial technology, big data platform},
location = {Chengdu, China},
series = {BDET 2018}
}

@inproceedings{10.1145/3378936.3378978,
author = {Hukkeri, Tanmay Sanjay and G, Shobha and Phal, Shubham Milind and Shetty, Jyothi and R, Yatish H and Mohammed, Naweed},
title = {Massively Scalable Image Processing on the HPCC Systems Big Data Platform},
year = {2020},
isbn = {9781450376907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378936.3378978},
doi = {10.1145/3378936.3378978},
abstract = {Today's fast-moving world sees an abundance of image data in everyday life. From messages to insurance claims to even judicial systems, image data plays a pivotal role in facilitating several critical Big Data applications. Some of these applications such as automatic license plate recognition (ALPR) use CCTV cameras to capture snapshots of traffic from real-time video, inadvertently resulting in the generation vast amounts of image data on a daily basis. This brings with it the herculean task of processing these images to extract the essential information as efficiently as possible. The conventional method of processing images in a sequential manner can be very time consuming on account of the vast multitude of images and the intensive computation involved in order to process these. Distributed image processing seeks to provide a solution to this problem by splitting the computations involved across multiple nodes. This paper presents a novel framework to implement distributed image processing via OpenCV on HPCC Systems distributed node architecture*, a set of high-performance computing clusters. The proposed approach when tested on the Indian License Plates Dataset was found to be 85 percent accurate. Additionally, a 30 percent decrease in computation time was observed when executed on a multi-node setup without any impact to accuracy.},
booktitle = {Proceedings of the 3rd International Conference on Software Engineering and Information Management},
pages = {26–31},
numpages = {6},
keywords = {OpenCV, HPCC Systems, Big Data, Image Processing, Distributed Computing, Automatic License Plate Recognition, Machine Learning},
location = {Sydney, NSW, Australia},
series = {ICSIM '20}
}

@article{10.14778/3402707.3402709,
author = {Campbell, David},
title = {Is It Still "Big Data" If It Fits in My Pocket?},
year = {2020},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402707.3402709},
doi = {10.14778/3402707.3402709},
abstract = {"Big Data" is a hot topic but, in many ways, we are still trying to define what the phrase "Big Data" means. For many, there are more questions than answers at this point. Is it about size alone? Complexity? Variability? Data shape? Price/performance? New workloads? New types of users? Are existing data models, data management systems, data languages, and BI/ETL tools relevant in this space? Is MapReduce really a "major step backwards"? I have spent time over the last several years trying to answer many of these questions to my own satisfaction. As part of the journey I have witnessed a number of natural patterns that emerge in big data processing. In this talk I will present a catalog of these patterns and illustrate them across a scale spectrum from megabytes to 100s of petabytes. Finally, I will offer some thoughts around a systems and research agenda for this new world.},
journal = {Proc. VLDB Endow.},
month = {jun},
pages = {694},
numpages = {1}
}

@article{10.1145/3381027,
author = {Herodotou, Herodotos and Chen, Yuxing and Lu, Jiaheng},
title = {A Survey on Automatic Parameter Tuning for Big Data Processing Systems},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3381027},
doi = {10.1145/3381027},
abstract = {Big data processing systems (e.g., Hadoop, Spark, Storm) contain a vast number of configuration parameters controlling parallelism, I/O behavior, memory settings, and compression. Improper parameter settings can cause significant performance degradation and stability issues. However, regular users and even expert administrators grapple with understanding and tuning them to achieve good performance. We investigate existing approaches on parameter tuning for both batch and stream data processing systems and classify them into six categories: rule-based, cost modeling, simulation-based, experiment-driven, machine learning, and adaptive tuning. We summarize the pros and cons of each approach and raise some open research problems for automatic parameter tuning.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {43},
numpages = {37},
keywords = {stream, self-tuning, Storm, Spark, Parameter tuning, MapReduce}
}

@inproceedings{10.1145/3282278.3282282,
author = {Casado-Vara, Roberto and de la Prieta, Fernando and Prieto, Javier and Corchado, Juan M.},
title = {Blockchain Framework for IoT Data Quality via Edge Computing},
year = {2018},
isbn = {9781450360500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282278.3282282},
doi = {10.1145/3282278.3282282},
abstract = {Smart home presents a challenge in control and monitoring of its wireless sensors networks (WSN) and the internet of things (IoT) devices which form it. The current IoT architectures are centralized, complex, with poor security in its communications and with upstream communication channels mainly. As a result, there are problems with data reliability. These problems include data missing, malicious data inserted, communications network overload, and overload of computing power at the central node. In this paper a new architecture is presented. This architecture based in blockchain introduce the edge computing layer and a new algorithm to improve data quality and false data detection.},
booktitle = {Proceedings of the 1st Workshop on Blockchain-Enabled Networked Sensor Systems},
pages = {19–24},
numpages = {6},
keywords = {data quality false data detection, IoT, edge computing, WSN, non linear control, Blockchain},
location = {Shenzhen, China},
series = {BlockSys'18}
}

@article{10.1145/2856059,
author = {Cassavia, Nunziato and Masciari, Elio and Pulice, Chiara and Sacc\`{a}, Domenico},
title = {Discovering User Behavioral Features to Enhance Information Search on Big Data},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/2856059},
doi = {10.1145/2856059},
abstract = {Due to the emerging Big Data paradigm, driven by the increasing availability of intelligent services easily accessible by a large number of users (e.g., social networks), traditional data management techniques are inadequate in many real-life scenarios. In particular, the availability of huge amounts of data pertaining to user social interactions, user preferences, and opinions calls for advanced analysis strategies to understand potentially interesting social dynamics. Furthermore, heterogeneity and high speed of user-generated data require suitable data storage and management tools to be designed from scratch. This article presents a framework tailored for analyzing user interactions with intelligent systems while seeking some domain-specific information (e.g., choosing a good restaurant in a visited area). The framework enhances a user's quest for information by exploiting previous knowledge about their social environment, the extent of influence the users are potentially subject to, and the influence they may exert on other users. User influence spread across the network is dynamically computed as well to improve user search strategy by providing specific suggestions, represented as tailored faceted features. Such features are the result of data exchange activity (called data posting) that enriches information sources with additional background information and knowledge derived from experiences and behavioral properties of domain experts and users. The approach is tested in an important application scenario such as tourist recommendation, but it can be profitably exploited in several other contexts, for example, viral marketing and food education.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {jul},
articleno = {7},
numpages = {33},
keywords = {user behavior, personal big data, intelligent recommendation, information extraction, NoSQL databases}
}

@inproceedings{10.5555/3233397.3233483,
author = {Albadarneh, Jafar and Talafha, Bashar and Al-Ayyoub, Mahmoud and Zaqaibeh, Belal and Al-Smadi, Mohammad and Jararweh, Yaser and Benkhelifa, Elhadj},
title = {Using Big Data Analytics for Authorship Authentication of Arabic Tweets},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {Authorship authentication of a certain text is concerned with correctly attributing it to its author based on its contents. It is a very important problem with deep root in history as many classical texts have doubtful attributions. The information age and ubiquitous use of the Internet is further complicating this problem and adding more dimensions to it. We are interested in the modern version of this problem where the text whose authorship needs authentication is an online text found in online social networks. Specifically, we are interested in the authorship authentication of tweets. This is not the only challenging aspect we consider here. Another challenging aspect is the language of the tweets. Most current works and existing tools support English. We chose to focus on the very important, yet largely understudied, Arabic language. Finally, we add another challenging aspect to the problem at hand by addressing it at a very large scale. We present our effort to employ big data analytics to address the authorship authentication problem of Arabic tweets. We start by crawling a dataset of more than 53K tweets distributed across 20 authors. We then use preprocessing steps to clean the data and prepare it for analysis. The next step is to compute the feature vectors of each tweet. We use the Bag-Of-Words (BOW) approach and compute the weights using the Term Frequency-Inverse Document Frequency (TF-IDF). Then, we feed the dataset to a Naive Bayes classifier implemented on a parallel and distributed computing framework known as Hadoop. To the best of our knowledge, none of the previous works on authorship authentication of Arabic text addressed the unique challenges associated with (1) tweets and (2) large-scale datasets. This makes our work unique on many levels. The results show that the testing accuracy is not very high (61.6%), which is expected in the very challenging setting that we consider.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {448–452},
numpages = {5},
location = {Limassol, Cyprus},
series = {UCC '15}
}

