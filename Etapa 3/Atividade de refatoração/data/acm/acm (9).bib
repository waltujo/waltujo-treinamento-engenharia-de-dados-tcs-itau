@inproceedings{10.1145/3205651.3205701,
author = {Dagdia, Zaineb Chelly},
title = {A Distributed Dendritic Cell Algorithm for Big Data},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3205701},
doi = {10.1145/3205651.3205701},
abstract = {In this work, we focus on the Dendritic Cell Algorithm (DCA), a bio-inspired classifier, and its limitation when coping with very large datasets. To overcome this limitation, we propose a novel distributed DCA version for data classification based on the MapReduce framework to distribute the functioning of this algorithm through a cluster of computing elements. Our experimental results show that our proposed distributed solution is suitable to enhance the performance of the DCA enabling the algorithm to be applied over big data classification problems.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {103–104},
numpages = {2},
keywords = {scalability, big data, distributed processing, classification, dendritic cell algorithm},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1145/2464157.2466485,
author = {Bu, Yingyi and Borkar, Vinayak and Xu, Guoqing and Carey, Michael J.},
title = {A Bloat-Aware Design for Big Data Applications},
year = {2018},
isbn = {9781450321006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2464157.2466485},
doi = {10.1145/2464157.2466485},
abstract = {Over the past decade, the increasing demands on data-driven business intelligence have led to the proliferation of large-scale, data-intensive applications that often have huge amounts of data (often at terabyte or petabyte scale) to process. An object-oriented programming language such as Java is often the developer's choice for implementing such applications, primarily due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, significant performance problems can often be seen --- the combination of the inefficiencies inherent in a managed run-time system and the impact of the huge amount of data to be processed in the limited memory space often leads to memory bloat and performance degradation at a surprisingly early stage.This paper proposes a bloat-aware design paradigm towards the development of efficient and scalable Big Data applications in object-oriented GC enabled languages. To motivate this work, we first perform a study on the impact of several typical memory bloat patterns. These patterns are summarized from the user complaints on the mailing lists of two widely-used open-source Big Data applications. Next, we discuss our design paradigm to eliminate bloat. Using examples and real-world experience, we demonstrate that programming under this paradigm does not incur significant programming burden. We have implemented a few common data processing tasks both using this design and using the conventional object-oriented design. Our experimental results show that this new design paradigm is extremely effective in improving performance --- even for the moderate-size data sets processed, we have observed 2.5x+ performance gains, and the improvement grows substantially with the size of the data set.},
booktitle = {Proceedings of the 2013 International Symposium on Memory Management},
pages = {119–130},
numpages = {12},
keywords = {design, big data applications, memory bloat},
location = {Seattle, Washington, USA},
series = {ISMM '13}
}

@article{10.5555/2904298.2904304,
author = {Mackey, Andrew L.},
title = {Incorporating Big Data Technology into Computing Curriculum: Conference Tutorial},
year = {2016},
issue_date = {May 2016},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {31},
number = {5},
issn = {1937-4771},
abstract = {Description: In this era of mobile devices, cloud computing, social networking and big data, organizations are collecting and generating data in massive quantities at unprecedented rates. The rapid decline in the cost of technology has enabled firms to expand their utilization of data analytics in an attempt to achieve a greater understanding of their business processes. As the tsunami of data continues to increase in volume, velocity and variety, the traditional methods of information management through which relational database systems were primarily, if not singularly, utilized have evolved to incorporate a combination of new hardware and software solutions to address the relatively nascent problems that accompany expanded data collection and utilization.},
journal = {J. Comput. Sci. Coll.},
month = {may},
pages = {38–39},
numpages = {2}
}

@inproceedings{10.1145/2628071.2671429,
author = {Harshvardhan and Amato, Nancy M. and Rauchweger, Lawrence},
title = {Processing Big Data Graphs on Memory-Restricted Systems},
year = {2014},
isbn = {9781450328098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628071.2671429},
doi = {10.1145/2628071.2671429},
abstract = {With the advent of big-data, processing large graphs quickly has become increasingly important. Most existing approaches either utilize in-memory processing techniques, which can only process graphs that fit completely in RAM, or disk-based techniques that sacrifice performance.In this work, we propose a novel RAM-Disk hybrid approach to graph processing that can scale well from a single shared-memory node to large distributed-memory systems. It works by partitioning the graph into subgraphs that fit in RAM and uses a paging-like technique to load subgraphs. We show that without modifying the algorithms, this approach can scale from small memory-constrained systems (such as tablets) to large-scale distributed machines with 16,000+ cores.},
booktitle = {Proceedings of the 23rd International Conference on Parallel Architectures and Compilation},
pages = {517–518},
numpages = {2},
keywords = {parallel graph processing, out-of-core graph algorithms, graph analytics, distributed computing, big data},
location = {Edmonton, AB, Canada},
series = {PACT '14}
}

@inproceedings{10.1145/2812428.2812429,
author = {Pokorn\'{y}, Jaroslav},
title = {Database Technologies in the World of Big Data},
year = {2015},
isbn = {9781450333573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2812428.2812429},
doi = {10.1145/2812428.2812429},
abstract = {Now we have a number of database technologies called usually NoSQL, like key-value, column-oriented, and document stores as well as search engines and graph databases. Whereas SQL software vendors offer advanced products with the capability to handle highly complex queries and transactions, NoSQL databases share rather characteristics concerning scaling and performance, as e.g. auto-sharding, distributed query support, and integrated caching. Their drawbacks can be a lack of schema or data consistency, difficulty in testing and maintaining, and absence of a higher query language. Complex data modelling and the SQL language as the only access tool to data are missing here. On the other hand, last studies show that both SQL and NoSQL databases have value for both for transactional and analytical Big Data. Top databases providers offer rearchitected database technologies combining row data stores with columnar in-memory compression enabling processing large data sets and analytical querying, often over massive, continuous data streams. The technological progress led to development of massively parallel processing analytic databases. The paper presents some details of current database technologies, their pros and cons in different application environments, and emerging trends in this area.},
booktitle = {Proceedings of the 16th International Conference on Computer Systems and Technologies},
pages = {1–12},
numpages = {12},
keywords = {big analytics, NewSQL databases, data distribution, transaction processing, database technologies, big data, NoSQL databases},
location = {Dublin, Ireland},
series = {CompSysTech '15}
}

@inproceedings{10.1145/2588555.2594530,
author = {Halperin, Daniel and Teixeira de Almeida, Victor and Choo, Lee Lee and Chu, Shumo and Koutris, Paraschos and Moritz, Dominik and Ortiz, Jennifer and Ruamviboonsuk, Vaspol and Wang, Jingjing and Whitaker, Andrew and Xu, Shengliang and Balazinska, Magdalena and Howe, Bill and Suciu, Dan},
title = {Demonstration of the Myria Big Data Management Service},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2594530},
doi = {10.1145/2588555.2594530},
abstract = {In this demonstration, we will showcase Myria, our novel cloud service for big data management and analytics designed to improve productivity. Myria's goal is for users to simply upload their data and for the system to help them be self-sufficient data science experts on their data -- self-serve analytics. Using a web browser, Myria users can upload data, author efficient queries to process and explore the data, and debug correctness and performance issues. Myria queries are executed on a scalable, parallel cluster that uses both state-of-the-art and novel methods for distributed query processing. Our interactive demonstration will guide visitors through an exploration of several key Myria features by interfacing with the live system to analyze big datasets over the web.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {881–884},
numpages = {4},
keywords = {big data management, data management service, Myria},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@article{10.1007/s00778-017-0489-y,
author = {Sagi, Tomer and Gal, Avigdor},
title = {Non-Binary Evaluation Measures for Big Data Integration},
year = {2018},
issue_date = {February  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0489-y},
doi = {10.1007/s00778-017-0489-y},
abstract = {The evolution of data accumulation, management, analytics, and visualization has led to the coining of the term big data, which challenges the task of data integration. This task, common to any matching problem in computer science involves generating alignments between structured data in an automated fashion. Historically, set-based measures, based upon binary similarity matrices (match/non-match), have dominated evaluation practices of matching tasks. However, in the presence of big data, such measures no longer suffice. In this work, we propose evaluation methods for non-binary matrices as well. Non-binary evaluation is formally defined together with several new, non-binary measures using a vector space representation of matching outcome. We provide empirical analyses of the usefulness of non-binary evaluation and show its superiority over its binary counterparts in several problem domains.},
journal = {The VLDB Journal},
month = {feb},
pages = {105–126},
numpages = {22},
keywords = {Evaluation, Data integration, Matching}
}

@inproceedings{10.1145/2295136.2295148,
author = {Bhatti, Rafae and LaSalle, Ryan and Bird, Rob and Grance, Tim and Bertino, Elisa},
title = {Emerging Trends around Big Data Analytics and Security: Panel},
year = {2012},
isbn = {9781450312950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2295136.2295148},
doi = {10.1145/2295136.2295148},
abstract = {This panel will discuss the interplay between key emerging security trends centered around big data analytics and security. With the explosion of big data and advent of cloud computing, data analytics has not only become prevalent but also a critical business need. Internet applications today consume vast amounts of data collected from heterogeneous big data repositories and provide meaningful insights from it. These include applications for business forecasting, investment and finance, healthcare and well-being, science and hi-tech, to name a few. Security and operational intelligence is one of the critical areas where big data analytics is expected to play a crucial role. Security analytics in a big data environment presents a unique set of challenges, not properly addressed by the existing security incident and event monitoring (or SIEM) systems that typically work with a limited set of traditional data sources (firewall, IDS, etc.) in an enterprise network. A big data environment presents both a great opportunity and a challenge due to the explosion and heterogeneity of the potential data sources that extend the boundary of analytics to social networks, real time streams and other forms of highly contextual data that is characterized by high volume and speed. In addition to meeting infrastructure challenges, there remain additional unaddressed issues, including but not limited to development of self-evolving threat ontologies, integrated network and application layer analytics, and detection of "low and slow" attacks. At the same time, security analytics requires a high degree of data assurance, where assurance implies that the data be trustworthy as well as managed in a privacy preserving manner. Our panelists represent individuals from industry, academia, and government who are at the forefront of big data security analytics. They will provide insights into these unique challenges, survey the emerging trends, and lay out a vision for future.},
booktitle = {Proceedings of the 17th ACM Symposium on Access Control Models and Technologies},
pages = {67–68},
numpages = {2},
keywords = {privacy, big data, analytics, security},
location = {Newark, New Jersey, USA},
series = {SACMAT '12}
}

@inproceedings{10.1145/2377978.2377984,
author = {Gu, Xiaoyan and Hou, Rui and Zhang, Ke and Zhang, Lixin and Wang, Weiping},
title = {Application-Driven Energy-Efficient Architecture Explorations for Big Data},
year = {2011},
isbn = {9781450314398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377978.2377984},
doi = {10.1145/2377978.2377984},
abstract = {Building energy-efficient systems is critical for big data applications. This paper investigates and compares the energy consumption and the execution time of a typical Hadoop-based big data application running on a traditional Xeon-based cluster and an Atom-based (Micro-server) cluster. Our experimental results show that the micro-server platform is more energy-efficient than the Xeon-based platform. Our experimental results also reveal that data compression and decompression accounts for a considerable percentage of the total execution time. More precisely, data compression/decompression occupies 7-11% of the execution time of the map tasks and 37.9-41.2% of the execution time of the reduce tasks. Based on our findings, we demonstrate the necessity of using a heterogeneous architecture for energy-efficient big data processing. The desired architecture takes the advantages of both micro-server processors and hardware compression/decompression accelerators. In addition, we propose a mechanism that enables the accelerators to perform more efficient data compression/decompression.},
booktitle = {Proceedings of the 1st Workshop on Architectures and Systems for Big Data},
pages = {34–40},
numpages = {7},
keywords = {performance, energy-efficient, big data},
location = {Galveston Island, Texas, USA},
series = {ASBD '11}
}

@inproceedings{10.1109/JCDL.2019.00118,
author = {Chen, Jiangping and Lu, Wei and Zavalina, Oksana},
title = {Organizing Data, Information, and Knowledge in Big Data Environments},
year = {2020},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00118},
doi = {10.1109/JCDL.2019.00118},
abstract = {The explosion of information and the availability of big data provide new significant challenges for digital libraries. For example, libraries face the "cyberinfrastructural challenge" and the need to develop better understanding of research data to support curation, sharing and reuse of data generated by data-intensive science [9][6]. The synergy between information science and data science is emerging to address these Big Data environment challenges. The most fruitful collaboration areas for this synergy include those related to information organization: "big metadata, smart metadata" the ways to leverage the "metadata capital"[4]. The proposed one-day workshop will seek to support the interdisciplinary collaboration in this important area. It will focus on the challenges and opportunities provided by Big Data environment for information and computing professionals to explore innovative strategies and solutions for organizing data, information, and knowledge.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {459–460},
numpages = {2},
keywords = {big data analytics, information organization, data management, knowledge organization},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@article{10.1145/3412497,
author = {Peng, Lingxi and Liu, Haohuai and Nie, Yangang and Xie, Ying and Tang, Xuan and Luo, Ping},
title = {The Transnational Happiness Study with Big Data Technology},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3412497},
doi = {10.1145/3412497},
abstract = {Happiness is a hot topic in academic circles. The study of happiness involves many disciplines, such as philosophy, psychology, sociology, and economics. However, there are few studies on the quantitative analysis of the factors affecting happiness. In this article, we used the well-known World Values Survey Wave 6 (WV6) dataset to quantitatively analyze the happiness of 57 countries with Big Data techniques. First, we obtained the seven most important factors by constructing happiness decision trees for each country. Calculating the frequencies of these factors, we obtained the 17 most important indicators for the prediction of happiness in the world. Then, we selected five representative countries, namely, Sweden, Japan, India, China, and the USA, and analyzed the indicators with the random forest method. We identified different patterns of factors that influence happiness in different countries. This study is a successful attempt to apply data mining technology in the social sciences, and the results are of practical significance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {5},
numpages = {12},
keywords = {feature selection, decision tree, happiness, Big Data}
}

@inproceedings{10.1145/2945408.2945419,
author = {Bersani, Marcello M. and Marconi, Francesco and Rossi, Matteo and Erascu, Madalina},
title = {A Tool for Verification of Big-Data Applications},
year = {2016},
isbn = {9781450344111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2945408.2945419},
doi = {10.1145/2945408.2945419},
abstract = {Quality-driven frameworks for developing data-intensive applications are becoming more and more popular, following the remarkable popularity of Big Data approaches. The DICE framework, designed within the DICE project (www.dice-h2020.eu), has the goal of offering a novel profile and tools for data-aware quality-driven development. One of its tools is the DICE Verification Tool (D-VerT), which allows designers to evaluate their design against safety properties, such as reachability of undesired configurations of the system. This paper describes the first version of D-VerT, available open source at github.com/dice-project/DICE-Verification.},
booktitle = {Proceedings of the 2nd International Workshop on Quality-Aware DevOps},
pages = {44–45},
numpages = {2},
keywords = {Formal verification, temporal logic},
location = {Saarbr\"{u}cken, Germany},
series = {QUDOS 2016}
}

@inproceedings{10.1145/3369555.3369573,
author = {Ntehelang, Gomotsegang and Isong, Bassey and Lugayizi, Francis and Dladlu, Nosipho},
title = {IoT-Based Big Data Analytics Issues in Healthcare},
year = {2020},
isbn = {9781450371803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369555.3369573},
doi = {10.1145/3369555.3369573},
abstract = {Internet of Things devices constantly generate big data which when analyzed reveals hidden patterns, information and trends, thus, enabling decision making. Several organization today utilized data analytics to improve organizational performance and deliver high quality of service and experience. Healthcare is not an exception and uses data analytics for some of its capabilities such as detecting diseases and diagnose patients at early stages, identifying high-risk patients and providing them treatment to reduce unnecessary hospitalization or readmission. However, the existing and expected increase of connected devices poses several challenges for data analysis, especially with little work being done to address them. Therefore, this paper brings together some of these challenges that needs to be addressed and some of the proposed solutions. We performed a review of some of the studies in the literature with a view of providing research directions for researchers.},
booktitle = {Proceedings of the 3rd International Conference on Telecommunications and Communication Engineering},
pages = {16–21},
numpages = {6},
keywords = {healthcare, big data analytics, internet of things},
location = {Tokyo, Japan},
series = {ICTCE '19}
}

@inproceedings{10.1109/CCGrid.2016.63,
author = {Ordonez, Carlos and Garc\'{\i}a-Garc\'{\i}a, Javier},
title = {Managing Big Data Analytics Workflows with a Database System},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.63},
doi = {10.1109/CCGrid.2016.63},
abstract = {A big data analytics workflow is long and complex, with many programs, tools and scripts interacting together. In general, in modern organizations there is a significant amount of big data analytics processing performed outside a database system, which creates many issues to manage and process big data analytics workflows. In general, data preprocessing is the most time-consuming task in a big data analytics workflow. In this work, we defend the idea of preprocessing, computing models and scoring data sets inside a database system. In addition, we discuss recommendations and experiences to improve big data analytics workflows by pushing data preprocessing (i.e. data cleaning, aggregation and column transformation) into a database system. We present a discussion of practical issues and common solutions when transforming and preparing data sets to improve big data analytics workflows. As a case study validation, based on experience from real-life big data analytics projects, we compare pros and cons between running big data analytics workflows inside and outside the database system. We highlight which tasks in a big data analytics workflow are easier to manage and faster when processed by the database system, compared to external processing.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {649–655},
numpages = {7},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/2967938.2970374,
author = {Arvind, Arvind},
title = {Big Data Analytics on Flash Storage with Accelerators},
year = {2016},
isbn = {9781450341219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2967938.2970374},
doi = {10.1145/2967938.2970374},
abstract = {Complex analytics of the vast amount of data collected via social media, cell phones, ubiquitous smart sensors, and satellites is likely to be the biggest economic driver for the IT industry over the next decade. For many "Big Data" applications, the limiting factor in performance is often the transportation of large amount of data from hard disks to where it can be processed, i.e. DRAM. We will present BlueDBM, an architecture for a scalable distributed flash store which overcomes this limitation by providing a high-performance, high-capacity, scalable random-access flash storage, and by allowing computation near the data via a FPGA-based programmable flash controller. We will present the preliminary results for two applications, (1) key-value store (KVS) and (2) sparse-matrix accelerator for graph processing, on BlueDBM consisting of 20 nodes and 20TB of flash.},
booktitle = {Proceedings of the 2016 International Conference on Parallel Architectures and Compilation},
pages = {1},
numpages = {1},
keywords = {big data analytics, nand flash storage, in-storage computing, hardware accelerators},
location = {Haifa, Israel},
series = {PACT '16}
}

@inproceedings{10.1145/3078564.3078571,
author = {Liu, Xiaoxia and Du, Yuejin and Sun, Feiqiang and Zhai, Lidong},
title = {Design of Adaptive Learning System Based on Big Data},
year = {2017},
isbn = {9781450352109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078564.3078571},
doi = {10.1145/3078564.3078571},
abstract = {In1 recent years, with the rise and development of big data technology, research about big data of education has gradually become a hot research. Big data of education is an important means of modernization educational change based on data acquisition and analysis technology. The major applications of big data in education are educational data mining, learning analysis and educational decision-making and so on [1]. With the combination of big data and education, adaptive learning system [2] gradually into our sight. Adaptive learning system is a system that can provide a personalized learning service for learners. The system can recommended personalized learning path and learning resources according to learners' various characteristics and behavioral tendencies, such as learning style, media tendency, interest, cognitive level and so on. This paper aims to design an adaptive learning system based on the big data in education. The system contains four modules: domain module, student module, adaptive recommendation module and visual display module.},
booktitle = {Proceedings of the 6th International Conference on Information Engineering},
articleno = {10},
numpages = {5},
keywords = {adaptive learning, big data, data mining},
location = {Dalian Liaoning, China},
series = {ICIE '17}
}

@inproceedings{10.1145/3301551.3301566,
author = {Hyun, Youyung and Hosoya, Ryuichi and Kamioka, Taro},
title = {The Implications of Big Data Analytics Orientation upon Deployment},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301566},
doi = {10.1145/3301551.3301566},
abstract = {Big data analytics (BDA) is becoming a key way for leading companies to outperform their peers by utilizing big data and better understanding its business environment. However, there is evidence that many firms are facing difficulties in deploying BDA effectively into their business processes because of data silos, the rigid separation of data across divisions. In order to overcome data silos and capture the full potential of big data, it is considered important to create a corporate culture that encourages communication and sharing of data across departments. Therefore, to identify an appropriate corporate culture that supports BDA deployment in the context of big data, our research introduces BDA orientation which facilitates data flow across separate divisions and values business decisions based on insights gained from BDA. Drawing on resource-based view (RBV) and upper echelon theory, our research model examines the impact of BDA orientation on the deployment of BDA. Also, this study investigates the role of BDA orientation that mediates the effects of top management team initiative and BDA infrastructure on BDA deployment. To test our proposed model, an online survey was administered for a quantitative analysis and data from 166 Japanese upper-level managers were collected. Our findings confirm the significant impact of BDA orientation on BDA deployment, and the mediating role of BDA orientation in the suggested relationships above.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {42–48},
numpages = {7},
keywords = {big data analytics orientation, Big data, BDA deployment},
location = {Hong Kong, Hong Kong},
series = {ICIT 2018}
}

@inproceedings{10.1145/2835596.2835614,
author = {Liu, Kuien and Yao, Yandong and Guo, Danhuai},
title = {On Managing Geospatial Big-Data in Emergency Management: Some Perspectives},
year = {2015},
isbn = {9781450339704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835596.2835614},
doi = {10.1145/2835596.2835614},
abstract = {With the rapid growth of mobile devices and applications, geo-tagged data is becoming increasingly important in emergency management and has become a major workload for big data storage systems. Traditional methods that storing geospatial data in centralized databases suffer from inevitable limitations such like scaling out with the growing size of geospatial data. In order to achieve scalability, a number of solutions on big geospatial data management are proposed in recent years. We can simply classify them into two kinds: extending on distributed databases, or migrating to big-data storage systems. For previous, they mostly adopt the massive parallel processing (MPP) based architecture, in which data are stored and retrieved in a set of independent nodes. Each node can be treated as a traditional databases instance with geospatial extension. For the latter, existing solutions tend to build an additional index layer above general-purpose distributed data stores, e.g., HBASE, CASSANDRA, MangoDB, etc., to support geospatial data while integrating the big-data lineage. However, there are no absolutely perfect data management systems on the earth. Some approaches are desired for execution efficiency while some others are better on fulfilling the programming level need for big data scenarios.In this paper, we analysis the requirements and challenges on geospatial big data storage in emergency management, succeed with discussion with individual perspective from practical cases. The purpose of this paper is not only focused on how to program a geospatial data storage platform but also on how to approve the rationality of geospatial big data system that we plan to build.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management},
articleno = {5},
numpages = {4},
keywords = {perspectives, emergency management, geospatial, big data},
location = {Bellevue, Washington},
series = {EM-GIS '15}
}

@inproceedings{10.1145/2612669.2612702,
author = {Bodik, Peter and Menache, Ishai and Naor, Joseph (Seffi) and Yaniv, Jonathan},
title = {Deadline-Aware Scheduling of Big-Data Processing Jobs},
year = {2014},
isbn = {9781450328210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2612669.2612702},
doi = {10.1145/2612669.2612702},
abstract = {This paper presents a novel algorithm for scheduling big data jobs on large compute clusters. In our model, each job is represented by a DAG consisting of several stages linked by precedence constraints. The resource allocation per stage is malleable, in the sense that the processing time of a stage depends on the resources allocated to it (the dependency can be arbitrary in general).The goal of the scheduler is to maximize the total value of completed jobs, where the value for each job depends on its completion time. We design an algorithm for the problem which guarantees an expected constant approximation factor when the cluster capacity is sufficiently high. To the best of our knowledge, this is the first constant-factor approximation algorithm for the problem. The algorithm is based on formulating the problem as a linear program and then rounding an optimal (fractional) solution into a feasible (integral) schedule using randomized rounding.},
booktitle = {Proceedings of the 26th ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {211–213},
numpages = {3},
keywords = {big data, deadline-aware scheduling, scheduling algorithms},
location = {Prague, Czech Republic},
series = {SPAA '14}
}

@inproceedings{10.1145/3411681.3412951,
author = {Liao, TongXin and Feng, XinHui and Sun, YuanLi and Wang, HongTing and Liao, Cong and Li, YuanBing},
title = {Online Teaching Platform Based on Big Data Recommendation System},
year = {2020},
isbn = {9781450375757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411681.3412951},
doi = {10.1145/3411681.3412951},
abstract = {The essence of big data education is the combination of network education and traditional education. The development of big data and Internet industry has brought new changes to the traditional learning model. The way to acquire knowledge is not limited to books and classrooms, and the Internet era is the era of information explosion, so the Internet has gradually become the most important channel for people to acquire knowledge. Therefore, the big data learning platform has gradually become a new mode of teaching adopted by schools. In the big data teaching mode, personalized recommendation system plays an indispensable role in online education.},
booktitle = {Proceedings of the 5th International Conference on Information and Education Innovations},
pages = {35–39},
numpages = {5},
keywords = {recommended, Online teaching and cloud platform, Keywords Big data},
location = {London, United Kingdom},
series = {ICIEI '20}
}

@inproceedings{10.1145/2339530.2378371,
author = {Jordan, Michael I. and Faloutsos, Christos and Gao, Wen and Han, Jiawei and Zheng, Zijian and Fayyad, Usuama},
title = {Panel on Mining the Big Data},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2378371},
doi = {10.1145/2339530.2378371},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.1145/2491894.2466485,
author = {Bu, Yingyi and Borkar, Vinayak and Xu, Guoqing and Carey, Michael J.},
title = {A Bloat-Aware Design for Big Data Applications},
year = {2013},
isbn = {9781450321006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491894.2466485},
doi = {10.1145/2491894.2466485},
abstract = {Over the past decade, the increasing demands on data-driven business intelligence have led to the proliferation of large-scale, data-intensive applications that often have huge amounts of data (often at terabyte or petabyte scale) to process. An object-oriented programming language such as Java is often the developer's choice for implementing such applications, primarily due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, significant performance problems can often be seen --- the combination of the inefficiencies inherent in a managed run-time system and the impact of the huge amount of data to be processed in the limited memory space often leads to memory bloat and performance degradation at a surprisingly early stage.This paper proposes a bloat-aware design paradigm towards the development of efficient and scalable Big Data applications in object-oriented GC enabled languages. To motivate this work, we first perform a study on the impact of several typical memory bloat patterns. These patterns are summarized from the user complaints on the mailing lists of two widely-used open-source Big Data applications. Next, we discuss our design paradigm to eliminate bloat. Using examples and real-world experience, we demonstrate that programming under this paradigm does not incur significant programming burden. We have implemented a few common data processing tasks both using this design and using the conventional object-oriented design. Our experimental results show that this new design paradigm is extremely effective in improving performance --- even for the moderate-size data sets processed, we have observed 2.5x+ performance gains, and the improvement grows substantially with the size of the data set.},
booktitle = {Proceedings of the 2013 International Symposium on Memory Management},
pages = {119–130},
numpages = {12},
keywords = {big data applications, memory bloat, design},
location = {Seattle, Washington, USA},
series = {ISMM '13}
}

@article{10.1145/2555670.2466485,
author = {Bu, Yingyi and Borkar, Vinayak and Xu, Guoqing and Carey, Michael J.},
title = {A Bloat-Aware Design for Big Data Applications},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {11},
issn = {0362-1340},
url = {https://doi.org/10.1145/2555670.2466485},
doi = {10.1145/2555670.2466485},
abstract = {Over the past decade, the increasing demands on data-driven business intelligence have led to the proliferation of large-scale, data-intensive applications that often have huge amounts of data (often at terabyte or petabyte scale) to process. An object-oriented programming language such as Java is often the developer's choice for implementing such applications, primarily due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, significant performance problems can often be seen --- the combination of the inefficiencies inherent in a managed run-time system and the impact of the huge amount of data to be processed in the limited memory space often leads to memory bloat and performance degradation at a surprisingly early stage.This paper proposes a bloat-aware design paradigm towards the development of efficient and scalable Big Data applications in object-oriented GC enabled languages. To motivate this work, we first perform a study on the impact of several typical memory bloat patterns. These patterns are summarized from the user complaints on the mailing lists of two widely-used open-source Big Data applications. Next, we discuss our design paradigm to eliminate bloat. Using examples and real-world experience, we demonstrate that programming under this paradigm does not incur significant programming burden. We have implemented a few common data processing tasks both using this design and using the conventional object-oriented design. Our experimental results show that this new design paradigm is extremely effective in improving performance --- even for the moderate-size data sets processed, we have observed 2.5x+ performance gains, and the improvement grows substantially with the size of the data set.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {119–130},
numpages = {12},
keywords = {big data applications, design, memory bloat}
}

@inproceedings{10.1109/CCGrid.2015.168,
author = {Yin, Jiangling and Wang, Jun},
title = {Optimize Parallel Data Access in Big Data Processing},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.168},
doi = {10.1109/CCGrid.2015.168},
abstract = {Recent years the Hadoop Distributed File System(HDFS) has been deployed as the bedrock for many parallel big data processing systems, such as graph processing systems, MPI-based parallel programs and scala/java-based Spark frameworks, which can efficiently support iterative and interactive data analysis in memory. The first part of my dissertation mainly focuses on studying parallel data access on distributed file systems, e.g, HDFS. Since the distributed I/O resources and global data distribution are often not taken into consideration, the data requests from parallel processes/executors will unfortunately be served in a remote or imbalanced fashion on the storage servers. In order to address these problems, we develop I/O middleware systems and matching-based algorithms to map parallel data requests to storage servers such that local and balanced data access can be achieved. The last part of my dissertation presents our plans to improve the performance of interactive data access in big data analysis. Specifically, most interactive analysis programs will scan through the entire data set regardless of which data is actually required. We plan to develop a content-aware method to quickly access required data without this laborious scanning process.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {721–724},
numpages = {4},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@article{10.1145/3089251.3089252,
author = {Xue, Bing and Zhang, Mengjie},
title = {Evolutionary Feature Manipulation in Data Mining/Big Data},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
url = {https://doi.org/10.1145/3089251.3089252},
doi = {10.1145/3089251.3089252},
abstract = {Known as the GIGO (Garbage In, Garbage Out) principle, the quality of the input data highly influences or even determines the quality of the output of any machine learning, big data and data mining algorithm. The input data which is often represented by a set of features may suffer from many issues. Feature manipulation is an effective means to improve the feature set quality, but it is a challenging task. Evolutionary computation (EC) techniques have shown advantages and achieved good performance in feature manipulation. This paper reviews recent advances on EC based feature manipulation methods in classifcation, clustering, regression, incomplete data, and image analysis, to provide the community the state-of-the-art work in the field.},
journal = {SIGEVOlution},
month = {may},
pages = {4–11},
numpages = {8}
}

@inproceedings{10.1145/3323878.3325802,
author = {Haesendonck, Gerald and Maroy, Wouter and Heyvaert, Pieter and Verborgh, Ruben and Dimou, Anastasia},
title = {Parallel RDF Generation from Heterogeneous Big Data},
year = {2019},
isbn = {9781450367660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323878.3325802},
doi = {10.1145/3323878.3325802},
abstract = {To unlock the value of increasingly available data in high volumes, we need flexible ways to integrate data across different sources. While semantic integration can be provided through RDF generation, current generators insufficiently scale in terms of volume. Generators are limited by memory constraints. Therefore, we developed the RMLStreamer, a generator that parallelizes the ingestion and mapping tasks of RDF generation across multiple instances. In this paper, we analyze what aspects are parallelizable and we introduce an approach for parallel RDF generation. We describe how we implemented our proposed approach, in the frame of the RMLStreamer, and how the resulting scaling behavior compares to other RDF generators. The RMLStreamer ingests data at 50% faster rate than existing generators through parallel ingestion.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {1},
numpages = {6},
keywords = {big data, semantic web, RDF generation, linked data},
location = {Amsterdam, Netherlands},
series = {SBD '19}
}

@inproceedings{10.5555/3192424.3192597,
author = {Jiang, Fan and Leung, Carson K. and Pazdor, Adam G. M.},
title = {Big Data Mining of Social Networks for Friend Recommendation},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {In the current era of big data, high volumes of valuable data can be easily collected and generated. Social networks are examples of generating sources of these big data. Users in these social networks are often linked by some interdependency such as friendship. As these big social networks keep growing, there are situations in which an individual user wants to find popular groups of friends so that he can recommend the same groups to other users. In this paper, we present a big data analytic solution that uses the MapReduce model in mining these big social networks for discovering groups of frequently connected users for friend recommendation. Evaluation results show the efficiency and practicality of our data analytic solution in mining big social networks, discovering popular users, and recommending friends.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {921–922},
numpages = {2},
keywords = {friendship, big data mining, big data, social networks, social network analysis},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.1145/2168556.2168563,
author = {Holmqvist, Kenneth and Nystr\"{o}m, Marcus and Mulvey, Fiona},
title = {Eye Tracker Data Quality: What It is and How to Measure It},
year = {2012},
isbn = {9781450312219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2168556.2168563},
doi = {10.1145/2168556.2168563},
abstract = {Data quality is essential to the validity of research results and to the quality of gaze interaction. We argue that the lack of standard measures for eye data quality makes several aspects of manufacturing and using eye trackers, as well as researching eye movements and vision, more difficult than necessary. Uncertainty regarding the comparability of research results is a considerable impediment to progress in the field. In this paper, we illustrate why data quality matters and review previous work on how eye data quality has been measured and reported. The goal is to achieve a common understanding of what data quality is and how it can be defined, measured, evaluated, and reported.},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
pages = {45–52},
numpages = {8},
keywords = {eye tracker, eye movements, latency, data quality, accuracy, precision},
location = {Santa Barbara, California},
series = {ETRA '12}
}

@inproceedings{10.1145/3396956.3398253,
author = {Potiguara Carvalho, Artur and Potiguara Carvalho, Fernanda and Dias Canedo, Edna and Potiguara Carvalho, Pedro Henrique},
title = {Big Data, Anonymisation and Governance to Personal Data Protection},
year = {2020},
isbn = {9781450387910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396956.3398253},
doi = {10.1145/3396956.3398253},
abstract = {In a massive processing data era, an emerging impasse has taking scenario: privacy. In this context, personal data receive particular attention, witch its laws and guidelines that ensure better and legal use of data. The General Data Protection Regulation (GDPR) - in the European Union - and the Brazilian General Data Protection Law (LGPD) - in Brazil - lead to anonymisation (and its processes and techniques) as a way to reach secure use of personal data. However, expectations placed on this tool must be reconsidered according to risks and limits of its use, mainly when this technique is applied to Big Data. We discussed whether anonymisation used in conjunction with good data governance practices could provide greater protection for privacy. We conclude that good governance practices can strengthen privacy in anonymous data belonging to a Big Data, and we present a suggestive governance framework aimed at privacy.},
booktitle = {The 21st Annual International Conference on Digital Government Research},
pages = {185–195},
numpages = {11},
keywords = {Governance, Anonymisation, Personal Data Protection, Big Data, Privacy},
location = {Seoul, Republic of Korea},
series = {dg.o '20}
}

@article{10.14778/3368289.3368299,
author = {Leeka, Jyoti and Rajan, Kaushik},
title = {Incorporating Super-Operators in Big-Data Query Optimizers},
year = {2019},
issue_date = {November 2019},
publisher = {VLDB Endowment},
volume = {13},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3368289.3368299},
doi = {10.14778/3368289.3368299},
abstract = {The cost of big-data analytics is dominated by shuffle operations that induce multiple disk reads, writes and network transfers. This paper proposes a new class of optimization rules that are specifically aimed at eliminating shuffles where possible. The rules substitute multiple shuffle inducing operators (Join, UnionAll, Spool, GroupBy) with a single streaming operator which implements an entire sub-query. We call such operators super-operators.A key challenge with adding new rules that substitute sub-queries with super-operators is that there are many variants of the same sub-query that can be implemented via minor modifications to the same super-operator. Adding each as a separate rule leads to a search space explosion. We propose several extensions to the query optimizer to address this challenge. We propose a new abstract representation for operator trees that captures all possible sub-queries that a super-operator implements. We propose a new rule matching algorithm that can efficiently search for abstract operator trees. Finally we extend the physical operator interface to introduce new parametric super-operators.We implement our changes in SCOPE, a state-of-the-art production big-data optimizer used extensively at Microsoft. We demonstrate that the proposed optimizations provide significant reduction in both resource cost (average 1.7x) and latency (average 1.5x) on several production queries, and do so without increasing optimization time.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {348–361},
numpages = {14}
}

@inproceedings{10.1145/3085228.3085275,
author = {Gong, Yiwei and Janssen, Marijn},
title = {Enterprise Architectures for Supporting the Adoption of Big Data},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085275},
doi = {10.1145/3085228.3085275},
abstract = {Governments from all over the world are struggling to take advantage of big data developments. Enterprise Architecture (EA) can be used as an instrument to integrate big data (BD) in the existing business processes and ICT-landscape. In this policy paper, we explore the role of EA in the adoption of BD. For this, we adopted a qualitative case study approach and investigated a large administrative organization that was in the process of adopting BD. We found in our case study that the first attempts were focused on integrating big data in the current landscape, but this encountered too many challenges that halt progress. To overcome the challenges, a separate BD department and accompanying infrastructure was created. The strategy was first to reap the benefits of BD and to understand what should be done, and thereafter integrating the working systems in the existing landscape. The findings suggest that current infrastructures might not be suitable for integrating BD and substantial changes are needed first. In the case the role of BD needed to be first clarified before EA could play a role in adopting BD. EA should deal with the uncertainties and complexities by ensuring a configurable landscape, by providing an incremental approach for adapting the infrastructure step-by-step, before the benefits of big data can be gained. Developing an incremental migration plan was found to be a key aspect for the adoption of BD.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {505–510},
numpages = {6},
keywords = {infrastructure, e-government, ICT-architecture, enterprise architecture, big data, open data, BOLD},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@article{10.1145/3362121,
author = {Firmani, Donatella and Tanca, Letizia and Torlone, Riccardo},
title = {Ethical Dimensions for Data Quality},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3362121},
doi = {10.1145/3362121},
journal = {J. Data and Information Quality},
month = {dec},
articleno = {2},
numpages = {5},
keywords = {source selection, Data integration, knowledge extraction}
}

@article{10.1145/2424840.2424842,
author = {Wilkes, Stephany},
title = {Some Impacts of "Big Data" on Usability Practice},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2166-1200},
url = {https://doi.org/10.1145/2424840.2424842},
doi = {10.1145/2424840.2424842},
abstract = {Two shifts in the technological landscape -- the era of "big data" and the popularity of Agile software development methodologies -- have made users (and specifically data about them) central to the development process and broadened the definition of user-centered design and usability testing. This paper briefly describes the impact of these shifts on the usability practice. Rudimentary data types useful to usability practitioners are introduced, as well as helpful data tools and required skills. The paper concludes with a list of methodological and pedagogical gaps that should be addressed.},
journal = {Commun. Des. Q. Rev},
month = {jun},
pages = {25–32},
numpages = {8},
keywords = {SIGDOC, ACM proceedings}
}

@article{10.1145/3337065,
author = {Dai, Hong-Ning and Wong, Raymond Chi-Wing and Wang, Hao and Zheng, Zibin and Vasilakos, Athanasios V.},
title = {Big Data Analytics for Large-Scale Wireless Networks: Challenges and Opportunities},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3337065},
doi = {10.1145/3337065},
abstract = {The wide proliferation of various wireless communication systems and wireless devices has led to the arrival of big data era in large-scale wireless networks. Big data of large-scale wireless networks has the key features of wide variety, high volume, real-time velocity, and huge value leading to the unique research challenges that are different from existing computing systems. In this article, we present a survey of the state-of-art big data analytics (BDA) approaches for large-scale wireless networks. In particular, we categorize the life cycle of BDA into four consecutive stages: Data Acquisition, Data Preprocessing, Data Storage, and Data Analytics. We then present a detailed survey of the technical solutions to the challenges in BDA for large-scale wireless networks according to each stage in the life cycle of BDA. Moreover, we discuss the open research issues and outline the future directions in this promising area.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {99},
numpages = {36},
keywords = {wireless networks, machine learning, Big data}
}

@inproceedings{10.1145/3277104.3277107,
author = {Gao, Fei and Dutta, Ananya and Liu, Jiangjiang},
title = {Content-Based Textual Big Data Analysis and Compression},
year = {2018},
isbn = {9781450365406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277104.3277107},
doi = {10.1145/3277104.3277107},
abstract = {With the growing enhancement of technology and the Internet, the number of people who are using the Internet is increasing daily. Users are engaged in web searching and accessing different types of websites, such as social media, banking, etc. As a result, a large volume of data is being generated in every day. It is necessary to load this data for analysis purposes. However, memory space and transmission time are the most important factors of limited processing. In most cases, we only need to extract the important textual data from these vast raw datasets. In this work, we propose content-based compression (CBC) for textual data analysis on the basis of the Huffman Code. The data is pre-analyzed to find very high frequency words and then a shorter symbol is inserted to replace those words. This compression approach is performed in an effort to maintain the original format of the data so that, compressed data structure could be completely transparent to Hadoop platform. The algorithm is evaluated on a set of real world data sets (e.g. Amazon movie review, food review, etc.) and a 52.4% average data size reduction is obtained from the experiment. Though this gain may seem modest, this can be supplementary to all other compression optimization techniques. Furthermore, the proposed technique can be effectively applied for the big data optimization purpose.},
booktitle = {Proceedings of the 2018 International Conference on Computing and Big Data},
pages = {7–12},
numpages = {6},
keywords = {Huffman Tree Algorithm, Hadoop, Compression, text-based encoding},
location = {Charleston, SC, USA},
series = {ICCBD '18}
}

@inproceedings{10.1145/3417990.3422004,
author = {Khalajzadeh, Hourieh and Verma, Tarun and Simmons, Andrew J. and Grundy, John and Abdelrazek, Mohamed and Hosking, John},
title = {User-Centred Tooling for Modelling of Big Data Applications},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3422004},
doi = {10.1145/3417990.3422004},
abstract = {We outline the key requirements for a Big Data modelling recommender tool. Our web-based tool is suitable for capturing system requirements in big data analytics applications involving diverse stakeholders. It promotes awareness of the datasets and algorithm implementations that are available to leverage in the design of the solution. We implement these ideas in BiDaML-web, a proof of concept recommender system for Big Data applications, and evaluate the tool using an empirical study with a group of 16 target end-users. Participants found the integrated recommender and technique suggestion tools helpful and highly rated the overall BiDaML web-based modelling experience. BiDaML-web is available at https://bidaml.web.app/ and the source code can be accessed at https://github.com/tarunverma23/bidaml.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {7},
numpages = {5},
keywords = {big data applications, recommender, BiDaML},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3105831.3105841,
author = {Costa, Carlos and Santos, Maribel Yasmina},
title = {The SusCity Big Data Warehousing Approach for Smart Cities},
year = {2017},
isbn = {9781450352208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105831.3105841},
doi = {10.1145/3105831.3105841},
abstract = {Nowadays, the concept of Smart City provides a rich analytical context, highlighting the need to store and process vast amounts of heterogeneous data flowing at different velocities. This data is defined as Big Data, which imposes significant difficulties in traditional data techniques and technologies. Data Warehouses (DWs) have long been recognized as a fundamental enterprise asset, providing fact-based decision support for several organizations. The concept of DW is evolving. Traditionally, Relational Database Management Systems (RDBMSs) are used to store historical data, providing different analytical perspectives regarding several business processes. With the current advancements in Big Data techniques and technologies, the concept of Big Data Warehouse (BDW) emerges to surpass several limitations of traditional DWs. This paper presents a novel approach for designing and implementing BDWs, which has been supporting the SusCity data visualization platform. The BDW is a crucial component of the SusCity research project in the context of Smart Cities, supporting analytical tasks based on data collected in the city of Lisbon.},
booktitle = {Proceedings of the 21st International Database Engineering &amp; Applications Symposium},
pages = {264–273},
numpages = {10},
keywords = {NoSQL, Big Data Warehousing, Data Warehouse, Smart Cities, Big Data, Hadoop},
location = {Bristol, United Kingdom},
series = {IDEAS '17}
}

@article{10.5555/2835377.2835394,
author = {Eckroth, Joshua},
title = {Foundations of a Cross-Disciplinary Pedagogy for Big Data},
year = {2016},
issue_date = {January 2016},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {31},
number = {3},
issn = {1937-4771},
abstract = {The increasing awareness of "big data" is transforming the academic and business landscape across many disciplines. Yet, big data programming environments are still too complex for non-programmers to utilize. To our knowledge, only computer scientists are ever exposed to big data processing concepts and tools in undergraduate education. Furthermore, non-computer scientists may lack sufficient common ground with computer scientists to explain their specialized big data processing needs. In order to bridge this gap and enhance collaboration among persons with big data processing needs and persons who are trained in programming and system building, we propose the foundations of a cross-disciplinary pedagogy that exposes big data processing paradigms and design decisions at an abstract level. With these tools, students and experts from different disciplines can more effectively collaborate on solving big data problems.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {110–118},
numpages = {9}
}

@inproceedings{10.1145/3281375.3281393,
author = {Alouneh, Sahel and Hababeh, Ismail and Alajrami, Tamer},
title = {Toward Big Data Analysis to Improve Enterprise Information Security},
year = {2018},
isbn = {9781450356220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281375.3281393},
doi = {10.1145/3281375.3281393},
abstract = {In recent years, big data and cloud computing are considered key trends of modern computer technology. Extracting valuable information is the key purpose of analyzing big data that needs to be secured in order to avoid any potential risks. Most cloud systems applications contain sensitive data, such as; financial, legal and private information. Therefore, threats on such data may put cloud systems holding this data at high risk. The demand on securing cloud systems applications has been increasing rapidly; however, big data protection is still a challenge. This paper proposes a new methodology to protect big data during analysis by classifying data before any action such as moving, copying or processing take place. Big data files are classified according to the criticality level of their contents into three categories from the most to the least sensitive: restricted, confidential and public. Based on big data classification, the encryption algorithm AES 128 is applied on confidential big data, while the encryption algorithm AES 256 is applied on the restricted big data files. The experimental results show that our method enhances the performance of big data analysis systems and outperforms other approaches in the literature.},
booktitle = {Proceedings of the 10th International Conference on Management of Digital EcoSystems},
pages = {106–109},
numpages = {4},
keywords = {encryption, threats, data classification},
location = {Tokyo, Japan},
series = {MEDES '18}
}

@inproceedings{10.1145/3402569.3409038,
author = {Changxin, Song and Ke, Ma},
title = {Research on Big Data Mining Method of Bioinformatics},
year = {2020},
isbn = {9781450377546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402569.3409038},
doi = {10.1145/3402569.3409038},
abstract = {With the updating and upgrading of hospital network information diagnosis and treatment, in the medical disease diagnosis, data processing and scientific research, we usually use a variety of big data and cloud computing technology, combined with clinical biology, medicine and other diseases diagnosis and treatment plan, and carry out biological information data collection, mining, analysis, processing and storage. In order to meet the needs of data variables for different medical treatment activities, this article focuses on the analysis of the causes of hypertension diseases in different regions around the essential hypertension (EH) by using the improved ant colony optimization algorithm (IACO), including the external environment, racial differences, genetic heterogeneity, and the single nucleotide polymorphism (SNP) polymorphism data and SNP-SNP combination in each gene coding region. Information mining and parallel computing analysis are carried out to obtain more objective and accurate results of complex disease assessment.},
booktitle = {Proceedings of the 5th International Conference on Distance Education and Learning},
pages = {166–169},
numpages = {4},
keywords = {big data mining, research, methods, Bioinformatics},
location = {Beijing, China},
series = {ICDEL 2020}
}

@inproceedings{10.1145/3230348.3230368,
author = {Youness, Madani and Mohammed, Erritali and Jamaa, Bengourram},
title = {Twitter Data Classification Using Big Data Technologies},
year = {2018},
isbn = {9781450363754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230348.3230368},
doi = {10.1145/3230348.3230368},
abstract = {Tweets classification or in general the classification of the social network's data is a recent field of scientific research, where researchers look for new methods to classify users data (tweets, Facebook's post...) into classes (positive, negative, neutral).This type of scientific research called sentiment analysis (SA) or opinion mining and it allows to extract the feelings, opinions or attitudes expressed in a tweet or a facebook post ...In this article, we describe how we can collect and store a large volume of data, which is in the form of tweets, in Hadoop Distributed File System (HDFS), and how we can classify these tweets using different classification methods, making a comparison between the well-known machine learning algorithms and a dictionary based-approach using the AFINN dictionary. The experimental results show that the AFINN dictionary outperforms the well-known machine learning algorithms.},
booktitle = {Proceedings of the 2018 1st International Conference on Internet and E-Business},
pages = {124–129},
numpages = {6},
keywords = {sentiment analysis, data mining, Twitter, opinion mining, big data, Hadoop},
location = {Singapore, Singapore},
series = {ICIEB '18}
}

@inproceedings{10.1145/3006386.3006387,
author = {Raghothama, Jayanth and Shreenath, Vinutha Magal and Meijer, Sebastiaan},
title = {Analytics on Public Transport Delays with Spatial Big Data},
year = {2016},
isbn = {9781450345811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006386.3006387},
doi = {10.1145/3006386.3006387},
abstract = {The increasing pervasiveness of location-aware technologies is leading to the rise of large, spatio-temporal datasets and to the opportunity of discovering usable knowledge about the behaviors of people and objects. Applied extensively in transportation, spatial big data and its analytics can deliver useful insights on a number of different issues such as congestion, delays, public transport reliability and so on. Predominantly studied for its use in operational management, spatial big data can be used to provide insight in strategic applications as well, from planning and design to evaluation and management. Such large scale, streaming spatial big data can be used in the improvement of public transport, for example the design of public transport networks and reliability. In this paper, we analyze GTFS data from the cities of Stockholm and Rome to gain insight on the sources and factors influencing public transport delays in the cities. The analysis is performed on a combination of GTFS data with data from other sources. The paper points to key issues in the analysis of real time data, driven by the contextual setting in the two cities.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {28–33},
numpages = {6},
keywords = {public transport, big data, decision making},
location = {Burlingame, California},
series = {BigSpatial '16}
}

@article{10.1145/2976744,
author = {Yu, Kui and Wu, Xindong and Ding, Wei and Pei, Jian},
title = {Scalable and Accurate Online Feature Selection for Big Data},
year = {2016},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/2976744},
doi = {10.1145/2976744},
abstract = {Feature selection is important in many big data applications. Two critical challenges closely associate with big data. First, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Second, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a <underline>S</underline>calable and <underline>A</underline>ccurate <underline>O</underline>n<underline>L</underline>ine <underline>A</underline>pproach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintains a parsimonious model over time in an online manner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real datasets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on datasets of extremely high dimensionality and have superior performance over the state-of-the-art feature selection methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {dec},
articleno = {16},
numpages = {39},
keywords = {big data, extremely high dimensionality, group features, Online feature selection}
}

@inproceedings{10.1145/3147213.3155013,
author = {Chang, Wo},
title = {NIST Big Data Reference Architecture for Analytics and Beyond},
year = {2017},
isbn = {9781450351492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147213.3155013},
doi = {10.1145/3147213.3155013},
abstract = {Big Data is the term used to describe the deluge of data in our networked, digitized, sensor-laden, information driven world. There is a broad agreement among commercial, academic, and government leaders about the remarkable potential of "Big Data" to spark innovation, fuel commerce, and drive progress. The availability of vast data resources carries the potential to answer questions previously out of reach. However, there is also broad agreement on the ability of Big Data to overwhelm traditional approaches. Big Data architectures come in many shapes and forms ranging from academic research settings to product-oriented workflows. With massive-scale dynamic data being generate from social media, Internet of Things, Smart Cities, and others, it is critical to analyze these data in real-time and provide proactive decision. With the advancement of computer architecture in multi-cores and GPUs, and fast communication between CPUs and GPUs, parallel processing utilizes these platforms could optimize resources at a reduced time. This presentation will provide the past, current, and future activities of the NIST Big Data Public Working Group (NBD-PWG) and how the NIST Reference Architecture may address the rate at which data volumes, speeds, and complexity are growing requires new forms of computing infrastructure to enable Big Data analytics interoperability such that analytics tools can be re-usable, deployable, and operational. The focus of NBD-PWG is to form a community of interest from industry, academia, and government, with the goal of developing consensus definitions, taxonomies, secure reference architectures, and standards roadmap which would create vendor-neutral, technology and infrastructure agnostic framework. The aim is to enable Big Data stakeholders to pick-and-choose best analytics tools for their processing under the most suitable computing platforms and clusters while allowing value-additions from Big Data service providers and flow of data between the stakeholders in a cohesive and secure manner.},
booktitle = {Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {3},
numpages = {1},
keywords = {many cpus/cores/gpus, big data reference architecture, big data analytics, high-performance computing},
location = {Austin, Texas, USA},
series = {UCC '17}
}

@inproceedings{10.1145/3341161.3343518,
author = {Xylogiannopoulos, Konstantinos and Karampelas, Panagiotis and Alhajj, Reda},
title = {Multivariate Motif Detection in Local Weather Big Data},
year = {2020},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3343518},
doi = {10.1145/3341161.3343518},
abstract = {In recent years, there are very frequent reports of disasters attributed to the climate change and there are several reports that these extreme phenomena will further affect people not only as weather disasters but also indirectly with the shortage of natural resources such as water or food due to the climate change. Towards this direction, there is an on-going research that studies weather phenomena by collecting data not only in the surface of the globe but also at the different levels of the atmosphere. Having such a large volume of data, traditional numerical weather prediction models may not be able to assimilate those data and extract knowledge useful for the prediction of extreme phenomena. Thus, analysis of weather data has been transformed into a big data analytics problem which may enable weather scientists to better understand the interrelations of the weather variables and use the knowledge discovered to improve their prediction models. In this context, the current paper proposes a big data analytics methodology that is able to detect all common patterns between different weather variables in neighboring or distant points in a specific time window revealing useful associations between weather variables which is not possible to detect otherwise with the traditional numerical methods. The proposed methodology is based on a data structure that is able to store the magnitude of the weather data in different dimensions and a pattern detection algorithm which is able to detect all common patterns. The experimental results using weather data from the National Oceanic and Atmospheric Administration (NOAA) revealed interesting otherwise unknown patterns in two weather variables for two specific locations that were studied.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {749–756},
numpages = {8},
keywords = {ARPaD, weather analysis, LERP-RSA, data mining},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@article{10.5555/3007225.3007243,
author = {Geise, Mary Jo and Grant, Navneet and Miller, Rick},
title = {How is Big Data Changing What We Do? Panel Discussion},
year = {2016},
issue_date = {October 2016},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {32},
number = {1},
issn = {1937-4771},
abstract = {Big data is currently being created at a staggering estimate of 2.5 exabytes per day with data from sources such as sensors, social media sites, mobile devices, and business transaction records. With this quantity of generated and stored data (Volume), the speed at which these data are being generated (Velocity) and the various forms of data (Variety), traditional relational database processing techniques are unable to effectively handle the size and complexity of these data sets. This panel will discuss how businesses are currently dealing with big data, needs businesses have for scientists able to process these data, and how educational institutions can better prepare students for the challenges of big data.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {81},
numpages = {1}
}

@inproceedings{10.1145/2623330.2623615,
author = {Anagnostopoulos, Christos and Triantafillou, Peter},
title = {Scaling out Big Data Missing Value Imputations: Pythia vs. Godzilla},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623615},
doi = {10.1145/2623330.2623615},
abstract = {Solving the missing-value (MV) problem with small estimation errors in big data environments is a notoriously resource-demanding task. As datasets and their user community continuously grow, the problem can only be exacerbated. Assume that it is possible to have a single machine (`Godzilla'), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which accesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to access only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla's imputation estimation errors? In this paper we derive answers to these fundamentals questions and develop principled methods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pillars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive experimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {651–660},
numpages = {10},
keywords = {clustering, missing value, big data},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{10.1145/3097983.3098179,
author = {Yan, Yizhou and Cao, Lei and Kulhman, Caitlin and Rundensteiner, Elke},
title = {Distributed Local Outlier Detection in Big Data},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098179},
doi = {10.1145/3097983.3098179},
abstract = {In this work, we present the first distributed solution for the Local Outlier Factor (LOF) method -- a popular outlier detection technique shown to be very effective for datasets with skewed distributions. As datasets increase radically in size, highly scalable LOF algorithms leveraging modern distributed infrastructures are required. This poses significant challenges due to the complexity of the LOF definition, and a lack of access to the entire dataset at any individual compute machine. Our solution features a distributed LOF pipeline framework, called DLOF. Each stage of the LOF computation is conducted in a fully distributed fashion by leveraging our invariant observation for intermediate value management. Furthermore, we propose a data assignment strategy which ensures that each machine is self-sufficient in all stages of the LOF pipeline, while minimizing the number of data replicas. Based on the convergence property derived from analyzing this strategy in the context of real world datasets, we introduce a number of data-driven optimization strategies. These strategies not only minimize the computation costs within each stage, but also eliminate unnecessary communication costs by aggressively pushing the LOF computation into the early stages of the DLOF pipeline. Our comprehensive experimental study using both real and synthetic datasets confirms the efficiency and scalability of our approach to terabyte level data.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1225–1234},
numpages = {10},
keywords = {local outlier, distributed processing, big data},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3006299.3006311,
author = {Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Mar\'{\i}n-Torder, Eva},
title = {Towards a Comprehensive Data Lifecycle Model for Big Data Environments},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006311},
doi = {10.1145/3006299.3006311},
abstract = {A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource; however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation.Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity; and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {100–106},
numpages = {7},
keywords = {data organization, data lifecycle, vs challenges, data management, big data, data complexity},
location = {Shanghai, China},
series = {BDCAT '16}
}

@article{10.14778/3229863.3229867,
author = {Schelter, Sebastian and Lange, Dustin and Schmidt, Philipp and Celikel, Meltem and Biessmann, Felix and Grafberger, Andreas},
title = {Automating Large-Scale Data Quality Verification},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3229867},
doi = {10.14778/3229863.3229867},
abstract = {Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1781–1794},
numpages = {14}
}

@article{10.1145/3158341,
author = {Penkler, Dave},
title = {Technology and Business Challenges of Big Data in the Digital Economy: Big Data (Ubiquity Symposium)},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2018},
number = {January},
url = {https://doi.org/10.1145/3158341},
doi = {10.1145/3158341},
abstract = {The early digital economy during the dot-com days of internet commerce successfully faced its first big data challenges of click-stream analysis with map-reduce technology. Since then the digital economy has been becoming much more pervasive. As the digital economy evolves, looking to benefit from its burgeoning big data assets, an important technical-business challenge is emerging: How to acquire, store, access, and exploit the data at a cost that is lower than the incremental revenue or GDP that its exploitation generates. Especially now that efficiency increases, which lasted for 50 years thanks to improvements in semiconductor manufacturing, is slowing and coming to an end.},
journal = {Ubiquity},
month = {jan},
articleno = {3},
numpages = {9}
}

