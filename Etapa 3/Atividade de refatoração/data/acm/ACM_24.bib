@inproceedings{10.1145/3511808.3557185,title = {System-Auditing, Data Analysis and Characteristics of Cyber Attacks for Big Data Systems}, author = {Huang Liangyi , Hall Sophia , Shao Fei , Nihar Arafath , Chaudhary Vipin , Wu Yinghui , French Roger , Xiao Xusheng },year = {2022}, isbn = {9781450392365}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3511808.3557185}, doi = {10.1145/3511808.3557185}, abstract = {Using big data, distributed computing systems such as Apache Hadoop requires processing massive amount of data to support business and research applications. Thus, it is critical to ensure the cyber security of such systems. To better defend from advanced cyber attacks that pose threats to even well-protected enterprises, system-auditing based techniques have been adopted for monitoring system activities and assisting attack investigation. In this demo, we are building a system that collects system auditing logs from a big data system and performs data analysis to understand how system auditing can be used more effectively to assist attack investigation on big systems. We also built a demo application that detects unexpected file deletion and presents root causes for the deletion.}, location = {Atlanta, GA, USA}, series = {CIKM '22}, pages = {4872\u20134876}, numpages = {5}, keywords = {big data systems, cyber attack investigation, system auditing}}
@inproceedings{10.1145/3485190.3485204,title = {The\u00a0Effect Mechanism of Financial Technology on Big Data Industry Development in China}, author = {Yuan Lv Zhi , Mu Zhang },year = {2021}, isbn = {9781450384278}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3485190.3485204}, doi = {10.1145/3485190.3485204}, abstract = {In order to investigate the mechanism of financial technology on big data industry, this paper selected the relevant data of 30 provinces (autonomous regions and municipalities) in China from 2013 to 2018 to establish a dynamic panel data model to measure the impact of financial technology on big data industry; and then used the mediating effect test method to test the action path of financial technology on big data industry through big data enterprise financing constraints. The results show that the regression coefficient of financial technology to big data industry is significantly positive at the significance level of 5%, it indicates that the financial technology can directly promote the development of big data industry. In the control variables, the regional technological innovation ability and the degree of opening to the outside world play a significant role in promoting the big data industry. In addition, the big data enterprise financing constraints have a partial mediating effect, the mediating effect account for 12.27% of the total effect, it indicates that the financial technology can indirectly promote the development of big data industry by alleviating the big data enterprise financing constraints.}, location = {Chengdu, China}, series = {IMMS 2021}, pages = {82\u201388}, numpages = {7}, keywords = {mediating effect, big data industry, dynamic panel data model, enterprise financing constraints, financial technology}}
@inproceedings{10.1145/2396556.2396578,title = {Distributed adaptive routing for big-data applications running on data center networks}, author = {Zahavi Eitan , Keslassy Isaac , Kolodny Avinoam },year = {2012}, isbn = {9781450316859}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2396556.2396578}, doi = {10.1145/2396556.2396578}, abstract = {With the growing popularity of big-data applications, Data Center Networks increasingly carry larger and longer traffic flows. As a result of this increased flow granularity, static routing cannot efficiently load-balance traffic, resulting in an increased network contention and a reduced throughput. Unfortunately, while adaptive routing can solve this load-balancing problem, network designers refrain from using it, because it also creates out-of-order packet delivery that can significantly degrade the reliable transport performance of the longer flows. In this paper, we show that by throttling each flow bandwidth to half of the network link capacity, a distributed-adaptive-routing algorithm is able to converge to a non-blocking routing assignment within a few iterations, causing minimal out-of-order packet delivery. We present a Markov chain model for distributed-adaptive-routing in the context of Clos networks that provides an approximation for the expected convergence time. This model predicts that for full-link-bandwidth traffic, the convergence time is exponential with the network size, so out-of-order packet delivery is unavoidable for long messages. However, with half-rate traffic, the algorithm converges within a few iterations and exhibits weak dependency on the network size. Therefore, we show that distributed-adaptive-routing may be used to provide a scalable and non-blocking routing even for long flows on a rearrangeably-non-blocking Clos network under half-rate conditions. The proposed model is evaluated and approximately fits the abstract system simulation model. Hardware implementation guidelines are provided and evaluated using a detailed flit-level InfiniBand simulation model. These results directly apply to adaptive-routing systems designed and deployed in various fields.}, location = {Austin, Texas, USA}, series = {ANCS '12}, pages = {99\u2013110}, numpages = {12}, keywords = {big-data, adaptive routing, data center networks}}
@inproceedings{10.1145/2538862.2544296,title = {Developing a game-based learning curriculum for \"Big Data\" in middle school (abstract only)}, author = {Mart\u00ednez-Arocho Allison G. , Buffum Philip Sheridan , Boyer Kristy Elizabeth },year = {2014}, isbn = {9781450326056}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2538862.2544296}, doi = {10.1145/2538862.2544296}, abstract = {Exposing students early to computer science may influence their choice of career, and there is increasing recognition that even for students who do not pursue computer science careers, computational literacy is important. This poster reports on a project targeting the development of a new middle school computer science curriculum. This research aims to highlight the role of computation in Big Data in the context of middle school computer science education, which serves as a catalyst to keep students engaged in computer science through middle school via the ENGAGE narrative game-based learning environment. This poster discusses steps taken to validate one activity meant to highlight the role of computation in the context of Big Data: skip list manipulation. While we found that most of the middle school students performed poorly in assessments after the skip list activities, several students showed they were capable of completing the activity successfully, implying that a repetition of the revised skip list study and additional pilot studies for other Big Data activities are needed to pave the way for the development of this Big Data curriculum. This activity will be just one part of a broader curriculum designed to showcase the social relevance and power of Big Data.}, location = {Atlanta, Georgia, USA}, series = {SIGCSE '14}, pages = {712}, numpages = {1}, keywords = {big data, middle school, engage, game-based learning}}
@inproceedings{10.5555/2555523.2555560,title = {CIVDDD collaborative research in big data analytics and visualization}, author = {Whitmer Barbara },year = {2013}, publisher = {IBM Corp.}, address = {USA}, abstract = {The Centre for Innovation in Information Visualization and Data-Driven Design (CIVDDD) is a Big Data research project collaboration funded by the Ontario Research Fund -- Research Excellence (ORF-RE). Research collaborators in the project include York University, OCAD University, the University of Toronto, and private sector partners (PSPs) to develop the next generation of data discovery, design, analytics, and visualization techniques for new computational tools, representational strategies, and interfaces. As the preeminent research hub for information analytics and scientific visualization in Ontario, CIVDDD has fifteen research teams in the four theme areas of Bioinformatics and Medical Applications, Interactive Visualization, Textual Visualization, and Scientific Visualization. The Workshop included a brief overview of CIVDDD research by the Principal Investigator Dr. Amir Asif, followed by three CIVDDD team presentations and demonstrations related to CASCON 2013 themes. These included: Graph Analytics and Biological Network Structures (Big Data and Cloud Computing), Social Media Data Visualization (Social Computing), and Dynamic Carbon Mapping in Urban Environments (Mobile Computing). Each Workshop presentation contained academic researchers and their private sector partner research collaborators. Each presentation was followed by a demonstration of the research application or visualization, and Q&A. An open discussion concluded the Workshop.}, location = {Ontario, Canada}, series = {CASCON '13}, pages = {344\u2013346}, numpages = {3}}
@inproceedings{10.1145/3535735.3535737,title = {Research on Improvement Scheme of MOOC Based on Sequential Recommendation Algorithm and Big Data}, author = {Tian Le Zhang , Weixin Ren , Yue Zhang },year = {2022}, isbn = {9781450396196}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3535735.3535737}, doi = {10.1145/3535735.3535737}, abstract = {With the advent of the information age, the rapid development of new-generation information technologies such as cloud computing and cloud storage has also led to changes in the education field, especially the massive open online courses (MOOC). At the same time, the society \u2018s demand for talents with computer technology is increasing. The undergraduate education in the field of computer science has gradually become the focus of undergraduate education in the information age. However, even if MOOC is used as a supplement to offline education, the learning effect varies from person to person due to the differences in students \u2018 learning methods and abilities. An improved MOOC model based on sequential recommendation algorithm and big data proposed in this study can provide an optimization idea for such problems. In the model testing session, this study randomly selected some undergraduates in 2020 grade majoring in computer science at the University of Electronic Science and Technology of China for comparative experiments, proving that the MOOC improvement program based on sequential recommendation algorithms and big data can effectively improve students \u2018 academic performance and contribute to the promotion of educational equity.}, location = {Belgrade, Serbia}, series = {ICIEI '22}, pages = {13\u201317}, numpages = {5}, keywords = {undergraduate education, MOOC, big data, sequential recommendation algorithm}}
@inproceedings{10.1145/2912152.2912159,title = {Towards Convergence of Extreme Computing and Big Data Centers}, author = {Matsuoka Satoshi },year = {2016}, isbn = {9781450343527}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2912152.2912159}, doi = {10.1145/2912152.2912159}, abstract = {Rapid growth in the use cases and demands for extreme computing and huge data processing is leading to convergence of the two infrastructures. Tokyo Tech.'s TSUBAME3.0, a 2017 addition to the highly successful TSUBAME2.5, will aim to deploy a series of innovative technologies, including ultra-efficient liquid cooling and power control, petabytes of non-volatile memory, as well as low cost Petabit-class interconnect. To address the challenges of such technology adoption, proper system architecture, software stack, and algorithm must be desgined and developed; these are being addressed by several of our ongoing research projects as well as prototypes, such as the TSUBAME-KFC/DL prototype which became #1 in the world in power efficiency on the Green500 twice in a row, the Billion-way Resiliency project that is investigating effective methods for future resilient supercomputers, as well as the Extreme Big Data (EBD) project which is looking at co-design development of convergent system stack given future extreme data and computing workloads. We are already successful in developing various algorithms and sottware substrates to manipulate big data elements directly on extreme supercomputers, such as graphs, tables (sort), trees, files, etc. and in fact became #1 in the world on the Graph 500 twice including the latest Nov. 2015 version. Our recent focus is also how to ssupport new workloads in categorizing big data represented by deep learning, and there we are collaborating with several partners such as DENSO to improve the scalability and predictability of such workloads; recent trial allowed scalablity to utilize 1146 GPUs for the entire week for a CNN workload. For TSUBAME3 and 2.5 combined we espect to increase such capabilities to over 80 Petaflops in early 2017, or 7 times faster than the K computer.}, location = {Kyoto, Japan}, series = {DIDC '16}, pages = {1}, numpages = {1}, keywords = {scalability, rapid growth, algorithms}}
@inproceedings{10.1109/WI-IAT.2014.185,title = {Multi-agent Architecture for Real-Time Big Data Processing}, author = {Twardowski Bartlomiej , Ryzko Dominik },year = {2014}, isbn = {9781479941438}, publisher = {IEEE Computer Society}, address = {USA}, url = {https://doi.org/10.1109/WI-IAT.2014.185}, doi = {10.1109/WI-IAT.2014.185}, abstract = {The paper describes the architecture for processing of Big Data in real-time based on multi-agent system paradigms. The overall approach to processing of offline and online data is presented. Possible applications of the architecture in the area of recommendation system is shown, however it is argued the approach is general purpose.}, series = {WI-IAT '14}, pages = {333\u2013337}, numpages = {5}}
@inproceedings{10.1145/3456415.3456416,title = {Big-Data-Based Research on the Architecture Design of University Hydropower Intelligent Decision Service Platform}, author = {Chen Ken , He Jiabei },year = {2021}, isbn = {9781450389174}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456415.3456416}, doi = {10.1145/3456415.3456416}, abstract = {With the continuous development and wide application of big data and artificial intelligence technology, how to efficiently use and mine the whole process data of university hydropower models, perception, business and flows, and realize the transformation of informationization of hydropower management to intelligentialize and wisdom, it has become one of the main tasks in the construction of universitiy informatization under the strategy of advocating energy conservation, lowcarbon sustainable development. Combining with the actual demand of university hydropower management, managing and serving the whole process of hydropower data collection, storage, analysis, monitoring and decision-making assistance, this paper proposes the architecture of an intelligent decision-making service platform for university hydropower on big data, and sorts out the core and key technologies in the platform development process and the current mainstream development frameworks and tools to provide technical references for the realization of intelligent hydropower management and application services in universities, and promote the overall planning and step-by-step implementation of smart campuses.}, location = {Shanghai, China}, series = {ICCBN 2021}, pages = {1\u20135}, numpages = {5}, keywords = {big data, hydropower information, intelligentization}}
@inproceedings{10.1145/2910674.2910685,title = {A Big-Data platform for Medical Knowledge Extraction from Electronic Health Records: Automatic Assignment of ICD-9 Codes}, author = {Sideris Costas , Shaikh Sakib , Kalantarian Haik , Sarrafzadeh Majid },year = {2016}, isbn = {9781450343374}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2910674.2910685}, doi = {10.1145/2910674.2910685}, abstract = {In this paper, we present a big data plarform for knowledge categorization in Electronic Health Records and examine its application to automatic assignment of ICD-9 codes. Our platform relies on reusable, adaptable components that can perform knowledge extraction at a large scale. For the ICD-9 automatic assignment, we build and validate our approach using data from the MIMIC II Clinical Database that contains over 20,000 discharge summaries. We show that our platform can achieve state of the art performance in this dataset and that the classification results improve with more data. Overall, in the first level of the ICD-9 hierarchy our algorithm achieves an average precision of 79.7% for an average recall of 70.2%.}, location = {Corfu, Island, Greece}, series = {PETRA '16}, pages = {1\u20132}, numpages = {2}, keywords = {knowledge extraction, ICD, big data}}
@inproceedings{10.1145/3473141.3473248,title = {Research on Financial Risk Management of E-commerce Enterprises in the Era of Big Data}, author = {Cong Xiaoqi },year = {2021}, isbn = {9781450389723}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3473141.3473248}, doi = {10.1145/3473141.3473248}, abstract = {The wide application of big data and the rapid development of e-commerce are constantly changing the business model and financial management model of enterprises.In the era of big data, the electronic invoices, online orders, purchases, wages, cash flow, electronic taxation and other operating data and financial data of electronic enterprises and business enterprises will be more transparent, and enterprise financial management is facing intelligent opportunities and challenges. The application of artificial intelligence technology in e-commerce companies in the era of big data puts forward higher requirements on the comprehensive capabilities of corporate financial personnel and financial management processes. This article conducts an in-depth analysis of the financial management risks of e-commerce enterprises in the current big data environment, and further studies the application of big data in the financial risk management of e-commerce enterprises in the era of big data, risk factors, the quality of financial personnel, business process reengineering and other key issues, Expounds the relationship between big data and financial risk management, analyzes and summarizes the financial risk management strategies adopted by e-commerce companies, builds the overall structure of the big data analysis platform for e-commerce companies, and effectively improves e-commerce companies\u2019 response to financial risks Ability and enhance the company's market competitiveness.}, location = {Bangkok, Thailand}, series = {ICFET '21}, pages = {195\u2013199}, numpages = {5}, keywords = {E-commerce, Financial risk management, Big Data, Competitive ability}}
@inproceedings{10.1145/3299869.3319898,title = {Speculative Distributed CSV Data Parsing for Big Data Analytics}, author = {Ge Chang , Li Yinan , Eilebrecht Eric , Chandramouli Badrish , Kossmann Donald },year = {2019}, isbn = {9781450356435}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3299869.3319898}, doi = {10.1145/3299869.3319898}, abstract = {There has been a recent flurry of interest in providing query capability on raw data in today's big data systems. These raw data must be parsed before processing or use in analytics. Thus, a fundamental challenge in distributed big data systems is that of efficient parallel parsing of raw data. The difficulties come from the inherent ambiguity while independently parsing chunks of raw data without knowing the context of these chunks. Specifically, it can be difficult to find the beginnings and ends of fields and records in these chunks of raw data. To parallelize parsing, this paper proposes a speculation-based approach for the CSV format, arguably the most commonly used raw data format. Due to the syntactic and statistical properties of the format, speculative parsing rarely fails and therefore parsing is efficiently parallelized in a distributed setting. Our speculative approach is also robust, meaning that it can reliably detect syntax errors in CSV data. We experimentally evaluate the speculative, distributed parsing approach in Apache Spark using more than 11,000 real-world datasets, and show that our parser produces significant performance benefits over existing methods.}, location = {Amsterdam, Netherlands}, series = {SIGMOD '19}, pages = {883\u2013899}, numpages = {17}, keywords = {parallel, parsing, csv, distributed}}
@inproceedings{10.1145/3454127.3456615,title = {State of art of PLS Regression for non quantitative data and in Big Data context}, author = {Al Marouni Yasmina , Bentaleb Youssef },year = {2021}, isbn = {9781450388719}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3454127.3456615}, doi = {10.1145/3454127.3456615}, abstract = {Partial Least Squares Regression (PLSR) is a data analysis method in high-dimensional settings, it is used as a dimension reduction method and also as a tool of linear regression. However, it has some problems in a big data context when the data is too large and has been designed to handle only quantitative variables.In this paper, we will present PLSR, then discuss adaptations and extensions of PLS regression to overcome these problems so that it can be more use-full in a big data context.}, location = {KENITRA, AA, Morocco}, series = {NISS2021}, pages = {1\u20135}, numpages = {5}, keywords = {data types, big data, PLS regression}}
@inproceedings{10.1145/3434778,title = {ExpanDrogram: Dynamic Visualization of Big Data Segmentation over Time}, author = {Khalemsky A. , Gelbard R. },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3434778}, doi = {10.1145/3434778}, abstract = {In dynamic and big data environments the visualization of a segmentation process over time often does not enable the user to simultaneously track entire pieces. The key points are sometimes incomparable, and the user is limited to a static visual presentation of a certain point. The proposed visualization concept, called ExpanDrogram, is designed to support dynamic classifiers that run in a big data environment subject to changes in data characteristics. It offers a wide range of features that seek to maximize the customization of a segmentation problem. The main goal of the ExpanDrogram visualization is to improve comprehensiveness by combining both the individual and segment levels, illustrating the dynamics of the segmentation process over time, providing \u201cversion control\u201d that enables the user to observe the history of changes, and more. The method is illustrated using different datasets, with which we demonstrate multiple segmentation parameters, as well as multiple display layers, to highlight points such as new trend detection, outlier detection, tracking changes in original segments, and zoom in/out for more/less detail. The datasets vary in size from a small one to one of more than 12 million records.}, pages = {1\u201327}, numpages = {27}, keywords = {temporal data visualization, cluster analysis, dendrogram, version control, taxonomy, Dynamic segmentation}}
@inproceedings{10.1145/269012.269023,title = {Data quality and systems theory}, author = {Orr Ken },year = {1998}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/269012.269023}, doi = {10.1145/269012.269023}, pages = {66\u201371}, numpages = {6}}
@inproceedings{10.1145/3442555.3442562,title = {Research on the Application of Big Data Analysis in the Homestay Industry in Kinmen}, author = {Li Zheng , Lin Chen , Wang Chien-Hua },year = {2020}, isbn = {9781450388092}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3442555.3442562}, doi = {10.1145/3442555.3442562}, abstract = {Leisure and tourism activities have become an indispensable way for people to relieve pressure nowadays, and the tourism industry has also seen the rise owing to the stable development of economy. The most obvious is the vigorous development of the homestay market. The tourists as demand for information generated, homestay owners recognize the individual a homestay owner can master the tourists as information is limited. It's easier on the human decision-making confusion. Therefore, through the application of big data analysis, it can assist the owner to make the best analysis and decision-making. This paper takes the homestay industry of Kinmen as an example to discuss how owners can collect and construct build their own big data for business and customer satisfaction, and then regularly statistical analysis, customer group analysis, internal problem improvement, marketing strategies and future development directions, etc. Next, taking association rules as example and the WEKA was used to analyze customer satisfaction. It is expected to be conducive to the improvement of the operation of the homestay industry, and to meet the needs of accommodation tourists and enhance the willingness of tourists to stay again.}, location = {Tokyo, Japan}, series = {ICCIP 2020}, pages = {38\u201342}, numpages = {5}, keywords = {Homestay, WEKA, Kinmen, big data, customer satisfaction}}
@inproceedings{10.1145/3284179.3284206,title = {Big Data in Education: Detection of ICT Factors Associated with School Effectiveness with Data Mining Techniques}, author = {Mart\u00ednez-Abad Fernando , Gamazo Adriana , Rodr\u00edguez-Conde Mar\u00eda Jos\u00e9 },year = {2018}, isbn = {9781450365185}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3284179.3284206}, doi = {10.1145/3284179.3284206}, abstract = {The search for non-contextual process factors associated with school effectiveness has become a line of research with very broad dissemination, primarily from the ubiquity of research that includes analysis of secondary data from large-scale evaluations. With the aim of detecting ICT factors related to school effectiveness from the Spanish sample of the PISA 2015 evaluation, this work applies statistical techniques of data mining, specifically decision trees, for the optimization of the process. The results demonstrate that, while the availability and excessive use of ICTs, both in the educational environment and in the home, are factors associated with low effectiveness, other more personal characteristics of the pupils, such as their perception of self-efficacy in the management of ICT, or their own interest in the use of technologies, can be considered as factors in the protection of school effectiveness. We conclude by discussing the relationship between school effectiveness and academic performance through the analysis of previous studies, focusing on the common elements detected within both perspectives.}, location = {Salamanca, Spain}, series = {TEEM'18}, pages = {145\u2013150}, numpages = {6}, keywords = {Data Mining, Information and Communication Technologies, Educational Evaluation, School Effectiveness, Secondary Education}}
@inproceedings{10.1145/3286606.3286782,title = {ACO-FFDP in incremental clustering for big data analysis}, author = {Bouhafer Fadwa , Heyouni Mohammed , El Haddadi Anass , Boulouard Zakaria },year = {2018}, isbn = {9781450365628}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3286606.3286782}, doi = {10.1145/3286606.3286782}, abstract = {The development of dyamic information analysis, like incremental clustering, is becoming a very important concern in big data. In this paper, we will propose a new incremental clustering algorithm, called \"ACO-FFDP-Incremental-Cluster\". This algorithm is a combination between \"FFDP\" a large graph visualization algorithm developed by our team, and \"ACO Algorithm\". FFDP will set an equilibrium positioning of the large graph; then it will provide the nodes final positions as a vector of coordinates. ACO algorithm will take this vector into consideration and try to find the best clustering configuration possible for new data.}, location = {Tetouan, Morocco}, series = {SCA '18}, pages = {1\u20137}, numpages = {7}, keywords = {Incremental clustering, Ant Colony Optimization, Swarm Intelligence}}
@inproceedings{10.14778/2733004.2733056,title = {Getting your big data priorities straight: a demonstration of priority-based QoS using social-network-driven stock recommendation}, author = {Zhang Rui , Jain Reshu , Sarkar Prasenjit , Rupprecht Lukas },year = {2014}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2733004.2733056}, doi = {10.14778/2733004.2733056}, abstract = {As we come to terms with various big data challenges, one vital issue remains largely untouched. That is the optimal multiplexing and prioritization of different big data applications sharing the same underlying infrastructure, for example, a public cloud platform. Given these demanding applications and the necessary practice to avoid over-provisioning, resource contention between applications is inevitable. Priority must be given to important applications (or sub workloads in an application) in these circumstances.This demo highlights the compelling impact prioritization could make, using an example application that recommends promising combinations of stocks to purchase based on relevant Twitter sentiment. The application consists of a batch job and an interactive query, ran simultaneously. Our underlying solution provides a unique capability to identify and differentiate application workloads throughout a complex big data platform. Its current implementation is based on Apache Hadoop and the IBM GPFS distributed storage system. The demo showcases the superior interactive query performance achievable by prioritizing its workloads and thereby avoiding I/O bandwidth contention. The query time is 3.6 \u00d7 better compared to no prioritization. Such a performance is within 0.3% of that of an idealistic system where the query runs without contention. The demo is conducted on around 3 months of Twitter data, pertinent to the S & P 100 index, with about 4 \u00d7 1012 potential stock combinations considered.}, pages = {1665\u20131668}, numpages = {4}}
@inproceedings{10.1145/3358331.3358341,title = {WeChat Rumor Analysis and Governance Based on Big Data}, author = {Zhang Lihong , Wang Juan , He Wei , Zhang Peng , Zhang Shuangshi },year = {2019}, isbn = {9781450372022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3358331.3358341}, doi = {10.1145/3358331.3358341}, abstract = {Based on the 200 rumor hot lists in recent three years, which were refuted by WeChat platform rumor filter, this paper conducted qualitative and quantitative analysis on the word frequency distribution, theme characteristics and hot categories of rumors, and proposed the rumor governance strategy based on big data thinking. The research results help us to grasp the mechanism and rules of rumor generation and propagation under the background of new media, and the proposed rumor governance strategies can also provide guidance for the online rumor regulatory authorities.}, location = {Dublin, Ireland}, series = {AIAM 2019}, pages = {1\u20134}, numpages = {4}, keywords = {governance, WeChat, word frequency, TF-IDF, rumor}}
@inproceedings{10.1145/2428616.2431055,title = {Hazy: Making it Easier to Build and Maintain Big-data Analytics: Racing to unleash the full potential of big data with the latest statistical and machine-learning techniques.}, author = {Kumar Arun , Niu Feng , R\u00e9 Christopher },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2428616.2431055}, doi = {10.1145/2428616.2431055}, abstract = {The rise of big data presents both big opportunities and big challenges in domains ranging from enterprises to sciences. The opportunities include better-informed business decisions, more efficient supply-chain management and resource allocation, more effective targeting of products and advertisements, better ways to \"organize the world\u2019s information,\" faster turnaround of scientific discoveries, etc.}, pages = {30\u201346}, numpages = {17}}
@inproceedings{10.14778/2824032.2824141,title = {Big plateaus of big data on the big island}, author = {Walter Todd },year = {2015}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2824032.2824141}, doi = {10.14778/2824032.2824141}, abstract = {In ancient texts, 40 was a magic number. It meant a \"lot\" or \"a long time.\" 40 years represented the time it took for a new generation to arise. A look back at 40 years of VLDB suggests that this applies to database researchers as well -- the young researchers of the early VLDBs are now the old folks of the database world, and a new generation is creating afresh. Over this period many plateaus of \"Big Data\" have challenged the database community and been conquered. But there is still no free lunch -- database research is really the science of trade-offs, many of which are no different today than 40 years ago. And of course the evolution of hardware technology continues to swing the trade-off pendulum while enabling new plateaus to be reached. Todd will take a look back at customer big data plateaus of the past. He will look at where we are today, then use his crystal ball and the lessons of the past to extrapolate the next several plateaus -- how they will be the same and how will they be different. Along the way we will have a little fun with some VLDB and Teradata history.}, pages = {2057}, numpages = {1}}
@inproceedings{10.1145/3254147,title = {Session details: HPC and Big Data Convergence}, author = {Antoniu Gabriel },year = {2016}, isbn = {9781450343534}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3254147}, doi = {10.1145/3254147}, location = {Kyoto, Japan}, series = {ScienceCloud '16}, pages = {}}
@inproceedings{10.1145/3194554.3194633,title = {Performance Impact of Emerging Memory Technologies on Big Data Applications: A Latency-Programmable System Emulation Approach}, author = {Chang Mu-Tien , Choi I. Stephen , Niu Dimin , Zheng Hongzhong },year = {2018}, isbn = {9781450357241}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3194554.3194633}, doi = {10.1145/3194554.3194633}, abstract = {This paper presents a performance analysis framework for studying emerging memories. The key component of the framework is a memory-latency programmable emulator, which is based on a FPGA-attached server system. The emulator allows users extend read and/or write latency. In addition, we use regression models to enable system performance studies for memory latencies beyond hardware limitations. Finally, we demonstrate Spark application case studies, analyzing the impact of two key characteristics of emerging memories: extended memory access times and enlarged memory capacities. Results show that the benefit of high capacity memory could outweigh the performance loss due to longer memory latency.}, location = {Chicago, IL, USA}, series = {GLSVLSI '18}, pages = {439\u2013442}, numpages = {4}, keywords = {big data, performance analysis, emerging memories}}
@inproceedings{10.1145/2554850.2555076,title = {Big data meets process mining: implementing the alpha algorithm with map-reduce}, author = {Evermann Joerg , Assadipour Ghazal },year = {2014}, isbn = {9781450324694}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2554850.2555076}, doi = {10.1145/2554850.2555076}, abstract = {Process mining is an approach to extract process models from event logs. Given the distributed nature of modern information systems, event logs are likely to be distributed across different physical machines. Map-Reduce is a scalable approach for efficient computations on distributed data. In this paper we present the design of a Map-Reduce implementation of the Alpha process mining algorithm, to take advantage of the scalability of the Map-Reduce approach. We provide a experimental results that show the performance and scalability of our implementation.}, location = {Gyeongju, Republic of Korea}, series = {SAC '14}, pages = {1414\u20131416}, numpages = {3}, keywords = {process mining, workflow management, map-reduce, alpha algorithm}}
@inproceedings{10.1145/3055635.3056660,title = {Modeling Intelligent Human Resources Systems (IRHS) using Big Data and Support Vector Machine (SVM)}, author = {Cahyani Anggita Dian , Budiharto Widodo },year = {2017}, isbn = {9781450348171}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3055635.3056660}, doi = {10.1145/3055635.3056660}, abstract = {The information available in the form of data in Human Resources (HR) to be analyzed will vary greatly depending on the type of organization. Fast and effective Human Resources Systems facilitates the success of an organization. Data science in HR can help determine whether there are patterns of churn in our data that could help predict future churn and find the right candidate of employee. In this paper, we describe the novel model of Intelligent Human Resources Systems (IHRS) using Big Data to analyze and predict about future status of the employees based on the Support Vector Machine (SVM).}, location = {Singapore, Singapore}, series = {ICMLC 2017}, pages = {137\u2013140}, numpages = {4}, keywords = {data science, Psychology, SVM, Human resources, big data}}
@inproceedings{10.1145/3149572.3149603,title = {The research of Data Integration and Business Intelligent based on drilling big data}, author = {Xiang Gao , Fang Wang },year = {2017}, isbn = {9781450353373}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3149572.3149603}, doi = {10.1145/3149572.3149603}, abstract = {With the development of information technology, ChangQing drilling engineering company has accumulated a large number of data about drilling, and the research foundation of drilling big data technology had been formed. But now, huge volumes of data are distributed in different heterogeneous data sources due to the long-term decentralized construction, which is hard to realize the comprehensive analysis of related data. In this paper, aiming at the practical problems, a data integration and business intelligent- project based on drilling big data has been put forward. Referencing to the knowledge of this field, the system applies kettle which is a data integration tool to realize the integration of ETL heterogeneous data resource, establishes the data warehouse based on theme, and uses fine report which is a business intelligence tools to organize the views of drilling big data according to different user's requires, shows flexibly in multi-perspective view, thus provides powerful data support for user's drilling decision.}, location = {Barcelona, Spain}, series = {ICIME 2017}, pages = {64\u201368}, numpages = {5}, keywords = {Multi-source heterogeneous big data, Data Integration, Data warehouse, Business Intelligence}}
@inproceedings{10.1145/2591062.2591088,title = {Formal verification problems in a big data world: towards a mighty synergy}, author = {Camilli Matteo },year = {2014}, isbn = {9781450327688}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2591062.2591088}, doi = {10.1145/2591062.2591088}, abstract = {Formal verification requires high performance data processing software for extracting knowledge from the unprecedented amount of data coming from analyzed systems. Since cloud based computing resources have became easily accessible, there is an opportunity for verification techniques and tools to undergo a deep technological transition to exploit the new available architectures. This has created an increasing interest in parallelizing and distributing verification techniques. In this paper we introduce a distributed approach which exploits techniques typically used by the bigdata community to enable verification of very complex systems using bigdata approaches and cloud computing facilities.}, location = {Hyderabad, India}, series = {ICSE Companion 2014}, pages = {638\u2013641}, numpages = {4}, keywords = {CTL, MapReduce, Formal Verification, Big Data}}
@inproceedings{10.1145/3144592.3144597,title = {Big data and algorithmic decision-making: can transparency restore accountability?}, author = {de Laat Paul B. },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3144592.3144597}, doi = {10.1145/3144592.3144597}, abstract = {Decision-making assisted by algorithms developed by machine learning is increasingly determining our lives. Unfortunately, full opacity about the process is the norm. Can transparency contribute to restoring accountability for such systems? Several objections are examined: the loss of privacy when data sets become public, the perverse effects of disclosure of the very algorithms themselves ('gaming the system' in particular), the potential loss of competitive edge, and the limited gains in answerability to be expected since sophisticated algorithms are inherently opaque. It is concluded that transparency is certainly useful, but only up to a point: extending it to the public at large is normally not to be advised. Moreover, in order to make algorithmic decisions understandable, models of machine learning to be used should either be interpreted ex post or be interpretable by design ex ante.}, pages = {39\u201353}, numpages = {15}, keywords = {opacity, algorithm, machine learning, accountability, transparency, interpretability}}
@inproceedings{10.5555/2946645.3007028,title = {Distributed coordinate descent method for learning with big data}, author = {Richt\u00e1rik Peter , Tak\u00e1\u010d Martin },year = {2016}, publisher = {JMLR.org}, abstract = {In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method for solving loss minimization problems with big data. We initially partition the coordinates (features) and assign each partition to a different node of a cluster. At every iteration, each node picks a random subset of the coordinates from those it owns, independently from the other computers, and in parallel computes and applies updates to the selected coordinates based on a simple closed-form formula. We give bounds on the number of iterations sufficient to approximately solve the problem with high probability, and show how it depends on the data and on the partitioning. We perform numerical experiments with a LASSO instance described by a 3TB matrix.}, pages = {2657\u20132681}, numpages = {25}, keywords = {stochastic methods, parallel coordinate descent, distributed algorithms, boosting}}
@inproceedings{10.1145/3341069.3342980,title = {A Case Study of HealthCare Platform using Big Data Analytics and Machine Learning}, author = {Islam M. D. Samiul , Liu Daizong , Wang Kewei , Zhou Pan , Yu Li , Wu Dapeng },year = {2019}, isbn = {9781450371858}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341069.3342980}, doi = {10.1145/3341069.3342980}, abstract = {The medical services in Bangladesh are shortage nowadays; people are suffering from getting the correct treatment from the hospital. With the low proportion of the doctors and the low per capita salary in Bangladesh, patients need to spend more money to get the appropriate treatments. Therefore, it is necessary to apply modern information technologies by which the scaffold between the patients and specialists can be reduced, and the patients can take proper treatment at a lower cost. Fortunately, we can solve this critical problem by utilizing interaction among electrical devices. With the big data collected from these devices, machine learning is a powerful tool for the data analytics because of its high accuracy, lower computational costs, and lower power consumption. This research is based on a case of study by the incorporation of the database, mobile application, web application and develops a novel platform through which the patients and the doctors can interact. In addition, the platform helps to store the patients' health data to make the final prediction using machine learning methods to get the proper healthcare treatment with the help of the machines and the doctors. The experiment result shows the high accuracy over 95% of the disease detection using machine learning methods, with the cost 90% lower than the local hospital in Bangladesh, which provides the strong support to implement of our platform in the remote area of the country.}, location = {Guangzhou, China}, series = {HPCCT '19}, pages = {139\u2013146}, numpages = {8}, keywords = {Big Data, Machine Learning, Healthcare, Data Mining, Disease Prediction}}
@inproceedings{10.1145/3255779,title = {Session details: Research session 28: big data}, author = {Candan Selcuk K. },year = {2014}, isbn = {9781450323765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3255779}, doi = {10.1145/3255779}, location = {Snowbird, Utah, USA}, series = {SIGMOD '14}, pages = {}}
@inproceedings{10.1145/3457682.3457775,title = {Visualization Analysis of Library Research in the Context of Big Data Based on Knowledge Map}, author = {Ke Chen },year = {2021}, isbn = {9781450389310}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3457682.3457775}, doi = {10.1145/3457682.3457775}, abstract = {The development of big data technology has brought a series of new content, opportunities and challenges to the library, and scholars have conducted many studies around this. This study obtained 98 related papers from the core collection of Web of Science, using the knowledge map research method, and using the CiteSpace software to analyze the number of annual papers, journals, authors, institutions, keywords and topic changes. The results show that scholars\u2019 attention to this field has gradually increased, and the number of annual papers has increased year by year. China is the country with the highest contribution to the research, and the contribution of Chinese scholars is higher than that of other countries. Big data, university library, data management and information service are the key research contents of this field. In the end, this paper makes a research prospect, and scholars should further strengthen the research on user behavior, user portrait and intellectual property risk.}, location = {Shenzhen, China}, series = {ICMLC 2021}, pages = {271\u2013278}, numpages = {8}, keywords = {Big Data, Knowledge Map, Library}}
@inproceedings{10.1145/3110218,title = {Dependable Deep Computation Model for Feature Learning on Big Data in Cyber-Physical Systems}, author = {Zhang Qingchen , Yang Laurence T. , Chen Zhikui , Li Peng },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3110218}, doi = {10.1145/3110218}, abstract = {With the ongoing development of sensor devices and network techniques, big data are being generated from the cyber-physical systems. Because of sensor equipment occasional failure and network transmission unreliability, a large number of low-quality data, such as noisy data and incomplete data, is collected from the cyber-physical systems. Low-quality data pose a remarkable challenge on deep learning models for big data feature learning. As a novel deep learning model, the deep computation model achieves superior performance for big data feature learning. However, it is difficult for the deep computation model to learn dependable features for low-quality data, since it uses the nonlinear function as the encoder. In this article, a dependable deep computation model is proposed for feature learning on low-quality big data in cyber-physical systems. Specially, a regularity is added into the objective function of the deep computation model to obtain reliable features in the intermediate-level representation space. Furthermore, a learning algorithm based on the back-propagation strategy is devised to train the parameters of the proposed model. Finally, experiments are conducted on three representative datasets and a real dataset to evaluate the effectiveness of the dependable deep computation model for low-quality big data feature learning. Results show that the proposed model achieves a remarkable result for the tasks of classification, restoration, and prediction, proving the potential of this work for practical applications in cyber-physical systems.}, pages = {1\u201317}, numpages = {17}, keywords = {feature learning, back-propagation algorithm, big data, dependable deep computation model, Cyber-physical systems}}
@inproceedings{10.1145/2993318.2993351,title = {Using DevOps Principles to Continuously Monitor RDF Data Quality}, author = {Meissner Roy , Junghanns Kurt },year = {2016}, isbn = {9781450347525}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2993318.2993351}, doi = {10.1145/2993318.2993351}, abstract = {One approach to continuously achieve a certain data quality level is to use an integration pipeline that continuously checks and monitors the quality of a data set according to defined metrics. This approach is inspired by Continuous Integration pipelines, that have been introduced in the area of software development and DevOps to perform continuous source code checks. By investigating in possible tools to use and discussing the specific requirements for RDF data sets, an integration pipeline is derived that joins current approaches of the areas of software-development and semantic-web as well as reuses existing tools. As these tools have not been built explicitly for CI usage, we evaluate their usability and propose possible workarounds and improvements. Furthermore, a real-world usage scenario is discussed, outlining the benefit of the usage of such a pipeline.}, location = {Leipzig, Germany}, series = {SEMANTiCS 2016}, pages = {189\u2013192}, numpages = {4}, keywords = {Continuous Integration, Data Quality, Data Integration, RDF, Instant Feedback, DevOps, Quality Monitoring}}
@inproceedings{10.1145/3473714.3473822,title = {Research on Dynamic Division Method of Distribution Area under Big Data Environment}, author = {Mu Xiangwei , Jiang Jingjing , Zhu Guoqing , Li Kequan },year = {2021}, isbn = {9781450390231}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3473714.3473822}, doi = {10.1145/3473714.3473822}, abstract = {There are path planning issues under restrictions such as time windows when the express delivery point is delivered, and the delivery address of express parcels in actual delivery may be scattered. The volume of express parcels is extremely uneven for the express delivery point. When the amount of express delivery exceeds the load of the express delivery point too much, pure path planning becomes meaningless. Therefore, considering the actual situation and taking the distribution route and the maximum delivery volume of the express point as the main constraints, a possible route planning is conducted in the paper according to the express delivery address before the distribution point enters the express point. Then, based on the big data of the Internet of Things, the distribution area of the express delivery point is dynamically divided, and the express delivery that meets the maximum delivery volume and optimal route planning is allocated to the corresponding neighboring express delivery points. Moreover, the overall equilibrium optimization of each express delivery point is performed to obtain the final dynamically divided distribution area. Experiments have proved that the optimization algorithm in the paper has higher calculation accuracy than other methods, which can achieve more efficient arrangements for the division of logistics distribution areas and improve the distribution rate of load balancing and the quality of delivery services.}, location = {Guangzhou, China}, series = {ICCIR '21}, pages = {619\u2013625}, numpages = {7}, keywords = {Genetic algorithm, Ant colony algorithm, Tabu algorithm, Area division, Internet of Things, Big data, Logistics and distribution}}
@inproceedings{10.1145/1529282.1529334,title = {Towards a maturity model for corporate data quality management}, author = {H\u00fcner Kai M. , Ofner Martin , Otto Boris },year = {2009}, isbn = {9781605581668}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1529282.1529334}, doi = {10.1145/1529282.1529334}, abstract = {High-quality corporate data is a prerequisite for world-wide business process harmonization, global spend analysis, integrated service management, and compliance with regulatory and legal requirements. Corporate Data Quality Management (CDQM) describes the quality oriented organization and control of a company's key data assets such as material, customer, and vendor data. With regard to the aforementioned business drivers, companies demand an instrument to assess the progress and performance of their CDQM initiative. This paper proposes a reference model for CDQM maturity assessment. The model is intended to be used for supporting the build process of CDQM. A case study shows how the model has been successfully implemented in a real-world scenario.}, location = {Honolulu, Hawaii}, series = {SAC '09}, pages = {231\u2013238}, numpages = {8}, keywords = {reference modeling, maturity models, corporate data quality, action research, design research, data quality management}}
@inproceedings{10.1145/3335484.3335494,title = {IPGOD: An Integrated Visualization Platform Based on Big Data Mining and Cloud Computing}, author = {Chen Wei-Yu , Lu Peggy Joy , Shiau Steven },year = {2019}, isbn = {9781450362788}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3335484.3335494}, doi = {10.1145/3335484.3335494}, abstract = {With big data analytics and open data mining becoming increasingly important in this information explosion era, a highly efficient approach to providing an integrated service is by combining these two topics. Therefore, to maximize the convenience of Taiwan's open data utilization and to enrich users' experiences with big data analytics, this paper proposes the Integrated Platform for Government Open Data (IPGOD). The platform consists of a \"Data System\" based on a cloud data warehouse and an \"Analytics System\" based on machine learning utilities; these two systems can work individually or in an integrated manner. Moreover, we leverage the Apache Spark cloud platform to enhance low latency response and high performance. The experimental results demonstrate that the proposed IPGOD realizes the open data warehouse effectively and derives machine learning visualization in a user-friendly and intelligent way.}, location = {Guangzhou, China}, series = {ICBDC '19}, pages = {11\u201316}, numpages = {6}, keywords = {Cloud computing, Open data, Big data mining, Machine learning}}
@inproceedings{10.14778/3303753.3303762,title = {An experimental evaluation of garbage collectors on big data applications}, author = {Xu Lijie , Guo Tian , Dou Wensheng , Wang Wei , Wei Jun },year = {2019}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3303753.3303762}, doi = {10.14778/3303753.3303762}, abstract = {Popular big data frameworks, ranging from Hadoop MapReduce to Spark, rely on garbage-collected languages, such as Java and Scala. Big data applications are especially sensitive to the effectiveness of garbage collection (i.e., GC), because they usually process a large volume of data objects that lead to heavy GC overhead. Lacking in-depth understanding of GC performance has impeded performance improvement in big data applications. In this paper, we conduct the first comprehensive evaluation on three popular garbage collectors, i.e., Parallel, CMS, and G1, using four representative Spark applications. By thoroughly investigating the correlation between these big data applications' memory usage patterns and the collectors' GC patterns, we obtain many findings about GC inefficiencies. We further propose empirical guidelines for application developers, and insightful optimization strategies for designing big-data-friendly garbage collectors.}, pages = {570\u2013583}, numpages = {14}}
@inproceedings{10.1145/3257759,title = {Session details: Big Data Analytics for Health}, author = {April Alain },year = {2016}, isbn = {9781450342247}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3257759}, doi = {10.1145/3257759}, location = {Montr\u00e9al, Qu\u00e9bec, Canada}, series = {DH '16}, pages = {}}
@inproceedings{10.1145/3411564.3411612,title = {Big Data Analytics applied in Supply Chain Management: A Systematic Mapping Study}, author = {Souza Thiago Vieira de , Farias Kleinner , Bischoff Vinicius },year = {2020}, isbn = {9781450388733}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411564.3411612}, doi = {10.1145/3411564.3411612}, abstract = {In recent years, the capacity of big data analytics (BDA) has attracted the significant attention of researchers linked to academia and industry professionals. This capacity is related to the possibility of managing informations advanced to reach its supply chain. In other words, information technology uses integrated systems, which facilitates innovation and the diffusion of knowledge throughout this supply chain. However, researchers and professionals still need to explore the capacity potential of BDA, in order to improve supply chain operational decision-making skills. This work classifies the state-of-the-art literature that applied BDA to the supply chain management (SCM). A Systematic Mapping Study was elaborated based on literature guidelines. A total of 50 primary studies were selected through a filtering process from initially 5,437 studies. These primary studies were used to answer the six research questions. The result of the classification showed that 64% of the studies are related to supply-chain management; most of the studies carried out empirical research; and approximately 50% of the primary studies investigated models for optimization process. This research provides to academics and industry professionals the gaps and future challenges related to BDA for SCM.}, location = {S\u00e3o Bernardo do Campo, Brazil}, series = {SBSI'20}, pages = {1\u20138}, numpages = {8}, keywords = {Data Analysis, Information Systems, Supply-Chain, Big Data Analytics}}
@inproceedings{10.1145/2955129.2955190,title = {Simulation and Analysis of Classification Optimization model of Temperature Sensing Big Data in Intelligent Building}, author = {Zhang Fuquan , Mao Zijing , Ding Gangyi },year = {2016}, isbn = {9781450341295}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2955129.2955190}, doi = {10.1145/2955129.2955190}, abstract = {The temperature sensor network in intelligent building classified collection of big data processing has the problem of big data redundancy interference, which results in unable to determine the fixed filter thresholds. This paper proposed Chaos differential disturbance based fuzzy C-means clustering model for big temperature sensing data classification tasks. It requires to analyze temperature sensor in the intelligent building big distributed structure model of data in a database storage system, the big data information flow feature fusion and time series analysis. Based on traditional fuzzy c-means clustering processing, we introduced chaos disturbance to avoid the classification into local convergence and local optimum, and therefore improve the performance of data clustering. The testing results show that our proposed classification method effectively reduces the error rate for classification tasks of temperature data in intelligent building and have achieved the best performance among the existing algorithms.}, location = {Union, NJ, USA}, series = {MISNC, SI, DS 2016}, pages = {1\u20135}, numpages = {5}, keywords = {Intelligent building, Temperature sensor, Big data, Classification}}
@inproceedings{10.1145/3018896.3018975,title = {Students' performance tracking in distributed open education using big data analytics}, author = {Hussein Ashraf S. , Khan Hamayun A. },year = {2017}, isbn = {9781450347747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3018896.3018975}, doi = {10.1145/3018896.3018975}, abstract = {The field of Big Data Analytics (BDA) is advancing rapidly, and it is finding adoption in diverse areas such as Health, Commerce, Logistics, Retail and Manufacturing to name a few. Adoption of BDA techniques in the field of Higher Education is new, and it is steadily increasing. In this work, BDA techniques have been applied to track the Key Academic Performance Indicators (KAPIs) related to students at the Arab Open University (AOU) and to support the corresponding decisions in this regard. Since the AOU is a Pan Arab multi-campus distributed institution operating in 8 countries and makes extensive use of a wide range of cloud based applications to manage the students' life cycle, hence it is an ideal candidate for adoption of BDA techniques to track students' KAPIs across the AOU multiple country campuses. In order to achieve this objective, we have used IBM Watson Analytics (WA) platform to track the students' KAPIs. As a pilot project, we have focused in this work on the Information Technology and Computing (ITC) academic programme across the AOU. The Exploration and Business Intelligence BDA capabilities of WA have enabled us to analyze and track the academic KAPIs of the ITC students across AOU country campuses while the Predictive Analytics (PA) has led to identifying the dominant factors behind some of our problems such as students drop out rates. One of the most promising outcomes is the decision support dashboards such as the one related to the Student Risk Factor (SRF). By identifying At Risk Students, such dashboard can act as an \"Early Alert System\" to enable the AOU management to take corrective action to provide needed support to such students.}, location = {Cambridge, United Kingdom}, series = {ICC '17}, pages = {1\u20138}, numpages = {8}, keywords = {academic key performance indicators, big data analytics, watson analytics, educational data analytics, student information systems}}
@inproceedings{10.1145/3467691.3467697,title = {Fast Parallel Constrained Viterbi Algorithm for Big Data with Applications to Financial Time Series}, author = {Sassi Imad , Anter Samir , Bekkhoucha Abdelkrim },year = {2021}, isbn = {9781450384940}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3467691.3467697}, doi = {10.1145/3467691.3467697}, abstract = {A new fast parallel constrained Viterbi algorithm for big data is proposed in this paper. We provide a detailed analysis of its performance on big data frameworks. This performance analysis includes the evaluation of execution time, speedup, and prediction accuracy. Additionally, we compare the impact of the proposed approach on the performance of our parallel constrained algorithm with other benchmark versions. We use synthetic data and real-world data in our experiments to describe the behavior of our algorithm for different data sizes and different numbers of nodes. We demonstrate that this algorithm is fast, highly efficient, and scalable when it runs on spark framework and its prediction quality is acceptable since there is no deterioration or reduction observed.}, location = {Chengdu, China}, series = {ICRSA 2021}, pages = {50\u201355}, numpages = {6}, keywords = {fast parallel constrained, scalability, big data, spark, optimization, Hmm, Viterbi}}
@inproceedings{10.1145/3429351.3431745,title = {Selecting a JVM Garbage Collector for Big Data and Cloud Services}, author = {Tavakolisomeh Sanaz },year = {2020}, isbn = {9781450382007}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3429351.3431745}, doi = {10.1145/3429351.3431745}, abstract = {Memory management is responsible for allocating and releasing the memory used by applications (e.g., in the Java Virtual Machine-JVM). There are several garbage collectors (GCs) each designed to target different performance metrics, making it very hard for developers to decide which GC to use for a particular application. We start with a review of existing GC algorithms. Then, we intend to evaluate throughput, pause time, and memory usage in existing JVM GCs using benchmark suites like DaCapo and Renaissance. The goal is to find the trade-offs between the above mentioned performance metrics to have a better understanding of which GC helps fulfilling certain application requirements.}, location = {Delft, Netherlands}, series = {Middleware'20 Doctoral Symposium}, pages = {22\u201325}, numpages = {4}, keywords = {JVM, Garbage Collection Algorithm, Cloud, Garbage Collector, Big Data, Memory Management}}
@inproceedings{10.1145/3211890.3211914,title = {Big Data Analytics for Smart Cities: The H2020 CLASS Project}, author = {Qui\u00f1ones Eduardo , Bertogna Marko , Hadad Erez , Ferrer Ana Juan , Chiantore Luca , Reboa Alfredo },year = {2018}, isbn = {9781450358491}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3211890.3211914}, doi = {10.1145/3211890.3211914}, location = {Haifa, Israel}, series = {SYSTOR '18}, pages = {130}, numpages = {1}}
@inproceedings{10.1145/3486611.3491121,title = {Application of occupant behavior prediction model on residential big data analysis}, author = {Mo Yunjeong , Zhao Dong },year = {2021}, isbn = {9781450391146}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3486611.3491121}, doi = {10.1145/3486611.3491121}, abstract = {Occupant behavior is multifaceted, and a systematic approach is required to understand occupant behavior comprehensively. This research aims to define a structure of the relationship between energy consumption, building technology, and occupant behavior, using the Occupant Behavior Prediction Model. The model can predict and explain occupant energy usage-related activities. A machine learning approach is used to develop the model, and datasets from the American Time Use Survey (ATUS) are used to verify the model. The results show that the energy use activities with higher predictive performances are more stable and habitual compared to the ones with lower predictive performances. The prediction accuracy achieved by this model for these habitual activities reached as high as 99%. The findings imply that the building systems and control strategies need to be adjusted to accommodate habitual energy use behaviors, rather than changing the behaviors. In addition, educational interventions seem more effective on the less habitual behaviors, which often change.}, location = {Coimbra, Portugal}, series = {BuildSys '21}, pages = {349\u2013352}, numpages = {4}, keywords = {energy use prediction, big data, urban scale data analysis, residential building, occupant behavior}}
@inproceedings{10.1145/3383972.3384034,title = {Towards Big Data Analytics and Mining for UK Traffic Accident Analysis, Visualization & Prediction}, author = {Feng Mingchen , Zheng Jiangbin , Ren Jinchang , Liu Yanqin },year = {2020}, isbn = {9781450376426}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3383972.3384034}, doi = {10.1145/3383972.3384034}, abstract = {Road traffic accident (RTA) is a big issue to our society due to it is among the main causes of traffic congestion, human death, health problems, environmental pollution, and economic losses. Facing these fatal and unexpected traffic accidents, understanding what happened and discover factors that relate to them and then make alarms in advance play critical roles for possibly effective traffic management and reduction of accidents. This paper presents our work to establish a novel big data analytics platform for UK traffic accident analysis using machine learning and deep learning techniques. Our system consists of three parts in which we first cluster accident incidents in an interactive Google map to highlight some hotspots and then narratively visualize accident attributes to uncover potentially related factors, finally we explored several state-of-the-art machine learning, deep learning and time series forecasting models to predict the number of road accidents in the future. The experimental results show that our big data processing platform can not only effectively handle large amount of data but also give new insights into what happened and reasonably prediction of what will happen in the future to assist decision making, which will undoubtedly show its great value as a generic platform for other big data analytics fields.}, location = {Shenzhen, China}, series = {ICMLC 2020}, pages = {225\u2013229}, numpages = {5}, keywords = {Big Data Analytics, Time series Forecasting, Traffic Accident Analysis, Deep Learning}}
@inproceedings{10.1145/3349341.3349375,title = {Research on the Application of \"MOOC\" in Modern Distance Education under the Background of Big Data}, author = {Liu Yongfu , Zhao Xin },year = {2019}, isbn = {9781450371506}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3349341.3349375}, doi = {10.1145/3349341.3349375}, abstract = {Starting from the characteristics of the big data era, the researcher reveals the trend of information-based teaching in the big data era, and the changes of resource view, teaching view and teacher development view brought about by the information-based teaching reform, which is of great significance for deepening the reform of modern distance education curriculum and building a new teaching team. Combining with the actual situation of information-based teaching in the era of big data change, it is considered that flipping classroom, MOOC and micro-course are the first wave of data change education. The significant feature of flipping classroom and micro-course is the innovation of information-based teaching in the field of education in the era of cloud computing and big data.}, location = {Wuhan, Hubei, China}, series = {AICS 2019}, pages = {93\u201396}, numpages = {4}, keywords = {Big Data, Distance Education, MOOC}}
@inproceedings{10.1145/2740908.2778845,title = {On the Role of Data Quality in Improving Web Information Value}, author = {Cappiello Cinzia },year = {2015}, isbn = {9781450334730}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2740908.2778845}, doi = {10.1145/2740908.2778845}, abstract = {n today's information era, every day more and more information is generated and people, on the one hand, have advantages due the increasing support in decision processes and, on the other hand, are experiencing difficulties in the selection of the right data to use. That is, users may leverage on more data but at the same time they may not be able to fully value such data since they lack the necessary knowledge about their provenance and quality. The data quality research area provides quality assessment and improvement methods that can be a valuable support for users that have to deal with the complexity of Web content. In fact, such methods help users to identify the suitability of information for their purposes. Most of the methods and techniques proposed, however, address issues for structured data and/or for defined contexts. Clearly, they cannot be easily used on the Web, where data come from heterogeneous sources and the context of use is most of the times unknown.In this keynote, the need for new assessment techniques is highlighted together with the importance of tracking data provenance as well as the reputation and trustworthiness of the sources. In fact, it is well known that the increase of data volume often corresponds to an increase of value, but to maximize such value the data sources to be used have to carefully analyzed, selected and integrated depending on the specific context of use. The talk discusses the data quality dimensions necessary to analyze different Web data sources and provides a set of illustrative examples that show how to maximize the quality of gathered information.}, location = {Florence, Italy}, series = {WWW '15 Companion}, pages = {1433}, numpages = {1}, keywords = {data quality, data quality assessment, web quality}}