@inproceedings{10.1145/3291801.3291827,
author = {Hu, Xinwu and Luo, Pengcheng and Zhang, Xiaonan and Wang, Jun and Zhou, Tianren},
title = {Research on the Effectiveness Evaluation of Big Data in Combat Simulation},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291827},
doi = {10.1145/3291801.3291827},
abstract = {With the development of complex system simulation techniques, computational capabilities, and data management capabilities, the simulation results tend to be big data. There are also high-dimension, high-redundancy, and high-correlation issues among indexes. Based on the above background, a two-layer Autoencoder neural network is used for feature extraction and dimensionality reduction. Then, 10 deep neural network models are established for index learning. The experimental results show that the 32-layer Resent network works best for low-dimensional data effectiveness evaluation.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {70–75},
numpages = {6},
keywords = {data dimension reduction, effectiveness evaluation, big data, system simulation, neural network},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/3234698.3234723,
author = {El Bousty, Hicham and krit, Salah-ddine and Elasikri, Mohamed and Dani, Hassan and Karimi, Khaoula and Bendaoud, Kaoutar and Kabrane, Mustapha},
title = {Investigating Business Intelligence in the Era of Big Data: Concepts, Benefits and Challenges},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234723},
doi = {10.1145/3234698.3234723},
abstract = {Business intelligence suppose retrieving value from data floating in the organization environment. It provides methods and tools for collecting, storing, formatting and analyzing data for the purpose of helping managers in decision-making. At the start, only data from enterprise internal activities were examined. Now and in this turbulent business environment, organizations should incorporate analysis of the huge amount of external data gathered from multifarious sources. It is argued that BI systems accuracy depends on quantity of data at their disposal, yet some storage and analysis methods are phased out and should be reviewed by academics and practitioners.This paper presents an overview of BI challenges in the context of Big Data (BD) and some available solutions provided, either by using Cloud Computing (CC) or improving Data Warehouse (DW) efficiency.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {25},
numpages = {9},
keywords = {Cloud Computing, Business Intelligence, Data Warehouse, Big Data},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

@article{10.1145/2500489,
author = {Rakthanmanon, Thanawin and Campana, Bilson and Mueen, Abdullah and Batista, Gustavo and Westover, Brandon and Zhu, Qiang and Zakaria, Jesin and Keogh, Eamonn},
title = {Addressing Big Data Time Series: Mining Trillions of Time Series Subsequences Under Dynamic Time Warping},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2500489},
doi = {10.1145/2500489},
abstract = {Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms, including classification, clustering, motif discovery, anomaly detection, and so on. The difficulty of scaling a search to large datasets explains to a great extent why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine massive time series for the first time. We demonstrate the following unintuitive fact: in large datasets we can exactly search under Dynamic Time Warping (DTW) much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. In particular, the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining papers ever published. We explain how our ideas allow us to solve higher-level time series data mining problems such as motif discovery and clustering at scales that would otherwise be untenable. Moreover, we show how our ideas allow us to efficiently support the uniform scaling distance measure, a measure whose utility seems to be underappreciated, but which we demonstrate here. In addition to mining massive datasets with up to one trillion datapoints, we will show that our ideas also have implications for real-time monitoring of data streams, allowing us to handle much faster arrival rates and/or use cheaper and lower powered devices than are currently possible.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {sep},
articleno = {10},
numpages = {31},
keywords = {similarity search, lower bounds, Time series}
}

@inproceedings{10.1145/304182.304568,
author = {Jarke, Matthias and Quix, Christoph and Blees, Guido and Lehmann, Dirk and Michalk, Gunter and Stierl, Stefan},
title = {Improving OLTP Data Quality Using Data Warehouse Mechanisms},
year = {1999},
isbn = {1581130848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/304182.304568},
doi = {10.1145/304182.304568},
abstract = {Research and products for the integration of heterogeneous legacy source databases in data warehousing have addressed numerous data quality problems in or between the sources. Such a solution is marketed by Team4 for the decision support of mobile sales representatives, using advanced view maintenance and replication management techniques in an environment based on relational data warehouse technology and Lotus Notes-based client systems. However, considering total information supply chain management, the capture of poor operational data, to be cleaned later in the data warehouse, appears sub-optimal. Based on the observation that decision support clients are often closely linked to operational data entry, we have addressed the problem of mapping the data warehouse data quality techniques back to data quality measures for improving OLTP data. The solution requires a warehouse-to-OLTP workflow which employs a combination of view maintenance and view update techniques.},
booktitle = {Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data},
pages = {536–537},
numpages = {2},
location = {Philadelphia, Pennsylvania, USA},
series = {SIGMOD '99}
}

@article{10.1145/304181.304568,
author = {Jarke, Matthias and Quix, Christoph and Blees, Guido and Lehmann, Dirk and Michalk, Gunter and Stierl, Stefan},
title = {Improving OLTP Data Quality Using Data Warehouse Mechanisms},
year = {1999},
issue_date = {June 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/304181.304568},
doi = {10.1145/304181.304568},
abstract = {Research and products for the integration of heterogeneous legacy source databases in data warehousing have addressed numerous data quality problems in or between the sources. Such a solution is marketed by Team4 for the decision support of mobile sales representatives, using advanced view maintenance and replication management techniques in an environment based on relational data warehouse technology and Lotus Notes-based client systems. However, considering total information supply chain management, the capture of poor operational data, to be cleaned later in the data warehouse, appears sub-optimal. Based on the observation that decision support clients are often closely linked to operational data entry, we have addressed the problem of mapping the data warehouse data quality techniques back to data quality measures for improving OLTP data. The solution requires a warehouse-to-OLTP workflow which employs a combination of view maintenance and view update techniques.},
journal = {SIGMOD Rec.},
month = {jun},
pages = {536–537},
numpages = {2}
}

@proceedings{10.1145/3291801,
title = {ICBDR 2018: Proceedings of the 2nd International Conference on Big Data Research},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Over the past few years big data has emerged a new major pluri-disciplinary research area whose objective is to deal with massive datasets and that are too large to be manipulated by conventional modelling and current computational approaches. As the datasets often available in many disciplines and application areas exponentially grow thanks to the increasing availability of novel sensors and devices, there is nowadays an urgent need to develop novel methods and computational approaches to deal with the high volumes and variety of the large data sources generated in many fields. The range of research challenges still opened are extremely large, from the integration, storage, manipulation, data mining and visualization techniques to the development of computing architectures, cloud and distributed computing platforms to scalable storage systems. Clearly, big data challenges should require multidisciplinary approaches, when different points of view, ideas and experiences should be shared, this favoring exchanges and cross-fertilization on recent trends as well as research directions to consider.},
location = {Weihai, China}
}

@article{10.1145/1659225.1659228,
author = {Even, Adir and Shankaranarayanan, G.},
title = {Dual Assessment of Data Quality in Customer Databases},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/1659225.1659228},
doi = {10.1145/1659225.1659228},
abstract = {Quantitative assessment of data quality is critical for identifying the presence of data defects and the extent of the damage due to these defects. Quantitative assessment can help define realistic quality improvement targets, track progress, evaluate the impacts of different solutions, and prioritize improvement efforts accordingly. This study describes a methodology for quantitatively assessing both impartial and contextual data quality in large datasets. Impartial assessment measures the extent to which a dataset is defective, independent of the context in which that dataset is used. Contextual assessment, as defined in this study, measures the extent to which the presence of defects reduces a dataset’s utility, the benefits gained by using that dataset in a specific context. The dual assessment methodology is demonstrated in the context of Customer Relationship Management (CRM), using large data samples from real-world datasets. The results from comparing the two assessments offer important insights for directing quality maintenance efforts and prioritizing quality improvement solutions for this dataset. The study describes the steps and the computation involved in the dual-assessment methodology and discusses the implications for applying the methodology in other business contexts and data environments.},
journal = {J. Data and Information Quality},
month = {dec},
articleno = {15},
numpages = {29},
keywords = {information value, total data quality management, Data quality, databases, CRM, customer relationship management}
}

@inproceedings{10.1145/2640087.2644194,
author = {Ch'ng, Eugene},
title = {The Value of Using Big Data Technologies in Computational Social Science},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644194},
doi = {10.1145/2640087.2644194},
abstract = {The discovery of phenomena in social networks has prompted renewed interests in the field. Data in social networks however can be massive, requiring scalable Big Data architecture. Conversely, research in Big Data needs the volume and velocity of social media data for testing its scalability. Not only so, appropriate data processing and mining of acquired datasets involve complex issues in the variety, veracity, and variability of the data, after which visualisation must occur before we can see fruition in our efforts. This extended abstract presents topical, multimodal, and longitudinal social media datasets from the integration of various scalable open source technologies. The full article details the process that led to the discovery of social information landscapes within the Twitter social network, highlighting the experience of dealing with social media datasets, using a funneling approach so that data becomes manageable.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {33},
numpages = {2},
keywords = {open source, twitter, Social network analysis, computational social science, data mining},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/3053600.3053624,
author = {Zibitsker, Boris and Lupersolsky, Alex},
title = {Modeling Expands Value of Performance Testing for Big Data Applications},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053624},
doi = {10.1145/3053600.3053624},
abstract = {Performance testing of Big Data applications is performed typically on small test environment with limited volume of data. The results of these types of tests do not take into consideration differences between test and production hardware and software environment and contention for resources with many applications in production environments. In this paper we will review application of the modeling for extending the results of performance testing, predicting how new application will perform in production environment. We will review how modeling results can be used to evaluate different options and justify decisions during design, development, implementation and performance management of the production environment.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {119–123},
numpages = {5},
keywords = {performance assurance, big data infrastructure, benchmark, performance testing, performance prediction., performance engineering, performance models, big data applications},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/3220228.3220236,
author = {Saraee, Mo and Silva, Charith},
title = {A New Data Science Framework for Analysing and Mining Geospatial Big Data},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220236},
doi = {10.1145/3220228.3220236},
abstract = {Geospatial Big Data analytics are changing the way that businesses operate in many industries. Although a good number of research works have reported in the literature on geospatial data analytics and real-time data processing of large spatial data streams, only a few have addressed the full geospatial big data analytics project lifecycle and geospatial data science project lifecycle. Big data analysis differs from traditional data analysis primarily due to the volume, velocity and variety characteristics of the data being processed. One of a motivation of introducing new framework is to address these big data analysis challenges. Geospatial data science projects differ from most traditional data analysis projects because they could be complex and in need of advanced technologies in comparison to the traditional data analysis projects. For this reason, it is essential to have a process to govern the project and ensure that the project participants are competent enough to carry on the process. To this end, this paper presents, new geospatial big data mining and machine learning framework for geospatial data acquisition, data fusion, data storing, managing, processing, analysing, visualising and modelling and evaluation. Having a good process for data analysis and clear guidelines for comprehensive analysis is always a plus point for any data science project. It also helps to predict required time and resources early in the process to get a clear idea of the business problem to be solved.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {98–102},
numpages = {5},
keywords = {data science, big data, geospatial big data, data mining, machine learning},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.5555/352925.352970,
author = {Parssian, Amir and Sarkar, Sumit and Jacob, Varghese S.},
title = {Assessing Data Quality for Information Products},
year = {1999},
publisher = {Association for Information Systems},
address = {USA},
booktitle = {Proceedings of the 20th International Conference on Information Systems},
pages = {428–433},
numpages = {6},
location = {Charlotte, North Carolina, USA},
series = {ICIS '99}
}

@inproceedings{10.1145/2839509.2850516,
author = {Nagar, Anurag},
title = {Enhancing Teaching of Big Data by Using Real World Datasets},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2850516},
doi = {10.1145/2839509.2850516},
abstract = {This lightning talk will focus on our experience of teaching a graduate level Big Data course. Traditionally, such courses have relied on "WordCount" style problems, which involve computing the simple count of words in a corpus of documents using the distributed MapReduce framework. While this is certainly a good way of introducing the students to the BigData framework, more real world examples are needed to motivate students. Further, since a majority of courses require students to work on a large project as part of this course, it is essential that they have access to a diverse and interesting set of data. In our course, we experimented with various data sources, such as text from real-time, streaming news articles, twitter feeds, and property price data from various zip codes in a county. The students were involved in gathering the data, designing and implementing MapReduce style algorithms for distributed processing, and presenting their findings. The feedback was extremely positive and we would like to develop this approach further. In this talk, we will present some ideas on how to collect and analyze real world datasets that are suitable for Big Data analysis. We would also encourage further inputs from the audience about this topic.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {496},
numpages = {1},
keywords = {distributed computing teaching, datasets for teaching big data, big data teaching},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/3017680.3022386,
author = {Nagar, Anurag},
title = {Developing Big Data Curriculum with Open Source Infrastructure (Abstract Only)},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3022386},
doi = {10.1145/3017680.3022386},
abstract = {This lightning talk will focus on our experience of developing and managing large undergraduate and graduate Big Data courses. The demand for trained professionals in the field of Big Data technologies is huge, and there is urgent need to develop and update courses in this area. One of the biggest hurdles for many schools is establishment, maintenance, and constant update of high performance computing infrastructure. Further, the technology landscape for Big Data is constantly evolving, and newer technologies, such as Apache Spark, require significant expenditure to set up and upgrade at the cluster level. Traditional infrastructure at most higher educational institutions is insufficient for this, and is also not able to scale up to meet the expectations of large class sizes and multiple simultaneous sessions. In this lightening talk, we will share our experience of running large undergraduate and graduate Big Data courses using open source infrastructure. Some of this infrastructure is cloud based, while others require students to create virtualized environment on their personal computers. Both types of resources are freely available, easy to setup, and provide students with enough computational power to run most academic tasks and projects. We will provide specific examples of using such technologies for common tasks, such as setting up a distributed file system, running MapReduce algorithms on large datasets, performing large scale machine learning and graph mining using Apache Spark, and maintaining a high availability Cassandra instance.},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {700–701},
numpages = {2},
keywords = {big data teaching, distributed computing teaching, infrastructure for teaching big data},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@inproceedings{10.1145/2743065.2743121,
author = {Amudhavel, J. and Sathian, D. and Raghav, R. S. and Rao, Dhanawada Nirmala and Dhavachelvan, P. and Kumar, K. Prem},
title = {Big Data Scalability, Methods and Its Implications: A Survey of Current Practice},
year = {2015},
isbn = {9781450334419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2743065.2743121},
doi = {10.1145/2743065.2743121},
abstract = {In the recent years; with the rise in usage of the devices that could connect itself to the network and could share data, there is a steady increase in the number of applications that are being introduced for providing various services to the users who rely on the devices that are being connected on the network to use the application. The biggest issue that these applications will face is how these applications will have to handle the data that is being generated by its users and also how these applications will provide the security to the data. For any application it is important to provide the security to the data of its users. Some of the major applications will involve high privacy data of the users which providing security will play a vital role and any compromise in the security [7] aspects of the applications will lead to enormous loss. The second issue that the application must focus upon is the scalability. There are two important key points why the scalability [8] is important. One, when the applications is being created it is the services that is being more focused upon rather than the count of the users that could use so providing a scalable system that could incorporate as many users as the users rise [9] is important for the application. Second, the hardware and the software configuration for the system will not be more focused upon during the development of the system, even though the hardware and the software configuration would be focused upon it is to be seen than they are satisfied for the services [10] the application provide. So providing a scalable system that can adapt the change of the hardware and of the software as they are being upgraded is an important element [11] in any part of the applications.},
booktitle = {Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering &amp; Technology (ICARCSET 2015)},
articleno = {56},
numpages = {5},
keywords = {big-data, Scalability, privacy driven data},
location = {Unnao, India},
series = {ICARCSET '15}
}

@inproceedings{10.1145/288195.288292,
author = {Disney, Anne M. and Johnson, Philip M.},
title = {Investigating Data Quality Problems in the PSP},
year = {1998},
isbn = {1581131089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/288195.288292},
doi = {10.1145/288195.288292},
abstract = {The Personal Software Process (PSP) is used by software engineers to gather and analyze data about their work. Published studies typically use data collected using the PSP to draw quantitative conclusions about its impact upon programmer behavior and product quality. However, our experience using PSP in both industrial and academic settings revealed problems both in collection of data and its later analysis. We hypothesized that these two kinds of data quality problems could make a significant impact upon the value of PSP measures. To test this hypothesis, we built a tool to automate the PSP and then examined 89 projects completed by ten subjects using the PSP manually in an educational setting. We discovered 1539 primary errors and categorized them by type, subtype, severity, and age. To examine the collection problem we looked at the 90 errors that represented impossible combinations of data and at other less concrete anomalies in Time Recording Logs and Defect Recording Logs. To examine the analysis problem we developed a rule set, corrected the errors as far as possible, and compared the original and corrected data. This resulted in significant differences for measures such as yield and the cost-performance ratio, confirming our hypothesis. Our results raise questions about the accuracy of manually collected and analyzed PSP data, indicate that integrated tool support may be required for high quality PSP data analysis, and suggest that external measures should be used when attempting to evaluate the impact of the PSP upon programmer behavior and product quality.},
booktitle = {Proceedings of the 6th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {143–152},
numpages = {10},
keywords = {automated process support, defects, empirical software engineering, personal software process, measurement dysfunction},
location = {Lake Buena Vista, Florida, USA},
series = {SIGSOFT '98/FSE-6}
}

@article{10.1145/291252.288292,
author = {Disney, Anne M. and Johnson, Philip M.},
title = {Investigating Data Quality Problems in the PSP},
year = {1998},
issue_date = {Nov. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/291252.288292},
doi = {10.1145/291252.288292},
abstract = {The Personal Software Process (PSP) is used by software engineers to gather and analyze data about their work. Published studies typically use data collected using the PSP to draw quantitative conclusions about its impact upon programmer behavior and product quality. However, our experience using PSP in both industrial and academic settings revealed problems both in collection of data and its later analysis. We hypothesized that these two kinds of data quality problems could make a significant impact upon the value of PSP measures. To test this hypothesis, we built a tool to automate the PSP and then examined 89 projects completed by ten subjects using the PSP manually in an educational setting. We discovered 1539 primary errors and categorized them by type, subtype, severity, and age. To examine the collection problem we looked at the 90 errors that represented impossible combinations of data and at other less concrete anomalies in Time Recording Logs and Defect Recording Logs. To examine the analysis problem we developed a rule set, corrected the errors as far as possible, and compared the original and corrected data. This resulted in significant differences for measures such as yield and the cost-performance ratio, confirming our hypothesis. Our results raise questions about the accuracy of manually collected and analyzed PSP data, indicate that integrated tool support may be required for high quality PSP data analysis, and suggest that external measures should be used when attempting to evaluate the impact of the PSP upon programmer behavior and product quality.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {nov},
pages = {143–152},
numpages = {10},
keywords = {empirical software engineering, measurement dysfunction, personal software process, automated process support, defects}
}

@inproceedings{10.5555/2819289.2819295,
author = {Mirakhorli, Mehdi and Chen, Hong-Mei and Kazman, Rick},
title = {Mining Big Data for Detecting, Extracting and Recommending Architectural Design Concepts},
year = {2015},
publisher = {IEEE Press},
abstract = {An architecture recommender system can help programmers make better design choices to address their architectural quality attribute concerns while doing their daily programming tasks. We mine big data to detect and extract a large set of architectural design concepts, such as design patterns, design tactics, architecture styles, etc., to be used in our architecture recommender system called ARS. However, mining big data poses many practical challenges for system implementation. The volume, velocity and variety of our data set, like all other big data systems, requires careful planning. This first challenge is to select appropriate technologies from the large number of available products for our system implementation. Building on these technologies our greatest challenge is to custom-fit our algorithms to the parallel processing platform we have selected for ARS, to meet our performance goals.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {15–18},
numpages = {4},
keywords = {patterns, design knowledge, open architecture, tactics, mining internet scale software repositories},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@inproceedings{10.1145/3358528.3359551,
author = {Liu, Cong and Chen, Zhenxiang and Cao, Dong and Shang, Mingyue},
title = {Application of Recommender System in Intelligent Community under Big Data Scenario},
year = {2019},
isbn = {9781450371926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358528.3359551},
doi = {10.1145/3358528.3359551},
abstract = {Nowadays, intelligent community is one of indispensable parts in social construction. The construction of intelligent community promotes the construction and development of intelligent city, which can improve residents' living quality. Eating is an important part inhuman lives. In this paper, we develop a food recommendation system. This system is based on big data, Association Rule-based Recommendation and Collaborative Filtering Recommendation. By analyzing a large number of historical user behaviors, this system recommends restaurants, supermarkets, recipes and meal delivery service.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
pages = {92–96},
numpages = {5},
keywords = {Recommender system, Intelligent community, Big data},
location = {Jinan, China},
series = {ICBDT2019}
}

@inproceedings{10.1145/1370788.1370799,
author = {Liebchen, Gernot A. and Shepperd, Martin},
title = {Data Sets and Data Quality in Software Engineering},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370799},
doi = {10.1145/1370788.1370799},
abstract = {OBJECTIVE - to assess the extent and types of techniques used to manage quality within software engineering data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets. METHOD - we perform a systematic review of available empirical software engineering studies. RESULTS - only 23 out of the many hundreds of studies assessed, explicitly considered data quality. CONCLUSIONS - first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need more research into means of identifying, and ideally repairing, noisy cases. Third, it should become routine to use sensitivity analysis to assess conclusion stability with respect to the assumptions that must be made concerning noise levels.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {39–44},
numpages = {6},
keywords = {data quality, data sets, prediction, empirical research},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@article{10.5555/3069658.3069683,
author = {DePratti, Roland and Dancik, Garrett M. and Lucci, Fred and Sampson, Russell D.},
title = {Development of an Introductory Big Data Programming and Concepts Course},
year = {2017},
issue_date = {June 2017},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {32},
number = {6},
issn = {1937-4771},
abstract = {Computer scientists have been developing techniques to glean useful information from datasets for decades. The nascent disciplines of Big Data and Data Science have evolved over the last 10 years due to the rapid explosion in the amount of data collected by scientists, businesses, and other organizations. It is imperative that the next generation of workers is educated with the necessary knowledge to confront Big Data problems. It is the role of higher education institutions to train future data scientists and Big Data practitioners to fill those positions that the marketplace needs. This paper describes the choices and decisions made by one higher education institution to develop a course in Big Data Programming and Concepts that will be part of a future concentration in Data Science.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {175–182},
numpages = {8}
}

@article{10.1145/3148239,
author = {Bertossi, Leopoldo and Milani, Mostafa},
title = {Ontological Multidimensional Data Models and Contextual Data Quality},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3148239},
doi = {10.1145/3148239},
abstract = {Data quality assessment and data cleaning are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD model), which can be used to model and represent contexts as logic-based ontologies. The data under assessment are mapped into the context for additional analysis, processing, and quality data extraction. The resulting contexts allow for the representation of dimensions, and multidimensional data quality assessment becomes possible. At the core of a multidimensional context, we include a generalized multidimensional data model and a Datalog± ontology with provably good properties in terms of query answering. These main components are used to represent dimension hierarchies, dimensional constraints, and dimensional rules and define predicates for quality data specification. Query answering relies on and triggers navigation through dimension hierarchies and becomes the basic tool for the extraction of quality data. The OMD model is interesting per se beyond applications to data quality. It allows for a logic-based and computationally tractable representation of multidimensional data, extending previous multidimensional data models with additional expressive power and functionalities.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {14},
numpages = {36},
keywords = {query answering, Datalog±, weakly-sticky programs, Ontology-based data access}
}

@inproceedings{10.1145/3251403,
author = {Serrano, Manuel},
title = {Session Details: Data Quality and Security},
year = {2009},
isbn = {9781605588162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251403},
doi = {10.1145/3251403},
booktitle = {Proceedings of the First International Workshop on Model Driven Service Engineering and Data Quality and Security},
location = {Hong Kong, China},
series = {MoSE+DQS '09}
}

@inproceedings{10.1145/3007818.3007838,
author = {Eom, Chris Soo-Hyun and Lee, Wookey and Lee, James Jung-Hun},
title = {Spammer Detection for Real-Time Big Data Graphs},
year = {2016},
isbn = {9781450347549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3007818.3007838},
doi = {10.1145/3007818.3007838},
abstract = {In recent years, prodigious explosion of social network services may trigger new business models. However, it has negative aspects such as personal information spill or spamming, as well. Amongst conventional spam detection approaches, the studies which are based on vertex degrees or Local Clustering Coefficient have been caused false positive results so that normal vertices can be specified as spammers. In this paper, we propose a novel approach by employing the circuit structure in the social networks, which demonstrates the advantages of our work through the experiment.},
booktitle = {Proceedings of the Sixth International Conference on Emerging Databases: Technologies, Applications, and Theory},
pages = {51–60},
numpages = {10},
keywords = {circuit, shortest path, local clustering coefficient, graph, spammer},
location = {Jeju, Republic of Korea},
series = {EDB '16}
}

@inproceedings{10.1109/CCGrid.2015.175,
author = {Zeng, Xuezhi and Ranjan, Rajiv and Strazdins, Peter and Garg, Saurabh Kumar and Wang, Lizhe},
title = {Cross-Layer SLA Management for Cloud-Hosted Big Data Analytics Applications},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.175},
doi = {10.1109/CCGrid.2015.175},
abstract = {As we come to terms with various big data challenges, one vital issue remains largely untouched. That is service level agreement (SLA) management to deliver strong Quality of Service (QoS) guarantees for big data analytics applications (BDAA) sharing the same underlying infrastructure, for example, a public cloud platform. Although SLA and QoS are not new concepts as they originated much before the cloud computing and big data era, its importance is amplified and complexity is aggravated by the emergence of time-sensitive BDAAs such as social network-based stock recommendation and environmental monitoring. These applications require strong QoS guarantees and dependability from the underlying cloud computing platform to accommodate real-time responses while handling ever-increasing complexities and uncertainties. Hence, the over-reaching goal of this PhD research is to develop novel simulation, modeling and benchmarking tools and techniques that can aid researchers and practitioners in studying the impact of uncertainties (contention, failures, anomalies, etc.) on the final SLA and QoS of a cloud-hosted BDAA.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {765–768},
numpages = {4},
keywords = {service level agreement, cloud computing, big data},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@article{10.1145/2481528.2481537,
author = {Stonebraker, Michael and Madden, Sam and Dubey, Pradeep},
title = {Intel "Big Data" Science and Technology Center Vision and Execution Plan},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/2481528.2481537},
doi = {10.1145/2481528.2481537},
abstract = {Intel has moved to a collaboration model with universities consisting of "Science and Technology Centers" (ISTCs). These are located at a "hub" university with participation from other universities, contain embedded Intel personnel, and are focused on some research theme. Intel held a national competition for a 5th Science and Technology center in 2012 and selected a proposal from M.I.T. with a theme of "Big Data". This paper presents the big data vision of this technology center and the execution plan for the first few years.},
journal = {SIGMOD Rec.},
month = {may},
pages = {44–49},
numpages = {6}
}

@inproceedings{10.1145/3361242.3361260,
author = {He, Tianxing and Yu, Shengcheng and Wang, Ziyuan and Li, Jieqiong and Chen, Zhenyu},
title = {From Data Quality to Model Quality: An Exploratory Study on Deep Learning},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3361260},
doi = {10.1145/3361242.3361260},
abstract = {In the field of deep learning, people strive to construct high-quality deep neural networks (DNNs) to improve the accuracy of predicting. As well known, the quality of training data have great impacts on the quality of DNN models, since all the DNN models are obtained by training using these training data. However, there is not any reported systematic study on how the quality of training data affects the quality of DNN model. To study the relationships between data quality and model quality, we mainly consider four aspects of data quality including Skewed Classes, Sample Complexity, Label Quality, and Noisy Data in this paper. We design experiments on MNIST and Cifar-10, and attempt to find out the influences of four aspects on the quality of DNN models. Pearson correlation coefficient and Spearman correlation coefficient are utilized to evaluate such influences. Experimental results show that all the four aspects of data quality have significant impacts on the quality of DNN models. It means that the decrease of data quality in these four aspects will reduce the accuracy of the DNN models.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {18},
numpages = {6},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@inproceedings{10.1145/3404687.3404708,
author = {Zhai, Chenggong and Su, XiSheng and Zhang, Hua Ping},
title = {Research on Wartime Oil Consumption Based on Big Data},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404708},
doi = {10.1145/3404687.3404708},
abstract = {Oil consumption prediction is a pre calculation of the amount of oil consumed in a certain period of time or when completing a certain task. It is the basis of wartime oil service organization plan and an important work of wartime oil departments. The purpose of oil consumption is to determine oil reserves scientifically and reasonably, stipulate oil consumption quota, organize oil forwarding and replenishment, deploy oil support force, prepare oil support plan and provide important quantitative basis, so as to improve the accuracy, planning and initiative of oil support.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {51–54},
numpages = {4},
keywords = {Big Data, Forecast, WartimeOil Consumption},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1145/2950290.2983930,
author = {Gulzar, Muhammad Ali and Interlandi, Matteo and Condie, Tyson and Kim, Miryung},
title = {BigDebug: Interactive Debugger for Big Data Analytics in Apache Spark},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983930},
doi = {10.1145/2950290.2983930},
abstract = {To process massive quantities of data, developers leverage data-intensive scalable computing (DISC) systems in the cloud, such as Google's MapReduce, Apache Hadoop, and Apache Spark. In terms of debugging, DISC systems support post-mortem log analysis but do not provide interactive debugging features in realtime. This tool demonstration paper showcases a set of concrete usecases on how BigDebug can help debug Big Data Applications by providing interactive, realtime debug primitives. To emulate interactive step-wise debugging without reducing throughput, BigDebug provides simulated breakpoints to enable a user to inspect a program without actually pausing the entire computation. To minimize unnecessary communication and data transfer, BigDebug provides on-demand watchpoints that enable a user to retrieve intermediate data using a guard and transfer the selected data on demand. To support systematic and efficient trial-and-error debugging, BigDebug also enables users to change program logic in response to an error at runtime and replay the execution from that step. BigDebug is available for download at http://web.cs.ucla.edu/~miryung/software.html},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {1033–1037},
numpages = {5},
keywords = {big data analytics, interactive tools, Debugging, fault localization and recovery, data-intensive scalable computing (DISC)},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@article{10.1145/2668897,
author = {Barocas, Solon and Nissenbaum, Helen},
title = {Big Data's End Run around Procedural Privacy Protections},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/2668897},
doi = {10.1145/2668897},
abstract = {Recognizing the inherent limitations of consent and anonymity.},
journal = {Commun. ACM},
month = {oct},
pages = {31–33},
numpages = {3}
}

@inproceedings{10.1145/3327962.3331455,
author = {Chow, Sherman S. M.},
title = {Can We Securely Outsource Big Data Analytics with Lightweight Cryptography?},
year = {2019},
isbn = {9781450367882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3327962.3331455},
doi = {10.1145/3327962.3331455},
abstract = {Advances in cryptography such as secure multiparty computation (SMC) and fully-/somewhat-homomorphic encryption (FHE/SHE) have already provided a generic solution to the problem of processing encrypted data; however, they are still not that efficient if one directly applies them for big data analytics.Many cryptographers have recently designed specialized privacy-preserving frameworks for neural networks. While promising, they are still not entirely satisfactory. Gazelle (Usenix Security 2018) supports inference but not training. SecureNN (PoPETS 2019), with the help of non-colluding servers, is still orders of magnitudes slower than plaintext training/inferencing.To narrow the gap between theory and practice, we put forward a new paradigm for privacy-preserving big data analytics which leverages both trusted processor such as Intel SGX (Software Guard Extensions) and (untrusted) GPU (Graphics Processing Unit). Note that SGX is not a silver bullet in this scenario. In general, SGX is subject to a memory constraint which can be easily exceeded by a single layer of the (evergrowing) neural networks. Relying on the generic solution such as paging mechanism is, again, inefficient. GPU is an ideal platform for deep learning, yet, we do not want to assume it is trusted. We thus still need cryptographic techniques.In this keynote, we will briefly survey the research landscape of privacy-preserving machine learning, point out the obstacles brought by seemingly slight changes of requirements (e.g., a single query from different data sources, multiple model owners, outsourcing a trained model to an untrusted cloud), and highlight a number of settings which aids in ensuring privacy without heavyweight cryptography. We will also discuss two notable recent works, Graviton (OSDI 2018) and Slalom (ICLR 2019), and our ongoing research.},
booktitle = {Proceedings of the Seventh International Workshop on Security in Cloud Computing},
pages = {1},
numpages = {1},
keywords = {homomorphic encryption, applied cryptography, neural networks},
location = {Auckland, New Zealand},
series = {SCC '19}
}

@inproceedings{10.1145/3305275.3305285,
author = {Yang, Junyan and Schultz, Henrik and Zheng, Yi and Cao, Jun},
title = {Considerations of the Paradigms of Urban Design Teaching Application about Big Data},
year = {2018},
isbn = {9781450365703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3305275.3305285},
doi = {10.1145/3305275.3305285},
abstract = {The refinement and rationalization of urban design has become an important trend of restructuring and development during the crucial stage of China's urban transformation. At the same time, big data technology is developing rapidly, with the availability of various types of high-precision data being gradually increased. The combination of big data and urban design plays an important role in the urban design of transition period. This paper describes the design of urban education system under the big data platform. With the traditional teaching model, which include four stage (research, topic, design and expression), big data technologies are divided into four aspects to consider how to apply the big data to the teaching of urban design. Specifically summarized: big data "accelerate" researching, big data "secondary" analysis, big data "enhanced" design and big data of "optimization" expression. The aim of this research is to let the traditional teaching system and cutting-edge technology of large data collide with each other fusion, which can not only enrich the existing education system, but also establish a complete technical structure. On this basis, it can help students establish and improve the construction, and the ability to learn new technologies, to promote the progress of urban design techniques.},
booktitle = {Proceedings of the International Symposium on Big Data and Artificial Intelligence},
pages = {48–52},
numpages = {5},
keywords = {Big data, urban design, teaching system, application Paradigm},
location = {Hong Kong, Hong Kong},
series = {ISBDAI '18}
}

@inproceedings{10.1145/3167918.3167924,
author = {Al-Mansoori, Ahmed and Yu, Shui and Xiang, Yong and Sood, Keshav},
title = {A Survey on Big Data Stream Processing in SDN Supported Cloud Environment},
year = {2018},
isbn = {9781450354363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167918.3167924},
doi = {10.1145/3167918.3167924},
abstract = {Big data is the term which denotes data with features such as voluminous data, a variety of data and streaming data as well. Processing big data became essential for enterprises to garner general intelligence and avoid biased conclusions. Due to these features, big data processing is considered to be a challenging task. Big data Processing should rely on a robust network. Cloud computing offers a suitable environment for these processes. However, it is more challenging when we move big data to the cloud, as managing the cloud resources is the main issue. Software Defined Network (SDN) has a potential solution to this issue. In this paper, first, we survey the present state of the art of SDN, cloud computing, and Big data Stream processing (BDSP). Then, we discuss SDN in the context of Big Data Stream Processing in Cloud environment. Finally, critical issues and research opportunity are discussed.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {12},
numpages = {11},
keywords = {resource optimization, big data, cost optimization, SDN, big data stream processing, cloud computing},
location = {Brisband, Queensland, Australia},
series = {ACSW '18}
}

@inproceedings{10.1145/3299869.3320240,
author = {Giannakouris, Victor and Fernandez, Alejandro and Simitsis, Alkis and Babu, Shivnath},
title = {Cost-Effective, Workload-Adaptive Migration of Big Data Applications to the Cloud},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3320240},
doi = {10.1145/3299869.3320240},
abstract = {More than 10,000 enterprises worldwide use the big data stack composed of multiple distributed systems. At Unravel, we build the next-generation APM platform for the big data stack, and we have worked with a representative sample of these enterprises that covers most industry verticals. This sample covers the spectrum of choices for deploying the big data stack across on-premises datacenters, private and public cloud deployments, and hybrid combinations of these. In this paper, we present a solution for assisting enterprises planning the migration of their big data stacks from on-premises deployments to the cloud. Our solution is goal driven and adapts to various migration scenarios. We present the system architecture we built and several cloud mapping options. We also describe a demonstration script that involves practical, real-world use-cases of the path to cloud adoption.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1909–1912},
numpages = {4},
keywords = {application performance management, cloud migration, big data stack},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/2640087.2644162,
author = {Ch'ng, Eugene},
title = {The Value of Using Big Data Technologies in Computational Social Science},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644162},
doi = {10.1145/2640087.2644162},
abstract = {The discovery of phenomena in social networks has prompted renewed interests in the field. Data in social networks however can be massive, requiring scalable Big Data architecture. Conversely, research in Big Data needs the volume and velocity of social media data for testing its scalability. Not only so, appropriate data processing and mining of acquired datasets involve complex issues in the variety, veracity, and variability of the data, after which visualisation must occur before we can see fruition in our efforts. This article presents topical, multimodal, and longitudinal social media datasets from the integration of various scalable open source technologies. The article details the process that led to the discovery of social information landscapes within the Twitter social network, highlighting the experience of dealing with social media datasets, using a funneling approach so that data becomes manageable. The article demonstrated the feasibility and value of using scalable open source technologies for acquiring massive, connected datasets for research in the social sciences.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {6},
numpages = {4},
keywords = {computational social science, open source, twitter, social network analysis, data mining},
location = {Beijing, China},
series = {BigDataScience '14}
}

@article{10.5555/2753024.2753046,
author = {DePratti, Roland A.},
title = {Challenges in Designing an Introductory Course in Big Data Programming: Lightning Talk},
year = {2015},
issue_date = {June 2015},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {30},
number = {6},
issn = {1937-4771},
abstract = {We live in a world where massive amounts of data are being generated, leading to advances in disciplines including physics, astronomy, biology, sociology, and business. This so-called Big Data cannot be stored and analyzed using traditional data storage and processing applications, yet its successful storage, mining, and analyses are critical for advances in the fields mentioned above and others. Since April 2014, four Computer Science professors from Eastern Connecticut State University have been participating in Big Data training exercises and developing an Introductory Course in Big Data Programming. However, the broad scope of Big Data and its relative newness pose key challenges to course development.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {104–105},
numpages = {2}
}

@article{10.1145/3158421.3158427,
author = {Jennex, Murray E.},
title = {Big Data, the Internet of Things, and the Revised Knowledge Pyramid},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0095-0033},
url = {https://doi.org/10.1145/3158421.3158427},
doi = {10.1145/3158421.3158427},
abstract = {The knowledge pyramid has been used for several years to illustrate the hierarchical relationships between data, information, knowledge, and wisdom. An earlier version of this paper presented a revised knowledge-KM pyramid that included processes such as filtering and sense making, reversed the pyramid by positing there was more knowledge than data, and showed knowledge management as an extraction of the pyramid. This paper expands the revised knowledge pyramid to include the Internet of Things and Big Data. The result is a revision of the data aspect of the knowledge pyramid. Previous thought was of data as reflections of reality as recorded by sensors. Big Data and the Internet of Things expand sensors and readings to create two layers of data. The top layer of data is the traditional transaction / operational data and the bottom layer of data is an expanded set of data reflecting massive data sets and sensors that are near mirrors of reality. The result is a knowledge pyramid that appears as an hourglass.},
journal = {SIGMIS Database},
month = {nov},
pages = {69–79},
numpages = {11},
keywords = {knowledge pyramid, big data, knowledge management, internet of things, analytics}
}

@article{10.1145/2766196.2766199,
author = {Wang, Fusheng and Aji, Ablimit and Vo, Hoang},
title = {High Performance Spatial Queries for Spatial Big Data: From Medical Imaging to GIS},
year = {2015},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
url = {https://doi.org/10.1145/2766196.2766199},
doi = {10.1145/2766196.2766199},
abstract = {Support of high performance queries on large volumes of spatial data has become increasingly important in many application domains, including geospatial problems in numerous disciplines, location based services, and emerging medical imaging applications. There are two major challenges for managing massive spatial data to support spatial queries: the explosion of spatial data, and the high computational complexity of spatial queries. Our goal is to develop a general framework to support high performance spatial queries and analytics for spatial big data on MapReduce and CPU-GPU hybrid platforms. In this paper, we introduce Hadoop-GIS -- a scalable and high performance spatial data warehousing system for running large scale spatial queries on Hadoop. Hadoop-GIS supports multiple types of spatial queries on MapReduce through skew-aware spatial partitioning, on-demand indexing, customizable spatial query engine RESQUE, implicit parallel spatial query execution on MapReduce, and effective methods for amending query results through handling boundary objects. To accelerate compute-intensive geometric operations, GPU based geometric computation algorithms are integrated into MapReduce pipelines. Our experiments have demonstrated that Hadoop-GIS is highly efficient and scalable, and outperforms parallel spatial DBMS for compute-intensive spatial queries.},
journal = {SIGSPATIAL Special},
month = {apr},
pages = {11–18},
numpages = {8}
}

@inproceedings{10.1145/2485922.2485944,
author = {Wu, Lisa and Barker, Raymond J. and Kim, Martha A. and Ross, Kenneth A.},
title = {Navigating Big Data with High-Throughput, Energy-Efficient Data Partitioning},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485944},
doi = {10.1145/2485922.2485944},
abstract = {The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {249–260},
numpages = {12},
keywords = {specialized functional unit, microarchitecture, accelerator, streaming data, data partitioning},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1145/2508148.2485944,
author = {Wu, Lisa and Barker, Raymond J. and Kim, Martha A. and Ross, Kenneth A.},
title = {Navigating Big Data with High-Throughput, Energy-Efficient Data Partitioning},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485944},
doi = {10.1145/2508148.2485944},
abstract = {The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {249–260},
numpages = {12},
keywords = {data partitioning, specialized functional unit, streaming data, accelerator, microarchitecture}
}

@inproceedings{10.1145/3319619.3322045,
author = {Jurczuk, Krzysztof and Czajkowski, Marcin and Kretowski, Marek},
title = {Multi-GPU Approach for Big Data Mining: Global Induction of Decision Trees},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3322045},
doi = {10.1145/3319619.3322045},
abstract = {This paper identifies scalability bounds of the evolutionary induced decision trees (DT)s. In order to conquer the barriers concerning the large-scale data we propose a novel multi-GPU approach. It incorporates the knowledge of the global DT induction and EA parallelization. The search for a tree structure and tests is performed sequentially by a CPU, while the fitness calculations are delegated to GPUs, thus the core evolution is unchanged. The results show that the evolutionary induction is accelerated several thousand times by using up to 4 GPUs on datasets with up to 1 billion objects.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {175–176},
numpages = {2},
keywords = {big data, scalability bounds, graphics processing unit (GPU), parallel computing, decision trees, CUDA, evolutionary data mining},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/3358528.3358584,
author = {Xu, Jiangying and Du, Lixin and Song, Chenyang and Li, Chao and Ren, Zhi and Zhu, Bo},
title = {Big Data Service Request Prediction Based on Historical Behavior Time Series},
year = {2019},
isbn = {9781450371926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358528.3358584},
doi = {10.1145/3358528.3358584},
abstract = {Big data analysis service has been used widely in our society. For example, in financial field, users often use big data analysis services to analyze stocks, assets, and accounts in real-time investment decision-making. Therefore, real-time service response is very important from the perspective of user experience. Caching data and analysis results have been used widely in industrial practice. But these caches generally are passive, rigid and inefficient. Proactive caching approach for time-consuming data services is a worthwhile research problem. In addition, we have encountered this problem in a practical enterprise application. In this paper, we propose a data service request prediction approach based on historical user behavior time series analysis. Results show that this approach can improve the response speed of backend data services effectively.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
pages = {77–81},
numpages = {5},
keywords = {User Behavior Prediction, Time Window, Time Series, Service Scheduling},
location = {Jinan, China},
series = {ICBDT2019}
}

@inproceedings{10.1109/CCGrid.2016.85,
author = {Nicolae, Bogdan and Costa, Carlos and Misale, Claudia and Katrinis, Kostas and Park, Yoonho},
title = {Towards Memory-Optimized Data Shuffling Patterns for Big Data Analytics},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.85},
doi = {10.1109/CCGrid.2016.85},
abstract = {Big data analytics is an indispensable tool in transforming science, engineering, medicine, healthcare, finance and ultimately business itself. With the explosion of data sizes and need for shorter time-to-solution, in-memory platforms such as Apache Spark gain increasing popularity. However, this introduces important challenges, among which data shuffling is particularly difficult: on one hand it is a key part of the computation that has a major impact on the overall performance and scalability so its efficiency is paramount, while on the other hand it needs to operate with scarce memory in order to leave as much memory available for data caching. In this context, efficient scheduling of data transfers such that it addresses both dimensions of the problem simultaneously is non-trivial. State-of-the-art solutions often rely on simple approaches that yield sub-optimal performance and resource usage. This paper contributes a novel shuffle data transfer strategy that dynamically adapts to the computation with minimal memory utilization, which we briefly underline as a series of design principles.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {409–412},
numpages = {4},
keywords = {big data analytics, data shuffling, memory-efficient I/O, elastic buffering},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/3341620.3341622,
author = {Zhou, Wei and Luo, Danxue and Chen, Jin},
title = {Review and Investigate the Mapping Knowledge Domain of Financial Big Data Research},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341622},
doi = {10.1145/3341620.3341622},
abstract = {In today's society, large data is widely used in various fields owing to its quantitative and objective characteristics. Therefore, there have been an increasing number of investigations researching on financial issues related to big data in recent years. It is believed that analyzing the status quo and emerging trends of financial research and big data research and the beginner who are interested in financial and big data research. To do so, this paper provides the mapping knowledge domain of financial and big data research based on 724 papers on Web of Science (WoS) from 1992 to 2018 by using CiteSpace, which is an effective tool for scientometric studies. The visualization analyses of cited reference cluster, collaborations networks, author co-citation and timeline view are presented in this study to show the research streams and the papers that made significant theoretical contributions. Also, the authors in this research area are analyzed in detail. Besides, the specific hot spots and emerging trends can be identified. There are two contributions in this study. Firstly, we give the comprehensive investigation about the status quo and emerging trends of financial and big data research in the recent 26 years. Secondly, we make the development of financial research and big data easier and direct to learn for beginners.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {1–9},
numpages = {9},
keywords = {Financial research, Knowledge domain visualization, Science mapping, Big data},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3338906.3338939,
author = {Bagherzadeh, Mehdi and Khatchadourian, Raffi},
title = {Going Big: A Large-Scale Study on What Big Data Developers Ask},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338939},
doi = {10.1145/3338906.3338939},
abstract = {Software developers are increasingly required to write big data code. However, they find big data software development challenging. To help these developers it is necessary to understand big data topics that they are interested in and the difficulty of finding answers for questions in these topics. In this work, we conduct a large-scale study on Stackoverflow to understand the interest and difficulties of big data developers. To conduct the study, we develop a set of big data tags to extract big data posts from Stackoverflow; use topic modeling to group these posts into big data topics; group similar topics into categories to construct a topic hierarchy; analyze popularity and difficulty of topics and their correlations; and discuss implications of our findings for practice, research and education of big data software development and investigate their coincidence with the findings of previous work.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {432–442},
numpages = {11},
keywords = {Stackoverflow, Big data topic popularity, Big data topics, Big data topic difficulty, Big data topic hierarchy},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3281375.3281404,
author = {Ishikawa, Hiroshi and Kato, Daiju and Masaki, Endo and Hirota, Masaharu},
title = {Generalized Difference Method for Generating Integrated Hypotheses in Social Big Data},
year = {2018},
isbn = {9781450356220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281375.3281404},
doi = {10.1145/3281375.3281404},
abstract = {Recently there is strong demand for analytic methodology as to generation of integrated hypotheses for applications involving different sources of social big data. In this paper, first, we introduce an abstract data model for integrating data management and data mining by using mathematical concepts of families, collections of sets to facilitate reproducibility and accountability required for social big data applications. Next, we propose generalized difference methods as a methodology for integrated analysis based on different sources of data. Finally, we validate our proposal by applying it to three use cases by using our data model as their description.},
booktitle = {Proceedings of the 10th International Conference on Management of Digital EcoSystems},
pages = {13–22},
numpages = {10},
keywords = {difference method, hypothesis generation, integrated analysis, data model, social big data, data management, data mining},
location = {Tokyo, Japan},
series = {MEDES '18}
}

@inproceedings{10.1145/2694730.2694732,
author = {Hurt, Kathlene and John, Eugene},
title = {Analysis of Memory Sensitive SPEC CPU2006 Integer Benchmarks for Big Data Benchmarking},
year = {2015},
isbn = {9781450333382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694730.2694732},
doi = {10.1145/2694730.2694732},
abstract = {Benchmarking for Big Data is done at the system level, but with processors now being designed specifically for Cloud Computing and Big Data applications, optimization can now be done at the node level. The purpose of this work is to analyze three SPEC CPU2006 Integer benchmarks (libquantum, h264ref and hmmer) that were deemed "highly memory sensitive" in other works to determine their potential as Big Data processor benchmarks. Program characteristics like instruction count, instruction mix, locality, and memory footprint were analyzed. Through this preliminary analysis, these benchmarks were determined to be potential Big Data node-level benchmarks, but more analysis will have to be done in future work.},
booktitle = {Proceedings of the 1st Workshop on Performance Analysis of Big Data Systems},
pages = {11–16},
numpages = {6},
keywords = {memory, spec, cache, big data, benchmarks},
location = {Austin, Texas, USA},
series = {PABS '15}
}

@inproceedings{10.1145/3178212.3178229,
author = {Li, Jiaxue and Song, Wei and Fong, Simon},
title = {Real-Time Analysis and Visualization for Big Data of Energy Consumption},
year = {2017},
isbn = {9781450354882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178212.3178229},
doi = {10.1145/3178212.3178229},
abstract = {This paper proposes a research on real-time analysis and visualization for big data of energy consumption. In this research, we access real-time energy consumption data from cloud storage by a Transmission Control Protocol/Internet Protocol (TCP/IP). In order to optimize K-Means clustering algorithm, we implement CUDA C programming to finish data-intensive calculation in the Graphic Processing Unit (GPU), which enhances the efficiency of analysis for big data of energy consumption. Meanwhile, to realize data visualization, we draw the data mining results in a multidimensional plane utilizing DirectX, which is a standard graphics API. We also render the original energy consumption data directly in the form of four-dimensional geometry with the plane together, so as to obtain more useful information intuitively.},
booktitle = {Proceedings of the 2017 International Conference on Software and E-Business},
pages = {13–16},
numpages = {4},
keywords = {energy consumption, DirectX, big data, CUDA, K-Means},
location = {Hong Kong, Hong Kong},
series = {ICSEB 2017}
}

@inproceedings{10.1145/3344948.3344986,
author = {Castellanos, Camilo and Varela, Carlos A. and Correal, Dario},
title = {Measuring Performance Quality Scenarios in Big Data Analytics Applications: A DevOps and Domain-Specific Model Approach},
year = {2019},
isbn = {9781450371421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344948.3344986},
doi = {10.1145/3344948.3344986},
abstract = {Big data analytics (BDA) applications use advanced analysis algorithms to extract valuable insights from large, fast, and heterogeneous data sources. These complex BDA applications require software design, development, and deployment strategies to deal with volume, velocity, and variety (3vs) while sustaining expected performance levels. BDA software complexity frequently leads to delayed deployments, longer development cycles and challenging performance monitoring. This paper proposes a DevOps and Domain Specific Model (DSM) approach to design, deploy, and monitor performance Quality Scenarios (QS) in BDA applications. This approach uses high-level abstractions to describe deployment strategies and QS enabling performance monitoring. Our experimentation compares the effort of development, deployment and QS monitoring of BDA applications with two use cases of near mid-air collisions (NMAC) detection. The use cases include different performance QS, processing models, and deployment strategies. Our results show shorter (re)deployment cycles and the fulfillment of latency and deadline QS for micro-batch and batch processing.},
booktitle = {Proceedings of the 13th European Conference on Software Architecture - Volume 2},
pages = {165–172},
numpages = {8},
keywords = {domain specific model, DevOps, big data analytics, software architecture, performance quality scenarios},
location = {Paris, France},
series = {ECSA '19}
}

@inproceedings{10.1145/3306500.3306552,
author = {Lee, Kuan-Yin and Hsu, Yin-Chiech},
title = {Big Data for Loyalty Program Management in Hypermarket},
year = {2019},
isbn = {9781450366021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306500.3306552},
doi = {10.1145/3306500.3306552},
abstract = {Loyalty programs lead to a natural split of a firm's customer base into members and nonmembers. To manage both groups effectively, it is essential to know how they concern about, such as services or promotions. This article, set in context of the second hypermarket density in Asia, examines the impact of satisfaction on store patronage and explores moderator roles of employee interaction and price sensitivity between members and nonmembers. Therefore, a survey was performed among 317 hypermarket members and nonmembers from top three settings in Taiwan. The study demonstrates that the satisfaction and store patronage behavior relationship of members stronger than nonmembers. And moderator of employee interaction and price sensitivity of members has stronger effect between satisfaction and store patronage than nonmembers. According to inconsistent relation between satisfaction and store patronage in past studies, the study extend existing theories of retention to incorporate contingency relationships, especially among members and nonmembers to manage retailer-both customer relationship better.},
booktitle = {Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning},
pages = {363–367},
numpages = {5},
keywords = {store patronage, satisfaction, loyalty program, shopping characteristics},
location = {Tokyo, Japan},
series = {IC4E '19}
}

@inproceedings{10.5555/2888619.2888700,
author = {Dong, Wen},
title = {Weaving Multi-Agent Modeling and Big Data for Stochastic Process Inference},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {In this paper, we develop a stochastic process tool to tell the stories behind big data with agent-based models. Specifically, we identify an agent-based model as a stochastic process that generates the big data, and make inferences by solving the agent-based model under the constraint of the data. We hope to use this tool to create a bridge between those who have access to big data and those who use agent-based simulators to convey their insight about these data.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {713–724},
numpages = {12},
location = {Huntington Beach, California},
series = {WSC '15}
}

@inproceedings{10.1109/CCGRID.2018.00097,
author = {Makrani, Hosein Mohammadi and Rafatirad, Setareh and Houmansadr, Amir and Homayoun, Houman},
title = {Main-Memory Requirements of Big Data Applications on Commodity Server Platform},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00097},
doi = {10.1109/CCGRID.2018.00097},
abstract = {The emergence of big data frameworks requires computational and memory resources that can naturally scale to manage massive amounts of diverse data. It is currently unclear whether big data frameworks such as Hadoop, Spark, and MPI will require high bandwidth and large capacity memory to cope with this change. The primary purpose of this study is to answer this question through empirical analysis of different memory configurations available for commodity server and to assess the impact of these configurations on the performance Hadoop and Spark frameworks, and MPI based applications. Our results show that neither DRAM capacity, frequency, nor the number of channels play a critical role on the performance of all studied Hadoop as well as most studied Spark applications. However, our results reveal that iterative tasks (e.g. machine learning) in Spark and MPI are benefiting from a high bandwidth and large capacity memory.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {653–660},
numpages = {8},
keywords = {memory, big data, performance, hadoop, Spark},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/3110025.3120989,
author = {O'Halloran, Sharyn and Nowaczyk, Nikolai and Gallagher, Donal},
title = {Big Data and Graph Theoretic Models: Simulating the Impact of Collateralization on a Financial System},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3120989},
doi = {10.1145/3110025.3120989},
abstract = {In this paper, we simulate and analyze the impact of financial regulations concerning the collateralization of derivative trades on systemic risk. We represent a financial system using a weighted directed graph model. We enhance a novel open source risk engine to automatically classify a financial regulation for its impact on systemic risk. The analysis finds that introducing collateralization does reduce the costs of resolving a financial system in crisis. It does not, however, change the distribution of risk in the system. The analysis also highlights the importance of scenario based testing using hands on metrics to quantify the notion of system risk.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {1056–1064},
numpages = {9},
keywords = {stochastic Linear Gauss-Markov model, systemic risk, variation margin, collateralizations, Monte Carlo simulation, graph theoretic models, financial risk analytics, big data, initial margin, open source risk engine},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3255772,
author = {Finkelstein, Shel},
title = {Session Details: Industry Session 4: Big Data Systems},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255772},
doi = {10.1145/3255772},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

