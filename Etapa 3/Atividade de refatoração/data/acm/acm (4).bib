@inproceedings{10.1145/2939672.2939736,
author = {Wang, Hongjian and Kifer, Daniel and Graif, Corina and Li, Zhenhui},
title = {Crime Rate Inference with Big Data},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939736},
doi = {10.1145/2939672.2939736},
abstract = {Crime is one of the most important social problems in the country, affecting public safety, children development, and adult socioeconomic status. Understanding what factors cause higher crime is critical for policy makers in their efforts to reduce crime and increase citizens' life quality. We tackle a fundamental problem in our paper: crime rate inference at the neighborhood level. Traditional approaches have used demographics and geographical influences to estimate crime rates in a region. With the fast development of positioning technology and prevalence of mobile devices, a large amount of modern urban data have been collected and such big data can provide new perspectives for understanding crime. In this paper, we used large-scale Point-Of-Interest data and taxi flow data in the city of Chicago, IL in the USA. We observed significantly improved performance in crime rate inference compared to using traditional features. Such an improvement is consistent over multiple years. We also show that these new features are significant in the feature importance analysis.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {635–644},
numpages = {10},
keywords = {crime inference, heterogeneous data, spatial-temporal data, big data},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3085228.3085319,
author = {Shi, Jing and Ai, Xiaoyan and Cao, Ziyi},
title = {Can Big Data Improve Public Policy Analysis?},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085319},
doi = {10.1145/3085228.3085319},
abstract = {We conduct a systematic review targeting the idea of whether big data can improve public policy analysis. The ideas about the benefits and risks of applying big data technologies on public policy analysis are summarized. Big data can provide accurate, proactive, and participatory policy analysis but at the same time it raises risks in data privacy, data misuse and bias, and inequality. We then examine the detailed implementation of big data in each public policy stage. The experiments or practical applications from the relevant literature demonstrate how exactly big data is used to improve policy analysis. This study will guide governments on adopting big data for public policy analysis.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {552–561},
numpages = {10},
keywords = {big data, policy analysis, Public policy, data privacy},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@inproceedings{10.1145/3328833.3328841,
author = {Adenuga, Kayode I. and Muniru, Idris O. and Sadiq, Fatai I. and Adenuga, Rahmat O. and Solihudeen, Muhammad J.},
title = {Big Data in Healthcare: Are We Getting Useful Insights from This Avalanche of Data?},
year = {2019},
isbn = {9781450361057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328833.3328841},
doi = {10.1145/3328833.3328841},
abstract = {The benefits of deriving useful insights from avalanche of data available everywhere cannot be overemphasized. Big Data analytics can revolutionize the healthcare industry. It can also ensure functional productivity, help forecast and suggest feedbacks to disease outbreaks, enhance clinical practice, and optimize healthcare expenditure which cuts across all stakeholders in healthcare sectors. Notwithstanding these immense capabilities available in the general application of big data; studies on derivation of useful insights from healthcare data that can enhance medical practice have received little academic attention. Therefore, this study highlighted the possibility of making very insightful healthcare outcomes with big data through a simple classification problem which classifies the tendency of individuals towards specific drugs based on personality measures. Our model though trained with less than 2000 samples and with a simple neural network architecture achieved mean accuracies of 76.87% (sd=0.0097) and 75.86% (sd=0.0123) for the 0.15 and 0.05 validation sets respectively. The relatively acceptable performance recorded by our model despite the small dataset could largely be attributed to number of attributes in our dataset. It is essential to uncover some of the many complexities in our societies in relations to healthcare; and through many machine learning architectures like the neural networks these complex relationships can be discovered},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Information Engineering},
pages = {196–199},
numpages = {4},
keywords = {Analytics, Big Data, Benefits, Challenges},
location = {Cairo, Egypt},
series = {ICSIE '19}
}

@inproceedings{10.1145/2744680.2744687,
author = {Fang, Xiu Susie},
title = {Generating Actionable Knowledge from Big Data},
year = {2015},
isbn = {9781450335294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2744680.2744687},
doi = {10.1145/2744680.2744687},
abstract = {The last few years have seen a rapid increase of sheer amount of data produced and communicated over the Internet and the Web. While it is widely believed that the availability of such ``Big Data'' holds the potential to revolutionize many aspects of our modern society (e.g., intelligent transportation, environmental monitoring, and energy saving), many challenges need to be addressed before this potential can be realized. This PhD project focuses on one critical challenge, namely extracting actionable knowledge from Big Data. Tremendous efforts have been contributed on mining large-scale data on the Web and constructing comprehensive knowledge bases (KBs). However, existing knowledge extraction systems retrieve data from limited types of Web sources. In addition, data fusion approaches consider very little of the noises produced by those knowledge extraction systems. Consequently, the constructed KBs are far from being comprehensive and accurate. In this paper, we present our initial design of a framework for extracting machine-readable data with high precision and recall from four types of data sources, namely Web texts, Document Object Model (DOM) trees, existing KBs, and query stream. Confidence scores are attached to the resulting knowledge, which can be used to further improve the knowledge fusion results.},
booktitle = {Proceedings of the 2015 ACM SIGMOD on PhD Symposium},
pages = {3–8},
numpages = {6},
keywords = {dom tree, knowledge fusion, knowledge base},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15 PhD Symposium}
}

@inproceedings{10.1145/3193077.3193079,
author = {Han, Xiao and Qingdong-Du},
title = {Interaction Between Big Data and Cognitive Science},
year = {2018},
isbn = {9781450363594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193077.3193079},
doi = {10.1145/3193077.3193079},
abstract = {Big data is sweeping in a storm way, the world was rushed into the age of big data.With the advent of the era of big data, people's way of thinking will also have a huge change. Therefore, we must from the previous thinking of small data quickly converted into the thinking of big data,to adapt to this innovative change. This revolution not only brings new challenges but also provides new opportunities for traditional cognitive science. Big data makes science from the pursuit of causality to pay attention to relevance;Through "let the data sound" put forward "the science starts data",added a new logical path for scientific discovery;The law of cause and effect was supplemented by the law of data, and widened the scope of scientific law.Big data brings new development to traditional scientific epistemology,thus formed the understanding science of the large data.},
booktitle = {Proceedings of the 2nd International Conference on Compute and Data Analysis},
pages = {1–5},
numpages = {5},
keywords = {cognitive science, correlation, Big data},
location = {DeKalb, IL, USA},
series = {ICCDA 2018}
}

@inproceedings{10.1145/2791347.2791380,
author = {Zakerzadeh, Hessam and Aggarwal, Charu C. and Barker, Ken},
title = {Privacy-Preserving Big Data Publishing},
year = {2015},
isbn = {9781450337090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791347.2791380},
doi = {10.1145/2791347.2791380},
abstract = {The problem of privacy-preserving data mining has been studied extensively in recent years because of its importance as a key enabler in the sharing of massive data sets. Most of the work in privacy has focussed on issues involving the quality of privacy preservation and utility, though there has been little focus on the issue of scalability in privacy preservation. The reason for this is that anonymization has generally been seen as a batch and one-time process in the context of data sharing. However, in recent years, the sizes of data sets have grown tremendously to a point where the effective application of the current algorithms is becoming increasingly difficult. Furthermore, the transient nature of recent data sets has resulted in an increased need for the repeated application of such methods on the newer data sets which have been collected. Repeated application demands even greater computational efficiency in order to be practical. For example, an algorithm with quadratic complexity is unlikely to be implementable in reasonable time over terabyte scale data sets. A bigger issue is that larger data sets are likely to be addressed by distributed frameworks such as MapReduce. In such frameworks, one has to address the additional issue of minimizing data transfer across different nodes, which is the bottleneck. In this paper, we discuss the first approach towards privacy-preserving data mining of very massive data sets using MapReduce. We study two most widely-used privacy models k-anonymity and l-diversity for anonymization, and present experimental results illustrating the effectiveness of the approach.},
booktitle = {Proceedings of the 27th International Conference on Scientific and Statistical Database Management},
articleno = {26},
numpages = {11},
location = {La Jolla, California},
series = {SSDBM '15}
}

@inproceedings{10.1145/2890602.2890619,
author = {Morabito, Vincenzo and Viscusi, Gianluigi and Themistocleus, Marinos},
title = {Big Data and Analytics Leaders: The Changing Role of CIO},
year = {2016},
isbn = {9781450342032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2890602.2890619},
doi = {10.1145/2890602.2890619},
abstract = {This article investigates the changing role of the Chief Information Officer (CIO) at organizational level with regard to the rise of Big Data and Big Data analytics as a potential source of innovation and competitive advantage. The paper aims to provide a theoretical contribution to the research stream on the topic, by further exploring the emergent properties and understandings related to the role of CIO. As a consequence of the need to adopt advanced technologies, the CIO has been named to master the current unheard information growth for business innovation. To this end we present the results of a qualitative research based on grounded theory carried out on data concerning CIOs of medium and large companies from different industries in the Italian market. Finally, a substantive theory and categories are discussed, showing the role of generation gap and power of new entrants as well as of project and execution excellence on the making of identity and recognition of the CIO as relevant at the time of Big Data analytics.},
booktitle = {Proceedings of the 2016 ACM SIGMIS Conference on Computers and People Research},
pages = {39–46},
numpages = {8},
keywords = {big data, CIO, analytics, grounded theory},
location = {Alexandria, Virginia, USA},
series = {SIGMIS-CPR '16}
}

@inproceedings{10.1145/2640087.2644153,
author = {Hu, Chunming},
title = {Computing Issues for Big Data: Theory, Systems and Applications},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644153},
doi = {10.1145/2640087.2644153},
abstract = {Big data may contain big values, but also brings lots of challenges to the computing theory, architecture, framework, knowledge discovery algorithms, and domain specific tools and applications. Beyond the 4-V or 5-V characters of big datasets, the data processing shows the features like inexact, incremental, and inductive manner. This brings new research opportunities to research community across theory, systems, algorithms, and applications. Is there some new "theory" for the big data? How to handle the data computing algorithms in an operatable manner? This report shares some view on new challenges identified, and covers some of the application scenarios such as micro-blog data analysis and data processing in building next generation search engines.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {2},
numpages = {1},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/3378904.3378907,
author = {Venkatraman, Sitalakshmi and Overmars, Anthony and Fahd, Kiran and Parvin, Sazia and Kaspi, Samuel},
title = {Security Challenges for Big Data and IoT},
year = {2020},
isbn = {9781450376839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378904.3378907},
doi = {10.1145/3378904.3378907},
abstract = {Recently, two terms, namely Big Data and Internet of Things (IoT) have gained popularity individually. However, their interconnections are not fully explored and understood. It is expected that the fusion of Big Data and IoT would create many complex systems for smart cities. While the data from IoT lies in Big Data, the scale of operations are completely different in terms of providing the required real-time analytics for such smart systems. Even though NoSQL databases and other next generation solutions could be deployed to achieve real-time responses, the major security challenges need to be understood as mission critical and sensitive data intertwines Big Data and IoT. In this paper, we identify the security challenges shared by the closely-knit Big Data and IoT in three main risk areas: i) NoSQL security vulnerabilities, ii) mobile IoT (M-IoT) security and privacy constraints and iii) encryption key security threats. We perform a comparative study of security vulnerabilities of NoSQL databases and identify the security and privacy constraints of M-IoT networks. Encryption key attacks for resource constrained IoT devices are also illustrated mathematically. Overall, this paper explores new research directions in these prime areas of security and privacy that would result in solution opportunities for a meaningful fusion of Big Data and IoT for a smart environment.},
booktitle = {Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology},
pages = {1–6},
numpages = {6},
keywords = {factorization, NoSQL, challenges, IoT, Big data, attacks, security},
location = {Singapore, China},
series = {BDET 2020}
}

@inproceedings{10.1145/3230744.3230751,
author = {Kwon, Byungjun and Yu, Moonwon and Jang, Hanyoung and Cho, KyuHyun and Lee, Hyundong and Hahn, Taesung},
title = {Deep Motion Transfer without Big Data},
year = {2018},
isbn = {9781450358170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230744.3230751},
doi = {10.1145/3230744.3230751},
abstract = {This paper presents a novel motion transfer algorithm that copies content motion into a specific style character. The input consists of two motions. One is a content motion such as walking or running, and the other is movement style such as zombie or Krall. The algorithm automatically generates the synthesized motion such as walking zombie, walking Krall, running zombie, or running Krall. In order to obtain natural results, the method adopts the generative power of deep neural networks. Compared to previous neural approaches, the proposed algorithm shows better quality, runs extremely fast, does not require big data, and supports user-controllable style weights.},
booktitle = {ACM SIGGRAPH 2018 Posters},
articleno = {58},
numpages = {2},
keywords = {character animation synthesis, deep neural networks},
location = {Vancouver, British Columbia, Canada},
series = {SIGGRAPH '18}
}

@inproceedings{10.1145/2699026.2699116,
author = {Kuzu, Mehmet and Islam, Mohammad Saiful and Kantarcioglu, Murat},
title = {Distributed Search over Encrypted Big Data},
year = {2015},
isbn = {9781450331913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2699026.2699116},
doi = {10.1145/2699026.2699116},
abstract = {Nowadays, huge amount of documents are increasingly transferred to the remote servers due to the appealing features of cloud computing. On the other hand, privacy and security of the sensitive information in untrusted cloud environment is a big concern. To alleviate such concerns, encryption of sensitive data before its transfer to the cloud has become an important risk mitigation option. Encrypted storage provides protection at the expense of a significant increase in the data management complexity. For effective management, it is critical to provide efficient selective document retrieval capability on the encrypted collection. In fact, considerable amount of searchable symmetric encryption schemes have been designed in the literature to achieve this task. However, with the emergence of big data everywhere, available approaches are insufficient to address some crucial real-world problems such as scalability.In this study, we focus on practical aspects of a secure keyword search mechanism over encrypted data. First, we propose a provably secure distributed index along with a parallelizable retrieval technique that can easily scale to big data. Second, we integrate authorization into the search scheme to limit the information leakage in multi-user setting where users are allowed to access only particular documents. Third, we offer efficient updates on the distributed secure index. In addition, we conduct extensive empirical analysis on a real dataset to illustrate the efficiency of the proposed practical techniques.},
booktitle = {Proceedings of the 5th ACM Conference on Data and Application Security and Privacy},
pages = {271–278},
numpages = {8},
keywords = {searchable encryption, security, privacy},
location = {San Antonio, Texas, USA},
series = {CODASPY '15}
}

@inproceedings{10.1145/2743065.2743099,
author = {Amudhavel, J. and Padmapriya, V. and Gowri, V. and Lakshmipriya, K. and Kumar, K. Prem and Thiyagarajan, B.},
title = {Perspectives, Motivations and Implications Of Big Data Analytics},
year = {2015},
isbn = {9781450334419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2743065.2743099},
doi = {10.1145/2743065.2743099},
abstract = {As today there is an enormous volume of data, examining these large sets contains structure and unstructured data of different types and sizes; big data analytics is used. Data Analytics allows the user to analyze the unusable data to make a faster and better decision. The Latest supply chain professionals are suffused with data, which provokes various new ways of thoughts regarding how the data are produced, ordered, controlled and analyzed. Data Quality in Supply-Chain Management is used to monitor and control the data. This paper presents the knowledge infrastructure about trends in big data analytics. Big Data can also be given as an all-encompassing term for any collection of data sets so large or complex that it becomes difficult to process using traditional data processing applications. The challenges include examination, confine, extent, exploration, sharing, storage, relocate, and visualization and privacy infringement. The Big Data have their application in various fields like in Tourism, Climate Research and many other fields.},
booktitle = {Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering &amp; Technology (ICARCSET 2015)},
articleno = {34},
numpages = {5},
keywords = {unstructured data, application, monitor, Big data, exploration, infringement, Data analytics},
location = {Unnao, India},
series = {ICARCSET '15}
}

@inproceedings{10.1145/3340017.3340032,
author = {Lichy, Jessica and Kachour, Maher},
title = {Big Data Perception &amp; Usage: A Micro-Firm Perspective (The Case of the French Traditional Restaurant Sector)},
year = {2019},
isbn = {9781450362375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340017.3340032},
doi = {10.1145/3340017.3340032},
abstract = {This study contributes to the literature on Big Data and, specifically, the barriers that prevent micro-firms (fewer than 10 employees) from integrating digital solutions, in the context of the French traditional restaurant sector. Using focus group interviews followed by survey methodology, the authors examine the perception and usage of Big Data, from the perspective of micro-firm managers/owners. The results suggest that a combination of factors affect how micro-firms adopt/accept Big Data technologies including: perception of Big Data as a source for developing the business, uncertainty regarding return-on-investment, and awareness of the opportunities that Big Data can deliver. This study extends the literature on Big Data by offering a contemporary perspective of micro-firm managers/owners who face the challenge of assessing how and where they could innovate their business model with regard to Big Data.},
booktitle = {Proceedings of the 2019 3rd International Conference on E-Commerce, E-Business and E-Government},
pages = {89–94},
numpages = {6},
keywords = {restaurants, France, Big Data, perception and usage, micro-firms},
location = {Lyon, France},
series = {ICEEG 2019}
}

@inproceedings{10.1145/2345316.2345320,
author = {Berkovich, Simon and Liao, Duoduo},
title = {On Clusterization of "Big Data" Streams},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345320},
doi = {10.1145/2345316.2345320},
abstract = {Big Data refers to the rising flood of digital data from many different sources, including the sensors, digitizers, scanners, mobile phones, cameras, software-based tools, internet, and so on. "Big" and "diverse" are two important characteristics of Big Data. The diversity of the Big Data, such as text, geometry, image, video, or sound, also increases difficulties of big data processing.Coping with the "Big Data" problems requires a radical change in the philosophy of the organization of information processing. Primarily, the Big Data approach has to modify the underlying computational model in order to manage the uncertainty in the access to information items in a huge nebulous environment. As a result, the produced outcomes are directly influenced only by some active part of all information items, while the rest of the available information items just indirectly affect the choice of the active part. An analogous functionality exhibits the organization of the brain featuring the unconsciousness, and a characteristic similarity shows the retrieval process in Google.In this talk, we introduce a novel method for on-the-fly clusterization of amorphous data from diverse sources. The devised construction is based on the previously developed FuzzyFind Dictionary reversing the error-correction scheme of Golay Code. This clusterization involves processing of intensive continuous data streams that can be effectively implemented using multi-core pipelining with forced interrupts. The suggested clusterization is especially suitable for the Big Data computational model as it materializes the requirement of purposeful selection of information items in unsteady framework of cloud computing and stream processing. Furthermore, the uncertainties in relation to the considered method of clusterization are moderated due to the idea of the bounded rationality, an approach that does not require a complete exact knowledge for sensible decision-making.},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {3},
numpages = {1},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@inproceedings{10.1145/2676723.2677318,
author = {Hazzan, Orit and Shaffer, Clifford A.},
title = {Big Data in Computer Science Education Research},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2677318},
doi = {10.1145/2676723.2677318},
abstract = {Recent years have seen the emergence of applications and concepts that rely on the involvement of the general public (the "crowd") and, consequently, create big data (e.g., MOOCs, search engines, crowdsourcing, crowdfunding, citizen/crowd science, and more). Education in particular is changing dramatically with the use of online resources and courses that generate large streams of data. In this special session, we ask: What research questions in computer science education can be explored using big data? And how can computer science education researchers apply big data analysis to support education in other disciplines? To answer these and related questions, we focus in this special interactive session on how computer science education research can be promoted by integrating big data into the research process.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {591–592},
numpages = {2},
keywords = {big data, citizen science, computer science education, research in computer science education},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1145/3375555.3383121,
author = {Ivanov, Todor and Singhal, Rekha},
title = {Tutorial on Benchmarking Big Data Analytics Systems},
year = {2020},
isbn = {9781450371094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375555.3383121},
doi = {10.1145/3375555.3383121},
abstract = {The proliferation of big data technology and faster computing systems led to pervasions of AI based solutions in our life. There is need to understand how to benchmark systems used to build AI based solutions that have a complex pipeline of pre-processing, statistical analysis, machine learning and deep learning on data to build prediction models. Solution architects, engineers and researchers may use open-source technology or proprietary systems based on desired performance requirements. The performance metrics may be data pre-processing time, model training time and model inference time. We do not see a single benchmark answering all questions of solution architects and researchers. This tutorial covers both practical and research questions on relevant Big Data and Analytics benchmarks.},
booktitle = {Companion of the ACM/SPEC International Conference on Performance Engineering},
pages = {50–53},
numpages = {4},
keywords = {benchmarking, ML, big data, analytics, AI},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@article{10.14778/1453856.1453980,
author = {Chiang, Fei and Miller, Ren\'{e}e J.},
title = {Discovering Data Quality Rules},
year = {2008},
issue_date = {August 2008},
publisher = {VLDB Endowment},
volume = {1},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/1453856.1453980},
doi = {10.14778/1453856.1453980},
abstract = {Dirty data is a serious problem for businesses leading to incorrect decision making, inefficient daily operations, and ultimately wasting both time and money. Dirty data often arises when domain constraints and business rules, meant to preserve data consistency and accuracy, are enforced incompletely or not at all in application code.In this work, we propose a new data-driven tool that can be used within an organization's data quality management process to suggest possible rules, and to identify conformant and non-conformant records. Data quality rules are known to be contextual, so we focus on the discovery of context-dependent rules. Specifically, we search for conditional functional dependencies (CFDs), that is, functional dependencies that hold only over a portion of the data. The output of our tool is a set of functional dependencies together with the context in which they hold (for example, a rule that states for CS graduate courses, the course number and term functionally determines the room and instructor). Since the input to our tool will likely be a dirty database, we also search for CFDs that almost hold. We return these rules together with the non-conformant records (as these are potentially dirty records).We present effective algorithms for discovering CFDs and dirty values in a data instance. Our discovery algorithm searches for minimal CFDs among the data values and prunes redundant candidates. No universal objective measures of data quality or data quality rules are known. Hence, to avoid returning an unnecessarily large number of CFDs and only those that are most interesting, we evaluate a set of interest metrics and present comparative results using real datasets. We also present an experimental study showing the scalability of our techniques.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1166–1177},
numpages = {12}
}

@inproceedings{10.1145/2957276.2997027,
author = {Verma, Nitya},
title = {Towards Re-Orienting the Big Data Rhetoric},
year = {2016},
isbn = {9781450342766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957276.2997027},
doi = {10.1145/2957276.2997027},
abstract = {Data analytics and BI (business intelligence) systems are the most prominent user-facing manifestation of 'big data' and the related computational turn in thinking within organizations. However, the big data mythologies-specifically that data can offer more accurate, objective and truthful forms of intelligence and knowledge-impact, reinforce, and reproduce certain epistemological biases. In my research, I study these big data technologies in human services related contexts to examine knowledge claims and the strengths and limitations of big data.},
booktitle = {Proceedings of the 19th International Conference on Supporting Group Work},
pages = {505–508},
numpages = {4},
keywords = {business intelligence, data analytics, big data},
location = {Sanibel Island, Florida, USA},
series = {GROUP '16}
}

@inproceedings{10.1145/3150919.3150923,
author = {Wang, Shaohua and Zhong, Yang and Lu, Hao and Wang, Erqi and Yun, Weiying and Cai, Wenwen},
title = {Geospatial Big Data Analytics Engine for Spark},
year = {2017},
isbn = {9781450354943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3150919.3150923},
doi = {10.1145/3150919.3150923},
abstract = {With the rapid development of geospatial data acquisition and processing technology, the scale of spatial data is expanding. Mass production applications put forward higher requirements for the performance of geospatial data analysis. In this study, we developed a geospatial big data analytics engine based on SuperMap iObject for Java and Apache Spark. The geospatial big data analytics engine can increase the RDD representation ability of spatial data. The spatial indexing can make the spatial calculation on the nodes of the Spark cluster distributed and efficient. The experimental results show that compared with the traditional algorithm, the geospatial big data analytics engine for Spark has better execution efficiency.},
booktitle = {Proceedings of the 6th ACM SIGSPATIAL Workshop on Analytics for Big Geospatial Data},
pages = {42–45},
numpages = {4},
keywords = {distributed computing, Cross-platform GIS, SuperMap GIS, Spark, Geospatial big data},
location = {Redondo Beach, CA, USA},
series = {BigSpatial'17}
}

@inproceedings{10.1145/2609876.2609884,
author = {Paletz, Susannah B. F.},
title = {Multidisciplinary Teamwork and Big Data},
year = {2014},
isbn = {9781450329385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609876.2609884},
doi = {10.1145/2609876.2609884},
abstract = {In this presentation, I discuss four constructs vital to successful multidisciplinary teamwork: shared mental models, communicating unique information, conflict, and analogy. I highlight the literature and provide lessons learned for each.},
booktitle = {Proceedings of the 2014 Workshop on Human Centered Big Data Research},
pages = {32–35},
numpages = {4},
keywords = {Teams, teamwork, unshared information, conflict, shared mental models, communication, unique information, disagreement, analogy},
location = {Raleigh, NC, USA},
series = {HCBDR '14}
}

@inproceedings{10.1145/3404555.3404630,
author = {Dou, Jinfeng and Chu, Lei and Cao, Jiabao and Qiu, Yang and Zhao, BaoLin},
title = {Efficient Optimized Strategy of Big Data Retrieval},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404630},
doi = {10.1145/3404555.3404630},
abstract = {With the development of new information technologies, the accumulation of data volume has been exploding, and big data retrieval has played an increasingly important role in big data technology. The challenge of data retrieval are the improvement of retrieval accuracy and retrieval speed. Aiming at the demand of big data platform for efficient data retrieval, an efficient optimized strategy is proposed. We found when the primary key query is used, the query response can be quick. However, when using a non-primary key query, the cache table needs to be comprehensively scanned and the longer response delay may be induced. This paper proposes a secondary index based on Solr to increase the accuracy of information retrieval and the quality of user experience. Then a cache-heat evaluation algorithm categorizes data according to access frequency to reduce query latency. Moreover, an index optimization method based on memory cache updates the cache to save space and enhance utilization. The experiments and simulation demonstrate that the proposed strategy can effectively improves the big data retrieval.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {109–116},
numpages = {8},
keywords = {index, Big data, hadoop, data retrieval, solr},
location = {Tianjin, China},
series = {ICCAI '20}
}

@article{10.1145/2627534.2627564,
author = {Al-Jaroodi, Jameela and Mohamed, Nader and Eid, Abdulla},
title = {Dual Direction Big Data Download and Analysis},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627564},
doi = {10.1145/2627534.2627564},
abstract = {The term Big Data was recently coined as the amount of generated and stored digital data has grown so rapidly that it has become very hard to store, manage and analyze without coming up with new techniques that can cope with such challenges. Finding innovative approaches to support big data analysis has become a priority as both the research community and the industry are trying to make use of these huge amounts of available data. In this paper we introduce a new approach to enhance the overall big data analysis performance. The approach calls for utilizing data set replication, parallel download, and parallel processing over multiple compute nodes. The main concept calls for simultaneously parallelizing the download of the data (in partitions) from multiple replicated sites to multiple compute nodes that will also perform the analysis in parallel. Then the results are given to the client that requested the analysis.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {98–101},
numpages = {4},
keywords = {parallel processing, dual direction processing, data replication, big data}
}

@inproceedings{10.1145/3345252.3345282,
author = {Petrova-Antonova, Dessislava and Krasteva, Iva and Ilieva, Sylvia and Pavlova, Irena},
title = {Conceptual Architecture of GATE Big Data Platform},
year = {2019},
isbn = {9781450371490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345252.3345282},
doi = {10.1145/3345252.3345282},
abstract = {Today we experience a data-driven society. All human activities, industrial processes and research lead to data generation of unprecedented scale, spurring new products, services and businesses. Big Data and its application have been a target for European Commission -- with more than 100 FP7 and about 50 H2020 funded projects under Big Data domain. GATE project aims to establish and sustain in the long run a Centre of Excellence as collaborative environment for conducting Big Data research and innovation, facilitated by GATE platform and Innovation Labs. This paper proposes a conceptual architecture of GATE platform, that is holistic, symbiotic, open, evolving and data-integrated. It is also modular and with component-based design that allows to position a mix of products and tools from different providers. GATE platform will enable start-ups, SMEs and large enterprises, as well as other organizations in a wide range of sectors, to build advanced Data driven services and applications. The usability of the proposed architecture is proven through a development of a sample time series data visualization application. Its architecture follows the proposed one through implementation of required components using open technology stack.},
booktitle = {Proceedings of the 20th International Conference on Computer Systems and Technologies},
pages = {261–268},
numpages = {8},
keywords = {Smart City, Big Data, GATE Platform, Emerging Architectures, Big Data Value Chain},
location = {Ruse, Bulgaria},
series = {CompSysTech '19}
}

@inproceedings{10.1145/2640087.2644173,
author = {Yu, Keming and Aristodemou, Katerina and Becker, Frauke and Lord, Joann},
title = {Fast Mode Regression in Big Data Analysis},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644173},
doi = {10.1145/2640087.2644173},
abstract = {Grasping hidden patterns and unknown correlations of big data quickly and precisely is required in our time. Developing methodology for big data is the focus of big data analysis. Methods do not need to be expensive but meaningful and easy to implement. This paper proposes a mode-based fast pattern grasping method and mode-based fully parametric regression for big data analysis. By taking more than a decade of the 'Health Survey for England' data as an example, we apply the method to uncover the dependency of a response variable on other factors. The dependency relation can be used to check the effect of covariances on the pattern and make pattern or trend prediction easily and quickly.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {24},
numpages = {3},
keywords = {mode regression, Gamma distribution, mode, BMI},
location = {Beijing, China},
series = {BigDataScience '14}
}

@article{10.1145/2617660,
author = {Wright, Alex},
title = {Big Data Meets Big Science},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2617660},
doi = {10.1145/2617660},
abstract = {Next-generation scientific instruments are forcing researchers to question the limits of massively parallel computing.},
journal = {Commun. ACM},
month = {jul},
pages = {13–15},
numpages = {3}
}

@inproceedings{10.1145/3404687.3404694,
author = {Raza, Muhammad Umair and XuJian, Zhao},
title = {A Comprehensive Overview of BIG DATA Technologies: A Survey},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404694},
doi = {10.1145/3404687.3404694},
abstract = {In as much as the approaches of the new revolution, machines including transmission media like social media sites, nowadays quantity of data swell hastily. So, size is the core and only facet that leaps the mention of BIG DATA. In this article, an effort to touch a comprehensive view of big data technologies, because of the swift evolution of data by an industry trying the academic press to catch up. This paper also offers a unified explanation of big data as well as the analytics methods. A practical discriminate characteristic of this paper is core analytics associated with unstructured data which is more than 90% of big data. To deal with complicated Big Data problems, great work has been done. This paper analyzes contemporary Big Data technologies. Therein article further strengthens the necessity to formulate new tools for analytics. It bestows not sole an intercontinental overview of big data techniques even though the valuation according to big data Hadoop Ecosystem. It classifies and debates the main technologies feature, challenges, and usage as well.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {23–31},
numpages = {9},
keywords = {HDFS, Apache Hadoop, MapReduce, Big Data Technology},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1145/2839509.2844651,
author = {Eckroth, Joshua},
title = {Teaching Big Data with a Virtual Cluster},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2844651},
doi = {10.1145/2839509.2844651},
abstract = {Both industry and academia are confronting the challenge of big data, i.e., data processing that involves data so voluminous or arriving at such high velocity that no single commodity machine is capable of storing or processing them all. A common approach to handling big data is to divide and distribute the processing job to a cluster of machines. Ideally, a course that teaches students how to work with big data would provide students access to a cluster for hands-on practice. However, a cluster of physical, on-premise machines may be prohibitively expensive, particularly at smaller institutions with smaller budgets.In this report, we summarize our experiences developing and using a virtual cluster in a big data mining and analytics course at a small private liberal arts college. A single moderately-sized server hosts a cluster of virtual machines, which run the popular Apache Hadoop system. The virtual cluster gives students hands-on experience and costs less than an equal number of physical machines. It is also easily constructed and reconfigured. We describe our implementation, analyze its performance characteristics, and compare costs with physical clusters and the Amazon Elastic MapReduce cloud service. We summarize our use of the virtual cluster in the classroom and show student feedback.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {175–180},
numpages = {6},
keywords = {big data, virtual machines, curriculum, cloud computing},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/2896387.2900335,
author = {Cuzzocrea, Alfredo},
title = {Warehousing and Protecting Big Data: State-Of-The-Art-Analysis, Methodologies, Future Challenges},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900335},
doi = {10.1145/2896387.2900335},
abstract = {This paper proposes a comprehensive critical survey on the issues of warehousing and protecting big data, which are recognized as critical challenges of emerging big data research. Indeed, both are critical aspects to be considered in order to build truly, high-performance and highly-flexible big data management systems. We report on state-of-the-art approaches, methodologies and trends, and finally conclude by providing open problems and challenging research directions to be considered by future efforts.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {14},
numpages = {7},
keywords = {Big Data, Big Data Analytics, Protecting Big Data, Warehousing Big Data},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1145/2623330.2630819,
author = {Eagle, Nathan},
title = {Big Data for Social Good},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2630819},
doi = {10.1145/2623330.2630819},
abstract = {Petabytes of data about human movements, transactions and communication patterns are being generated by everyday technologies such as mobile phones &amp; credit cards. This unprecedented volume of information facilitates a novel set of research questions applicable to a wide range of development issues. In collaboration involving 237 mobile phone operators across 102 countries, Jana's mobile technology platform can instantly poll and compensate 3.48 billion active mobile subscriptions. This talk will discuss how insights gained from living in Kenya became the genesis of a technology company currently working with global clients in over 50 countries, including P&amp;G, Google, Unilever, Danone, General Mills, Nestle, Johnson &amp; Johnson, Microsoft, the World Bank, and the United Nations. After providing an overview of the mobile and social media landscapes in emerging markets, we discuss a system that implements polls &amp; mobile subscription compensation. The presentation will conclude by emphasizing the value of consumer data in underserved and understudied regions of the world.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1522},
numpages = {1},
keywords = {social media, emerging markets, mobile technology, polling systems},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{10.1145/2612733.2612779,
author = {Harrison, Teresa M.},
title = {Building Government's Capacity for Big Data Analysis},
year = {2014},
isbn = {9781450329019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2612733.2612779},
doi = {10.1145/2612733.2612779},
abstract = {"Big data" has emerged as a compelling topic for e-government practitioners who have the opportunity to develop strategies for exploiting the data streams they currently possess or can access in an effort to make government organizations smarter and more responsive to their constituencies. This panel brings together researchers who are exploring big data analytic capabilities and opportunities in government organizations as well as the technical, organizational and ethical constraints that government practitioners are likely to face.},
booktitle = {Proceedings of the 15th Annual International Conference on Digital Government Research},
pages = {306–308},
numpages = {3},
keywords = {text analytics, policy informatics, communication, auditors, political science, social networks, natural language processing, Mexico, census data, big data},
location = {Aguascalientes, Mexico},
series = {dg.o '14}
}

@inproceedings{10.1145/3399715.3400860,
author = {Reis, Thoralf and Bornschlegl, Marco X. and Hemmje, Matthias L.},
title = {Big Data Analysis, AI, and Visualization Workshop: Road Mapping Infrastructures for Artificial Intelligence Supporting Advanced Visual Big Data Analysis},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400860},
doi = {10.1145/3399715.3400860},
abstract = {The overall scope and goal of the workshop is to bring together researchers active in the areas of Artificial Intelligence (AI), Big Data Analysis, and Visualization to achieve a road map, which can support the acceleration in research and data science activities by means of transforming, enriching, and deploying AI models and algorithms as well as intelligent advanced visual user interfaces supporting creation, configuration, management, and usage of distributed Big Data Analysis. Big Data Analysis and AI mutually support each other: AI-powered algorithms empower data scientists to analyze Big Data and thereby exploit its full potential whereas Big Data enables AI experts to comfortably design, validate, and deploy AI models. One of the workshop's objectives is the examination of the importance and necessity of a third, a more straightforward relationship of Big Data and AI: AI supporting all user stereotypes and organizations involved in Big Data Analysis on their exploration journey from raw input data to insight and effectuation.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {109},
numpages = {2},
keywords = {AI2VIS4BigData, AI, Visualization, Big Data Analysis},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/2345316.2345322,
author = {Lopez, Xavier},
title = {Big Data and Advanced Spatial Analytics},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345322},
doi = {10.1145/2345316.2345322},
abstract = {Today's business and government organizations are challenged when trying to manage and analyze information from enterprise databases, streaming servers, social media and open source. This is compounded by the complexity of integrating diverse data types (relational, text, spatial, images, spreadsheets) and their representations (customers, products, suppliers, events, and locations) - all of which need to be understood and re-purposed in different contexts. Identifying meaningful patterns across these different information sources is non-trivial. Moreover, conventional IT tools, such as conventional data warehousing and business intelligence alone, are insufficient at handling the volumes, velocity and variety of content at hand. A new framework and associated tools are needed. Dr. Lopez outlines how data scientists and analysts are applying Spatial and Semantic Web concepts to make sense of this Big Data stream. He will describe new approaches oriented toward search, discovery, linking, and analyzing information on the Web, and throughout the enterprise. The role of Map Reduce is described, as is importance of engineered systems to simplify the creation and configuration of Big Data environments. The key take away is use of spatial and linked open data concepts to enhance content alignment, interoperability, discovery and analytics in the Big Data stream.},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {5},
numpages = {1},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@inproceedings{10.1145/2345316.2345329,
author = {Pino, Robinson E.},
title = {Cloud/Big Data Computing for Defense},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345329},
doi = {10.1145/2345316.2345329},
abstract = {The ever growing necessity for Big Data processing within the industry, government, and specially within defense applications causes the need and requirement for the fast development of new technologies. In addition, the protection of Big Data can be a serious problem because security is commonly an afterthought during technology development, and the exponentially increasing rate at which new data is generated presents many challenges. Although conventional Turing computation has been remarkably successful, it does not scale well and is failing to adapt to novel application domains in cyberspace. Fortunately, Turing formalism for computation represents only a subset of all possible computational possibilities. Unconventional computing - the quest for new algorithms and physical implementations of novel computing paradigms based on and inspired by principles of information processing in physical and biological systems - may help to solve some of the information overflow problems facing the Defense community. These and other topics will be covered by our diverse panel of experts.},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {10},
numpages = {1},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@inproceedings{10.1145/2693561.2693563,
author = {Klein, John and Gorton, Ian},
title = {Runtime Performance Challenges in Big Data Systems},
year = {2015},
isbn = {9781450333405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2693561.2693563},
doi = {10.1145/2693561.2693563},
abstract = {Big data systems are becoming pervasive. They are distributed systems that include redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient to assure runtime performance in production. This is due to the scale of the deployed system, the continually evolving workloads, and the unpredictable quality of service of the shared infrastructure. Consequently, a solution for addressing performance requirements needs sophisticated runtime observability and measurement. Observability gives real-time insights into a system's health and status, both at the system and application level, and provides historical data repositories for forensic analysis, capacity planning, and predictive analytics. Due to the scale and heterogeneity of big data systems, significant challenges exist in the design, customization and operations of observability capabilities. These challenges include economical creation and insertion of monitors into hundreds or thousands of computation and data nodes, efficient, low overhead collection and storage of measurements (which is itself a big data problem), and application-aware aggregation and visualization. In this paper we propose a reference architecture to address these challenges, which uses a model-driven engineering toolkit to generate architecture-aware monitors and application-specific visualizations.},
booktitle = {Proceedings of the 2015 Workshop on Challenges in Performance Methods for Software Development},
pages = {17–22},
numpages = {6},
keywords = {big data, observability, model-driven engineering},
location = {Austin, Texas, USA},
series = {WOSP '15}
}

@article{10.1145/2641398,
author = {Lehikoinen, Juha and Koistinen, Ville},
title = {In Big Data We Trust?},
year = {2014},
issue_date = {September + October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {1072-5520},
url = {https://doi.org/10.1145/2641398},
doi = {10.1145/2641398},
journal = {Interactions},
month = {sep},
pages = {38–41},
numpages = {4}
}

@inproceedings{10.1145/3191697.3213801,
author = {Marra, Matteo},
title = {Debugging Support for Big Data Processing Applications},
year = {2018},
isbn = {9781450355131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3191697.3213801},
doi = {10.1145/3191697.3213801},
abstract = {Current trends in Big Data processing indicate that the volume, velocity and variety of data are increasing quickly due to an explosion on diversity and number of sources of information. This poses challenges for Big Data frameworks to be able to meet the new requirements of the emerging real-time streaming data processing applications. This research project focuses on the academic study of integrated development environments and debugging tools to assist the software development of Big Data applications.},
booktitle = {Conference Companion of the 2nd International Conference on Art, Science, and Engineering of Programming},
pages = {241–242},
numpages = {2},
keywords = {Meta-Level Interfaces, Big Data, Tools, Debugging},
location = {Nice, France},
series = {Programming'18 Companion}
}

@inproceedings{10.1145/2378356.2378369,
author = {Brunozzi, Simone},
title = {Big Data and NoSQL with Amazon DynamoDB},
year = {2012},
isbn = {9781450317528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2378356.2378369},
doi = {10.1145/2378356.2378369},
abstract = {This technical talk will briefly introduce the NoSQL approach, describe why it's important for Big Data tasks, and then switch to DynamoDB, a NoSQL managed database that runs on the Amazon Web Services cloud. The talk will then cover DynamoDB's data model, compare it with other NoSQL solutions, and provide some examples and a demo of the service.},
booktitle = {Proceedings of the 2012 Workshop on Management of Big Data Systems},
pages = {41–42},
numpages = {2},
keywords = {big data, nosql, dynamodb},
location = {San Jose, California, USA},
series = {MBDS '12}
}

@inproceedings{10.1145/2487575.2506178,
author = {Sun, Jimeng and Reddy, Chandan K.},
title = {Big Data Analytics for Healthcare},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2506178},
doi = {10.1145/2487575.2506178},
abstract = {Large amounts of heterogeneous medical data have become available in various healthcare organizations (payers, providers, pharmaceuticals). Those data could be an enabling resource for deriving insights for improving care delivery and reducing waste. The enormity and complexity of these datasets present great challenges in analyses and subsequent applications to a practical clinical environment. In this tutorial, we introduce the characteristics and related mining challenges on dealing with big medical data. Many of those insights come from medical informatics community, which is highly related to data mining but focuses on biomedical specifics. We survey various related papers from data mining venues as well as medical informatics venues to share with the audiences key problems and trends in healthcare analytics research, with different applications ranging from clinical text mining, predictive modeling, survival analysis, patient similarity, genetic data analysis, and public health. The tutorial will include several case studies dealing with some of the important healthcare applications.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1525},
numpages = {1},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2538862.2538877,
author = {Silva, Yasin N. and Dietrich, Suzanne W. and Reed, Jason M. and Tsosie, Lisa M.},
title = {Integrating Big Data into the Computing Curricula},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538862.2538877},
doi = {10.1145/2538862.2538877},
abstract = {An important recent technological development in computer science is the availability of highly distributed and scalable systems to process Big Data, i.e., datasets with high volume, velocity and variety. Given the extensive and effective use of systems incorporating Big Data in many application scenarios, these systems have become a key component in the broad landscape of database systems. This fact creates the need to integrate the study of Big Data Management Systems as part of the computing curricula. This paper presents well-structured guidelines to perform this integration by describing the important types of Big Data systems and demonstrating how each type of system can be integrated into the curriculum. A key contribution of this paper is the description of an array of course resources, e.g., virtual machines, sample projects, and in-class exercises, and how these resources support the learning outcomes and enable a hands-on experience with Big Data technologies.},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
pages = {139–144},
numpages = {6},
keywords = {databases curricula, big data management systems},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1145/2479440.2482677,
author = {Alexandrov, Alexander and Br\"{u}cke, Christoph and Markl, Volker},
title = {Issues in Big Data Testing and Benchmarking},
year = {2013},
isbn = {9781450321518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479440.2482677},
doi = {10.1145/2479440.2482677},
abstract = {The academic community and industry are currently researching and building next generation data management systems. These systems are designed to analyze data sets of high volume with high data ingest rates and short response times executing complex data analysis algorithms on data that does not adhere to relational data models. As these big data systems differ from standard relational database systems with respect to data and workloads, the traditional benchmarks used by the database community are insufficient. In this paper, we describe initial solutions and challenges with respect to big data generation, methods for creating realistic, privacy-aware, and arbitrarily scalable data sets, workloads, and benchmarks from real world data. We will in particular discuss why we feel that workloads currently discussed in the testing and benchmarking community do not capture the real complexity of big data and highlight several research challenges with respect to massively-parallel data generation and data characterization.},
booktitle = {Proceedings of the Sixth International Workshop on Testing Database Systems},
articleno = {1},
numpages = {5},
keywords = {data generation, big data, benchmarking, workloads, data profiling},
location = {New York, New York},
series = {DBTest '13}
}

@article{10.14778/2735461.2735465,
author = {Graefe, Goetz and Volos, Haris and Kimura, Hideaki and Kuno, Harumi and Tucek, Joseph and Lillibridge, Mark and Veitch, Alistair},
title = {In-Memory Performance for Big Data},
year = {2014},
issue_date = {September 2014},
publisher = {VLDB Endowment},
volume = {8},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/2735461.2735465},
doi = {10.14778/2735461.2735465},
abstract = {When a working set fits into memory, the overhead imposed by the buffer pool renders traditional databases non-competitive with in-memory designs that sacrifice the benefits of a buffer pool. However, despite the large memory available with modern hardware, data skew, shifting workloads, and complex mixed workloads make it difficult to guarantee that a working set will fit in memory. Hence, some recent work has focused on enabling in-memory databases to protect performance when the working data set almost fits in memory. Contrary to those prior efforts, we enable buffer pool designs to match in-memory performance while supporting the "big data" workloads that continue to require secondary storage, thus providing the best of both worlds. We introduce here a novel buffer pool design that adapts pointer swizzling for references between system objects (as opposed to application objects), and uses it to practically eliminate buffer pool overheads for memoryresident data. Our implementation and experimental evaluation demonstrate that we achieve graceful performance degradation when the working set grows to exceed the buffer pool size, and graceful improvement when the working set shrinks towards and below the memory and buffer pool sizes.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {37–48},
numpages = {12}
}

@article{10.1145/1536616.1536632,
author = {Jacobs, Adam},
title = {The Pathologies of Big Data},
year = {2009},
issue_date = {August 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/1536616.1536632},
doi = {10.1145/1536616.1536632},
abstract = {Scale up your datasets enough and your apps come undone. What are the typical problems and where do the bottlenecks surface?},
journal = {Commun. ACM},
month = {aug},
pages = {36–44},
numpages = {9}
}

@inproceedings{10.1145/3312714.3312728,
author = {Hajiheydari, Nastaran and Talafidaryani, Mojtaba and Khabiri, SeyedHossein},
title = {IoT Big Data Value Map: How to Generate Value from IoT Data},
year = {2019},
isbn = {9781450362351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312714.3312728},
doi = {10.1145/3312714.3312728},
abstract = {Huge sources of business value are emerging due to big data generated by the Internet of Things (IoT) technologies paired with Machine Learning (ML) and Data Mining (DM) techniques' ability to harness and extract hidden knowledge from data and consequently learning and improving spontaneously. This paper reviews different examples of analyzing big data generated through IoT in previous studies and in various domains; then claims their business Value Proposition Map deploying Value Proposition Canvas as a novel conceptual tool. As a result, the proposed unprecedented framework of this paper entitled "IoT Big Data Value Map" shows a roadmap from raw data to real-world business value creation, blossomed out of a kind of three-pillar structure: IoT, Data Mining, and Value Proposition Map. The result of this study paves the way for prototyping business models in this field based on value invention from huge data analysis generated by IoT devices in different industries. Furthermore, researchers may complete this map by associating proposed framework with potential customers' profile and their expectations.},
booktitle = {Proceedings of the 5th International Conference on E-Society, e-Learning and e-Technologies},
pages = {98–103},
numpages = {6},
keywords = {IoT Big Data Value Map, Value Proposition Canvas, Value Proposition Map, Data Mining, Internet of Things (IoT)},
location = {Vienna, Austria},
series = {ICSLT '19}
}

@article{10.1145/2168931.2168932,
author = {Wakkary, Ron and Stolterman, Erik},
title = {WELCOME Interacting with Big Data},
year = {2012},
issue_date = {May + June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1072-5520},
url = {https://doi.org/10.1145/2168931.2168932},
doi = {10.1145/2168931.2168932},
journal = {Interactions},
month = {may},
pages = {5},
numpages = {1}
}

@inproceedings{10.1145/2612733.2612762,
author = {Bertot, John Carlo and Butler, Brian S. and Travis, Diane M.},
title = {Local Big Data: The Role of Libraries in Building Community Data Infrastructures},
year = {2014},
isbn = {9781450329019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2612733.2612762},
doi = {10.1145/2612733.2612762},
abstract = {Communities face opportunities and challenges in many areas, including education, health and wellness, workforce and economic development, housing, and the environment [21]. At the same time, governments have significant fiscal constraints on their ability to address these challenges and opportunities. Through a combination of open government, open data, and civic engagement, however, governments, citizens, civil society groups, and others are reinventing the relationship between governments and the governed by developing crowdsourced and other innovative solutions for community advancement. Underlying this reinvention and innovation is data -- particularly local data about housing, air quality, graduation rates, literacy rates, poverty, disease, and more. And yet, not all communities have the capacity to create, work with, or leverage data at the local level. Using a case study approach in a medium-sized U.S. city, this paper focuses on the issues that smaller communities face when seeking to create local data infrastructures and the extent to which libraries can develop their capabilities, capacity, and abilities to work with community information and data to facilitate community engagement and high-impact, locally relevant analytics.},
booktitle = {Proceedings of the 15th Annual International Conference on Digital Government Research},
pages = {17–23},
numpages = {7},
keywords = {libraries, data curation, data management, big data, community engagement, data infrastructure, communities},
location = {Aguascalientes, Mexico},
series = {dg.o '14}
}

@inproceedings{10.1145/3148055.3149209,
author = {Tang, Yan and Li, Mingzheng and Wang, Wangsong and Xuan, Pengcheng and Geng, Kun},
title = {Quality-Aware Movie Recommendation System on Big Data},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3149209},
doi = {10.1145/3148055.3149209},
abstract = {The movie recommendation is one of the most active application domain for recommendation systems (RS). However, with the rapid growth in the number of films, users have vastly different needs for the quality of the movie. In addition, facing big data, the traditional stand-alone RS is incapable to meet the need of an accurate and prompt recommendation. Aiming at solving these challenges, in this paper, we first parallelize the collaborative filtering to improve the computational efficiency, then we propose a quality-aware big data based movie recommendation system.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {273–274},
numpages = {2},
keywords = {map-reduce, hadoop, recommender systems, quality-aware},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.5555/2693848.2694145,
author = {Wilschut, Tim and Adan, Ivo J. B. F. and Stokkermans, Joep},
title = {Big Data in Daily Manufacturing Operations},
year = {2014},
publisher = {IEEE Press},
abstract = {Big data analytics is at the brink of changing the landscape in NXP Semiconductors Back End manufacturing operations. Numerous IT tools, implemented over the last decade, collect gigabytes of data daily, though the potential value of this data still remains to be explored. In this paper, the software tool called Heads Up is presented. Heads Up intelligently scans, filters, and explores the data with use of simulation. The software provides real-time relevant information, which is of high value in daily, as well as long term, production management. The software tool has been introduced at the NXP high volume manufacturing plant GuangDong China, where it is about to shift the paradigm on manufacturing operations.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {2364–2375},
numpages = {12},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/3010089.3010127,
author = {Louhi, Ibrahim and Boudjeloud-Assala, Lydia and Tamisier, Thomas},
title = {Big Data Clustering Using Data Streams Approach},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010127},
doi = {10.1145/3010089.3010127},
abstract = {In this paper we propose to process big data using a data streams approach. The data set is divided into subsets, each subsets is considered as a time window from a data stream. Our approach uses a neighborhood-based clustering. Instead of processing each new element one by one, we propose to process each group of new elements simultaneously. A clustering is applied on each new group using neighborhood graphs. The obtained clusters are then used to incrementally construct a representative graph of the data. The data graph is visualized in real time with specific visualizations that reflect the processing algorithm. To validate the approach, we apply it to different data streams and we compare it with known data stream clustering approaches.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {42},
numpages = {8},
keywords = {Big Data, Clustering, Neighborhood Graphs, Data Streams},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/2505515.2514697,
author = {Murphy, Kevin},
title = {From Big Data to Big Knowledge},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2514697},
doi = {10.1145/2505515.2514697},
abstract = {We are drowning in big data, but a lot of it is hard to interpret. For example, Google indexes about 40B webpages, but these are just represented as bags of words, which don't mean much to a computer. To get from "strings to things", Google introduced the Knowledge Graph (KG), which is a database of facts about entities (people, places, movies, etc.) and their relations (nationality, geo-containment, actor roles, etc). KG is based on Freebase, but supplements it with various other structured data sources. Although KG is very large (about 500M nodes/ entities, and 30B edges/ relations), it is still very incomplete. For example, 94% of the people are missing their place of birth, and 78% have no known nationality - these are examples of missing links in the graph. In addition, we are missing many nodes (corresponding to new entities), as well as new types of nodes and edges (corresponding to extensions to the schema). In this talk, I will survey some of the efforts we are engaged in to try to "grow" KG automatically using machine learning methods. In particular, I will summarize our work on the problems of entity linkage, relation extraction, and link prediction, using data extracted from natural language text as well as tabular data found on the web.},
booktitle = {Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management},
pages = {1917–1918},
numpages = {2},
keywords = {information extraction, knowledge bases, machine learning},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@inproceedings{10.1145/3341042.3341056,
author = {Li, Guiqin and Du, Zhipeng and Gao, Zhiyuan and Chen, Feng},
title = {Ethics Education of Information and Big Data},
year = {2019},
isbn = {9781450372220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341042.3341056},
doi = {10.1145/3341042.3341056},
abstract = {With the rapid development of information technology, the extensive application of information network technology and smart devices in the era of big data has not only brought great convenience to people, but also triggered a series of new ethical issues.College students, as the main group of information activities in the era of big data, are unable to integrate information ethics knowledge with behaviors due to their lack of information screening ability and output ability, resulting in various information anomies. Based on the actual curriculum research, this paper points out the current situation and reasons of information ethics faced by college students, and puts forward the corresponding countermeasures, which has a positive and farreaching significance for coordinating the ethical order of network society and promoting the construction of a harmonious society.},
booktitle = {Proceedings of the 2019 International Conference on Modern Educational Technology},
pages = {96–100},
numpages = {5},
keywords = {Engineering ethics, Information ethics, Big data, Education},
location = {Nanjing, China},
series = {ICMET 2019}
}

