@inproceedings{10.1145/3436286.3436318,title = {Exploring the Relationship between Aviation Service Quality and Customer Satisfaction Based on Big Data Technology}, author = {Ling Hong , Weiguo Chen },year = {2020}, isbn = {9781450376457}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3436286.3436318}, doi = {10.1145/3436286.3436318}, abstract = {As the consumption level of my country's residents increases, customers' requirements for the quality of aviation services are also increasing. Airlines must constantly improve service quality to be satisfied with the increasing demands of customers. The application of big data technology is conducive to improving the quality of aviation services and improving customer satisfaction in the air transportation industry. The relation between aviation quality of service and customer satisfaction is studied in this article. At the same time, how big data technology should be used to improve aviation service quality and improve customer satisfaction is discussed in this article.}, location = {Johannesburg, South Africa}, series = {ISBDAI '20}, pages = {168\u2013173}, numpages = {6}, keywords = {Service Quality, Customer Satisfaction, Aviation, Big Data Technology}}
@inproceedings{10.1109/BDC.2014.11,title = {A Practical Approach to Scalable Big Data Computing for the Personalization of Services at Samsung}, author = {Ahnn Jong Hoon },year = {2014}, isbn = {9781479918973}, publisher = {IEEE Computer Society}, address = {USA}, url = {https://doi.org/10.1109/BDC.2014.11}, doi = {10.1109/BDC.2014.11}, abstract = {We observe that the recent advances in big data computing have empowered the personalization of service including model-based services such as speech recognition, face recognition, and context-aware service. Various sources of user's logs can be utilized in remodeling, adapting, and personalizing pretrained models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner on top of Samsung' big data infrastructure. An automatic speech recognition (ASR) service such as Samsung's S-Voice, Apple's SIRI is one of the representative examples. Recently advances in ASR married with big data technologies drive more personalized services in many areas of services. A speaker adaptation is now a well-accepted technology that requires huge computation cost in creating a personalized acoustic model and corresponding language model over several billions of Samsung product users. We implement a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. An optimal set of performance optimization is suggested to have the best performance such as workflow compaction, file compression, best file system selection among several distributed file systems.}, series = {BDC '14}, pages = {64\u201373}, numpages = {10}, keywords = {Scalability, Big Data, Speech Recognition, Personalization, Cloud Computing, Hadoop}}
@inproceedings{10.1145/3391274,title = {Proceedings of The International Workshop on Semantic Big Data},year = {2020}, isbn = {9781450379748}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The goal of this workshop is to bring together academic researchers and industry practitioners to address the challenges and report and exchange the research findings in Semantic Big Data, including new approaches, techniques and applications, make substantial theoretical and empirical contributions to, and significantly advance the state of the art of Semantic Big Data.}, location = {Portland, Oregon}}
@inproceedings{10.1145/3404555.3404630,title = {Efficient Optimized Strategy of Big Data Retrieval}, author = {Dou Jinfeng , Chu Lei , Cao Jiabao , Qiu Yang , Zhao BaoLin },year = {2020}, isbn = {9781450377089}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3404555.3404630}, doi = {10.1145/3404555.3404630}, abstract = {With the development of new information technologies, the accumulation of data volume has been exploding, and big data retrieval has played an increasingly important role in big data technology. The challenge of data retrieval are the improvement of retrieval accuracy and retrieval speed. Aiming at the demand of big data platform for efficient data retrieval, an efficient optimized strategy is proposed. We found when the primary key query is used, the query response can be quick. However, when using a non-primary key query, the cache table needs to be comprehensively scanned and the longer response delay may be induced. This paper proposes a secondary index based on Solr to increase the accuracy of information retrieval and the quality of user experience. Then a cache-heat evaluation algorithm categorizes data according to access frequency to reduce query latency. Moreover, an index optimization method based on memory cache updates the cache to save space and enhance utilization. The experiments and simulation demonstrate that the proposed strategy can effectively improves the big data retrieval.}, location = {Tianjin, China}, series = {ICCAI '20}, pages = {109\u2013116}, numpages = {8}, keywords = {Big data, index, solr, data retrieval, hadoop}}
@inproceedings{10.1145/3132498.3132510,title = {Characterizing big data software architectures: a systematic mapping study}, author = {Sena Bruno , Allian Ana Paula , Nakagawa Elisa Yumi },year = {2017}, isbn = {9781450353250}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3132498.3132510}, doi = {10.1145/3132498.3132510}, abstract = {Big data is a broad term for large, dynamic, and complex data sets that have brought great challenges to be addressed by traditional software systems. It has also demanded advanced software architectures (i.e., the big data software architectures) prepared to deal with the continuous expansion of the volume of data as well as to take advantage of new technologies for big data context. However, the main characteristics, basic requirements, and modules and organization of big data architectures are not still widely known. Besides that, no detailed overview about them is available. The main contribution of this paper is to present the state of the art related to big data software architectures; for this, we conducted a Systematic Mapping Study. As results, an essential set of eight requirements for big data architectures was identified, besides a collection of five modules that are fundamental to adequately enable the data flow. We also intend these results can guide architects in the development of software systems for this new challenging scenario of big data management.}, location = {Fortaleza, Cear\u00e1, Brazil}, series = {SBCARS '17}, pages = {1\u201310}, numpages = {10}, keywords = {systematic mapping study, reference architecture, software architecture, big data system}}
@inproceedings{10.1145/584792.584894,title = {Rule-based data quality}, author = {Loshin David },year = {2002}, isbn = {1581134924}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/584792.584894}, doi = {10.1145/584792.584894}, abstract = {In the business intelligence/data warehouse user community, there is a growing confusion as to the difference between data cleansing and data quality. While many data cleansing products can help in applying data edits to name and address data, or help in transforming data during an ETL process, there is usually no persistence in this cleansing. This paper describes how we have implemented a business rules approach to build a data validation engine, called GuardianIQ, that transforms declarative data quality rules into code that objectively measures and reports levels of data quality based on user expectations.}, location = {McLean, Virginia, USA}, series = {CIKM '02}, pages = {614\u2013616}, numpages = {3}, keywords = {business rules, data validation, data quality}}
@inproceedings{10.1145/2839509.2844651,title = {Teaching Big Data with a Virtual Cluster}, author = {Eckroth Joshua },year = {2016}, isbn = {9781450336857}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2839509.2844651}, doi = {10.1145/2839509.2844651}, abstract = {Both industry and academia are confronting the challenge of big data, i.e., data processing that involves data so voluminous or arriving at such high velocity that no single commodity machine is capable of storing or processing them all. A common approach to handling big data is to divide and distribute the processing job to a cluster of machines. Ideally, a course that teaches students how to work with big data would provide students access to a cluster for hands-on practice. However, a cluster of physical, on-premise machines may be prohibitively expensive, particularly at smaller institutions with smaller budgets.In this report, we summarize our experiences developing and using a virtual cluster in a big data mining and analytics course at a small private liberal arts college. A single moderately-sized server hosts a cluster of virtual machines, which run the popular Apache Hadoop system. The virtual cluster gives students hands-on experience and costs less than an equal number of physical machines. It is also easily constructed and reconfigured. We describe our implementation, analyze its performance characteristics, and compare costs with physical clusters and the Amazon Elastic MapReduce cloud service. We summarize our use of the virtual cluster in the classroom and show student feedback.}, location = {Memphis, Tennessee, USA}, series = {SIGCSE '16}, pages = {175\u2013180}, numpages = {6}, keywords = {big data, virtual machines, cloud computing, curriculum}}
@inproceedings{10.1145/2693561.2693563,title = {Runtime Performance Challenges in Big Data Systems}, author = {Klein John , Gorton Ian },year = {2015}, isbn = {9781450333405}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2693561.2693563}, doi = {10.1145/2693561.2693563}, abstract = {Big data systems are becoming pervasive. They are distributed systems that include redundant processing nodes, replicated storage, and frequently execute on a shared 'cloud' infrastructure. For these systems, design-time predictions are insufficient to assure runtime performance in production. This is due to the scale of the deployed system, the continually evolving workloads, and the unpredictable quality of service of the shared infrastructure. Consequently, a solution for addressing performance requirements needs sophisticated runtime observability and measurement. Observability gives real-time insights into a system's health and status, both at the system and application level, and provides historical data repositories for forensic analysis, capacity planning, and predictive analytics. Due to the scale and heterogeneity of big data systems, significant challenges exist in the design, customization and operations of observability capabilities. These challenges include economical creation and insertion of monitors into hundreds or thousands of computation and data nodes, efficient, low overhead collection and storage of measurements (which is itself a big data problem), and application-aware aggregation and visualization. In this paper we propose a reference architecture to address these challenges, which uses a model-driven engineering toolkit to generate architecture-aware monitors and application-specific visualizations.}, location = {Austin, Texas, USA}, series = {WOSP '15}, pages = {17\u201322}, numpages = {6}, keywords = {observability, big data, model-driven engineering}}
@inproceedings{10.5555/2555523.2555559,title = {Big data and analytics}, author = {Statchuk Craig , Iles Michael , Thomas Fenny },year = {2013}, publisher = {IBM Corp.}, address = {USA}, abstract = {Business Analytics is maturing and moving towards mass adoption. The emergence of big data increases the need for innovative tools and methodologies. Of particular interest is the established Business Intelligence market segment, built on structured data and reporting. How does big data affect methodologies like ETL, modeling and report authoring? Business Intelligence is at a crossroads between less formal data analysis at scale and business imperatives like regulatory reporting that runs an enterprise. This paper highlights new technologies and services that move the methodologies of old into the data-centric world of high volume and velocity that defines the modern information landscape.}, location = {Ontario, Canada}, series = {CASCON '13}, pages = {341\u2013343}, numpages = {3}}
@inproceedings{10.1145/3206157,title = {Proceedings of the 2018 International Conference on Big Data and Education},year = {2018}, isbn = {9781450363587}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {ICBDE 2018 provides a scientific platform for local and international scientists, engineers, and technologists, who work in all aspects of Big Data and Education. In addition to the contributed papers, internationally-known experts from several countries are also invited to deliver keynote speeches at ICBDE 2018.}, location = {Honolulu, HI, USA}}
@inproceedings{10.1145/3361758.3361761,title = {Research and Top-level Framework Design on Unified Resource Management of Big Data in Strategic Consulting}, author = {Song Meina , Xu Xiangyu , Haihong E. },year = {2019}, isbn = {9781450372466}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3361758.3361761}, doi = {10.1145/3361758.3361761}, abstract = {This paper puts forward a set of top-level framework design methodology for unified data resource management aiming at the characteristics of big data, multi-source and heterogeneous, and the difficulty of unified organization and management, and completes the top-level framework of big data for strategic consulting based on this methodology for the scenario of strategic consulting. This paper investigates the unified data resource integration model at home and abroad, and proposed a set of architecture design methodology based on it. The methodology includes three aspects: metadata-driven, hierarchical organization, separation and reorganization. Then the article takes the real data as an example and gives the preliminary results of the model to prove the validity of the model.}, location = {Melbourn, VIC, Australia}, series = {BDIOT 2019}, pages = {8\u201312}, numpages = {5}, keywords = {Top level framework, Strategic consultation, Unified Resource Management, Big data}}
@inproceedings{10.1145/3191697.3213801,title = {Debugging support for big data processing applications}, author = {Marra Matteo },year = {2018}, isbn = {9781450355131}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3191697.3213801}, doi = {10.1145/3191697.3213801}, abstract = {Current trends in Big Data processing indicate that the volume, velocity and variety of data are increasing quickly due to an explosion on diversity and number of sources of information. This poses challenges for Big Data frameworks to be able to meet the new requirements of the emerging real-time streaming data processing applications. This research project focuses on the academic study of integrated development environments and debugging tools to assist the software development of Big Data applications.}, location = {Nice, France}, series = {Programming'18 Companion}, pages = {241\u2013242}, numpages = {2}, keywords = {Tools, Meta-Level Interfaces, Debugging, Big Data}}
@inproceedings{10.1145/2663715.2669615,title = {Privacy Aspects in Big Data Integration: Challenges and Opportunities}, author = {Christen Peter },year = {2014}, isbn = {9781450315838}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2663715.2669615}, doi = {10.1145/2663715.2669615}, abstract = {Big Data projects often require data from several sources to be integrated before they can be used for analysis. Once data have been integrated, they allow more detailed analysis that would otherwise not be possible. Accordingly, recent years have seen an increasing interest in techniques that facilitate the integration of data from diverse sources. Whenever data about individuals, or otherwise sensitive data, are to be integrated across organizations, privacy and confidentiality have to be considered. Domains where privacy preservation during data integration is of importance include business collaborations, health research, national censuses, the social sciences, crime and fraud detection, and homeland security. Increasingly, applications in these domains require data from diverse sources (both internal and external to an organization) to be integrated.Consequently, in the past decade, various techniques have been developed that aim to facilitate data integration without revealing any private or confidential information about the databases and records that are integrated. These techniques either provably prevent leakage of any private information, or they provide some empirical numerical measure of the risk of disclosure of private information.In the first part of this presentation we provide a background on data integration, and illustrate the importance of preserving privacy during data integration with several application scenarios. We then given an overview of the main concepts and techniques that have been developed to facilitate data integration in such ways that no private or confidential information is being revealed. We focus on privacy-preserving record linkage (PPRL), where so far most research has been conducted. We describe the basic protocols used in PPRL, and several key technologies employed in these protocols. Finally, we discuss the challenges privacy poses to data integration in the era of Big Data, and we discuss directions and opportunities in this research area.}, location = {Shanghai, China}, series = {PSBD '14}, pages = {1}, numpages = {1}, keywords = {data matching, privacy-preserving record linkage, multi-party, scalability, privacy techniques}}
@inproceedings{10.1145/3220199,title = {Proceedings of the 3rd International Conference on Big Data and Computing},year = {2018}, isbn = {9781450364263}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {We would like to extend our warmest welcome to all of you to Shenzhen city, China's first Special Economic Zone, to attend 2018 International Conference on Big Data and Computing (ICBDC 2018), from 28 April to 30 April, 2018.}, location = {Shenzhen, China}}
@inproceedings{10.1145/3071088.3071090,title = {Big data visualization: promises & pitfalls}, author = {Hepworth Katherine },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3071088.3071090}, doi = {10.1145/3071088.3071090}, abstract = {A few weeks ago, I was having dinner with a friend when a controversial subject came up. My friend had an extremely strong opinion about the harm caused by vaccination, and his argument went something like this: \"I've seen the data. There was an infographic laying it all out.\" He couldn't remember specific numbers from the visualization he'd seen or the author of the article. He couldn't even remember the name of the publication, but the data visualization's overall argument was firmly lodged in his mind. His situation is not unique, and it provides telling insights on how we, as humans, perceive and respond to big data visualization.}, pages = {7\u201319}, numpages = {13}}
@inproceedings{10.1145/3507524,title = {2021 4th International Conference on Computing and Big Data},year = {2021}, isbn = {9781450387194}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Wuhan, China}}
@inproceedings{10.1145/3423603.3424056,title = {Recent applications of big data in finance}, author = {Tekaya Balkiss , Feki Sirine El , Tekaya Tasnim , Masri Hela },year = {2020}, isbn = {9781405377539}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3423603.3424056}, doi = {10.1145/3423603.3424056}, abstract = {The financial sector generates tremendous amounts of data daily. Consequently, more attention is being focused on transforming that data into actionable knowledge. Big data and data science techniques have revolutionized the business world; the focus of this paper is to study the extent of this transformation in the financial field by considering research works discussing big data in financial markets, banking, credit risk management, fraud detection and insurance and illustrating some real life applications and challenges of this technology.}, location = {Virtual Event, Tunisia}, series = {DTUC '20}, pages = {1\u20136}, numpages = {6}, keywords = {data science, banking, finance, big data}}
@inproceedings{10.1145/3340017.3340032,title = {Big Data Perception & Usage: A Micro-Firm Perspective (The Case of the French Traditional Restaurant Sector)}, author = {Lichy Jessica , Kachour Maher },year = {2019}, isbn = {9781450362375}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3340017.3340032}, doi = {10.1145/3340017.3340032}, abstract = {This study contributes to the literature on Big Data and, specifically, the barriers that prevent micro-firms (fewer than 10 employees) from integrating digital solutions, in the context of the French traditional restaurant sector. Using focus group interviews followed by survey methodology, the authors examine the perception and usage of Big Data, from the perspective of micro-firm managers/owners. The results suggest that a combination of factors affect how micro-firms adopt/accept Big Data technologies including: perception of Big Data as a source for developing the business, uncertainty regarding return-on-investment, and awareness of the opportunities that Big Data can deliver. This study extends the literature on Big Data by offering a contemporary perspective of micro-firm managers/owners who face the challenge of assessing how and where they could innovate their business model with regard to Big Data.}, location = {Lyon, France}, series = {ICEEG 2019}, pages = {89\u201394}, numpages = {6}, keywords = {perception and usage, micro-firms, France, Big Data, restaurants}}
@inproceedings{10.1145/2854006.2854008,title = {Data Quality: From Theory to Practice}, author = {Fan Wenfei },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2854006.2854008}, doi = {10.1145/2854006.2854008}, abstract = {Data quantity and data quality, like two sides of a coin, are equally important to data management. This paper provides an overview of recent advances in the study of data quality, from theory to practice. We also address challenges introduced by big data to data quality management.}, pages = {7\u201318}, numpages = {12}}
@inproceedings{10.1109/BDC.2014.21,title = {Monitoring Data Streams at Process Level in Scientific Big Data Batch Clusters}, author = {Kuehn Eileen , Fischer Max , Jung Christopher , Petzold Andreas , Streit Achim },year = {2014}, isbn = {9781479918973}, publisher = {IEEE Computer Society}, address = {USA}, url = {https://doi.org/10.1109/BDC.2014.21}, doi = {10.1109/BDC.2014.21}, abstract = {The operation of scientific big data centres requires an overall monitoring and perception of system components. Insights into internal and external network traffic is of high importance for understanding specific data flows regarding storage accesses, firewall configurations, and the scheduling of batch jobs on clusters for computing/analysis of data. However, wide adoptions of federated storage, the handling of numerous job on many-core nodes, or the execution of job pilots inside the batch system complicate current data stream monitoring attempts. Therefore, the rising complexity requires new approaches to extend available solutions. As existing batch system monitoring and related system monitoring tools do not support measurements at batch job level, a new tool has been developed and put into operation at the Grid Ka data and computing centre at KIT for monitoring continuous data streams. Obtained results can for example be used to realise an optimisation of LAN/WAN setups based on measured data flows to adapt to the actual needs. This paper describes the current approach being implemented at the Grid Ka batch cluster and presents first analysis results showing the significance of measurements. The described approach is consecutively applied to the context of computing for high-energy physics.}, series = {BDC '14}, pages = {90\u201395}, numpages = {6}, keywords = {big data, network monitoring, data streams, performance measurements, distributed systems, data analysis}}
@inproceedings{10.1145/3366650,title = {Proceedings of the 2nd International Conference on Computing and Big Data},year = {2019}, isbn = {9781450372909}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The 2nd International Conference on Computing and Big Data (ICCBD 2019) and its workshop the International Conference on Computer, Software Engineering and Applications (CSEA 2019) were held in Taichung Software Park, Taichung, Taiwan during October 18-20, 2019.}, location = {Taichung, Taiwan}}
@inproceedings{10.1145/3396452.3396453,title = {A Study on College Students' Mental Health Education and Early Warning Mechanism Based on Big Data}, author = {Conglin Cheng , Lin Li , Yi Li , Lei Tan },year = {2020}, isbn = {9781450374989}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3396452.3396453}, doi = {10.1145/3396452.3396453}, abstract = {Since the mental health education of college students has attracted more and more attention, it is of great importance and necessity to strengthen the construction of psychological warning mechanism for college students. With the advent of the new media era, it is time to apply big data technology to mental health education in colleges and universities and analyze the mental health data of college students. It can be said that the construction of an early warning mechanism based on big data and the realization of crisis intervention will help promote the innovative development of mental health education for college students.}, location = {London, United Kingdom}, series = {ICBDE '20}, pages = {1\u20134}, numpages = {4}, keywords = {big data, mental health education, psychological early warning mechanism}}
@inproceedings{10.1145/3323878,title = {Proceedings of the International Workshop on Semantic Big Data},year = {2019}, isbn = {9781450367660}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The goal of this workshop is to bring together academic researchers and industry practitioners to address the challenges and report and exchange the research findings in Semantic Big Data, including new approaches, techniques and applications, make substantial theoretical and empirical contributions to, and significantly advance the state of the art of Semantic Big Data.}, location = {Amsterdam, Netherlands}}
@inproceedings{10.1145/2939672.2939736,title = {Crime Rate Inference with Big Data}, author = {Wang Hongjian , Kifer Daniel , Graif Corina , Li Zhenhui },year = {2016}, isbn = {9781450342322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2939672.2939736}, doi = {10.1145/2939672.2939736}, abstract = {Crime is one of the most important social problems in the country, affecting public safety, children development, and adult socioeconomic status. Understanding what factors cause higher crime is critical for policy makers in their efforts to reduce crime and increase citizens' life quality. We tackle a fundamental problem in our paper: crime rate inference at the neighborhood level. Traditional approaches have used demographics and geographical influences to estimate crime rates in a region. With the fast development of positioning technology and prevalence of mobile devices, a large amount of modern urban data have been collected and such big data can provide new perspectives for understanding crime. In this paper, we used large-scale Point-Of-Interest data and taxi flow data in the city of Chicago, IL in the USA. We observed significantly improved performance in crime rate inference compared to using traditional features. Such an improvement is consistent over multiple years. We also show that these new features are significant in the feature importance analysis.}, location = {San Francisco, California, USA}, series = {KDD '16}, pages = {635\u2013644}, numpages = {10}, keywords = {big data, crime inference, spatial-temporal data, heterogeneous data}}
@inproceedings{10.1145/3451400,title = {2021 4th International Conference on Big Data and Education},year = {2021}, isbn = {9781450389389}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {London, United Kingdom}}
@inproceedings{10.1145/2640087.2644194,title = {The Value of Using Big Data Technologies in Computational Social Science}, author = {Ch'ng Eugene },year = {2014}, isbn = {9781450328913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2640087.2644194}, doi = {10.1145/2640087.2644194}, abstract = {The discovery of phenomena in social networks has prompted renewed interests in the field. Data in social networks however can be massive, requiring scalable Big Data architecture. Conversely, research in Big Data needs the volume and velocity of social media data for testing its scalability. Not only so, appropriate data processing and mining of acquired datasets involve complex issues in the variety, veracity, and variability of the data, after which visualisation must occur before we can see fruition in our efforts. This extended abstract presents topical, multimodal, and longitudinal social media datasets from the integration of various scalable open source technologies. The full article details the process that led to the discovery of social information landscapes within the Twitter social network, highlighting the experience of dealing with social media datasets, using a funneling approach so that data becomes manageable.}, location = {Beijing, China}, series = {BigDataScience '14}, pages = {1\u20132}, numpages = {2}, keywords = {open source, data mining, twitter, computational social science, Social network analysis}}
@inproceedings{10.1145/2640087.2644162,title = {The Value of Using Big Data Technologies in Computational Social Science}, author = {Ch'ng Eugene },year = {2014}, isbn = {9781450328913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2640087.2644162}, doi = {10.1145/2640087.2644162}, abstract = {The discovery of phenomena in social networks has prompted renewed interests in the field. Data in social networks however can be massive, requiring scalable Big Data architecture. Conversely, research in Big Data needs the volume and velocity of social media data for testing its scalability. Not only so, appropriate data processing and mining of acquired datasets involve complex issues in the variety, veracity, and variability of the data, after which visualisation must occur before we can see fruition in our efforts. This article presents topical, multimodal, and longitudinal social media datasets from the integration of various scalable open source technologies. The article details the process that led to the discovery of social information landscapes within the Twitter social network, highlighting the experience of dealing with social media datasets, using a funneling approach so that data becomes manageable. The article demonstrated the feasibility and value of using scalable open source technologies for acquiring massive, connected datasets for research in the social sciences.}, location = {Beijing, China}, series = {BigDataScience '14}, pages = {1\u20134}, numpages = {4}, keywords = {computational social science, social network analysis, twitter, data mining, open source}}
@inproceedings{10.1145/3341042.3341056,title = {Ethics Education of Information and Big Data}, author = {Li Guiqin , Du Zhipeng , Gao Zhiyuan , Chen Feng },year = {2019}, isbn = {9781450372220}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341042.3341056}, doi = {10.1145/3341042.3341056}, abstract = {With the rapid development of information technology, the extensive application of information network technology and smart devices in the era of big data has not only brought great convenience to people, but also triggered a series of new ethical issues.College students, as the main group of information activities in the era of big data, are unable to integrate information ethics knowledge with behaviors due to their lack of information screening ability and output ability, resulting in various information anomies. Based on the actual curriculum research, this paper points out the current situation and reasons of information ethics faced by college students, and puts forward the corresponding countermeasures, which has a positive and farreaching significance for coordinating the ethical order of network society and promoting the construction of a harmonious society.}, location = {Nanjing, China}, series = {ICMET 2019}, pages = {96\u2013100}, numpages = {5}, keywords = {Big data, Engineering ethics, Education, Information ethics}}
@inproceedings{10.1145/2694730.2694734,title = {Experimentation as a Tool for the Performance Evaluation of Big Data Systems}, author = {Apon Amy W. },year = {2015}, isbn = {9781450333382}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2694730.2694734}, doi = {10.1145/2694730.2694734}, abstract = {The complex big data systems of today are difficult, if not impossible, to model analytically. The challenges of these distributed and parallel data processing systems include heterogeneous network communication, a mix of storage, memory, and computing devices, and common failures of communication and devices. Particular challenges with big data systems include the variety and volume of data that place previously unseen stresses on distributed computing systems. Experimentation using production-quality hardware and software and realistic data is required to understand system tradeoffs. At the same time, experimental evaluation has challenges, including access to hardware resources at scale, robust workload characterization, data characterization, configuration management of software and systems, and sometimes insidious optimization issues around the mix of software stacks or hardware/software resource allocation. In this talk we present a number of the research challenges when experimentation is used as a tool for the performance evaluation of big data systems, some approaches to solutions, and open questions for this area.}, location = {Austin, Texas, USA}, series = {PABS '15}, pages = {3}, numpages = {1}, keywords = {big data systems, data characterization, performance evaluation, experimentation, workload characterization}}
@inproceedings{10.1145/3063955.3063968,title = {The design of course architecture for big data}, author = {Wang Hongzhi , Gao Hong , Yin Shenjun , Zhu Jie },year = {2017}, isbn = {9781450348737}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3063955.3063968}, doi = {10.1145/3063955.3063968}, abstract = {Big data is one of the hottest topic in not only academic but also enterprise, which provide grate requirements for the people with knowledge and experiences of big data. However, current education architecture of computer science could not provide sufficient training for big data. For the education for people suitable for big data era, we attempt to design a novel course architecture. Such course architecture will not change the skeleton of traditional course architecture of computer science but just add content and subjects that is adaptive for big data. In this paper, we discuss the goal, architecture and content of the course architecture.}, location = {Shanghai, China}, series = {ACM TUR-C '17}, pages = {1\u20136}, numpages = {6}, keywords = {big data, course architecture, data science}}
@inproceedings{10.1145/3242840.3242842,title = {Load Forecasting Research Based on High Performance Intelligent Data Processing of Power Big Data}, author = {Xu Menghan , Huang Gaopan , Zhang Mingming , Cui Peng , Wang Chong },year = {2018}, isbn = {9781450365093}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3242840.3242842}, doi = {10.1145/3242840.3242842}, abstract = {The method proposed in this paper is a data analysis method that intelligently analyzes power big data and realizes the load forecasting of power grid. The method calls for the corresponding data from each database of big data platform by accepting the load forecast request from the client, and performs the load forecasting in the big data by improving the gray model of chaos genetic algorithm (CGA). After the completion of load forecasting, the final output to the client load forecasting results.}, location = {Beijing, China}, series = {ICACS '18}, pages = {55\u201360}, numpages = {6}, keywords = {load forecasting, Power big data, power grid, chaos genetic algorithm (CGA)}}
@inproceedings{10.1145/2743065.2743099,title = {Perspectives, Motivations and Implications Of Big Data Analytics}, author = {Amudhavel J. , Padmapriya V. , Gowri V. , Lakshmipriya K. , Kumar K. Prem , Thiyagarajan B. },year = {2015}, isbn = {9781450334419}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2743065.2743099}, doi = {10.1145/2743065.2743099}, abstract = {As today there is an enormous volume of data, examining these large sets contains structure and unstructured data of different types and sizes; big data analytics is used. Data Analytics allows the user to analyze the unusable data to make a faster and better decision. The Latest supply chain professionals are suffused with data, which provokes various new ways of thoughts regarding how the data are produced, ordered, controlled and analyzed. Data Quality in Supply-Chain Management is used to monitor and control the data. This paper presents the knowledge infrastructure about trends in big data analytics. Big Data can also be given as an all-encompassing term for any collection of data sets so large or complex that it becomes difficult to process using traditional data processing applications. The challenges include examination, confine, extent, exploration, sharing, storage, relocate, and visualization and privacy infringement. The Big Data have their application in various fields like in Tourism, Climate Research and many other fields.}, location = {Unnao, India}, series = {ICARCSET '15}, pages = {1\u20135}, numpages = {5}, keywords = {unstructured data, infringement, Data analytics, monitor, exploration, Big data, application}}
@inproceedings{10.1145/3422713,title = {Proceedings of the 2020 3rd International Conference on Big Data Technologies},year = {2020}, isbn = {9781450387859}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {It is our great pleasure to welcome you to attend the 4th International Conference on 2020 3rd International Conference on Big Data Technologies (ICBDT 2020) and it's workshop 2020 4th International Conference on Business Information Systems (ICBIS 2020), held during September 18-20, 2020, which is supported by Shandong University of Science and Technology, China, University of Jinan, Shandong University, Guizhou University, Shandong Normal University, Qingdao University, and other universities. With the world fighting together against coronavirus this year, ICBDT 2020 has been converted into a virtual conference for the safety of our participants.}, location = {Qingdao, China}}
@inproceedings{10.1145/3297730,title = {Proceedings of the 2018 International Conference on Big Data Engineering and Technology},year = {2018}, isbn = {9781450365826}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {As we know, Big Data is a popular term used to describe the exponential growth and availability of data, both structured and unstructured. Big Data may be as important to business and society due to its wide applications as the Internet has become. It is well-known that more data may lead to more accurate decision making. To raise awareness of the Big Data Engineering and Technology and its challenges, we hold this conference to offer you a unique opportunity to share ideas and experiences and to discuss evolving Big Data Engineering and Technology and its challenges.}, location = {Chengdu, China}}
@inproceedings{10.1145/3437075,title = {Proceedings of the 2020 International Conference on Big Data in Management},year = {2020}, isbn = {9781450375061}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The International Conference on Big Data in Management 2020 (ICBDM 2020) is co-sponsored by The University of Manchester and Loughborough University, with assistance from Dakota State University, College of Charleston, and Shanghai Information Center for Life Sciences, Chinese Academy of Sciences. Affected by the COVID-19 pandemic, ICBDM 2020 was held as a virtual conference this year, so I'd like to extend our appreciation for the understanding and support from all the participants.}, location = {Manchester, United Kingdom}}
@inproceedings{10.1145/3127942.3127961,title = {Determinants of Big Data Adoption and Success}, author = {Al-Qirim Nabeel , Tarhini Ali , Rouibah Kamel },year = {2017}, isbn = {9781450352840}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3127942.3127961}, doi = {10.1145/3127942.3127961}, abstract = {This research investigates the large hype surrounding big data (BD) and Analytics (BDA) in both academia and the business world. Initial insights pointed to large and complex amalgamations of different fields, techniques and tools. Above all, BD as a research field and as a business tool found to be under developing and is fraught with many challenges. The intention here in this research is to develop an adoption model of BD that could detect key success predictors. The research finds a great interest and optimism about BD value that fueled this current buzz behind this novel phenomenon. Like any disruptive innovation, its assimilation in organizations oppressed with many challenges at various contextual levels. BD would provide different advantages to organizations that would seriously consider all its perspectives alongside its lifecycle in the pre-adoption or adoption or implementation phases. The research attempts to delineate the different facets of BD as a technology and as a management tool highlighting different contributions, implications and recommendations. This is of great interest to researchers, professional and policy makers.}, location = {Jeju Island, Republic of Korea}, series = {ICACS '17}, pages = {88\u201392}, numpages = {5}, keywords = {big data strategy, big data challenges, Big data analytics, big data success factors}}
@inproceedings{10.1145/3456887.3457461,title = {English Education Innovation Model Based on Big Data}, author = {Xiaoxiao Duan , Ping Duan },year = {2021}, isbn = {9781450389969}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456887.3457461}, doi = {10.1145/3456887.3457461}, abstract = {With the gradual acceleration of globalization, English, as the world's first language, is becoming more and more important in various fields. In China's education, English education mode is not very perfect. Therefore, we can combine the technical characteristics of the current era, grasp the technology of the times, use big data to find out the shortcomings of English education, and put forward appropriate solutions combined with the actual situation. This paper analyzes the advantages of big data technology applied to English teaching, analyzes and discusses the advantages of English education mode under the background of big data compared with the traditional education mode by taking two schools with the same category as the research object, and then the experiment shows that the former can improve the efficiency of the whole teaching work, and the students in the two schools can improve the efficiency of the whole teaching work In comparison, 37% of the participants in the experimental group were satisfied, 30% of them were very satisfied, while only 27% of the control group were satisfied. Therefore, the first mock exam is to introduce the innovation of English education mode in the context of big data. Based on this, the paper also makes some research and Discussion on the development of this model.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2021}, pages = {1058\u20131062}, numpages = {5}, keywords = {Big Data Environment, Big Data, Innovation Model, English Education}}
@inproceedings{10.1145/3129757.3129762,title = {Official statistics embrace big data: a review of current and developing international practice}, author = {Plekhanov Dmitriy },year = {2017}, isbn = {9781450354127}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3129757.3129762}, doi = {10.1145/3129757.3129762}, abstract = {Big Data is a phenomenon brought about by the rapid spread of digital devices and massive increase in information flow in the modern world. The digitalization of society provides new sources of information about our society in general. The main aim of the official statistics is to provide the public with relevant information on a timely basis. Therefore it is actually the duty of statisticians to explore opportunities provided by Big Data sources and try to find ways to generate information for the public good. This paper provides a brief overview of pilot projects carried out by national statistical organizations around the world and analyses main challenges related to the use of Big Data sources in the realm of official statistics.}, location = {St. Petersburg, Russia}, series = {eGose '17}, pages = {22\u201326}, numpages = {5}, keywords = {big data, data analysis, official statistics}}
@inproceedings{10.1145/3322134,title = {Proceedings of the 2019 International Conference on Big Data and Education},year = {2019}, isbn = {9781450361866}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The proceedings present a selection of the papers submitted to the conference from universities, research institutes and industries. All of the papers were subjected to peer-review by conference committee members and international reviewers. The papers selected for publishing in the proceedings depended on their quality and their relevancy to the conference. The proceedings tend to present to the readers the recent advances in the field of Big Data and Education and various related areas.}, location = {London, United Kingdom}}
@inproceedings{10.1145/2627534.2627564,title = {Dual direction big data download and analysis}, author = {Al-Jaroodi Jameela , Mohamed Nader , Eid Abdulla },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2627534.2627564}, doi = {10.1145/2627534.2627564}, abstract = {The term Big Data was recently coined as the amount of generated and stored digital data has grown so rapidly that it has become very hard to store, manage and analyze without coming up with new techniques that can cope with such challenges. Finding innovative approaches to support big data analysis has become a priority as both the research community and the industry are trying to make use of these huge amounts of available data. In this paper we introduce a new approach to enhance the overall big data analysis performance. The approach calls for utilizing data set replication, parallel download, and parallel processing over multiple compute nodes. The main concept calls for simultaneously parallelizing the download of the data (in partitions) from multiple replicated sites to multiple compute nodes that will also perform the analysis in parallel. Then the results are given to the client that requested the analysis.}, pages = {98\u2013101}, numpages = {4}, keywords = {big data, data replication, dual direction processing, parallel processing}}
@inproceedings{10.1145/2676723.2677318,title = {Big Data in Computer Science Education Research}, author = {Hazzan Orit , Shaffer Clifford A. },year = {2015}, isbn = {9781450329668}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2676723.2677318}, doi = {10.1145/2676723.2677318}, abstract = {Recent years have seen the emergence of applications and concepts that rely on the involvement of the general public (the \"crowd\") and, consequently, create big data (e.g., MOOCs, search engines, crowdsourcing, crowdfunding, citizen/crowd science, and more). Education in particular is changing dramatically with the use of online resources and courses that generate large streams of data. In this special session, we ask: What research questions in computer science education can be explored using big data? And how can computer science education researchers apply big data analysis to support education in other disciplines? To answer these and related questions, we focus in this special interactive session on how computer science education research can be promoted by integrating big data into the research process.}, location = {Kansas City, Missouri, USA}, series = {SIGCSE '15}, pages = {591\u2013592}, numpages = {2}, keywords = {computer science education, research in computer science education, citizen science, big data}}
@inproceedings{10.1145/2479440.2482677,title = {Issues in big data testing and benchmarking}, author = {Alexandrov Alexander , Br\u00fccke Christoph , Markl Volker },year = {2013}, isbn = {9781450321518}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2479440.2482677}, doi = {10.1145/2479440.2482677}, abstract = {The academic community and industry are currently researching and building next generation data management systems. These systems are designed to analyze data sets of high volume with high data ingest rates and short response times executing complex data analysis algorithms on data that does not adhere to relational data models. As these big data systems differ from standard relational database systems with respect to data and workloads, the traditional benchmarks used by the database community are insufficient. In this paper, we describe initial solutions and challenges with respect to big data generation, methods for creating realistic, privacy-aware, and arbitrarily scalable data sets, workloads, and benchmarks from real world data. We will in particular discuss why we feel that workloads currently discussed in the testing and benchmarking community do not capture the real complexity of big data and highlight several research challenges with respect to massively-parallel data generation and data characterization.}, location = {New York, New York}, series = {DBTest '13}, pages = {1\u20135}, numpages = {5}, keywords = {data profiling, workloads, benchmarking, big data, data generation}}
@inproceedings{10.1145/3461015,title = {Data and Process Quality Evaluation in a Textual Big Data Archiving System}, author = {Fugini Mariagrazia , Finocchi Jacopo },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3461015}, doi = {10.1145/3461015}, abstract = {The article presents a textual Big Data analytics solution developed in a real setting as a part of a high-capacity document digitization and storage system. A software based on machine learning techniques performs automated extraction and processing of textual contents. The work focuses on performance and data confidence evaluation and describes the approach to computing a set of indicators for textual data quality. It then presents experimental results.}, pages = {1\u201319}, numpages = {19}, keywords = {Big Data analytics, unstructured Big Data, content management, text analytics, machine learning, data quality}}
@inproceedings{10.1145/2666310.2666481,title = {Efficient spatial query processing for big data}, author = {Lee Kisung , Ganti Raghu K. , Srivatsa Mudhakar , Liu Ling },year = {2014}, isbn = {9781450331319}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2666310.2666481}, doi = {10.1145/2666310.2666481}, abstract = {Spatial queries are widely used in many data mining and analytics applications. However, a huge and growing size of spatial data makes it challenging to process the spatial queries efficiently. In this paper we present a lightweight and scalable spatial index for big data stored in distributed storage systems. Experimental results show the efficiency and effectiveness of our spatial indexing technique for different spatial queries.}, location = {Dallas, Texas}, series = {SIGSPATIAL '14}, pages = {469\u2013472}, numpages = {4}, keywords = {spatial query, big data, spatial indexing}}
@inproceedings{10.1145/3538950,title = {Proceedings of the 4th International Conference on Big Data Engineering},year = {2022}, isbn = {9781450395632}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Beijing, China}}
@inproceedings{10.1145/2331042.2331060,title = {Parallel machine learning on big data}, author = {Langford John },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2331042.2331060}, doi = {10.1145/2331042.2331060}, abstract = {On algorithms for parallel machine learning, and why they need to be more efficient.}, pages = {60\u201362}, numpages = {3}}
@inproceedings{10.1145/3010089,title = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},year = {2016}, isbn = {9781450347792}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The last decades, the fast growth of the information and the communication technology has leaded to overwhelming amount of data pushing our communication network to its limit. The big data volume is produced from an assortment of users and wireless mobile technologies, and are to be processed and stored in powerful datacenters. Indeed, conventional wireless communication networks cannot deal with the massive amount of exchanged data and there is a strong demand to create a fast and scalable inter-connected heterogeneous wireless network for the big data. These advanced big data wireless networks should address all big data lifecycle (access networks, Internet backbone, intra and inter datacenter networks) and satisfy transmission QoS.}, location = {Blagoevgrad, Bulgaria}}
@inproceedings{10.1145/2538862.2538877,title = {Integrating big data into the computing curricula}, author = {Silva Yasin N. , Dietrich Suzanne W. , Reed Jason M. , Tsosie Lisa M. },year = {2014}, isbn = {9781450326056}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2538862.2538877}, doi = {10.1145/2538862.2538877}, abstract = {An important recent technological development in computer science is the availability of highly distributed and scalable systems to process Big Data, i.e., datasets with high volume, velocity and variety. Given the extensive and effective use of systems incorporating Big Data in many application scenarios, these systems have become a key component in the broad landscape of database systems. This fact creates the need to integrate the study of Big Data Management Systems as part of the computing curricula. This paper presents well-structured guidelines to perform this integration by describing the important types of Big Data systems and demonstrating how each type of system can be integrated into the curriculum. A key contribution of this paper is the description of an array of course resources, e.g., virtual machines, sample projects, and in-class exercises, and how these resources support the learning outcomes and enable a hands-on experience with Big Data technologies.}, location = {Atlanta, Georgia, USA}, series = {SIGCSE '14}, pages = {139\u2013144}, numpages = {6}, keywords = {big data management systems, databases curricula}}
@inproceedings{10.1145/2591971.2611389,title = {Conquering big data with spark and BDAS}, author = {Stoica Ion },year = {2014}, isbn = {9781450327893}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2591971.2611389}, doi = {10.1145/2591971.2611389}, abstract = {Today, big and small organizations alike collect huge amounts of data, and they do so with one goal in mind: extract \"value\" through sophisticated exploratory analysis, and use it as the basis to make decisions as varied as personalized treatment and ad targeting. Unfortunately, existing data analytics tools are slow in answering queries, as they typically require to sift through huge amounts of data stored on disk, and are even less suitable for complex computations, such as machine learning algorithms. These limitations leave the potential of extracting value of big data unfulfilled.To address this challenge, we are developing Berkeley Data Analytics Stack (BDAS), an open source data analytics stack that provides interactive response times for complex computations on massive data. To achieve this goal, BDAS supports efficient, large-scale in-memory data processing, and allows users and applications to trade between query accuracy, time, and cost. In this talk, I'll present the architecture, challenges, results, and our experience with developing BDAS, with a focus on Apache Spark, an in-memory cluster computing engine that provides support for a variety of workloads, including batch, streaming, and iterative computations. In a relatively short time, Spark has become the most active big data project in the open source community, and is already being used by over one hundred of companies and research institutions.}, location = {Austin, Texas, USA}, series = {SIGMETRICS '14}, pages = {193}, numpages = {1}, keywords = {cluster computing, big data, distributed algorithms}}
@inproceedings{10.1145/2637364.2611389,title = {Conquering big data with spark and BDAS}, author = {Stoica Ion },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2637364.2611389}, doi = {10.1145/2637364.2611389}, abstract = {Today, big and small organizations alike collect huge amounts of data, and they do so with one goal in mind: extract \"value\" through sophisticated exploratory analysis, and use it as the basis to make decisions as varied as personalized treatment and ad targeting. Unfortunately, existing data analytics tools are slow in answering queries, as they typically require to sift through huge amounts of data stored on disk, and are even less suitable for complex computations, such as machine learning algorithms. These limitations leave the potential of extracting value of big data unfulfilled.To address this challenge, we are developing Berkeley Data Analytics Stack (BDAS), an open source data analytics stack that provides interactive response times for complex computations on massive data. To achieve this goal, BDAS supports efficient, large-scale in-memory data processing, and allows users and applications to trade between query accuracy, time, and cost. In this talk, I'll present the architecture, challenges, results, and our experience with developing BDAS, with a focus on Apache Spark, an in-memory cluster computing engine that provides support for a variety of workloads, including batch, streaming, and iterative computations. In a relatively short time, Spark has become the most active big data project in the open source community, and is already being used by over one hundred of companies and research institutions.}, pages = {193}, numpages = {1}, keywords = {distributed algorithms, big data, cluster computing}}
@inproceedings{10.1145/3239438.3239485,title = {Disease Trajectory Visualization System Based on Big Data Analytics}, author = {Chien Ting-Ying , Chen Chong-Yi , Jin Guo-Lun , Ting Hsien-Wei },year = {2018}, isbn = {9781450363891}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3239438.3239485}, doi = {10.1145/3239438.3239485}, abstract = {Background: For health promotion, recognition of risk factors that may modify health needs is necessary, and people then have the opportunity to change their behavior if they are aware of the risk factors. Big data analysis of health data is emerging as a new trend in research that may lead to discovery of unknown facts, and visualization of data is a convenient way in which to understand the characteristics of the data. This study used the National Health Insurance Research Database (NHIRD) as the basis to construct a visualization model for disease trajectory analysis.Methods: The NHIRD, which includes inpatient expenditure by admission (DD) and ambulatory care expenditure by visit (CD) data, was used in this study. We analyzed the medical care of patients, such as medications, surgery, medical expenditure and total length of stay, and calculated the number of patients who suffered other diseases within the next 6 months, 12 months and 24 months. A Sankey diagram was employed to show the disease trajectory. Based on data analytics, we constructed a system that helps the user to easily understand the disease trajectory.System implementation: The system includes two panels, a user input panel and an output panel. Using the input panel, the user can input basic information, such as gender, age, date and the queried disease. Based on user input and analytical models, the system will show the detailed trajectory for each queried disease, such as medical expenditure, medications and total length of stay in hospital, and uses Sankey diagrams to show the disease trajectory.Conclusions: This study constructed a visualization system based on analysis of the NHIRD. The Taiwan National Health Insurance Administration, Ministry of Health and Welfare, constructed the Health Bank, which contains health data of all individuals. This method may enable prediction of the risks of diseases in the future.}, location = {Tsukuba, Japan}, series = {ICMHI '18}, pages = {73\u201375}, numpages = {3}, keywords = {Decision-making system, National Health Insurance Research Database, Big Data}}