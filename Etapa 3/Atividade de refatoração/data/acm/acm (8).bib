@inproceedings{10.1145/3154943.3154946,
author = {Singh, Ruchi and Ananth, Yashaswi and Woo, Dr. Jongwook},
title = {Big Data Analysis of Local Business and Reviews},
year = {2017},
isbn = {9781450353120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3154943.3154946},
doi = {10.1145/3154943.3154946},
abstract = {In this paper, we have analyzed the local business data and reviews to get insights on the popularity of a business and factors responsible for it. We have also analyzed the sentiments of the customer reviews for better understanding of business popularity. The total size of the dataset is 180 MB and we have adopted cloud computing service like Big Data Hadoop using HiveQL and Pig for query. Our preferred choice of framework for this project was Big Data Hadoop primarily because it is an open source software and its scalability and flexibility best suited our requirements.This project has helped us in understanding various aspects of the Local Businesses; factors driving their popularity, customer review patterns and regions that favor certain businesses the most, are a few of the many aspects to name. Analyzing the customer sentiments based on their reviews has helped us in realizing the importance of customer satisfaction, also it is possible to derive action plans to improve business performance to keep up with the competition.},
booktitle = {Proceedings of the International Conference on Electronic Commerce},
articleno = {3},
numpages = {5},
keywords = {local business, visualization, hive, IBM bluemix, big data, reviews, azure, data analysis, hadoop},
location = {Pangyo, Seongnam, Republic of Korea},
series = {ICEC '17}
}

@inproceedings{10.1145/3249182,
author = {Wagner, Stefan},
title = {Session Details: Data Quality},
year = {2011},
isbn = {9781450308519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249182},
doi = {10.1145/3249182},
booktitle = {Proceedings of the 8th International Workshop on Software Quality},
location = {Szeged, Hungary},
series = {WoSQ '11}
}

@inproceedings{10.1145/3284103.3284112,
author = {Xing, Han and Zhang, Ke and Yang, Zifan and Sun, Lianying and Liu, Yi},
title = {Traffic State Estimation with Big Data},
year = {2018},
isbn = {9781450360449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284103.3284112},
doi = {10.1145/3284103.3284112},
abstract = {Traffic state estimation helps urban traffic control and management. In this paper, a traffic state estimation model based on the fusion of Hidden Markov model and SEA algorithm is proposed considering the randomness and volatility of traffic systems. Traffic data of average travel speed in selected city were collected, and the mean and fluctuation values of average travel speed in adjacent time windows were calculated. With Hidden Markov model, the system state network is defined according to mean values and fluctuation values. The operation efficiency of traffic system, as well as stability and trend values, were calculated with System Effectiveness Analysis (SEA) algorithm based on system state network. Calculation results show that the method perform well and can be applied to both traffic state assessment of certain road sections and large scale road networks.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience},
articleno = {9},
numpages = {5},
keywords = {SEA, Traffic State Estimation, System State Network, HMM},
location = {Seattle, WA, USA},
series = {Safety and Resilience'18}
}

@article{10.1145/2451856.2451869,
author = {Kim, Jeffrey and Lund, Arnie and Dombrowski, Caroline},
title = {Telling the Story in Big Data},
year = {2013},
issue_date = {May + June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1072-5520},
url = {https://doi.org/10.1145/2451856.2451869},
doi = {10.1145/2451856.2451869},
journal = {Interactions},
month = {may},
pages = {48–51},
numpages = {4}
}

@inproceedings{10.1145/3035918.3058737,
author = {Gulzar, Muhammad Ali and Interlandi, Matteo and Condie, Tyson and Kim, Miryung},
title = {Debugging Big Data Analytics in Spark with BigDebug},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3058737},
doi = {10.1145/3035918.3058737},
abstract = {To process massive quantities of data, developers leverage Data-Intensive Scalable Computing (DISC) systems such as Apache Spark. In terms of debugging, DISC systems support only post-mortem log analysis and do not provide any debugging functionality. This demonstration paper showcases BigDebug: a tool enhancing Apache Spark with a set of interactive debugging features that can help users in debug their Big Data Applications.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1627–1630},
numpages = {4},
keywords = {debugging, automatic fault localization, data-intensive scalable computing, disc, interactive tools, big data analytics},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3297730.3297744,
author = {Lu, Shan and Qiao, Rongrong},
title = {The Wisdom City Based on the Big Data Technology},
year = {2018},
isbn = {9781450365826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297730.3297744},
doi = {10.1145/3297730.3297744},
abstract = {Big data, Web of Things and cloud computing have emerged and are widely used. Only by making full use of the development of emerging technologies, wisdom city can realize its own wisdom city essence. This article will tell you how to combine big data with wisdom cities. Big data is not only the practical application of big data, but also the further improvement for intelligence of the city's level.},
booktitle = {Proceedings of the 2018 International Conference on Big Data Engineering and Technology},
pages = {36–39},
numpages = {4},
keywords = {Cloud Computing, Wisdom City, support technology, Big data, development},
location = {Chengdu, China},
series = {BDET 2018}
}

@inproceedings{10.1145/2676723.2693616,
author = {Bart, Austin Cory},
title = {Situating Computational Thinking with Big Data: Pedagogy and Technology (Abstract Only)},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2693616},
doi = {10.1145/2676723.2693616},
abstract = {As Computational Thinking becomes pervasive in undergraduate programs, new students must be educated in meaningful, authentic contexts that they find both motivating and relatable. I propose working with big data as a novel context for introductory programming, authentic given its importance in diverse fields such as agriculture, history, and more. Big data is considered difficult to use because of its inherent technical obstacles. To overcome these difficulties, I introduce a new project: CORGIS - a "Collection of Real-time, Giant, Interesting, Situated Datasets". The CORGIS project comprises a collection of libraries that provide an interface to big data for students, architectures for rapidly enabling new datasets, and a web-based textbook platform for disseminating relevant course materials. This textbook features an online block-based programming environment, real-time collaborative text editing, and continuous server-side storage. In this poster, I describe the educational theory guiding this work, the novel technolgy created and deployed, and the initial, promising results.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {719},
numpages = {1},
keywords = {Motivation, Big data, CORGIS, Computational Thinking},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1145/3195106.3195172,
author = {Anshari, Muhammad and Almunawar, Mohammad Nabil and Lim, Syamimi Ariff},
title = {Big Data and Open Government Data in Public Services},
year = {2018},
isbn = {9781450363532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195106.3195172},
doi = {10.1145/3195106.3195172},
abstract = {Big data is a relatively new approach in managing and analyzing a huge amount of dynamic data to discover useful information and knowledge. Even though big data is still in its infancy, it has been benefiting private and public organizations in large scale. Since public sector organizations have to maintain large amount of data from several domains, to utilize big data it is crucial that any challenges and issues faced in adapting big data to be addressed. This paper discusses the benefits and risks for accommodating big data and open government data for public services.},
booktitle = {Proceedings of the 2018 10th International Conference on Machine Learning and Computing},
pages = {140–144},
numpages = {5},
keywords = {Open Data, Public Organization, Big data, Open Government Data},
location = {Macau, China},
series = {ICMLC 2018}
}

@inproceedings{10.1145/1376916.1376940,
author = {Fan, Wenfei},
title = {Dependencies Revisited for Improving Data Quality},
year = {2008},
isbn = {9781605581521},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376916.1376940},
doi = {10.1145/1376916.1376940},
abstract = {Dependency theory is almost as old as relational databases themselves, and has traditionally been used to improve the quality of schema, among other things. Recently there has been renewed interest in dependencies for improving the quality of data. The increasing demand for data quality technology has also motivated revisions of classical dependencies, to capture more inconsistencies in real-life data, and to match, repair and query the inconsistent data. This paper aims to provide an overview of recent advances in revising classical dependencies for improving data quality.},
booktitle = {Proceedings of the Twenty-Seventh ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
pages = {159–170},
numpages = {12},
keywords = {data quality, dependency},
location = {Vancouver, Canada},
series = {PODS '08}
}

@inproceedings{10.1145/2663204.2669985,
author = {Tosun, Cafer},
title = {Smart Multimodal Interaction through Big Data},
year = {2014},
isbn = {9781450328852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663204.2669985},
doi = {10.1145/2663204.2669985},
abstract = {Smart phones and mobile technologies have changed software usage dramatically. Ease of use and simplicity has made software accessible to a huge number of users. In addition, technological advancements in multimodal interaction are opening new frontiers in software. Users are interacting with software systems through multiple channels such as gestures and speech. Touch screens, cameras, sensors, and wearable devices are enablers of this interaction. The user expectation is that the interaction with business software also becomes as simple as the interaction with consumer software. In particular, through the usage of mobile devices, consumer and business software is coming closer together. Next generation software systems and applications will have to enable smart, seamless and contextual multimodal interaction capabilities. New tools, technologies and solutions will be required to increase the ease of use and to build the user experience of the future.},
booktitle = {Proceedings of the 16th International Conference on Multimodal Interaction},
pages = {282},
numpages = {1},
keywords = {smart city, human-computer interaction, sap hana, connected living, user-centered software, internet of things, user experience, interactive software, augmented reality},
location = {Istanbul, Turkey},
series = {ICMI '14}
}

@article{10.1145/3386687,
author = {Bertossi, Leopoldo and Geerts, Floris},
title = {Data Quality and Explainable AI},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3386687},
doi = {10.1145/3386687},
abstract = {In this work, we provide some insights and develop some ideas, with few technical details, about the role of explanations in Data Quality in the context of data-based machine learning models (ML). In this direction, there are, as expected, roles for causality, and explainable artificial intelligence. The latter area not only sheds light on the models, but also on the data that support model construction. There is also room for defining, identifying, and explaining errors in data, in particular, in ML, and also for suggesting repair actions. More generally, explanations can be used as a basis for defining dirty data in the context of ML, and measuring or quantifying them. We think dirtiness as relative to the ML task at hand, e.g., classification.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {11},
numpages = {9},
keywords = {Machine learning, bias, causes, fairness}
}

@article{10.1145/2688072,
author = {Nair, Ravi},
title = {Big Data Needs Approximate Computing: Technical Perspective},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/2688072},
doi = {10.1145/2688072},
journal = {Commun. ACM},
month = {dec},
pages = {104},
numpages = {1}
}

@article{10.1007/s00778-018-0515-8,
author = {Liu, Yuchen and Liu, Hai and Xiao, Dongqing and Eltabakh, Mohamed Y.},
title = {Adaptive Correlation Exploitation in Big Data Query Optimization},
year = {2018},
issue_date = {December  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-018-0515-8},
doi = {10.1007/s00778-018-0515-8},
abstract = {Correlations among the data attributes are abundant and inherent in most application domains. These correlations, if managed in systematic and efficient ways, would enable various optimization opportunities. Unfortunately, the state-of-art techniques are all heavily tailored toward optimizing factors intrinsic to relational databases, e.g., predicate selectivity, random I/O accesses, and secondary indexes, which are mostly not applicable to the modern big data infrastructures, e.g., Hadoop and Spark. In this paper, we propose the EXORD$$^+$$+ system for exploiting the data's correlations in big data query optimization. EXORD$$^+$$+ supports two types of correlations; hard (which does not allow for exceptions) and soft (which allows for exceptions). We introduce a three-phase approach for managing soft correlations including: (1) validating and judging the worthiness of soft correlations, (2) selecting and preparing the soft correlations for deployment, and (3) exploiting the correlations in query optimization. EXORD$$^+$$+ introduces a novel cost-benefit model for adaptively selecting the most beneficial soft correlations given a query workload. We show the complexity of this problem (NP-Hard) and propose a heuristic to efficiently solve it in a polynomial time. Moreover, we present incremental maintenance algorithms for efficiently updating the system's state under data appends and workload changes. EXORD$$^+$$+ prototype is implemented as an extension to the Hive engine on top of Hadoop. The experimental evaluation shows the potential of EXORD$$^+$$+ in achieving more than 10x speedup while introducing minimal storage overheads.},
journal = {The VLDB Journal},
month = {dec},
pages = {873–898},
numpages = {26},
keywords = {Big data, Data correlations, Soft and hard correlations, Incremental maintenance, Query optimization}
}

@inproceedings{10.1145/2756406.2756924,
author = {Xie, Zhiwu and Chen, Yinlin and Speer, Julie and Walters, Tyler and Tarazaga, Pablo A. and Kasarda, Mary},
title = {Towards Use And Reuse Driven Big Data Management},
year = {2015},
isbn = {9781450335942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2756406.2756924},
doi = {10.1145/2756406.2756924},
abstract = {We propose a use and reuse driven big data management approach that fuses the data repository and data processing capabilities in a co-located, public cloud. It answers to the urgent data management needs from the growing number of researchers who don't fit in the big science/small science dichotomy. This approach will allow researchers to more easily use, manage, and collaborate around big data sets, as well as give librarians the opportunity to work alongside the researchers to preserve and curate data while it is still fresh and being actively used. This also provides the technological foundation to foster a sharing culture more aligned with the open source software development paradigm than the lone-wolf, gift-exchanging small science sharing or the top-down, highly structured big science sharing. To materialize this vision, we provide a system architecture consisting of a scalable digital repository system coupled with the co-located cloud storage and cloud computing, as well as a job scheduler and a deployment management system. Motivated by Virginia Tech's Goodwin Hall instrumentation project, we implemented and evaluated a prototype. The results show not only sufficient capacities for this particular case, but also near perfect linear storage and data processing scalabilities under moderately high workload.},
booktitle = {Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {65–74},
numpages = {10},
keywords = {cloud computing, sensor data, digital library, big data, smart infrastructure, digital repository},
location = {Knoxville, Tennessee, USA},
series = {JCDL '15}
}

@inproceedings{10.1145/2757218.2757221,
author = {Eckroth, Joshua},
title = {Towards a Cross-Disciplinary Pedagogy for Big Data},
year = {2015},
isbn = {9781450335973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757218.2757221},
doi = {10.1145/2757218.2757221},
abstract = {Many disciplines are confronting the challenge of "big data," i.e., databases or data streams that are so massive or deliver data at such high velocity that a single commodity machine is unable to store or process the data. Mining and analyzing big data requires specialized algorithms and methodologies due to the fundamentally distributed and parallel natures of the workloads. To our knowledge, existing pedagogies in most disciplines do not prepare students to work with big data. We aim to develop a cross-disciplinary pedagogy that prepares all students to tackle data mining and analysis challenges of the future.},
booktitle = {Proceedings of The 2015 NSF Workshop on Curricular Development for Computing in Context},
articleno = {7},
numpages = {1},
location = {DeLand, FL, USA},
series = {CIC '15}
}

@inproceedings{10.1145/3206157.3206166,
author = {Khan, Nawsher and Alsaqer, Mohammed and Shah, Habib and Badsha, Gran and Abbasi, Aftab Ahmad and Salehian, Soulmaz},
title = {The 10 Vs, Issues and Challenges of Big Data},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206166},
doi = {10.1145/3206157.3206166},
abstract = {In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {52–56},
numpages = {5},
keywords = {Data Management, Big Data},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/2382416.2382418,
author = {Weber, Samuel},
title = {Big Data Privacy and Security Challenges},
year = {2012},
isbn = {9781450316613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382416.2382418},
doi = {10.1145/2382416.2382418},
abstract = {The ability to collect and organize large data sets has proven to be transformational: instead of just a linear improvement of older techniques, the ability to effectively process huge amounts of information creates radically new abilities and opportunities. Unfortunately, these abilities come with new security, privacy and legal risks: privacy that was previously protected only by the fact that data gathering was difficult can now be trivially violated. This talk will describe research challenges that arise in this field.},
booktitle = {Proceedings of the 2012 ACM Workshop on Building Analysis Datasets and Gathering Experience Returns for Security},
pages = {1–2},
numpages = {2},
keywords = {security, privacy, legal aspects},
location = {Raleigh, North Carolina, USA},
series = {BADGERS '12}
}

@article{10.1145/3299887.3299892,
author = {Hirzel, Martin and Baudart, Guillaume and Bonifati, Angela and Della Valle, Emanuele and Sakr, Sherif and Akrivi Vlachou, Akrivi},
title = {Stream Processing Languages in the Big Data Era},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3299887.3299892},
doi = {10.1145/3299887.3299892},
abstract = {This paper is a survey of recent stream processing languages, which are programming languages for writing applications that analyze data streams. Data streams, or continuous data flows, have been around for decades. But with the advent of the big-data era, the size of data streams has increased dramatically. Analyzing big data streams yields immense advantages across all sectors of our society. To analyze streams, one needs to write a stream processing application. This paper showcases several languages designed for this purpose, articulates underlying principles, and outlines open challenges.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {29–40},
numpages = {12}
}

@inproceedings{10.1145/2642769.2642798,
author = {Gray, Ian and Chan, Yu and Audsley, Neil C. and Wellings, Andy},
title = {Architecture-Awareness for Real-Time Big Data Systems},
year = {2014},
isbn = {9781450328753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642769.2642798},
doi = {10.1145/2642769.2642798},
abstract = {Existing programming models for distributed and cloud-based systems tend to abstract away from the architectures of individual target nodes, concentrating instead on higher-level issues of algorithm representation (MapReduce etc.). However, as programmers begin to tackle the issue of Big Data, increasing data volumes are forcing developers to reconsider this approach and to optimise their software heavily. JUNIPER is an EU-funded project which assists Big Data developers to create architecture-aware software in a way that is suitable for the target domain, and provides higher performance, portability, and real-time guarantees.},
booktitle = {Proceedings of the 21st European MPI Users' Group Meeting},
pages = {151–156},
numpages = {6},
location = {Kyoto, Japan},
series = {EuroMPI/ASIA '14}
}

@inproceedings{10.1145/3424978.3425010,
author = {Man, Rui and Zhou, Guomin and Fan, Jingchao},
title = {Research on Scientific Data Management in Big Data Era},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425010},
doi = {10.1145/3424978.3425010},
abstract = {Scientific data is an important strategic resource in the era of big data. Efficient management and wide circulation are the key ways to enhance the value of scientific data resources. With the transformation of the industrial society into the information society, the importance of scientific data management is also increasing all over the world, which continuously promotes the maturity of scientific data management and sharing. In this article, through comprehensive research of scientific data management ideas, policies, practices and results, the analysis summarizes the advanced experience of international scientific data management, for the similar problems and challenges existing in the research in China, puts forward the future a period of time the direction and suggestions on the development of scientific data management: 1. the specification of various kinds of degree of the standardization of scientific data resources; 2. To strengthen data mining capacity; 3. To strengthen the cultivation of talents in data science; 4. To strengthen international cooperation and enhance core competitiveness in the big data era.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {32},
numpages = {6},
keywords = {Scientific data, Scientific data management, Big data, Opening and sharing of data resource},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.1145/2487575.2491135,
author = {Neumann, Chris},
title = {Using "Big Data" to Solve "Small Data" Problems},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2491135},
doi = {10.1145/2487575.2491135},
abstract = {The brief history of knowledge discovery is filled with products that promised to bring "BI to the masses". But how do you build a product that truly bridges the gap between the conceptual simplicity of "questions and answers" and the structure needed to query traditional data stores?In this talk, Chris Neumann will discuss how DataHero applied the principles of user-centric design and development over a year and a half to create a product with which more than 95% of new users can get answers on their first attempt. He'll demonstrate the process DataHero uses to determine the best combination of algorithms and user interface concepts needed to create intuitive solutions to potentially complex interactions, including: Determining the structure of files uploaded by usersAccurately identifying data types within filesPresenting users with an optimal visualization for any combination of dataHelping users to ask questions of data when they don't know what to do Chris will also talk about what it's like to start a "Big Data" company and how he applied lessons from his time as the first engineer at Aster Data Systems to DataHero.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1140},
numpages = {1},
keywords = {data mining, analytics, big data},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2463676.2467801,
author = {Aboulnaga, Ashraf and Babu, Shivnath},
title = {Workload Management for Big Data Analytics},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2467801},
doi = {10.1145/2463676.2467801},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {929–932},
numpages = {4},
keywords = {mapreduce, parallel database systems, workload management, analytics},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/3264560.3264571,
author = {Lwin, Thurein Kyaw and Bogdanov, A. V.},
title = {Definition and Scope of Big Data Problem},
year = {2018},
isbn = {9781450364744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264560.3264571},
doi = {10.1145/3264560.3264571},
abstract = {This article examines what is BigData, the current state of affairs, CAP Theorem, prospects for development and BigData problems. Today the volumes of information grow exponentially in order to respond faster to changes in IT field, to obtain competitive advantages, to increase production efficiency, it is necessary to obtain, process and analyze a huge amount of data. The biggest problem with the large data is the cost of processing. The equipment will have to be updated on a regular basis. So, it does not lose a minimal performance when the amount of data increases. The second problem is again related to the large amount of information that needs to be processed. BigData opens up new horizons for technology in planning production, education, healthcare and other industries. If their development continues, BigData technologies can raise information, as a factor of production, to a whole new level of quality. Information will not only be equal to labor and capital, but also it will probably become the most important resource of the modern technology.},
booktitle = {Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing},
pages = {38–41},
numpages = {4},
keywords = {big data development, Big Data, analysis data, Component, Large Data, CAP, IT, systematization, information processing},
location = {Barcelona, Spain},
series = {ICCBDC'18}
}

@inproceedings{10.1145/3134302.3134335,
author = {Petrova-Antonova, Dessislava and Georgieva, Olga and Ilieva, Sylvia},
title = {Modelling of Educational Data Following Big Data Value Chain},
year = {2017},
isbn = {9781450352345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134302.3134335},
doi = {10.1145/3134302.3134335},
abstract = {Big Data is attracting increasing amount of attention among academy, industry and citizens. It poses both opportunities and challenges for society as a whole. In order to gain value from Big Data, it needs to be processed and analysed in an appropriate way, and the results have to be presented in a visual manner as to be able to effectively support decision making. Following the current trends in Big Data, this paper aims to prove the value-creation of Big Data by proposing a new data model in the field of the primary and secondary education in Bulgaria. It follows the Big Data Value Chain concept in order to group the schools depending on the concentration of students with learning deficits and risk of premature leaving the education system. The primary purpose of the proposed Data Model is to drive decisions by turning information into intelligence.},
booktitle = {Proceedings of the 18th International Conference on Computer Systems and Technologies},
pages = {88–95},
numpages = {8},
keywords = {Data model, Big Data, Decision support, Big Data Value Chain, Education},
location = {Ruse, Bulgaria},
series = {CompSysTech'17}
}

@article{10.1145/2331042.2331060,
author = {Langford, John},
title = {Parallel Machine Learning on Big Data},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331060},
doi = {10.1145/2331042.2331060},
abstract = {On algorithms for parallel machine learning, and why they need to be more efficient.},
journal = {XRDS},
month = {sep},
pages = {60–62},
numpages = {3}
}

@inproceedings{10.1145/2912160.2912205,
author = {Ghosh, Debopriya and Chun, Soon Ae and Shafiq, Basit and Adam, Nabil R.},
title = {Big Data-Based Smart City Platform: Real-Time Crime Analysis},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912205},
doi = {10.1145/2912160.2912205},
abstract = {One of the challenges governments and communities face to achieve smart city goals is dealing with enormous amount of data available - sensors, devices, social media, Web activities and commerce, tracking devices, all generate enormous amount of data, so called Big Data. Our goal is to empower the city government and its citizens to create a safer city by enabling crime and risk analysis of unstructured crime reports, criminal history of suspects, auto-license data, location-specific data, etc. for crime fighting efforts. We present intelligent solutions for Data-based Smart City Platform in Newark, NJ. We used a Machine Learning approach to automate and help crime analysts identify the connected entities and events by collecting, integrating and analyzing diverse data sources to generate alerts and predictions for new knowledge and insights that lead to better decision making and optimized actions.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {58–66},
numpages = {9},
keywords = {smart city, machine learning, crime analysis, Local Government},
location = {Shanghai, China},
series = {dg.o '16}
}

@inproceedings{10.5555/2616606.2617095,
author = {Wang, Yu and Li, Boxun and Luo, Rong and Chen, Yiran and Xu, Ningyi and Yang, Huazhong},
title = {Energy Efficient Neural Networks for Big Data Analytics},
year = {2014},
isbn = {9783981537024},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {The world is experiencing a data revolution to discover knowledge in big data. Large scale neural networks are one of the mainstream tools of big data analytics. Processing big data with large scale neural networks includes two phases: the training phase and the operation phase. Huge computing power is required to support the training phase. And the energy efficiency (power efficiency) is one of the major considerations of the operation phase. We first explore the computing power of GPUs for big data analytics and demonstrate an efficient GPU implementation of the training phase of large scale recurrent neural networks (RNNs). We then introduce a promising ultra-high energy efficient implementation of neural networks' operation phase by taking advantage of the emerging memristor technique. Experiment results show that the proposed GPU implementation of RNNs is able to achieve 2 ~ 11\texttimes{} speed-up compared with the basic CPU implementation. And the scaled-up recurrent neural network trained with GPUs realizes an accuracy of 47% on the Microsoft Research Sentence Completion Challenge, the best result achieved by a single RNN on the same dataset. In addition, the proposed memristor-based implementation of neural networks demonstrates power efficiency of &gt; 400 GFLOPS/W and achieves energy savings of 22\texttimes{} on the HMAX model compared with its pure digital implementation counterpart.},
booktitle = {Proceedings of the Conference on Design, Automation &amp; Test in Europe},
articleno = {345},
numpages = {2},
location = {Dresden, Germany},
series = {DATE '14}
}

@inproceedings{10.1145/2792838.2799670,
author = {Sar Shalom, Oren and Berkovsky, Shlomo and Ronen, Royi and Ziklik, Elad and Amihood, Amir},
title = {Data Quality Matters in Recommender Systems},
year = {2015},
isbn = {9781450336925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2792838.2799670},
doi = {10.1145/2792838.2799670},
abstract = {Although data quality has been recognized as an important factor in the broad information systems research, it has received little attention in recommender systems. Data quality matters are typically addressed in recommenders by ad-hoc cleansing methods, which prune noisy or unreliable records from the data. However, the setting of the cleansing parameters is often done arbitrarily, without thorough consideration of the data characteristics. In this work, we turn to two central data quality problems in recommender systems: sparsity and redundancy. We devise models for setting data-dependent thresholds and sampling levels, and evaluate these using a collection of public and proprietary datasets. We observe that the models accurately predict data cleansing parameters, while having minor effect on the accuracy of the generated recommendations.},
booktitle = {Proceedings of the 9th ACM Conference on Recommender Systems},
pages = {257–260},
numpages = {4},
keywords = {data quality, redundancy, recommender systems, sparsity},
location = {Vienna, Austria},
series = {RecSys '15}
}

@article{10.1145/2932707,
author = {Fang, Ruogu and Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Iyengar, S. S.},
title = {Computational Health Informatics in the Big Data Age: A Survey},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2932707},
doi = {10.1145/2932707},
abstract = {The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {12},
numpages = {36},
keywords = {survey, 4V challenges, clinical decision support, Big data analytics, computational health informatics, data mining, machine learning}
}

@inproceedings{10.1145/3010089.3010113,
author = {Albattah, Waleed},
title = {The Role of Sampling in Big Data Analysis},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010113},
doi = {10.1145/3010089.3010113},
abstract = {Nowadays, a huge amount of data is being continuously generated in all fields of life by all systems and applications on daily basis. This amount of data is increasing at a rapid pace. The collection of this huge amount of data leads to challenges of storage, analysis, etc. Since, processing this huge amount of data is a challenge for learning algorithms of Artificial Intelligence (machine learning), therefore, in this paper, we target the Sampling of data in big data paradigm. We analyze, how and what will be the role of sampling the training data so that machine algorithms are able to rapidly and precisely build a training model from this huge amount of data. The data normally represents some model. Since, it is extremely difficult to learn a pattern or a model from huge data (precisely and quickly). Thus, this paper studies and analyzes the role of sampling in data sets. Though our data sets cover only specific areas, the training data is objectively similar in nature for machine learning algorithms. We concentrate on how sampling will play its role in specific fields of Artificial Intelligence, i.e., training and testing paradigm and in real time as well. From the experimental analysis, we observe that re-sampling the data has a very slight impact on the over-all classification performance. However, it has greater impact of the execution time as seen from experimental results. Finally, we believe the results in this paper should be viewed as indicative for future research rather than conclusive.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {28},
numpages = {5},
keywords = {sampling, big data analysis, sample, machine-learning, Big data},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/2680821.2680824,
author = {Yu, Se-young and Brownlee, Nevil and Mahanti, Aniket},
title = {Performance and Fairness Issues in Big Data Transfers},
year = {2014},
isbn = {9781450332828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2680821.2680824},
doi = {10.1145/2680821.2680824},
abstract = {We present performance and fairness analysis of two TCP- based (GridFTP and FDT) and one UDP-based (UDT) big data transfer protocols. We perform long-haul performance experiments using a 10 Gb/s national network, and conduct fairness tests in our 10 Gb/s local network. Our results show that GridFTP with jumbo frames provides fast data transfers. GridFTP is also fair in sharing bandwidth with competing background TCP flows.},
booktitle = {Proceedings of the 2014 CoNEXT on Student Workshop},
pages = {9–11},
numpages = {3},
keywords = {fairness, performance, big data transfer protocols},
location = {Sydney, Australia},
series = {CoNEXT Student Workshop '14}
}

@inproceedings{10.1145/2740908.2745843,
author = {Yuan, Nicholas Jing},
title = {Mining Social and Urban Big Data},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2745843},
doi = {10.1145/2740908.2745843},
abstract = {In recent years, with the rapid development of positioning technologies, online social networks, sensors and smart devices, large scale human behavioral data are now readily available. The growing availability of such behavioral data provides us unprecedented opportunities to gain more in depth understanding of users in both the physical world and cyber world, especially in online social networks. In this talk, I will introduce our recent research efforts in social and urban mining based on large-scale human behavioral datasets showcased by two projects: 1) LifeSpec: Modeling the spectrum of urban lifestyles based on heterogeneous online social network data. 2) L2P: Inferring demographic attributes from location check-ins.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1103},
numpages = {1},
keywords = {performance, experimentation, algorithms},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1109/CCGRID.2017.107,
author = {Haroun, Amir and Mostefaoui, Ahmed and Dessables, Fran\c{c}ois},
title = {A Big Data Architecture for Automotive Applications: PSA Group Deployment Experience},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.107},
doi = {10.1109/CCGRID.2017.107},
abstract = {Vehicles have become moving sensor platforms collecting huge volumes of data from their various embedded sensors. This data has a great value for automotive manufacturers and vehicles owners. Indeed, connected vehicles data can be used in a large broad of automotive services ranging from safety services to well-being services (e.g. fatigue detection). However, vehicle fleets send big volumes of data that traditional computing and storage approaches are not able to manage efficiently. In this paper, we present the experience of the PSA Group1 on leveraging big data in automotive context. We describe in depth the big data architecture deployed within the PSA Group and the underlaying technologies/products used in each component.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {921–928},
numpages = {8},
keywords = {Reference Architecture, Big Data, Connected Vehicles},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1109/CCGrid.2015.174,
author = {Cuzzocrea, Alfredo and Moussa, Rim and Xu, Guandong and Grasso, Giorgio Mario},
title = {Cloud-Based OLAP over Big Data: Application Scenarios and Performance Analysis},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.174},
doi = {10.1109/CCGrid.2015.174},
abstract = {Following our previous research results, in this paper we provide two authoritative application scenarios that build on top of OLAP*, a middleware for parallel processing of OLAP queries that truly realizes effective and efficiently OLAP over Big Data. We have provided two authoritative case studies, namely parallel OLAP data cube processing and virtual OLAP data cube design, for which we also propose a comprehensive performance evaluation and analysis. Derived analysis clearly confirms the benefits of our proposed framework.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {921–927},
numpages = {7},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/2756406.2756943,
author = {Kanan, Tarek and Zhang, Xuan and Magdy, Mohamed and Fox, Edward},
title = {Big Data Text Summarization for Events: A Problem Based Learning Course},
year = {2015},
isbn = {9781450335942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2756406.2756943},
doi = {10.1145/2756406.2756943},
abstract = {Problem/project Based Learning (PBL) is a highly effective student-centered teaching method, where student teams learn by solving problems. This paper describes an instance of PBL applied to digital library education. We show the design, implementation, results, and partial evaluation of a Computational Linguistics course that provides students an opportunity to engage in active learning about adding value to digital libraries with large collections of text, i.e., one aspect of "big data." Students are engaging in PBL with the semester long challenge of generating good English summaries of an event, given a large collection from our webpage archives. Six teams, each working with a different type of event, and applying three different summarization methods, learned how to generate good summaries; these have fair precision relative to the Wikipedia page that describes their event.},
booktitle = {Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {87–90},
numpages = {4},
keywords = {digital libraries, text summarization, big data, computational linguistics, problem based learning},
location = {Knoxville, Tennessee, USA},
series = {JCDL '15}
}

@inproceedings{10.1145/2627770.2627773,
author = {Chatziantoniou, Damianos and Tselai, Florents},
title = {Introducing Data Connectivity in a Big Data Web},
year = {2014},
isbn = {9781450329972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627770.2627773},
doi = {10.1145/2627770.2627773},
abstract = {Until recently, when relational systems was the main data management option and SQL the de facto language for querying/analyzing data, ODBC was an excellent API for applications to interact with the data provider. Standardization of data retrieval has helped innovation and productivity, allowing application developers to focus on the core of their ideas. However, the big data era added variety to all aspects of data facilitation: variety in data management options, variety in data formats, variety in querying/analyzing tasks. In this chaotic situation, standardizing data connectivity is more important than ever. What should be the replacement of ODBC? In this paper, we propose ODMC (Open Data Management Connectivity), a client-server protocol between data management entities (DMEs). A DME is anything that manages/manipulates data. In that respect, spreadsheets, java programs, Hadoop, RDBMs, stream engines, NoSQL, etc., all act as DMEs. In addition, there is no distinction between applications and data management servers, as in ODBC. A DME can be a data consumer in an ODMC instance and a data producer in another. This composability principle allows for the definition of analysis workflows. We present a preliminary implementation of ODMC for python-based DMEs. We argue that ODMC is simple, intuitive, scalable and suitable for both persistent and stream data.},
booktitle = {Proceedings of Workshop on Data Analytics in the Cloud},
pages = {1–4},
numpages = {4},
keywords = {big data integration, big data web, ODMC, big data interoperability, open data management connectivity, data connectivity},
location = {Snowbird, UT, USA},
series = {DanaC'14}
}

@article{10.1145/1240616.1240623,
author = {Even, Adir and Shankaranarayanan, G.},
title = {Utility-Driven Assessment of Data Quality},
year = {2007},
issue_date = {May 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {0095-0033},
url = {https://doi.org/10.1145/1240616.1240623},
doi = {10.1145/1240616.1240623},
abstract = {Data consumers assess quality within specific business contexts or decision tasks. The same data resource may have an acceptable level of quality for some contexts but this quality may be unacceptable for other contexts. However, existing data quality metrics are mostly derived impartially, disconnected from the specific contextual characteristics. This study argues for the need to revise data quality metrics and measurement techniques to incorporate and better reflect contextual assessment. It contributes to that end by developing new metrics for assessing data quality along commonly used dimensions - completeness, validity, accuracy, and currency. The metrics are driven by data utility, a conceptual measure of the business value that is associated with the data within a specific usage context. The suggested data quality measurement framework uses utility as a scaling factor for calculating quality measurements at different levels of data hierarchy. Examples are used to demonstrate the use of utility-driven assessment in real-world data management scenarios and the broader implications for data management are discussed},
journal = {SIGMIS Database},
month = {may},
pages = {75–93},
numpages = {19},
keywords = {information products, data quality, metadata, database, data management, utility, decision making, information value}
}

@inproceedings{10.1145/3379247.3379298,
author = {Zhai, Chenggong and Liu, Nan and Li, Jianxiang},
title = {Application Analysis of Big Data Technology in Feeding Industry},
year = {2020},
isbn = {9781450376730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379247.3379298},
doi = {10.1145/3379247.3379298},
abstract = {In recent years, our army attaches great importance to the application of big data technology in the field of military logistics. With the concern of the head of the CMC, relevant departments actively deploy the application of big data technology. At present, big data technology is only applied in some units of the military feeding Industry, which is in the initial stage and has a huge development space. This paper mainly introduces the application status of big data technology, the application ideas and related supporting measures in the feeding Industry.},
booktitle = {Proceedings of 2020 the 6th International Conference on Computing and Data Engineering},
pages = {142–146},
numpages = {5},
keywords = {Business Process Reengineering, big data, Feeding Industry},
location = {Sanya, China},
series = {ICCDE 2020}
}

@inproceedings{10.1145/3415958.3433082,
author = {Dessalk, Yared Dejene and Nikolov, Nikolay and Matskin, Mihhail and Soylu, Ahmet and Roman, Dumitru},
title = {Scalable Execution of Big Data Workflows Using Software Containers},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433082},
doi = {10.1145/3415958.3433082},
abstract = {Big Data processing involves handling large and complex data sets, incorporating different tools and frameworks as well as other processes that help organisations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, require taking advantage of the elasticity of cloud infrastructures for scalability. In this paper, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies and message-oriented middleware (MOM) to enable highly scalable workflow execution. The approach is demonstrated in a use case together with a set of experiments that demonstrate the practical applicability of the proposed approach for the scalable execution of Big Data workflows. Furthermore, we present a scalability comparison of our proposed approach with that of Argo Workflows - one of the most prominent tools in the area of Big Data workflows.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {76–83},
numpages = {8},
keywords = {Domain-specific languages, Software containers, Big Data workflows},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/2769458.2769484,
author = {Theodoropoulos, Georgios},
title = {Simulation in the Era of Big Data: Trends and Challenges},
year = {2015},
isbn = {9781450335836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2769458.2769484},
doi = {10.1145/2769458.2769484},
abstract = {The emergence of extreme scale computing systems and the data explosion have presented an unprecedented opportunity for the analysis of systems at a rapidly increasing scale, complexity and granularity. This paradigm shift calls for an intermingling of 'what-if' and data analytics approaches, however the worlds of Simulation and Big Data have so far been largely separate. The talk will focus on the interplay between simulation, data and emerging computational platforms, identifying gaps and opportunities and discussing some concrete examples of interacting scalable data infrastructures and agent-based simulations.},
booktitle = {Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {1},
numpages = {1},
keywords = {agent-based modelling, simulation, big data, exascale},
location = {London, United Kingdom},
series = {SIGSIM PADS '15}
}

@inproceedings{10.1145/2513190.2517828,
author = {Cuzzocrea, Alfredo and Bellatreche, Ladjel and Song, Il-Yeol},
title = {Data Warehousing and OLAP over Big Data: Current Challenges and Future Research Directions},
year = {2013},
isbn = {9781450324120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513190.2517828},
doi = {10.1145/2513190.2517828},
abstract = {In this paper, we highlight open problems and actual research trends in the field of Data Warehousing and OLAP over Big Data, an emerging term in Data Warehousing and OLAP research. We also derive several novel research directions arising in this field, and put emphasis on possible contributions to be achieved by future research efforts.},
booktitle = {Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP},
pages = {67–70},
numpages = {4},
keywords = {olap, big data, big multidimensional data, data warehousing},
location = {San Francisco, California, USA},
series = {DOLAP '13}
}

@inproceedings{10.1145/3299819.3299841,
author = {bt Yusof Ali, Hazirah Bee and bt Abdullah, Lili Marziana and Kartiwi, Mira and Nordin, Azlin},
title = {Risk Assessment for Big Data in Cloud: Security, Privacy and Trust},
year = {2018},
isbn = {9781450366236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299819.3299841},
doi = {10.1145/3299819.3299841},
abstract = {The alarming rate of big data usage in the cloud makes data exposed easily. Cloud which consists of many servers linked to each other is used for data storage. Having owned by third parties, the security of the cloud needs to be looked at. Risks of storing data in cloud need to be checked further on the severity level. There should be a way to access the risks. Thus, the objective of this paper is to use SLR so that we can have extensive background of literatures on risk assessment for big data in cloud computing environment from the perspective of security, privacy and trust.},
booktitle = {Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference},
pages = {63–67},
numpages = {5},
keywords = {Risk Assessment, Cloud, Security, Privacy, Big Data, Trust},
location = {Tokyo, Japan},
series = {AICCC '18}
}

@article{10.1145/3178315.3178323,
author = {Arruda, Darlan},
title = {Requirements Engineering in the Context of Big Data Applications},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3178315.3178323},
doi = {10.1145/3178315.3178323},
abstract = {Requirements Engineering (RE) plays an essential role in the software engineering process, being considered as one of the most critical phases of the software development life-cycle. As we might expect, then, the Requirements Engineering would play a similar role in the context of Big Data applications. However, practicing Requirements Engineering is a challenging and complex task. It involves (i) stakeholders with diverse backgrounds and levels of knowledge, (ii) different application domains, (iii) it is expensive and error-prone, (iii) it is important to be aligned with business goals, to name a few. Because it involves such complex activities, a lot has to be understood in order to properly address Requirements Engineering. Especially, when the technology domain (e.g., Big Data) is not yet well explored. In this context, this paper describes a research plan on Requirements Engineering involving the development of Big Data applications. The high-level goal is to investigate: (i) On the technical front, the Requirements Engineering activities with respect to the analysis and specification of Big Data requirements and, (ii) on the management side, the relationship between RE and Business Goals in the development of Big Data Software applications.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {1–6},
numpages = {6},
keywords = {big data requirements engineering, big data applications, empirical studies, empirical software engineering., business goals}
}

@inproceedings{10.1145/2660168.2660177,
author = {Rose, Stephen and Tuppen, Sandra},
title = {Prospects for a Big Data History of Music},
year = {2014},
isbn = {9781450330022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660168.2660177},
doi = {10.1145/2660168.2660177},
abstract = {This position paper sets out the possibility of a musicology based on the analysis of musical-bibliographical metadata as Big Data. It outlines the work underway, as part of the AHRC-funded project A Big Data History of Music, to align seven major datasets of musical-bibliographical metadata. After discussing some of the technical challenges of data alignment, it suggests how analysis and visualization of this data might transform musicological understandings of cultural transmission and canon formation.},
booktitle = {Proceedings of the 1st International Workshop on Digital Libraries for Musicology},
pages = {1–3},
numpages = {3},
keywords = {canon, bibliography, music publishing, metadata, Musicology},
location = {London, United Kingdom},
series = {DLfM '14}
}

@inproceedings{10.1145/2837060.2837089,
author = {Kim, Seokyeon and Jeong, Seongmin and An, Sung Uk and Yoo, Jae Seok and Han, Sang Min and Yeon, Hanbyul and Yoo, Sangbong and Jang, Yun},
title = {Big Data Visual Analytics System for Disease Pattern Analysis},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837089},
doi = {10.1145/2837060.2837089},
abstract = {As the world trend is elapsing toward the big data world, researchers have gradually become interested in data analysis and data visualization. We propose a visual analytics system for analyzing the big data for disease pattern analysis. The system is designed for not only experts but also novice users. We have focused on designing the system to support a short learning curve and scalability. The system is implemented on the web and we utilize Spring Framework for distributed processing with consideration to scalability. Furthermore, we have collected epidemic disease data from a public data repository that the government opens to the public and we analyze the data to reveal the causality of disease outbreak through our system.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {175–179},
numpages = {5},
keywords = {Data Visualization, Visual Analytics System, Big Data},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/3387168.3387220,
author = {Yu, Song},
title = {Integrated Retrieval System Based on Medical Big Data},
year = {2020},
isbn = {9781450376259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387168.3387220},
doi = {10.1145/3387168.3387220},
abstract = {This paper presented an integrated retrieval system based on medical big data, aiming at the characteristics of medical data, such as diversity, large quantity and complex structure. The data center is built with data extraction module and data storage module in the system. The index module is built based on Solr and the data is presented in a webpage. The system keeps proper loose coupling among modules with high availability and extension. In a hospital application environment, this system realizes the function of real-time retrieval, and provides effective support for clinical decision.},
booktitle = {Proceedings of the 3rd International Conference on Vision, Image and Signal Processing},
articleno = {59},
numpages = {5},
keywords = {Medical Big Data, Data storage, Solr},
location = {Vancouver, BC, Canada},
series = {ICVISP 2019}
}

@inproceedings{10.1145/2628194.2628251,
author = {Liu, Xiufeng and Iftikhar, Nadeem and Xie, Xike},
title = {Survey of Real-Time Processing Systems for Big Data},
year = {2014},
isbn = {9781450326278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628194.2628251},
doi = {10.1145/2628194.2628251},
abstract = {In recent years, real-time processing and analytics systems for big data--in the context of Business Intelligence (BI)--have received a growing attention. The traditional BI platforms that perform regular updates on daily, weekly or monthly basis are no longer adequate to satisfy the fast-changing business environments. However, due to the nature of big data, it has become a challenge to achieve the real-time capability using the traditional technologies. The recent distributed computing technology, MapReduce, provides off-the-shelf high scalability that can significantly shorten the processing time for big data; Its open-source implementation such as Hadoop has become the de-facto standard for processing big data, however, Hadoop has the limitation of supporting real-time updates. The improvements in Hadoop for the real-time capability, and the other alternative real-time frameworks have been emerging in recent years. This paper presents a survey of the open source technologies that support big data processing in a real-time/near real-time fashion, including their system architectures and platforms.},
booktitle = {Proceedings of the 18th International Database Engineering &amp; Applications Symposium},
pages = {356–361},
numpages = {6},
keywords = {systems, big data, survey, architectures, real-time},
location = {Porto, Portugal},
series = {IDEAS '14}
}

@inproceedings{10.1145/2609876.2609877,
author = {Vogel, Kathleen M.},
title = {Big Data and the Invisible, Social Dimensions of Science},
year = {2014},
isbn = {9781450329385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609876.2609877},
doi = {10.1145/2609876.2609877},
abstract = {In this paper, I describe the challenges of using Big Data approaches in assessing security threats from the life sciences.},
booktitle = {Proceedings of the 2014 Workshop on Human Centered Big Data Research},
pages = {1–3},
numpages = {3},
keywords = {bioterrorism, Big Data, organizational context, life sciences, tacit knowledge, socio-technical},
location = {Raleigh, NC, USA},
series = {HCBDR '14}
}

@inproceedings{10.1145/3265689.3265721,
author = {Pan, Zhiwen and Ji, Wen and Chen, Yiqiang and Dai, Lianjun and Zhang, Jun},
title = {Big Data Management and Analytics for Disability Datasets},
year = {2018},
isbn = {9781450365871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265689.3265721},
doi = {10.1145/3265689.3265721},
abstract = {The disability datasets is the datasets which contains the information of disabled populations. By analyzing these datasets, professionals who work with disabled populations can have a better understanding of how to make working plans and policies, so that they support the populations in a better way. In this paper, we proposed a big data management and mining approach for disability datasets. The contributions of this paper are follows: 1) our proposed approach can improve the quality of disability data by estimating miss attribute values and detecting anomaly and low-quality data instances. 2) Our proposed approach can explore useful patterns which reflect the correlation, association and interactional between the disability data attributes. Experiments are conducted at the end to evaluate the performance of our approach.},
booktitle = {Proceedings of the 3rd International Conference on Crowd Science and Engineering},
articleno = {32},
numpages = {6},
keywords = {Decision support systems, Big data analytics, Disability Dataset, Data Management, Disability Population, Data Mining},
location = {Singapore, Singapore},
series = {ICCSE'18}
}

@inproceedings{10.1145/3205651.3205701,
author = {Dagdia, Zaineb Chelly},
title = {A Distributed Dendritic Cell Algorithm for Big Data},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3205701},
doi = {10.1145/3205651.3205701},
abstract = {In this work, we focus on the Dendritic Cell Algorithm (DCA), a bio-inspired classifier, and its limitation when coping with very large datasets. To overcome this limitation, we propose a novel distributed DCA version for data classification based on the MapReduce framework to distribute the functioning of this algorithm through a cluster of computing elements. Our experimental results show that our proposed distributed solution is suitable to enhance the performance of the DCA enabling the algorithm to be applied over big data classification problems.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {103–104},
numpages = {2},
keywords = {scalability, big data, distributed processing, classification, dendritic cell algorithm},
location = {Kyoto, Japan},
series = {GECCO '18}
}

