@inproceedings{10.1145/3352700.3352709,title = {Big Data Network Flow Processing Using Apache Spark}, author = {Je\u0159\u00e1bek Kamil , Ry\u0161av\u00fd Ond\u0159ej },year = {2019}, isbn = {9781450376365}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3352700.3352709}, doi = {10.1145/3352700.3352709}, abstract = {The increasing amount of traffic flows captured as a part of network monitoring activities makes the analysis more complicated. One of the goals for network traffic analysis is to identify malicious communication. In the paper, we present a new system for big data network flow classification and clustering. The proposed system is based on the popular big data engines such as Apache Spark and Apache Ignite. The conducted experiments demonstrate the feasibility of the proposed approach and show the possible scalability.}, location = {Bucharest, Romania}, series = {ECBS '19}, pages = {1\u20139}, numpages = {9}, keywords = {Apache Ignite, Network flows, Apache Spark, Big Data, Cassandra}}
@inproceedings{10.1145/2481244.2481246,title = {Mining big data: current status, and forecast to the future}, author = {Fan Wei , Bifet Albert },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2481244.2481246}, doi = {10.1145/2481244.2481246}, abstract = {Big Data is a new term used to identify datasets that we can not manage with current methodologies or data mining software tools due to their large size and complexity. Big Data mining is the capability of extracting useful information from these large datasets or streams of data. New mining techniques are necessary due to the volume, variability, and velocity, of such data. The Big Data challenge is becoming one of the most exciting opportunities for the years to come. We present in this issue, a broad overview of the topic, its current status, controversy, and a forecast to the future. We introduce four articles, written by influential scientists in the field, covering the most interesting and state-of-the-art topics on Big Data mining.}, pages = {1\u20135}, numpages = {5}}
@inproceedings{10.1145/2588555.2610512,title = {Opportunistic physical design for big data analytics}, author = {LeFevre Jeff , Sankaranarayanan Jagan , Hacigumus Hakan , Tatemura Junichi , Polyzotis Neoklis , Carey Michael J. },year = {2014}, isbn = {9781450323765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2588555.2610512}, doi = {10.1145/2588555.2610512}, abstract = {Big data analytical systems, such as MapReduce, perform aggressive materialization of intermediate job results in order to support fault tolerance. When jobs correspond to exploratory queries submitted by data analysts, these materializations yield a large set of materialized views that we propose to treat as an opportunistic physical design. We present a semantic model for UDFs that enables effective reuse of views containing UDFs along with a rewrite algorithm that provably finds the minimum-cost rewrite under certain assumptions. An experimental study on real-world datasets using our prototype based on Hive shows that our approach can result in dramatic performance improvements.}, location = {Snowbird, Utah, USA}, series = {SIGMOD '14}, pages = {851\u2013862}, numpages = {12}, keywords = {exploratory analysis, opportunistic views, opportunistic physical design, UDFs, query processing, big data, query rewriting}}
@inproceedings{10.1145/3404512,title = {Proceedings of the 2020 2nd International Conference on Big Data Engineering},year = {2020}, isbn = {9781450377225}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {At this conference, I served as Conference Program Chair. Prof. Kai Hwang from Chinese University of Hong Kong, China, Prof. Yao Liang from Purdue University School of Science, Indiana University Purdue University, USA and I shared our speeches as keynote speakers. Dr Wei Li from Central Queensland University, Australia, Dr Ka-Chun Wong from City University of Hong Kong, Hong Kong and Dr. Gabriella Casalino from University of Bari, Italy served as the Invited Speakers. They also shared their research during the conference.}, location = {Shanghai, China}}
@inproceedings{10.1145/2331042.2331062,title = {Profile Jeff Dean Big data at Google}, author = {Yang Edward Z. , Simmons Robert J. },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2331042.2331062}, doi = {10.1145/2331042.2331062}, pages = {69}, numpages = {1}}
@inproceedings{10.1145/2903150.2908078,title = {Heterogeneous chip multiprocessor architectures for big data applications}, author = {Homayoun Houman },year = {2016}, isbn = {9781450341288}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2903150.2908078}, doi = {10.1145/2903150.2908078}, abstract = {Emerging big data analytics applications require a significant amount of server computational power. The costs of building and running a computing server to process big data and the capacity to which we can scale it are driven in large part by those computational resources. However, big data applications share many characteristics that are fundamentally different from traditional desktop, parallel, and scale-out applications. Big data analytics applications rely heavily on specific deep machine learning and data mining algorithms, and are running a complex and deep software stack with various components (e.g. Hadoop, Spark, MPI, Hbase, Impala, MySQL, Hive, Shark, Apache, and MangoDB) that are bound together with a runtime software system and interact significantly with I/O and OS, exhibiting high computational intensity, memory intensity, I/O intensity and control intensity. Current server designs, based on commodity homogeneous processors, will not be the most efficient in terms of performance/watt for this emerging class of applications. In other domains, heterogeneous architectures have emerged as a promising solution to enhance energy-efficiency by allowing each application to run on a core that matches resource needs more closely than a one-size-fits-all core. A heterogeneous architecture integrates cores with various micro-architectures and accelerators to provide more opportunity for efficient workload mapping. In this work, through methodical investigation of power and performance measurements, and comprehensive system level characterization, we demonstrate that a heterogeneous architecture combining high performance big and low power little cores is required for efficient big data analytics applications processing, and in particular in the presence of accelerators and near real-time performance constraints.}, location = {Como, Italy}, series = {CF '16}, pages = {400\u2013405}, numpages = {6}, keywords = {application characterization, power, accelerator, performance, big data, heterogeneous architectures}}
@inproceedings{10.5555/2819289,title = {Proceedings of the First International Workshop on BIG Data Software Engineering},year = {2015}, publisher = {IEEE Press}, abstract = {Big Data is about extracting valuable information from data in order to use it in intelligent ways such as to revolutionize decision-making in businesses, science and society. The continuous and tremendous growth of data volume and velocity combined with easier access to data and the availability of powerful IT systems have led to intensified activities around Big Data.BIGDSE 2015 aims to explore opportunities that Big Data technology offers to software engineering, both in research and practice. In addition, BIGDSE will look at the software engineering challenges imposed by building Big Data software systems. The workshop brings together researchers and practitioners working in the areas of Big Data, software engineering and software analytics to discuss research challenges, recent developments, novel applications and scenarios, as well as methods, techniques, experiences, and tools to leverage and exploit the opportunities offered by Big Data.}, location = {Florence, Italy}}
@inproceedings{10.1145/1839379.1839397,title = {Certifying data quality conformance}, author = {Helfert Markus , Hossain Fakir },year = {2010}, isbn = {9781450302432}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1839379.1839397}, doi = {10.1145/1839379.1839397}, abstract = {Many researchers and practitioners have been attracted to improve data quality due to its monumental importance as a key success factor. Mathematical and statistical models have been deployed to information systems to introduce constrain and transaction based mechanisms to prevent data quality related problems. Entire management of the process and roles involved in data generation has also been scrutinized. Vast amount of knowledge base has been progressed in this area; however, most of the approaches are limited from practical perspective. System development process incorporating quality modelling is rarely integrated. Quality related meta data is absent from most information system. Neither process mapping nor data modelling provides sufficient provision to measure quality or certification of data in the information systems. Furthermore, ongoing monitoring of data for quality conformance through a separate process is expensive and time consuming. Recognising this limitation and aiming to provide a practical-orient comprehensive approach, we propose a process centric quality focused system design incorporating data product quality, conformance monitoring and certification. In this paper we focus on the self certification of data quality based on our earlier work on the process centric framework for ongoing data quality monitoring.}, location = {Sofia, Bulgaria}, series = {CompSysTech '10}, pages = {95\u2013100}, numpages = {6}, keywords = {quality monitoring, information quality, ongoing data product monitoring, data quality certification, information manufacturing}}
@inproceedings{10.1145/1526993.1526999,title = {Data quality in web archiving}, author = {Spaniol Marc , Denev Dimitar , Mazeika Arturas , Weikum Gerhard , Senellart Pierre },year = {2009}, isbn = {9781605584881}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1526993.1526999}, doi = {10.1145/1526993.1526999}, abstract = {Web archives preserve the history of Web sites and have high long-term value for media and business analysts. Such archives are maintained by periodically re-crawling entire Web sites of interest. From an archivist's point of view, the ideal case to ensure highest possible data quality of the archive would be to \"freeze\" the complete contents of an entire Web site during the time span of crawling and capturing the site. Of course, this is practically infeasible. To comply with the politeness specification of a Web site, the crawler needs to pause between subsequent http requests in order to avoid unduly high load on the site's http server. As a consequence, capturing a large Web site may span hours or even days, which increases the risk that contents collected so far are incoherent with the parts that are still to be crawled. This paper introduces a model for identifying coherent sections of an archive and, thus, measuring the data quality in Web archiving. Additionally, we present a crawling strategy that aims to ensure archive coherence by minimizing the diffusion of Web site captures. Preliminary experiments demonstrate the usefulness of the model and the effectiveness of the strategy.}, location = {Madrid, Spain}, series = {WICOW '09}, pages = {19\u201326}, numpages = {8}, keywords = {web archiving, data quality, temporal coherence}}
@inproceedings{10.1145/3409501.3409523,title = {Strengthen the establishment of enterprise logistics management system by means of big data}, author = {Bo Yu , Yongke Chen },year = {2020}, isbn = {9781450375603}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3409501.3409523}, doi = {10.1145/3409501.3409523}, abstract = {Big data plays an irreplaceable role in China's logistics industry. Enterprises need to reconstruct their logistics management system with the help of big data technology. This paper expounds the application advantages of big data technology in enterprise logistics management, puts forward the contradictions in enterprise logistics management under the background of big data, and explores the application of big data technology in enterprise logistics management from four aspects of safety management system, inventory management mode, logistics distribution management and talent team construction.}, location = {Qingdao, China}, series = {HPCCT &amp; BDAI '20}, pages = {209\u2013211}, numpages = {3}, keywords = {big data, information, logistics}}
@inproceedings{10.1145/3274250.3275113,title = {Big Data Service Delivery Network}, author = {Xinhua E. , Zhu Binjie },year = {2018}, isbn = {9781450365383}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3274250.3275113}, doi = {10.1145/3274250.3275113}, abstract = {Big data service is a promising technology in Internet. Quality of service of big data services is a very important indicator. A service delivery network was presented in this paper to reduce service delays. The web services were distribution to the edge of the network to making it closer to users, so the network delay is small. A services distribution method with QoS guarantee was presented in this paper. Friendly degrees were measured in this method between the servers. According to the friendly degree determine the coverage areas of a copy. It takes up less resource under the premise of QoS guaranteeing.}, location = {Porto, Portugal}, series = {ICoMS '18}, pages = {89\u201391}, numpages = {3}, keywords = {Web service, Distribution method, SDN}}
@inproceedings{10.1145/3338840.3355683,title = {Traffic big data assisted broadcast in vehicular networks}, author = {Guleng Siri , Wu Celimuge , Yoshinaga Tsutomu , Ji Yusheng },year = {2019}, isbn = {9781450368438}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3338840.3355683}, doi = {10.1145/3338840.3355683}, abstract = {Multi-hop broadcast communications are required for vehicular Internet-of-Things applications including intelligent transport systems, autonomous driving, and collision avoidance systems. However, conducing efficient broadcasting in vehicular ad hoc networks (VANETs) is particularly challenging due to the vehicle mobility and various vehicle densities. In this paper, we propose a traffic big data assisted broadcast scheme in VANETs. The proposed scheme uses vehicle traffic big data to estimate vehicle density, and then uses the prediction information to enhance the procedure of multi-hop broadcasting. By enhancing a receiver-oriented broadcast approach with vehicle density prediction, the proposed scheme can provide a high dissemination ratio with low broadcast redundancy. We use real traffic big data to conduct prediction and then generate realistic vehicular network simulations to show the performance of the proposed scheme.}, location = {Chongqing, China}, series = {RACS '19}, pages = {236\u2013240}, numpages = {5}, keywords = {broadcast, traffic big data, VANETs}}
@inproceedings{10.1145/3097983.3105814,title = {Addressing Challenges with Big Data for Media Measurement}, author = {Mazumdar Mainak },year = {2017}, isbn = {9781450348874}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3097983.3105814}, doi = {10.1145/3097983.3105814}, abstract = {The digital media and TV - which is increasingly digitized, have amassed and generating enormous amount of data. While extremely useful, the big data generated by these platforms poses unique challenges for Data Scientists working on developing measurement framework and metrics. Most practitioners optimize speed and scale at the expense of accuracy, which is critical for any measurement. And, the trade-off between bias and variance is not in consideration. In this paper, we will demonstrate how Nielsen is combining proprietary ground truth data and methodologies with Big Data to address the accuracy and bias/variance challenges. We argue that high quality ground truth or training set is pre-requisite to deploying Big Data for high quality media measurement. To illustrate the point, we will share how Nielsen is combining its proprietary high quality panels with Set Top Box for TV measurement in the U.S.}, location = {Halifax, NS, Canada}, series = {KDD '17}, pages = {23}, numpages = {1}, keywords = {proprietary data, data mining, digital media, big data, tv, media measurement}}
@inproceedings{10.5555/3172795.3172852,title = {Big data analytics: challenges and applications to health care}, author = {Wong Serene , Jurisica Igor },year = {2017}, publisher = {IBM Corp.}, address = {USA}, abstract = {With recent technological advancement, biomedical data is growing rapidly, in terms of volume, quality and depth. This creates many challenges, and in this workshop we focused on how one can turn this data using \"big data analytics\" into knowledge that can be used effectively. One of the main building blocks of this process is diverse networks - typed graphs that provide detailed annotation of relationships among measured entities.}, location = {Markham, Ontario, Canada}, series = {CASCON '17}, pages = {338}, numpages = {1}}
@inproceedings{10.5555/3192424.3192597,title = {Big data mining of social networks for friend recommendation}, author = {Jiang Fan , Leung Carson K. , Pazdor Adam G. M. },year = {2016}, isbn = {9781509028467}, publisher = {IEEE Press}, abstract = {In the current era of big data, high volumes of valuable data can be easily collected and generated. Social networks are examples of generating sources of these big data. Users in these social networks are often linked by some interdependency such as friendship. As these big social networks keep growing, there are situations in which an individual user wants to find popular groups of friends so that he can recommend the same groups to other users. In this paper, we present a big data analytic solution that uses the MapReduce model in mining these big social networks for discovering groups of frequently connected users for friend recommendation. Evaluation results show the efficiency and practicality of our data analytic solution in mining big social networks, discovering popular users, and recommending friends.}, location = {Davis, California}, series = {ASONAM '16}, pages = {921\u2013922}, numpages = {2}, keywords = {big data, big data mining, friendship, social network analysis, social networks}}
@inproceedings{10.1145/3418688,title = {2020 the 3rd International Conference on Computing and Big Data},year = {2020}, isbn = {9781450387866}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Taichung, Taiwan}}
@inproceedings{10.1145/2896825,title = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},year = {2016}, isbn = {9781450341523}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {Big Data is about extracting valuable information from data in order to use it in intelligent ways such as to revolutionize decision-making in businesses, science and society. Big Data may open up radical new ways and unprecedented opportunities of attacking software engineering problems. Already now forums, forges, blogs, Q&A sites, and social networks, provide a wealth of data that may be analysed to uncover new requirements, provide evidence on usage and development trends of application frameworks, or to perform empirical studies involving real-world software developers. In addition, real-time data collected from mobile and cloud applications may be analysed to detect user trends, preferences, and optimization opportunities.BIGDSE 2016 features contributions and discussions that explore opportunities that Big Data technology offers to software engineering, both in research and practice (\"big data for software engineering\"). BIGDSE also looks at the software engineering challenges imposed by building Big Data software systems (\"software engineering for big data\").}, location = {Austin, Texas}}
@inproceedings{10.1145/2769458.2769484,title = {Simulation in the era of Big Data: Trends and Challenges}, author = {Theodoropoulos Georgios },year = {2015}, isbn = {9781450335836}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2769458.2769484}, doi = {10.1145/2769458.2769484}, abstract = {The emergence of extreme scale computing systems and the data explosion have presented an unprecedented opportunity for the analysis of systems at a rapidly increasing scale, complexity and granularity. This paradigm shift calls for an intermingling of 'what-if' and data analytics approaches, however the worlds of Simulation and Big Data have so far been largely separate. The talk will focus on the interplay between simulation, data and emerging computational platforms, identifying gaps and opportunities and discussing some concrete examples of interacting scalable data infrastructures and agent-based simulations.}, location = {London, United Kingdom}, series = {SIGSIM PADS '15}, pages = {1}, numpages = {1}, keywords = {big data, exascale, agent-based modelling, simulation}}
@inproceedings{10.1145/3084381.3084422,title = {Systematic Literature Review of Big Data Analytics}, author = {Eachempati Prajwal , Srivastava Praveen Ranjan },year = {2017}, isbn = {9781450350372}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3084381.3084422}, doi = {10.1145/3084381.3084422}, abstract = {This paper aims to identify some emerging sectors that apply big-data analytics through a systematic literature review conducted by capturing the existing work done in this subject area by academicians and industry experts worldwide and specifically in India backed by a detailed domain-wise, nation-wise and within India, an institute-wise analysis of the contributions made. Based on the existing work, the need for applying analytics in Banking and Finance is emphasized through the paper and a premise is provided for conducting future research in this domain.}, location = {Bangalore, India}, series = {SIGMIS-CPR '17}, pages = {177\u2013178}, numpages = {2}, keywords = {tools, business, domains, scopus, databases, big-data, finance, analytics}}
@inproceedings{10.1145/2463676.2463707,title = {The big data ecosystem at LinkedIn}, author = {Sumbaly Roshan , Kreps Jay , Shah Sam },year = {2013}, isbn = {9781450320375}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2463676.2463707}, doi = {10.1145/2463676.2463707}, abstract = {The use of large-scale data mining and machine learning has proliferated through the adoption of technologies such as Hadoop, with its simple programming semantics and rich and active ecosystem. This paper presents LinkedIn's Hadoop-based analytics stack, which allows data scientists and machine learning researchers to extract insights and build product features from massive amounts of data. In particular, we present our solutions to the ``last mile'' issues in providing a rich developer ecosystem. This includes easy ingress from and egress to online systems, and managing workflows as production processes. A key characteristic of our solution is that these distributed system concerns are completely abstracted away from researchers. For example, deploying data back into the online system is simply a 1-line Pig command that a data scientist can add to the end of their script. We also present case studies on how this ecosystem is used to solve problems ranging from recommendations to news feed updates to email digesting to descriptive analytical dashboards for our members.}, location = {New York, New York, USA}, series = {SIGMOD '13}, pages = {1125\u20131134}, numpages = {10}, keywords = {big data, data mining, offline processing, machine learning, data pipeline, hadoop}}
@inproceedings{10.1145/3472163.3472171,title = {Rigorous Measurement Model for Validity of Big Data: MEGA Approach}, author = {Bhardwaj Dave , Ormandjieva Olga },year = {2021}, isbn = {9781450389914}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3472163.3472171}, doi = {10.1145/3472163.3472171}, abstract = {Big Data is becoming a substantial part of the decision-making processes in both industry and academia, especially in areas where Big Data may have a profound impact on businesses and society. However, as more data is being processed, data quality is becoming a genuine issue that negatively affects credibility of the systems we build because of the lack of visibility and transparency of the underlying data. Therefore, Big Data quality measurement is becoming increasingly necessary in assessing whether data can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses Big Data quality measurement modelling and automation by proposing a novel quality measurement framework for Big Data (MEGA) that objectively assesses the underlying quality characteristics of Big Data (also known as the V's of Big Data) at each step of the Big Data Pipelines. Five of the Big Data V's (Volume, Variety, Velocity, Veracity and Validity) are currently automated by the MEGA framework. In this paper, a new theoretically valid quality measurement model is proposed for an essential quality characteristic of Big Data, called Validity. The proposed measurement information model for Validity of Big Data is a hierarchy of 4 derived measures / indicators and 5 based measures. Validity measurement is illustrated on a running example.}, location = {Montreal, QC, Canada}, series = {IDEAS '21}, pages = {285\u2013291}, numpages = {7}, keywords = {Big Data, Measurement Hierarchical Model, Quality Characteristics (V's), Representational Theory of Measurement,, Validity}}
@inproceedings{10.1145/3105831.3105841,title = {The SusCity Big Data Warehousing Approach for Smart Cities}, author = {Costa Carlos , Santos Maribel Yasmina },year = {2017}, isbn = {9781450352208}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3105831.3105841}, doi = {10.1145/3105831.3105841}, abstract = {Nowadays, the concept of Smart City provides a rich analytical context, highlighting the need to store and process vast amounts of heterogeneous data flowing at different velocities. This data is defined as Big Data, which imposes significant difficulties in traditional data techniques and technologies. Data Warehouses (DWs) have long been recognized as a fundamental enterprise asset, providing fact-based decision support for several organizations. The concept of DW is evolving. Traditionally, Relational Database Management Systems (RDBMSs) are used to store historical data, providing different analytical perspectives regarding several business processes. With the current advancements in Big Data techniques and technologies, the concept of Big Data Warehouse (BDW) emerges to surpass several limitations of traditional DWs. This paper presents a novel approach for designing and implementing BDWs, which has been supporting the SusCity data visualization platform. The BDW is a crucial component of the SusCity research project in the context of Smart Cities, supporting analytical tasks based on data collected in the city of Lisbon.}, location = {Bristol, United Kingdom}, series = {IDEAS '17}, pages = {264\u2013273}, numpages = {10}, keywords = {Big Data Warehousing, Big Data, Hadoop, Data Warehouse, Smart Cities, NoSQL}}
@inproceedings{10.14778/2733004.2733015,title = {Big data small footprint: the design of a low-power classifier for detecting transportation modes}, author = {Yu Meng-Chieh , Yu Tong , Wang Shao-Chen , Lin Chih-Jen , Chang Edward Y. },year = {2014}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2733004.2733015}, doi = {10.14778/2733004.2733015}, abstract = {Sensors on mobile phones and wearables, and in general sensors on IoT (Internet of Things), bring forth a couple of new challenges to big data research. First, the power consumption for analyzing sensor data must be low, since most wearables and portable devices are power-strapped. Second, the velocity of analyzing big data on these devices must be high, otherwise the limited local storage may overflow.This paper presents our hardware-software co-design of a classifier for wearables to detect a person's transportation mode (i.e., still, walking, running, biking, and on a vehicle). We particularly focus on addressing the big-data small-footprint requirement by designing a classifier that is low in both computational complexity and memory requirement. Together with a sensor-hub configuration, we are able to drastically reduce power consumption by 99%, while maintaining competitive mode-detection accuracy. The data used in the paper is made publicly available for conducting research.}, pages = {1429\u20131440}, numpages = {12}, keywords = {classification, transportation mode, sensor hub, big data small footprint, support vector machines, context-aware computing}}
@inproceedings{10.1145/2487575.2491135,title = {Using \"big data\" to solve \"small data\" problems}, author = {Neumann Chris },year = {2013}, isbn = {9781450321747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2487575.2491135}, doi = {10.1145/2487575.2491135}, abstract = {The brief history of knowledge discovery is filled with products that promised to bring \"BI to the masses\". But how do you build a product that truly bridges the gap between the conceptual simplicity of \"questions and answers\" and the structure needed to query traditional data stores?In this talk, Chris Neumann will discuss how DataHero applied the principles of user-centric design and development over a year and a half to create a product with which more than 95% of new users can get answers on their first attempt. He'll demonstrate the process DataHero uses to determine the best combination of algorithms and user interface concepts needed to create intuitive solutions to potentially complex interactions, including: Determining the structure of files uploaded by usersAccurately identifying data types within filesPresenting users with an optimal visualization for any combination of dataHelping users to ask questions of data when they don't know what to do Chris will also talk about what it's like to start a \"Big Data\" company and how he applied lessons from his time as the first engineer at Aster Data Systems to DataHero.}, location = {Chicago, Illinois, USA}, series = {KDD '13}, pages = {1140}, numpages = {1}, keywords = {analytics, data mining, big data}}
@inproceedings{10.1145/2513549.2514739,title = {Big data opportunities and challenges for IR, text mining and NLP}, author = {Plale Beth },year = {2013}, isbn = {9781450324151}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2513549.2514739}, doi = {10.1145/2513549.2514739}, abstract = {Big Data poses challenges for text analysis and natural language processing due to its characteristics of volume, veracity, and velocity of the data. The sheer volume in terms of numbers of documents challenges traditional local repository and index systems for large-scale analysis and mining. Computation, storage and data representation must work together to provide rapid access, search, and mining of the deep knowledge in the large text collection. Text under copyright poses additional barriers to computational access, where analysis has to be separated from human consumption of the original text. Data preprocessing, in most cases, remains a daunting task for big textual data particularly data veracity is questionable due to age of original materials. Data velocity is rate of change of the data but can also be the rate at which changes and corrections are made.The HathiTrust Research Center (HTRC) provides new opportunities for IR, NLP and text mining research. HTRC is the research arm of HathiTrust, a consortium that stewards the digital library of content from research libraries around the country. With close to 11 million volumes in HathiTrust collection, HTRC aims to provide large-scale computational access and analytics to these text resources.With the goal of facilitating scholar's work, HTRC establishes a cyberinfrastructure of software, staff, and services to assist researchers and developers more easily process and mine large scale textual data effectively and efficiently. The primary users of HTRC are digital humanities, informatics, and librarians. They are of different research backgrounds and expertise and thus a variety of tools are made available to them.In the HTRC model of computing, computation moves to the data, and services grow up around the corpus to serve the research community. In this manner, the architecture is cloud-based. Moving algorithms to the data is important because the copyrighted content must be protected, however, a side benefit is that the paradigm frees scholars from worrying about managing a large corpus of data.The text analytics currently supported in HTRC is the SEASR suite of analytical algorithms (www.seasr.org). SEASR algorithms, which are written as workflows, include entity extraction, tag cloud, topic modeling, NaiveBayes, Date Entities to Similie Timeline.In this talk, I introduce the collections, architecture, and text analytics of HTRC, with a focus on the challenges of a BigData corpus and what that means for data storage, access, and large-scale computation.HTRC is building a user community to better understand and support researcher needs. It opens many exciting possibilities for the NLP, text mining, IR types of research: with so large an amount of textual data and many candidate algorithms, with support for researcher contributed algorithms, many interesting research questions emerge and many interesting results are to follow.}, location = {San Francisco, California, USA}, series = {UnstructureNLP '13}, pages = {1\u20132}, numpages = {2}, keywords = {big data access, nlp, hathitrust, text mining and analysis, information retrieval}}
@inproceedings{10.1145/2835596.2835614,title = {On managing geospatial big-data in emergency management: some perspectives}, author = {Liu Kuien , Yao Yandong , Guo Danhuai },year = {2015}, isbn = {9781450339704}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2835596.2835614}, doi = {10.1145/2835596.2835614}, abstract = {With the rapid growth of mobile devices and applications, geo-tagged data is becoming increasingly important in emergency management and has become a major workload for big data storage systems. Traditional methods that storing geospatial data in centralized databases suffer from inevitable limitations such like scaling out with the growing size of geospatial data. In order to achieve scalability, a number of solutions on big geospatial data management are proposed in recent years. We can simply classify them into two kinds: extending on distributed databases, or migrating to big-data storage systems. For previous, they mostly adopt the massive parallel processing (MPP) based architecture, in which data are stored and retrieved in a set of independent nodes. Each node can be treated as a traditional databases instance with geospatial extension. For the latter, existing solutions tend to build an additional index layer above general-purpose distributed data stores, e.g., HBASE, CASSANDRA, MangoDB, etc., to support geospatial data while integrating the big-data lineage. However, there are no absolutely perfect data management systems on the earth. Some approaches are desired for execution efficiency while some others are better on fulfilling the programming level need for big data scenarios.In this paper, we analysis the requirements and challenges on geospatial big data storage in emergency management, succeed with discussion with individual perspective from practical cases. The purpose of this paper is not only focused on how to program a geospatial data storage platform but also on how to approve the rationality of geospatial big data system that we plan to build.}, location = {Bellevue, Washington}, series = {EM-GIS '15}, pages = {1\u20134}, numpages = {4}, keywords = {perspectives, emergency management, big data, geospatial}}
@inproceedings{10.1145/3305160.3305211,title = {A Managerial Framework for Intelligent Big Data Analytics}, author = {Sun Zhaohao , Huo Yanxia },year = {2019}, isbn = {9781450366427}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3305160.3305211}, doi = {10.1145/3305160.3305211}, abstract = {Intelligent big data analytics is an emerging paradigm for integrating big data, analytics, and artificial intelligence. The objective of this paper is to provide a managerial framework of intelligent big data analytics. More specifically, this paper proposes a managerial framework of intelligent big data analytics, which consists of intelligent big data analytics as a science, technology, system, service and management for improving business decision making. Then it elaborates intelligent big data analytics for management taking into account main managerial functions: planning, organising, leading and controlling. The proposed approach in this paper might facilitate the research and development of business analytics, big data analytics, business intelligence, artificial intelligence and data science.}, location = {Bali, Indonesia}, series = {ICSIM 2019}, pages = {152\u2013156}, numpages = {5}, keywords = {artificial intelligence, intelligent analytics, intelligent big data analytics, management analytics}}
@inproceedings{10.1145/3291801.3291833,title = {Visual Analysis of Big Data Based on Movies Released in China}, author = {Shaorong He , Zhifeng Xie , Jianbo Huang },year = {2018}, isbn = {9781450364768}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3291801.3291833}, doi = {10.1145/3291801.3291833}, abstract = {China film market has become the second largest box office market just after North America today, the Chinese film market is less than 1/5 size of the now, in 10 years ago. The Chinese film market is growing so fast, the reasons research on this phenomenon become a hot spot. In this article, we tried to find the rule of films box office market based on the historical data of China released film. Gaining film market rules will help film-makers understand audience preferences and make decisions on future movie projects. We collected the movies data released in China in recent 15 years which contains box office, and basic information (such as release date, language, type, country, etc.), correspondingly, we also collected the movie audience's com-ments on the internet, more than 100000 comments in total, There is no public movies released in China set provided by a inde-pendent agency until now, According to the obtained data, we analyzed the movies data distribution rules in the month, year, genre, using language, We also visualized the proportion of all films in the rating (from douban.com), and analyzed collectively the quality of domestic released films. We conducted word cloud processing on the comments of the highest box office movie Wolf warriors 2, which demonstrate intuitively the public's discussion on this movie.}, location = {Weihai, China}, series = {ICBDR 2018}, pages = {80\u201385}, numpages = {6}, keywords = {data analysis, Movies released in China, visual analysis}}
@inproceedings{10.5555/2602724.2602725,title = {Big data meets computer science}, author = {Hendler Jim },year = {2014}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {As \"big data\" moves from buzzword to practice, campus departments must increasingly figure out where data science fits into their curricula. Clearly such an interdisciplinary area crosses traditional boundaries ranging from statistics traditionally taught in mathematics or engineering departments, a new method of scientific discovery for biologists and chemists, a new challenge for ethicists and political scientists, and a new realm for design and electronic arts, etc. Within computer science departments, it currently seems to reside in the machine learning and knowledge discovery areas where the metaphor of big data as \"the new oil\" to be mined is pursued. In this talk, however, I opine that just as oil is important for the energy it generates, which powers the technologies of modern life, data is increasingly important for the information it generates, which will power the information applications of the future. We will explore some of these emerging trends, ranging from high performance modeling to the Watson AI system, looking at what we might want to be teaching our students if they are to be leaders in this emerging area.}, pages = {5\u20136}, numpages = {2}}
@inproceedings{10.1145/2247596.2247598,title = {Inside \"Big Data management\": ogres, onions, or parfaits?}, author = {Borkar Vinayak , Carey Michael J. , Li Chen },year = {2012}, isbn = {9781450307901}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2247596.2247598}, doi = {10.1145/2247596.2247598}, abstract = {In this paper we review the history of systems for managing \"Big Data\" as well as today's activities and architectures from the (perhaps biased) perspective of three \"database guys\" who have been watching this space for a number of years and are currently working together on \"Big Data\" problems. Our focus is on architectural issues, and particularly on the components and layers that have been developed recently (in open source and elsewhere) and on how they are being used (or abused) to tackle challenges posed by today's notion of \"Big Data\". Also covered is the approach we are taking in the ASTERIX project at UC Irvine, where we are developing our own set of answers to the questions of the \"right\" components and the \"right\" set of layers for taming the \"Big Data\" beast. We close by sharing our opinions on what some of the important open questions are in this area as well as our thoughts on how the dataintensive computing community might best seek out answers.}, location = {Berlin, Germany}, series = {EDBT '12}, pages = {3\u201314}, numpages = {12}}
@inproceedings{10.1145/3078564.3078571,title = {Design of adaptive learning system based on big data}, author = {Liu Xiaoxia , Du Yuejin , Sun Feiqiang , Zhai Lidong },year = {2017}, isbn = {9781450352109}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3078564.3078571}, doi = {10.1145/3078564.3078571}, abstract = {In1 recent years, with the rise and development of big data technology, research about big data of education has gradually become a hot research. Big data of education is an important means of modernization educational change based on data acquisition and analysis technology. The major applications of big data in education are educational data mining, learning analysis and educational decision-making and so on [1]. With the combination of big data and education, adaptive learning system [2] gradually into our sight. Adaptive learning system is a system that can provide a personalized learning service for learners. The system can recommended personalized learning path and learning resources according to learners' various characteristics and behavioral tendencies, such as learning style, media tendency, interest, cognitive level and so on. This paper aims to design an adaptive learning system based on the big data in education. The system contains four modules: domain module, student module, adaptive recommendation module and visual display module.}, location = {Dalian Liaoning, China}, series = {ICIE '17}, pages = {1\u20135}, numpages = {5}, keywords = {big data, data mining, adaptive learning}}
@inproceedings{10.1007/s00778-014-0357-y,title = {The Stratosphere platform for big data analytics}, author = {Alexandrov Alexander , Bergmann Rico , Ewen Stephan , Freytag Johann-Christoph , Hueske Fabian , Heise Arvid , Kao Odej , Leich Marcus , Leser Ulf , Markl Volker , Naumann Felix , Peters Mathias , Rheinl\u00e4nder Astrid , Sax Matthias J. , Schelter Sebastian , H\u00f6ger Mareike , Tzoumas Kostas , Warneke Daniel },year = {2014}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg}, url = {https://doi.org/10.1007/s00778-014-0357-y}, doi = {10.1007/s00778-014-0357-y}, abstract = {We present Stratosphere, an open-source software stack for parallel data analysis. Stratosphere brings together a unique set of features that allow the expressive, easy, and efficient programming of analytical applications at very large scale. Stratosphere's features include \"in situ\" data processing, a declarative query language, treatment of user-defined functions as first-class citizens, automatic program parallelization and optimization, support for iterative programs, and a scalable and efficient execution engine. Stratosphere covers a variety of \"Big Data\" use cases, such as data warehousing, information extraction and integration, data cleansing, graph analysis, and statistical analysis applications. In this paper, we present the overall system architecture design decisions, introduce Stratosphere through example queries, and then dive into the internal workings of the system's components that relate to extensibility, programming model, optimization, and query execution. We experimentally compare Stratosphere against popular open-source alternatives, and we conclude with a research outlook for the next years.}, pages = {939\u2013964}, numpages = {26}, keywords = {Big data, Graph processing, Distributed systems, Query Optimization, Parallel databases, Text mining, Query processing, Data cleansing}}
@inproceedings{10.5555/2694443.2694450,title = {Big data technologies circa 2012}, author = {Borkar Vinayak , Carey Michael J. },year = {2012}, publisher = {Computer Society of India}, address = {Mumbai, Maharashtra, IND}, abstract = {The growth of the World Wide Web has led to an astronomical amount of data being generated. More recently, the amount of user-generated content has seen tremendous expansion thanks to social media like Facebook and Twitter. Enterprises, researchers, and even governments consider this data to be an invaluable source of insight into people's behavior, creating a race to analyze as much data as possible. This race has driven virtually everyone, ranging from Web companies to brick and mortar businesses, into a \"Big Data\" frenzy. On the systems side, traditional relational databases have proven to be un-scalable, too expensive, too rigid, and/or too heavy-weight for dealing with current Big Data problems. As a result, there has been an explosion in the number of systems being developed, both within industry as well as in academia, to manage massive amounts of data.}, location = {Pune, India}, series = {COMAD '12}, pages = {12\u201314}, numpages = {3}}
@inproceedings{10.1145/2345316.2345328,title = {Cloud computing & big data computing}, author = {Xue Zhiming },year = {2012}, isbn = {9781450311137}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2345316.2345328}, doi = {10.1145/2345316.2345328}, abstract = {The amount of data each organization deals with today has been rapidly growing. However, analyzing large datasets commonly referred to as \"big data\" has been a huge challenge due to lack of suitable tools and adequate computing resources. Why are organizations, both in public sector and private sector, so keen on unlocking business insights from all structured and unstructured data? What is the current state of big data solutions and service providers? How effective are some of the solutions that have been put into real world practices? What is the current state of cloud computing technologies? What impacts have cloud computing technologies available in public clouds and private clouds had on the way organizations addressing big data challenges? How to secure big data in the clouds? What are the future roadmaps for cloud-based big data solutions, especially for geospatial related applications?This panel discussion will include a short presentation or discussion related to big data and cloud computing by each panelist, followed by questions and questions from the audience and the panel.}, location = {Washington, D.C., USA}, series = {COM.Geo '12}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/2644866.2644870,title = {Humanist-centric tools for big data: berkeley prosopography services}, author = {Schmitz Patrick , Pearce Laurie },year = {2014}, isbn = {9781450329491}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2644866.2644870}, doi = {10.1145/2644866.2644870}, abstract = {In this paper, we describe Berkeley Prosopography Services (BPS), a new set of tools for prosopography - the identification of individuals and study of their interactions - in support of humanities research. Prosopography is an example of \"big data\" in the humanities, characterized not by the size of the datasets, but by the way that computational and data-driven methods can transform scholarly workflows. BPS is based upon re-usable infrastructure, supporting generalized web services for corpus management, social network analysis, and visualization. The BPS disambiguation model is a formal implementation of the traditional heuristics used by humanists, and supports plug-in rules for adaptation to a wide range of domain corpora. A workspace model supports exploratory research and collaboration. We contrast the BPS model of configurable heuristic rules to other approaches for automated text analysis, and explain how our model facilitates interpretation by humanist researchers. We describe the significance of the BPS assertion model in which researchers assert conclusions or possibilities, allowing them to override automated inference, to explore ideas in what-if scenarios, and to formally publish and subscribe-to asserted annotations among colleagues, and/or with students. We present an initial evaluation of researchers' experience using the tools to study corpora of cuneiform tablets, and describe plans to expand the application of the tools to a broader range of corpora.}, location = {Fort Collins, Colorado, USA}, series = {DocEng '14}, pages = {179\u2013188}, numpages = {10}, keywords = {web-services, digital humanities, annotation, prosopography, assertions, social network analysis, cyberinfrastructure, big data}}
@inproceedings{10.1145/3110025.3119402,title = {Anomaly Detection on Big Data in Financial Markets}, author = {Ahmed Mohiuddin , Choudhury Nazim , Uddin Shahadat },year = {2017}, isbn = {9781450349932}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3110025.3119402}, doi = {10.1145/3110025.3119402}, abstract = {In the modern financial market, market participants use big data analytics to gain valuable insight on historical market data for better decision making. Complying with the three vs (i.e., velocity, volume and variety) of big data, the financial market is considered as a complex system comprised of many interacting high-frequency traders those make decisions based on the relative strengths of these interactions. Researchers have put substantial scholarly input to deal with these anomalies. From the big data perspective, anomaly detection in financial data has widely been ignored despite many organisations store, process and disseminate financial market data for interested customers to assist them to make informed decision abd create competitive advantages. Considering the presence of anomalies in voluminous data from myriad data sources may generate catastrophic decision through misunderstandings of market behaviour. Therefore, in this study, we applied a standard set of anomaly detection techniques, used in big data based on nearest-neighbours, clustering and statistical approaches, to detect rare anomalies present within the historical daily trading information for five years (i.e., 2009--2013) for each stock listed on the Australian Security Exchange (ASX). We also measured the performance of these anomaly detection techniques using a number of metrics to highlight the best performing algorithm. The experimental results suggest that the LOF(Local Outlier Factor) and CMGOS(Clustering-based Multivariate Gaussian Outlier Score) are the best performing anomaly detection techniques.}, location = {Sydney, Australia}, series = {ASONAM '17}, pages = {998\u20131001}, numpages = {4}, keywords = {Financial Markets, Financial Big Data, Anomaly Detection}}
@inproceedings{10.1145/3348400.3348414,title = {The Impact of Big Data on Health Care Services in Australia: Using Big Data Analytics to Categorise and Deal with Patients}, author = {Karim Shakir , Gide Ergun , Sandu Raj },year = {2019}, isbn = {9781450371674}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3348400.3348414}, doi = {10.1145/3348400.3348414}, abstract = {Big Data is the biggest emerging trend and promise in today's technology-driven world. It is continuing to create a lot of buzz in not only the field of technology, but across the world. It promises substantial involvements, vast changes, modernizations, and integration with and within people's ongoing life. It makes the world more demanding and helps with making prompt and appropriate decisions in real time. This paper aims to provide a comprehensive analysis of the health industry and health care system in Australia that are relevant to the consequences formed by Big Data. This paper primarily uses a secondary research analysis method to provide a wide-ranging investigation into the positive and negative consequences of health issues relevant to Big Data, the architects of those consequences, and those overstated by the consequences. The secondary resources are subject to journal articles, reports, conference proceedings, media articles, corporation-based documents, blogs and other appropriate information. In the future, the investigation will continue by employing Mixed Methodology (Qualitative and Quantitative) in relation to Big Data usage in the Australian Health industry. The paper initially finds that Big Data is an evidence source in health care and provides useful insight into the Australian healthcare system. It is steadily reducing the cost of the Australian healthcare system and improving patients' outcomes in Australia. Big data can not only improve the affairs between public and health enterprises, but can also make life better by increasing efficiency and modernization.}, location = {Sydney, NSW, Australia}, series = {ICMSTTL 2019}, pages = {34\u201338}, numpages = {5}, keywords = {Big Data (BD), High Risk and High Cost Patients, Australian Health Care Services, Health Care System}}
@inproceedings{10.5555/2819289.2819298,title = {Research opportunities for the big data era of software engineering}, author = {DeLine Robert },year = {2015}, publisher = {IEEE Press}, abstract = {Big Data Analysis is becoming a widespread practice on many software development projects, and statisticians and data analysts are working alongside developers, testers and program managers. Because data science is still an emerging discipline in software projects, there are many opportunities where software engineering researchers can help improve practice. In terms of productivity, data scientists need support for exploratory analysis of large datasets, relief from clerical tasks like data cleaning, and easier paths for live deployment of new analyses. In terms of correctness, data scientists need help in preserving data meaning and provenance, and non-experts need help avoiding analysis errors. In terms of communication and coordination, teams need more approachable ways to discuss uncertainty and risk, and support for data-driven decision making needs to become available to all roles. This position paper describes these open problems and points to ongoing research beginning to tackle them.}, location = {Florence, Italy}, series = {BIGDSE '15}, pages = {26\u201329}, numpages = {4}}
@inproceedings{10.1145/3221269.3221294,title = {Point pattern search in big data}, author = {Porto Fabio , Rittmeyer Jo\u00e3o N. , Ogasawara Eduardo , Krone-Martins Alberto , Valduriez Patrick , Shasha Dennis },year = {2018}, isbn = {9781450365055}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3221269.3221294}, doi = {10.1145/3221269.3221294}, abstract = {Consider a set of points P in space with at least some of the pairwise distances specified. Given this set P, consider the following three kinds of queries against a database D of points : (i) pure constellation query: find all sets S in D of size |P| that exactly match the pairwise distances within P up to an additive error \u03f5; (ii) isotropic constellation queries: find all sets S in D of size |P| such that there exists some scale factor f for which the distances between pairs in S exactly match f times the distances between corresponding pairs of P up to an additive \u03f5; (iii) non-isotropic constellation queries: find all sets S in D of size |P| such that there exists some scale factor f and for at least some pairs of points, a maximum stretch factor mi,j > 1 such that (f X mi,jXdist(pi, pj))+\u03f5 > dist(si,sj) > (f X dist(pi, pj)) - \u03f5. Finding matches to such queries has applications to spatial data in astronomical, seismic, and any domain in which (approximate, scale-independent) geometrical matching is required. Answering the isotropic and non-isotropic queries is challenging because scale factors and stretch factors may take any of an infinite number of values. This paper proposes practically efficient sequential and distributed algorithms for pure, isotropic, and non-isotropic constellation queries. As far as we know, this is the first work to address isotropic and non-isotropic queries.}, location = {Bozen-Bolzano, Italy}, series = {SSDBM '18}, pages = {1\u201312}, numpages = {12}, keywords = {distance matching, pattern search, big data, geometrical patterns, spatial patterns, isotropic, point set registration}}
@inproceedings{10.1145/3416921.3416936,title = {An Analysis of Scientific Production in Big Data Knowledge Domain on Google Books, YouTube and IEEE Explore\u00ae Digital Library}, author = {Gotsev Lyubomir , Shoikova Elena },year = {2020}, isbn = {9781450375382}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3416921.3416936}, doi = {10.1145/3416921.3416936}, abstract = {The paper aims to reveal the current state of book, video and article production in Big Data Knowledge Domain on particular platforms by examining the capabilities of Application Programming Interface (API) technology in conducting scientific data-driven research. Queries append public records from Google Books, YouTube and IEEE Explore\u00ae Digital Library to two research paradigms (sets of data sets): Big Data (incl. Analysis, Engineering, Architecture, Governance, Management, Frameworks) and Big Data interdisciplinary fields (Data Science, Data Mining, Deep Learning, Machine Learning, Artificial Intelligence). Metadata from more than 25 000 conference papers, 2000 books over the past 50 years, and 4 000 videos for the last 12 years, matching the searching criteria, has been stored and analyzed. The outputs are summarized in statistics, forecasting, rating key findings by various attributes: title, author, publisher, research field, category, subject, publication year, description, view count, and a combination of mentioned metadata in cross-tables. Nearly a half of billion video views; a half of million article reference count; a twofold increase in the number of papers in Machine learning over past three years compared to the total number in the same field for entire 1988-2016 period; 1:2:12 overall books-to-videos-to conference papers ratio; 61.3% of last year's video production just in a month (Jan-2020); the earliest found usage of \"Artificial Intelligence\" expression in a printed law document dated 1848 are few curious examples of analysis findings. The paper presents non-commercial research and retrieved data is collected entirely from public records.}, location = {Virtual, United Kingdom}, series = {ICCBDC '20}, pages = {10\u201314}, numpages = {5}, keywords = {Scientific Publishing, API, Big Data, Data Analysis}}
@inproceedings{10.1145/3422713.3422715,title = {Design and Implementation of Big Data Management Platform for Android Applications}, author = {Han Bing , Chen Zhenxiang , Liu Cong , Shang Mingyue },year = {2020}, isbn = {9781450387859}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3422713.3422715}, doi = {10.1145/3422713.3422715}, abstract = {In recent years, the number of Android malicious applications has grown rapidly. In the field of network security, the detection of Android malicious applications has been a hot spot. Traditional Android malicious application detection has two methods: dynamic detection and static detection. With the development of network technology, network traffic has increased dramatically, analysis and researchers pay attention to malware detection based on network traffic. As the number of applications increases, application data management becomes particularly important. This paper proposes a method of the collection, store, analysis and visualization of Android applications. This platform provides a simple way to access data, which has broad application prospects.}, location = {Qingdao, China}, series = {ICBDT 2020}, pages = {36\u201340}, numpages = {5}, keywords = {Data management, Network traffic, Android, Malicious applications}}
@inproceedings{10.1145/3186549.3186559,title = {Data Quality: The Role of Empiricism}, author = {Sadiq Shazia , Dasu Tamraparni , Dong Xin Luna , Freire Juliana , Ilyas Ihab F. , Link Sebastian , Miller Miller J. , Naumann Felix , Zhou Xiaofang , Srivastava Divesh },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3186549.3186559}, doi = {10.1145/3186549.3186559}, abstract = {We outline a call to action for promoting empiricism in data quality research. The action points result from an analysis of the landscape of data quality research. The landscape exhibits two dimensions of empiricism in data quality research relating to type of metrics and scope of method. Our study indicates the presence of a data continuum ranging from real to synthetic data, which has implications for how data quality methods are evaluated. The dimensions of empiricism and their inter-relationships provide a means of positioning data quality research, and help expose limitations, gaps and opportunities.}, pages = {35\u201343}, numpages = {9}}
@inproceedings{10.1145/3030207.3053670,title = {Technology Migration Challenges in a Big Data Architecture Stack}, author = {Singhal Rekha , Kunde Shruti },year = {2017}, isbn = {9781450344043}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3030207.3053670}, doi = {10.1145/3030207.3053670}, abstract = {Application and/or data migration is a result of limitations in existing system architecture to handle new requirements and the availability of newer, more efficient technology. In any big data architecture, technology migration is staggered across multiple levels and poses functional (related to components of the architecture and underlying infrastructure) and non-functional (QoS) challenges such as availability, reliability and performance guarantees in the target architecture. In this paper, (1) we outline a big data architecture stack and identify research problems arising out of the technology migration in this scenario (2) we propose a smart rule engine system which facilitates the decision making process for the technology to be used at different layers in the architecture during migration.}, location = {L&apos;Aquila, Italy}, series = {ICPE '17}, pages = {159\u2013160}, numpages = {2}, keywords = {big data, migration, performance}}
@inproceedings{10.1145/3299819.3299841,title = {Risk Assessment for Big Data in Cloud: Security, Privacy and Trust}, author = {bt Yusof Ali Hazirah Bee , bt Abdullah Lili Marziana , Kartiwi Mira , Nordin Azlin },year = {2018}, isbn = {9781450366236}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3299819.3299841}, doi = {10.1145/3299819.3299841}, abstract = {The alarming rate of big data usage in the cloud makes data exposed easily. Cloud which consists of many servers linked to each other is used for data storage. Having owned by third parties, the security of the cloud needs to be looked at. Risks of storing data in cloud need to be checked further on the severity level. There should be a way to access the risks. Thus, the objective of this paper is to use SLR so that we can have extensive background of literatures on risk assessment for big data in cloud computing environment from the perspective of security, privacy and trust.}, location = {Tokyo, Japan}, series = {AICCC '18}, pages = {63\u201367}, numpages = {5}, keywords = {Security, Privacy, Big Data, Cloud, Trust, Risk Assessment}}
@inproceedings{10.1145/3130218.3130236,title = {Adaptive Manycore Architectures for Big Data Computing}, author = {Doppa Janardhan Rao , Kim Ryan Gary , Isakov Mihailo , Kinsy Michel A. , Kwon Hyouk Jun , Krishna Tushar },year = {2017}, isbn = {9781450349840}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3130218.3130236}, doi = {10.1145/3130218.3130236}, abstract = {This work presents a cross-layer design of an adaptive manycore architecture to address the computational needs of emerging big data applications within the technological constraints of power and reliability. From the circuits end, we present links with reconfigurable repeaters that allow single-cycle traversals across multiple hops, creating fast single-cycle paths on demand. At the microarchitecture end, we present a router with bi-directional links, unified virtual channel (VC) structure, and the ability to perform self-monitoring and self-configuration around faults. We present our vision for self-aware manycore architectures and argue that machine learning techniques are very appropriate to efficiently control various configurable on-chip resources in order to realize this vision. We provide concrete learning algorithms for core and NoC reconfiguration; and dynamic power management to improve the performance, energy-efficiency, and reliability over static designs to meet the demands of big data computing. We also discuss future challenges to push the state-of-the-art on fully adaptive manycore architectures.}, location = {Seoul, Republic of Korea}, series = {NOCS '17}, pages = {1\u20138}, numpages = {8}, keywords = {Machine learning, Big data computing, Power management, Interconnect networks, Adaptive manycore architectures}}
@inproceedings{10.1145/3524383.3524433,title = {Big Data in the Era of Pandemic COVID-19 : Application of IoT based data analytics, Machine Learning and Artificial Intelligence}, author = {Abdul Jalil Nasir , Wong Ei Leen Mikkay },year = {2022}, isbn = {9781450395793}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3524383.3524433}, doi = {10.1145/3524383.3524433}, abstract = {Over the last couple of years, Covid-19 has caused an uproar and chaos throughout the world. The occurrence of Covid-19 is an unprecedented event which has led towards a substantial number of lost humans\u2019 lives and mayhem in the economic, social, and most importantly, healthcare systems across the world. In order to gain control of the pandemic, it is extremely pertinent to truly grasp the characteristics and behavior of the coronavirus which can be done by gathering and evaluating related big data. Furthermore, big data analytics tools are known to play an important role in building knowledge which are vital for decision making and precautionary measures. Across the world, it is evident that both government and non-governmental organizations have been working hand-in-hand to deploy big data technology. There is a plethora of data analytics methods available thus, the intention of this work is to assemble available methods which can be applied in the current pandemic.}, location = {Shanghai, China}, series = {ICBDE '22}, pages = {361\u2013367}, numpages = {7}, keywords = {Pandemic, Machine Learning, Big Data, IoT based data analytics, Artificial Intelligence, COVID-19}}
@inproceedings{10.1145/2612669.2612702,title = {Deadline-aware scheduling of big-data processing jobs}, author = {Bodik Peter , Menache Ishai , Naor Joseph (Seffi) , Yaniv Jonathan },year = {2014}, isbn = {9781450328210}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2612669.2612702}, doi = {10.1145/2612669.2612702}, abstract = {This paper presents a novel algorithm for scheduling big data jobs on large compute clusters. In our model, each job is represented by a DAG consisting of several stages linked by precedence constraints. The resource allocation per stage is malleable, in the sense that the processing time of a stage depends on the resources allocated to it (the dependency can be arbitrary in general).The goal of the scheduler is to maximize the total value of completed jobs, where the value for each job depends on its completion time. We design an algorithm for the problem which guarantees an expected constant approximation factor when the cluster capacity is sufficiently high. To the best of our knowledge, this is the first constant-factor approximation algorithm for the problem. The algorithm is based on formulating the problem as a linear program and then rounding an optimal (fractional) solution into a feasible (integral) schedule using randomized rounding.}, location = {Prague, Czech Republic}, series = {SPAA '14}, pages = {211\u2013213}, numpages = {3}, keywords = {deadline-aware scheduling, big data, scheduling algorithms}}
@inproceedings{10.1145/3450287,title = {Event Prediction in the Big Data Era: A Systematic Survey}, author = {Zhao Liang },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3450287}, doi = {10.1145/3450287}, abstract = {Events are occurrences in specific locations, time, and semantics that nontrivially impact either our society or the nature, such as earthquakes, civil unrest, system failures, pandemics, and crimes. It is highly desirable to be able to anticipate the occurrence of such events in advance to reduce the potential social upheaval and damage caused. Event prediction, which has traditionally been prohibitively challenging, is now becoming a viable option in the big data era and is thus experiencing rapid growth, also thanks to advances in high performance computers and new Artificial Intelligence techniques. There is a large amount of existing work that focuses on addressing the challenges involved, including heterogeneous multi-faceted outputs, complex (e.g., spatial, temporal, and semantic) dependencies, and streaming data feeds. Due to the strong interdisciplinary nature of event prediction problems, most existing event prediction methods were initially designed to deal with specific application domains, though the techniques and evaluation procedures utilized are usually generalizable across different domains. However, it is imperative yet difficult to cross-reference the techniques across different domains, given the absence of a comprehensive literature survey for event prediction. This article aims to provide a systematic and comprehensive survey of the technologies, applications, and evaluations of event prediction in the big data era. First, systematic categorization and summary of existing techniques are presented, which facilitate domain experts\u2019 searches for suitable techniques and help model developers consolidate their research at the frontiers. Then, comprehensive categorization and summary of major application domains are provided to introduce wider applications to model developers to help them expand the impacts of their research. Evaluation metrics and procedures are summarized and standardized to unify the understanding of model performance among stakeholders, model developers, and domain experts in various application domains. Finally, open problems and future directions are discussed. Additional resources related to event prediction are included in the paper website: http://cs.emory.edu/\u223clzhao41/projects/event_prediction_site.html.}, pages = {1\u201337}, numpages = {37}, keywords = {big data, Event prediction, artificial intelligence}}
@inproceedings{10.1145/3445945,title = {2020 the 4th International Conference on Big Data Research (ICBDR'20)},year = {2020}, isbn = {9781450387750}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Tokyo, Japan}}
@inproceedings{10.1145/1376916.1376940,title = {Dependencies revisited for improving data quality}, author = {Fan Wenfei },year = {2008}, isbn = {9781605581521}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1376916.1376940}, doi = {10.1145/1376916.1376940}, abstract = {Dependency theory is almost as old as relational databases themselves, and has traditionally been used to improve the quality of schema, among other things. Recently there has been renewed interest in dependencies for improving the quality of data. The increasing demand for data quality technology has also motivated revisions of classical dependencies, to capture more inconsistencies in real-life data, and to match, repair and query the inconsistent data. This paper aims to provide an overview of recent advances in revising classical dependencies for improving data quality.}, location = {Vancouver, Canada}, series = {PODS '08}, pages = {159\u2013170}, numpages = {12}, keywords = {dependency, data quality}}