@inproceedings{10.1145/3183440.3190334,
author = {Gulzar, Muhammad Ali},
title = {Interactive and Automated Debugging for Big Data Analytics},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3190334},
doi = {10.1145/3183440.3190334},
abstract = {An abundance of data in many disciplines of science, engineering, national security, health care, and business has led to the emerging field of Big Data Analytics that run in a cloud computing environment. To process massive quantities of data in the cloud, developers leverage Data-Intensive Scalable Computing (DISC) systems such as Google's MapReduce, Hadoop, and Spark.Currently, developers do not have easy means to debug DISC applications. The use of cloud computing makes application development feel more like batch jobs and the nature of debugging is therefore post-mortem. Developers of big data applications write code that implements a data processing pipeline and test it on their local workstation with a small sample data, downloaded from a TB-scale data warehouse. They cross fingers and hope that the program works in the expensive production cloud. When a job fails or they get a suspicious result, data scientists spend hours guessing at the source of the error, digging through post-mortem logs. In such cases, the data scientists may want to pinpoint the root cause of errors by investigating a subset of corresponding input records.The vision of my work is to provide interactive, real-time and automated debugging services for big data processing programs in modern DISC systems with minimum performance impact. My work investigates the following research questions in the context of big data analytics: (1) What are the necessary debugging primitives for interactive big data processing? (2) What scalable fault localization algorithms are needed to help the user to localize and characterize the root causes of errors? (3) How can we improve testing efficiency during iterative development of DISC applications by reasoning the semantics of dataflow operators and user-defined functions used inside dataflow operators in tandem?To answer these questions, we synthesize and innovate ideas from software engineering, big data systems, and program analysis, and coordinate innovations across the software stack from the user-facing API all the way down to the systems infrastructure.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {509–511},
numpages = {3},
keywords = {data-intensive scalable computing (DISC), and data cleaning, fault localization, big data, debugging and testing, automated debugging, data provenance, test minimization},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2746090.2746117,
author = {McGinnis, John O. and Stein, Branden},
title = {Originalism, Hypothesis Testing and Big Data},
year = {2015},
isbn = {9781450335225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2746090.2746117},
doi = {10.1145/2746090.2746117},
abstract = {In this paper, we describe how data mining and hypothesis testing can advance the analysis of originalism in American constitutional law.},
booktitle = {Proceedings of the 15th International Conference on Artificial Intelligence and Law},
pages = {201–205},
numpages = {5},
keywords = {originalism, hypothesis testing, big data},
location = {San Diego, California},
series = {ICAIL '15}
}

@article{10.14778/2824032.2824067,
author = {Hu, Xueyang and Yuan, Mingxuan and Yao, Jianguo and Deng, Yu and Chen, Lei and Yang, Qiang and Guan, Haibing and Zeng, Jia},
title = {Differential Privacy in Telco Big Data Platform},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824067},
doi = {10.14778/2824032.2824067},
abstract = {Differential privacy (DP) has been widely explored in academia recently but less so in industry possibly due to its strong privacy guarantee. This paper makes the first attempt to implement three basic DP architectures in the deployed telecommunication (telco) big data platform for data mining applications. We find that all DP architectures have less than 5% loss of prediction accuracy when the weak privacy guarantee is adopted (e.g., privacy budget parameter ε ≥ 3). However, when the strong privacy guarantee is assumed (e.g., privacy budget parameter ε ≤ 0:1), all DP architectures lead to 15% ~ 30% accuracy loss, which implies that real-word industrial data mining systems cannot work well under such a strong privacy guarantee recommended by previous research works. Among the three basic DP architectures, the Hybridized DM (Data Mining) and DB (Database) architecture performs the best because of its complicated privacy protection design for the specific data mining algorithm. Through extensive experiments on big data, we also observe that the accuracy loss increases by increasing the variety of features, but decreases by increasing the volume of training data. Therefore, to make DP practically usable in large-scale industrial systems, our observations suggest that we may explore three possible research directions in future: (1) Relaxing the privacy guarantee (e.g., increasing privacy budget ε) and studying its effectiveness on specific industrial applications; (2) Designing specific privacy scheme for specific data mining algorithms; and (3) Using large volume of data but with low variety for training the classification models.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1692–1703},
numpages = {12}
}

@inproceedings{10.1145/2463676.2463707,
author = {Sumbaly, Roshan and Kreps, Jay and Shah, Sam},
title = {The Big Data Ecosystem at LinkedIn},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2463707},
doi = {10.1145/2463676.2463707},
abstract = {The use of large-scale data mining and machine learning has proliferated through the adoption of technologies such as Hadoop, with its simple programming semantics and rich and active ecosystem. This paper presents LinkedIn's Hadoop-based analytics stack, which allows data scientists and machine learning researchers to extract insights and build product features from massive amounts of data. In particular, we present our solutions to the ``last mile'' issues in providing a rich developer ecosystem. This includes easy ingress from and egress to online systems, and managing workflows as production processes. A key characteristic of our solution is that these distributed system concerns are completely abstracted away from researchers. For example, deploying data back into the online system is simply a 1-line Pig command that a data scientist can add to the end of their script. We also present case studies on how this ecosystem is used to solve problems ranging from recommendations to news feed updates to email digesting to descriptive analytical dashboards for our members.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1125–1134},
numpages = {10},
keywords = {machine learning, hadoop, data mining, data pipeline, big data, offline processing},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/1851476.1851558,
author = {Na'im, Aisa and Crawl, Daniel and Indrawan, Maria and Altintas, Ilkay and Sun, Shulei},
title = {Monitoring Data Quality in Kepler},
year = {2010},
isbn = {9781605589428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851476.1851558},
doi = {10.1145/1851476.1851558},
abstract = {Data quality is an important component of modern scientific discovery. Many scientific discovery processes consume data from a diverse array of resources such as streaming sensor networks, web services, and databases. The validity of a scientific computation's results is highly dependent on the quality of these input data. Scientific workflow systems are being increasingly used to automate scientific computations by facilitating experiment design, data capture, integration, processing, and analysis. These workflows may execute in grid or cloud environments, and if the data produced during workflow execution is deemed unusable or low in quality, execution should stop to prevent wasting these valuable resources. We propose an approach in the Kepler scientific workflow system for monitoring data quality and demonstrate its use for oceanography and bioinformatics domains.},
booktitle = {Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
pages = {560–564},
numpages = {5},
keywords = {grid computing, data quality management, scientific workflow},
location = {Chicago, Illinois},
series = {HPDC '10}
}

@inproceedings{10.1145/2609876.2609891,
author = {Elm, William C.},
title = {User Evaluation Methodology Framework in Big Data Environments},
year = {2014},
isbn = {9781450329385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609876.2609891},
doi = {10.1145/2609876.2609891},
abstract = {From an initial discussion of "analytic tradecraft will be different when enabled with Big Data and Big Data enabled tools", this HCBDR breakout group framed an initial context to ground the discussions to follow. The evaluation methodology must cover the triple of Organizational Context, Analyst(s) and Data/Tools all surrounding the Mission Needs. The instrumentation and measurement methodology must consider the holistic combination of all of these elements. Further, it must both measure what *is* occurring at those elements, as well as what is missing from each (felt to be a much more difficult task). The final recommendations include a blend of technical indicators (e.g. system session log information) as well as a sophisticated mix of ethnographic observations important to fully understanding the cognitive performance of the analyst and technology together. Several key issues (such as analysis quality assessment) remain unanswered.},
booktitle = {Proceedings of the 2014 Workshop on Human Centered Big Data Research},
pages = {59–63},
numpages = {5},
keywords = {User Evaluation, Big Data, Analysis, Workshop, Cognitive Systems Engineering, Ethnographic, Methodology, Joint Cognitive System, Human Centered, Technology Enabled Tradecraft},
location = {Raleigh, NC, USA},
series = {HCBDR '14}
}

@inproceedings{10.1145/2389686.2389688,
author = {Megler, V. M. and Maier, David},
title = {When Big Data Leads to Lost Data},
year = {2012},
isbn = {9781450317191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2389686.2389688},
doi = {10.1145/2389686.2389688},
abstract = {For decades, scientists bemoaned the scarcity of observational data to analyze and against which to test their models. Exponential growth in data volumes from ever-cheaper environmental sensors has provided scientists with the answer to their prayers: "big data". Now, scientists face a new challenge: with terabytes, petabytes or exabytes of data at hand, stored in thousands of heterogeneous datasets, how can scientists find the datasets most relevant to their research interests? If they cannot find the data, then they may as well never have collected it; that data is lost to them. Our research addresses this challenge, using an existing scientific archive as our test-bed. We approach this problem in a new way: by adapting Information Retrieval techniques, developed for searching text documents, into the world of (primarily numeric) scientific data. We propose an approach that uses a blend of automated and "semi-curated" methods to extract metadata from large archives of scientific data. We then perform searches over the extracted metadata, returning results ranked by similarity to the query terms. We briefly describe an implementation performed at an ocean observatory to validate the proposed approach. We propose performance and scalability research to explore how continued archive growth will affect our goal of interactive response, no matter the scale.},
booktitle = {Proceedings of the 5th Ph.D. Workshop on Information and Knowledge},
pages = {1–8},
numpages = {8},
keywords = {scientific data, ranked data search},
location = {Maui, Hawaii, USA},
series = {PIKM '12}
}

@inproceedings{10.1145/3170521.3170535,
author = {Rastogi, Avnish Kumar and Narang, Nitin and Siddiqui, Zamir Ahmad},
title = {Imbalanced Big Data Classification: A Distributed Implementation of SMOTE},
year = {2018},
isbn = {9781450363976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170521.3170535},
doi = {10.1145/3170521.3170535},
abstract = {In the domain of machine learning, quality of data is most critical component for building good models. Predictive analytics is an AI stream used to predict future events based on historical learnings and is used in diverse fields like predicting online frauds, oil slicks, intrusion attacks, credit defaults, prognosis of disease cells etc. Unfortunately, in most of these cases, traditional learning models fail to generate required results due to imbalanced nature of data. Here imbalance denotes small number of instances belonging to the class under prediction like fraud instances in the total online transactions. The prediction in imbalanced classification gets further limited due to factors like small disjuncts which get accentuated during the partitioning of data when learning at scale. Synthetic generation of minority class data (SMOTE [<u>1</u>]) is one pioneering approach by Chawla [<u>1</u>] to offset said limitations and generate more balanced datasets. Although there exists a standard implementation of SMOTE in python, it is unavailable for distributed computing environments for large datasets. Bringing SMOTE to distributed environment under spark is the key motivation for our research. In this paper we present our algorithm, observations and results for synthetic generation of minority class data under spark using Locality Sensitivity Hashing [LSH]. We were able to successfully demonstrate a distributed version of Spark SMOTE which generated quality artificial samples preserving spatial distribution1.},
booktitle = {Proceedings of the Workshop Program of the 19th International Conference on Distributed Computing and Networking},
articleno = {14},
numpages = {6},
keywords = {locality sensitivity hashing, map reduce, nearest neighbors, spark, SMOTE, imbalanced classification},
location = {Varanasi, India},
series = {Workshops ICDCN '18}
}

@inproceedings{10.1145/2658840.2658842,
author = {Bonifati, Angela and Ciucanu, Radu and Lemay, Aur\'{e}lien and Staworko, S\l{}awek},
title = {A Paradigm for Learning Queries on Big Data},
year = {2014},
isbn = {9781450331869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658840.2658842},
doi = {10.1145/2658840.2658842},
abstract = {Specifying a database query using a formal query language is typically a challenging task for non-expert users. In the context of big data, this problem becomes even harder as it requires the users to deal with database instances of big sizes and hence difficult to visualize. Such instances usually lack a schema to help the users specify their queries, or have an incomplete schema as they come from disparate data sources. In this paper, we propose a novel paradigm for interactive learning of queries on big data, without assuming any knowledge of the database schema. The paradigm can be applied to different database models and a class of queries adequate to the database model. In particular, in this paper we present two instantiations that validated the proposed paradigm for learning relational join queries and for learning path queries on graph databases. Finally, we discuss the challenges of employing the paradigm for further data models and for learning cross-model schema mappings.},
booktitle = {Proceedings of the First International Workshop on Bringing the Value of "Big Data" to Users (Data4U 2014)},
pages = {7–12},
numpages = {6},
keywords = {learning, user interactions, big data, Query inference},
location = {Hangzhou, China},
series = {Data4U '14}
}

@inproceedings{10.1145/2644866.2644870,
author = {Schmitz, Patrick and Pearce, Laurie},
title = {Humanist-Centric Tools for Big Data: Berkeley Prosopography Services},
year = {2014},
isbn = {9781450329491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2644866.2644870},
doi = {10.1145/2644866.2644870},
abstract = {In this paper, we describe Berkeley Prosopography Services (BPS), a new set of tools for prosopography - the identification of individuals and study of their interactions - in support of humanities research. Prosopography is an example of "big data" in the humanities, characterized not by the size of the datasets, but by the way that computational and data-driven methods can transform scholarly workflows. BPS is based upon re-usable infrastructure, supporting generalized web services for corpus management, social network analysis, and visualization. The BPS disambiguation model is a formal implementation of the traditional heuristics used by humanists, and supports plug-in rules for adaptation to a wide range of domain corpora. A workspace model supports exploratory research and collaboration. We contrast the BPS model of configurable heuristic rules to other approaches for automated text analysis, and explain how our model facilitates interpretation by humanist researchers. We describe the significance of the BPS assertion model in which researchers assert conclusions or possibilities, allowing them to override automated inference, to explore ideas in what-if scenarios, and to formally publish and subscribe-to asserted annotations among colleagues, and/or with students. We present an initial evaluation of researchers' experience using the tools to study corpora of cuneiform tablets, and describe plans to expand the application of the tools to a broader range of corpora.},
booktitle = {Proceedings of the 2014 ACM Symposium on Document Engineering},
pages = {179–188},
numpages = {10},
keywords = {cyberinfrastructure, digital humanities, prosopography, social network analysis, web-services, assertions, big data, annotation},
location = {Fort Collins, Colorado, USA},
series = {DocEng '14}
}

@inproceedings{10.1145/3221269.3221294,
author = {Porto, Fabio and Rittmeyer, Jo\~{a}o N. and Ogasawara, Eduardo and Krone-Martins, Alberto and Valduriez, Patrick and Shasha, Dennis},
title = {Point Pattern Search in Big Data},
year = {2018},
isbn = {9781450365055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3221269.3221294},
doi = {10.1145/3221269.3221294},
abstract = {Consider a set of points P in space with at least some of the pairwise distances specified. Given this set P, consider the following three kinds of queries against a database D of points : (i) pure constellation query: find all sets S in D of size |P| that exactly match the pairwise distances within P up to an additive error ϵ; (ii) isotropic constellation queries: find all sets S in D of size |P| such that there exists some scale factor f for which the distances between pairs in S exactly match f times the distances between corresponding pairs of P up to an additive ϵ; (iii) non-isotropic constellation queries: find all sets S in D of size |P| such that there exists some scale factor f and for at least some pairs of points, a maximum stretch factor mi,j &gt; 1 such that (f X mi,jXdist(pi, pj))+ϵ &gt; dist(si,sj) &gt; (f X dist(pi, pj)) - ϵ. Finding matches to such queries has applications to spatial data in astronomical, seismic, and any domain in which (approximate, scale-independent) geometrical matching is required. Answering the isotropic and non-isotropic queries is challenging because scale factors and stretch factors may take any of an infinite number of values. This paper proposes practically efficient sequential and distributed algorithms for pure, isotropic, and non-isotropic constellation queries. As far as we know, this is the first work to address isotropic and non-isotropic queries.},
booktitle = {Proceedings of the 30th International Conference on Scientific and Statistical Database Management},
articleno = {21},
numpages = {12},
keywords = {big data, geometrical patterns, pattern search, distance matching, isotropic, spatial patterns, point set registration},
location = {Bozen-Bolzano, Italy},
series = {SSDBM '18}
}

@article{10.14778/2733004.2733045,
author = {Cao, Lei and Wang, Qingyang and Rundensteiner, Elke A.},
title = {Interactive Outlier Exploration in Big Data Streams},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733045},
doi = {10.14778/2733004.2733045},
abstract = {We demonstrate our VSOutlier system for supporting interactive exploration of outliers in big data streams. VSOutlier not only supports a rich variety of outlier types supported by innovative and efficient outlier detection strategies, but also provides a rich set of interactive interfaces to explore outliers in real time. Using the stock transactions dataset from the US stock market and the moving objects dataset from MITRE, we demonstrate that the VSOutlier system enables analysts to more efficiently identify, understand, and respond to phenomena of interest in near real-time even when applied to high volume streams.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1621–1624},
numpages = {4}
}

@inproceedings{10.1145/3110025.3119402,
author = {Ahmed, Mohiuddin and Choudhury, Nazim and Uddin, Shahadat},
title = {Anomaly Detection on Big Data in Financial Markets},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3119402},
doi = {10.1145/3110025.3119402},
abstract = {In the modern financial market, market participants use big data analytics to gain valuable insight on historical market data for better decision making. Complying with the three vs (i.e., velocity, volume and variety) of big data, the financial market is considered as a complex system comprised of many interacting high-frequency traders those make decisions based on the relative strengths of these interactions. Researchers have put substantial scholarly input to deal with these anomalies. From the big data perspective, anomaly detection in financial data has widely been ignored despite many organisations store, process and disseminate financial market data for interested customers to assist them to make informed decision abd create competitive advantages. Considering the presence of anomalies in voluminous data from myriad data sources may generate catastrophic decision through misunderstandings of market behaviour. Therefore, in this study, we applied a standard set of anomaly detection techniques, used in big data based on nearest-neighbours, clustering and statistical approaches, to detect rare anomalies present within the historical daily trading information for five years (i.e., 2009--2013) for each stock listed on the Australian Security Exchange (ASX). We also measured the performance of these anomaly detection techniques using a number of metrics to highlight the best performing algorithm. The experimental results suggest that the LOF(Local Outlier Factor) and CMGOS(Clustering-based Multivariate Gaussian Outlier Score) are the best performing anomaly detection techniques.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {998–1001},
numpages = {4},
keywords = {Financial Markets, Financial Big Data, Anomaly Detection},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3127479.3132685,
author = {Yang, Chen and Guo, Qi and Meng, Xiaofeng and Xin, Rihui and Wang, Chunkai},
title = {Revisiting Performance in Big Data Systems: An Resource Decoupling Approach},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3132685},
doi = {10.1145/3127479.3132685},
abstract = {Big data systems for large-scale data processing are now in widespread use. To improve their performance, both academia and industry have expended a great deal of effort in the analysis of performance bottlenecks. Most big data systems, as Hadoop and Spark, allow distributed computing across clusters. As a result, the execution of systems always parallelizes the use of the CPU, memory, disk and network. If a given resource has the greatest limiting impact on performance, systems will be bottlenecked on it. For a system designer, it is effective for the improvement of performance to tune the bottleneck resource. The key point for the aforementioned scenario is how to determine the bottleneck resource. The nature clue is to quantify the impact of the four major components and identify one causing the greatest impact factor as the bottleneck resource.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {639},
numpages = {1},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/2968456.2976765,
author = {Neshatpour, Katayoun and Sasan, Avesta and Homayoun, Houman},
title = {Big Data Analytics on Heterogeneous Accelerator Architectures},
year = {2016},
isbn = {9781450344838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968456.2976765},
doi = {10.1145/2968456.2976765},
abstract = {In this paper, we present the implementation of big data analytics applications in a heterogeneous CPU+FPGA accelerator architecture. We develop the MapReduce implementation of K-means, K nearest neighbor, support vector machine and Naive Bayes in a Hadoop Streaming environment that allows developing mapper/reducer functions in a non-Java based language suited for interfacing with FPGA-based hardware accelerating environment. We present a full implementation of the HW+SW mappers on the Zynq FPGA platform. A promising speedup as well as energy-efficiency gains of upto 4.5X and 22X is achieved, respectively, in an end-to-end Hadoop implementation.},
booktitle = {Proceedings of the Eleventh IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis},
articleno = {16},
numpages = {3},
location = {Pittsburgh, Pennsylvania},
series = {CODES '16}
}

@article{10.14778/2733004.2733037,
author = {Lei, Chuan and Zhuang, Zhongfang and Rundensteiner, Elke A. and Eltabakh, Mohamed Y.},
title = {Redoop Infrastructure for Recurring Big Data Queries},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733037},
doi = {10.14778/2733004.2733037},
abstract = {This demonstration presents the Redoop infrastructure, the first full-fledged MapReduce framework with native support for recurring big data queries. Recurring queries, repeatedly being executed for long periods of time over evolving high-volume data, have become a bedrock component in most large-scale data analytic applications. Redoop is a comprehensive extension to Hadoop that pushes the support and optimization of recurring queries into Hadoop's core functionality. While backward compatible with regular MapReduce jobs, Redoop achieves an order of magnitude better performance than Hadoop for recurring workloads. Redoop employs innovative window-aware optimization techniques for such recurring workloads including adaptive window-aware data partitioning, cache-aware task scheduling, and inter-window caching mechanisms. We will demonstrate Redoop's capabilities on a compute cluster against real life workloads including click-stream and sensor data analysis.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1589–1592},
numpages = {4}
}

@inproceedings{10.1145/2612733.2619954,
author = {Villase\~{n}or, Elio and Estrada, Hugo},
title = {Informetric Mapping of "Big Data" in FI-WARE},
year = {2014},
isbn = {9781450329019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2612733.2619954},
doi = {10.1145/2612733.2619954},
abstract = {Today, governmental entities are embracing new trends in information technology. One of the technological developments that generate more excitement is Big Data; because this technology let us analyze the huge amount of information produced by the government and is useful for decisions making. On the other hand, the Future Internet platform of the European Community (FI- WARE) is one of the most powerful trends around the world and has aroused more interest in governments. This technology is based on a set of Generic Enablers (GE) for various applications, including Big Data. The FI-WARE is a platform under construction and knowing how this process performed is essential to join in this monumental effort and take advantages of its benefits. This document presents the results of the application of text and data mining techniques as well as informetric mapping to gain understanding regarding the development of Big Data technology present in the FI- WARE.},
booktitle = {Proceedings of the 15th Annual International Conference on Digital Government Research},
pages = {348–349},
numpages = {2},
keywords = {informetric analysis, FI-ware, generic enablers, big data},
location = {Aguascalientes, Mexico},
series = {dg.o '14}
}

@inproceedings{10.1145/3041021.3053063,
author = {Zhang, Da and Kabuka, Mansur R.},
title = {Top-K Entity Units Retrieval Over Big Data},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3053063},
doi = {10.1145/3041021.3053063},
abstract = {During the past several years, data size has increased explosively. This data explosion tendency has impacted various fields ranging from biomedical engineering, business consulting to social media and mobile application. Big Data is a two sided sword. While it provides incredibly treasured insights in commercial scope and innovative discovery in the scientific field, Big Data also has many challenges, such as complication in data storage, data processing, data analysis and data visualization. Among all these challenges, keyword searching over a large volume of data prevails as one of the four tasks defined by Bizer et al. at the year of 2012. Keyword searching refers to retrieving the objects relevant to the entities of concern using scientific computational methods. Consequently, efficiently solving the problem of keyword searching can contribute as a foundation to diverse Big Data applications.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1269–1272},
numpages = {4},
keywords = {keyword searching, information retrieval, big data},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@article{10.1145/3278607,
author = {Song, Qingquan and Ge, Hancheng and Caverlee, James and Hu, Xia},
title = {Tensor Completion Algorithms in Big Data Analytics},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3278607},
doi = {10.1145/3278607},
abstract = {Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jan},
articleno = {6},
numpages = {48},
keywords = {multilinear data analysis, tensor decomposition, dynamic data analysis, Tensor, tensor completion, tensor factorization, big data analytics}
}

@inproceedings{10.1145/2928294.2928302,
author = {Tatu, Marta and Werner, Steven and Balakrishna, Mithun and Erekhinskaya, Tatiana and Moldovan, Dan},
title = {Semantic Question Answering on Big Data},
year = {2016},
isbn = {9781450342995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2928294.2928302},
doi = {10.1145/2928294.2928302},
abstract = {This article describes a high-precision semantic question answering (SQA) engine for large datasets. We employ an RDF store to index the semantic information extracted from large document collections and a natural language to SPARQL conversion module to find desired information. In order to be able to find answers to complex questions in structured/unstructured data resources, our system produces rich semantic structures from the data resources and then transforms the extracted knowledge into an RDF representation. In order to facilitate easy access to the information stored in the RDF semantic index, our system accepts a user's natural language questions, translates them into SPARQL queries and returns a precise answer back to the user. Our improvements in performance over a regular free text search index-based question answering engine prove that SQA can benefit greatly from the addition and consumption of deep semantic information.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {10},
numpages = {6},
keywords = {RDF, question answering, unstructured data, SPARQL},
location = {San Francisco, California},
series = {SBD '16}
}

@inproceedings{10.1145/3102254.3102271,
author = {Cuzzocrea, Alfredo and Loia, Vincenzo and Tommasetti, Aurelio},
title = {Big-Data-Driven Innovation for Enterprises: Innovative Big Value Paradigms for next-Generation Digital Ecosystems},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102271},
doi = {10.1145/3102254.3102271},
abstract = {Among the various interpretations and meanings of the well-known Vs (Volume, Velocity, Variety) of Big Data, V as Value represents the most significant and critical innovation for enterprises, which are a well-known case of digital ecosystems. The key issue for big enterprise data consists in extracting knowledge for creating new value and innovation for the target enterprise. Therefore, the big data analytics phase plays a critical role to this end. Following these considerations, in this paper we provide the following three contributions: (i) an overview of most relevant proposals in the context of big data innovation for enterprises; (ii) a reference architecture for supporting advanced big data analytics over big enterprise data; (iii) a discussion on future challenges in the context of big data innovation for enterprises.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {35},
numpages = {5},
keywords = {big enterprise data, big value, big data analytics, big data, big data prediction, digital ecosystems},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.5555/2819289.2819294,
author = {Madhavji, Nazim H. and Miranskyy, Andriy and Kontogiannis, Kostas},
title = {Big Picture of Big Data Software Engineering: With Example Research Challenges},
year = {2015},
publisher = {IEEE Press},
abstract = {In the rapidly growing field of Big Data, we note that a disproportionately larger amount of effort is being invested in infrastructure development and data analytics in comparison to applications software development -- approximately a 80:20 ratio. This prompted us to create a context model of Big Data Software Engineering (BDSE) containing various elements --- such as development practice, Big Data systems, corporate decision-making, and research --- and their relationships. The model puts into perspective where various types of stakeholders fit in. From the research perspective, we describe example challenges in BDSE, specifically requirements, architectures, and testing and maintenance.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {11–14},
numpages = {4},
keywords = {research challenges, context model, software engineering, big data, applications},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@inproceedings{10.1145/2025528.2025537,
author = {Khan, Azam and Hornb\ae{}k, Kasper},
title = {Big Data from the Built Environment},
year = {2011},
isbn = {9781450309240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025528.2025537},
doi = {10.1145/2025528.2025537},
abstract = {As sensor networks in buildings continue to grow in number and heterogeneity, occupants can become empowered to better control their environment for comfort maximization and energy minimization. Since buildings are the primary consumers of energy and are the dominant cause of greenhouse gases, apps that help occupants to understand and control their interactions with a building could be extremely beneficial to society. However, the massive raw data sets that could be collected must be aggregated and visualized to be usable which presents significant data handling, information visualization, and interaction challenges. In the context of Project Dasher, a prototype building site for exploring these issues, we discuss lessons learned and challenges ahead to develop ubiquitous computing support for sustainability.},
booktitle = {Proceedings of the 2nd International Workshop on Research in the Large},
pages = {29–32},
numpages = {4},
keywords = {data aggregation, massive data sets, sustainability, augmented reality, building information model, app},
location = {Beijing, China},
series = {LARGE '11}
}

@inproceedings{10.1109/CCGrid.2015.85,
author = {Chen, Peng and Plale, Beth},
title = {Big Data Provenance Analysis and Visualization},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.85},
doi = {10.1109/CCGrid.2015.85},
abstract = {Provenance captured from E-Science experimentation is often large and complex, for instance, from agent-based simulations that have tens of thousands of heterogeneous components interacting over extended time periods. The subject of study of my dissertation is the use of E-Science provenance at scale. My initial research studied the visualization of large provenance graphs and proposed an abstract representation of provenance that supports useful data mining. Recent work involves analyzing large provenance data generated from agent-based simulations on a single machine. In continuation, I propose stream processing techniques to support the continuous and realtime analysis of data provenance, which is captured from agent based simulations on HPC and thus has unprecedented volume and complexity.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {797–800},
numpages = {4},
keywords = {stream processing, big data, mining, data provenance, visualization},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/2659532.2659593,
author = {Radenski, Atanas},
title = {Big Data, High-Performance Computing, and MapReduce},
year = {2014},
isbn = {9781450327534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659532.2659593},
doi = {10.1145/2659532.2659593},
abstract = {We discuss the emergence of data-intensive computing and then explore the applicability of business-oriented big-data platforms, such as Hadoop MapReduce, to traditional scientific computing processes. In particular, we investigate the suitability of MapReduce parallelism for simulation of grid-based models by developing message-passing MapReduce algorithms and empirically evaluating their performance on the Amazon's Elastic MapReduce cloud. We outline MapReduce challenges (such as insufficient speed) and opportunities (such as fault-tolerance and ease of use) in scientific computing.},
booktitle = {Proceedings of the 15th International Conference on Computer Systems and Technologies},
pages = {13–24},
numpages = {12},
keywords = {relaxation, Hadoop, life simulation, partitioning, MapReduce, local aggregation},
location = {Ruse, Bulgaria},
series = {CompSysTech '14}
}

@inproceedings{10.1145/2808719.2816984,
author = {Deng, Xin and Wu, Donghui},
title = {Big Data and Predictive Modeling Topics in Healthcare},
year = {2015},
isbn = {9781450338530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808719.2816984},
doi = {10.1145/2808719.2816984},
abstract = {This panel discussion will first review a few current big data and predictive modeling topics in healthcare in both providers and payers marketspace, including the problems, current status and challenges, and opportunities for big data and future trends. Follow the opening remarks, the panelists will provide their own insights and point of views on some of the topics, and interactive discussion among themselves and audience. Among many of the topics, a few examples are: Improve quality outcomes through analytics and predictive modeling; Reduce and prevent hospital re-admission through risk predictions and case management; Discover Abuse, Waste and Fraud in Healthcare Providers; Improve population health through personalized interventions; Understand the population signed through Health Insurance Exchange Marketplace (HIX); Population Risk Adjustment; Improve CMS STARS rating; Provider Quality Measurement and Regulator Reporting; Provider Risk and Revenue management; Unified Patient Record, Big Data Challenges for Providers and Payers, etc.},
booktitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {677},
numpages = {1},
keywords = {data integration, big data platform, health informatics, population health management, predictive analytics},
location = {Atlanta, Georgia},
series = {BCB '15}
}

@inproceedings{10.1145/3292500.3340400,
author = {Heckerman, David},
title = {Exploiting High Dimensionality in Big Data},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3340400},
doi = {10.1145/3292500.3340400},
abstract = {There are two aspects of data that make them big: sample size and dimensionality. The advantages of large sample size have long been touted. In contrast, high dimensionality has typically been seen as an obstacle to successful analysis. In this talk, using the area of genomics as an example, I will illustrate some of the advantages of high dimensionality.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3172},
numpages = {1},
keywords = {Invited Talk},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1145/2627534.2627556,
author = {Madan, Bharat B. and Banik, Manoj},
title = {Attack Tolerant Architecture for Big Data File Systems},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627556},
doi = {10.1145/2627534.2627556},
abstract = {Data driven decisions derived from big data have become critical in many application domains, fueling the demand for collection, transportation, storage and processing of massive volumes of data. Such applications have made data a valuable resource that needs to be provided appropriate security. High value associated with big data sets has rendered big data storage systems attractive targets for cyber attackers, whose goal is to compromise the Confidentiality, Integrity and Availability of data and information. Common defense strategy for protecting cyber assets has been to first take preventive measures, and if these fail, detecting intrusions and finally recovery. Unfortunately, attackers have developed tremendous technical sophistication to defeat most defensive mechanisms. Alternative strategy is to design architectures which are intrinsically attack tolerant. This paper describes a technique that involves eliminating single point of security failures through fragmentation, coding, dispersion and reassembly. It is shown that this technique can be successfully applied to routing, networked storage systems, and big data file systems to make them attack tolerant.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {65–69},
numpages = {5},
keywords = {data availability, data integrity, data confidentiality, attack tolerance, secure storage, big-data security}
}

@inproceedings{10.1145/3209281.3209360,
author = {Rashed, Alaa Hussain and Karakaya, Ziya and Yazici, Ali},
title = {Big Data on Cloud for Government Agencies: Benefits, Challenges, and Solutions},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209360},
doi = {10.1145/3209281.3209360},
abstract = {Big Data and Cloud computing are the most important technologies that give the opportunity for government agencies to gain a competitive advantage and improve their organizations. On one hand, Big Data implementation requires investing a significant amount of money in hardware, software, and workforce. On the other hand, Cloud Computing offers an unlimited, scalable and on-demand pool of resources which provide the ability to adopt Big Data technology without wasting on the financial resources of the organization and make the implementation of Big Data faster and easier. The aim of this study is to conduct a systematic literature review in order to collect data to identify the benefits and challenges of Big Data on Cloud for government agencies and to make a clear understanding of how combining Big Data and Cloud Computing help to overcome some of these challenges. The last objective of this study is to identify the solutions for related challenges of Big Data. Four research questions were designed to determine the information that is related to the objectives of this study. Data is collected using literature review method and the results are deduced from there.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {8},
numpages = {9},
keywords = {challenges, benefits, cloud computing, big data, big data on cloud, government agencies, solutions},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/2351316.2351329,
author = {Hoi, Steven C. H. and Wang, Jialei and Zhao, Peilin and Jin, Rong},
title = {Online Feature Selection for Mining Big Data},
year = {2012},
isbn = {9781450315470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351316.2351329},
doi = {10.1145/2351316.2351329},
abstract = {Most studies of online learning require accessing all the attributes/features of training instances. Such a classical setting is not always appropriate for real-world applications when data instances are of high dimensionality or the access to it is expensive to acquire the full set of attributes/features. To address this limitation, we investigate the problem of Online Feature Selection (OFS) in which the online learner is only allowed to maintain a classifier involved a small and fixed number of features. The key challenge of Online Feature Selection is how to make accurate prediction using a small and fixed number of active features. This is in contrast to the classical setup of online learning where all the features are active and can be used for prediction. We address this challenge by studying sparsity regularization and truncation techniques. Specifically, we present an effective algorithm to solve the problem, give the theoretical analysis, and evaluate the empirical performance of the proposed algorithms for online feature selection on several public datasets. We also demonstrate the application of our online feature selection technique to tackle real-world problems of big data mining, which is significantly more scalable than some well-known batch feature selection algorithms. The encouraging results of our experiments validate the efficacy and efficiency of the proposed techniques for large-scale applications.},
booktitle = {Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
pages = {93–100},
numpages = {8},
keywords = {online learning, feature selection, classification},
location = {Beijing, China},
series = {BigMine '12}
}

@inproceedings{10.1145/2872518.2890512,
author = {Giles, C. Lee},
title = {Scholarly Big Data Knowledge and Semantics},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890512},
doi = {10.1145/2872518.2890512},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {371},
numpages = {1},
keywords = {digital libraries, semantic search, big data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/3352700.3352709,
author = {Je\v{r}\'{a}bek, Kamil and Ry\v{s}av\'{y}, Ond\v{r}ej},
title = {Big Data Network Flow Processing Using Apache Spark},
year = {2019},
isbn = {9781450376365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352700.3352709},
doi = {10.1145/3352700.3352709},
abstract = {The increasing amount of traffic flows captured as a part of network monitoring activities makes the analysis more complicated. One of the goals for network traffic analysis is to identify malicious communication. In the paper, we present a new system for big data network flow classification and clustering. The proposed system is based on the popular big data engines such as Apache Spark and Apache Ignite. The conducted experiments demonstrate the feasibility of the proposed approach and show the possible scalability.},
booktitle = {Proceedings of the 6th Conference on the Engineering of Computer Based Systems},
articleno = {9},
numpages = {9},
keywords = {Big Data, Cassandra, Network flows, Apache Spark, Apache Ignite},
location = {Bucharest, Romania},
series = {ECBS '19}
}

@inproceedings{10.1145/3184407.3184420,
author = {Ardagna, Danilo and Barbierato, Enrico and Evangelinou, Athanasia and Gianniti, Eugenio and Gribaudo, Marco and Pinto, T\'{u}lio B. M. and Guimar\~{a}es, Anna and Couto da Silva, Ana Paula and Almeida, Jussara M.},
title = {Performance Prediction of Cloud-Based Big Data Applications},
year = {2018},
isbn = {9781450350952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3184407.3184420},
doi = {10.1145/3184407.3184420},
abstract = {Data heterogeneity and irregularity are key characteristics of big data applications that often overwhelm the existing software and hardware infrastructures. In such context, the exibility and elasticity provided by the cloud computing paradigm over a natural approach to cost-effectively adapting the allocated resources to the application's current needs. Yet, the same characteristics impose extra challenges to predicting the performance of cloud-based big data applications, a central step in proper management and planning. This paper explores two modeling approaches for performance prediction of cloud-based big data applications. We evaluate a queuing-based analytical model and a novel fast ad-hoc simulator in various scenarios based on different applications and infrastructure setups. Our results show that our approaches can predict average application execution times with 26% relative error in the very worst case and about 12% on average. Moreover, our simulator provides performance estimates 70 times faster than state of the art simulation tools.},
booktitle = {Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {192–199},
numpages = {8},
keywords = {performance modeling, spark, big data, approximate methods, simulation},
location = {Berlin, Germany},
series = {ICPE '18}
}

@article{10.1145/253769.253804,
author = {Strong, Diane M. and Lee, Yang W. and Wang, Richard Y.},
title = {Data Quality in Context},
year = {1997},
issue_date = {May 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/253769.253804},
doi = {10.1145/253769.253804},
journal = {Commun. ACM},
month = {may},
pages = {103–110},
numpages = {8}
}

@inproceedings{10.1145/2433396.2433459,
author = {Yang, Qiang},
title = {Big Data, Lifelong Machine Learning and Transfer Learning},
year = {2013},
isbn = {9781450318693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2433396.2433459},
doi = {10.1145/2433396.2433459},
abstract = {A major challenge in today's world is the Big Data problem, which manifests itself in Web and Mobile domains as rapidly changing and heterogeneous data streams. A data-mining system must be able to cope with the influx of changing data in a continual manner. This calls for Lifelong Machine Learning, which in contrast to the traditional one-shot learning, should be able to identify the learning tasks at hand and adapt to the learning problems in a sustainable manner. A foundation for lifelong machine learning is transfer learning, whereby knowledge gained in a related but different domain may be transferred to benefit learning for a current task. To make effective transfer learning, it is important to maintain a continual and sustainable channel in the life time of a user in which the data are annotated. In this talk, I outline the lifelong machine learning situations, give several examples of transfer learning and applications for lifelong machine learning, and discuss cases of successful extraction of data annotations to meet the Big Data challenge.},
booktitle = {Proceedings of the Sixth ACM International Conference on Web Search and Data Mining},
pages = {505–506},
numpages = {2},
keywords = {lifelong machine learning, transfer learning, big data},
location = {Rome, Italy},
series = {WSDM '13}
}

@inproceedings{10.1145/2508859.2512502,
author = {Reznik, Leon and Bertino, Elisa},
title = {POSTER: Data Quality Evaluation: Integrating Security and Accuracy},
year = {2013},
isbn = {9781450324779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508859.2512502},
doi = {10.1145/2508859.2512502},
abstract = {Data quality (DQ) is essential to achieve data trustworthiness, as it assures that data is free of errors, complete, and consistent. This paper proposes an approach to evaluate DQ in multichannel sensor networks and systems with heterogeneous data sources. The approach integrates various DQ indicators ranging from traditional data accuracy metrics to network security and business performance measures. It demonstrates the advantage of including security metrics into the DQ evaluation for the design optimization of data fusion procedures and even the whole data collection and communication systems. The DQ metrics composition and calculus are discussed. However, the major attention is paid to the analysis of the relationship between conventional data accuracy metrics and network security indicators.},
booktitle = {Proceedings of the 2013 ACM SIGSAC Conference on Computer &amp; Communications Security},
pages = {1367–1370},
numpages = {4},
keywords = {computer security evaluation, data quality, data fusion., data accuracy},
location = {Berlin, Germany},
series = {CCS '13}
}

@inproceedings{10.1145/2745844.2745889,
author = {Mirhoseini, Azalia and Songhori, Ebrahim M. and Darvish Rouhani, Bita and Koushanfar, Farinaz},
title = {Flexible Transformations For Learning Big Data},
year = {2015},
isbn = {9781450334860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745844.2745889},
doi = {10.1145/2745844.2745889},
abstract = {This paper proposes a domain-specific solution for iterative learning of big and dense (non-sparse) datasets. A large host of learning algorithms, including linear and regularized regression techniques, rely on iterative updates on the data connectivity matrix in order to converge to a solution. The performance of such algorithms often severely degrade when it comes to large and dense data. Massive dense datasets not only induce obligatory large number of arithmetics, but they also incur unwanted message passing cost across the processing nodes. Our key observation is that despite the seemingly dense structures, in many applications, data can be transformed into a new space where sparse structures become revealed. We propose a scalable data transformation scheme that enables creating versatile sparse representations of the data. The transformation can be tuned to benefit the underlying platform's cost and constraints. Our evaluations demonstrate significant improvement in energy usage, runtime, and mem},
booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
pages = {453–454},
numpages = {2},
keywords = {performance optimization, sparse factorization, subspace sampling, big and dense data},
location = {Portland, Oregon, USA},
series = {SIGMETRICS '15}
}

@article{10.1145/2796314.2745889,
author = {Mirhoseini, Azalia and Songhori, Ebrahim M. and Darvish Rouhani, Bita and Koushanfar, Farinaz},
title = {Flexible Transformations For Learning Big Data},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2796314.2745889},
doi = {10.1145/2796314.2745889},
abstract = {This paper proposes a domain-specific solution for iterative learning of big and dense (non-sparse) datasets. A large host of learning algorithms, including linear and regularized regression techniques, rely on iterative updates on the data connectivity matrix in order to converge to a solution. The performance of such algorithms often severely degrade when it comes to large and dense data. Massive dense datasets not only induce obligatory large number of arithmetics, but they also incur unwanted message passing cost across the processing nodes. Our key observation is that despite the seemingly dense structures, in many applications, data can be transformed into a new space where sparse structures become revealed. We propose a scalable data transformation scheme that enables creating versatile sparse representations of the data. The transformation can be tuned to benefit the underlying platform's cost and constraints. Our evaluations demonstrate significant improvement in energy usage, runtime, and mem},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {453–454},
numpages = {2},
keywords = {performance optimization, sparse factorization, big and dense data, subspace sampling}
}

@article{10.1145/3419634,
author = {Bansal, Maggi and Chana, Inderveer and Clarke, Siobh\'{a}n},
title = {A Survey on IoT Big Data: Current Status, 13 V’s Challenges, and Future Directions},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3419634},
doi = {10.1145/3419634},
abstract = {Driven by the core technologies, i.e., sensor-based autonomous data acquisition and the cloud-based big data analysis, IoT automates the actuation of data-driven intelligent actions on the connected objects. This automation enables numerous useful real-life use-cases, such as smart transport, smart living, smart cities, and so on. However, recent industry surveys reflect that data-related challenges are responsible for slower growth of IoT in recent years. For this reason, this article presents a systematic and comprehensive survey on IoT Big Data (IoTBD) with the aim to identify the uncharted challenges for IoTBD. This article analyzes the state-of-the-art academic works in IoT and big data management across various domains and proposes a taxonomy for IoTBD management. Then, the survey explores the IoT portfolio of major cloud vendors and provides a classification of vendor services for the integration of IoT and IoTBD on their cloud platforms. After that, the survey identifies the IoTBD challenges in terms of 13 V’s challenges and envisions IoTBD as “Big Data 2.0.” Then the survey provides comprehensive analysis of recent works that address IoTBD challenges by highlighting their strengths and weaknesses to assess the recent trends and future research directions. Finally, the survey concludes with discussion on open research issues for IoTBD.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {131},
numpages = {59},
keywords = {big data 2.0, V’s challenges for IoT big data, IoT big data survey, cloud computing in IoT, cloud IoT services, IoT big data}
}

@article{10.1145/2908216.2908222,
author = {Geslevich-Packin, Nizan and Lev-Aretz, Yafit},
title = {Big Data and Social Netbanks: What Happens When Tech Companies Become Financial Companies?},
year = {2016},
issue_date = {0March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0095-2737},
url = {https://doi.org/10.1145/2908216.2908222},
doi = {10.1145/2908216.2908222},
abstract = {Social netbanks hold much potential as alternative financial institutions that could better serve marginalized populations. But they bring with them a number of regulatory complications and informational risks. We explore the rise of big data and social netbanks, and describe some of the challenges they present.},
journal = {SIGCAS Comput. Soc.},
month = {mar},
pages = {36–40},
numpages = {5},
keywords = {banks, social networks, netbanks, regulation}
}

@inproceedings{10.1145/3434581.3434721,
author = {Luo, Liangfu},
title = {Design of Big Data Algorithm Based on MapReduce},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434721},
doi = {10.1145/3434581.3434721},
abstract = {With the widespread application of Internet technology, the utilization rate of similar blogs and social networks has been significantly improved. With the intervention of cloud computing and other technologies, a large amount of data generated during browsing the Internet can be effectively accumulated and processed, which indicates that big data has been integrated into people's daily life. MapReduce, as a parallel programming environment, can effectively deal with big data related problems, and has been applied in large Internet companies such as Google and Amazon Maxdiff, an efficient histogram algorithm based on MapReduce, includes accurate algorithm and approximate algorithm, and proposes two-table equijoin algorithm and multistandard equijoin algorithm for data skew And optimize the efficiency of the connection algorithm when one or more data in the data set appear too much. This paper starts with the efficiency optimization of big data connection algorithm based on MapReduce, and studies the efficiency optimization of equijoin algorithm, data skew connection algorithm and connection algorithm in detail, and then puts forward an algorithm that can effectively improve the program execution efficiency, hoping to provide reference for the follow-up research work.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {722–724},
numpages = {3},
keywords = {Big data, MapReduce, Algorithm},
location = {Weihai City, China},
series = {ICASIT 2020}
}

@inproceedings{10.1145/2903150.2908078,
author = {Homayoun, Houman},
title = {Heterogeneous Chip Multiprocessor Architectures for Big Data Applications},
year = {2016},
isbn = {9781450341288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2903150.2908078},
doi = {10.1145/2903150.2908078},
abstract = {Emerging big data analytics applications require a significant amount of server computational power. The costs of building and running a computing server to process big data and the capacity to which we can scale it are driven in large part by those computational resources. However, big data applications share many characteristics that are fundamentally different from traditional desktop, parallel, and scale-out applications. Big data analytics applications rely heavily on specific deep machine learning and data mining algorithms, and are running a complex and deep software stack with various components (e.g. Hadoop, Spark, MPI, Hbase, Impala, MySQL, Hive, Shark, Apache, and MangoDB) that are bound together with a runtime software system and interact significantly with I/O and OS, exhibiting high computational intensity, memory intensity, I/O intensity and control intensity. Current server designs, based on commodity homogeneous processors, will not be the most efficient in terms of performance/watt for this emerging class of applications. In other domains, heterogeneous architectures have emerged as a promising solution to enhance energy-efficiency by allowing each application to run on a core that matches resource needs more closely than a one-size-fits-all core. A heterogeneous architecture integrates cores with various micro-architectures and accelerators to provide more opportunity for efficient workload mapping. In this work, through methodical investigation of power and performance measurements, and comprehensive system level characterization, we demonstrate that a heterogeneous architecture combining high performance big and low power little cores is required for efficient big data analytics applications processing, and in particular in the presence of accelerators and near real-time performance constraints.},
booktitle = {Proceedings of the ACM International Conference on Computing Frontiers},
pages = {400–405},
numpages = {6},
keywords = {application characterization, accelerator, big data, heterogeneous architectures, power, performance},
location = {Como, Italy},
series = {CF '16}
}

@inproceedings{10.1145/3338840.3355683,
author = {Guleng, Siri and Wu, Celimuge and Yoshinaga, Tsutomu and Ji, Yusheng},
title = {Traffic Big Data Assisted Broadcast in Vehicular Networks},
year = {2019},
isbn = {9781450368438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338840.3355683},
doi = {10.1145/3338840.3355683},
abstract = {Multi-hop broadcast communications are required for vehicular Internet-of-Things applications including intelligent transport systems, autonomous driving, and collision avoidance systems. However, conducing efficient broadcasting in vehicular ad hoc networks (VANETs) is particularly challenging due to the vehicle mobility and various vehicle densities. In this paper, we propose a traffic big data assisted broadcast scheme in VANETs. The proposed scheme uses vehicle traffic big data to estimate vehicle density, and then uses the prediction information to enhance the procedure of multi-hop broadcasting. By enhancing a receiver-oriented broadcast approach with vehicle density prediction, the proposed scheme can provide a high dissemination ratio with low broadcast redundancy. We use real traffic big data to conduct prediction and then generate realistic vehicular network simulations to show the performance of the proposed scheme.},
booktitle = {Proceedings of the Conference on Research in Adaptive and Convergent Systems},
pages = {236–240},
numpages = {5},
keywords = {traffic big data, VANETs, broadcast},
location = {Chongqing, China},
series = {RACS '19}
}

@inproceedings{10.1145/2351316.2351322,
author = {Yang, Hang and Fong, Simon},
title = {Incrementally Optimized Decision Tree for Noisy Big Data},
year = {2012},
isbn = {9781450315470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351316.2351322},
doi = {10.1145/2351316.2351322},
abstract = {How to extract meaningful information from big data has been a popular open problem. Decision tree, which has a high degree of knowledge interpretation, has been favored in many real world applications. However noisy values commonly exist in high-speed data streams, e.g. real-time online data feeds that are prone to interference. When processing big data, it is hard to implement pre-processing and sampling in full batches. To solve this tradeoff, this paper proposes a new incremental decision tree algorithm so called incrementally optimized very fast decision tree (iOVFDT). The experiment evaluates the proposed algorithm in comparison to existing methods under noisy data streams environment. Result shows iOVFDT has outperformance on the aspects of higher accuracy and smaller model size.},
booktitle = {Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
pages = {36–44},
numpages = {9},
keywords = {decision tree classification, incremental optimization, data stream mining, optimized very fast decision tree},
location = {Beijing, China},
series = {BigMine '12}
}

@inproceedings{10.1145/3366650.3366667,
author = {Li, Yihao and Wang, Jin},
title = {Online Updating Algorithms of Statistical Methods for Big Data},
year = {2019},
isbn = {9781450372909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366650.3366667},
doi = {10.1145/3366650.3366667},
abstract = {In this paper, we discuss online updating algorithms for Big Data. One of the main challenges of Big Data is the limitation of data storage. In the Big Data stream environment, online computation sometimes requires fast updates without the use of historical data. The focus of this research is on efficient online update algorithms for basic statistical computations, including mean, variance, covariance, skewness, kurtosis, confidence interval, test statistic, and linear regression. We demonstrate the implementation of R Language through a linear regress example.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Big Data},
pages = {81–85},
numpages = {5},
keywords = {Online Algorithm, Big Data, Linear Regression, Skewness, Kurtosis, Sample Moment},
location = {Taichung, Taiwan},
series = {ICCBD 2019}
}

@article{10.1145/3130983,
author = {Yang, Su and Wang, Minjie and Wang, Wenshan and Sun, Yi and Gao, Jun and Zhang, Weishan and Zhang, Jiulong},
title = {Predicting Commercial Activeness over Urban Big Data},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130983},
doi = {10.1145/3130983},
abstract = {This study aims at revealing how commercial hotness of urban commercial districts (UCDs) is shaped by social contexts of surrounding areas so as to render predictive business planning. We define social contexts for a given region as the number of visitors, the region functions, the population and buying power of local residents, the average price of services, and the rating scores of customers, which are computed from heterogeneous data including taxi GPS trajectories, point of interests, geographical data, and user-generated comments. Then, we apply sparse representation to discover the impactor factor of each variable of the social contexts in terms of predicting commercial activeness of UCDs under a linear predictive model. The experiments show that a linear correlation between social contexts and commercial activeness exists for Beijing and Shanghai based on an average prediction accuracy of 77.69% but the impact factors of social contexts vary from city to city, where the key factors are rich life services, diversity of restaurants, good shopping experience, large number of local residents with relatively high purchasing power, and convenient transportation. This study reveals the underlying mechanism of urban business ecosystems, and promise social context-aware business planning over heterogeneous urban big data.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {119},
numpages = {20},
keywords = {Crowdsourcing, Economic Ecosystems, Social Intelligence, Context Awareness, Urban Informatics}
}

@inproceedings{10.1145/2896825.2896838,
author = {Noorwali, Ibtehal and Arruda, Darlan and Madhavji, Nazim H.},
title = {Understanding Quality Requirements in the Context of Big Data Systems},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896838},
doi = {10.1145/2896825.2896838},
abstract = {While the domain of big data is anticipated to affect many aspects of human endeavour, there are numerous challenges in building big data applications among which is how to address big data characteristics in quality requirements. In this paper, we propose a novel, unified, approach for specifying big data characteristics (e.g., velocity of data arrival) in quality requirements (i.e., those requirements specifying attributes such as performance, reliability, availability, security, etc.). Several examples are given to illustrate the integrated specifications. As this is early work, further experimentation is needed in different big data situations and quality requirements and, beyond that, in a variety of project settings.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {76–79},
numpages = {4},
keywords = {software engineering, specification, requirements engineering, big data, quality requirements},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/3209415.3209479,
author = {Kudo, Hiroko},
title = {Bridging Big Data and Policy Making: A Case Study of Failure},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209479},
doi = {10.1145/3209415.3209479},
abstract = {This paper investigates into a question through a failure case. The research question is; why we often fail to use the existing information and knowledge, including big data to design and/or implement public policies? The case is; the ticketing data, which was collected during the London Olympic Games and its so far under usage to design public policy related to health, well-being, and physical activities of the citizens. The research adopts qualitative analysis, including analysis of primary documents and semi-directive interviews. There is a limitation of single case study: however the case well represents the research question to provide preliminary investigation and to generate hypotheses for further studies.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {609–615},
numpages = {7},
keywords = {evidence-based policy making, policy design, Big Data, government failure, government policy},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@inproceedings{10.1145/3291801.3291813,
author = {He, Juan and Qiao, Lin},
title = {Intellectual Property Risks and Protection Mechanisms of Big Data},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291813},
doi = {10.1145/3291801.3291813},
abstract = {In order to deepen the innovative application of big data in different industries and accelerate the research on big data technologies, it is important to clarify the intellectual property risks of big data and to understand the intellectual property protection mechanisms. In data processing, attention should be paid to the risk of copyright infringement of web crawlers in data acquisition process; the risk of copyright infringement of cloud computing platform in data storage process; and the risk of personal privacy infringement caused by big data analysis and mining. When big data is integrated with traditional industries such as retail industry, logistics industry and health care industry, intellectual property protection mechanisms such as copyright protection of database, trade secrets protection, anti-competition regulation of data monopolies, and data privacy regulation should be given extensive attention.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {38–42},
numpages = {5},
keywords = {Protection Mechanism, Intellectual Property, Data Processing, Big Data, Risk},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/872757.872875,
author = {Johnson, Theodore and Dasu, Tamraparni},
title = {Data Quality and Data Cleaning: An Overview},
year = {2003},
isbn = {158113634X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/872757.872875},
doi = {10.1145/872757.872875},
abstract = {Data quality is a serious concern in any data-driven enterprise, often creating misleading findings during data mining, and causing process disruptions in operational databases. The manifestations of data quality problems can be very expensive- "losing" customers, "misplacing" billions of dollars worth of equipment, misallocated resources due to glitched forecasts, and so on. Solving data quality problems typically requires a very large investment of time and energy -- often 80% to 90% of a data analysis project is spent in making the data reliable enough that the results can be trusted.In this tutorial, we present a multi disciplinary approach to data quality problems. We start by discussing the meaning of data quality and the sources of data quality problems. We show how these problems can be addressed by a multidisciplinary approach, combining techniques from management science, statistics, database research, and metadata management. Next, we present an updated definition of data quality metrics, and illustrate their application with a case study. We conclude with a survey of recent database research that is relevant to data quality problems, and suggest directions for future research.},
booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
pages = {681},
numpages = {1},
location = {San Diego, California},
series = {SIGMOD '03}
}

@inproceedings{10.1145/2640087.2644168,
author = {Cho, Wonhee and Lim, Yoojin and Lee, Hwangro and Varma, Mohan Krishna and Lee, Moonsoo and Choi, Eunmi},
title = {Big Data Analysis with Interactive Visualization Using R Packages},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644168},
doi = {10.1145/2640087.2644168},
abstract = {Compared to the traditional data storing, processing, analyzing and visualization which have been performed, Big data requires evolutionary technologies of massive data processing on distributed and parallel systems, such as Hadoop system. Big data analytic systems, thus, have been popular to derive important decision making in various areas. However, visualization on analytic system faces various limitation due to the huge amount of data. This brings the necessity of interactive visualization techniques beyond the traditional static visualization. R has been used and improved for a big data analysis and mining tool. Also, R is supported with various and abundant packages for different targets with visualization. However interactive visualization packages are not easily found in the market. This paper compares and analyzes interactive web packages with visualization packages for R. This paper also proposes interactive web visualized analysis environment for big data with a combination of interactive web packages and visualization packages. In particular, Big data analysis techniques with sensed data are presented as the result by reflecting the decision view on sensing field.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {18},
numpages = {6},
keywords = {Visualization, Mining, Big data, R, Hadoop},
location = {Beijing, China},
series = {BigDataScience '14}
}

