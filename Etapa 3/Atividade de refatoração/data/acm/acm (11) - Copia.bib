@inproceedings{10.1145/2377576.2377578,
author = {Brunner, Christopher and Peynot, Thierry},
title = {Visual Metrics for the Evaluation of Sensor Data Quality in Outdoor Perception},
year = {2010},
isbn = {9781450302906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377576.2377578},
doi = {10.1145/2377576.2377578},
abstract = {This paper proposes an experimental study of quality metrics that can be applied to visual and infrared images acquired from cameras onboard an unmanned ground vehicle (UGV). The relevance of existing metrics in this context is discussed and a novel metric is introduced. Selected metrics are evaluated on data collected by a UGV in clear and challenging environmental conditions, represented in this paper by the presence of airborne dust or smoke.},
booktitle = {Proceedings of the 10th Performance Metrics for Intelligent Systems Workshop},
pages = {1–8},
numpages = {8},
keywords = {perception, visual/infrared camera, computer vision, unmanned ground vehicle, quality metrics},
location = {Baltimore, Maryland},
series = {PerMIS '10}
}

@inproceedings{10.1145/2623330.2630806,
author = {Feng, Mengling and Ghassemi, Mohammad and Brennan, Thomas and Ellenberger, John and Hussain, Ishrar and Mark, Roger},
title = {Management and Analytic of Biomedical Big Data with Cloud-Based in-Memory Database and Dynamic Querying: A Hands-on Experience with Real-World Data},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2630806},
doi = {10.1145/2623330.2630806},
abstract = {Analyzing Biomedical Big Data (BBD) is computationally expensive due to high dimensionality and large data volume. Performance and scalability issues of traditional database management systems (DBMS) often limit the usage of more sophisticated and complex data queries and analytic models. Moreover, in the conventional setting, data management and analysis use separate software platforms. Exporting and importing large amounts of data across platforms require a significant amount of computational and I/O resources, as well as potentially putting sensitive data at a security risk. In this tutorial, the participants will learn the difference between in-memory DBMS and traditional DBMS through hands-on exercises using SAP's cloud-based HANA in-memory DBMS in conjunction with the Multi-parameter Intelligent Monitoring in Intensive Care (MIMIC) dataset. MIMIC is an open-access critical care EHR archive (over 4TB in size) and consists of structured, unstructured and waveform data. Furthermore, this tutorial will seek to educate the participants on how a combination of dynamic querying, and in-memory DBMS may enhance the management and analysis of complex clinical data.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1970},
numpages = {1},
keywords = {big data, in-memory dbms, dynamic querying, biomedical data},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{10.1145/3325112.3325212,
author = {Hagen, Loni and Seon Yi, Hye and Pietri, Siana and E. Keller, Thomas},
title = {Processes, Potential Benefits, and Limitations of Big Data Analytics: A Case Analysis of 311 Data from City of Miami},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325212},
doi = {10.1145/3325112.3325212},
abstract = {As part of the open government movement, an increasing number of 311 call centers have made their datasets available to the public. Studies have found that 311 request patterns are associated with personal attributes and living conditions. Most of these studies used New York City 311 data. In this study, we use 311 data from the City of Miami, a smaller local government, as a case study. This study contributes to digital government research and practices by making suggestions on best practices regarding the use of big data analytics on 311 data. In addition, we discuss limitations of 311 data and analytics results. Finally, we expect our results to inform decision making within the City of Miami government and other local governments.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {1–10},
numpages = {10},
keywords = {311 data, e-government, information visualization, big data analytics},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@inproceedings{10.1145/2818869.2818906,
author = {Yung, Chung},
title = {Mining Massive Web Log Data of an Official Tourism Web Site as a Step towards Big Data Analysis in Tourism},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818906},
doi = {10.1145/2818869.2818906},
abstract = {The focus of this paper is on the conceptual and technical solution design to the analysis of massive web log data of an official tourism web site when integrated with web mining and big data technology. With the rapid development of Internet and World Wide Web, web log becomes one the fastest growing user generated contents, and web log mining plays an important role in many fields, such as personalized information service, design and service improvement of web sites. The underlying technology for analyzing massive web log data includes web log mining and big data analysis. In this paper, we give a comprehensive overview at the underlying technology, and then we propose an open architecture of big data solution design in tourism with mining the massive web log data. We include the discussion on the difficulties in implementing the proposed architecture as a conclusion.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {62},
numpages = {4},
keywords = {Web log mining, big data analysis, big data in tourism},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@inbook{10.5555/3042094.3042407,
author = {Chien, Chen-Fu and Chen, Ying-Jen and Wu, Jei-Zheng},
title = {Big Data Analytics for Modeling WAT Parameter Variation Induced by Process Tool in Semiconductor Manufacturing and Empirical Study},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {With the feature size shrinkage in advanced technology nodes, the modeling of process variations has become more critical for troubleshooting and yield enhancement. Misalignment among equipment tools or chambers in process stages is a major source of process variations. Because a process flow contains hundreds of stages during semiconductor fabrication, tool/chamber misalignment may more significantly affect the variation of transistor parameters in a wafer acceptance test. This study proposes a big data analytic framework that simultaneously considers the mean difference between tools and wafer-to-wafer variation and identifies possible root causes for yield enhancement. An empirical study was conducted to demonstrate the effectiveness of proposed approach and obtained promising results.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {2512–2522},
numpages = {11}
}

@inproceedings{10.1145/3411174.3411195,
author = {Lubis, Arif Ridho and Nasution, Mahyuddin K. M. and Sitompul, Opim Salim and Zamzami, Elviawaty Muisa},
title = {A Framework of Utilizing Big Data of Social Media to Find Out the Habits of Users Using Keyword},
year = {2020},
isbn = {9781450387668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411174.3411195},
doi = {10.1145/3411174.3411195},
abstract = {Optimal search in search engines is very urgent, especially search performs on social media search engine. Very big data in social media have not been not used as many as expected, making the existed data are only limited to the data themselves. However, using keywords on social media search engines could only produces incomplete and inaccurate data thereby the search results only have limited usage. This paper contains a framework using keywords in social media search engines that aims to gain users' habits by utilizing social media data. The framework offered is in the form of steps to optimize the search engines, so that the optimal social media search engine could be used as an entry point to find the desired data on specific social media user. The step is initialized by analyzing existing posts to get more specific and accurate data from social media users. The social media system then could use those specific keywords to identify the identity of a specific user in the information space that can be accessed by the search engines.},
booktitle = {Proceedings of the 8th International Conference on Computer and Communications Management},
pages = {140–144},
numpages = {5},
keywords = {social media keywords, social media search engine, Big data, user habit},
location = {Singapore, Singapore},
series = {ICCCM '20}
}

@inproceedings{10.1145/3338188.3338218,
author = {Zhang, Fan},
title = {Model Design of Eco-Carrying Capacity of Sports Tourist Attractions Based on Big Data Analysis},
year = {2019},
isbn = {9781450362931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338188.3338218},
doi = {10.1145/3338188.3338218},
abstract = {The traditional entropy weight TOPSIS-based eco-environmental carrying capacity model of sports tourist attractions uses grey correlation coefficient to select and identify the carrying capacity of calculation. Its large data analysis ability is poor, and the effect of analysis on the overall carrying capacity of sports tourist attractions is poor. For this reason, a model of carrying capacity of eco-environment of Sports Tourist Attractions Based on large data analysis is designed, and the overall framework of carrying capacity model of sports tourist attractions is designed. The database of model data stores all relevant information in the same nodes, so as to facilitate the analysis and management of large data. The ecological footprint method is used to calculate the carrying capacity of the eco-environment of sports tourist attractions. The ecological pollution of tourist attractions can be controlled within the scope of automatic recovery of the environment. The ecological footprint of soil environment, water resources environment, biological environment and pollutants is calculated to obtain the carrying capacity of the eco-environment of tourist attractions. Based on the ecological early warning module of the design model of carrying capacity of sports tourist attractions, the different carrying status of scenic spots is displayed by different colors to realize the early warning of carrying capacity of scenic spots. The experimental results show that the designed model is effective in carrying capacity analysis of scenic spots, and has a good application effect in large data environment.},
booktitle = {Proceedings of the 5th International Conference on Frontiers of Educational Technologies},
pages = {89–92},
numpages = {4},
keywords = {Carrying Capacity Model, Sports Tourist Attractions, Large Data Analysis, Ecological Footprint, Entropy Weight, Ecological Environment},
location = {Beijing, China},
series = {ICFET 2019}
}

@inproceedings{10.1145/3407703.3407706,
author = {Zhao, Zhengxian and Xu, Quanli and Peng, Shuangyun and Hong, Liang},
title = {Analyzing Spatial-Temporal Patterns of House Price Based on Network Big Data in the Main City Zone of Kunming},
year = {2020},
isbn = {9781450377270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407703.3407706},
doi = {10.1145/3407703.3407706},
abstract = {With the rapid development of information technologies such as the Internet, the Internet of Things and cloud computing, network big data is widely used in various fields of society, and making good use of network big data is great significance to the sustainable development of cities. Based on the network big data as the data source, this paper analyzes the spatial distribution and spatial-temporal pattern of house prices in the main city zone of Kunming through the Getis-Ord Gi* index and Kriging method, and quantifies the spatial-temporal changes of house prices through statistical analysis and spatial overlay analysis. The result shows that in hot and cold spot analysis, hot spot (high house price agglomeration area) is surrounded by cold spot (low house price agglomeration area), and Chenggong District is the cold spot dominant area. The house price in the main city zone of Kunming forms two high-value areas in the spatial distribution, and presents a pile-shaped ring structure with a central high price and low price in circumference. During the study period, the overall house price fluctuations were small, showing a pattern of north high and south low. The results of statistical analysis and spatial overlay analysis show that there is a significant difference in house prices among various administrative districts. The high-value clusters and low-value clusters in house prices are randomly distributed in various administrative regions. The price changes in Chenggong District are the most significant, and Panlong District has the smallest change.},
booktitle = {Proceedings of the 2020 Artificial Intelligence and Complex Systems Conference},
pages = {5–10},
numpages = {6},
keywords = {Getis-Ord Gi* Index, Network big data, Spatial-temporal patterns, Kriging, House price},
location = {Wuhan, China},
series = {AICSconf '20}
}

@inproceedings{10.1145/2018673.2018679,
author = {Wu, Leon and Kaiser, Gail and Rudin, Cynthia and Anderson, Roger},
title = {Data Quality Assurance and Performance Measurement of Data Mining for Preventive Maintenance of Power Grid},
year = {2011},
isbn = {9781450308427},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2018673.2018679},
doi = {10.1145/2018673.2018679},
abstract = {Ensuring reliability as the electrical grid morphs into the "smart grid" will require innovations in how we assess the state of the grid, for the purpose of proactive maintenance, rather than reactive maintenance; in the future, we will not only react to failures, but also try to anticipate and avoid them using predictive modeling (machine learning and data mining) techniques. To help in meeting this challenge, we present the Neutral Online Visualization-aided Autonomic evaluation framework (NOVA) for evaluating machine learning and data mining algorithms for preventive maintenance on the electrical grid. NOVA has three stages provided through a unified user interface: evaluation of input data quality, evaluation of machine learning and data mining results, and evaluation of the reliability improvement of the power grid. A prototype version of NOVA has been deployed for the power grid in New York City, and it is able to evaluate machine learning and data mining systems effectively and efficiently.},
booktitle = {Proceedings of the First International Workshop on Data Mining for Service and Maintenance},
pages = {28–32},
numpages = {5},
keywords = {data quality assurance, power grid, preventive maintenance, machine learning, data mining, performance measurement},
location = {San Diego, California},
series = {KDD4Service '11}
}

@inproceedings{10.1145/2535708.2535714,
author = {Sherman, Recinda and Henry, Kevin and Lee, David},
title = {The Impact of Data Quality on Spatial Analysis of Cancer Registry Data: The Example of Missing Stage at Diagnosis and Late-Stage Colorectal Cancer},
year = {2013},
isbn = {9781450325295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2535708.2535714},
doi = {10.1145/2535708.2535714},
abstract = {Most disease surveillance systems currently geocode case data. This, coupled with advances in geographic analysis tools, has led to a rise in epidemiologic studies on distribution of disease that rely on analysis of secondary data, e.g. from cancer registries. However, while the data and tools are available for performing geospatial analyses, there are challenges with which methodologies to apply, how to interpret and translate results, and how results are impacted by data quality. The issue of data quality is the subject of this paper.Mapping cancer rates highlights spatial patterns that can help elucidate environmental, clinical, or social causality pathways that drive differences in disease burden by geographic locations. Locating areas with high rates of cancer incidence or variations by stage at diagnoses can help prioritize cancer control efforts. Once the geographic patterns of cancer are mapped, the ideal action is to follow with effective public health interventions for the high risk communities. However, before using results of spatial research to inform public health response, it is important to consider whether the results are spurious due to methodological issues, such as data quality. Missing or incorrect data can distort research conclusions and result in ineffective public health policy.Using colorectal cancer (CRC) as an example, the impact of missing stage at diagnosis on late-stage at diagnosis cluster detection is evaluated. The impact on cluster detection, area-based modeling, and distance from services analysis is described.},
booktitle = {Proceedings of the Second ACM SIGSPATIAL International Workshop on the Use of GIS in Public Health},
pages = {18–26},
numpages = {9},
keywords = {screening disparities, colorectal cancer, area-based measures, stage at diagnosis, data quality, cluster detection},
location = {Orlando, Florida},
series = {HealthGIS '13}
}

@proceedings{10.1145/3363459,
title = {UrbSys'19: Proceedings of the 1st ACM International Workshop on Urban Building Energy Sensing, Controls, Big Data Analysis, and Visualization},
year = {2019},
isbn = {9781450370141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The advancements and availability of low-cost, low-energy sensors have improved energy and environmental sensing exponentially. Besides the millions of sensors used for monitoring, the improved accuracy of these sensors offer greater resolution for modeling 'what-if' scenarios in near real-time harnessing the vast computational power. Similarly, big data analysis has enabled city-scale modeling of energy and environmental impact using, among others, energy-efficient 'smaller' machine learning algorithms and/or physics-based modeling approaches. Coupled with interactive data visualization including Virtual Reality (VR), urban-scale energy and environmental systems modeling has become an exciting niche at the intersection of computer science and urban / architecture / mechanical engineering disciplines. The 1st International Urban Building Energy Sensing, Controls, Big Data Analysis, and Visualization (UrbSys) Workshop intends to capture recent exciting work by research experts, from U.S. universities and U.S. national laboratories, at this nexus that supports sustainable urban systems' design and engineering through state-of-the-art sensing, controls, modeling, and visualization.},
location = {New York, NY, USA}
}

@inproceedings{10.1145/3411170.3411271,
author = {Delnevo, Giovanni},
title = {A Study on The Limits and Potential of Machine Predictions from a Human-Big Data-Machine Interaction Perspective},
year = {2020},
isbn = {9781450375597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411170.3411271},
doi = {10.1145/3411170.3411271},
abstract = {Machine learning systems are obtaining outstanding results in several fields and are now used in various applications, products, and services. However, in order to minimize their harms and exploit their benefits, it is fundamental to understand the limits and the potentialities of algorithms that learn directly from data. In my PhD thesis, I would like to investigate how the limits of machine predictions influence the interaction between humans, computers, and big data.},
booktitle = {Proceedings of the 6th EAI International Conference on Smart Objects and Technologies for Social Good},
pages = {234–235},
numpages = {2},
keywords = {human-computer-interaction, human-in-the-loop method, Machine learning},
location = {Antwerp, Belgium},
series = {GoodTechs '20}
}

@inproceedings{10.1109/DS-RT.2015.17,
author = {Hu, Yangyang and Wang, Lizhe and Liu, Yingze and Chen, Dan and Li, Xiaoli},
title = {Towards an Efficient Multi-Way Factorization of Multi-Dimensional Big Data across a GPU Cluster},
year = {2015},
isbn = {9781467378222},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DS-RT.2015.17},
doi = {10.1109/DS-RT.2015.17},
abstract = {It has long been an important issue in various disciplines to examine massive multi-dimensional data by extracting the embedded multi-way factors. With the quick increases in both scales and dimensions of data under analysis, research challenges arise in order to reflect the dynamics of large-scale tensors while introducing no significant distortions in the factorization procedure in sophisticated applications. A massively parallel computing framework, namely H-PARAFAC, has been developed to enable Parallel Factor Analysis (PARAFAC) of massive tensors upon a "divide-and-conquer" theory (a modified alternating least squares approach). The hierarchical framework incorporates a coarse-grained model for coordinating the processing of subtensors and a fine-grained parallel model for computing each subtensor and fusing sub-factors. Experiments have been performed on a GPU cluster, and the results indicate that (1) the proposed method breaks the limitation on the size of data to be factorized, and (2) it dramatically outperforms the traditional counterparts in terms of both scalability and efficiency, e.g., the runtime increases linearly with the data volume increases in the order of n3.},
booktitle = {Proceedings of the 19th International Symposium on Distributed Simulation and Real Time Applications},
pages = {18–24},
numpages = {7},
keywords = {Parallel Computing, Multi-dimensional Data Processing, Parallel Factor Analysis, Factorization, Real-time Application},
location = {Chengdu, Sichuan, China},
series = {DS-RT 2015}
}

@proceedings{10.1145/3098593,
title = {Big-DAMA '17: Proceedings of the Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},
year = {2017},
isbn = {9781450350549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Los Angeles, CA, USA}
}

@inproceedings{10.1145/2663761.2664221,
author = {Ho, Phuc-Tran and Kim, Hee-Sun and Kim, Sung-Ryul},
title = {Application of Sim-Hash Algorithm and Big Data Analysis in Spam Email Detection System},
year = {2014},
isbn = {9781450330602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663761.2664221},
doi = {10.1145/2663761.2664221},
abstract = {Currently, there are many effective techniques that are used for filtering spam emails. However, spammers have mostly identified the weakness of those methods in order to bypass current detection systems. In this paper, we propose a novel similarity-based method that implements the fingerprinting technique on parallel processing framework. Furthermore, meet-in-the-middle approach is used in our method to achieve a higher accuracy in the spam email detection system. Our experimental result demonstrates the improved efficiency of this study.},
booktitle = {Proceedings of the 2014 Conference on Research in Adaptive and Convergent Systems},
pages = {242–246},
numpages = {5},
keywords = {spam, fingerprint, sim-hash, similarity-based, MapReduce, HBase},
location = {Towson, Maryland},
series = {RACS '14}
}

@inproceedings{10.1145/3378065.3378075,
author = {Jia, Tingfang and Dong, Wen},
title = {Research and Exploration on Quantitative Management of University Classes in the Age of Big Data},
year = {2020},
isbn = {9781450361910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378065.3378075},
doi = {10.1145/3378065.3378075},
abstract = {This paper analyses the problems existing in the class management of colleges and Universities under the current credit system model, and proposes to establish a quantitative management evaluation system suitable for colleges and universities by applying the quantitative management method, so as to help schools to evaluate objectively the quantitative management of students, classes and class teachers. Class quantitative management is to improve and update students and classes' information in real time through information system, and to establish class health index (CHI) and student health index (SHI) based on these data information, and then to calculate class management index (CMI) synthetically through SHI and CHI. Through the data of these three indexes, the evaluation objects are analyzed and evaluated. The purpose of evaluation is not only to evaluate, but also to guide the subject of evaluation to improve and develop in time according to the evaluation results. Therefore, the implementation links of the process management of the quantitative index system include: mobilizing and creating a quantitative management atmosphere, collecting real-time data, timely data analysis, guiding education, quantitative management evaluation and summary. The whole evaluation system pays more attention to the evaluation process than previous studies, and carries out empirical application research with the class as the data source.},
booktitle = {Proceedings of the 2019 4th International Conference on Intelligent Information Processing},
pages = {48–51},
numpages = {4},
keywords = {Quantitative Management, Class Management, University},
location = {China, China},
series = {ICIIP 2019}
}

@article{10.1145/2874239.2874279,
author = {Felzmann, Heike and Beyan, Timur and Ryan, Mark and Beyan, Oya},
title = {Implementing an Ethical Approach to Big Data Analytics in Assistive Robotics for Elderly with Dementia},
year = {2016},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0095-2737},
url = {https://doi.org/10.1145/2874239.2874279},
doi = {10.1145/2874239.2874279},
abstract = {In this paper, we analyse the ethical relevance of emerging informational aspects in robotics for the area of care robotics. We identify specific informational characteristics of contemporary and emerging robots, especially the fact of their increasing informational connectedness. We then outline specific ethical considerations arising in the design process in the H2020 project MARIO which aims to develop a care robot for persons with mild to moderate dementia in home and residential care settings. Ethical considerations regarding specific functionalities of the proposed care robot are outlined.},
journal = {SIGCAS Comput. Soc.},
month = {jan},
pages = {280–286},
numpages = {7},
keywords = {value-sensitive design, information ethics, care robotics, privacy}
}

@inproceedings{10.1145/3404555.3404602,
author = {Deng, Jianzhi and Zhou, Yuehan and Cheng, Xiaohui and Li, Tianyu and Qin, Chuling},
title = {Biological Big Data Analysis of Competing Endogenous RNA Network and MRNA Biomarker in Liver Cancer},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404602},
doi = {10.1145/3404555.3404602},
abstract = {In our research, we try to find out the Competing Endogenous RNA Network (ceRNA) and the biomarker of Liver cancer (LC). 490 differentially expressed mRNAs, 248 differentially expressed lncRNAs and 66 differentially expressed miRNAs were screened from the TCGA liver data. Among then, the differentially expressed mRNAs were enriched in 88 biological process, 16 cellular component and 27 molecular function of the gene ontology. And they were mostly enriched in extracellular region, extracellular space, integral component of plasma membrane, regulation of transcription and DNA-templated sequence-specific DNA binding. 14 DElncRNAs, 11 DEmiRNAs and 4 DEmRNAs were built the ceRNA network based on their inter-regulatory. The up-regulated mRNA in liver tumor samples, CCNE1, was regard as the biomarker of liver cancer by the proof of survival analysis and receiver operating characteristic analysis.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {94–98},
numpages = {5},
keywords = {liver cancer, ceRNA, TCGA, CCNE1, survival analysis},
location = {Tianjin, China},
series = {ICCAI '20}
}

@inproceedings{10.1145/3349341.3349486,
author = {Chen, Chunli and Liu, Huifang and Wang, Zhenhua},
title = {Analysis and Design of Urban Traffic Congestion in Urban Intelligent Transportation System Based on Big Data and Internet of Things},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349486},
doi = {10.1145/3349341.3349486},
abstract = {With the rapid development of big cities, the pressure of traffic congestion is increasing, and intelligent transportation is the fundamental way to solve traffic problems. Intelligent transportation system is a transportation management system that effectively integrates information technology, data communication technology, sensor technology, control technology and data processing technology. Large-scale traffic data management, integration and mining are key technologies. This paper briefly introduces the current situation of intelligent transportation in various countries, analyses the causes of urban traffic congestion in detail, and discusses the concept, composition and relationship between various subsystems of urban intelligent transportation system. Using the Internet of Things technology to collect traffic big data, based on the Big Data analysis technology to construct a traffic prediction model for traffic situation prediction, combined with real-time monitoring and analysis, the combination of remote traffic indicator control and optimization strategy control for traffic collaborative management. The causes of traffic congestion are analyzed in detail, specific solutions are proposed, and suggestions for transportation infrastructure planning are proposed.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {659–665},
numpages = {7},
keywords = {Internet of Things, Intelligent Transportation System, Big Data, Traffic Congestion},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3293663.3293680,
author = {Salman, Muhammad and Husna, Diyanatul and Wicaksono, Adhitya and Ratna, Anak Agung Putri},
title = {Evaluation and Analysis of Capacity Scheduler and Fair Scheduler in Hadoop Framework on Big Data Technology},
year = {2018},
isbn = {9781450366410},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293663.3293680},
doi = {10.1145/3293663.3293680},
abstract = {Apache Hadoop is an open source framework that implements MapReduce. It is scalable, reliable, and fault tolerant. Scheduling is an important process in Hadoop MapReduce. It is because scheduling has responsibility to allocate resources for running applications based on resource capacity, queues, running tasks, and the number of users. Changing single node to multi node Hadoop cluster can optimize HDFS, but quite costly. Scheduler performs the function of scheduling based on resource requirements, such as memory, CPU, disk, and network. The most general purpose of scheduling algorithm is minimizing the time of completing a task. Hadoop Scheduling is an independent module where users are able to design their own scheduler based on the application's actual need. So it can fulfill the specific need of the business in accordance with the desired result. This research will analyze the characteristic of Capacity Scheduler and Fair Scheduler.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Virtual Reality},
pages = {1–5},
numpages = {5},
keywords = {Big Data, Hadoop, Capacity Scheduler, YARN, Fair Scheduler},
location = {Nagoya, Japan},
series = {AIVR 2018}
}

@inproceedings{10.1145/2994551.2996712,
author = {Lee, Sukhoon and Park, JaeYeon and Kim, Doyeop and Kim, Tae Young and Park, Rae Woong and Yoon, Dukyong and Ko, JeongGil},
title = {Constructing a Bio-Signal Repository from an Intensive Care Unit for Effective Big-Data Analysis: Poster Abstract},
year = {2016},
isbn = {9781450342636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2994551.2996712},
doi = {10.1145/2994551.2996712},
abstract = {Analyzing large quantities of bio-signal data can lead to new findings in patient status diagnosis and medical emergency event prediction. Specifically, improvements in machine learning schemes suggest that by inputting clinical waveforms, designing mechanisms to predict medical emergencies, such as ventricular arrhythmia or sepsis, can soon be possible. However, we are still lacking the data-vaults that provide such clinically useful bio-signal data. With the goal of providing such an environment, this work focuses on developing a data repository for bio-signals collected from a hospital's intensive care init (ICU). Specifically, we design our data collection system to effectively store data from at-bed patient monitors and also integrate sensing information from bed-embedded sensing platforms, which allow filtering of noisy bio-signal samples caused by motion artifacts.},
booktitle = {Proceedings of the 14th ACM Conference on Embedded Network Sensor Systems CD-ROM},
pages = {372–373},
numpages = {2},
location = {Stanford, CA, USA},
series = {SenSys '16}
}

@inproceedings{10.1145/3424978.3425006,
author = {Bu, Naipeng Tom and Yin, Zihan and Barry, Ng, Jay and Kong, Haiyan},
title = {Research on the Influence of Big Data Knowledge Transfer on Value of Co-Creation and Competitive Advantage of Smart Tourism Destinations},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425006},
doi = {10.1145/3424978.3425006},
abstract = {Big data analysis used for smart tourism destinations has become a new trend in the increasing demand for tourism. However, relevant research on this problem is limited. This paper reports a literature review, in-depth interviews, and content analysis about tourism destinations and tourists. This study aims to discuss how knowledge transfer based on big data promotes the value of tourists and the competitive advantages of smart tourism destinations through value co-creation between them. An influencing model is explored to reflect such an interactive process. This study also makes some contributions to tourism destinations.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {28},
numpages = {8},
keywords = {Smart tourism destinations, Knowledge transfer, Big data, Value co-creation, Value, Tourists},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.1145/2505515.2505810,
author = {Liu, Xiaozhong and Chen, Miao and Ding, Ying and Song, Min},
title = {Workshop Summary for the 2013 International Workshop on Mining Unstructured Big Data Using Natural Language Processing},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2505810},
doi = {10.1145/2505515.2505810},
booktitle = {Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management},
pages = {2547–2548},
numpages = {2},
keywords = {big data, information retrieval, text mining, natural language processing},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@inproceedings{10.1145/3264746.3264768,
author = {Bok, Kyoungsoo and Choi, Kitae and Lim, Jongtae and Yoo, Jaesoo},
title = {Load Balancing Scheme for Supporting Real-Time Processing of Big Data in Distributed in-Memory Systems},
year = {2018},
isbn = {9781450358859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264746.3264768},
doi = {10.1145/3264746.3264768},
abstract = {In this paper, we propose a new load balancing scheme which performs data migration or replication according to the loading conditions in heterogeneous distributed in-memory environments. The proposed scheme replicates hot data when the hot data occurs on the node where a load occurs. If the loading of the node increases in the absence of hot data, the data is migrated through an adjustment of the storage space. It was shown through various performance evaluations that the proposed load balancing scheme improved the overall load balancing performance.},
booktitle = {Proceedings of the 2018 Conference on Research in Adaptive and Convergent Systems},
pages = {170–174},
numpages = {5},
keywords = {replication, big data, load balancing, migration, in-memory},
location = {Honolulu, Hawaii},
series = {RACS '18}
}

@inproceedings{10.5555/3382225.3382460,
author = {Zhao, Ying and Zhou, Charles C. and Bellonio, Jennie K.},
title = {Multilayer Value Metrics Using Lexical Link Analysis and Game Theory for Discovering Innovation from Big Data and Crowd-Sourcing},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {We demonstrated a machine learning and artificial intelligence method, i.e., lexical link analysis (LLA) to discover different layers of semantic network that contribute to innovative ideas from big data. The LLA is an unsupervised machine learning paradigm that does not require manually labeled training data. Multilayer value metrics are defined based on game theory for LLA. We showed the following results: 1) the value metrics generated from LLA in a use case of an internet game and crowd-sourcing; 2) the results from LLA are validated and correlated with the ground truth; 3) the game-theoretic LLA can help an information provider to present the information in the most valuable way. The information presentation can solve a problem (e.g., a search request of innovation) that no other information providers can solve (i.e., expertise). In addition, it ties also to a broader context that the unique value can propagate through the consensus. Based on the game-theoretic LLA, an information provider should not always present expertise content or authoritative content but rather with a mixed strategy where each type of content is presented with certain probabilities for the best value overall.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1145–1151},
numpages = {7},
keywords = {pareto superior, lexical link analysis, unsupervised learning, social welfare, nash equilibrium, big data, crowd-sourcing, pareto efficient, game theory},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/3429630.3429642,
author = {Wang, Shaozhuo},
title = {Research on the Management and Education System of Students in Clinical Practice in Medical Colleges in the Age of Big Data},
year = {2020},
isbn = {9781450388528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429630.3429642},
doi = {10.1145/3429630.3429642},
abstract = {Big data technology plays an important role in the management of college students. In this context, there are some problems existing in the clinical learning stage of medical college students, such as the imperfect hospital-college linkage education system, insufficient information management in affiliated hospitals, weak consciousness of big data, as well as single data collection and analysis, etc. This article analyzes the current situation of student management in the clinical internship stage of medical colleges in the age of big data, proposes to establish a sound school-university linkage system, optimize the information management system of affiliated hospitals, and use artificial neural networks to perform data information digging and analyzing the students’ school conditions, providing decision-making basis for managers.},
booktitle = {2020 The 4th International Conference on Digital Technology in Education},
pages = {83–87},
numpages = {5},
keywords = {medical colleges, education, big data, management},
location = {Busan, Republic of Korea},
series = {ICDTE 2020}
}

@inproceedings{10.1145/3404512.3404527,
author = {Deng, Jianzhi and Zhou, Yuehan and Tang, Weixian},
title = {Gene Big Data Analysis of Differentially Expressed LncRNA and MiRNA in Liver Cancer with Different Gender},
year = {2020},
isbn = {9781450377225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404512.3404527},
doi = {10.1145/3404512.3404527},
abstract = {In this paper, we try to find the differentially expressed lncRNA and differentially expressed miRNA of Liver cancer, especially between the different gender's patients. The differentially expressed genes were screened from TCGA liver data. Based on the extracted differentially expressed lncRNAs and differentially expressed miRNAs, we reveal an 8-lncRNA (TTTY14, UCA1, LINC00162, TTTY10, XIST, ERVH48-1, ZFY-AS1 and TTTY15) to 3-miRNA (hsa-mir-506, hsa-mir-508, hsa-mir-205) regulatory network of 13 pairs inter-regulatory between male and female. The 8 differentially expressed lncRNAs in the lncRNA-miRNA regulatory network were analyzed by the multivariable COX regression model, and LINC00162 and TTTY10 were found as the co-expression differentially expressed lncRNAs. After survival kmplot analysis and receiver operating characteristic analysis of the co-expression differentially expressed lncRNAs, TTTY10 was selected and proved as the potential biomarker of liver cancer for the diagnose and therapy.},
booktitle = {Proceedings of the 2020 2nd International Conference on Big Data Engineering},
pages = {24–28},
numpages = {5},
keywords = {liver cancer, TCGA, regulatory network, COX model, gender, TTTY10},
location = {Shanghai, China},
series = {BDE 2020}
}

@inproceedings{10.1145/3372938.3373003,
author = {Alcabnani, Sara and Oubezza, Mohamed and Elkafi, Jamal},
title = {An Approach for the Implementation of Semantic Big Data Analytics in the Social Business Intelligence Process on Distributed Environments (Cloud Computing)},
year = {2020},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3373003},
doi = {10.1145/3372938.3373003},
abstract = {Managing and extracting useful knowledge from social media sources is a challenge. It has attracted a lot of attention from universities and industry. To meet this challenge, semantic analysis of textual data is the subject matter.Today, with the connection present everywhere and at any time, considerable data is born. These data or data become a key player for understanding, analyzing, anticipating and solving major economic, political, social and scientific problems. Data also changes our working procedures, our cultural environment, even restructuring our way of thinking. And just as the scientific, managerial and financial world is interested in Big Data, a new discipline is growing: Fast Data. In addition to the salient volume of data; another variant becomes decisive, the ability to efficiently process data in all their diversity, transforming it into knowledge by providing the right information to the right person at the right time, or even using it to predict the future.The exploitation of Big Data requires the proposition of new adapted mathematical and IT approaches but also a reengineering of managerial approaches for the control of the informational environment of a public or private organization. While basing itself on a strategic information management approach such as Economic Intelligence (EI). The latter combines and encompasses Business Intelligence techniques for internal data management and business intelligence techniques for monitoring and controlling external information flows. However, Big Data, as a boundless source of information for EI, has upset the traditional EI process, which requires a reengineering of the EI approach. My research works perfectly in this context characterized by an uncertain and unpredictable environment.We ask to propose an ontology-based, service-oriented, agile and scalable Social Business Intelligence approach to extract the semantics of textual data and define the domain of massive data. In other words, we semantically analyze social data at two levels, namely the level of the entity and the level of the domain.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {65},
numpages = {6},
keywords = {Fast Data, Ontology, Cloud, Distributed Processing, Big Data, Social BI},
location = {Rabat, Morocco},
series = {BDIoT'19}
}

@inproceedings{10.1145/3314344.3332490,
author = {Zemicheal, Tadesse and Dietterich, Thomas G.},
title = {Anomaly Detection in the Presence of Missing Values for Weather Data Quality Control},
year = {2019},
isbn = {9781450367141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314344.3332490},
doi = {10.1145/3314344.3332490},
abstract = {Accurate weather data is important for improving agricultural productivity in developing countries. Unfortunately, weather sensors can fail for a wide variety of reasons. One approach to detecting failed sensors is to identify statistical anomalies in the joint distribution of sensor readings. This powerful method can break down if some of the sensor readings are missing. This paper evaluates five strategies for handling missing values in anomaly detection: (a) mean imputation, (b) MAP imputation, (c) reduction (reduced-dimension anomaly detectors via feature bagging), (d) marginalization (for density estimators only), and (e) proportional distribution (for tree-based methods only). Our analysis suggests that MAP imputation and proportional distribution should give better results than mean imputation, reduction, and marginalization. These hypotheses are largely confirmed by experimental studies on synthetic data and on anomaly detection benchmark data sets using the Isolation Forest (IF), LODA, and EGMM anomaly detection algorithms. However, marginalization worked surprisingly well for EGMM, and there are exceptions where reduction works well on some benchmark problems. We recommend proportional distribution for IF, MAP imputation for LODA, and marginalization for EGMM.},
booktitle = {Proceedings of the 2nd ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {65–73},
numpages = {9},
location = {Accra, Ghana},
series = {COMPASS '19}
}

@inproceedings{10.1145/3325185,
author = {Iyer, Mahesh},
title = {Session Details: FPGA Special Session: Advances in Adaptable Heterogeneous Computing and Acceleration for Big Data},
year = {2019},
isbn = {9781450362535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325185},
doi = {10.1145/3325185},
booktitle = {Proceedings of the 2019 International Symposium on Physical Design},
location = {San Francisco, CA, USA},
series = {ISPD '19}
}

@inproceedings{10.1145/3254770,
author = {Atzmueller, Martin and Chin, Alvin and Trattner, Christoph},
title = {Session Details: Modeling Social Media: Mining Big Data in Social Media and the Web (MSM 2014)},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254770},
doi = {10.1145/3254770},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.1145/3247781,
author = {Chatfield, Akemi Takeoka and Ojo, Adegboyega and Puron-Cid, Gabriel and Reddick, Christopher},
title = {Session Details: Open and Big Data Analytics in Government: Pathways from Insights to Public Value},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247781},
doi = {10.1145/3247781},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3247782,
author = {Chatfield, Akemi Takeoka and Ojo, Adegboyega and Puron-Cid, Gabriel and Reddick, Christopher},
title = {Session Details: Open and Big Data Analytics in Government: Pathways from Insights to Public Value},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247782},
doi = {10.1145/3247782},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@article{10.1145/2700833,
author = {Xu, Hongjiang},
title = {What Are the Most Important Factors for Accounting Information Quality and Their Impact on AIS Data Quality Outcomes?},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2700833},
doi = {10.1145/2700833},
abstract = {The accounting information system (AIS) is one of the most critical systems in any organization. Data quality plays a critical role in a data-intensive, knowledge-based economy. The objective of this study is to identify the most important factors for accounting information quality and their impact on AIS data quality outcomes. The article includes an extensive literature review and summarizes studies in quality management, data quality, accounting information systems, and enterprise planning in helping to identify a set of critical success factors for data quality. The study uses empirical data to answer the research question and test the research hypothesis. Study results show that the top three most important factors that affect accounting information systems’ data quality are top management commitment, the nature of the accounting information systems (such as the suitability of the systems), and input controls. The article further uses regression analysis to test the effect of those factors on AIS data quality, finding that there is a significant positive relationship between the perceived performance of the three most important factors and perceived AIS data quality outcomes.},
journal = {J. Data and Information Quality},
month = {mar},
articleno = {14},
numpages = {22},
keywords = {critical success factors, Data quality, accounting information systems}
}

@proceedings{10.1145/3229607,
title = {Big-DAMA '18: Proceedings of the 2018 Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},
year = {2018},
isbn = {9781450359047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Budapest, Hungary}
}

@inproceedings{10.1145/3247931,
author = {Gillis, Timothy},
title = {Session Details: Session 1: Visual Recognition of Families In the Wild: A Big Data Challenge},
year = {2017},
isbn = {9781450355117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247931},
doi = {10.1145/3247931},
booktitle = {Proceedings of the 2017 Workshop on Recognizing Families In the Wild},
location = {Mountain View, California, USA},
series = {RFIW '17}
}

@proceedings{10.1145/1651415,
title = {MoSE+DQS '09: Proceedings of the First International Workshop on Model Driven Service Engineering and Data Quality and Security},
year = {2009},
isbn = {9781605588162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {These proceedings include the papers accepted for the First International Workshop on Model Driven Service Engineering and Data Quality and Security (MoSE+DQS 2009), which was held in Hong Kong, on November 6th 2009.This workshop included two different tracks focusing on Model Driven Service Engineering (MoSE track) and Data Quality and Security (DQS track).Regarding the first issue we can see that Model-Driven Engineering (MDE) deals with the provision of models, transformations between them and code generators to address software development. One of the main advantages of model-driven approaches is the provision of a conceptual structure where the models used by business managers and analysts can be traced towards more detailed models used by software developers. This kind of alignment between high level business specifications and the lower level Service Oriented Architectures (SOA) is a crucial aspect in the field of Service-Oriented Development (SOD) where meaningful business services and business process specifications are those that can give support to real business environment usually changing with increasing speed. SOD has become currently in one of the major research topics in the field of software engineering, leading the appearance of a novel and emerging discipline called Service Engineering (SE), which aim to bring together benefits of SOA and Business Process Management (BPM). SE focuses on the identification of service (a client-provider interaction that creates value for the client) as first class elements for the software construction. The convergence of SE with MDE holds out the promise of rapid and accurate development of software that serves software users' goals.On the other hand, Information technologies are becoming one of the most important aspects for organizations. The business value of the data stored in the company databases has been growing to become one of the most important assets of the company. These data represent one crucial asset for tactic, strategic and operational decisions. Due to this important role of the data, companies should assure the access to the data to several users guaranteeing the right levels of quality they need to accomplish the task they have to do.Data Quality is a crucial issue in assessing the quality of business decisions support systems. Many aspects are related with the quality of the data, such as integrity, completeness, actuality and several other factors that make this kind of quality a multidimensional issue and a difficult issue. Data Security is another crucial aspect on information systems, not only because it affects Data Quality, but also because current information systems store sensitive and private data that should be treated rightly. Also, as Data Quality and Data Security are not independent concepts, the relationship between both concepts is worth being analyzed in order to give organizations some tools that can help in assuring both data dimensions.The Workshop on Model Driven Service Engineering and Data Quality and Security intends to provide a forum for researchers and practitioners working on different issues related to SE in conjunction with MDE, boarding open research problems in this area as well as practical experiences. The workshop is also focused on auditing, measuring, predicting, evaluating, controlling, assuring and improving the quality and security of data. Particular interests include methods, modelling languages, development methodologies and techniques in these fields.The six full papers (an acceptance rate of 54.5%) and four short papers were selected very carefully by the Program Committee in order to ensure a high quality workshop.},
location = {Hong Kong, China}
}

@inproceedings{10.1145/3247790,
author = {Chatfield, Akemi Takeoka and Ojo, Adegboyega and Puron-Cid, Gabriel and Reddick, Christopher},
title = {Session Details: Open and Big Data Analytics in Government: Pathways from Insights to Public Value},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247790},
doi = {10.1145/3247790},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3018896.3018974,
author = {Vega, M. A. L\'{o}pez and Couturier, S.},
title = {Design of a Big Data GIS Platform for a Near-Real-Time Environmental Monitoring System Based on GOES-R Satellite Imagery},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018974},
doi = {10.1145/3018896.3018974},
abstract = {Geographic Information Systems will gradually tend to incorporate technologies such as Big Data and the Internet of things, so that data can be processed to information towards near real time spatial analysis with unprecedented power and applications. Perhaps one of the most visible obstacles to this is the large amount of data involved. Indeed, the magnitude of stored data which implies its constant acquisition and generation, is rapidly growing to Terabyte systems through to Petabyte systems. By designing a near real time environmental monitoring system based on GOES-R next generation satellite imagery, we show in this paper that current tools actually allow the proper management of such amount of data and enable the integration of a variety of data sources that makes possible the rapid analysis of one of the most voluminous spatial dataset available.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {74},
numpages = {4},
keywords = {GOES-R, big data, GIS, spatial database},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3369555.3369557,
author = {Abdullah, Samsuri and Ismail, Marzuki and Ahmed, Ali Najah and Mansor, Wan Nurdiyana Wan},
title = {Big Data Analytics and Artificial Intelligence in Air Pollution Studies for the Prediction of Particulate Matter Concentration},
year = {2020},
isbn = {9781450371803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369555.3369557},
doi = {10.1145/3369555.3369557},
abstract = {Statistical modeling has found not suitable to be used when predicting the particulate matter (PM10) as it is non-linear in nature. The complexity and nonlinearity of PM10 concentration in the atmosphere are known best captured by the nonlinear model which emerges nowadays such as Multi-Layer Perceptron Neural Network (MLP-NN). In order to assess the capability of MLP-NN model in predicting the PM10 concentration, a statistical or traditional model known as Multiple Linear Regression (MLR) was also developed as a reference model. The daily air quality data and meteorological variables from the year 2010-2014 were assembled in developing the models. The MLP-NN model with the combination of logsig and purelin activation function revealed 75.5% of the variance in data with 6.59 μg/m3 (RMSE) and 88.0% of the variance in data with 6.30 μg/m3 (RMSE), during training and testing phase, respectively. The MLP-NN model improves by 61.5% and reducing the 62.2% error as compared to the MLR model. This model is appropriate for operational used by respected authorities in managing air quality in maintaining sustainability and as an early warning during an unhealthy level of air quality.},
booktitle = {Proceedings of the 3rd International Conference on Telecommunications and Communication Engineering},
pages = {90–94},
numpages = {5},
keywords = {meteorological factors, prediction, air quality, sustainability, nonlinear},
location = {Tokyo, Japan},
series = {ICTCE '19}
}

@article{10.1145/1577840.1577841,
author = {Madnick, Stuart E. and Lee, Yang W.},
title = {Editorial Letter for the Special Issue on Data Quality in Databases and Information Systems},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1577840.1577841},
doi = {10.1145/1577840.1577841},
journal = {J. Data and Information Quality},
month = {sep},
articleno = {6},
numpages = {2}
}

@inproceedings{10.1145/3418094.3418130,
author = {Bastwadkar, Meghana and McGregor, Carolyn and Balaji, S.},
title = {A Cloud Based Big Data Health-Analytics-as-a-Service Framework to Support Low Resource Setting Neonatal Intensive Care Unit},
year = {2020},
isbn = {9781450377768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418094.3418130},
doi = {10.1145/3418094.3418130},
abstract = {Critical care patients are monitored by a range of medical devices collecting high frequency data. New computing frameworks and platforms are being proposed to review and analyze the data in detail. The application of these approaches in a low resource setting is challenged by the approaches used for data acquisition. Software as a Service (SaaS) is a form of cloud computing where a cloud-based software application enables the storage, analysis and visualization of data within the cloud. A subset of SaaS is Health Analytics as a Service (HAaaS), which provides software to support health analytics in the cloud. The objective of this study is to design, implement, and demonstrate an extendable big-data compatible HAaaS framework that offers both real-time and retrospective analysis where data acquisition is not tightly coupled. A data warehousing framework is presented to facilitate analysis within a low resource setting. The framework has been instantiated in the Artemis platform within the context of the Belgaum Children Hospital (BCH) case study. Initial end-to-end testing with the Nellcor monitor (bedside monitor at BCH), which was not connected to any human, was completed. This testing confirms the functionality of the new Artemis cloud instance to receive data from test device using an alternate data acquisition approach.},
booktitle = {Proceedings of the 4th International Conference on Medical and Health Informatics},
pages = {30–36},
numpages = {7},
keywords = {cloud computing, decision support system, analytics-as-a-service, retrospective analysis, big data, health informatics, premature infants, real-time analytics, physiological data},
location = {Kamakura City, Japan},
series = {ICMHI '20}
}

@inproceedings{10.1145/3206157.3206164,
author = {Kyo, Koki},
title = {A Method for Big Data Analysis of the Impact of Economic and Social Events on Japanese Stock Prices},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206164},
doi = {10.1145/3206157.3206164},
abstract = {In this paper, we propose an approach for isolating the effects of economic and social events on stock prices. Using a newly-proposed Bayesian modeling technique, we decompose the daily time series of stock price data into three components: a trend component, a cyclical component, and an irregular component. We can then analyze the behavior of each estimated component in relation to economic and social events. As an empirical example, we analyze the daily time series for closing values of the Nikkei Stock Average (NSA) from January 4, 2000 to November 28, 2017, and examine relationships between the estimated components of NSA and significant events together with variations in the economic and social.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {5–10},
numpages = {6},
keywords = {state space model, daily stock price data, Nikkei Stock Average, Bayesian modeling},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@article{10.1145/3338002,
author = {She, James},
title = {Introduction to the Special Issue on Big Data, Machine Learning, and AI Technologies for Art and Design},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3338002},
doi = {10.1145/3338002},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jul},
articleno = {57},
numpages = {3}
}

@proceedings{10.1145/2501221,
title = {BigMine '13: Proceedings of the 2nd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
year = {2013},
isbn = {9781450323246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The aim of this workshop is to bring together people from both academia and industry to present their most recent work related to big-data issues, and exchange ideas and thoughts in order to advance this big-data challenge, which has been considered as one of the most exciting opportunities in the past 10 years.Recent years have witnessed a dramatic increase in our ability to collect data from various sensors, devices, in different formats, from independent or connected applications. This data flood has outpaced our capability to process, analyze, store and understand these datasets. Consider the Internet data. The web pages indexed by Google were around one million in 1998, but quickly reached 1 billion in 2000 and have already exceeded 1 trillion in 2008. This rapid expansion is accelerated by the dramatic increase in acceptance of social networking applications, such as Facebook, Twitter, Weibo, etc., that allow users to create contents freely and amplify the already huge Web volume. Furthermore, with mobile phones becoming the sensory gateway to get real-time data on people from different aspects, the vast amount of data that mobile carrier can potentially process to improve our daily life has significantly outpaced our past CDR (call data record)- based processing for billing purposes only. It can be foreseen that Internet of things (IoT) applications will raise the scale of data to an unprecedented level. People and devices (from home coffee machines to cars, to buses, railway stations and airports) are all loosely connected. Trillions of such connected components will generate a huge data ocean, and valuable information must be discovered from the data to help improve quality of life and make our world a better place. For example, after we get up every morning, in order to optimize our commute time to work and complete the optimization before we arrive at office, the system needs to process information from traffic, weather, construction, police activities to our calendar schedules, and perform deep optimization under the tight time constraints. In all these applications, we are facing significant challenges in leveraging the vast amount of data, including challenges in (1) system capabilities (2) algorithmic design (3) business models.},
location = {Chicago, Illinois}
}

@inproceedings{10.1145/3379156.3391374,
author = {B. Adhanom, Isayas and Lee, Samantha C. and Folmer, Eelke and MacNeilage, Paul},
title = {GazeMetrics: An Open-Source Tool for Measuring the Data Quality of HMD-Based Eye Trackers},
year = {2020},
isbn = {9781450371346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379156.3391374},
doi = {10.1145/3379156.3391374},
abstract = {As virtual reality (VR) garners more attention for eye tracking research, knowledge of accuracy and precision of head-mounted display (HMD) based eye trackers becomes increasingly necessary. It is tempting to rely on manufacturer-provided information about the accuracy and precision of an eye tracker. However, unless data is collected under ideal conditions, these values seldom align with on-site metrics. Therefore, best practices dictate that accuracy and precision should be measured and reported for each study. To address this issue, we provide a novel open-source suite for rigorously measuring accuracy and precision for use with a variety of HMD-based eye trackers. This tool is customizable without having to alter the source code, but changes to the code allow for further alteration. The outputs are available in real time and easy to interpret, making eye tracking with VR more approachable for all users.},
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {19},
numpages = {5},
keywords = {precision, eye movements, Eye tracking, eye tracker data quality, virtual reality, accuracy},
location = {Stuttgart, Germany},
series = {ETRA '20 Short Papers}
}

@inproceedings{10.1145/3372177.3373351,
author = {Lomakin, Nikolay and Shokhnekh, Anna and Sazonov, Sergey and Maramygin, Maxim and Tkachenko, Denis and Angel, Olga},
title = {Digital Ai "Decision Tree" for Predicting Russian GDP Value Based on Big Data Mining to Ensure Balanced and Sustainable Economic Growth},
year = {2020},
isbn = {9781450372442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372177.3373351},
doi = {10.1145/3372177.3373351},
abstract = {The relevance of the research study is due to the fact that the article attempts to prove or falsify the hypothesis that the "AI-Decision Tree" neural network model makes it possible to obtain a forecast of Russia's GDP for various scenarios.Various aspects of the AI application in the field of big data processing, deep learning and forecasting have been investigated in the article. However, experience has proven that, certain issues of using artificial intelligence require further scientific research in order to achieve a balanced and sustainable growth of the financial and economic system.Theoretical foundations of sustainable economic growth in the country have been studied. The authors have reviewed modern domestic and foreign literature on the topic and paid special attention to the issues of balanced financial and economic system and sustainable economic growth in modern conditions.We demonstrate the factors increasing risk and market uncertainty and other in order to achieve a balanced and sustainable growth of the financial system based on the AI-system "Decision Tree" developed.The trends in functioning of the financial and economic system have been determined; the dynamics of the balanced profit volumes in real sector organizations has been traced quarterly for the period of 2015-2018. Live data of the Federal State Statistics Service showed that the balanced financial result (profit except for loss) of organizations (apart from small business entities, banks, insurance organizations and state and municipal institutions) in current prices decreased by 8.5% in 2017.In order to visualize the dynamics of the effective factor - GDP a neural network model "AI-quantization of data" has been developed.In order to achieve a balanced and sustainable growth of the financial system based on the AI-system "Decision Tree" developed.},
booktitle = {Proceedings of the 2019 International SPBPU Scientific Conference on Innovations in Digital Economy},
articleno = {48},
numpages = {6},
keywords = {Decision Tree, AI-system, Digital, Big Data, GDP forecast},
location = {Saint Petersburg, Russian Federation},
series = {SPBPU IDE '19}
}

@inproceedings{10.1145/2583008.2583016,
author = {Cechanowicz, Jared and Gutwin, Carl and Brownell, Briana and Goodfellow, Larry},
title = {Effects of Gamification on Participation and Data Quality in a Real-World Market Research Domain},
year = {2013},
isbn = {9781450328159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2583008.2583016},
doi = {10.1145/2583008.2583016},
abstract = {Gamification has become an increasingly popular way to improve user engagement and motivation, but there is currently a lack of empirical research to demonstrate that increased gamification provides these benefits. To help address this problem we designed three versions of a gamified market research survey and tested them alongside the established industry standard in a study of over 600 participants. We also highlight examples where game elements compromise respondent data, and provide design solutions that correct the problem without losing the motivational benefits of gamification.},
booktitle = {Proceedings of the First International Conference on Gameful Design, Research, and Applications},
pages = {58–65},
numpages = {8},
keywords = {games, market research, gamification},
location = {Toronto, Ontario, Canada},
series = {Gamification '13}
}

@inproceedings{10.1145/1868328.1868348,
author = {Fern\'{a}ndez-Diego, Marta and Mart\'{\i}nez-G\'{o}mez, M\'{o}nica and Torralba-Mart\'{\i}nez, Jos\'{e}-Mar\'{\i}a},
title = {Sensitivity of Results to Different Data Quality Meta-Data Criteria in the Sample Selection of Projects from the ISBSG Dataset},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868348},
doi = {10.1145/1868328.1868348},
abstract = {Background: Most prediction models, e.g. effort estimation, require preprocessing of data. Some datasets, such as ISBSG, contain data quality meta-data which can be used to filter out low quality cases from the analysis. However, an agreement has not been reached yet between researchers about these data quality selection criteria.Aims: This paper aims to analyze the influence of data quality meta-data criteria in the number of selected projects, which can have influence in the models obtained. For this, a case study has been selected to gain a more complete understanding of what might be important to focus in future research.Method: Data quality meta-data selection criteria of some works based on ISBSG dataset which propose prediction models were reviewed first. Considerable attention has been paid to two data quality meta-data variables in ISBSG dataset Release 11 which are Data Quality Rating and Unadjusted Function Point Rating. Secondly, this paper considers data from 830 projects which have been collected from the ISBSG dataset after a preliminary screening. This first screening leads mainly to a subset of projects with comparable definitions in size and effort. Then data quality meta-data criteria are applied in order to infer their influence.Results: Overall, it seems that data selection criteria, regardless data quality meta-data concerns, involve an important reduction in sample size. From 5052 projects, only 830 are really considered. Then 262 projects remain for analysis if the maximum quality rate is applied for both data quality meta-data variables. But, since the initial data preparation focuses the problem of missingness for a certain purpose, data quality criteria seem not to be the clue for the analysis results. However, some variability has been observed.Conclusions: Whilst this analysis is supported by a case study, it is hoped that it contributes to a better understanding of the subject. In fact, results found suggest that in those studies where the selection criteria of projects are not very strictly applied, these data quality criteria must be carefully taken into account.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {13},
numpages = {9},
keywords = {effort, functional size, software projects, datasets, prediction models, empirical research, data quality meta-data},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/2030112.2030244,
author = {Cramer, Henriette and Rost, Mattias and Bentley, Frank and Shamma, David Ayman},
title = {2nd Workshop on Research in the Large. Using App Stores, Wide Distribution Channels and Big Data in Ubicomp Research},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030244},
doi = {10.1145/2030112.2030244},
abstract = {With the proliferation of app stores and the advancement of mobile devices, research that might have only been tested with a dozen participants in the past can now be released to millions. This offers huge opportunities, but also requires adaptations of existing methods in dealing with large deployments and making sense of large data sets. This workshop provides a forum for researchers to exchange experiences and strategies for wide distribution of applications as well as gathering and analyzing large scale data sets.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {619–620},
numpages = {2},
keywords = {research in the large, research methods, big data, distribution channels, app stores},
location = {Beijing, China},
series = {UbiComp '11}
}

