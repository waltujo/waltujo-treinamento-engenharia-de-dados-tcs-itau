@article{10.1145/2598902,
author = {Goodman, Elizabeth},
title = {Design and Ethics in the Era of Big Data},
year = {2014},
issue_date = {May-June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1072-5520},
url = {https://doi.org/10.1145/2598902},
doi = {10.1145/2598902},
journal = {Interactions},
month = {may},
pages = {22–24},
numpages = {3}
}

@inproceedings{10.5555/3233397.3233483,
author = {Albadarneh, Jafar and Talafha, Bashar and Al-Ayyoub, Mahmoud and Zaqaibeh, Belal and Al-Smadi, Mohammad and Jararweh, Yaser and Benkhelifa, Elhadj},
title = {Using Big Data Analytics for Authorship Authentication of Arabic Tweets},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {Authorship authentication of a certain text is concerned with correctly attributing it to its author based on its contents. It is a very important problem with deep root in history as many classical texts have doubtful attributions. The information age and ubiquitous use of the Internet is further complicating this problem and adding more dimensions to it. We are interested in the modern version of this problem where the text whose authorship needs authentication is an online text found in online social networks. Specifically, we are interested in the authorship authentication of tweets. This is not the only challenging aspect we consider here. Another challenging aspect is the language of the tweets. Most current works and existing tools support English. We chose to focus on the very important, yet largely understudied, Arabic language. Finally, we add another challenging aspect to the problem at hand by addressing it at a very large scale. We present our effort to employ big data analytics to address the authorship authentication problem of Arabic tweets. We start by crawling a dataset of more than 53K tweets distributed across 20 authors. We then use preprocessing steps to clean the data and prepare it for analysis. The next step is to compute the feature vectors of each tweet. We use the Bag-Of-Words (BOW) approach and compute the weights using the Term Frequency-Inverse Document Frequency (TF-IDF). Then, we feed the dataset to a Naive Bayes classifier implemented on a parallel and distributed computing framework known as Hadoop. To the best of our knowledge, none of the previous works on authorship authentication of Arabic text addressed the unique challenges associated with (1) tweets and (2) large-scale datasets. This makes our work unique on many levels. The results show that the testing accuracy is not very high (61.6%), which is expected in the very challenging setting that we consider.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {448–452},
numpages = {5},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@proceedings{10.1145/3208352,
title = {SBD'18: Proceedings of the International Workshop on Semantic Big Data},
year = {2018},
isbn = {9781450357791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Houston, TX, USA}
}

@article{10.1145/3090057,
author = {El-Mawass, Nour and Alaboodi, Saad},
title = {Data Quality Challenges in Social Spam Research},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3090057},
doi = {10.1145/3090057},
journal = {J. Data and Information Quality},
month = {sep},
articleno = {4},
numpages = {4},
keywords = {machine learning, Reproducibility, supervised learning, online social networks, social spam detection}
}

@proceedings{10.1145/3297730,
title = {BDET 2018: Proceedings of the 2018 International Conference on Big Data Engineering and Technology},
year = {2018},
isbn = {9781450365826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {As we know, Big Data is a popular term used to describe the exponential growth and availability of data, both structured and unstructured. Big Data may be as important to business and society due to its wide applications as the Internet has become. It is well-known that more data may lead to more accurate decision making. To raise awareness of the Big Data Engineering and Technology and its challenges, we hold this conference to offer you a unique opportunity to share ideas and experiences and to discuss evolving Big Data Engineering and Technology and its challenges.},
location = {Chengdu, China}
}

@proceedings{10.1145/3322134,
title = {ICBDE'19: Proceedings of the 2019 International Conference on Big Data and Education},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The proceedings present a selection of the papers submitted to the conference from universities, research institutes and industries. All of the papers were subjected to peer-review by conference committee members and international reviewers. The papers selected for publishing in the proceedings depended on their quality and their relevancy to the conference. The proceedings tend to present to the readers the recent advances in the field of Big Data and Education and various related areas.},
location = {London, United Kingdom}
}

@inproceedings{10.1145/3361785.3361803,
author = {Zareravasan, Ahad and Ashrafi, Amir},
title = {An Empirical Investigation on Big Data Analytics (BDA) and Innovation Performance},
year = {2019},
isbn = {9781450372329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361785.3361803},
doi = {10.1145/3361785.3361803},
abstract = {Nowadays, big data analytics (BDA) have widely used in our business environment as an undeniable function for firms to not only survive in turbulence but also have the opportunity to be ahead of their major competitors. One of the promising aspects of BDA relates to its influence on innovation performance. In line, the present study proposed a conceptual model in order to investigate the relationship between BDA use and innovation performance by considering the role of dynamic capability (DC) theory. In this research, we consider firm agility in terms of DC theory and decompose it into three main factors contacting sensing agility, decision making agility, and acting agility. The research model and required data were analyzed using Partial Least Squares (PLS)/Structured Equation Modelling (SEM). The outcome of this study indicates that firms would be able to increase their innovation performance from a DC theory. This study also shows that BDA use has a positive influence on sensing agility of firms.},
booktitle = {Proceedings of the 3rd International Conference on Business and Information Management},
pages = {97–101},
numpages = {5},
keywords = {innovation performance, agility, dynamic capabilities (DC), big data analytics (BDA)},
location = {Paris, France},
series = {ICBIM '19}
}

@inproceedings{10.1145/3327964.3328493,
author = {To, Alex and Meymandpour, Rouzbeh and Davis, Joseph G. and Jourjon, Guillaume and Chan, Jonathan},
title = {A Linked Data Quality Assessment Framework for Network Data},
year = {2019},
isbn = {9781450367899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3327964.3328493},
doi = {10.1145/3327964.3328493},
abstract = {For network analysts, understanding how traffic flows through a network is crucial to network management and forensics such as network monitoring, vulnerability assessment and defence. In order to understand how traffic flows through a network, network analysts typically access multiple, disparate data sources and mentally fuse this information. Providing some sort of automated support is crucial for network management. However, information about the quality of the network data sources is essential in order to build analyst's trust in automated tools. This paper presents SydNet, a novel Linked Data quality assessment framework which allows analysts to define quality dimensions and metrics which provide an accurate reflection of the quality of the data sources. The SydNet architecture also provides a number of novel fusion heuristics which can be used to fuse data from various network data sources. We demonstrate the utility of the SydNet architecture using CAIDA longitudinal topological data from a recent 24 months period and we demonstrate that our approach was able to detect dataset quality anomalies that would require further investigation.},
booktitle = {Proceedings of the 2nd Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)},
articleno = {4},
numpages = {8},
keywords = {Data quality, CAIDA, Network},
location = {Amsterdam, Netherlands},
series = {GRADES-NDA'19}
}

@inproceedings{10.1145/3396452.3396458,
author = {Su, Ying and Zhang, Yong},
title = {Automatic Construction of Subject Knowledge Graph Based on Educational Big Data},
year = {2020},
isbn = {9781450374989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396452.3396458},
doi = {10.1145/3396452.3396458},
abstract = {In this paper, we propose an automatic construction method of subject knowledge graph for educational applications. The subject knowledge graph is constructed based on educational big data by using a bootstrapping strategy to gradually expand knowledge points and connections between them. In this paper two different datasets are used. One is the subject teaching resources such as syllabuses, teaching plans, textbooks and etc., which is used to automatically construct the core of subject knowledge graph so as to reduce the dependence on the manual annotation. Meanwhile the high-quality of subject teaching resources is the guarantee of accuracy of the knowledge graph core. The other dataset is the massive Internet encyclopedia texts, which is used to expand and complete the subject knowledge graph. As to algorithm, this paper utilizes the BERT-BiLSTM-CRF model to automatically identify the subject knowledge points, and then evaluates the relationship between the knowledge points by calculating their semantic similarity, PMI and Normalized Google Distance between them. The experimental results show that BERT-BiLSTM-CRF outperforms the baselines significantly, and the three kinds of relationship evaluation models have achieved good results. Finally, computer science and physics science are taken as examples to construct the subject knowledge graphs successfully, which show the effectiveness of our method.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Big Data and Education},
pages = {30–36},
numpages = {7},
keywords = {normalized google distance, intelligent education, knowledge graph, point mutual information, BERT-BILSTM-CRF},
location = {London, United Kingdom},
series = {ICBDE '20}
}

@inproceedings{10.1109/BDC.2014.18,
author = {Wozniak, Justin M. and Sharma, Hemant and Armstrong, Timothy G. and Wilde, Michael and Almer, Jonathan D. and Foster, Ian},
title = {Big Data Staging with MPI-IO for Interactive X-Ray Science},
year = {2014},
isbn = {9781479918973},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BDC.2014.18},
doi = {10.1109/BDC.2014.18},
abstract = {New techniques in X-ray scattering science experiments produce large data sets that can require millions of high-performance processing hours per week of computation for analysis. In such applications, data is typically moved from X-ray detectors to a large parallel file system shared by all nodes of a peta scale supercomputer and then is read repeatedly as different science application tasks proceed. However, this straightforward implementation causes significant contention in the file system. We propose an alternative approach in which data is instead staged into and cached in compute node memory for extended periods, during which time various processing tasks may efficiently access it. We describe here such a big data staging framework, based on MPI-IO and the Swift parallel scripting language. We discuss a range of large-scale data management issues involved in X-ray scattering science and measure the performance benefits of the new staging framework for high-energy diffraction microscopy, an important emerging application in data-intensive X-ray scattering. We show that our framework accelerates scientific processing turnaround from three months to under 10 minutes, and that our I/O technique reduces input overheads by a factor of 5 on 8K Blue Gene/Q nodes.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing},
pages = {26–34},
numpages = {9},
keywords = {Broadcast, scattering, MPI, Swift, Blue Gene, X-ray, MPI-IO},
series = {BDC '14}
}

@article{10.1145/3263376,
author = {Lall, Ashwin and Czajkowski, Grzegorz and Wang, Haixun},
title = {Session Details: Special Issue on Big Data Analytics Workshop},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/3263376},
doi = {10.1145/3263376},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
numpages = {1}
}

@inproceedings{10.1145/3352411.3352450,
author = {Heng, Li and Longfu, Zhou and Qian, Yang},
title = {Research on Safety Control of Refined Oil Depot Based on Big Data Technology},
year = {2019},
isbn = {9781450371414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352411.3352450},
doi = {10.1145/3352411.3352450},
abstract = {Big data technology can efficiently collect, store, analyze and mine the massive safety control data which is generated in the daily operation of refined oil depot. It can also realize the early warning of safety risk, complete the safety assessment as well as help make the safety decision of refined oil depot. This paper will firstly introduce the traditional safety control information systems of refined oil depot. Secondly, the study will dive into the different data types of safety control, and then will construct the big data platform for security control. By discussing the key technologies of security control big data platform, this research aims to help build big data of refined oil depot, to promote the development of big data technology within the sector of safety control of refined oil depot, and finally to make it as a "sharp weapon " in safety control of refined oil depot.},
booktitle = {Proceedings of the 2019 2nd International Conference on Data Science and Information Technology},
pages = {249–254},
numpages = {6},
keywords = {Refined oil depot, Big data platform, Big data technology, Safety control},
location = {Seoul, Republic of Korea},
series = {DSIT 2019}
}

@proceedings{10.1145/3422713,
title = {ICBDT 2020: Proceedings of the 2020 3rd International Conference on Big Data Technologies},
year = {2020},
isbn = {9781450387859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to attend the 4th International Conference on 2020 3rd International Conference on Big Data Technologies (ICBDT 2020) and it's workshop 2020 4th International Conference on Business Information Systems (ICBIS 2020), held during September 18-20, 2020, which is supported by Shandong University of Science and Technology, China, University of Jinan, Shandong University, Guizhou University, Shandong Normal University, Qingdao University, and other universities. With the world fighting together against coronavirus this year, ICBDT 2020 has been converted into a virtual conference for the safety of our participants.},
location = {Qingdao, China}
}

@inproceedings{10.1145/3150928.3150943,
author = {Llwaah, Faris and Ca\l{}a, Jacek and Thomas, Nigel},
title = {Runtime Performance Prediction of Big Data Workflows with I/O-Aware Simulation},
year = {2017},
isbn = {9781450363464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3150928.3150943},
doi = {10.1145/3150928.3150943},
abstract = {Modelling and simulation of Big Data analytics processes running in the cloud is a difficult problem which introduces many challenges. The major one is the collection of training data which is scarce and costly to obtain, due to large scale and long runtime of those processes. In our previous work, we proposed a methodology to model, simulate and predict the runtime of Big Data processes such as complex Next Generation Sequencing (NGS) pipelines. The major contribution of our simulation methodology is that it provides a reasonable prediction of runtime for testing data much larger than the training inputs. To further improve the accuracy of prediction we present now an extension of our previous work that can model cloud data storage. Our simulation framework is based on CloudSim and WorkflowSim, to which we have added a shared storage component. We present the design and implementation of the storage extension together with an evaluation performed on selected scientific workflows: the Pegasus Montage workflow and NGS pipeline implemented in e-Science Central. The evaluation shows that the proposed extension works correctly and can improve prediction accuracy for our largest 390 GB input dataset by about 16% when compared to previous results.},
booktitle = {Proceedings of the 11th EAI International Conference on Performance Evaluation Methodologies and Tools},
pages = {74–81},
numpages = {8},
keywords = {Data-intensive simulation, WorkflowSim, Big data, Next generation sequencing pipeline},
location = {Venice, Italy},
series = {VALUETOOLS 2017}
}

@inproceedings{10.1145/2791405.2791549,
author = {Sheshasaayee, Ananthi and Lakshmi, J. V. N.},
title = {A Theoretical Model for Big Data Analytics Using Machine Learning Algorithms},
year = {2015},
isbn = {9781450333610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791405.2791549},
doi = {10.1145/2791405.2791549},
abstract = {Big Data processing is currently becoming increasingly important in modern era due to continuous growth of the amount of data generated in various fields. Architecture for Big Data usually ranges across multiple machines and clusters consisting of various sub systems. To potentially speed up the processing, a unified way of machine learning is applied on MapReduce frame work. A broadly applicable programming model MapReduce is applied on different learning algorithms belonging to machine learning family for all business decisions. This paper presents parallel implementation of various machine learning algorithms, includes K-Means, Logistic Regression implemented on top of MapReduce model.},
booktitle = {Proceedings of the Third International Symposium on Women in Computing and Informatics},
pages = {635–639},
numpages = {5},
keywords = {Serial Implementation, Logistic Regression, Hadoop, MapReduce, Parallel Implementation},
location = {Kochi, India},
series = {WCI '15}
}

@inproceedings{10.1145/2882903.2903744,
author = {LeFevre, Jeff and Liu, Rui and Inigo, Cornelio and Paz, Lupita and Ma, Edward and Castellanos, Malu and Hsu, Meichun},
title = {Building the Enterprise Fabric for Big Data with Vertica and Spark Integration},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2903744},
doi = {10.1145/2882903.2903744},
abstract = {Enterprise customers increasingly require greater flexibility in the way they access and process their Big Data while at the same time they continue to request advanced analytics and access to diverse data sources. Yet customers also still require the robustness of enterprise class analytics for their mission-critical data. In this paper, we present our initial efforts toward a solution that satisfies the above requirements by integrating the HPE Vertica enterprise database with Apache Spark's open source big data computation engine. In particular, it enables fast, reliable transferring of data between Vertica and Spark; and deploying Machine Learning models created by Spark into Vertica for predictive analytics on Vertica data. This integration provides a fabric on which our customers get the best of both worlds: it extends Vertica's extensive SQL analytics capabilities with Spark's machine learning library (MLlib), giving Vertica users access to a wide range of ML functions; it also enables customers to leverage Spark as an advanced ETL engine for all data that require the guarantees offered by Vertica.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {63–75},
numpages = {13},
keywords = {connector, analytics, database, big data, PMML, vertica, spark},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.5555/3233397.3233443,
author = {Clemente-Castell\'{o}, Francisco J. and Nicolae, Bogdan and Katrinis, Kostas and Rafique, M. Mustafa and Mayo, Rafael and Fern\'{a}ndez, Juan Carlos and Loreti, Daniela},
title = {Enabling Big Data Analytics in the Hybrid Cloud Using Iterative Mapreduce},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {The cloud computing model has seen tremendous commercial success through its materialization via two prominent models to date, namely public and private cloud. Recently, a third model combining the former two service models as on-/off-premise resources has been receiving significant market traction: hybrid cloud. While state of art techniques that address workload performance prediction and efficient workload execution over hybrid cloud setups exist, how to address data-intensive workloads - including Big Data Analytics - in similar environments is nascent. This paper addresses this gap by taking on the challenge of bursting over hybrid clouds for the benefit of accelerating iterative MapReduce applications. We first specify the challenges associated with data locality and data movement in such setups. Subsequently, we propose a novel technique to address the locality issue, without requiring changes to the MapReduce framework or the underlying storage layer. In addition, we contribute with a performance prediction methodology that combines modeling with micro-benchmarks to estimate completion time for iterative MapReduce applications, which enables users to estimate cost-to-solution before committing extra resources from public clouds. We show through experimentation in a dual-Openstack hybrid cloud setup that our solutions manage to bring substantial improvement at predictable cost-control for two real-life iterative MapReduce applications: large-scale machine learning and text analysis.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {290–299},
numpages = {10},
keywords = {data locality, mapreduce, hybrid cloud, performance prediction, big data analytics, iterative applications},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@article{10.14778/2367502.2367519,
author = {Chen, Yanpei and Alspaugh, Sara and Katz, Randy},
title = {Interactive Analytical Processing in Big Data Systems: A Cross-Industry Study of MapReduce Workloads},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367519},
doi = {10.14778/2367502.2367519},
abstract = {Within the past few years, organizations in diverse industries have adopted MapReduce-based systems for large-scale data processing. Along with these new users, important new workloads have emerged which feature many small, short, and increasingly interactive jobs in addition to the large, long-running batch jobs for which MapReduce was originally designed. As interactive, large-scale query processing is a strength of the RDBMS community, it is important that lessons from that field be carried over and applied where possible in this new domain. However, these new workloads have not yet been described in the literature. We fill this gap with an empirical analysis of MapReduce traces from six separate business-critical deployments inside Facebook and at Cloudera customers in e-commerce, telecommunications, media, and retail. Our key contribution is a characterization of new MapReduce workloads which are driven in part by interactive analysis, and which make heavy use of query-like programming frameworks on top of MapReduce. These workloads display diverse behaviors which invalidate prior assumptions about MapReduce such as uniform data access, regular diurnal patterns, and prevalence of large jobs. A secondary contribution is a first step towards creating a TPC-like data processing benchmark for MapReduce.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1802–1813},
numpages = {12}
}

@inproceedings{10.1145/3424978.3424999,
author = {Zhang, Ganghong and Huo, Chao and He, Jinhong and Gao, Jian and Yin, Zhibin and Luo, Anqin},
title = {A Strategy and Architecture Based on Big Data for Power Internet of Things},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3424999},
doi = {10.1145/3424978.3424999},
abstract = {Although many methods for collecting electricity data from smart terminals are available, especially for power Internet of things, alternatives have to face to the lack of effective management and analysis methods and hard technical realities for such large data. In order to solve these problems, this paper firstly, summarized the development of power Internet of things and big data, then analyzed the motivation and goals of building power Internet of things using big data technology according to the ubiquitous smart terminals applied in power areas, and illustrated the supporting technologies of big data when effectively serving the power Internet of things. Facing the challenges caused by big data in power of Internet of things, this paper proposed a strategy to handle big data that focused on cloud computing, data mining, and machine learning. In addition, we presented a basic architecture for cloud computing platforms and proposed the establishment of an operating center for power Internet of things. Possible solutions to collect data, data modeling, and data analysis and decision were proposed. We also proposed a typical forecasting system based on big data platform. For power Internet of things, taking advantage of big data and cloud computing technologies would be an effective strategy for improving decision support and analytics applied in power Internet of things. In the near future, this would be a great challenge and opportunity in power Internet of things.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {21},
numpages = {5},
keywords = {Smart terminal, Electricity data, Cloud computing, Power internet of things, Big data},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.1145/3348445.3348479,
author = {Puarungroj, Wichai and Phromkhot, Suchada and Boonsirisumpun, Narong and Pongpatrakant, Pathapong and Sangpradid, Satith},
title = {WebGIS for Managing Household Data within a Provincial Big Data Project},
year = {2019},
isbn = {9781450371957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3348445.3348479},
doi = {10.1145/3348445.3348479},
abstract = {The government agencies require decision support information before commencing their community development projects in rural areas. However, such information is not always available or does not meet their requirements. This research presents the design and development of the WebGIS, which is intended to store and provide spatially related household information for government agencies. This research has been conducted as a part of a provincial big data project. In this research, the spatial database system and the data visualization of the database were designed and developed by focusing on the details of each house in the targeted villages. The data were collected by the researchers from the study areas, which comprised 5 villages in Loei and Khonkaen Provinces in Thailand. The important household and location data were collected and combined with the community data from the Community Development Office. The GIS was developed using QGIS where the geolocation of each house in the villages was applied on the map derived from Google map. The data were analyzed and visualized in different formats such as color, table, and graph in order to establish the data classification and summarization. The system and data were finally evaluated by the Community Development Office and community leaders in terms of system performance and data accuracy.},
booktitle = {Proceedings of the 7th International Conference on Computer and Communications Management},
pages = {115–118},
numpages = {4},
keywords = {Community development, Geographic information systems, Spatial database, Village improvement, WebGIS},
location = {Bangkok, Thailand},
series = {ICCCM '19}
}

@inproceedings{10.1145/3269206.3269208,
author = {Nidzwetzki, Jan Kristof and G\"{u}ting, Ralf Hartmut},
title = {BBoxDB - A Scalable Data Store for Multi-Dimensional Big Data},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269208},
doi = {10.1145/3269206.3269208},
abstract = {BBoxDB is a distributed and highly available key-bounding-box-value store which enhances the classical key-value data model with an axis-parallel bounding box. The bounding box describes the location of the values in an n-dimensional space, and enables BBoxDB to efficiently distribute multi-dimensional data across a cluster of nodes. Well-known geometric algorithms (such as the K-D Tree) are used to create distribution regions (multi-dimensional shards). Distribution regions are created dynamically, based on the stored data. BBoxDB stores data of multiple tables co-partitioned, which enables efficient distributed spatial joins. Spatial joins on co-partitioned tables can be executed without data shuffling between nodes. A two-level index structure is employed to retrieve stored data quickly. We demonstrate the interaction with the system, the dynamic creation of distribution regions and the data redistribution feature of BBoxDB.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1867–1870},
numpages = {4},
keywords = {distributed system, multi-dimensional big data, co-partitioned data, multi-dimensional data store, spatial join},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/2649387.2660825,
author = {McDermott, Jason},
title = {Promises and Challenges in Analysis of Biological Big Data},
year = {2014},
isbn = {9781450328944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2649387.2660825},
doi = {10.1145/2649387.2660825},
abstract = {The advent of multiple new technologies for measuring many components in biological systems offers a huge opportunity and challenge for researchers. An important question is how to make sense of the mountains of data that describe different aspects of the same, or similar, biological systems. We are taking several approaches to this problem in terms of statistical methods and data mining, network and pathway analysis, and generation of testable biological hypotheses. We discuss applications of these approaches to study host-pathogen interactions and cancer, and talk about future opportunities and challenges in this area.},
booktitle = {Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {680},
numpages = {1},
keywords = {modeling, cancer, big data, proteomics, systems biology, network biology, omics, data integration, host-pathogen interactions},
location = {Newport Beach, California},
series = {BCB '14}
}

@inproceedings{10.1145/3383913.3383917,
author = {Ruoyu, Ren and Shulin, Yang and Qi, Zhang},
title = {Research on Copyright Protection of Digital Publishing in the Era of Big Data},
year = {2020},
isbn = {9781450372879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383913.3383917},
doi = {10.1145/3383913.3383917},
abstract = {With the development of network information technology, the era of big data brings new opportunities for digital publishing industry. Big data publishing has become one of the trends of future publishing, and has become the core competitiveness of publishing industry, and also brings new challenges for digital publishing. Due to the increasing popularity of data sharing in the era of big data, the problem of network infringement has also been gradually paid attention to, so the research on copyright protection is very important. Based on the characteristics and current situation of the development of digital publishing in the big data environment, this paper analyzes in detail the main problems and difficulties of copyright protection in the big data era. On this basis, this paper puts forward the era of big data is copyright protection of digital publishing strategy, in order to further improve the construction of copyright protection of digital publishing system.},
booktitle = {Proceedings of the 2019 2nd International Conference on Information Hiding and Image Processing},
pages = {39–43},
numpages = {5},
keywords = {copyright protection, digital publishing, big data},
location = {London, United Kingdom},
series = {IHIP 2019}
}

@inproceedings{10.1145/3241748.3241773,
author = {Dai, Hong and Tao, Ye and Shi, Tian-Wei},
title = {Research on Mobile Learning and Micro Course in the Big Data Environment},
year = {2018},
isbn = {9781450364812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241748.3241773},
doi = {10.1145/3241748.3241773},
abstract = {Put forward design of micro course mobile teaching system through analyzing the change of knowledge acquisition way and teaching role in the big data environment. The system platform includes the learner terminal platform and the teacher terminal platform. Learners use mobile terminals to active learn utilizing fragments of time. Mine the value information to take advantage of collecting education data in the process of using system. At the same time, the paper sets forth learning resource organization model. Form a complete data base of teaching process through the integration of a variety of teaching resources. Carry out data mining analysis according to the learner's behavior of micro course mobile learning. The results of the mining analysis provide a basis for teachers to adjust teaching content and improve teaching methods. The paper also presents the design of system network structure from the perspective of big data. The system architecture has been implemented. The paper proposes an innovative education evaluation mechanism through the mining analysis of education data. Form the objective and innovative evaluation mechanism between teachers and learners.},
booktitle = {Proceedings of the 2018 2nd International Conference on E-Education, E-Business and E-Technology},
pages = {48–51},
numpages = {4},
keywords = {Mobile Learning, Big Data, Evaluation Mechanism, Micro Course},
location = {Beijing, China},
series = {ICEBT '18}
}

@inproceedings{10.1145/3318464.3380584,
author = {Siddiqui, Tarique and Jindal, Alekh and Qiao, Shi and Patel, Hiren and Le, Wangchao},
title = {Cost Models for Big Data Query Processing: Learning, Retrofitting, and Our Findings},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380584},
doi = {10.1145/3318464.3380584},
abstract = {Query processing over big data is ubiquitous in modern clouds, where the system takes care of picking both the physical query execution plans and the resources needed to run those plans, using a cost-based query optimizer. A good cost model, therefore, is akin to better resource efficiency and lower operational costs. Unfortunately, the production workloads at Microsoft show that costs are very complex to model for big data systems. In this work, we investigate two key questions: (i) can we learn accurate cost models for big data systems, and (ii) can we integrate the learned models within the query optimizer. To answer these, we make three core contributions. First, we exploit workload patterns to learn a large number of individual cost models and combine them to achieve high accuracy and coverage over a long period. Second, we propose extensions to Cascades framework to pick optimal resources, i.e, number of containers, during query planning. And third, we integrate the learned cost models within the Cascade-style query optimizer of SCOPE at Microsoft. We evaluate the resulting system, Cleo, in a production environment using both production and TPC-H workloads. Our results show that the learned cost models are 2 to 3 orders of magnitude more accurate, and 20X more correlated with the actual runtimes, with a large majority (70%) of the plan changes leading to substantial improvements in latency as well as resource usage.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {99–113},
numpages = {15},
keywords = {machine learning, resource optimization, cost models, query optimization},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3344948.3344988,
author = {Ali, Shaukat and Damiani, Ferruccio and Dustdar, Schahram and Sanseverino, Marialuisa and Viroli, Mirko and Weyns, Danny},
title = {Big Data from the Cloud to the Edge: The Aggregate Computing Solution},
year = {2019},
isbn = {9781450371421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344948.3344988},
doi = {10.1145/3344948.3344988},
abstract = {We advocate a novel concept of dependable intelligent edge systems (DIES) i.e., the edge systems ensuring a high degree of dependability (e.g., security, safety, and robustness) and autonomy because of their applications in critical domains. Building DIES entail a paradigm shift in architectures for acquiring, storing, and processing potentially large amounts of complex data: data management is placed at the edge between the data sources and local processing entities, with loose coupling to storage and processing services located in the cloud. As such, the literal definition of edge and intelligence is adopted, i.e., the ability to acquire and apply knowledge and skills is shifted towards the edge of the network, outside the cloud infrastructure. This paradigm shift offers flexibility, auto configuration, and auto diagnosis, but also introduces novel challenges.},
booktitle = {Proceedings of the 13th European Conference on Software Architecture - Volume 2},
pages = {177–180},
numpages = {4},
keywords = {adaptation, formal methods, dependability},
location = {Paris, France},
series = {ECSA '19}
}

@proceedings{10.1145/3010089,
title = {BDAW '16: Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The last decades, the fast growth of the information and the communication technology has leaded to overwhelming amount of data pushing our communication network to its limit. The big data volume is produced from an assortment of users and wireless mobile technologies, and are to be processed and stored in powerful datacenters. Indeed, conventional wireless communication networks cannot deal with the massive amount of exchanged data and there is a strong demand to create a fast and scalable inter-connected heterogeneous wireless network for the big data. These advanced big data wireless networks should address all big data lifecycle (access networks, Internet backbone, intra and inter datacenter networks) and satisfy transmission QoS.},
location = {Blagoevgrad, Bulgaria}
}

@proceedings{10.1145/3372454,
title = {ICBDR 2019: Proceedings of the 2019 3rd International Conference on Big Data Research},
year = {2019},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {As the datasets often available in many disciplines and application areas exponentially grow thanks to the increasing availability of novel sensors and computational resources, there is nowadays still a need to develop novel approaches, methods and computational frameworks to deal with the high volumes and variety of the large data sources generated in many fields. The range of research challenges still opened are extremely large, from the representation, integration, storage, manipulation, data mining and visualization techniques to the development of secure computing architectures, cloud and distributed computing platforms to highly scalable storage systems. Clearly, big data challenges should require novel approaches and success stories should be shared, this favoring exchanges, cross-fertilization and finally advances as well as future research directions to consider.},
location = {Cergy-Pontoise, France}
}

@inproceedings{10.1145/2925995.2926010,
author = {J\"{a}ger, Alexandra and Breu, Ruth},
title = {Providing Support for Automated Product Data Quality Assurance: A Case Study},
year = {2016},
isbn = {9781450340649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2925995.2926010},
doi = {10.1145/2925995.2926010},
abstract = {Data quality is of utmost importance in large product databases. This is especially true for food products, since potentially health-critical data is contained. With growing database size, manual quality assurance becomes infeasible. GS1 Sync, governed by GS1 Austria, is rapidly becoming the largest national food product database. In order to support manual quality assurance, we have conceptualized a process to conduct product data quality assurance in an automatic way, based on defining rules and classifying product data. In order to evaluate our approach, we have implemented a prototype and performed a proof-of-concept. Although our research is still a work-in-progress, we were able to show that our approach is able to find a substantial number of issues that did not appear during manual control.},
booktitle = {Proceedings of the The 11th International Knowledge Management in Organizations Conference on The Changing Face of Knowledge Management Impacting Society},
articleno = {19},
numpages = {6},
keywords = {Product Data Quality, Product Data Classification, GS1 Sync},
location = {Hagen, Germany},
series = {KMO '16}
}

@inproceedings{10.1145/2593728.2593733,
author = {Iren, Deniz and Kul, Gokhan and Bilgen, Semih},
title = {Utilization of Synergetic Human-Machine Clouds: A Big Data Cleaning Case},
year = {2014},
isbn = {9781450328579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593728.2593733},
doi = {10.1145/2593728.2593733},
abstract = {Cloud computing and crowdsourcing are growing trends in IT. Combining the strengths of both machine and human clouds within a hybrid design enables us to overcome certain problems and achieve efficiencies. In this paper we present a case in which we developed a hybrid, throw-away prototype software system to solve a big data cleaning problem in which we corrected and normalized a data set of 53,822 academic publication records. The first step in our solution consists of utilization of external DOI query web services to label the records with matching DOIs. Then we used customized string similarity calculation algorithms based on Levensthein Distance and Jaccard Index to grade the similarity between records. Finally we used crowdsourcing to identify duplicates among the residual record set consisting of similar yet not identical records. We consider this proof of concept to be successful and report that we achieved certain results that we could not have achieved by using either human or machine clouds alone.},
booktitle = {Proceedings of the 1st International Workshop on CrowdSourcing in Software Engineering},
pages = {15–18},
numpages = {4},
keywords = {Crowdservice, Cloud Computing, Crowdsourcing},
location = {Hyderabad, India},
series = {CSI-SE 2014}
}

@inproceedings{10.1145/3341620.3341630,
author = {Chakravaram, Venkamaraju and G., Vidya Sagar Rao and Srinivas, Jangirala and Ratnakaram, Sunitha},
title = {The Role of Big Data, Data Science and Data Analytics in Financial Engineering},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341630},
doi = {10.1145/3341620.3341630},
abstract = {Financial engineering is the process of creating innovative solutions for the existing financial problems of a company by using applications of mathematical methods. Financial engineering uses tools and knowledge from the fields of computer science, big data, data science, data analytics, statistics, economics and applied mathematics to address current financial issues as well as to devise new and innovative financial products. Financial Engineering is helpful in derivative pricing, financial regulation, execution, corporate finance, portfolio management, risk management, trading of structured products. Therefore, financial engineering is used by Commercial Banks, Investment Banks, Insurance companies and other fund hedging agencies. The present study focus on the role of big data, data science and data analytics in financial engineering as a successful tool at all stages of insurance business management practices. How these insurance companies are using said three data tools effectively as fasteners of financial engineering for the successful design, development and implementation of innovative business processes and products in this competitive and ever-changing insurance market with innovative product features and strategies.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {44–50},
numpages = {7},
keywords = {Big Data, Data Analytics, Data Science, Insurance, Financial Engineering},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3289430.3289469,
author = {Yang, Dongmin and Zhou, Panpan and Kang, Jiaxin},
title = {Research on Green Development of Shaanxi Industry Under The Background of Big Data},
year = {2018},
isbn = {9781450365192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289430.3289469},
doi = {10.1145/3289430.3289469},
abstract = {Under the background of "Internet +" and supply side reform, the rapid development of industrialization and information technology has promoted the leaping development of big data industry, and the rapid development of big data industry is an important driving force for the green development of industry. Large data in all walks of life cross-weaving, as an important factor of production, plays an important role in the green development of industry. This paper briefly expounds the present situation of the green development of Shaanxi industry from three aspects of agriculture, industry and service industry in Shaanxi Province, and puts forward that large data can increase the added value of green agricultural production, promote the green transformation of industry, and innovate the green service mode. The big data industry is becoming a new economic growth pole and promoting Shaanxi industry in green transformation and upgrading.},
booktitle = {Proceedings of the 2018 2nd International Conference on Big Data and Internet of Things},
pages = {13–17},
numpages = {5},
keywords = {Green economy, Green development, Industrial transformation, Big data},
location = {Beijing, China},
series = {BDIOT 2018}
}

@inproceedings{10.1145/2676536.2676543,
author = {Li, Xun and Li, Wenwen and Anselin, Luc and Rey, Sergio and Koschinsky, Julia},
title = {A MapReduce Algorithm to Create Contiguity Weights for Spatial Analysis of Big Data},
year = {2014},
isbn = {9781450331326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676536.2676543},
doi = {10.1145/2676536.2676543},
abstract = {Spatial analysis of Big data is a key component of Cyber-GIS. However, how to utilize existing cyberinfrastructure (e.g. large computing clusters) to perform parallel and distributed spatial analysis on Big data remains a huge challenge. Problems such as efficient spatial weights creation, spatial statistics and spatial regression of Big data still need investigation. In this research, we propose a MapReduce algorithm for creating contiguity-based spatial weights. This algorithm provides the ability to create spatial weights from very large spatial datasets efficiently by using computing resources that are organized in the Hadoop framework. It works in the paradigm of MapReduce: mappers are distributed in computing clusters to find contiguous neighbors in parallel, then reducers collect the results and generate the weights matrix. To test the performance of this algorithm, we design experiment to create contiguity-based weights matrix from artificial spatial data with up to 190 million polygons using Amazon's Hadoop framework called Elastic MapReduce. The experiment demonstrates the scalability of this parallel algorithm which utilizes large computing clusters to solve the problem of creating contiguity weights on Big data.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {50–53},
numpages = {4},
keywords = {spatial weights, mapreduce, big data},
location = {Dallas, Texas},
series = {BigSpatial '14}
}

@inproceedings{10.1145/3363542.3363548,
author = {Ng, T. S. Eugene},
title = {Big Data and Optical Lightpaths Driven Data Center Network},
year = {2019},
isbn = {9781450368780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3363542.3363548},
doi = {10.1145/3363542.3363548},
booktitle = {Proceedings of the ACM SIGCOMM 2019 Workshop on Optical Systems Design},
articleno = {6},
numpages = {1},
location = {Beijing, China},
series = {OptSys '19}
}

@inproceedings{10.1145/3277139.3277144,
author = {Pu, Guoli and Li, Yuanyuan and Bai, Ju},
title = {The Effect of Big Data Analytics on Firms Sustainable Competitive Advantage of Quality: A Theory Framework},
year = {2018},
isbn = {9781450364867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277139.3277144},
doi = {10.1145/3277139.3277144},
abstract = {How can firms achieve sustainable competitive advantage of quality (SCAQ)? Some typical cases show that big data analytics is a possible approach. Based on the theory of dynamic capability, this paper constructs a theoretical framework and possible hypotheses of big data analytics (BDA) in the supply chain field that influences the firms SCAQ. The theoretical framework includes: (1) the definition and the dimensions of big data supply chain analytic (BDSCA) and SCAQ; (2) the internal and external factors that influence the use of BDSCA based on the TOE framework; (3) the path and effect of BDSCA on SCQR; (4) moderating effects of industry characters and environmental uncertainty. The research contributes to define the connotation and characteristics of BDSCA from the perspective of management, clarify the impact mechanism of BDSCA on SCAQ, and seek ways to improve the firms SCAQ in the area of big data.},
booktitle = {Proceedings of the 2018 International Conference on Information Management &amp; Management Science},
pages = {33–37},
numpages = {5},
keywords = {industry characters, big data analytics, TOE framework, sustainable competitive advantage of quality, environmental uncertainty},
location = {Chengdu, China},
series = {IMMS '18}
}

@article{10.1145/3110291,
author = {Corsar, David and Edwards, Peter},
title = {Challenges of Open Data Quality: More Than Just License, Format, and Customer Support},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3110291},
doi = {10.1145/3110291},
journal = {J. Data and Information Quality},
month = {sep},
articleno = {3},
numpages = {4},
keywords = {Open data}
}

@inproceedings{10.1145/3410566.3410591,
author = {Gillet, Annabelle and Leclercq, \'{E}ric and Savonnet, Marinette and Cullot, Nadine},
title = {Empowering Big Data Analytics with Polystore and Strongly Typed Functional Queries},
year = {2020},
isbn = {9781450375030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410566.3410591},
doi = {10.1145/3410566.3410591},
abstract = {Polystores are of primary importance to tackle the diversity and the volume of Big Data, as they propose to store data according to specific use cases. Nevertheless, analytics frameworks often lack a uniform interface allowing to fully access and take advantage of the various models offered by the polystore. It also should be ensured that the typing of the algebraic expressions built with data manipulation operators can be checked and that schema can be inferred before starting to execute the operators (type-safe).Tensors are good candidates for supporting a pivot data model. They are powerful abstract mathematical objects which can embed complex relationships between entities and that are used in major analytics frameworks. However, they are far away from data models, and lack high level operators to manipulate their content, resulting in bad coding habits and less maintainability, and sometimes poor performances.With TDM (Tensor Data Model), we propose to join the best of both worlds, to take advantage of modeling capabilities of tensors by adding schema and data manipulation operators to them. We developed an implementation in Scala using Spark, providing users with a type-safe and schema inference mechanism that guarantees the technical and functional correctness of composed expressions on tensors at compile time. We show that this extension does not induce overhead and allows to outperform Spark query optimizer using bind join.},
booktitle = {Proceedings of the 24th Symposium on International Database Engineering &amp; Applications},
articleno = {13},
numpages = {10},
keywords = {query language, high performance data analytics, tensor, polystore},
location = {Seoul, Republic of Korea},
series = {IDEAS '20}
}

@inproceedings{10.5555/2755753.2757169,
author = {Kanoun, Karim and van der Schaar, Mihaela},
title = {Big-Data Streaming Applications Scheduling with Online Learning and Concept Drift Detection},
year = {2015},
isbn = {9783981537048},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Several techniques have been proposed to adapt Big-Data streaming applications to resource constraints. These techniques are mostly implemented at the application layer and make simplistic assumptions about the system resources and they are often agnostic to the system capabilities. Moreover, they often assume that the data streams characteristics and their processing needs are stationary, which is not true in practice. In fact, data streams are highly dynamic and may also experience concept drift, thereby requiring continuous online adaptation of the throughput and quality to each processing task. Hence, existing solutions for Big-Data streaming applications are often too conservative or too aggressive. To address these limitations, we propose an online energy-efficient scheduler which maximizes the QoS (i.e., throughput and output quality) of Big-Data streaming applications under energy and resources constraints. Our scheduler uses online adaptive reinforcement learning techniques and requires no offline information. Moreover, our scheduler is able to detect concept drifts and to smoothly adapt the scheduling strategy. Our experiments realized on a chain of tasks modeling real-life streaming application demonstrate that our scheduler is able to learn the scheduling policy and to adapt it such that it maximizes the targeted QoS given energy constraint as the Big-Data characteristics are dynamically changing.},
booktitle = {Proceedings of the 2015 Design, Automation &amp; Test in Europe Conference &amp; Exhibition},
pages = {1547–1550},
numpages = {4},
location = {Grenoble, France},
series = {DATE '15}
}

@inproceedings{10.1145/3404687.3404705,
author = {Yau, Peter ChunYu and Wong, Dennis and Luen, Woo Hok and Leung, Joseph},
title = {Understanding Consumer Behavior by Big Data Visualization in the Smart Space Laboratory},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404705},
doi = {10.1145/3404687.3404705},
abstract = {In this paper, we describe a proof-of-concept (PoC) methodology to understand consumer behavior and spending pattern via visualization analysis in a custom-made smart space laboratory. This laboratory simulates the real-world shopping environment, allows big data generation and collection from various kinds of shopping activities. Data were captured from the service users who are having their technical and business training in a controlled setting environment. Consumer behavior modeling will be described, technical detail such as environment construction, theory, logic, framework, infrastructure, and architecture will also be discussed in this paper. Preliminary results showed that both "holding time" and the "frequency on the spots" have a certain relationship to the purchase decision which made by the consumer (i.e. service user in the laboratory): the longer stay time where the service user is located, the higher chances that the product(s) will be purchased.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {13–17},
numpages = {5},
keywords = {Smart Space, Big data, Visualization, Internet of Things (IoT), Spending pattern, Consumer behavior, Heat map},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1145/3293614.3293624,
author = {Santos, Anne C. M. and Pereira, \'{A}vner J. S. and Oliveira, Manoela R. and Macedo, Hendrik T. and Nascimento, Rog\'{e}rio P. C.},
title = {Building Software Products with Use Open Data and Big Data in Smart Cities},
year = {2018},
isbn = {9781450365727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293614.3293624},
doi = {10.1145/3293614.3293624},
abstract = {The use of Big Data and Open Data has been increasing and becoming a tendency in the last years. Big Data is about collect, store and analysis and interpretation of datasets so big and complex that traditional applications of data processing are not appropriate to your treatment. Open Data is related to opening of data: by opening, it is understood that data must be public and available free. Looking for information transparency, government data should be open. Every day more cities and countries are opening your data. The Open Data emerge as a special paradigm in smart cities. The main goal of Big and Open Data technologies in a smart city is provide system development that can be useful to the citizens. In this work, we analyze the state of utilization of Big Data and Open data in technological solutions with interoperability between software products to smart cities. 79 publications were found and 25 of them were selected, listing some technologies able to answer the research questions of this work.},
booktitle = {Proceedings of the Euro American Conference on Telematics and Information Systems},
articleno = {3},
numpages = {7},
keywords = {Open Data, Big Data, Smart City},
location = {Fortaleza, Brazil},
series = {EATIS '18}
}

@inproceedings{10.1145/2769458.2769491,
author = {Ellul, Natacha and Capocchi, Laurent and Santucci, Jean-Francois},
title = {Big Data Decision Making Based on Predictive Data Analysis Using DEVS Simulations},
year = {2015},
isbn = {9781450335836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2769458.2769491},
doi = {10.1145/2769458.2769491},
abstract = {Methods of processing and analyzing traditional data does not answer to the emergence of Big Data stemming from social networks and mobile applications. One of the best ways to bring the perspective of the customers to business decisions is by using data analysis to allow a company to deal with the customer experience for improved management and better profits. The work in progress presented in this paper concerns the development of an approach integrating discrete-event Modeling and Simulation and statistical learning methods in order to perform both customer understanding through data classification and predictive modeling through data prediction. This work involves the integration of statistical learning algorithms in the DEVS formalism.},
booktitle = {Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {257–258},
numpages = {2},
keywords = {discrete event modeling, devsimpy, simulation, artificial neural network, big data},
location = {London, United Kingdom},
series = {SIGSIM PADS '15}
}

@inproceedings{10.1145/2666652.2666664,
author = {Xiang, Junlong and Westerlund, Magnus and Sovilj, Du\v{s}an and Pulkkis, G\"{o}ran},
title = {Using Extreme Learning Machine for Intrusion Detection in a Big Data Environment},
year = {2014},
isbn = {9781450331531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666652.2666664},
doi = {10.1145/2666652.2666664},
abstract = {Extending state-of-the-art machine learning algorithms to highly scalable (big data) analysis environments is crucial for the handling of authentic datasets in Intrusion Detection Systems (IDS). Traditional supervised learning methods are considered to be too slow for use in these environments. Therefore, we propose the use of Extreme Learning Machine (ELM) for detecting network intrusion attempts. We show they hold great promise for the field by employing a MapReduce based variant evaluated on the open source tool Hadoop.},
booktitle = {Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop},
pages = {73–82},
numpages = {10},
keywords = {intrusion detection, big data, mapreduce, classification, extreme learning machine},
location = {Scottsdale, Arizona, USA},
series = {AISec '14}
}

@article{10.1145/2331042.2331050,
author = {Bonino, Dario and De Russis, Luigi},
title = {Mastering Real-Time Big Data with Stream Processing Chains},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331050},
doi = {10.1145/2331042.2331050},
abstract = {To conciliate application logic concerns with event handling performance, we introduce the spChains processing framework.},
journal = {XRDS},
month = {sep},
pages = {83–86},
numpages = {4}
}

@inproceedings{10.1145/3206157.3206173,
author = {Panqiu, Jiang},
title = {Research on College Students' Psychological Crisis Intervention in the Context of Big Data},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206173},
doi = {10.1145/3206157.3206173},
abstract = {Early warning and intervention of college students' psychological crisis are becoming an important subject to college mental health education. However, there is a lack of relevance and effectiveness in warning and intervention of college students' psychological crisis. With the development of big data mining techniques, the application of using big data technology in the early warning and intervention of college students' psychological crisis will improve the accuracy of college students' psychological crisis detection and establish an effective model of crisis early warning in order to make risk intervention in time. This paper discusses the application of big data in the field of the early warning and intervention of college students 'psychological crisis and establishes an effective countermeasure of early warning and intervention of psychological crisis and builds the mental health education team so as to dynamically grasp the condition of their psychological health to effectively develop mental health education.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {33–37},
numpages = {5},
keywords = {college student, psychological crisis intervention, Big data},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/3318299.3318384,
author = {Zhou, LiangQi and Xu, HongZhen and Wei, Li and Zhang, Quan and Zhou, Fei and Li, ZhuoPei},
title = {Air Big Data Outlier Detection Based on Infinite Gauss Bayesian and CNN},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318384},
doi = {10.1145/3318299.3318384},
abstract = {Air quality has always been a hot issue of concern to the people, the environmental protection department and the government. Among the massive air quality data, abnormal data can interfere with subsequent experiments and analysis. Therefore, it is necessary to detect abnormal data to improve the accuracy of the data. However, traditional air outlier detection methods require at least one year's data to make inferences about air quality. This paper firstly analyzes the characteristics of air quality big data, and then proposes a framework based on Bayesian non-parametric clustering, namely Dirichlet Process (DP) clustering framework, to realize the outlier detection of air quality. The framework optimizes Gaussian mixture model into infinite Gaussian mixture model according to the results of data analysis, and uses neural network to cluster the data processed by infinite Gaussian mixture model, which effectively improves the clustering accuracy and avoids the need of collecting a large number of training data.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {317–321},
numpages = {5},
keywords = {Dirichlet process, outlier detection, Air quality, Bayesian clustering, neural Network},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1145/3149572.3149603,
author = {Xiang, Gao and Fang, Wang},
title = {The Research of Data Integration and Business Intelligent Based on Drilling Big Data},
year = {2017},
isbn = {9781450353373},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149572.3149603},
doi = {10.1145/3149572.3149603},
abstract = {With the development of information technology, ChangQing drilling engineering company has accumulated a large number of data about drilling, and the research foundation of drilling big data technology had been formed. But now, huge volumes of data are distributed in different heterogeneous data sources due to the long-term decentralized construction, which is hard to realize the comprehensive analysis of related data. In this paper, aiming at the practical problems, a data integration and business intelligent- project based on drilling big data has been put forward. Referencing to the knowledge of this field, the system applies kettle which is a data integration tool to realize the integration of ETL heterogeneous data resource, establishes the data warehouse based on theme, and uses fine report which is a business intelligence tools to organize the views of drilling big data according to different user's requires, shows flexibly in multi-perspective view, thus provides powerful data support for user's drilling decision.},
booktitle = {Proceedings of the 9th International Conference on Information Management and Engineering},
pages = {64–68},
numpages = {5},
keywords = {Data Integration, Multi-source heterogeneous big data, Business Intelligence, Data warehouse},
location = {Barcelona, Spain},
series = {ICIME 2017}
}

@inproceedings{10.1145/2976749.2978378,
author = {Xu, Zhang and Wu, Zhenyu and Li, Zhichun and Jee, Kangkook and Rhee, Junghwan and Xiao, Xusheng and Xu, Fengyuan and Wang, Haining and Jiang, Guofei},
title = {High Fidelity Data Reduction for Big Data Security Dependency Analyses},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978378},
doi = {10.1145/2976749.2978378},
abstract = {Intrusive multi-step attacks, such as Advanced Persistent Threat (APT) attacks, have plagued enterprises with significant financial losses and are the top reason for enterprises to increase their security budgets. Since these attacks are sophisticated and stealthy, they can remain undetected for years if individual steps are buried in background "noise." Thus, enterprises are seeking solutions to "connect the suspicious dots" across multiple activities. This requires ubiquitous system auditing for long periods of time, which in turn causes overwhelmingly large amount of system audit events. Given a limited system budget, how to efficiently handle ever-increasing system audit logs is a great challenge. This paper proposes a new approach that exploits the dependency among system events to reduce the number of log entries while still supporting high-quality forensic analysis. In particular, we first propose an aggregation algorithm that preserves the dependency of events during data reduction to ensure the high quality of forensic analysis. Then we propose an aggressive reduction algorithm and exploit domain knowledge for further data reduction. To validate the efficacy of our proposed approach, we conduct a comprehensive evaluation on real-world auditing systems using log traces of more than one month. Our evaluation results demonstrate that our approach can significantly reduce the size of system logs and improve the efficiency of forensic analysis without losing accuracy.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {504–516},
numpages = {13},
keywords = {forensics, data reduction, intrusion detection, dependency analysis},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/2513549.2514739,
author = {Plale, Beth},
title = {Big Data Opportunities and Challenges for IR, Text Mining and NLP},
year = {2013},
isbn = {9781450324151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513549.2514739},
doi = {10.1145/2513549.2514739},
abstract = {Big Data poses challenges for text analysis and natural language processing due to its characteristics of volume, veracity, and velocity of the data. The sheer volume in terms of numbers of documents challenges traditional local repository and index systems for large-scale analysis and mining. Computation, storage and data representation must work together to provide rapid access, search, and mining of the deep knowledge in the large text collection. Text under copyright poses additional barriers to computational access, where analysis has to be separated from human consumption of the original text. Data preprocessing, in most cases, remains a daunting task for big textual data particularly data veracity is questionable due to age of original materials. Data velocity is rate of change of the data but can also be the rate at which changes and corrections are made.The HathiTrust Research Center (HTRC) provides new opportunities for IR, NLP and text mining research. HTRC is the research arm of HathiTrust, a consortium that stewards the digital library of content from research libraries around the country. With close to 11 million volumes in HathiTrust collection, HTRC aims to provide large-scale computational access and analytics to these text resources.With the goal of facilitating scholar's work, HTRC establishes a cyberinfrastructure of software, staff, and services to assist researchers and developers more easily process and mine large scale textual data effectively and efficiently. The primary users of HTRC are digital humanities, informatics, and librarians. They are of different research backgrounds and expertise and thus a variety of tools are made available to them.In the HTRC model of computing, computation moves to the data, and services grow up around the corpus to serve the research community. In this manner, the architecture is cloud-based. Moving algorithms to the data is important because the copyrighted content must be protected, however, a side benefit is that the paradigm frees scholars from worrying about managing a large corpus of data.The text analytics currently supported in HTRC is the SEASR suite of analytical algorithms (www.seasr.org). SEASR algorithms, which are written as workflows, include entity extraction, tag cloud, topic modeling, NaiveBayes, Date Entities to Similie Timeline.In this talk, I introduce the collections, architecture, and text analytics of HTRC, with a focus on the challenges of a BigData corpus and what that means for data storage, access, and large-scale computation.HTRC is building a user community to better understand and support researcher needs. It opens many exciting possibilities for the NLP, text mining, IR types of research: with so large an amount of textual data and many candidate algorithms, with support for researcher contributed algorithms, many interesting research questions emerge and many interesting results are to follow.},
booktitle = {Proceedings of the 2013 International Workshop on Mining Unstructured Big Data Using Natural Language Processing},
pages = {1–2},
numpages = {2},
keywords = {hathitrust, text mining and analysis, nlp, big data access, information retrieval},
location = {San Francisco, California, USA},
series = {UnstructureNLP '13}
}

@inproceedings{10.1145/3018896.3018975,
author = {Hussein, Ashraf S. and Khan, Hamayun A.},
title = {Students' Performance Tracking in Distributed Open Education Using Big Data Analytics},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018975},
doi = {10.1145/3018896.3018975},
abstract = {The field of Big Data Analytics (BDA) is advancing rapidly, and it is finding adoption in diverse areas such as Health, Commerce, Logistics, Retail and Manufacturing to name a few. Adoption of BDA techniques in the field of Higher Education is new, and it is steadily increasing. In this work, BDA techniques have been applied to track the Key Academic Performance Indicators (KAPIs) related to students at the Arab Open University (AOU) and to support the corresponding decisions in this regard. Since the AOU is a Pan Arab multi-campus distributed institution operating in 8 countries and makes extensive use of a wide range of cloud based applications to manage the students' life cycle, hence it is an ideal candidate for adoption of BDA techniques to track students' KAPIs across the AOU multiple country campuses. In order to achieve this objective, we have used IBM Watson Analytics (WA) platform to track the students' KAPIs. As a pilot project, we have focused in this work on the Information Technology and Computing (ITC) academic programme across the AOU. The Exploration and Business Intelligence BDA capabilities of WA have enabled us to analyze and track the academic KAPIs of the ITC students across AOU country campuses while the Predictive Analytics (PA) has led to identifying the dominant factors behind some of our problems such as students drop out rates. One of the most promising outcomes is the decision support dashboards such as the one related to the Student Risk Factor (SRF). By identifying At Risk Students, such dashboard can act as an "Early Alert System" to enable the AOU management to take corrective action to provide needed support to such students.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {75},
numpages = {8},
keywords = {watson analytics, student information systems, academic key performance indicators, big data analytics, educational data analytics},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/2640087.2644170,
author = {Jupin, Joseph and Shi, Justin Y.},
title = {Identity Tracking in Big Data: Preliminary Research Using In-Memory Data Graph Models for Record Linkage and Probabilistic Signature Hashing for Approximate String Matching in Big Health and Human Services Databases},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644170},
doi = {10.1145/2640087.2644170},
abstract = {Our research explores the practice of Record Linkage (RL), also known as Entity Resolution, Record Matching and the Object Identity Problem, in Big health services databases as is commonly practiced within the domain, and some of the approximate string matching methods used for this purpose. We also propose potential improvements to RL and string matching that have been shown in experiments to increase the quality and efficiency for information systems tasked with this problem. We have developed an in-memory graph-based data model, Aggregate Link and Iterative Match (ALIM), which compresses data by eliminating redundancy and stores alias, approximate and phonetic match links between stored data. We have also developed an enhanced edit-distance optimization, the Probabilistic Signature Hash Filter (PSH), which can perform the Damerau-Levenshtein (DL) edit-distance comparison nearly 6000 times faster than DL alone and produce the same exact approximate match results. Our experiments show significant accuracy and performance gains over a system currently in use by a local health department.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {20},
numpages = {8},
keywords = {Entity Resolution, Graph Models, Record Linkage, Record Matching, Object Identity Problem, Filtering, String Matching, Hashing, Signatures},
location = {Beijing, China},
series = {BigDataScience '14}
}

