@inproceedings{10.1109/CCGrid.2016.61,title = {On efficient hierarchical storage for big data processing}, author = {Krish K. R. , Wadhwa Bharti , Iqbal M. Safdar , Rafique M. Mustafa , Butt Ali R. },year = {2016}, isbn = {9781509024520}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2016.61}, doi = {10.1109/CCGrid.2016.61}, abstract = {A promising trend in storage management for big data frameworks, such as Hadoop and Spark, is the emergence of heterogeneous and hybrid storage systems that employ different types of storage devices, e.g. SSDs, RAMDisks, etc., alongside traditional HDDs. However, scheduling data accesses or requests to an appropriate storage device is non-trivial and depends on several factors such as data locality, device performance, and application compute and storage resources utilization. To this end, we present Dux, an application-attuned dynamic data management system for data processing frameworks, which aims to improve overall application I/O throughput by efficiently using SSDs only for workloads that are expected to benefit from them rather than the extant approach of storing a fraction of the overall workloads in SSDs. The novelty of Dux lies in profiling application performance on SSDs and HDDs, analyzing the resulting I/O behavior, and considering the available SSDs at runtime to dynamically place data in an appropriate storage tier. Evaluation of Dux with trace-driven simulations using synthetic Facebook workloads shows that even when using 5.5\u00d7 fewer SSDs compared to a SSD-only solution, Dux incurs only a small (5%) performance overhead, and thus offers an affordable and efficient storage tier management.}, location = {Cartagena, Columbia}, series = {CCGRID '16}, pages = {403\u2013408}, numpages = {6}, keywords = {data-intensive computing, performance prediction, MapReduce, heterogeneous storage, tiered storage}}
@inproceedings{10.1145/3342827.3342843,title = {Big Data Framework for Scalable and Efficient Biomedical Literature Mining in the Cloud}, author = {Shen Zhengru , Wang Xi , Spruit Marco },year = {2019}, isbn = {9781450362795}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3342827.3342843}, doi = {10.1145/3342827.3342843}, abstract = {The massive size of available biomedical literature requires researchers to utilize novel big data technologies in data storage and analysis. Among them is cloud computing which has become the most popular solution for big data applications in industry. However, many bioinformaticians still rely on expensive and inefficient in-house infrastructure to discover knowledge from biomedical literature. Although some cloud-based solutions were constructed recently, they failed to sufficiently address a few key issues including scalability, flexibility, and reusability. Moreover, no study has taken computational cost into consideration. To fill the gap, we proposed a cloud-based big data framework that enables researchers to perform reproducible and scalable large-scale biomedical literature mining in an efficient and cost-effective way. Additionally, a cloud agnostic platform was constructed and then evaluated on two open access corpora with millions of full-text biomedical articles. The results indicate that our framework supports scalable and efficient large-scale biomedical literature mining.}, location = {Tokushima, Japan}, series = {NLPIR 2019}, pages = {80\u201386}, numpages = {7}, keywords = {biomedical literature, big data, document classification, topic modeling, text mining, cloud computing}}
@inproceedings{10.1145/3448823.3448879,title = {Modelling Mobile Signal Strength by Combining Geospatial Big Data and Artificial Intelligence}, author = {Fraccaro Paolo , Benatan Matt , Reusch Katharina , Fare Clyde , Edwards Blair , Pyzer-Knapp Edward },year = {2020}, isbn = {9781450389532}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3448823.3448879}, doi = {10.1145/3448823.3448879}, abstract = {Estimating mobile signal strength accurately is a crucial task for network providers and their customers. However, current methodologies to estimate mobile signal strength present limitations in their practical implementation (i.e. physical based models), portability (i.e. spatial interpolation methods), simplicity and accuracy (i.e. path loss models). In this paper we present a novel approach that takes advantage of geospatial Big Data and advanced Artificial Intelligence to predict mobile signal strength at scale. Particularly, we used open access geo-spatial information about weather, tree coverage, land use, imperviousness, altitude and network infrastructure (i.e. a total of 174 features) to train and test uncertainty-aware artificial neural networks to predict mobile signal strength on data from the NetBravo crowdsourcing platform across all the United Kingdom (UK). Our model scored a best performance of 7.9 (standard deviation of 0.2) dBm for Root Mean Squared Error and 5.7 (standard deviation of 0.4) dBm for the Mean Absolute Error. Feature importance analysis showed that mobile cell tower characteristics and geospatial features showing the distribution of imperviousness and tree cover density over the line of sight between the mobile cell tower and the receiver as well as relative humidity were among the top 20 most important features.}, location = {Bangkok, Thailand}, series = {ICVISP 2020}, pages = {1\u20135}, numpages = {5}, keywords = {Mobile Signal Strength, Geospatial, Big Data, Artificial Neural Networks Learning}}
@inproceedings{10.1145/1458527.1458538,title = {A \"quick and dirty\" website data quality indicator}, author = {Askira Gelman Irit , Barletta Anthony L. },year = {2008}, isbn = {9781605582597}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1458527.1458538}, doi = {10.1145/1458527.1458538}, abstract = {This short paper outlines a research study in progress, which is motivated by the perception that the spelling error rate of a document can serve as a rudimentary proxy for the degree of quality control exercised in its creation, and, subsequently, indicate its quality. One objective of this research is to validate this understanding. Ultimately, the goal of this research is to take advantage of such an association. In particular, we propose a simple, \"quick and dirty\" metric for assisting in the evaluation of the quality of websites. This metric utilizes the reported hit counts of search engine queries on a pre-determined set of commonly misspelled words.}, location = {Napa Valley, California, USA}, series = {WICOW '08}, pages = {43\u201346}, numpages = {4}, keywords = {data quality, information credibility, information quality, data quality indicator, indicator, quick and dirty, web data}}
@inproceedings{10.4108/icst.valuetools.2013.254398,title = {Modeling apache hive based applications in big data architectures}, author = {Barbierato Enrico , Gribaudo Marco , Iacono Mauro },year = {2013}, isbn = {9781936968480}, publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)}, address = {Brussels, BEL}, url = {https://doi.org/10.4108/icst.valuetools.2013.254398}, doi = {10.4108/icst.valuetools.2013.254398}, abstract = {Performance prediction for Big Data applications is a powerful tool supporting designers and administrators in achieving a better exploitation of their computing resources. Big Data architectures are complex, continuously evolving and adaptive, thus a rapid design and verification modeling approach can be fit to the needs. As a result, a minimal semantic gap between models and applications would enable a wider number of designers to directly benefit from the results. The paper presents a multiformalism modeling approach based on a one-to-one mapping of Apache Hive querying primitives to modeling primitives. This approach exploits a combination of proper Big Data specific submodels and Petri nets to enable modeling of conventional application logic.}, location = {Torino, Italy}, series = {ValueTools '13}, pages = {30\u201338}, numpages = {9}}
@inproceedings{10.1145/3152723.3152735,title = {Case-based Reasoning Intelligent Tutoring System: An Application of Big Data and IoT}, author = {Masood Mona , Mokmin Nur Azlina Mohamed },year = {2017}, isbn = {9781450353564}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3152723.3152735}, doi = {10.1145/3152723.3152735}, abstract = {The Big Data technology has paved a new way to provide inputs for the educational activities. The Case-based Reasoning (CBR) is an Artificial Intelligence (AI) algorithmthat is widely used for Big Data application. CBR has the ability to give a solution based on previous experiences like a human in making a decision. Thus, this study has designed and developed an Intelligent Tutoring System (ITS) that utilized the CBR to suggest the most suitable learning material to the students based on the real-time information of the students' profiles. The application has been tested in actual setting and the results show that the students that have been presented with personalized learning materials performed significantly better than the students that were presented with non-personalized lessons. The application also has the ability to calculate the similarity between cases accurately and presented the students with the most suitable learning materials.}, location = {Osaka, Japan}, series = {ICBDR 2017}, pages = {28\u201332}, numpages = {5}, keywords = {Algebra, Learning Style, Mathematics, Intelligent Tutoring System, Artificial Intelligence, Interpersonal, Case-based Reasoning, Understanding, Instructional, Mastery, Self-Expressive, Internet of Things, Big Data}}
@inproceedings{10.1145/3482632.3483031,title = {Application of Big Data Technology in Computer Classroom Teaching}, author = {Zhu Jia },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483031}, doi = {10.1145/3482632.3483031}, abstract = {With the development of society, under the influence of high technology such as big data, cloud computing, network technology, and mobile Internet, education has gradually got rid of the traditional teaching methods and the limitations of traditional teaching methods. This article takes college students as the research theme. In order to obtain more complete survey data, the survey is based on an online questionnaire. In this online questionnaire survey, a total of 11 computer classroom teachers from colleges and universities at the city, county, and town levels were invited to supplement the paper questionnaire survey. After checking and verifying the retrieved questionnaire, the excel software technology and statistical data will be used to effectively analyze the questionnaire. Experimental research shows that 84% of teachers believe that big data education is related to computer teaching. They believe that the use of information technology to collect computer learning behaviors in colleges and universities can help improve the efficiency or quality of computer teaching, while only 16% of teachers are concerned about big data. The impact on computer teaching is unclear. Research confirms that high-data technology is related to computer classroom teaching, and the use of information technology to collect computer-based learning behavior can help improve the effectiveness or quality of computer classroom teaching. Big data technology has great potential in computer classroom teaching and will provide practical theoretical support and help for computer classroom teaching. Although the application of big data related technologies in the field of education in my country is still in the initial stage, it is still of great significance for deepening the reform of education and teaching.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {836\u2013839}, numpages = {4}}
@inproceedings{10.1145/3357223.3366029,title = {Big Data Processing at Microsoft: Hyper Scale, Massive Complexity, and Minimal Cost}, author = {Patel Hiren , Jindal Alekh , Szyperski Clemens },year = {2019}, isbn = {9781450369732}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3357223.3366029}, doi = {10.1145/3357223.3366029}, abstract = {The past decade has seen a tremendous interest in large-scale data processing at Microsoft. Typical scenarios include building business-critical pipelines such as advertiser feedback loop, index builder, and relevance/ranking algorithms for Bing; analyzing user experience telemetry for Office, Windows or Xbox; and gathering recommendations for products like Windows and Xbox. To address these needs a first-party big data analytics platform, referred to as Cosmos, was developed in the early 2010s at Microsoft. Cosmos makes it possible to store data at exabyte scale and process in a serverless form factor, with SCOPE [4] being the query processing workhorse. Over time, however, several newer challenges have emerged, requiring major technical innovations in Cosmos to meet these newer demands. In this abstract, we describe three such challenges from the query processing viewpoint, and our approaches to handling them.Hyper Scale. Cosmos has witnessed a significant growth in usage from its early days, from the number of customers (starting from Bing to almost every single business unit at Microsoft today), to the volume of data processed (from petabytes to exabytes today), to the amount of processing done (from tens of thousands of SCOPE jobs to hundreds of thousands of jobs today, across hundreds of thousands of machines). Even a single job can consume tens of petabytes of data and produce similar volumes of data by running millions of tasks in parallel. Our approach to handle this unprecedented scale is two fold. First, we decoupled and disaggregated the query processor from storage and resource management components, thereby allowing different components in the Cosmos stack to scale independently. Second, we scaled the data movement in the SCOPE query processor with quasilinear complexity [2]. This is crucial since data movement is often the most expensive step, and hence the bottleneck, in massive-scale data processing.Massive Complexity. Cosmos workloads are also highly complex. Thanks to adoption across the whole of Microsoft, Cosmos needs to support workloads that are representative of multiple industry segments, including search engine (Bing), operating system (Windows), workplace productivity (Office), personal computing (Surface), gaming (XBox), etc. To handle such diverse workloads, our approach has been to provide a one-size-fits-all experience. First of all, to make it easy for the customers to express their computations, SCOPE supports different types of queries, from batch to interactive to streaming and machine learning. Second, SCOPE supports both structured and unstructured data processing. Likewise, multiple data formats, including both propriety and open source source such as Parquet, are supported. Third, users can write business logic using a mix of declarative and imperative languages, over even different imperative languages such as C# and Python, in the same job. Furthermore, users can express all of the above in simple data flow style computation for better readability and maintainability. Finally, considering the diverse workload mix inside Microsoft, we have come to realization that it is not possible to fits all scenarios using SCOPE. Therefore, we also support the popular Spark query processing engine. Overall, the one-size-fits-all query processing experience in Cosmos covers very diverse workloads, including data formats, programming languages, and the backend engines.Minimal Cost. While scale and complexity are hard by themselves, the biggest challenge is to achieve all of that at minimal cost. In fact, there is a pressing need to improve Cosmos efficiency and reduce operational costs. This is challenging due to several reasons. First, optimizing a SCOPE job is hard considering that the SCOPE DAGs are super large (up to 1000s of operators in single job!), and the optimization estimates (cardinality, cost, etc.) are often way off from the actuals. Second, SCOPE optimizes a given query, while the operational costs depend on the overall workload. Therefore workload optimization becomes very important. And finally, SCOPE jobs are typically interlinked in data pipelines, i.e., the output of one job is consumed by other jobs. This means that workload optimization needs to be aware of these dependencies. Our approach is to develop a feedback loop to learn from past workloads in order to optimize the future ones. Specifically, we leverage machine learning to learn models for optimizing individual jobs [3], apply multi-query optimizations to optimize the costs of overall workload [1], and build dependency graphs to identify and optimize for the data pipelines.}, location = {Santa Cruz, CA, USA}, series = {SoCC '19}, pages = {490}, numpages = {1}}
@inproceedings{10.1145/3156346.3156347,title = {AI and Big Data Analytics for Health and Bioinformatics}, author = {Kwoh Chee Keong },year = {2017}, isbn = {9781450353502}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3156346.3156347}, doi = {10.1145/3156346.3156347}, abstract = {With the technological advances that allow for high throughput profiling of biological systems at a low cost. The low cost of data generation is leading us to the \"big data\" era. The availability of big data provides unprecedented opportunities but also raises new challenges for data mining and analysis. In this talk, I will start will the concepts in the analysis of big data, specifically the AI algorithms.My group has in The Biomedical Informatics Lab (BIL) is a research Centre is the focus of the education, research and development, and human-resource training in heath informatics and bioinformatics at NTU. The mission of BIL is to provide the interdisciplinary environment and training for students and researchers to engage in leading and cutting edge research in bioinformatics, and thereby become a part of the life sciences workforce in Singapore and elsewhere.This talk, by presenting selected research activities, will provide an overview of some of the innovative and creative approaches with the application of AI in big data analytics to address the challenges and solutions in both health and bioinformatics.}, location = {Nha Trang City, Viet Nam}, series = {CSBio '17}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/3510457.3513034,title = {An empirical study on quality issues of eBay's big data SQL analytics platform}, author = {Zhu Feng , Xu Lijie , Ma Gang , Ji Shuping , Wang Jie , Wang Gang , Zhang Hongyi , Wan Kun , Wang Mingming , Zhang Xingchao , Wang Yuming , Li Jingpin },year = {2022}, isbn = {9781450392266}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510457.3513034}, doi = {10.1145/3510457.3513034}, abstract = {Big data SQL analytics platform has evolved as the key infrastructure for business data analysis. Compared with traditional costly commercial RDBMS, scalable solutions with open-source projects, such as SQL-on-Hadoop, are more popular and attractive to enterprises. In eBay, we build Carmel, a company-wide interactive SQL analytics platform based on Apache Spark. Carmel has been serving thousands of customers from hundreds of teams globally for more than 3 years. Meanwhile, despite the popularity of open-source based big data SQL analytics platforms, few empirical studies on service quality issues (e.g., job failure) were carried out for them. However, a deep understanding of service quality issues and taking right mitigation are significant to the ease of manual maintenance efforts. To fill this gap, we conduct a comprehensive empirical study on 1,884 real-word service quality issues from Carmel. We summarize the common symptoms and identify the root causes with typical cases. Stakeholders including system developers, researchers, and platform maintainers can benefit from our findings and implications. Furthermore, we also present lessons learned from critical cases in our daily practice, as well as insights to motivate automatic tool support and future research directions.}, location = {Pittsburgh, Pennsylvania}, series = {ICSE-SEIP '22}, pages = {33\u201342}, numpages = {10}, keywords = {SQL on hadoop, open source, big data, empirical study}}
@inproceedings{10.1145/3498765.3498810,title = {Thinking on the Cultivation of Innovative Talents in Computer Majors Facing Big Data Applications}, author = {Jiao Ge , Li Lang , Deng Hongwei , Zheng Guangyong , Zou Yi , Zhao Junxia },year = {2021}, isbn = {9781450385114}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3498765.3498810}, doi = {10.1145/3498765.3498810}, abstract = {In order to solve the problems in the training of big data talents in colleges and universities, such as unclear training objectives, not close combination of practical teaching and industrial enterprises, and low training quality, this paper proposes a training mode of innovative computer talents oriented to the application of big data. The mode to train applied talents as the goal, USES the OBE's education idea, through the revision of the talent training scheme, optimizing curriculum system, reforming teaching methods, practical teaching system construction, improve teachers' skills, reform the assessment content and ways to improve the quality of talent training, meet the social demand for big data and technical personnel.}, location = {Wuhan, China}, series = {ICETC 2021}, pages = {291\u2013296}, numpages = {6}, keywords = {talent cultivation, computer professional, big data, practical teaching}}
@inproceedings{10.1145/3383583.3398560,title = {Research on the Development of Library and Information Science in the Era of Big Data}, author = {Liao Longwen , Li Qi , Chen Junyan },year = {2020}, isbn = {9781450375856}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3383583.3398560}, doi = {10.1145/3383583.3398560}, abstract = {To explore the key research technology knowledge graphs related to Library and Information Science(LIS), articles between 1998 and 2020 were collected from the \"Web of Science TM\". By using the visualization software CiteSpace, the pivotal literature related to big data in the field of LIS, as well as countries, institutions, and keywords, were visualized and recognized. The results show that the research hot spots in this field mainly include: big data brings influences and challenges to LIS, big data analysis technology, and data management and user privacy.}, location = {Virtual Event, China}, series = {JCDL '20}, pages = {453\u2013454}, numpages = {2}, keywords = {citespace, LIS, big data}}
@inproceedings{10.14778/3137765.3137829,title = {Complex event recognition in the big data era}, author = {Giatrakos Nikos , Artikis Alexander , Deligiannakis Antonios , Garofalakis Minos },year = {2017}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3137765.3137829}, doi = {10.14778/3137765.3137829}, abstract = {The concept of event processing is established as a generic computational paradigm in various application fields, ranging from data processing in Web environments, over maritime and transport, to finance and medicine. Events report on state changes of a system and its environment. Complex Event Recognition (CER) in turn, refers to the identification of complex/composite events of interest, which are collections of simple events that satisfy some pattern, thereby providing the opportunity for reactive and proactive measures. Examples include the recognition of attacks in computer network nodes, human activities on video content, emerging stories and trends on the Social Web, traffic and transport incidents in smart cities, fraud in electronic marketplaces, cardiac arrhythmias, and epidemic spread. In each scenario, CER allows to make sense of Big event Data streams and react accordingly. The goal of this tutorial is to provide a step-by-step guide for realizing CER in the Big Data era. To do so, it elaborates on major challenges and describes algorithmic toolkits for optimized manipulation of event streams characterized by high volume, velocity and/or lack of veracity, placing emphasis on distributed CER over potentially heterogeneous (data variety) event sources. Finally, we highlight future research directions in the field.}, pages = {1996\u20131999}, numpages = {4}}
@inproceedings{10.1145/3477911.3477921,title = {Big Data Analysis Driven Decision Making System Ensuring Energy Security of a Country}, author = {Islam Mahmudul , Hasan Mahady },year = {2021}, isbn = {9781450390521}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3477911.3477921}, doi = {10.1145/3477911.3477921}, abstract = {Energy is one of the key factors for a country's economic and social growth. Bangladesh is constantly seeking sustainable energy sources for securing its increasing energy demand as energy security is a national concern. Currently, Bangladesh is producing 64% of electricity using natural gas while 25% is coming from petroleum and the rest from coal, renewable sources [1]. To reduce pressure from the natural gas government is trying to diversify the energy sources. To achieve sustainable diversified energy sources, the Sustainable and Renewable Energy Development Authority (SREDA) has taken many initiatives. Energy Master Plan has been created which targets to improve Primary Energy consumption per GDP by 20% in the year 2030. Many software applications have been developed by SREDA to monitor the progress. We propose to develop another system for energy data analysis and generate reports accordingly. Policymakers can use those reports to make strategic decisions to secure the country's energy consumption, distribution and manage the demand side. In this research work, we discussed how a big data-based solution can be developed and used to forecast the energy balance which ensures the energy security of Bangladesh. We also discuss different aspects of the proposed software system in detail including challenges and possible solutions to overcome those challenges.}, location = {Vienna, Austria}, series = {ICCTA 2021}, pages = {60\u201365}, numpages = {6}, keywords = {Big Data, Data Analysis, Sustainable energy with big data, Energy security, Renewable Energy}}
@inproceedings{10.1145/3529836.3529950,title = {Research on Hypoxia and Fatigue Resistant Food at Plateau Based on Big Data Technology}, author = {Zhai Chenggong , Zhang Heng , Li Xiaoli , Chen Shan , Zhou Liyong , Wu Rangming },year = {2022}, isbn = {9781450395700}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3529836.3529950}, doi = {10.1145/3529836.3529950}, abstract = {The thin air, low oxygen content, harsh environment, sparse population, desolate ground and difficult concealment in the severe cold area of the plateau have a great impact on the food security of personnel going to the severe cold area of the plateau. Researchers went to the food security in the severe cold areas of the plateau, made solid preparations for various security, and achieved results for food security at the first time, which plays an important role in building and completing the food system in China. Focusing on the impact and requirements of plateau cold areas on food security, combined with the difficulties and practical problems faced by personnel going to plateau cold areas and big data technology, this paper analyzes the overview and characteristics of big data, the needs of plateau hypoxia resistance and fatigue resistance, and the advantages of plateau hypoxia resistance and fatigue resistance food, and puts forward the plateau hypoxia resistance and fatigue resistance based on big data The conception of anti fatigue food system, and the plateau food data safety system is constructed.}, location = {Guangzhou, China}, series = {ICMLC 2022}, pages = {153\u2013159}, numpages = {7}, keywords = {Hypoxia tolerance, Big data, Resist fatigue, Plateau}}
@inproceedings{10.1007/s00778-019-00557-w,title = {Complex event recognition in the Big Data era: a survey}, author = {Giatrakos Nikos , Alevizos Elias , Artikis Alexander , Deligiannakis Antonios , Garofalakis Minos },year = {2020}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg}, url = {https://doi.org/10.1007/s00778-019-00557-w}, doi = {10.1007/s00778-019-00557-w}, abstract = {The concept of event processing is established as a generic computational paradigm in various application fields. Events report on state changes of a system and its environment. Complex event recognition (CER) refers to the identification of composite events of interest, which are collections of simple, derived events that satisfy some pattern, thereby providing the opportunity for reactive and proactive measures. Examples include the recognition of anomalies in maritime surveillance, electronic fraud, cardiac arrhythmias and epidemic spread. This survey elaborates on the whole pipeline from the time CER queries are expressed in the most prominent languages, to algorithmic toolkits for scaling-out CER to clustered and geo-distributed architectural settings. We also highlight future research directions.}, pages = {313\u2013352}, numpages = {40}, keywords = {Big Data, Parallelism, Distributed processing, Complex event recognition languages, Elasticity, Complex event recognition}}
@inproceedings{10.1145/3419604.3419620,title = {A Frequency-Category Based Feature Selection in Big Data for Text Classification}, author = {Amazal Houda , Ramdani Mohammed , Kissi Mohamed },year = {2020}, isbn = {9781450377331}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3419604.3419620}, doi = {10.1145/3419604.3419620}, abstract = {In big data era, text classification is considered as one of the most important machine learning application domain. However, to build an efficient algorithm for classification, feature selection is a fundamental step to reduce dimensionality, achieve better accuracy and improve time execution. In the literature, most of the feature ranking techniques are document based. The major weakness of this approach is that it favours the terms occurring frequently in the documents and neglects the correlation between the terms and the categories. In this work, unlike the traditional approaches which deal with documents individually, we use mapreduce paradigm to process the documents of each category as a single document. Then, we introduce a parallel frequency-category feature selection method independently of any classifier to select the most relevant features. Experimental results on the 20-Newsgroups dataset showed that our approach improves the classification accuracy to 90.3%. Moreover, the system maintains the simplicity and lower execution time.}, location = {Rabat, Morocco}, series = {SITA'20}, pages = {1\u20136}, numpages = {6}, keywords = {MapReduce, Text classification, Naive Bayes, Machine Learning, Big Data, TF-IDF}}
@inproceedings{10.1145/3286606.3286793,title = {Knowledge Based Access Control a model for security and privacy in the Big Data}, author = {El Haourani Lamia , Elkalam Anas Abou , Ouahman Abdelah Ait },year = {2018}, isbn = {9781450365628}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3286606.3286793}, doi = {10.1145/3286606.3286793}, abstract = {The most popular features of Big Data revolve around the so-called \"3V\" criterion: Volume, Variety and Velocity. Big Data is based on the massive collection and in-depth analysis of personal data, with a view to profiling, or even marketing and commercialization, thus violating citizens' privacy and the security of their data.In this article we discuss security and privacy solutions in the context of Big Data. We then focus on access control and present our new model called Knowledge-based Access Control (KBAC); this strengthens the access control already deployed in the target company (e.g., based on \"RBAC\" role or \"ABAC\" attributes for example) by adding a semantic access control layer. KBAC offers thinner access control, tailored to Big Data, with effective protection against intrusion attempts and unauthorized data inferences.}, location = {Tetouan, Morocco}, series = {SCA '18}, pages = {1\u20138}, numpages = {8}, keywords = {access control model, KBAC, Big Data, privacy, security}}
@inproceedings{10.1145/3155133.3155138,title = {Trend and applications of Big Data and IoT techniques}, author = {Onizuka Makoto },year = {2017}, isbn = {9781450353281}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3155133.3155138}, doi = {10.1145/3155133.3155138}, abstract = {As people say \"Data is the new oil,\" Big data is expected to make a large impact on our society and economics by mining hidden knowledge and rules from the data. In particular, the structure of the real world data is changing from traditional relational data model to more generalized graph data model, as the web and social media are getting popular in the world. One of the most important technical challenges here is to efficiently analyze large graph data that express various types of relationship between people, items, and places. In this talk, we overview the trend of Big Data and IoT and then explain our research on distributed query optimization on cloud environment and efficient graph mining algorithms. Finally, we introduce some of our interesting applications of Big Data: 1) social network analysis by employing graph mining algorithms, 2) business data analysis by exploratory data analysis techniques, and 3) Smart route recommendation system empowered by IoT.}, location = {Nha Trang City, Viet Nam}, series = {SoICT 2017}, pages = {5}, numpages = {1}}
@inproceedings{10.1145/2539150.2539154,title = {NoSQL databases - no panacea for Big Data processing}, author = {Pokorny Jaroslav },year = {2013}, isbn = {9781450321136}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2539150.2539154}, doi = {10.1145/2539150.2539154}, abstract = {Big Data, big number of users, and cloud computing are driving the adoption of new database architectures, particularly NoSQL databases. Both research and practice indicate that traditional universal DBMS architecture hardly satisfies new trends in data processing in such environment. NoSQL databases enable better application development productivity through a more flexible data model, greater ability to scale dynamically to support more users and data, an ability to develop highly responsive applications and more complex processing of data.On the other hand, NoSQL databases support solving data problems only partially. We will describe their basic features like horizontal scalability and concurrency model, which offer mostly weaker tools for querying and transactions processing than relational SQL-like database systems do. We will also present some data models and querying capabilities of NoSQL databases in more detail as well as an overview of some their representatives.The NoSQL system properties mentioned imply that most of them are unsuitable, e.g., for the DW and BI querying or, in general, for the enterprise data processing. Consequently, new database architectures and various hybrid solutions are developed. We will point out on these actual problems and present some of the current approaches in detail.}, location = {Vienna, Austria}, series = {IIWAS '13}, pages = {4}, numpages = {1}}
@inproceedings{10.1145/2642769.2642802,title = {HPC in Big Data Age: An Evaluation Report for Java-Based Data-Intensive Applications Implemented with Hadoop and OpenMPI}, author = {Cheptsov Alexey },year = {2014}, isbn = {9781450328753}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2642769.2642802}, doi = {10.1145/2642769.2642802}, abstract = {The current IT technologies have a strong need for scaling up the high-performance analysis to large-scale datasets. Tremendously increased over the last few years volume and complexity of data gathered in both public (such as on the web) and enterprise (e.g. digitalized internal document base) domains have posed new challenges to providers of high performance computing (HPC) infrastructures, which is recognised in the community as Big Data problem. On contrast to the typical HPC applications, the Big Data ones are not oriented on reaching the peak performance of the infrastructure and thus offer more opportunities for the \"capacity\" infrastructure model rather than for the \"capability\" one, making the use of Cloud infrastructures preferable over the HPC. However, considering the more and more vanishing difference between these two infrastructure types, i.e. Cloud and HPC, it makes a lot of sense to investigate the abilities of traditional HPC infrastructure to execute Big Data applications as well, despite their relatively poor efficiency as compared with the traditional, very optimized HPC ones. This paper discusses the main state-of-the-art parallelisation techniques utilised in both Cloud and HPC domains and evaluates them on an exemplary text processing application on a testbed HPC cluster.}, location = {Kyoto, Japan}, series = {EuroMPI/ASIA '14}, pages = {175\u2013180}, numpages = {6}, keywords = {MapReduce, Cloud, Hadoop, HPC, MPI}}
@inproceedings{10.1145/3207677.3278079,title = {Towards the Big Data in Official Statistics: An Analytic Service Framework for Distributed Multiple Sourced Heterogeneous Datasets}, author = {Zhao Zhuo , Li Xingying , Li Shanzi , Wu Yixuan , Zhao Xin },year = {2018}, isbn = {9781450365123}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3207677.3278079}, doi = {10.1145/3207677.3278079}, abstract = {High volumes1 of business data is continuously produced by different kinds of information system, which provides big values for the official statistics. However, it is not easy to leverage big volume of business data for the statistical analysis under existing technologies, since they are generally distributed multiple sourced heterogeneous data set. In this paper, we first present the problem scenario and discuss in details the challenges confronting with the problem. Then, we propose an analytical framework for the distributed multiple sourced heterogeneous data set based on the service oriented architecture. Finally, we present a prototype of elementary statistical services for the primary data analysis tasks based on the proposed analytic service framework.}, location = {Hohhot, China}, series = {CSAE '18}, pages = {1\u20135}, numpages = {5}, keywords = {Big data, multiple sourced dataset, official statistics, web services, distributed computing, data mining}}
@inproceedings{10.1145/3014812.3014886,title = {Privacy preserving in big data sets through multiple shuffle}, author = {Qu Youyang , Xu Jiyang , Yu Shui },year = {2017}, isbn = {9781450347686}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3014812.3014886}, doi = {10.1145/3014812.3014886}, abstract = {Big data privacy-preserving has attracted increasing attention of researchers in recent years. But existing models are so complicated and time-consuming that they are not easy to implement. In this paper, we propose a more feasible and efficient model for big data sets privacy-preserving using shuffling multiple attributes(M-Shuffle) to achieve a tradeoff between data utility and privacy. Our strategy is firstly categorize all the records into some groups using K-means algorithm according to the sensitive attributes. Then we choose the columns to be shuffled using entropy. At last we introduce the random shuffle algorithm to our model to break the correlation among the columns of big data sets. Experiments on real-world datasets show that our framework achieves excellent data utility and efficiency while satisfying privacy-preserving.}, location = {Geelong, Australia}, series = {ACSW '17}, pages = {1\u20138}, numpages = {8}, keywords = {privacy-preserving, M-shuffle machenism, K-means}}
@inproceedings{10.1145/3012258.3012268,title = {Novel Method for Organizational Evaluation and Practice Based on Big Data Analysis}, author = {Qian Xuesheng , Xu Yifeng , Zhang Jing , Zhao Wei },year = {2016}, isbn = {9781450347617}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3012258.3012268}, doi = {10.1145/3012258.3012268}, abstract = {Organizational evaluation plays an essential role in the decision-making and administrative management. Although there are numbers of conventional evaluation methods, the assessing result is subjective and limited by the application fields of data utilization, let along other drawbacks. With the perspective of big data, this thesis employs lower granularity and promotes the big-data based novel organizational evaluation pattern, in order to overcome the shortage of the conventional evaluation system. Then the thesis applies the big data method to elucidate an arduous assessing analysis case - the typical school evaluation problem with the process of standardizing and normalizing the 200,000 students' activity information in hundreds of k-12 schools of a district in Shanghai. Accordingly, the thesis accomplishes the assessment diagnosis on the school management and shares the effective supplements and risk warnings compared with the conventional ones.}, location = {Istanbul, Turkey}, series = {ICIME 2016}, pages = {30\u201335}, numpages = {6}, keywords = {Chinese Short Texts De-duplication, Big-Data Assessment, Organizational Evaluation, Data Acquisition, Behavior Record}}
@inproceedings{10.14778/2733004.2733069,title = {Knowledge bases in the age of big data analytics}, author = {Suchanek Fabian M. , Weikum Gerhard },year = {2014}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2733004.2733069}, doi = {10.14778/2733004.2733069}, abstract = {This tutorial gives an overview on state-of-the-art methods for the automatic construction of large knowledge bases and harnessing them for data and text analytics. It covers both big-data methods for building knowledge bases and knowledge bases being assets for big-data applications. The tutorial also points out challenges and research opportunities.}, pages = {1713\u20131714}, numpages = {2}}
@inproceedings{10.1145/1363686.1363915,title = {Managing data quality in a terabyte-scale sensor archive}, author = {Cutt Bryce , Lawrence Ramon },year = {2008}, isbn = {9781595937537}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1363686.1363915}, doi = {10.1145/1363686.1363915}, abstract = {Sensor networks collect vast amounts of real-time information about the environment, business processes, and systems. Archived sensor data is valuable for long-term analysis and decision making, which requires it be suitably archived, indexed, and validated. In this paper, we describe a general approach to managing and improving data quality by the generation and validation of metadata and the logging of workflow events. The approach has been implemented within a system archiving terabytes of U.S. weather radar data. The data quality system has resulted in the detection of data errors while simplifying the administration of the complex archive system.}, location = {Fortaleza, Ceara, Brazil}, series = {SAC '08}, pages = {982\u2013986}, numpages = {5}, keywords = {data quality, scientific data, hydrology, real-time warehouse, archive, sensor network}}
@inproceedings{10.1145/2983642,title = {A Deployment Optimization Scheme Over Multimedia Big Data for Large-Scale Media Streaming Application}, author = {Wu Taotao , Dou Wanchun , Wu Fan , Tang Shaojie , Hu Chunhua , Chen Jinjun },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2983642}, doi = {10.1145/2983642}, abstract = {With the prosperity of media streaming applications over the Internet in the past decades, multimedia data has sharply increased (categorized as multimedia big data), which exerts more pressure on the infrastructure, such as networking of the application provider. In order to move this hurdle, an increasing number of traditional media streaming applications have migrated from a private server cluster onto the cloud. With the elastic resource provisioning and centralized management of the cloud, the operational costs of media streaming application providers can decrease dramatically. However, to the best of our knowledge, existing migration solutions do not fully take viewer information such as hardware condition into consideration. In this article, we consider the deployment optimization problem named ODP by leveraging local memories at each viewer. Considering the NP-hardness of calculating the optimal solution, we turn to propose computationally tractable algorithms. Specifically, we unfold the original problem into two interactive subproblems: coarse-grained migration subproblem and fine-grained scheduling subproblem. Then, the corresponding offline approximation algorithms with performance guarantee and computational efficiency are given. The results of extensive evaluation show that compared with the baseline algorithm without leveraging local memories at viewers, our proposed algorithms and their online versions can decrease total bandwidth reservation and enhance the utilization of bandwidth reservation dramatically.}, pages = {1\u201323}, numpages = {23}, keywords = {multimedia big data, cloud, deployment optimization, local memory, Large-scale media streaming application}}
@inproceedings{10.1109/CCGRID.2017.143,title = {Evaluation of HPC-Big Data Applications Using Cloud Platforms}, author = {Salaria Shweta , Brown Kevin , Jitsumoto Hideyuki , Matsuoka Satoshi },year = {2017}, isbn = {9781509066100}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGRID.2017.143}, doi = {10.1109/CCGRID.2017.143}, abstract = {The path to HPC-Big Data convergence has resulted in numerous researches that demonstrate the performance trade-off between running applications on supercomputers and cloud platforms. Previous studies typically focus either on scientific HPC benchmarks or previous cloud configurations, failing to consider all the new opportunities offered by current cloud offerings. We present a comparative study of the performance of representative big data benchmarks, or \"Big Data Ogres\", and HPC benchmarks running on supercomputer and cloud. Our work distinguishes itself from previous studies in a way that we explore the latest generation of compute-optimized Amazon Elastic Compute Cloud instances, C4 for our experimentation on cloud. Our results reveal that Amazon C4 instances with increased compute performance and low variability in results make EC2-based cluster feasible for scientific computing and its applications in simulations, modeling and analysis.}, location = {Madrid, Spain}, series = {CCGrid '17}, pages = {1053\u20131061}, numpages = {9}, keywords = {Supercomputers, Performance evaluation, Graph500, Amazon EC2 C4}}
@inproceedings{10.1145/3341069.3341086,title = {The Construction Study of College Informationization Teaching Service System under the Background of Big Data}, author = {Pengxi Li },year = {2019}, isbn = {9781450371858}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341069.3341086}, doi = {10.1145/3341069.3341086}, abstract = {Under the background of the rapid development of big data technology, the construction of college informationization teaching service system is the basis of the informationization teaching in colleges and universities. That is very important for the success of the informatization in Universities what is service realization model, business logic, architecture and platform conform to the whole development strategy of universities. Information management organization supports the planning, implementation, operation, maintenance and management of business information system. This paper analyzes the reform mode of college education information service system supported by big data technology. Based on the analysis of the reform mode of college informationization teaching service system supported by big data technology, this paper puts forward the design idea of post system based on big data. At the same time, with the case of \"big data assisted employment\", the post design and adjustment were carried out. The results show that big data assisted employment has greatly improved the efficiency and quality of the school's employment department, providing students with better employment security. Finally, the problems that need to be solved in the informatization teaching service are sorted out.}, location = {Guangzhou, China}, series = {HPCCT '19}, pages = {185\u2013189}, numpages = {5}, keywords = {Big data, Teaching informatization, Service system}}
@inproceedings{10.1145/3488466.3488493,title = {Discussion on Customizable Education of Colleges Based on Educational Big Data: Customizable Education of Colleges}, author = {Cai Huiying },year = {2021}, isbn = {9781450384995}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3488466.3488493}, doi = {10.1145/3488466.3488493}, abstract = {With the improvement of network, online teaching has gradually become an important auxiliary teaching mean. In the whole process of teaching and learning, a large amount of educational data is produced. Educational data contain a lot of information to be mined, which is helpful to improve the quality of learning and teaching. This paper will explore how to integrate these educational data that comes from different educational platform to guide customizable education which refers to determine the learning or teaching content independently. To realize the customizable education, the evaluation indicators and detail scheme to make use of the educational big data are proposed. The scheme consists of the acquisition, analysis and visualization of the data for different evaluation indicators. The purpose of this paper is to make these data guide undergraduates to carry out targeted autonomous learning according to their states to promote the learning progress. It can also guide teachers to carry out targeted teaching activities to improve teaching quality. And it is also helpful for teaching managers to have an insight into the learning state of students and the teaching state of the teachers, so as to put forward more reasonable teaching plans and countermeasures. This paper provided a whole framework for the application of the educational big data which can be extended by different educational institutions.}, location = {Busan, Republic of Korea}, series = {ICDTE 2021}, pages = {61\u201366}, numpages = {6}, keywords = {Educational big data, Customizable, Visualization, Mining}}
@inproceedings{10.1145/3075564.3078885,title = {Sorting big data on heterogeneous near-data processing systems}, author = {Vermij Erik , Fiorin Leandro , Hagleitner Christoph , Bertels Koen },year = {2017}, isbn = {9781450344876}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3075564.3078885}, doi = {10.1145/3075564.3078885}, abstract = {Big data workloads assumed recently a relevant importance in many business and scientific applications. Sorting elements efficiently in big data workloads is a key operation. In this work, we analyze the implementation of the mergesort algorithm on heterogeneous systems composed of CPUs and near-data processors located on the system memory channels. For configurations with equal number of active CPU cores and near-data processors, our experiments show a performance speedup of up to 2.5, as well as up to 2.5x energy-per-solution reduction.}, location = {Siena, Italy}, series = {CF'17}, pages = {349\u2013354}, numpages = {6}}
@inproceedings{10.5555/3344051.3344077,title = {Using jupyter notebooks in a big data programming course}, author = {DePratti Roland },year = {2019}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {In a Big Data Programming course, students often need basic instruction in a new programming language, i.e. Python, Scala, R. Traditional programming language instruction involves a textbook and an Interactive Development Environment (IDE). In a course that already included two textbooks and instruction on Big Data frameworks, the author was looking for an effective way to deliver instructional text and the interactive development capabilities of an IDE that would not add additional cost to the student.}, pages = {157\u2013159}, numpages = {3}}
@inproceedings{10.1145/3399205.3399228,title = {Adoption of Big Data, Cloud Computing & IoT in Morocco Perception of Public Administrations Collaborators}, author = {Moumen Aniss },year = {2020}, isbn = {9781450375788}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3399205.3399228}, doi = {10.1145/3399205.3399228}, abstract = {The integration of new technologies such as big data, cloud computing, and the Internet of Things (IoT), constitute a new challenge for existing information systems of local administrations.In this work, we present the results of interviews conducted with the public administrations in the South-eastern region of Morocco, and as conclusion we expose two proposed models induced by this study.}, location = {Al-Hoceima, Morocco}, series = {GEOIT4W-2020}, pages = {1\u20134}, numpages = {4}, keywords = {Information System, IoT, Big data, Cloud Computing}}
@inproceedings{10.5555/2735522.2735575,title = {Smart big data analytics as a service framework: a proposal}, author = {Khalifa Shadi , Martin Patrick },year = {2014}, publisher = {IBM Corp.}, address = {USA}, abstract = {We propose the Smart Big Data Analytics as a Service framework. A framework to empower in-house business users with intelligent assistance throughout the analytics process. It provides distributed in-memory data processing and an easy-to-learn-and-use analytics query language for data exploration, preprocessing and analytical workflow orchestration. The framework is designed as a service to take advantage of the Cloud's features.}, location = {Markham, Ontario, Canada}, series = {CASCON '14}, pages = {327\u2013330}, numpages = {4}}
@inproceedings{10.1109/TCBB.2014.2351800,title = {Heterogeneous cloud framework for big data genome sequencing}, author = {Wang Chao , Li Xi , Chen Peng , Wang Aili , Zhou Xuehai , Yu Hong },year = {2015}, publisher = {IEEE Computer Society Press}, address = {Washington, DC, USA}, url = {https://doi.org/10.1109/TCBB.2014.2351800}, doi = {10.1109/TCBB.2014.2351800}, abstract = {The next generation genome sequencing problem with short (long) reads is an emerging field in numerous scientific and big data research domains. However, data sizes and ease of access for scientific researchers are growing and most current methodologies rely on one acceleration approach and so cannot meet the requirements imposed by explosive data scales and complexities. In this paper, we propose a novel FPGA-based acceleration solution with MapReduce framework on multiple hardware accelerators. The combination of hardware acceleration and MapReduce execution flow could greatly accelerate the task of aligning short length reads to a known reference genome. To evaluate the performance and other metrics, we conducted a theoretical speedup analysis on a MapReduce programming platform, which demonstrates that our proposed architecture have efficient potential to improve the speedup for large scale genome sequencing applications. Also, as a practical study, we have built a hardware prototype on the real Xilinx FPGA chip. Significant metrics on speedup, sensitivity, mapping quality, error rate, and hardware cost are evaluated, respectively. Experimental results demonstrate that the proposed platform could efficiently accelerate the next generation sequencing problem with satisfactory accuracy and acceptable hardware cost.}, pages = {166\u2013178}, numpages = {13}, keywords = {FPGA, mapping, genome sequencing, short reads, reconfigurable hardware}}
@inproceedings{10.1145/3356998.3365776,title = {Risk prediction and assessment of foodborne disease based on big data}, author = {Zhang Mingke , Guo Danhuai , Hu Jinyong , Jin Wei },year = {2019}, isbn = {9781450369657}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3356998.3365776}, doi = {10.1145/3356998.3365776}, abstract = {In recent years, the outbreak of foodborne diseases has been on an upward trend clearly. It is of great significance for us to predict the outbreak of foodborne diseases accurately and conduct quantitative risk assessment timely. Traditional prediction methods based on a single data source have drawbacks such as complex prediction processes and inaccurate prediction results. In this article, we figure out the scientific issues of how to improve the temporal and spatial accuracy of foodborne disease outbreak risk prediction in Beijing. Firstly, we analyze the different foodborne disease risk factors caused by the spread of water pollution in Beijing, and study the methods of collecting and preprocessing multi-source data. Then, through the comparison of different regression models and parameters tuning, we propose the use of Gradient Boosting Regression (GBR) model as a multi-source data fusion model to predict the outbreak of foodborne disease. Finally, we use the risk map to detect and predict foodborne disease outbreak in different business districts of Beijing based on visualization techniques, aiming to provide prevention and control assessment for decision-makers quickly and precisely.}, location = {Chicago, Illinois}, series = {EM-GIS '19}, pages = {1\u20136}, numpages = {6}, keywords = {big data, machine learning, foodborne disease, risk assessment}}
@inproceedings{10.1145/2935882,title = {Big data analytics and revision of the common rule}, author = {Metcalf Jacob },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2935882}, doi = {10.1145/2935882}, abstract = {Reconsidering traditional research ethics given the emergence of big data analytics.}, pages = {31\u201333}, numpages = {3}}
@inproceedings{10.1145/2856059,title = {Discovering User Behavioral Features to Enhance Information Search on Big Data}, author = {Cassavia Nunziato , Masciari Elio , Pulice Chiara , Sacc\u00e0 Domenico },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2856059}, doi = {10.1145/2856059}, abstract = {Due to the emerging Big Data paradigm, driven by the increasing availability of intelligent services easily accessible by a large number of users (e.g., social networks), traditional data management techniques are inadequate in many real-life scenarios. In particular, the availability of huge amounts of data pertaining to user social interactions, user preferences, and opinions calls for advanced analysis strategies to understand potentially interesting social dynamics. Furthermore, heterogeneity and high speed of user-generated data require suitable data storage and management tools to be designed from scratch. This article presents a framework tailored for analyzing user interactions with intelligent systems while seeking some domain-specific information (e.g., choosing a good restaurant in a visited area). The framework enhances a user's quest for information by exploiting previous knowledge about their social environment, the extent of influence the users are potentially subject to, and the influence they may exert on other users. User influence spread across the network is dynamically computed as well to improve user search strategy by providing specific suggestions, represented as tailored faceted features. Such features are the result of data exchange activity (called data posting) that enriches information sources with additional background information and knowledge derived from experiences and behavioral properties of domain experts and users. The approach is tested in an important application scenario such as tourist recommendation, but it can be profitably exploited in several other contexts, for example, viral marketing and food education.}, pages = {1\u201333}, numpages = {33}, keywords = {user behavior, information extraction, NoSQL databases, intelligent recommendation, personal big data}}
@inproceedings{10.1145/2783258.2783372,title = {Efficient Online Evaluation of Big Data Stream Classifiers}, author = {Bifet Albert , de Francisci Morales Gianmarco , Read Jesse , Holmes Geoff , Pfahringer Bernhard },year = {2015}, isbn = {9781450336642}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2783258.2783372}, doi = {10.1145/2783258.2783372}, abstract = {The evaluation of classifiers in data streams is fundamental so that poorly-performing models can be identified, and either improved or replaced by better-performing models. This is an increasingly relevant and important task as stream data is generated from more sources, in real-time, in large quantities, and is now considered the largest source of big data. Both researchers and practitioners need to be able to effectively evaluate the performance of the methods they employ. However, there are major challenges for evaluation in a stream. Instances arriving in a data stream are usually time-dependent, and the underlying concept that they represent may evolve over time. Furthermore, the massive quantity of data also tends to exacerbate issues such as class imbalance. Current frameworks for evaluating streaming and online algorithms are able to give predictions in real-time, but as they use a prequential setting, they build only one model, and are thus not able to compute the statistical significance of results in real-time. In this paper we propose a new evaluation methodology for big data streams. This methodology addresses unbalanced data streams, data where change occurs on different time scales, and the question of how to split the data between training and testing, over multiple models.}, location = {Sydney, NSW, Australia}, series = {KDD '15}, pages = {59\u201368}, numpages = {10}, keywords = {online learning, data streams, classification, evaluation}}
@inproceedings{10.1145/3075564.3078884,title = {Big Data Analytics on Large-Scale Scientific Datasets in the INDIGO-DataCloud Project}, author = {Fiore Sandro , Palazzo Cosimo , D'Anca Alessandro , Elia Donatello , Londero Elisa , Knapic Cristina , Monna Stephen , Marcucci Nicola M. , Aguilar Fernando , P\u0142\u00f3ciennik Marcin , De Lucas Jes\u00fas E. Marco , Aloisio Giovanni },year = {2017}, isbn = {9781450344876}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3075564.3078884}, doi = {10.1145/3075564.3078884}, abstract = {In the context of the EU H2020 INDIGO-DataCloud project several use case on large scale scientific data analysis regarding different research communities have been implemented. All of them require the availability of large amount of data related to either output of simulations or observed data from sensors and need scientific (big) data solutions to run data analysis experiments. More specifically, the paper presents the case studies related to the following research communities: (i) the European Multidisciplinary Seafloor and water column Observatory (INGV-EMSO), (ii) the Large Binocular Telescope, (iii) LifeWatch, and (iv) the European Network for Earth System Modelling (ENES).}, location = {Siena, Italy}, series = {CF'17}, pages = {343\u2013348}, numpages = {6}, keywords = {Workflow, scientific use case, ensemble analysis, big data}}
@inproceedings{10.1145/2232817.2232911,title = {Responsibility for research data quality in open access: a slovenian case}, author = {Stebe Janez },year = {2012}, isbn = {9781450311540}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2232817.2232911}, doi = {10.1145/2232817.2232911}, abstract = {In the framework of a project aiming to realize a strategy of open research data access in Slovenia in accordance with OECD principles, we conducted a series of interviews with different target audiences in order to assess the initial conditions in the area of data handling. The data creators and data services expressed a high level of awareness about data quality issues, especially in relation to good publication potential. Barriers to ensuring the greater accessibility of data in the future include the little recognition and reputation for doing the related extra work involved in preparing data and documentation, the need for financial rewards for such additional work, and the undeveloped culture of data exchange in general. The motivation to provide open access to such data will involve a combination of requirements prescribed for data delivery, and the provision of support services and financial rewards, in particular changing the views held by the professional scientific community about the benefits of open data for research activities.}, location = {Washington, DC, USA}, series = {JCDL '12}, pages = {401\u2013402}, numpages = {2}, keywords = {culture, data quality, stakeholders attitudes, open data}}
@inproceedings{10.14778/3352063.3352128,title = {Customizable and scalable fuzzy join for big data}, author = {Chen Zhimin , Wang Yue , Narasayya Vivek , Chaudhuri Surajit },year = {2019}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3352063.3352128}, doi = {10.14778/3352063.3352128}, abstract = {Fuzzy join is an important primitive for data cleaning. The ability to customize fuzzy join is crucial to allow applications to address domain-specific data quality issues such as synonyms and abbreviations. While efficient indexing techniques exist for single-node implementations of customizable fuzzy join, the state-of-the-art scale-out techniques do not support customization, and exhibit poor performance and scalability characteristics. We describe the design of a scale-out fuzzy join operator that supports customization. We use a locality-sensitive-hashing (LSH) based signature scheme, and introduce optimizations that result in significant speed up with negligible impact on recall. We evaluate our implementation on the Azure Databricks version of Spark using several real-world and synthetic data sets. We observe speedups exceeding 50X compared to the best-known prior scale-out technique, and close to linear scalability with data size and number of nodes.}, pages = {2106\u20132117}, numpages = {12}}
@inproceedings{10.1145/2987386.2987429,title = {Parallel Job Processing Technique for Real-time Big-Data Processing Framework}, author = {Son Jae Gi , Kang Ji-Woo , An Jae-Hoon , Ahn Hyung-Joo , Chun Hyo-Jung , Kim Jung-Guk },year = {2016}, isbn = {9781450344555}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2987386.2987429}, doi = {10.1145/2987386.2987429}, abstract = {Since the introduction of big data, numerous researches aiming to improve the accuracy and speed of data processing has been conducted. Many platforms that can process real-time data were developed for this purpose. Most standard data processing platforms used Spark Streaming as data analysis layer. However, its limitation in performance calls for a better alternative. This paper introduces a new data processing framework, Squall. Squall utilizes parallel processing and allows real-time data processing using streaming modules. Go was used for development. Through various experiments, the performance of our newly developed framework on processing real-time data was compared to the performance of the previously existing framework completing the same task. Results show quantitative evidence that Squall excel the platforms that use Spark Streaming. Our future work includes making modifications that will improve Squall's performance.}, location = {Odense, Denmark}, series = {RACS '16}, pages = {226\u2013229}, numpages = {4}, keywords = {Parallel Job Processing, Apache Spark, Realtime Packet Analysis, Real-time Big-Data Processing Framework, Squall}}
@inproceedings{10.1145/3483816.3483836,title = {Fusion from Big Data to Smart Data to enhance quality of Information Systems}, author = {Febiri Frank , Yihum Amare Meseret , Hub Miloslav },year = {2021}, isbn = {9781450390545}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3483816.3483836}, doi = {10.1145/3483816.3483836}, abstract = {The term \u201csmartness\u201d in the data framework indicates relevancy based on the intended purpose of data. The Internet of Things (IoT) and advancements in technology have resulted in an ever-increasing pool of data available to all institutions to derive meaning and make sound decisions from them. The research presented in this paper explored the role smart data play in information systems quality through a qualitative study of how using the large pool of data (big data) and fusing it to smart data organizations can make sound and intelligent decisions using the available techniques. We use an existing architecture for a public institution to analyze how data ingestion can be achieved with minimum challenges. The findings suggest that even though there is a large pool of data for most organizations, it is becoming more challenging to use this data to make organizational sense due to the challenges posed by such data. The realization of smart data and its benefits in information systems helps improve the quality of information systems, reducing cost and promoting the smartness agenda of today's organization.}, location = {Jeju, Republic of Korea}, series = {ICMECG 2021}, pages = {112\u2013117}, numpages = {6}, keywords = {Big Data, Information systems, Quality measures, Smart data}}
@inproceedings{10.1145/3321408.3322865,title = {Research on the cultivation of new engineering talents based on educational big data}, author = {Zhao Junmin , Li Dengao , Wang Xiaoyu , Bai Xiaohong , Zhuang Shasha },year = {2019}, isbn = {9781450371582}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3321408.3322865}, doi = {10.1145/3321408.3322865}, abstract = {With the development and application of education and big data, big data continues to focus on classroom teaching and learning. In view of the normal application of the teaching platform and the demand of social talents, it is of practical significance to make an empirical study on the educational big data analysis based on the educational platform. The interaction of education platform can be described by \"STM triangle model\", including teacher-media interaction, student-media interaction, teacher-student interaction, student-teacher interaction, student-student interaction and data relationship. The data mining process of educational platform is divided into target understanding, data cleaning, data analysis, data presentation and other steps. Normal response to Education platform Based on the real data, an index analysis model of teacher-student interaction is constructed, and an empirical analysis is carried out to provide a practical example for the analysis and application of education big data.}, location = {Chengdu, China}, series = {ACM TURC '19}, pages = {1\u20132}, numpages = {2}, keywords = {data mining, educational platform, STM mode, education big data}}
@inproceedings{10.14778/1920841.1921063,title = {Big data and cloud computing: new wine or just new bottles?}, author = {Agrawal Divyakant , Das Sudipto , El Abbadi Amr },year = {2010}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/1920841.1921063}, doi = {10.14778/1920841.1921063}, abstract = {Cloud computing is an extremely successful paradigm of service oriented computing and has revolutionized the way computing infrastructure is abstracted and used. Three most popular cloud paradigms include: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). The concept however can also be extended to Database as a Service and many more. Elasticity, pay-per-use, low upfront investment, low time to market, and transfer of risks are some of the major enabling features that make cloud computing a ubiquitous paradigm for deploying novel applications which were not economically feasible in a traditional enterprise infrastructure settings. This has seen a proliferation in the number of applications which leverage various cloud platforms, resulting in a tremendous increase in the scale of the data generated as well as consumed by such applications. Scalable database management systems (DBMS) -- both for update intensive application workloads, as well as decision support systems for descriptive and deep analytics -- are thus a critical part of cloud infrastructures.}, pages = {1647\u20131648}, numpages = {2}}
@inproceedings{10.1145/2534921.2534926,title = {Breaking the big data barrier by enhancing on-board sensor flexibility}, author = {Baumann Peter , Dumitru Alex , Merticariu Vlad , Misev Dimitar , Rusu Mihaela },year = {2013}, isbn = {9781450325349}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2534921.2534926}, doi = {10.1145/2534921.2534926}, abstract = {Modern sensors, such as hyperspectral cameras, deliver massive amounts of data. On board of satellites, the high volume is paired with low bandwidth and part-time availability, during overpasses. This leads to well-known availability problems and bottlenecks in today's remote sensing.We address this challenge by enhancing the on-board system with flexible filtering and processing capabilities based on the Array Analytics engine, rasdaman. Users then can exact request, which can lead to substantially decreased data traffic. Our project has been accepted for a CubeSat mission for which rasdaman now has been prepared. We present the project setup and core extensions done to rasdaman to this end.}, location = {Orlando, Florida}, series = {BigSpatial '13}, pages = {32\u201336}, numpages = {5}, keywords = {on-board intelligence, big data, rasdaman, array databases}}
@inproceedings{10.1145/3447548.3469468,title = {2nd International Workshop on Data Quality Assessment for Machine Learning}, author = {Patel Hima , Ishikawa Fuyuki , Berti-Equille Laure , Gupta Nitin , Mehta Sameep , Masuda Satoshi , Mujumdar Shashank , Afzal Shazia , Bedathur Srikanta , Nishi Yasuharu },year = {2021}, isbn = {9781450383325}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3447548.3469468}, doi = {10.1145/3447548.3469468}, abstract = {The 2nd International Workshop on Data Quality Assessment for Machine Learning (DQAML'21) is organized in conjunction with the Special Interest Group on Knowledge Discovery and Data Mining (SIGKDD). This workshop aims to serve as a forum for the presentation of research related to data quality assessment and remediation in AI/ML pipeline. Data quality is a critical issue in the data preparation phase and involves numerous challenging problems related to detection, remediation, visualization and evaluation of data issues. The workshop aims to provide a platform to researchers and practitioners to discuss such challenges across different modalities of data like structured, time series, text and graphical. The aim is to attract perspectives from both industrial and academic circles.}, location = {Virtual Event, Singapore}, series = {KDD '21}, pages = {4147\u20134148}, numpages = {2}, keywords = {machine learning, data quality, data assessment}}
@inproceedings{10.1145/3093338.3104145,title = {High Performance GP-Based Approach for fMRI Big Data Classification}, author = {Tahmassebi Amirhessam , Gandomi Amir H. , Meyer-B\u00e4se Anke },year = {2017}, isbn = {9781450352727}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3093338.3104145}, doi = {10.1145/3093338.3104145}, abstract = {We consider resting-state Functional Magnetic Resonance Imaging (fMRI) of two classes of patients: one that took the drug N-acetylcysteine (NAC) and the other one a placebo before and after a smoking cessation treatment. Our goal was to classify the relapse in nicotine-dependent patients as treatment or non-treatment based on their fMRI scans. 80% accuracy was obtained using Independent Component Analysis (ICA) along with Genetic Programming (GP) classifier using High Performance Computing (HPC) which we consider significant enough to suggest that there is a difference in the resting-state fMRI images of a smoker that undergoes this smoking cessation treatment compared to a smoker that receives a placebo.}, location = {New Orleans, LA, USA}, series = {PEARC17}, pages = {1\u20134}, numpages = {4}, keywords = {fMRI Big Data, Classification, High Performance Computing, Generic Programming}}
@inproceedings{10.5555/3233397.3233443,title = {Enabling big data analytics in the hybrid cloud using iterative mapreduce}, author = {Clemente-Castell\u00f3 Francisco J. , Nicolae Bogdan , Katrinis Kostas , Rafique M. Mustafa , Mayo Rafael , Fern\u00e1ndez Juan Carlos , Loreti Daniela },year = {2015}, isbn = {9780769556970}, publisher = {IEEE Press}, abstract = {The cloud computing model has seen tremendous commercial success through its materialization via two prominent models to date, namely public and private cloud. Recently, a third model combining the former two service models as on-/off-premise resources has been receiving significant market traction: hybrid cloud. While state of art techniques that address workload performance prediction and efficient workload execution over hybrid cloud setups exist, how to address data-intensive workloads - including Big Data Analytics - in similar environments is nascent. This paper addresses this gap by taking on the challenge of bursting over hybrid clouds for the benefit of accelerating iterative MapReduce applications. We first specify the challenges associated with data locality and data movement in such setups. Subsequently, we propose a novel technique to address the locality issue, without requiring changes to the MapReduce framework or the underlying storage layer. In addition, we contribute with a performance prediction methodology that combines modeling with micro-benchmarks to estimate completion time for iterative MapReduce applications, which enables users to estimate cost-to-solution before committing extra resources from public clouds. We show through experimentation in a dual-Openstack hybrid cloud setup that our solutions manage to bring substantial improvement at predictable cost-control for two real-life iterative MapReduce applications: large-scale machine learning and text analysis.}, location = {Limassol, Cyprus}, series = {UCC '15}, pages = {290\u2013299}, numpages = {10}, keywords = {hybrid cloud, mapreduce, data locality, performance prediction, iterative applications, big data analytics}}