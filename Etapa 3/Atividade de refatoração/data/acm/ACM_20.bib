@inproceedings{10.1145/2642769.2642802,title = {HPC in Big Data Age: An Evaluation Report for Java-Based Data-Intensive Applications Implemented with Hadoop and OpenMPI}, author = {Cheptsov Alexey },year = {2014}, isbn = {9781450328753}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2642769.2642802}, doi = {10.1145/2642769.2642802}, abstract = {The current IT technologies have a strong need for scaling up the high-performance analysis to large-scale datasets. Tremendously increased over the last few years volume and complexity of data gathered in both public (such as on the web) and enterprise (e.g. digitalized internal document base) domains have posed new challenges to providers of high performance computing (HPC) infrastructures, which is recognised in the community as Big Data problem. On contrast to the typical HPC applications, the Big Data ones are not oriented on reaching the peak performance of the infrastructure and thus offer more opportunities for the \"capacity\" infrastructure model rather than for the \"capability\" one, making the use of Cloud infrastructures preferable over the HPC. However, considering the more and more vanishing difference between these two infrastructure types, i.e. Cloud and HPC, it makes a lot of sense to investigate the abilities of traditional HPC infrastructure to execute Big Data applications as well, despite their relatively poor efficiency as compared with the traditional, very optimized HPC ones. This paper discusses the main state-of-the-art parallelisation techniques utilised in both Cloud and HPC domains and evaluates them on an exemplary text processing application on a testbed HPC cluster.}, location = {Kyoto, Japan}, series = {EuroMPI/ASIA '14}, pages = {175\u2013180}, numpages = {6}, keywords = {Cloud, MapReduce, Hadoop, MPI, HPC}}
@inproceedings{10.1145/3155133.3155138,title = {Trend and applications of Big Data and IoT techniques}, author = {Onizuka Makoto },year = {2017}, isbn = {9781450353281}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3155133.3155138}, doi = {10.1145/3155133.3155138}, abstract = {As people say \"Data is the new oil,\" Big data is expected to make a large impact on our society and economics by mining hidden knowledge and rules from the data. In particular, the structure of the real world data is changing from traditional relational data model to more generalized graph data model, as the web and social media are getting popular in the world. One of the most important technical challenges here is to efficiently analyze large graph data that express various types of relationship between people, items, and places. In this talk, we overview the trend of Big Data and IoT and then explain our research on distributed query optimization on cloud environment and efficient graph mining algorithms. Finally, we introduce some of our interesting applications of Big Data: 1) social network analysis by employing graph mining algorithms, 2) business data analysis by exploratory data analysis techniques, and 3) Smart route recommendation system empowered by IoT.}, location = {Nha Trang City, Viet Nam}, series = {SoICT 2017}, pages = {5}, numpages = {1}}
@inproceedings{10.1109/CCGRID.2017.143,title = {Evaluation of HPC-Big Data Applications Using Cloud Platforms}, author = {Salaria Shweta , Brown Kevin , Jitsumoto Hideyuki , Matsuoka Satoshi },year = {2017}, isbn = {9781509066100}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGRID.2017.143}, doi = {10.1109/CCGRID.2017.143}, abstract = {The path to HPC-Big Data convergence has resulted in numerous researches that demonstrate the performance trade-off between running applications on supercomputers and cloud platforms. Previous studies typically focus either on scientific HPC benchmarks or previous cloud configurations, failing to consider all the new opportunities offered by current cloud offerings. We present a comparative study of the performance of representative big data benchmarks, or \"Big Data Ogres\", and HPC benchmarks running on supercomputer and cloud. Our work distinguishes itself from previous studies in a way that we explore the latest generation of compute-optimized Amazon Elastic Compute Cloud instances, C4 for our experimentation on cloud. Our results reveal that Amazon C4 instances with increased compute performance and low variability in results make EC2-based cluster feasible for scientific computing and its applications in simulations, modeling and analysis.}, location = {Madrid, Spain}, series = {CCGrid '17}, pages = {1053\u20131061}, numpages = {9}, keywords = {Amazon EC2 C4, Supercomputers, Graph500, Performance evaluation}}
@inproceedings{10.1145/3207677.3278079,title = {Towards the Big Data in Official Statistics: An Analytic Service Framework for Distributed Multiple Sourced Heterogeneous Datasets}, author = {Zhao Zhuo , Li Xingying , Li Shanzi , Wu Yixuan , Zhao Xin },year = {2018}, isbn = {9781450365123}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3207677.3278079}, doi = {10.1145/3207677.3278079}, abstract = {High volumes1 of business data is continuously produced by different kinds of information system, which provides big values for the official statistics. However, it is not easy to leverage big volume of business data for the statistical analysis under existing technologies, since they are generally distributed multiple sourced heterogeneous data set. In this paper, we first present the problem scenario and discuss in details the challenges confronting with the problem. Then, we propose an analytical framework for the distributed multiple sourced heterogeneous data set based on the service oriented architecture. Finally, we present a prototype of elementary statistical services for the primary data analysis tasks based on the proposed analytic service framework.}, location = {Hohhot, China}, series = {CSAE '18}, pages = {1\u20135}, numpages = {5}, keywords = {distributed computing, web services, official statistics, data mining, multiple sourced dataset, Big data}}
@inproceedings{10.14778/2733004.2733069,title = {Knowledge bases in the age of big data analytics}, author = {Suchanek Fabian M. , Weikum Gerhard },year = {2014}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2733004.2733069}, doi = {10.14778/2733004.2733069}, abstract = {This tutorial gives an overview on state-of-the-art methods for the automatic construction of large knowledge bases and harnessing them for data and text analytics. It covers both big-data methods for building knowledge bases and knowledge bases being assets for big-data applications. The tutorial also points out challenges and research opportunities.}, pages = {1713\u20131714}, numpages = {2}}
@inproceedings{10.1145/2983642,title = {A Deployment Optimization Scheme Over Multimedia Big Data for Large-Scale Media Streaming Application}, author = {Wu Taotao , Dou Wanchun , Wu Fan , Tang Shaojie , Hu Chunhua , Chen Jinjun },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2983642}, doi = {10.1145/2983642}, abstract = {With the prosperity of media streaming applications over the Internet in the past decades, multimedia data has sharply increased (categorized as multimedia big data), which exerts more pressure on the infrastructure, such as networking of the application provider. In order to move this hurdle, an increasing number of traditional media streaming applications have migrated from a private server cluster onto the cloud. With the elastic resource provisioning and centralized management of the cloud, the operational costs of media streaming application providers can decrease dramatically. However, to the best of our knowledge, existing migration solutions do not fully take viewer information such as hardware condition into consideration. In this article, we consider the deployment optimization problem named ODP by leveraging local memories at each viewer. Considering the NP-hardness of calculating the optimal solution, we turn to propose computationally tractable algorithms. Specifically, we unfold the original problem into two interactive subproblems: coarse-grained migration subproblem and fine-grained scheduling subproblem. Then, the corresponding offline approximation algorithms with performance guarantee and computational efficiency are given. The results of extensive evaluation show that compared with the baseline algorithm without leveraging local memories at viewers, our proposed algorithms and their online versions can decrease total bandwidth reservation and enhance the utilization of bandwidth reservation dramatically.}, pages = {1\u201323}, numpages = {23}, keywords = {cloud, Large-scale media streaming application, multimedia big data, local memory, deployment optimization}}
@inproceedings{10.1145/1363686.1363915,title = {Managing data quality in a terabyte-scale sensor archive}, author = {Cutt Bryce , Lawrence Ramon },year = {2008}, isbn = {9781595937537}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1363686.1363915}, doi = {10.1145/1363686.1363915}, abstract = {Sensor networks collect vast amounts of real-time information about the environment, business processes, and systems. Archived sensor data is valuable for long-term analysis and decision making, which requires it be suitably archived, indexed, and validated. In this paper, we describe a general approach to managing and improving data quality by the generation and validation of metadata and the logging of workflow events. The approach has been implemented within a system archiving terabytes of U.S. weather radar data. The data quality system has resulted in the detection of data errors while simplifying the administration of the complex archive system.}, location = {Fortaleza, Ceara, Brazil}, series = {SAC '08}, pages = {982\u2013986}, numpages = {5}, keywords = {hydrology, scientific data, data quality, archive, real-time warehouse, sensor network}}
@inproceedings{10.1145/3012258.3012268,title = {Novel Method for Organizational Evaluation and Practice Based on Big Data Analysis}, author = {Qian Xuesheng , Xu Yifeng , Zhang Jing , Zhao Wei },year = {2016}, isbn = {9781450347617}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3012258.3012268}, doi = {10.1145/3012258.3012268}, abstract = {Organizational evaluation plays an essential role in the decision-making and administrative management. Although there are numbers of conventional evaluation methods, the assessing result is subjective and limited by the application fields of data utilization, let along other drawbacks. With the perspective of big data, this thesis employs lower granularity and promotes the big-data based novel organizational evaluation pattern, in order to overcome the shortage of the conventional evaluation system. Then the thesis applies the big data method to elucidate an arduous assessing analysis case - the typical school evaluation problem with the process of standardizing and normalizing the 200,000 students' activity information in hundreds of k-12 schools of a district in Shanghai. Accordingly, the thesis accomplishes the assessment diagnosis on the school management and shares the effective supplements and risk warnings compared with the conventional ones.}, location = {Istanbul, Turkey}, series = {ICIME 2016}, pages = {30\u201335}, numpages = {6}, keywords = {Organizational Evaluation, Behavior Record, Big-Data Assessment, Chinese Short Texts De-duplication, Data Acquisition}}
@inproceedings{10.1145/3341069.3341086,title = {The Construction Study of College Informationization Teaching Service System under the Background of Big Data}, author = {Pengxi Li },year = {2019}, isbn = {9781450371858}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341069.3341086}, doi = {10.1145/3341069.3341086}, abstract = {Under the background of the rapid development of big data technology, the construction of college informationization teaching service system is the basis of the informationization teaching in colleges and universities. That is very important for the success of the informatization in Universities what is service realization model, business logic, architecture and platform conform to the whole development strategy of universities. Information management organization supports the planning, implementation, operation, maintenance and management of business information system. This paper analyzes the reform mode of college education information service system supported by big data technology. Based on the analysis of the reform mode of college informationization teaching service system supported by big data technology, this paper puts forward the design idea of post system based on big data. At the same time, with the case of \"big data assisted employment\", the post design and adjustment were carried out. The results show that big data assisted employment has greatly improved the efficiency and quality of the school's employment department, providing students with better employment security. Finally, the problems that need to be solved in the informatization teaching service are sorted out.}, location = {Guangzhou, China}, series = {HPCCT '19}, pages = {185\u2013189}, numpages = {5}, keywords = {Big data, Teaching informatization, Service system}}
@inproceedings{10.1145/3488466.3488493,title = {Discussion on Customizable Education of Colleges Based on Educational Big Data: Customizable Education of Colleges}, author = {Cai Huiying },year = {2021}, isbn = {9781450384995}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3488466.3488493}, doi = {10.1145/3488466.3488493}, abstract = {With the improvement of network, online teaching has gradually become an important auxiliary teaching mean. In the whole process of teaching and learning, a large amount of educational data is produced. Educational data contain a lot of information to be mined, which is helpful to improve the quality of learning and teaching. This paper will explore how to integrate these educational data that comes from different educational platform to guide customizable education which refers to determine the learning or teaching content independently. To realize the customizable education, the evaluation indicators and detail scheme to make use of the educational big data are proposed. The scheme consists of the acquisition, analysis and visualization of the data for different evaluation indicators. The purpose of this paper is to make these data guide undergraduates to carry out targeted autonomous learning according to their states to promote the learning progress. It can also guide teachers to carry out targeted teaching activities to improve teaching quality. And it is also helpful for teaching managers to have an insight into the learning state of students and the teaching state of the teachers, so as to put forward more reasonable teaching plans and countermeasures. This paper provided a whole framework for the application of the educational big data which can be extended by different educational institutions.}, location = {Busan, Republic of Korea}, series = {ICDTE 2021}, pages = {61\u201366}, numpages = {6}, keywords = {Customizable, Educational big data, Mining, Visualization}}
@inproceedings{10.1145/2856059,title = {Discovering User Behavioral Features to Enhance Information Search on Big Data}, author = {Cassavia Nunziato , Masciari Elio , Pulice Chiara , Sacc\u00e0 Domenico },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2856059}, doi = {10.1145/2856059}, abstract = {Due to the emerging Big Data paradigm, driven by the increasing availability of intelligent services easily accessible by a large number of users (e.g., social networks), traditional data management techniques are inadequate in many real-life scenarios. In particular, the availability of huge amounts of data pertaining to user social interactions, user preferences, and opinions calls for advanced analysis strategies to understand potentially interesting social dynamics. Furthermore, heterogeneity and high speed of user-generated data require suitable data storage and management tools to be designed from scratch. This article presents a framework tailored for analyzing user interactions with intelligent systems while seeking some domain-specific information (e.g., choosing a good restaurant in a visited area). The framework enhances a user's quest for information by exploiting previous knowledge about their social environment, the extent of influence the users are potentially subject to, and the influence they may exert on other users. User influence spread across the network is dynamically computed as well to improve user search strategy by providing specific suggestions, represented as tailored faceted features. Such features are the result of data exchange activity (called data posting) that enriches information sources with additional background information and knowledge derived from experiences and behavioral properties of domain experts and users. The approach is tested in an important application scenario such as tourist recommendation, but it can be profitably exploited in several other contexts, for example, viral marketing and food education.}, pages = {1\u201333}, numpages = {33}, keywords = {personal big data, intelligent recommendation, information extraction, user behavior, NoSQL databases}}
@inproceedings{10.1109/TCBB.2014.2351800,title = {Heterogeneous cloud framework for big data genome sequencing}, author = {Wang Chao , Li Xi , Chen Peng , Wang Aili , Zhou Xuehai , Yu Hong },year = {2015}, publisher = {IEEE Computer Society Press}, address = {Washington, DC, USA}, url = {https://doi.org/10.1109/TCBB.2014.2351800}, doi = {10.1109/TCBB.2014.2351800}, abstract = {The next generation genome sequencing problem with short (long) reads is an emerging field in numerous scientific and big data research domains. However, data sizes and ease of access for scientific researchers are growing and most current methodologies rely on one acceleration approach and so cannot meet the requirements imposed by explosive data scales and complexities. In this paper, we propose a novel FPGA-based acceleration solution with MapReduce framework on multiple hardware accelerators. The combination of hardware acceleration and MapReduce execution flow could greatly accelerate the task of aligning short length reads to a known reference genome. To evaluate the performance and other metrics, we conducted a theoretical speedup analysis on a MapReduce programming platform, which demonstrates that our proposed architecture have efficient potential to improve the speedup for large scale genome sequencing applications. Also, as a practical study, we have built a hardware prototype on the real Xilinx FPGA chip. Significant metrics on speedup, sensitivity, mapping quality, error rate, and hardware cost are evaluated, respectively. Experimental results demonstrate that the proposed platform could efficiently accelerate the next generation sequencing problem with satisfactory accuracy and acceptable hardware cost.}, pages = {166\u2013178}, numpages = {13}, keywords = {short reads, genome sequencing, FPGA, mapping, reconfigurable hardware}}
@inproceedings{10.1145/3356998.3365776,title = {Risk prediction and assessment of foodborne disease based on big data}, author = {Zhang Mingke , Guo Danhuai , Hu Jinyong , Jin Wei },year = {2019}, isbn = {9781450369657}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3356998.3365776}, doi = {10.1145/3356998.3365776}, abstract = {In recent years, the outbreak of foodborne diseases has been on an upward trend clearly. It is of great significance for us to predict the outbreak of foodborne diseases accurately and conduct quantitative risk assessment timely. Traditional prediction methods based on a single data source have drawbacks such as complex prediction processes and inaccurate prediction results. In this article, we figure out the scientific issues of how to improve the temporal and spatial accuracy of foodborne disease outbreak risk prediction in Beijing. Firstly, we analyze the different foodborne disease risk factors caused by the spread of water pollution in Beijing, and study the methods of collecting and preprocessing multi-source data. Then, through the comparison of different regression models and parameters tuning, we propose the use of Gradient Boosting Regression (GBR) model as a multi-source data fusion model to predict the outbreak of foodborne disease. Finally, we use the risk map to detect and predict foodborne disease outbreak in different business districts of Beijing based on visualization techniques, aiming to provide prevention and control assessment for decision-makers quickly and precisely.}, location = {Chicago, Illinois}, series = {EM-GIS '19}, pages = {1\u20136}, numpages = {6}, keywords = {machine learning, big data, risk assessment, foodborne disease}}
@inproceedings{10.5555/2735522.2735575,title = {Smart big data analytics as a service framework: a proposal}, author = {Khalifa Shadi , Martin Patrick },year = {2014}, publisher = {IBM Corp.}, address = {USA}, abstract = {We propose the Smart Big Data Analytics as a Service framework. A framework to empower in-house business users with intelligent assistance throughout the analytics process. It provides distributed in-memory data processing and an easy-to-learn-and-use analytics query language for data exploration, preprocessing and analytical workflow orchestration. The framework is designed as a service to take advantage of the Cloud's features.}, location = {Markham, Ontario, Canada}, series = {CASCON '14}, pages = {327\u2013330}, numpages = {4}}
@inproceedings{10.1145/2935882,title = {Big data analytics and revision of the common rule}, author = {Metcalf Jacob },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2935882}, doi = {10.1145/2935882}, abstract = {Reconsidering traditional research ethics given the emergence of big data analytics.}, pages = {31\u201333}, numpages = {3}}
@inproceedings{10.1145/2783258.2783372,title = {Efficient Online Evaluation of Big Data Stream Classifiers}, author = {Bifet Albert , de Francisci Morales Gianmarco , Read Jesse , Holmes Geoff , Pfahringer Bernhard },year = {2015}, isbn = {9781450336642}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2783258.2783372}, doi = {10.1145/2783258.2783372}, abstract = {The evaluation of classifiers in data streams is fundamental so that poorly-performing models can be identified, and either improved or replaced by better-performing models. This is an increasingly relevant and important task as stream data is generated from more sources, in real-time, in large quantities, and is now considered the largest source of big data. Both researchers and practitioners need to be able to effectively evaluate the performance of the methods they employ. However, there are major challenges for evaluation in a stream. Instances arriving in a data stream are usually time-dependent, and the underlying concept that they represent may evolve over time. Furthermore, the massive quantity of data also tends to exacerbate issues such as class imbalance. Current frameworks for evaluating streaming and online algorithms are able to give predictions in real-time, but as they use a prequential setting, they build only one model, and are thus not able to compute the statistical significance of results in real-time. In this paper we propose a new evaluation methodology for big data streams. This methodology addresses unbalanced data streams, data where change occurs on different time scales, and the question of how to split the data between training and testing, over multiple models.}, location = {Sydney, NSW, Australia}, series = {KDD '15}, pages = {59\u201368}, numpages = {10}, keywords = {data streams, evaluation, classification, online learning}}
@inproceedings{10.1145/3075564.3078884,title = {Big Data Analytics on Large-Scale Scientific Datasets in the INDIGO-DataCloud Project}, author = {Fiore Sandro , Palazzo Cosimo , D'Anca Alessandro , Elia Donatello , Londero Elisa , Knapic Cristina , Monna Stephen , Marcucci Nicola M. , Aguilar Fernando , P\u0142\u00f3ciennik Marcin , De Lucas Jes\u00fas E. Marco , Aloisio Giovanni },year = {2017}, isbn = {9781450344876}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3075564.3078884}, doi = {10.1145/3075564.3078884}, abstract = {In the context of the EU H2020 INDIGO-DataCloud project several use case on large scale scientific data analysis regarding different research communities have been implemented. All of them require the availability of large amount of data related to either output of simulations or observed data from sensors and need scientific (big) data solutions to run data analysis experiments. More specifically, the paper presents the case studies related to the following research communities: (i) the European Multidisciplinary Seafloor and water column Observatory (INGV-EMSO), (ii) the Large Binocular Telescope, (iii) LifeWatch, and (iv) the European Network for Earth System Modelling (ENES).}, location = {Siena, Italy}, series = {CF'17}, pages = {343\u2013348}, numpages = {6}, keywords = {Workflow, big data, ensemble analysis, scientific use case}}
@inproceedings{10.1145/2232817.2232911,title = {Responsibility for research data quality in open access: a slovenian case}, author = {Stebe Janez },year = {2012}, isbn = {9781450311540}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2232817.2232911}, doi = {10.1145/2232817.2232911}, abstract = {In the framework of a project aiming to realize a strategy of open research data access in Slovenia in accordance with OECD principles, we conducted a series of interviews with different target audiences in order to assess the initial conditions in the area of data handling. The data creators and data services expressed a high level of awareness about data quality issues, especially in relation to good publication potential. Barriers to ensuring the greater accessibility of data in the future include the little recognition and reputation for doing the related extra work involved in preparing data and documentation, the need for financial rewards for such additional work, and the undeveloped culture of data exchange in general. The motivation to provide open access to such data will involve a combination of requirements prescribed for data delivery, and the provision of support services and financial rewards, in particular changing the views held by the professional scientific community about the benefits of open data for research activities.}, location = {Washington, DC, USA}, series = {JCDL '12}, pages = {401\u2013402}, numpages = {2}, keywords = {culture, stakeholders attitudes, open data, data quality}}
@inproceedings{10.1145/3483816.3483836,title = {Fusion from Big Data to Smart Data to enhance quality of Information Systems}, author = {Febiri Frank , Yihum Amare Meseret , Hub Miloslav },year = {2021}, isbn = {9781450390545}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3483816.3483836}, doi = {10.1145/3483816.3483836}, abstract = {The term \u201csmartness\u201d in the data framework indicates relevancy based on the intended purpose of data. The Internet of Things (IoT) and advancements in technology have resulted in an ever-increasing pool of data available to all institutions to derive meaning and make sound decisions from them. The research presented in this paper explored the role smart data play in information systems quality through a qualitative study of how using the large pool of data (big data) and fusing it to smart data organizations can make sound and intelligent decisions using the available techniques. We use an existing architecture for a public institution to analyze how data ingestion can be achieved with minimum challenges. The findings suggest that even though there is a large pool of data for most organizations, it is becoming more challenging to use this data to make organizational sense due to the challenges posed by such data. The realization of smart data and its benefits in information systems helps improve the quality of information systems, reducing cost and promoting the smartness agenda of today's organization.}, location = {Jeju, Republic of Korea}, series = {ICMECG 2021}, pages = {112\u2013117}, numpages = {6}, keywords = {Big Data, Smart data, Quality measures, Information systems}}
@inproceedings{10.1145/2987386.2987429,title = {Parallel Job Processing Technique for Real-time Big-Data Processing Framework}, author = {Son Jae Gi , Kang Ji-Woo , An Jae-Hoon , Ahn Hyung-Joo , Chun Hyo-Jung , Kim Jung-Guk },year = {2016}, isbn = {9781450344555}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2987386.2987429}, doi = {10.1145/2987386.2987429}, abstract = {Since the introduction of big data, numerous researches aiming to improve the accuracy and speed of data processing has been conducted. Many platforms that can process real-time data were developed for this purpose. Most standard data processing platforms used Spark Streaming as data analysis layer. However, its limitation in performance calls for a better alternative. This paper introduces a new data processing framework, Squall. Squall utilizes parallel processing and allows real-time data processing using streaming modules. Go was used for development. Through various experiments, the performance of our newly developed framework on processing real-time data was compared to the performance of the previously existing framework completing the same task. Results show quantitative evidence that Squall excel the platforms that use Spark Streaming. Our future work includes making modifications that will improve Squall's performance.}, location = {Odense, Denmark}, series = {RACS '16}, pages = {226\u2013229}, numpages = {4}, keywords = {Apache Spark, Realtime Packet Analysis, Parallel Job Processing, Squall, Real-time Big-Data Processing Framework}}
@inproceedings{10.1145/3321408.3322865,title = {Research on the cultivation of new engineering talents based on educational big data}, author = {Zhao Junmin , Li Dengao , Wang Xiaoyu , Bai Xiaohong , Zhuang Shasha },year = {2019}, isbn = {9781450371582}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3321408.3322865}, doi = {10.1145/3321408.3322865}, abstract = {With the development and application of education and big data, big data continues to focus on classroom teaching and learning. In view of the normal application of the teaching platform and the demand of social talents, it is of practical significance to make an empirical study on the educational big data analysis based on the educational platform. The interaction of education platform can be described by \"STM triangle model\", including teacher-media interaction, student-media interaction, teacher-student interaction, student-teacher interaction, student-student interaction and data relationship. The data mining process of educational platform is divided into target understanding, data cleaning, data analysis, data presentation and other steps. Normal response to Education platform Based on the real data, an index analysis model of teacher-student interaction is constructed, and an empirical analysis is carried out to provide a practical example for the analysis and application of education big data.}, location = {Chengdu, China}, series = {ACM TURC '19}, pages = {1\u20132}, numpages = {2}, keywords = {education big data, STM mode, educational platform, data mining}}
@inproceedings{10.14778/1920841.1921063,title = {Big data and cloud computing: new wine or just new bottles?}, author = {Agrawal Divyakant , Das Sudipto , El Abbadi Amr },year = {2010}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/1920841.1921063}, doi = {10.14778/1920841.1921063}, abstract = {Cloud computing is an extremely successful paradigm of service oriented computing and has revolutionized the way computing infrastructure is abstracted and used. Three most popular cloud paradigms include: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). The concept however can also be extended to Database as a Service and many more. Elasticity, pay-per-use, low upfront investment, low time to market, and transfer of risks are some of the major enabling features that make cloud computing a ubiquitous paradigm for deploying novel applications which were not economically feasible in a traditional enterprise infrastructure settings. This has seen a proliferation in the number of applications which leverage various cloud platforms, resulting in a tremendous increase in the scale of the data generated as well as consumed by such applications. Scalable database management systems (DBMS) -- both for update intensive application workloads, as well as decision support systems for descriptive and deep analytics -- are thus a critical part of cloud infrastructures.}, pages = {1647\u20131648}, numpages = {2}}
@inproceedings{10.1145/3529570.3529580,title = {Inference and Prediction in Big Data Using Sparse Gaussian Process Method}, author = {Yobsan Bayisa Leta , Wang Weidong , Wang Qing-xian , Meseret Debele Gurmu , Bona Debela Lamessa },year = {2022}, isbn = {9781450395809}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3529570.3529580}, doi = {10.1145/3529570.3529580}, abstract = {Gaussian process is one of computationally expensive algorithm for large datasets and lack of the flexibility to model different datasets is a common problem for modeling it. We introduce sparse Gaussian regression with the combination of designed kernels to solve the computational complexity of a traditional Gaussian process by taking pseudo input from large datasets and developing a model with better accuracy which enables Gaussian process application. We design a better combination of the kernel that can catch up with most of our data points. We demonstrate the approach on a large weather dataset and sales record dataset. Both are open source big datasets available online. Numerous experiments and comparisons with traditional Gaussian process methods using both large datasets demonstrate the efficiency and accuracy of sparse Gaussian processes.}, location = {Chengdu, China}, series = {ICDSP '22}, pages = {54\u201362}, numpages = {9}, keywords = {Inference, Pseudo-input, Gaussian Process, Kernel, Big data}}
@inproceedings{10.1145/3447548.3469468,title = {2nd International Workshop on Data Quality Assessment for Machine Learning}, author = {Patel Hima , Ishikawa Fuyuki , Berti-Equille Laure , Gupta Nitin , Mehta Sameep , Masuda Satoshi , Mujumdar Shashank , Afzal Shazia , Bedathur Srikanta , Nishi Yasuharu },year = {2021}, isbn = {9781450383325}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3447548.3469468}, doi = {10.1145/3447548.3469468}, abstract = {The 2nd International Workshop on Data Quality Assessment for Machine Learning (DQAML'21) is organized in conjunction with the Special Interest Group on Knowledge Discovery and Data Mining (SIGKDD). This workshop aims to serve as a forum for the presentation of research related to data quality assessment and remediation in AI/ML pipeline. Data quality is a critical issue in the data preparation phase and involves numerous challenging problems related to detection, remediation, visualization and evaluation of data issues. The workshop aims to provide a platform to researchers and practitioners to discuss such challenges across different modalities of data like structured, time series, text and graphical. The aim is to attract perspectives from both industrial and academic circles.}, location = {Virtual Event, Singapore}, series = {KDD '21}, pages = {4147\u20134148}, numpages = {2}, keywords = {data quality, machine learning, data assessment}}
@inproceedings{10.5555/3233397.3233443,title = {Enabling big data analytics in the hybrid cloud using iterative mapreduce}, author = {Clemente-Castell\u00f3 Francisco J. , Nicolae Bogdan , Katrinis Kostas , Rafique M. Mustafa , Mayo Rafael , Fern\u00e1ndez Juan Carlos , Loreti Daniela },year = {2015}, isbn = {9780769556970}, publisher = {IEEE Press}, abstract = {The cloud computing model has seen tremendous commercial success through its materialization via two prominent models to date, namely public and private cloud. Recently, a third model combining the former two service models as on-/off-premise resources has been receiving significant market traction: hybrid cloud. While state of art techniques that address workload performance prediction and efficient workload execution over hybrid cloud setups exist, how to address data-intensive workloads - including Big Data Analytics - in similar environments is nascent. This paper addresses this gap by taking on the challenge of bursting over hybrid clouds for the benefit of accelerating iterative MapReduce applications. We first specify the challenges associated with data locality and data movement in such setups. Subsequently, we propose a novel technique to address the locality issue, without requiring changes to the MapReduce framework or the underlying storage layer. In addition, we contribute with a performance prediction methodology that combines modeling with micro-benchmarks to estimate completion time for iterative MapReduce applications, which enables users to estimate cost-to-solution before committing extra resources from public clouds. We show through experimentation in a dual-Openstack hybrid cloud setup that our solutions manage to bring substantial improvement at predictable cost-control for two real-life iterative MapReduce applications: large-scale machine learning and text analysis.}, location = {Limassol, Cyprus}, series = {UCC '15}, pages = {290\u2013299}, numpages = {10}, keywords = {mapreduce, big data analytics, data locality, performance prediction, iterative applications, hybrid cloud}}
@inproceedings{10.1145/3281375.3281386,title = {A semantic-based model to represent multimedia big data}, author = {Rinaldi Antonio M. , Russo Cristiano },year = {2018}, isbn = {9781450356220}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3281375.3281386}, doi = {10.1145/3281375.3281386}, abstract = {The use of formal representation is a key task in the era of big data. In the context of multimedia big data this issue is stressed due to the intrinsic complexity nature of this kind of data. Moreover, the relations among objects should be clearly expressed and formalized to give the right meaning of data correlation. For this reason the design of formal models to represent and manage information is a necessary task to implement intelligent information systems. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of big data applications. Using these models the visualization of data and information become an intrinsic and strategic task for the analysis and exploration of multimedia BigData. In this paper we propose the use of a semantic approach to formalize the model structure of multimedia BigData. In addition, the recognition of multimodal features to represent concepts and linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. The proposed model has been implemented in a NoSQL graph database populated from different knowledge sources and a visualization of this very large knowledge base has been presented and discussed as a case study.}, location = {Tokyo, Japan}, series = {MEDES '18}, pages = {31\u201338}, numpages = {8}, keywords = {multimedia ontologies, semantics, semantic bigdata}}
@inproceedings{10.1145/2685553.2685558,title = {Ethics for Studying Online Sociotechnical Systems in a Big Data World}, author = {Fiesler Casey , Young Alyson , Peyton Tamara , Bruckman Amy S. , Gray Mary , Hancock Jeff , Lutters Wayne },year = {2015}, isbn = {9781450329460}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2685553.2685558}, doi = {10.1145/2685553.2685558}, abstract = {The evolution of social technology and research methods present ongoing challenges to studying people online. Recent high-profile cases have prompted discussion among both the research community and the general public about the ethical implications of researching humans, their information, and their activities in large-scale digital contexts. Examples of scientific and market research involving Facebook users and OKCupid clients exemplify the ethical complexities of both studying and manipulating online user behavior. When does data science become human subjects research, and what are our obligations to these subjects as researchers' Drawing from previous work around the ethics of digital research, one goal of this workshop is to work towards a set of guiding principles for CSCW scholars doing research online.}, location = {Vancouver, BC, Canada}, series = {CSCW'15 Companion}, pages = {289\u2013292}, numpages = {4}, keywords = {research ethics, sociotechnical systems, online communities, big data}}
@inproceedings{10.1145/2608020.2612731,title = {Big data challenges in simulation-based science}, author = {Parashar Manish },year = {2014}, isbn = {9781450329132}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2608020.2612731}, doi = {10.1145/2608020.2612731}, abstract = {Data-related challenges are quickly dominating computational and data-enabled sciences, and are limiting the potential impact of scientific applications enabled by current and emerging high-performance distributed computing environments. These data-intensive application workflows involve dynamic coordination, interactions and data coupling between multiple application process that run at scale on different resources, and with services for monitoring, analysis and visualization and archiving. In this talk I will explore data grand challenges in simulation-based science and investigate how solutions based on data sharing abstractions, managed data pipelines, in-memory data-staging, in-situ placement and execution, and in-transit data processing can be used to address these data challenges at extreme scales.}, location = {Vancouver, BC, Canada}, series = {DIDC '14}, pages = {1\u20132}, numpages = {2}, keywords = {in-situ data processing, data-intensive computing, extreme-scale computing, data staging, dataspaces}}
@inproceedings{10.14778/3229863.3229867,title = {Automating large-scale data quality verification}, author = {Schelter Sebastian , Lange Dustin , Schmidt Philipp , Celikel Meltem , Biessmann Felix , Grafberger Andreas },year = {2018}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3229863.3229867}, doi = {10.14778/3229863.3229867}, abstract = {Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.}, pages = {1781\u20131794}, numpages = {14}}
@inproceedings{10.1145/3393527.3393532,title = {Research on Copyright Protection Method of Big Data Based on Nash Equilibrium and Constraint Optimization}, author = {Shi Bin , YabinXu },year = {2020}, isbn = {9781450375344}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3393527.3393532}, doi = {10.1145/3393527.3393532}, abstract = {Data watermarking technology is an effective means to protect the copyright of big data. In order to embed robust and highly available data watermarks, firstly, based on the game theory, a Nash equilibrium model between watermark robustness and data quality is established to solve the optimal number of data partitioning. Then, the mapping relationship between data partitioning and watermark bit is established by using secure hash algorithm. Finally, under the constraint of data usability, the improved particle swarm optimization algorithm is used to calculate the optimal solution of data change for each data partitioning, and then the data is changed accordingly to complete the embedding of watermark bit. In order to verify the copyright ownership of big data, this paper also gives the corresponding watermark extraction method. Watermark extraction is the inverse process of watermark embedding. First, traverse all partitions and extract the possible embedded bit values in each data partitioning. Then, the actual embedded watermark bit is finally determined by majority voting strategies. The experimental results show that our proposed method can not only detect watermarks under different attack conditions, ensure the robustness of big data watermarks, but also achieve better data quality, and the comprehensive effect of data watermarks is better than the existing methods.}, location = {Hefei, China}, series = {ACM TURC'20}, pages = {21\u201325}, numpages = {5}, keywords = {Majority voting strategy, Copyright protection, Constrained optimization, Data watermarking, Particle swarm optimization algorithm, Nash equilibrium, Big data}}
@inproceedings{10.1145/2851613.2851881,title = {Virtualization technologies for the big data environment}, author = {Jlassi Aymen , Martineau Patrick },year = {2016}, isbn = {9781450337397}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2851613.2851881}, doi = {10.1145/2851613.2851881}, abstract = {Today, consumers request virtual resources like CPU, RAM, disk (etc.) supplied by the service providers (like Amazon) and they pay on a \"pay-as-you-go\" basis. Generally, the supervisors adopt virtualization technologies, which optimize resources usage and limit the operating cost. The virtualization technologies are classified in two categories. The first one concerns the heavy virtualization. Each virtual machines (VM) emulates hardware and embeds its own operating system (OS) that is completely isolated from the host OS. The second one concerns the light virtualization, which is based on the management of containers. The containers share the host OS kernel [5] while ensuring isolation. In this paper, we benchmark the performance and the energy consumption of an infrastructure that is based on the software Hadoop regarding the two technologies of virtualization. At first, we will identify the points to be improved concerning Hadoop performances and then we will reduce the deployment cost on the cloud. Second, the Hadoop community finds an in-depth study of the resources consumption depending on the environment of deployment. Our experiments are based on the comparison of the Docker technology (light virtualization) and VMware technology\u00ae (heavy virtualization). We come to the point that in most experiments the light technology offers better performances in completion time of workloads and it is more adapted to be used with the Hadoop software.}, location = {Pisa, Italy}, series = {SAC '16}, pages = {542\u2013545}, numpages = {4}, keywords = {Hadoop, resources consumption, benchmarks, virtualization}}
@inproceedings{10.1145/3501409.3501630,title = {Research on the application of cloud computing technology in computer big data analysis}, author = {Chen Genjin },year = {2021}, isbn = {9781450384322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3501409.3501630}, doi = {10.1145/3501409.3501630}, abstract = {With the development of the times, computer technology is making continuous progress. People's every move is generating data. In the face of such a large amount of data information, the traditional data processing technology has been unable to meet people's needs. Big data analysis and cloud computing technology came into being. Cloud computing and big data technology have far exceeded the traditional data processing technology in terms of accuracy and speed. Now cloud computing and big data analysis have become hot words in the society, and relevant experts and scholars are constantly studying these two technologies. Starting from the elaboration of cloud computing technology and big data technology, this paper explores the advantages and problems of cloud computing, deeply analyzes the application of cloud computing technology in computer big data, imagines the future of cloud computer technology, and provides some references for the future development of cloud computing technology.}, location = {Xiamen, China}, series = {EITCE 2021}, pages = {1257\u20131262}, numpages = {6}, keywords = {Computer technology, Research, Big data technology, Cloud computing technology}}
@inproceedings{10.1145/3297720,title = {Augmenting Data Quality through High-Precision Gender Categorization}, author = {M\u00fcller Daniel , Jain Pratiksha , Te Yieh-Funk },year = {2019}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297720}, doi = {10.1145/3297720}, abstract = {Mappings of first name to gender have been widely recognized as a critical tool for the completion, study, and validation of data records in a range of areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 4 million people, which was provided by a car insurance company. Then, we study how naming conventions have changed over time and how they differ by nationality. Next, we build a probabilistic first-name-to-gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in two-label and three-label settings and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies\u2019 outcomes and find that our mapping produces high-precision results. We validate that the additional information of nationality and year of birth improve the precision scores of name-to-gender mappings. Therefore, the proposed approach constitutes an efficient process for improving the data quality of organizations\u2019 records, if the gender attribute is missing or unreliable.}, pages = {1\u201318}, numpages = {18}, keywords = {patenting, record completion, Data quality improvement, gender name mapping}}
@inproceedings{10.1145/2487575.2487677,title = {Big data analytics with small footprint: squaring the cloud}, author = {Canny John , Zhao Huasha },year = {2013}, isbn = {9781450321747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2487575.2487677}, doi = {10.1145/2487575.2487677}, abstract = {This paper describes the BID Data Suite, a collection of hardware, software and design patterns that enable fast, large-scale data mining at very low cost. By co-designing all of these elements we achieve single-machine performance levels that equal or exceed reported cluster implementations for common benchmark problems. A key design criterion is rapid exploration of models, hence the system is interactive and primarily single-user. The elements of the suite are: (i) the data engine, a hardware design pattern that balances storage, CPU and GPU acceleration for typical data mining workloads, (ii) BIDMat, an interactive matrix library that integrates CPU and GPU acceleration and novel computational kernels (iii), BIDMach, a machine learning system that includes very efficient model optimizers, (iv) Butterfly mixing, a communication strategy that hides the latency of frequent model updates needed by fast optimizers and (v) Design patterns to improve performance of iterative update algorithms. We present several benchmark problems to show how the above elements combine to yield multiple orders-of-magnitude improvements for each problem.}, location = {Chicago, Illinois, USA}, series = {KDD '13}, pages = {95\u2013103}, numpages = {9}, keywords = {machine learning, data mining, toolkit, cluster, gpu}}
@inproceedings{10.1145/2588555.2618215,title = {Are we experiencing a big data bubble?}, author = {\u00d6zcan Fatma , Tatbul Nesime , Abadi Daniel J. , Kornacker Marcel , Mohan C. , Ramasamy Karthik , Wiener Janet },year = {2014}, isbn = {9781450323765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2588555.2618215}, doi = {10.1145/2588555.2618215}, location = {Snowbird, Utah, USA}, series = {SIGMOD '14}, pages = {1407\u20131408}, numpages = {2}, keywords = {SQL-on-HADOOP, NewSQL, NoSQL}}
@inproceedings{10.1145/3377817.3377836,title = {A Systematic Review on Big Data Analytics Frameworks for Higher Education - Tools and Algorithms}, author = {Otoo-Arthur David , Van Zyl Terence },year = {2019}, isbn = {9781450366496}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377817.3377836}, doi = {10.1145/3377817.3377836}, abstract = {The development of Big Data applications in education has drawn much attention in the last few years due to the enormous benefits it brings to improving teaching and learning. The integration of these Big Data applications in education generates massive data that put new demands to available processing technologies of data and extraction of useful information. Primarily, several higher educational institutions depend on the knowledge mined from these vast volumes of data to optimise the teaching and learning environment. However, Big Data in the higher education context has relied on traditional data techniques and platforms that are less efficient. This paper, therefore, conducts a Systematic Literature Review (SLR) that examines Big Data framework technologies in higher education outlining gaps that need a solution in Big Educational Data Analytics. We achieved this by summarising the current knowledge on the topic and recommend areas where educational institutions could focus on exploring the potential of Big Data Analytics. To this end, we reviewed 55 related articles out of 1543 selected from Six (6) accessible Computer Science databases between the period of 2007 and 2018, focusing on the development of the Big Data framework and its applicability in education for academic purposes. Our results show that very few researchers have tried to address the integrative use of Big Data framework and learning analytics in higher education. The review further suggests that there is an emerging best practice in applying Big Data Analytics to improve teaching and learning. However, this information does not appear to have been thoroughly examined in higher education. Hence, there is the need for a complete investigation to come up with comprehensive Big Data frameworks that build effective learning systems for instructors, learners, course designers and educational administrators.}, location = {Kuala Lumpur, Malaysia}, series = {EBIMCS '19}, pages = {1\u20139}, numpages = {9}, keywords = {Big Educational Data, MS, Learning Analytics, Data Mining, Big Data, Higher Education}}
@inproceedings{10.1145/3378936.3378950,title = {Application of Cloud Computing for Big Data in the X-Ray Crystallography Community}, author = {Tosson A. , Shokr M. , Pietsch U. },year = {2020}, isbn = {9781450376907}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3378936.3378950}, doi = {10.1145/3378936.3378950}, abstract = {The X-ray crystallography community has recently been affected by a significant increase in data volume caused by the use of advanced detector technologies and the new generation of high brilliance light sources. The fact that forced the decision makers to implement Big Data analytics, aiming to achieve a suitable environment for scientists at experimental and post-experimental phases. This paper demonstrates an extension of our approach towards a compact platform which provides the scientists with the digital ecosystem for the systematic harvest of data. It introduces an innovative solution to use warehousing and cloud computing to manage datasets collected by 2D energy-dispersive detectors, for an example. Moreover, it suggests that, deploying a Software as a Service (SaaS) cloud model, a public cloud data center, and cloud-based in-memory warehousing architecture, it is possible to dramatically reduce both hardware and processing costs.}, location = {Sydney, NSW, Australia}, series = {ICSIM '20}, pages = {1\u20134}, numpages = {4}, keywords = {Cloud computing, In-Memory warehousing, Big Data, Crystallography}}
@inproceedings{10.1145/3297280.3297474,title = {Evaluation of ACE properties of traditional SQL and NoSQL big data systems}, author = {Gonzalez-Aparicio Maria Teresa , Younas Muhammad , Tuya Javier , Casado Rub\u00e9n },year = {2019}, isbn = {9781450359337}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297280.3297474}, doi = {10.1145/3297280.3297474}, abstract = {Traditional SQL and NoSQL big data systems are the backbone for managing data in cloud, fog and edge computing. This paper develops a new system and adopts the TPC-DS industry standard benchmark in order to evaluate three key properties, availability, consistency and efficiency (ACE) of SQL and NoSQL systems. The contributions of this work are manifold. It evaluates and analyses the tradeoff between the ACE properties. It provides insight into the NoSQL systems and how they can be improved to be sustainable for a more wide range of applications. The evaluation shows that SQL provides stronger consistency, but at the expense of low efficiency and availability. NoSQL provides better efficiency and availability but lacks support for stronger consistency. In order for NoSQL systems to be more sustainable they need to implement transactional schemes that enforce stronger consistency as well as better efficiency and availability.}, location = {Limassol, Cyprus}, series = {SAC '19}, pages = {1988\u20131995}, numpages = {8}, keywords = {NoSQL, Riak, TPC-DS, big data, data consistency, SQL}}
@inproceedings{10.14778/2752939.2752945,title = {A performance study of big data on small nodes}, author = {Loghin Dumitrel , Tudor Bogdan Marius , Zhang Hao , Ooi Beng Chin , Teo Yong Meng },year = {2015}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2752939.2752945}, doi = {10.14778/2752939.2752945}, abstract = {The continuous increase in volume, variety and velocity of Big Data exposes datacenter resource scaling to an energy utilization problem. Traditionally, datacenters employ x86-64 (big) server nodes with power usage of tens to hundreds of Watts. But lately, low-power (small) systems originally developed for mobile devices have seen significant improvements in performance. These improvements could lead to the adoption of such small systems in servers, as announced by major industry players. In this context, we systematically conduct a performance study of Big Data execution on small nodes in comparison with traditional big nodes, and present insights that would be useful for future development. We run Hadoop MapReduce, MySQL and in-memory Shark workloads on clusters of ARM big. LITTLE boards and Intel Xeon server systems. We evaluate execution time, energy usage and total cost of running the workloads on self-hosted ARM and Xeon nodes. Our study shows that there is no one size fits all rule for judging the efficiency of executing Big Data workloads on small and big nodes. But small memory size, low memory and I/O bandwidths, and software immaturity concur in canceling the lower-power advantage of ARM servers. We show that I/O-intensive MapReduce workloads are more energy-efficient to run on Xeon nodes. In contrast, database query processing is always more energy-efficient on ARM servers, at the cost of slightly lower throughput. With minor software modifications, CPU-intensive MapReduce workloads are almost four times cheaper to execute on ARM servers.}, pages = {762\u2013773}, numpages = {12}}
@inproceedings{10.1145/3178461.3178464,title = {A Smart City Environmental Monitoring Network and Analysis Relying on Big Data Techniques}, author = {Tahat Ashraf , Aburub Ruba , Al-Zyoude Aseel , Talhi Chamseddine },year = {2018}, isbn = {9781450354387}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3178461.3178464}, doi = {10.1145/3178461.3178464}, abstract = {A new integrated environmental monitoring system to carry-out real-time measurements on board a moving vehicle is presented. It is composed of an arbitrary number of Electronic Measurements Units (EMU), a smart phone application to relay collected data, and a cloud Central Processing Platform (CPP) to perform analysis utilizing big data techniques and algorithms. Each EMU consists of an electric circuit that incorporates an ultra violet (UV) sensor, an air particles concentration sensor, a temperature sensor and a humidity sensor that all interface to a microcontroller. Bluetooth is employed for communication between the EMU and the smart phone application, while a 3G/4G cellular communications network furnishes the wireless connectivity to the remote CPP. When the collected data reaches the designated cloud server (CPP), it is immediately stored for subsequent analysis. Finally, big data statistical analysis (clustering and classification), mapping and plotting are performed to deduce correlations and to facilitate inferencing. Moreover, the scalability and low-cost of selected components of this realistic system makes it very feasible for large scale deployments in the context of smart cities initiatives, ad-hoc designs, or educational projects.}, location = {Casablanca, Morocco}, series = {ICSIM2018}, pages = {82\u201386}, numpages = {5}, keywords = {Big data, temperature sensor, smart phone, environment, telemetry, air particles, UV index}}
@inproceedings{10.1145/3093338.3093372,title = {Optimizing High Performance Big Data Cancer Workflows}, author = {Jimenez-Ruiz Ivan , Gonzalez-Mendez Ricardo , Ropelewski Alexander },year = {2017}, isbn = {9781450352727}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3093338.3093372}, doi = {10.1145/3093338.3093372}, abstract = {Appropriate optimization of bioinformatics workflows is vital to improve the timely discovery of variants implicated in cancer genomics. Sequenced human brain tumor data was assembled to optimize tool implementations and run various components of RNA sequence (RNA-seq) workflows. The measurable information produced by these tools account for the success rate and overall efficiency of a standardized and simultaneous analysis. We used the National Center for Biotechnology Information) Sequence Read Archive (NCBI-SRA) database to retrieve two transcriptomic datasets containing over 104 million reads as input data. We used these datasets to benchmark various file systems on the Bridges supercomputer to improve overall workflow throughput. Based on program and job timings, we report critical recommendations on selections of appropriate file systems and node types to efficiently execute these workflows.}, location = {New Orleans, LA, USA}, series = {PEARC17}, pages = {1\u20134}, numpages = {4}, keywords = {Memory, Workflows, Bioinformatics, Performance, File Systems, Supercomputing, Transcriptome, ACM proceedings, Genome, Timings}}
@inproceedings{10.1145/2983323.2983841,title = {Scalable Local-Recoding Anonymization using Locality Sensitive Hashing for Big Data Privacy Preservation}, author = {Zhang Xuyun , Leckie Christopher , Dou Wanchun , Chen Jinjun , Kotagiri Ramamohanarao , Salcic Zoran },year = {2016}, isbn = {9781450340731}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2983323.2983841}, doi = {10.1145/2983323.2983841}, abstract = {While cloud computing has become an attractive platform for supporting data intensive applications, a major obstacle to the adoption of cloud computing in sectors such as health and defense is the privacy risk associated with releasing datasets to third-parties in the cloud for analysis. A widely-adopted technique for data privacy preservation is to anonymize data via local recoding. However, most existing local-recoding techniques are either serial or distributed without directly optimizing scalability, thus rendering them unsuitable for big data applications. In this paper, we propose a highly scalable approach to local-recoding anonymization in cloud computing, based on Locality Sensitive Hashing (LSH). Specifically, a novel semantic distance metric is presented for use with LSH to measure the similarity between two data records. Then, LSH with the MinHash function family can be employed to divide datasets into multiple partitions for use with MapReduce to parallelize computation while preserving similarity. By using our efficient LSH-based scheme, we can anonymize each partition through the use of a recursive agglomerative $k$-member clustering algorithm. Extensive experiments on real-life datasets show that our approach significantly improves the scalability and time-efficiency of local-recoding anonymization by orders of magnitude over existing approaches.}, location = {Indianapolis, Indiana, USA}, series = {CIKM '16}, pages = {1793\u20131802}, numpages = {10}, keywords = {mapreduce, big data, LSH, privacy preservation, cloud}}
@inproceedings{10.1145/2568088.2576096,title = {Extreme big data processing in large-scale graph analytics and billion-scale social simulation}, author = {Suzumura Toyotaro },year = {2014}, isbn = {9781450327336}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2568088.2576096}, doi = {10.1145/2568088.2576096}, abstract = {This paper introduces some of the example applications handling extremely big data with supercomputers such as large-scale network analysis, X10-based large-scale graph analytics library, Graph500 benchmark, and billion-scale social simulation.}, location = {Dublin, Ireland}, series = {ICPE '14}, pages = {1\u20132}, numpages = {2}, keywords = {big data, graph, social simulation, supercomputer}}
@inproceedings{10.1145/2771299,title = {Bringing big data to the big tent}, author = {Goth Gregory },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2771299}, doi = {10.1145/2771299}, abstract = {Open source tools assist data science.}, pages = {17\u201319}, numpages = {3}}
@inproceedings{10.1145/3511211,title = {Unified Holistic Memory Management Supporting Multiple Big Data Processing Frameworks over Hybrid Memories}, author = {Chen Lei , Zhao Jiacheng , Wang Chenxi , Cao Ting , Zigman John , Volos Haris , Mutlu Onur , Lv Fang , Feng Xiaobing , Xu Guoqing Harry , Cui Huimin },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3511211}, doi = {10.1145/3511211}, abstract = {To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy inefficient. Emerging non-volatile memory (NVM) technologies offer high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages and executed on top of a managed runtime that already performs various dimensions of memory management. Supporting hybrid physical memories adds a new dimension, creating unique challenges in data replacement. This article proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division information is accurate enough to guide the GC for data layout, which hardly incurs overhead in data monitoring and moving. We implemented Panthera in OpenJDK and Apache Spark. Based on Big Data applications\u2019 memory access pattern, we also implemented a new profiling-guided optimization strategy, which is transparent to applications. With this optimization, our extensive evaluation demonstrates that Panthera reduces energy by 32\u201353% at less than 1% time overhead on average. To show Panthera\u2019s applicability, we extend it to QuickCached, a pure Java implementation of Memcached. Our evaluation results show that Panthera reduces energy by 28.7% at 5.2% time overhead on average.}, pages = {1\u201338}, numpages = {38}, keywords = {memory management, Big Data systems, Hybrid memories, garbage collection}}
@inproceedings{10.1145/3093241.3093265,title = {Social Login and Data Storage in the Big Data File System HDFS}, author = {Madani Youness , Bengourram Jemaa , Erritali Mohammed },year = {2017}, isbn = {9781450352413}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3093241.3093265}, doi = {10.1145/3093241.3093265}, abstract = {Studies have shown that the registration forms on Websites are ineffective because many people give false data, forget their login information to the site or just refuse to register, to overcome these problems a new type of authentication is born is the social authentication or social login which is a type of SSO(Single Sign-On),due to this type of authentication enrollment increases to a platform because the user registered to the platform with a simple click of a button authentication without passing by the step of filling a form, choose a username and a secure password. In this article, we will study the social authentication how it works, and how after the authorization of the user we can Retrieve personal data to complete registration, we can also use its social authorization on our facebook application to register its data on HDFS in a Big data system to analyze them and personalize its member space in the platform, using the Hadoop framework based on the MapReduce programming.}, location = {Lakeland, FL, USA}, series = {ICCDA '17}, pages = {91\u201397}, numpages = {7}, keywords = {SSO(Single Sign-On), Authentication, MapReduce, authorization, Hadoop, social authentication, HDFS, Big Data}}
@inproceedings{10.1145/3098593.3098597,title = {o'zapft is: Tap Your Network Algorithm's Big Data!}, author = {Blenk Andreas , Kalmbach Patrick , Kellerer Wolfgang , Schmid Stefan },year = {2017}, isbn = {9781450350549}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3098593.3098597}, doi = {10.1145/3098593.3098597}, abstract = {At the heart of many computer network planning, deployment, and operational tasks lie hard algorithmic problems. Accordingly, over the last decades, we have witnessed a continuous pursuit for ever more accurate and faster algorithms. We propose an approach to design network algorithms which is radically different from most existing algorithms. Our approach is motivated by the observation that most existing algorithms to solve a given hard computer networking problem overlook a simple yet very powerful optimization opportunity in practice: many network algorithms are executed repeatedly (e.g., for each virtual network request or in reaction to user mobility), and hence with each execution, generate interesting data: (problem,solution)-pairs. We make the case for leveraging the potentially big data of an algorithm's past executions to improve and speed up future, similar solutions, by reducing the algorithm's search space. We study the applicability of machine learning to network algorithm design, identify challenges and discuss limitations. We empirically demonstrate the potential of machine learning network algorithms in two case studies, namely the embedding of virtual networks (a packing optimization problem) and k-center facility location (a covering optimization problem), using a prototype implementation.}, location = {Los Angeles, CA, USA}, series = {Big-DAMA '17}, pages = {19\u201324}, numpages = {6}, keywords = {Computer Networks, Big Data, Algorithms, Machine Learning}}
@inproceedings{10.1145/2939672.2939859,title = {Parallel Lasso Screening for Big Data Optimization}, author = {Li Qingyang , Qiu Shuang , Ji Shuiwang , Thompson Paul M. , Ye Jieping , Wang Jie },year = {2016}, isbn = {9781450342322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2939672.2939859}, doi = {10.1145/2939672.2939859}, abstract = {Lasso regression is a widely used technique in data mining for model selection and feature extraction. In many applications, it remains challenging to apply the regression model to large-scale problems that have massive data samples with high-dimensional features. One popular and promising strategy is to solve the Lasso problem in parallel. Parallel solvers run multiple cores in parallel on a shared memory system to speedup the computation, while the practical usage is limited by the huge dimension in the feature space. Screening is a promising method to solve the problem of high dimensionality by discarding the inactive features and removing them from optimization. However, when integrating screening methods with parallel solvers, most of solvers cannot guarantee the convergence on the reduced feature matrix. In this paper, we propose a novel parallel framework by parallelizing screening methods and integrating it with our proposed parallel solver. We propose two parallel screening algorithms: Parallel Strong Rule (PSR) and Parallel Dual Polytope Projection (PDPP). For the parallel solver, we proposed an Asynchronous Grouped Coordinate Descent method (AGCD) to optimize the regression problem in parallel on the reduced feature matrix. AGCD is based on a grouped selection strategy to select the coordinate that has the maximum descent for the objective function in a group of candidates. Empirical studies on the real-world datasets demonstrate that the proposed parallel framework has a superior performance compared to the state-of-the-art parallel solvers.}, location = {San Francisco, California, USA}, series = {KDD '16}, pages = {1705\u20131714}, numpages = {10}, keywords = {parallel computing, lasso regression, screening rules, coordinate descent, aynchronized coordinate descent}}
@inproceedings{10.1145/2786752,title = {Phonetic analytics technology and big data: real-world cases}, author = {Shim J. P. , Koh J. , Fister S. , Seo H. Y. },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2786752}, doi = {10.1145/2786752}, abstract = {Data from phone interactions can help address customers' complaints, and predict their future purchasing behavior.}, pages = {84\u201390}, numpages = {7}}
@inproceedings{10.1145/3383455.3422529,title = {Quantifying ESG alpha using scholar big data: an automated machine learning approach}, author = {Chen Qian , Liu Xiao-Yang },year = {2020}, isbn = {9781450375849}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3383455.3422529}, doi = {10.1145/3383455.3422529}, abstract = {ESG (Environmental, social and governance) alpha strategy that makes sustainable investment has gained popularity among investors. The ESG fields of study in scholar big data is a valuable alternative data that reflects a company's long-term ESG commitment. However, it is considered a difficulty to quantitatively measure a company's ESG premium and its impact to the company's stock price using scholar big data. In this paper, we utilize ESG scholar data as alternative data to develop an automatic trading strategy and propose a practical machine learning approach to quantify the ESG premium of a company and capture the ESG alpha. First, we construct our ESG investment universe and apply feature engineering on the companies' ESG scholar data from the Microsoft Academic Graph database. Then, we train six complementary machine learning models using a combination of financial indicators and ESG scholar data features and employ an ensemble method to predict stock prices and automatically set up portfolio allocation. Finally, we manage our portfolio, trade and rebalance the portfolio allocation monthly using predicted stock prices. We backtest our ESG alpha strategy and compare its performance with benchmarks. The proposed ESG alpha strategy achieves a cumulative return of 2,154.4% during the backtesting period of ten years, which significantly outperforms the NASDAQ-100 index's 397.4% and S&P 500's 226.9%. The traditional financial indicators results in only 1,443.7%, thus our scholar data-based ESG alpha strategy is better at capturing ESG premium than traditional financial indicators.}, location = {New York, New York}, series = {ICAIF '20}, pages = {1\u20138}, numpages = {8}, keywords = {ESG alpha, AI in finance, alternative data, scholar data, quantitative investment}}