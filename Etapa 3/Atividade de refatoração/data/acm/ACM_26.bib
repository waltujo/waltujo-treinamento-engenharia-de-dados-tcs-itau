@inproceedings{10.1145/3482632.3483163,title = {The Application of Big Data Technology in Police Tactics Teaching}, author = {Zhang Ruolong },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483163}, doi = {10.1145/3482632.3483163}, abstract = {The advent of the era of big data has brought us many shocks. Especially for police teaching, it is both a challenge and an opportunity. In the current police tactics teaching, due to the peculiarities of the troops and the necessity of teaching methods, uniformity is necessary. In the implementation of teaching, some teachers walked into a teaching misunderstanding. At the same time, big data technology has strong application capabilities and promotion power for the ever-increasing scale of police data, which can better improve work efficiency and serve the masses. The purpose of this article is to study the application of big data technology in police tactics teaching. In this paper, by clarifying the police reform brought by big data and responding to the challenges of the big data era, researching data mining technology and algorithms, using outbound data and case data in police data as data sources, the data is preprocessed and multi-dimensional data modeling can be applied to police tactics teaching. This article starts from the actual work of police activities, with both theoretical accumulation and practical exploration. Theory and practice are closely integrated, and from the perspective of practical applications, focusing on the current organic combination of information and communication technology and police work, and finally come up with a solution to the shortcomings of the traditional police work system. Provide ideas for the information integration work carried out by police work. Experimental research shows that the biggest improvement is the risk-avoidance action. Before the use of big data technology, the learning efficiency was only 70.6%, and after the improvement, it was as high as 85.7%, an improvement of 15.1%. Generally speaking, after the improvement, both the basic theory teaching of police tactics and the basic movement teaching have made great progress.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1414\u20131418}, numpages = {5}}
@inproceedings{10.1145/3053600.3053621,title = {Recent Trends in Performance Modeling of Big Data Systems}, author = {Apte Varsha },year = {2017}, isbn = {9781450348997}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3053600.3053621}, doi = {10.1145/3053600.3053621}, abstract = {With the advent of big data through social media and continuous creation of digital footprints through various mobile devices, special-purpose programming models were developed that would make it easy to write programs to process such data. MapReduce and its Hadoop implementation is one of the most popular platforms for writing such programs. The MapReduce framework involves a \"map\" phase where various tasks work in parallel for intermediate processing of data and a \"reduce\" phase where again various tasks work in parallel to extract information from this processed data. Performance modeling of such systems will need different approaches than are used for traditional multi-threaded multi-core systems supporting Web applications, primarily because the dependencies and synchronization required between various tasks is not easily expressible using standard queuing network models. In this talk we will review work done by researchers to address this modeling problem. The work done encompasses first-principles calculations of execution time completion, queuing network models, and finally, simulation. We will review these efforts as well as highlight opportunities for further work in this area.}, location = {L&apos;Aquila, Italy}, series = {ICPE '17 Companion}, pages = {105}, numpages = {1}}
@inproceedings{10.5555/3042094.3042329,title = {Simulation of maintenance processes in the big data era}, author = {Volovoi Vitali },year = {2016}, isbn = {9781509044849}, publisher = {IEEE Press}, abstract = {Maintenance processes of repairable systems have been extensively studied in the past. The resulting simple solutions have proven to be remarkably effective. It requires complex and time-consuming simulations to improve on those simple solutions, and reliable input data is even harder to get. However, new technologies, epitomized by Big Data and the Internet of Things, change the data-availability part of the equation. As a result, there are new exciting possibilities for modeling more subtle effects, and developing processes for easily (and therefore frequently) updated inputs. Modeling decisions can be repeatedly tested on the data, and the models can be quickly adjusted to better reflect reality and even to compensate for missing pieces of the data. In this context, the transparency and simplicity of models becomes a larger virtue. Several examples of the insights based on real-world large-scale applications of predictive analytics using simulation are discussed.}, pages = {1872\u20131883}, numpages = {12}}
@inproceedings{10.1145/3449052,title = {Editorial: Special Issue on Quality Assessment and Management in Big Data\u2014Part I}, author = {Aljawarneh Shadi , Lara Juan A. },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3449052}, doi = {10.1145/3449052}, pages = {1\u20133}, numpages = {3}, keywords = {Quality assessment, quality management, big data}}
@inproceedings{10.1145/2537148.2537159,title = {Parallel routing on multi-core routers for big data transfers}, author = {Soran Ahmet , Akdemir Furkan Mustafa , Yuksel Murat },year = {2013}, isbn = {9781450325752}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2537148.2537159}, doi = {10.1145/2537148.2537159}, abstract = {Over the last several years, the deployment of multi-core routers has grown rapidly. However, big data transfers are not leveraging the powerful multi-core routers to the extent possible, particularly in the key function of routing. Our main goal is to find a way to use these cores more effectively and efficiently in routing the big data transfers. We propose a novel approach to parallelize data transfers by using each core in the routers to calculate a separate shortest path. For each core, we generate a different \"substrate\" topology in order to allow shortest path calculations to find a different end-to-end (e2e) path. By abstracting a different topology for each core, we indirectly steer each core to calculate a different e2e path in parallel to each other. The e2e big data transfers can use these shortest paths obtained from each substrate topology to increase the total throughput. We present an initial evaluation of the concept.}, location = {Santa Barbara, California, USA}, series = {CoNEXT Student Workhop '13}, pages = {35\u201338}, numpages = {4}, keywords = {multi-path routing, multi-core routers, load balancing}}
@inproceedings{10.1145/3449056,title = {Editorial: Special Issue on Quality Assessment and Management in Big Data\u2014Part II}, author = {Aljawarneh Shadi , Lara Juan A. },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3449056}, doi = {10.1145/3449056}, pages = {1\u20133}, numpages = {3}, keywords = {Quality assessment, quality management, big data}}
@inproceedings{10.5555/2693848.2693981,title = {Big data fueled process management of supply risks: sensing, prediction, evaluation and mitigation}, author = {He Miao , Ji Hao , Wang Qinhua , Ren Changrui , Lougee Robin },year = {2014}, publisher = {IEEE Press}, abstract = {Supplier risks jeopardize on-time or complete delivery of supply in a supply chain. Traditionally, a company can merely do an ex-post evaluation of a supplier's performance, and handles emergencies in a reactive rather than a proactive way. We propose an agile process management framework to monitor and manage supply risks. The innovation is two fold - Firstly, a business process is established to make sure that the right data, the right insights, and the right decision-makers are in place at the right time. Secondly, we install a big data analytics component, a simulation component and an optimization component into the business process. The big data analytics component senses and predicts supply disruptions with internally (operational) and external (environmental) data. The simulation component supports risk evaluation to convert predicted risk severity to key performance indices (KPIs) such as cost and stockout percentage. The optimization component assists the risk-hedging decision-making.}, location = {Savannah, Georgia}, series = {WSC '14}, pages = {1005\u20131013}, numpages = {9}}
@inproceedings{10.1145/3209914.3234637,title = {A Study on the Management Model of Smart Tourism Industry under the Era of Big Data}, author = {Hua Zhao },year = {2018}, isbn = {9781450364218}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3209914.3234637}, doi = {10.1145/3209914.3234637}, abstract = {With the rapid development of Internet and the communication technology, the construction of smart tourism is no longer a slogan that can not be realized. The construction of smart tourism in tourist destinations conforms to the strategic goal of tourism industry development in China. Based on the background of big data, this paper elaborated the connotation of big data and smart tourism, and built a large data platform to realize the forecast and feedback of smart tourism through the analysis of tourism development. The platform could be divided into government tourism platform, tourists platform, tourism enterprises platform and community residents platform relying on big data do their own duty. Eventually this paper put forward a construction model and path to realize the smart tourism platform.}, location = {Jeju, Republic of Korea}, series = {ICISS '18}, pages = {102\u2013106}, numpages = {5}, keywords = {smart tourism, management model, Big data}}
@inproceedings{10.1145/2788402.2788406,title = {Demystifying Casualties of Evictions in Big Data Priority Scheduling}, author = {Ros\u00e0 Andrea , Chen Lydia Y. , Birke Robert , Binder Walter },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2788402.2788406}, doi = {10.1145/2788402.2788406}, abstract = {The ever increasing size and complexity of large-scale datacenters enhance the difficulty of developing efficient scheduling policies for big data systems, where priority scheduling is often employed to guarantee the allocation of system resources to high priority tasks, at the cost of task preemption and resulting resource waste. A large number of related studies focuses on understanding workloads and their performance impact on such systems; nevertheless, existing works pay little attention on evicted tasks, their characteristics, and the resulting impairment on the system performance. In this paper, we base our analysis on Google cluster traces, where tasks can experience three diffierent types of unsuccessful events, namely eviction, kill and fail. We particularly focus on eviction events, i.e., preemption of task execution due to higher priority tasks, and rigorously quantify their performance drawbacks, in terms of wasted machine time and resources, with particular focus on priority. Motivated by the high dependency of eviction on underlying scheduling policies, we also study its statistical patterns and its dependency on other types of unsuccessful events. Moreover, by considering co-executed tasks and system load, we deepen the knowledge on priority scheduling, showing how priority and machine utilization affect the eviction process and related tasks.}, pages = {12\u201321}, numpages = {10}}
@inproceedings{10.1145/3297280.3297386,title = {How to implement a big data clustering algorithm: a brief report on lesson learned}, author = {Ianni Michele , Masciari Elio , Mazzeo Giuseppe M. , Zaniolo Carlo },year = {2019}, isbn = {9781450359337}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297280.3297386}, doi = {10.1145/3297280.3297386}, abstract = {The current era of Big Data [7] has forced both researchers and industries to rethink the computational solutions for analyzing massive data. In fact, a great deal of attention has been devoted to the design of new algorithms for analyzing information available from Twitter, Google, Facebook, and Wikipedia, just to cite a few of the main big data producers. Although this massive volume of data can be quite useful for people and companies, it makes analytical and retrieval operations really time consuming due to their high computational cost. A possible solution relies upon the possibility to cluster big data in a compact but still informative version of the entire data set. Obviously, such clustering techniques should produce clusters (or summaries) having high accuracy. Clustering algorithms could be beneficial in several application scenarios such as cybersecurity, user profiling and recommendation systems, to cite a few.}, location = {Limassol, Cyprus}, series = {SAC '19}, pages = {1073\u20131080}, numpages = {8}}
@inproceedings{10.1145/3510858.3511392,title = {On-line Monitoring of Capacitive Equipment Based on Big Data}, author = {Wei Dongliang , Wang Zhi , Zhou Jia , Chen Jiangtian },year = {2021}, isbn = {9781450390422}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510858.3511392}, doi = {10.1145/3510858.3511392}, abstract = {With the development of the country in information technology, and those technology products are also used in life. Such as capacitive devices in power systems. But capacitive equipment will also have some failures, so it is necessary to monitor it online. Now researchers have found a lot of online monitoring methods, but in most of the current monitoring methods, the monitoring process will produce a huge amount of data, which is very large, so that technicians may miss some important data. In order to solve this problem which can not be discovered early because of the excessive amount of data, this paper adopts some methods based on big data to dig through the data the data obtained by the dig algorithm are statistically analyzed in big data technology. Using the data collected in this paper, through the analysis and research of these data, the results show that the application of big data technology to the on-line monitoring of capacitive equipment is very accurate and practical.}, location = {Changsha, China}, series = {ICASIT 2021}, pages = {803\u2013807}, numpages = {5}}
@inproceedings{10.1145/3374749,title = {User and Entity Behavior Analysis under Urban Big Data}, author = {Tian Zhihong , Luo Chaochao , Lu Hui , Su Shen , Sun Yanbin , Zhang Man },year = {2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3374749}, doi = {10.1145/3374749}, abstract = {Recently, the urban network infrastructure has undergone a rapid expansion that is increasingly generating a large quantity of data and transforming our cities into smart cities. However, serious security problems arise with this development with more and more smart devices collecting private information under smart city scenario. In this article, we investigate the task of detecting insiders\u2019 anomalous behaviors to prevent urban big data leakage. Specifically, we characterize a user's daily activities from four perspectives and use several deep learning algorithms (long short-term memory (LSTM) and convolutional LSTM (convLSTM)) to calculate deviations between realistic actions and normalcy of daily behaviors and use multilayer perceptron (MLP) to identify abnormal behaviors according to those deviations. To evaluate the proposed multimodel-based system (MBS), we conducted experiments on the CERT (United States Computer Emergency Readiness Team) dataset. The experimental results show that our proposed MBS has a remarkable ability to learn the normal pattern of users\u2019 daily activities and detect anomalous behaviors.}, pages = {1\u201319}, numpages = {19}, keywords = {security, deep learning, anomaly detection, UEBA}}
@inproceedings{10.1145/3404649.3404656,title = {Innovation of Undergraduate Education Mode of the Financial Management Major in Big Data Era}, author = {Yao Xiaolin , Wei Qi , Zhang Qisong },year = {2020}, isbn = {9781450387781}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3404649.3404656}, doi = {10.1145/3404649.3404656}, abstract = {The fast development of a new generation of information technology represented by artificial intelligence has brought a far-reaching impact to the financial management activities in enterprises. In the future, big data, artificial intelligence and robot process automation will be widely applied, these promoted the transformation of traditional financial management into intelligent financial management [1]. How to meet the demand of financial management transformation in the big data era is an important issue that all universities and colleges should consider. By integrating OBE educational concept and CDIO engineering education mode, this paper reforms the curriculum system and teaching contents of financial management major of undergraduate education in order to improve students' ability of big data analysis. With the help of school-enterprise cooperation resources and technological advantages, the undergraduate education can cultivate compound and intelligent financial management talents to meet the needs of enterprises in the era of big data}, location = {Shanghai, China}, series = {ICEBT '20}, pages = {43\u201349}, numpages = {7}, keywords = {OBE-CDIO Mode, Big data, Artificial Intelligence, Robot Process Automation}}
@inproceedings{10.14778/2994509.2994519,title = {Exploiting soft and hard correlations in big data query optimization}, author = {Liu Hai , Xiao Dongqing , Didwania Pankaj , Eltabakh Mohamed Y. },year = {2016}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2994509.2994519}, doi = {10.14778/2994509.2994519}, abstract = {Big data infrastructures are increasingly supporting datasets that are relatively structured. These datasets are full of correlations among their attributes, which if managed in systematic ways would enable optimization opportunities that otherwise will be missed. Unlike relational databases in which discovering and exploiting the correlations in query optimization have been extensively studied, in big data infrastructures, such important data properties and their utilization have been mostly abandoned. The key reason is that domain experts may know many correlations but with a degree of uncertainty (fuzziness or softness). Since the data is big, it is very challenging to validate such correlations, judge their worthiness, and put strategies for utilizing them in query optimization. Existing techniques for exploiting soft correlations in RDBMSs, e.g., BHUNT, CORDS, and CM, are heavily tailored towards optimizing factors inherent in relational databases, e.g., predicate selectivity and random I/O accesses of secondary indexes, which are issues not applicable to big data infrastructures, e.g., Hadoop.In this paper, we propose the EXORD system to fill in this gap by exploiting the data's correlations in big data query optimization. EXORD supports two types of correlations; hard correlations---which are guaranteed to hold for all data records, and soft correlations---which are expected to hold for most, but not all, data records. We introduce a new three-phase approach for (1) Validating and judging the worthiness of soft correlations, (2) Selecting and preparing the soft correlations for deployment by specially handling the violating data records, and (3) Deploying and exploiting the correlations in query optimization. We propose a novel cost-benefit model for adaptively selecting the most beneficial soft correlations w.r.t a given query workload while minimizing the introduced overhead. We show the complexity of this problem (NP-Hard), and propose a heuristic to efficiently solve it in a polynomial time. EXORD can be integrated with various state-of-art big data query optimization techniques, e.g., indexing and partitioning. EXORD prototype is implemented as an extension to the Hive engine on top of Hadoop. The experimental evaluation shows the potential of EXORD in achieving more than 10x speedup while introducing minimal storage overheads.}, pages = {1005\u20131016}, numpages = {12}}
@inproceedings{10.1145/2910896.2925435,title = {Mining Advisor-Advisee Relationships in Scholarly Big Data: A Deep Learning Approach}, author = {Wang Wei , Liu Jiaying , Yu Shuo , Zhang Chenxin , Xu Zhenzhen , Xia Feng },year = {2016}, isbn = {9781450342292}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2910896.2925435}, doi = {10.1145/2910896.2925435}, abstract = {Mining advisor-advisee relationships can benefit many interesting applications such as advisor recommendation and protege performance analysis. Based on the hypothesis that, advisor-advisee relationships among researchers are hidden in scholarly big data, we propose in this work a deep learning based advisor-advisee relationship identification method which considers the personal properties and network characteristics with a stacked autoencoder model. To the best of our knowledge, this is the first time that a deep learning model is utilized to represent coauthor network features for relationships identification. Moreover, experiments demonstrate that the proposed method has better performance compared with other state-of-the-art methods.}, location = {Newark, New Jersey, USA}, series = {JCDL '16}, pages = {209\u2013210}, numpages = {2}, keywords = {deep learning, relationship mining, stacked autoencoders}}
@inproceedings{10.1145/3301551.3301610,title = {Research on the Transformation and Upgrading Path and Selection of Traditional Industries from the Perspective of Big Data}, author = {Li Yonghong , Zhang Shuwen , Jia Nan },year = {2018}, isbn = {9781450366298}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3301551.3301610}, doi = {10.1145/3301551.3301610}, abstract = {With the emergence of a new generation of information technology, big data has become an important driving force for current social development. Digitalization has become the main direction of the transformation and upgrading of traditional industries. As the product of current informatization, big data includes data quantity, data quality and data analysis ability. It is used as two different ways to interpret the value creation of big data, making it clear that it can promote the transformation and upgrading of traditional industries through value creation. Then, it puts forward the traditional industrial transformation and upgrading path from the perspective of big data, namely the linear path of \"traditional industry + digital\" and the transitional non-linear \"digital + traditional industry\". Its path selection will be analyzed by combining external and internal factors.}, location = {Hong Kong, Hong Kong}, series = {ICIT 2018}, pages = {54\u201359}, numpages = {6}, keywords = {Big data, Transformation and upgrading, Traditional industries, The path}}
@inproceedings{10.1145/3006299.3006320,title = {H2F: a hierarchical hadoop framework for big data processing in geo-distributed environments}, author = {Cavallo Marco , Polito Carmelo , Modica Giuseppe Di , Tomarchio Orazio },year = {2016}, isbn = {9781450346177}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3006299.3006320}, doi = {10.1145/3006299.3006320}, abstract = {Big data analysis requires adequate infrastructure and programming paradigms capable of processing large amount of data. Hadoop, the most known open-source implementation of the MapReduce paradigm, is widely employed in big data analysis frameworks. However, in many recent application scenarios data are natively distributed over different geographic regions in data centers which are inter-connected through network links with very lower bandwidth than those of the computing environments where traditionally Hadoop deployments are supposed to work. In such a context, Hadoop applications perform very poorly. To cope with these issues, we developed a Hierarchical Hadoop Framework (H2F) specifically designed to work on geodistributed data. In this work, we compare the performance of H2F with that of a plain Hadoop implementation. First results show that for very large amount of data the H2F solution performs better than the Hadoop.}, location = {Shanghai, China}, series = {BDCAT '16}, pages = {27\u201335}, numpages = {9}, keywords = {application profiling, hierarchical hadoop, mapreduce, big data, geographical computing environment}}
@inproceedings{10.1145/3349567.3351720,title = {An improved network interface card with query filter for big data systems: work-in-progress}, author = {Zhan Jinyu , Li Ying , Jiang Wei , Wu Junting , Zhu Jianping },year = {2019}, isbn = {9781450369237}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3349567.3351720}, doi = {10.1145/3349567.3351720}, abstract = {In this paper we approach to accelerate the data processing of storage and computing separated big data systems. We propose an improved Network Interface Card with Query Filter (NIC-QF), implemented by FPGA on storage nodes, to accelerate the data queries, which can also reduce the workload and communication overhead on computing nodes. NIC-QF is designed with query filtering accelerator and Network Interface Card (NIC) communicator, which can filter the original data on storage nodes as an implicit coprocessor and directly send the filtered data to computing nodes of big data systems. Filter units in NIC-QF can perform multiple SQL tasks in parallel, and each filter unit is internally pipelined, which can further speed up the data processing. Experiments with two benchmarks demonstrate the efficiency of our approach, which can achieve average up to 65.56% faster than the traditional approach.}, location = {New York, New York}, series = {CODES/ISSS '19}, pages = {1\u20132}, numpages = {2}, keywords = {network interface card, query filter, FPGA, storage and computing separated big data systems}}
@inproceedings{10.1145/3539781.3539795,title = {Toward a big data analysis system for historical newspaper collections research}, author = {Satheesan Sandeep Puthanveetil , Bhavya , Davies Adam , Craig Alan B. , Zhang Yu , Zhai ChengXiang },year = {2022}, isbn = {9781450394109}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3539781.3539795}, doi = {10.1145/3539781.3539795}, abstract = {The availability and generation of digitized newspaper collections have provided researchers in several domains with a powerful tool to advance their research. More specifically, digitized historical newspapers give us a magnifying glass into the past. In this paper, we propose a scalable and customizable big data analysis system that enables researchers to study complex questions about our society as depicted in news media for the past few centuries by applying cutting-edge text analysis tools to large historical newspaper collections. We discuss our experience with building a preliminary version of such a system, including how we have addressed the following challenges: processing millions of digitized newspaper pages from various publications worldwide, which amount to hundreds of terabytes of data; applying article segmentation and Optical Character Recognition (OCR) to historical newspapers, which vary between and within publications over time; retrieving relevant information to answer research questions from such data collections by applying human-in-the-loop machine learning; and enabling users to analyze topic evolution and semantic dynamics with multiple compatible analysis operators. We also present some preliminary results of using the proposed system to study the social construction of juvenile delinquency in the United States and discuss important remaining challenges to be tackled in the future.}, location = {Basel, Switzerland}, series = {PASC '22}, pages = {1\u201311}, numpages = {11}, keywords = {historical newspapers, social construction, newspaper article segmentation, big data analysis system, image analysis, information retrieval, natural language processing, text analysis, social science research, data visualization, juvenile delinquency}}
@inproceedings{10.1145/3453187.3453341,title = {Analysis of the Role of E-Commerce Law Based on Big Data on Live Network}, author = {Chen Sisi },year = {2020}, isbn = {9781450389099}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3453187.3453341}, doi = {10.1145/3453187.3453341}, abstract = {Nowadays, the industry of network live broadcasting platform is developing rapidly, which attracts people's attention. And in the network live broadcast, live with goods or network anchor recommended goods and other behavior has been common. Especially after the epidemic, through the network live marketing, has become an important means to promote economic recovery. However, the level of product quality in live network broadcasting is not uniform, and the problem that consumers' rights and interests cannot be protected is gradually revealed. Therefore, this paper discusses the role of e-commerce law based on big data on live network. In the discussion, this paper first analyzes the e-commerce law to clarify the applicability of the e-commerce law in the live network; secondly, through the investigation of the network anchor and fans, analyzes the business behavior in the network live broadcast; finally, analyzes the role of the e-commerce law based on big data on the network live broadcast. The results show that the e-commerce law based on big data has a good regulatory effect on webcast, which can promote the healthy development of webcast.}, location = {Wuhan, China}, series = {EBIMCS 2020}, pages = {234\u2013237}, numpages = {4}, keywords = {E-Commerce Law, Live Sales, Live Streaming, Big Data}}
@inproceedings{10.1145/1077501.1077503,title = {Handling data quality in entity resolution}, author = {Garcia-Molina Hector },year = {2005}, isbn = {1595931600}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1077501.1077503}, doi = {10.1145/1077501.1077503}, abstract = {Entity resolution (ER) is a problem that arises in many information integration scenarios: We have two or more sources containing records on the same set of real-world entities (e.g., customers).However, there are no unique identifiers that tell us what records from one source correspond to those in the other sources.Furthermore, the records representing the same entity may have differing information, e.g., one record may have the address misspelled, another record may be missing some fields.An ER algorithm attempts to identify the matching records from multiple sources (i.e., those corresponding to the same real-world entity), and merges the matching records as best it can.In many ER applications the input data has data quality or uncertainty values associated with it. Furthermore, the ER process itself introduces additional uncertainties, e.g., we may only be 90% confident that two given records actually correspond to the same real-world entity.In this talk Hector Garcia-Molina will discuss the challenges in representing quality/uncertainty/confidences in a way that is useful for the ER process.He will also present some preliminary ideas on how to perform ER with uncertain data. (This work is joint with Omar Benjelloun, David Menestrina, Qi Su, and Jennifer Widom).}, location = {Baltimore, Maryland}, series = {IQIS '05}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/3230744.3230753,title = {Small trees, big data: augmented reality model of air quality data via the chinese art of \"artificial\" tray planting}, author = {Prophet Jane , Kow Yong Ming , Hurry Mark },year = {2018}, isbn = {9781450358170}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3230744.3230753}, doi = {10.1145/3230744.3230753}, abstract = {Our prototype app, Pocket Penjing, built using Unity3D, takes its name from the Chinese \"Penjing.\" These tray plantings of miniature trees pre-date bonsai, often including miniature benches or figures to allude to people's relationship to the tree. App users choose a species, then create and name their tree. Swiping rotates a 3D globe showing flagged locations. Each flag represents a live online air quality monitoring station data stream that the app can scrape. Data is pulled in from the selected station and the AR window loads. The AR tree grows in real-time 3D. Its L-Systems form is determined by the selected live air quality data. We used this prototype as the basis of a two-part formative participatory design workshop with 63 participants.}, location = {Vancouver, British Columbia, Canada}, series = {SIGGRAPH '18}, pages = {1\u20132}, numpages = {2}, keywords = {gamification, polyaesthetics, augmented reality}}
@inproceedings{10.1145/3484622.3484626,title = {A Survey on Big Data Processing Frameworks for Mobility Analytics}, author = {Doulkeridis Christos , Vlachou Akrivi , Pelekis Nikos , Theodoridis Yannis },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3484622.3484626}, doi = {10.1145/3484622.3484626}, abstract = {In the current era of big spatial data, the vast amount of produced mobility data (by sensors, GPS-equipped devices, surveillance networks, radars, etc.) poses new challenges related to mobility analytics. A cornerstone facilitator for performing mobility analytics at scale is the availability of big data processing frameworks and techniques tailored for spatial and spatio-temporal data. Motivated by this pressing need, in this paper, we provide a survey of big data processing frameworks for mobility analytics. Particular focus is put on the underlying techniques; indexing, partitioning, query processing are essential for enabling efficient and scalable data management. In this way, this report serves as a useful guide of state-of-the-art methods and modern techniques for scalable mobility data management and analytics.}, pages = {18\u201329}, numpages = {12}}
@inproceedings{10.5555/3021955.3021960,title = {Big Data and Transparency: Using MapReduce functions to increase Public Expenditure transparency}, author = {Paiva Eduardo , Revoredo Kate },year = {2016}, isbn = {9788576693178}, publisher = {Brazilian Computer Society}, address = {Porto Alegre, BRA}, abstract = {Nowadays all government entity must maintain transparency portals that shows the all revenue and expenditure carried out daily. However, the mere availability of such information in government portals does not ensure an effective increase in the degree of transparency of these entities, because the large volume of data combined with the lack of standards makes it impossible any systematic monitoring of such data. This paper suggests the application of parallel programming techniques based on mapreduce programming paradigm to the identification of a predetermined set of products purchased by the Public Administration. It also proposes a way to consolidate this information to make easy viewing of disparities found in the large volume of data presented. The proposed solution was tested in a case study performed in the Transparency Portal of the Federal Government. The results suggest that the presented techniques constitute a promising approach to issues related to transparency areas, which normally handles large volumes of data, but it does not always provide quality information.}, location = {Florianopolis, Santa Catarina, Brazil}, series = {SBSI 2016}, pages = {25\u201332}, numpages = {8}, keywords = {text mining, Public transparency, Big data}}
@inproceedings{10.1145/3460179.3460180,title = {Research on the Design of Sports Injury Estimation Model based on Big Data}, author = {Dai Yibo },year = {2021}, isbn = {9781450388948}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3460179.3460180}, doi = {10.1145/3460179.3460180}, abstract = {In order to accurately estimate the sports injury risk of athletes during sports training, this paper divides the sports injury risk into three levels, designs the sports injury estimation index, selects RBF neural network as the model framework, and uses big data analysis technology to construct the sports injury estimation model. Bayesian model and Lagrange model are selected as the control group to test the accuracy and efficiency of this model in sports injury estimation. The test results show that compared with other models, this model can improve the accuracy and efficiency of sports injury estimation significantly, and can be used as a sports injury estimation tool.}, location = {Ho Chi Minh, Viet Nam}, series = {ICIIT '21}, pages = {1\u20136}, numpages = {6}, keywords = {big data analysis technology, RBF neural network, estimation model, sports injury}}
@inproceedings{10.1145/3352411.3352417,title = {Extending the National Lake Database of Malaysia (MyLake) as a Central Data Exchange using Big Data Integration}, author = {Yahya Farashazillah , Fazli Bashirah Mohd , Abdullah Mohd Fikri , Zulkifli Harlisa },year = {2019}, isbn = {9781450371414}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3352411.3352417}, doi = {10.1145/3352411.3352417}, abstract = {With the rise of heterogeneous large spatial and non-spatial data, systems are developed to manage these data sets. Big data emphasizes heterogeneity among systems leading to data integration issues due to the nature of big data which includes volume, variety and velocity. MyLake is a National Lake Database to manage information and knowledge sharing on lakes in Malaysia. At the moment, data are uploaded by each agency using MyLake as a platform. Nevertheless, this is carried out manually and require timely human effort. Each agency does one-to-one data integration (in-silo) where the integration is developed according to the agency-specific needs resulting from a possibility of integration issue across agencies. Therefore, this paper introduces a big data integration approach that extends the MyLake repository. The big data integration platform is a preliminary idea of how data can be shared, integrated, retrieved, and disseminated within a reliable and authenticated environment. The proposed centralized platform consisting of a set of standards, tools, repository and registry that enable multiple integrations between different agencies. The platform offers the potential to provide a reliable platform that acts as data retriever and disseminator.}, location = {Seoul, Republic of Korea}, series = {DSIT 2019}, pages = {30\u201335}, numpages = {6}, keywords = {Lake, Database, National Lake, Data Integration, Big Data, Big Data Integration}}
@inproceedings{10.1145/2479787.2479806,title = {Crafting a balance between big data utility and protection in the semantic data cloud}, author = {Hu Yuh-Jong , Cheng Kua-Ping , Huang Ya-Ling },year = {2013}, isbn = {9781450318501}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2479787.2479806}, doi = {10.1145/2479787.2479806}, abstract = {Structured big data of Personal Identifiable Information (PII) are acquired from everywhere and stored as microdata in a statistical database. Given a statistical disclosure control method, big data analysis and protection are enacted for outsourcing data sources. We flexibly glean the data utility to achieve effective data-driven decision-making. However, we still comply with the privacy protection principles while applying data analysis. In this paper, we propose three types of semantics-enabled policies for controlling access, handling data, and releasing data to craft a balance between data utility and protection. Structured big data are tagged with semantic metadata to enable semantics-enabled policy's direct processing and interpretation. Finally, we demonstrate how to craft a balance between data utility and protection with these types of semantics-enabled policies, combined with various statistical disclosure control methods.}, location = {Madrid, Spain}, series = {WIMS '13}, pages = {1\u201312}, numpages = {12}, keywords = {statistical disclosure control, world wide web, big data, data protection, data utility, semantic data cloud, semantics-enabled policy}}
@inproceedings{10.1145/3373086,title = {New Algorithms of Feature Selection and Big Data Assignment for CBR System Integrated by Bayesian Network}, author = {Guo Yuan , Sun Yu , Wu Kai , Jiang Kerong },year = {2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3373086}, doi = {10.1145/3373086}, abstract = {Under big data, the integrated system of case-based reasoning and Bayesian network has exhibited great advantage in implementing the intelligence of engineering application in many domains. To further improve the performance of the hybrid system, this article proposes Probability Change Measurement of Solution Parameters (PCMSP)\u2013Half-Division-Cross (HDC) method, which includes two algorithms, namely PCMSP and HDC algorithm. PCMSP algorithm can select principal problem features according to their effects upon all solution features measured by calculating the weighted relative probability (RP) change of all solution features caused by each problem feature. PCMSP algorithm can perfectly work under big data no matter how complex the data types are and how huge the data size is. HDC algorithm is used to assign the computation task of big data to enhance the efficiency of the integrated system. HDC algorithm assigns big data by grouping all the problem parameters into many small sub-groups and then distributing the data which covers the same sub-group of problem parameters to a slave node. HDC algorithm can guarantee enough efficiency of the integrated system under big data no matter how large the number of problem parameters is. Finally, lots of experiments are executed to validate the proposed method.}, pages = {1\u201320}, numpages = {20}, keywords = {big data, Feature selection, integrated system, CBR}}
@inproceedings{10.1145/2851613.2852015,title = {Social element of big data analytics: integrating social network with the internet of things: student research abstract}, author = {Ahmad Awais },year = {2016}, isbn = {9781450337397}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2851613.2852015}, doi = {10.1145/2851613.2852015}, abstract = {As we delve deeper into the Internet of Things (IoT), we are observing the intensive interaction and heterogeneous communication among different social objects over the Internet. Such knowledge gives us the concept of Social Internet of Things (SIoT). SIoT comprises billions of interconnected objects that generate massive volume of heterogeneous, multisource, dynamic, and sparse data, which lead a system towards a major computational challenges, such as processing, analyzing, and storing data in an efficient manner. To address this problem, we propose a system architecture for processing a stream of Big Data with the enhanced features of parallel processing techniques. The proposed architecture consists of three functional domains, i.e., object domain, SIoT server domain, and application domain. The performance of the system architecture is tested on Hadoop using UBUNTU 14.04 LTS core\u2122i5 machine with 3.2 GHz processor and 4 GB memory. The analysis and discussion show that the performance of the proposed system architecture fulfills the required desires if we increase the size of the datasets.}, location = {Pisa, Italy}, series = {SAC '16}, pages = {216\u2013217}, numpages = {2}}
@inproceedings{10.1145/2632168.2638835,title = {Language, compiler, and runtime system support towards highly scalable big data application (invited talk abstract)}, author = {Xu Guoqing },year = {2014}, isbn = {9781450329347}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2632168.2638835}, doi = {10.1145/2632168.2638835}, abstract = {Modern computing has entered the era of Big Data. Analyzing data from Twitter, Google, Facebook, Wikipedia, or the Human Genome Project requires the development of scalable platforms that can quickly extract useful information from an ocean of records collected from customers, clinical trial participants, program execution logs, or the Internet. Most of the existing Big Data applications, including Hadoop, Giraph, Hive, Pig, Mahout, or Hyracks are written managed, object-oriented languages such as Java. While the use of such languages simplifies development tasks, the (memory and execution) inefficiencies inherent in these languages can have large impact on the application performance and scalability. When object-orientation meets Big Data, performance problems are significantly magnified, making data-intensive computing systems fail to scale to large datasets. I will talk about several projects we are currently working on to scale Big Data applications by reducing the cost of a managed runtime. Particularly, I will talk about Facade, a compiler and runtime system we have developed to transform a Big Data application into an almost object-bounded application which has been shown to be much more efficient and scale to much larger datasets. I will also briefly mention two other projects, one attempting to provide a memory-oblivious programming model for developers to allow them to write a program without worrying about how to create threads and use memory, and second aiming to trim a big dataset with probabilistic guarantees to facilitate debugging/testing of a Big Data application.}, location = {San Jose, CA, USA}, series = {WODA+PERTEA 2014}, pages = {13}, numpages = {1}, keywords = {System Support, Highly Scalable Big Data Application}}
@inproceedings{10.1145/3331453.3362042,title = {Long Term Healthcare System for Elders by Using Internet of Things with Big Data}, author = {Lin Ching-Lung , Lin Huang-Liang , Lin Shu-Chi , Liu Yung-Te },year = {2019}, isbn = {9781450362948}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3331453.3362042}, doi = {10.1145/3331453.3362042}, abstract = {This paper proposes an elder care system that develops a multi-solution for Taiwan's current Long-Term Care 2.0. The system is designed by using the Internet of Things, Big data, cloud database, application of various sensors, and the integration of the experiences of the Long-Term Care Centers. We find a way to create the maximum effectiveness with the least resources, so that elders in long-term care centers can keep their ability of daily living activities (ADLs) and instrumental activities of daily living (IADLs), and it alleviates internal pressures and costs inside country under the continuing aging society.}, location = {Sanya, China}, series = {CSAE 2019}, pages = {1\u20135}, numpages = {5}, keywords = {Aging country, Big Data, Internet of Things, Long Term Healthcare 2.0}}
@inproceedings{10.1145/3495018.3495345,title = {Human Resource Information System Performance Test under Big Data Technology}, author = {Chen Xin , Yang Lirong , Sun Yanzhi },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495345}, doi = {10.1145/3495018.3495345}, abstract = {The era of big data has quietly arrived, which is a revolution that determines the development and future destiny of enterprises. Any enterprises that are not ready for this revolution will be eliminated by the era. This paper mainly studies the construction, analysis and management of human resource system in the era of big data. Based on the actual needs, this paper analyzes the business process and functional requirements of human resource management, completes the system architecture design, function module design, database design, realizes the system function module, and completes the test of the system function. The functional modules realized in this paper include: core personnel management, salary management and comprehensive inquiry. The human resource information system designed in this paper ensures the scientific nature, security, availability and portability of the system, meets the demand of data sharing, and plays a positive role in the whole human resource management cycle.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {1107\u20131111}, numpages = {5}}
@inproceedings{10.1145/3417188.3417214,title = {The Development Trend of Design Methodology under the Influence of Artificial Intelligence and Big Data}, author = {Shu Wei , Sun Fuliang , Li Yueen },year = {2020}, isbn = {9781450375481}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3417188.3417214}, doi = {10.1145/3417188.3417214}, abstract = {Traditional design methods are inspired by introverted self-salvation or creativity-driven design. In the era of big data, they are gradually driven by vast data. Design innovation without data is increasingly lacking in persuasion. The design of data participation increasingly faces market risks. Moreover, with the progress of artificial intelligence, such a technological innovation will eventually deconstruct the existing field of design innovation, its impact will continue, and it may fundamentally spawn new design ideas and methods.}, location = {Beijing, China}, series = {ICDLT '20}, pages = {104\u2013108}, numpages = {5}, keywords = {design methodology, Artificial intelligence, innovation, big data}}
@inproceedings{10.1145/3278229.3278235,title = {Health Assessment System Based on Big Data Analysis of Meridian Electrical Potential}, author = {Li Hui , Cheng Yibo , Li Yinghui , Ma Xiaochang , Li Delong },year = {2018}, isbn = {9781450364362}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3278229.3278235}, doi = {10.1145/3278229.3278235}, abstract = {This paper develops a health management and assessment system named Jinluo Kangbao Health Management System, which is based on big data analysis of meridian potential value. The main purpose is to achieve fast and low-cost diagnosis of disease, and to alleviate problems like unequal distribution of medical resources. The system contains three major modules, the meridian detector, the client software and the central database. During test process, the electrode of meridian detector contacts 24 acupuncture points of the body in particular order, and collects potential value of each acupuncture point. By constructing a calculation matrix, the potential values are corresponded to certain regions of a 24-dimensional space. Within this space, the positions are used to judge the status of the client's health condition, including at low-risk, at medium-risk, at high risk, in subclinical state or in clinical state. The collected data are analyzed by the central database, which also provides reliable medical advice. In this paper, we use Jinluo Kangbao system to check a client. We list the detailed results concerning major aspects of his health condition, and give him relevant medical advice. The output of this system shows high accuracy compared to the results provided by hospital.}, location = {Seoul, Republic of Korea}, series = {ICBIP '18}, pages = {75\u201380}, numpages = {6}, keywords = {Acupuncture point, Health assessment system, Big data analysis, Medical advice, Meridian detector}}
@inproceedings{10.1145/3507485.3507494,title = {Research on the Innovation Path of Business Model of E-Commerce Enterprises Affected by Big Data}, author = {LYU Xiaoyong },year = {2021}, isbn = {9781450385831}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3507485.3507494}, doi = {10.1145/3507485.3507494}, abstract = {With the rapid development of Internet technology, the collection and application of user data is becoming more and more of a concern for Internet companies. At the same time, enterprises have gained extremely high revenue due to the application of big data technology. The field of e-commerce is greatly influenced by the rapid development of big data technology. E-commerce enterprises can reform and innovate their business models based on their own platform advantages and with the power of big data technology in order to promote the good development of e-commerce enterprises. Therefore, the innovation of business models is first briefly introduced in this paper. Then, the important value of the application of big data to the innovation of business models of e-commerce enterprises is analyzed. Finally, the innovation path of the business model of e-commerce enterprises influenced by big data is presented.}, location = {Osaka, Japan}, series = {ICSEB 2021}, pages = {51\u201354}, numpages = {4}, keywords = {E-Commerce Companies, Innovation, Business Models, Big Data}}
@inproceedings{10.1145/1891879.1891881,title = {The Effects and Interactions of Data Quality and Problem Complexity on Classification}, author = {Blake Roger , Mangiameli Paul },year = {2011}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1891879.1891881}, doi = {10.1145/1891879.1891881}, abstract = {Data quality remains a persistent problem in practice and a challenge for research. In this study we focus on the four dimensions of data quality noted as the most important to information consumers, namely accuracy, completeness, consistency, and timeliness. These dimensions are of particular concern for operational systems, and most importantly for data warehouses, which are often used as the primary data source for analyses such as classification, a general type of data mining. However, the definitions and conceptual models of these dimensions have not been collectively considered with respect to data mining in general or classification in particular. Nor have they been considered for problem complexity. Conversely, these four dimensions of data quality have only been indirectly addressed by data mining research. Using definitions and constructs of data quality dimensions, our research evaluates the effects of both data quality and problem complexity on generated data and tests the results in a real-world case. Six different classification outcomes selected from the spectrum of classification algorithms show that data quality and problem complexity have significant main and interaction effects. From the findings of significant effects, the economics of higher data quality are evaluated for a frequent application of classification and illustrated by the real-world case.}, pages = {1\u201328}, numpages = {28}, keywords = {data quality metrics and measurements, information quality, Data quality, data mining}}
@inproceedings{10.14778/2536360.2536368,title = {Making queries tractable on big data with preprocessing: through the eyes of complexity theory}, author = {Fan Wenfei , Geerts Floris , Neven Frank },year = {2013}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2536360.2536368}, doi = {10.14778/2536360.2536368}, abstract = {A query class is traditionally considered tractable if there exists a polynomial-time (PTIME) algorithm to answer its queries. When it comes to big data, however, PTIME algorithms often become infeasible in practice. A traditional and effective approach to coping with this is to preprocess data off-line, so that queries in the class can be subsequently evaluated on the data efficiently. This paper aims to provide a formal foundation for this approach in terms of computational complexity. (1) We propose a set of \u03a0-tractable queries, denoted by \u03a0TQ0, to characterize classes of queries that can be answered in parallel poly-logarithmic time (NC) after PTIME preprocessing. (2) We show that several natural query classes are \u03a0-tractable and are feasible on big data. (3) We also study a set \u03a0TQ of query classes that can be effectively converted to \u03a0-tractable queries by refactorizing its data and queries for preprocessing. We introduce a form of NC reductions to characterize such conversions. (4) We show that a natural query class is complete for \u03a0TQ. (5) We also show that \u03a0TQ0 \u2282 P unless P = NC, i.e., the set \u03a0TQ0 of all \u03a0-tractable queries is properly contained in the set P of all PTIME queries. Nonetheless, \u03a0TQ = P, i.e., all PTIME query classes can be made \u03a0-tractable via proper refactorizations. This work is a step towards understanding the tractability of queries in the context of big data.}, pages = {685\u2013696}, numpages = {12}}
@inproceedings{10.1145/3459930.3470855,title = {Search feasibility in distributed MS-proteomics big data}, author = {Mohammad Umair , Saeed Fahad },year = {2021}, isbn = {9781450384506}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459930.3470855}, doi = {10.1145/3459930.3470855}, abstract = {Making large-scale Mass Spectrometry (MS) data FAIR (Findable, Accessible, Interoperable, Reusable) and democratizing access for the omics research community requires advance access and reuse mechanisms. In this work, we proposed a novel distributed data access infrastructure and developed a simulation test-bed to show the feasibility of this solution. In contrast to existing centralized approaches, participating nodes are relied upon to execute the search algorithm and search based on the comparison of raw spectra is supported as opposed to simple meta-data based searches. Simulation results using networking, stochastic modelling, and queuing theory, illustrated that search times were reduced by up-to 600 times for up-to a total of fifty billion spectra. Proteomics is vital because of the importance proteins to life and their role in state-of-the-art medicine such as custom drug delivery and cancer treatment. MS-based proteomics involves the fragmentation of proteins into peptide ions to generate raw MS spectra. Traditionally, scientists have relied on meta-data based searches of centralized repositories followed by complex database searches and protein sequencing. Though useful, this technique may result in missed datasets because of poor meta-data or sheer amount of effort and computational time needed. Recently, direct raw spectra search has been proposed with the development of centralized tools such as PeptideAtlas. However, PeptideAtlas hosts 13,000 spectra whereas systems supporting billions of spectra are needed. Let us assume users can submit one or more query spectra for search to a central controller. In the proposed novel distributed paradigm, the controller will forward the queries to several nodes hosting a total of multiple MS/MS datasets, where each of the nodes will run the search algorithm against against each spectrum in their local MS/MS dataset, and send the results as URLs/pointers and associated scores back to the controller. The controller will then collate the results and transmit them back to the users. To simulate the system performance, we focused on the distributed process between the controller and the participating nodes. We modeled the the nodes using computational devices present in typical research labs, communication links as the average achievable by combined fiber/Ethernet links, and data loads based on typical storage sizes of spectra and URLs. By running Monte Carlo simulations, we were able to obtain the response time to a single query for various scenarios and assuming an M/M/1 queue, we simulated the time degradation due to multiple requests by compounding over the number of requests with a load degradation factor. Testing results for fifty billion spectra indicated that using 500 distributed nodes can provide search results in 10s and 2000 nodes in 5s, a reduction by 100 and 200 times, respectively, compared to a centralized approach which requires 1000s. Considering typical capabilities of modern day servers and computers, a load factor of 0.001% was tested and indicated that the system provided constant time performance up-to 10k concurrent queries. Lastly, accounting for communication link degradation demonstrated that a trade-off can be achieved between performance and number of nodes. Therefore, it is worth investigating the implementation of a distributed big-data access infrastructure for proteomics.}, location = {Gainesville, Florida}, series = {BCB '21}, pages = {1}, numpages = {1}, keywords = {distributed infrastructure, networked database, big omics data, modelling and simulation, spectral search, mass spectrometry, proteomics}}
@inproceedings{10.1145/2818869.2818898,title = {A Hierarchical Distributed Fog Computing Architecture for Big Data Analysis in Smart Cities}, author = {Tang Bo , Chen Zhen , Hefferman Gerald , Wei Tao , He Haibo , Yang Qing },year = {2015}, isbn = {9781450337359}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2818869.2818898}, doi = {10.1145/2818869.2818898}, abstract = {The ubiquitous deployment of various kinds of sensors in smart cities requires a new computing paradigm to support Internet of Things (IoT) services and applications, and big data analysis. Fog Computing, which extends Cloud Computing to the edge of network, fits this need. In this paper, we present a hierarchical distributed Fog Computing architecture to support the integration of massive number of infrastructure components and services in future smart cities. To secure future communities, it is necessary to build large-scale, geospatial sensing networks, perform big data analysis, identify anomalous and hazardous events, and offer optimal responses in real-time. We analyze case studies using a smart pipeline monitoring system based on fiber optic sensors and sequential learning algorithms to detect events threatening pipeline safety. A working prototype was constructed to experimentally evaluate event detection performance of the recognition of 12 distinct events. These experimental results demonstrate the feasibility of the system's city-wide implementation in the future.}, location = {Kaohsiung, Taiwan}, series = {ASE BD&amp;SI '15}, pages = {1\u20136}, numpages = {6}, keywords = {Fog computing, smart city, pipeline safety monitoring, big data analysis, distributed computing architecture}}
@inproceedings{10.1145/3371158.3371161,title = {Concealed Multidimensional Data Aggregation in Big Data Wireless Sensor Networks}, author = {Maivizhi Radhakrishnan , Yogesh Palanichamy },year = {2020}, isbn = {9781450377386}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3371158.3371161}, doi = {10.1145/3371158.3371161}, abstract = {Wireless sensor networks (WSNs) deployed in a plethora of applications produce a significant portion of big data. Handling these huge volume of data is a critical challenge in a resource constrained wireless sensor networks. Data aggregation is the most practical and important paradigm in big data wireless sensor networks. It reduces the huge volume of data by combining the similar data and eliminating data redundancy and reduces thereby the resource consumption. However preserving data confidentiality and integrity along with en-route aggregation is a great challenge. In this paper, we propose a novel Concealed Multidimensional Data Aggregation (CMDA) protocol for big data wireless sensor networks. CMDA integrates super-increasing sequence and homomorphic encryption to structure the multidimensional data and protect the data privacy and a homomorphic signature to check the integrity of data. In addition, the proposed protocol filters false data packets and achieves data freshness. Security analysis reveals that the proposed protocol achieves end-to-end security and performance evaluation shows that CMDA incurs less communication overhead and consequently reduces energy consumption which enhances the lifetime of sensor networks. To the best of our knowledge, this is the first work that achieves end-to-end security in multidimensional data aggregation.}, location = {Hyderabad, India}, series = {CoDS COMAD 2020}, pages = {19\u201327}, numpages = {9}, keywords = {privacy homomorphism, energy efficiency, concealed data aggregation, wireless sensor networks, multidimensional data}}
@inproceedings{10.1145/3373376.3380611,title = {Big Data of the Past, from Venice to Europe}, author = {Kaplan Fr\u00e9d\u00e9ric },year = {2020}, isbn = {9781450371025}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3373376.3380611}, doi = {10.1145/3373376.3380611}, abstract = {In 2012, the Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL) and the University Ca'Foscari launched a program called the Venice Time Machine, whose goal was to develop a large-scale digitisation program to transform Venice's heritage into 'Big Data of the Past'. Millions of register pages and photographs have been scanned at the State Archive in Venice and at the Fondazione Giorgio Cini. These documents were analysed using the deep-learning artificial-intelligence methods developed at EPFL's Digital Humanities Laboratory in order to extract their textual and iconographic content and to make the data accessible via a search engine. The project has now expand to a European scale, including more than 500 institutions and 20 new cities jointly constructing a distributed digital information system mapping the social, cultural and geographical evolution of Europe. The project build upon existing platforms such as Europeana, and accelerate their development. While Europeana drives transformation throughout the cultural heritage sector with innovative standards, infrastructure and networks, Time Machine aims to design and implement advanced new digitisation and artificial intelligence technologies to mine Europe's vast cultural heritage, providing fair and free access to information that will support future scientific and technological developments in Europe.}, location = {Lausanne, Switzerland}, series = {ASPLOS '20}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/2627770.2627774,title = {Big-Data Management Use-Case: A Cloud Service for Creating and Analyzing Galactic Merger Trees}, author = {Loebman Sarah , Ortiz Jennifer , Choo Lee Lee , Orr Laurel , Anderson Lauren , Halperin Daniel , Balazinska Magdalena , Quinn Thomas , Governato Fabio },year = {2014}, isbn = {9781450329972}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2627770.2627774}, doi = {10.1145/2627770.2627774}, abstract = {We present the motivation, design, implementation, and preliminary evaluation for a service that enables astronomers to study the growth history of galaxies by following their `merger trees' in large-scale astrophysical simulations. The service uses the Myria parallel data management system as back-end and the D3 data visualization library within its graphical front-end. We demonstrate the service at the workshop on a ~5TB dataset.}, location = {Snowbird, UT, USA}, series = {DanaC'14}, pages = {1\u20134}, numpages = {4}, keywords = {Myria, Cloud service, astronomy, parallel data management}}
@inproceedings{10.1145/3127479.3129248,title = {An experimental comparison of complex object implementations for big data systems}, author = {Sikdar Sourav , Teymourian Kia , Jermaine Chris },year = {2017}, isbn = {9781450350280}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3127479.3129248}, doi = {10.1145/3127479.3129248}, abstract = {Many cloud-based data management and analytics systems support complex objects. Dataflow platforms such as Spark and Flink allow programmers to manipulate sets consisting of objects from a host programming language (often Java). Document databases such as MongoDB make use of hierarchical interchange formats---most popularly JSON---which embody a data model where individual records can themselves contain sets of records. Systems such as Dremel and AsterixDB allow complex nesting of data structures.Clearly, no system designer would expect a system that stores JSON objects as text to perform at the same level as a system based upon a custom-built physical data model. The question we ask is: How significant is the performance hit associated with choosing a particular physical implementation? Is the choice going to result in a negligible performance cost, or one that is debilitating? Unfortunately, there does not exist a scientific study of the effect of physical complex model implementation on system performance in the literature. Hence it is difficult for a system designer to fully understand performance implications of such choices. This paper is an attempt to remedy that.}, location = {Santa Clara, California}, series = {SoCC '17}, pages = {432\u2013444}, numpages = {13}, keywords = {complex objects implementation, experimental comparison, big data management, data serialization}}
@inproceedings{10.5555/2888619.2888976,title = {The impact of big data on M&S: do we need to get \"big\"?}, author = {Taylor Siman J. E. },year = {2015}, isbn = {9781467397414}, publisher = {IEEE Press}, abstract = {Driven by innovations such as mass customisation, complex supply chains, smart cities and emerging cyber-physical and Internet of Things systems, Big Data is presenting a fascinating range of challenges to Analytics. New fields are emerging such as Big Data Analytics and Data Science. Modeling & Simulation (M&S) is core to Analytics. Arguably, contemporary M&S practices cannot deal with the demands of Big Data. The implication of this is that M&S may not feature in the Big Data Analytics techniques and tools of the future. Based on recent experiences from the i4MS FP7 European Cloud-based Simulation platform for Manufacturing and Engineering (CloudSME) and associated industrial projects, this talk will outline the key challenges that Big Data has to M&S and strongly argue that M&S has to get \"Big\" to meet these challenges. Exciting opportunities lie ahead for multi-disciplinary teams of practitioners and researchers from OR/MS, Computer Science and domain specific fields. Indeed \"Big\" Simulation presents its own possibilities and the talk will conclude with thoughts on the potential for \"Big\" Simulation Analytics to move beyond Big Data into future Dynamic Data Driven Application Systems.}, location = {Huntington Beach, California}, series = {WSC '15}, pages = {3085}, numpages = {1}}
@inproceedings{10.1145/3282278.3282282,title = {Blockchain framework for IoT data quality via edge computing}, author = {Casado-Vara Roberto , de la Prieta Fernando , Prieto Javier , Corchado Juan M. },year = {2018}, isbn = {9781450360500}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3282278.3282282}, doi = {10.1145/3282278.3282282}, abstract = {Smart home presents a challenge in control and monitoring of its wireless sensors networks (WSN) and the internet of things (IoT) devices which form it. The current IoT architectures are centralized, complex, with poor security in its communications and with upstream communication channels mainly. As a result, there are problems with data reliability. These problems include data missing, malicious data inserted, communications network overload, and overload of computing power at the central node. In this paper a new architecture is presented. This architecture based in blockchain introduce the edge computing layer and a new algorithm to improve data quality and false data detection.}, location = {Shenzhen, China}, series = {BlockSys'18}, pages = {19\u201324}, numpages = {6}, keywords = {WSN, data quality false data detection, non linear control, Blockchain, IoT, edge computing}}
@inproceedings{10.1145/3416921.3416944,title = {OLAPing Big Social Data: Multidimensional Big Data Analytics over Big Social Data Repositories}, author = {Cuzzocrea Alfredo },year = {2020}, isbn = {9781450375382}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3416921.3416944}, doi = {10.1145/3416921.3416944}, abstract = {Nowadays, a great deal of attention is devoted to the relevant problem of supporting big data analytics from social systems (e.g., social networks, smart city applications, skill management platforms, and so forth). Following this innovative trend, the opportunity of adopting advanced OLAP-based tools for supporting the knowledge extraction phase from big social data represents the new frontiers for big social data computing. Indeed, the well-known features of multidimensional data analysis are able to support a \"rich\" extraction of actionable knowledge, beyond actual limitations of alternative procedural approaches. In line with this emerging research challenge, this paper explores benefits, limitations and challenges of OLAP-based big data analytics tools over (big) social data.}, location = {Virtual, United Kingdom}, series = {ICCBDC '20}, pages = {15\u201319}, numpages = {5}, keywords = {Big social data computing, OLAPing big social data, Big social data, Big data analytics}}
@inproceedings{10.1145/3040934,title = {Mobile Social Multimedia Analytics in the Big Data Era: An Introduction to the Special Issue}, author = {Ji Rongrong , Liu Wei , Xie Xing , Chen Yiqiang , Luo Jiebo },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3040934}, doi = {10.1145/3040934}, pages = {1\u20133}, numpages = {3}}
@inproceedings{10.1145/2978570,title = {Privacy-Preserving Multimedia Big Data Aggregation in Large-Scale Wireless Sensor Networks}, author = {Wu Dapeng , Yang Boran , Wang Honggang , Wang Chonggang , Wang Ruyan },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2978570}, doi = {10.1145/2978570}, abstract = {To preserve the privacy of multimedia big data and achieve the efficient data aggregation in wireless multimedia sensor networks (WMSNs), a distributed compressed sensing--based privacy-preserving data aggregation (DCSPDA) approach is proposed in this article. First, in this approach, the original multimedia sensor data are compressed and measured by distributed compressed sensing (DCS) and the compressed data measurements are uploaded to the sink, by which the inherent characteristics between sensor data can be obtained. Second, the original multimedia data are jointly recovered and the common and innovation sparse components are obtained through solving the optimization problem and linear equations at the sink. Third, through least squares support vector machine (LSSVM) learning of the sparse components, the sparse position configuration can be determined and disseminated for each node to conduct the privacy-preserving data configuration. After receiving the configuration message, original multimedia sensor data are accordingly customized, compressed, and measured by the common measurement matrix, aggregated at the cluster heads, and transmitted to the sink. Finally, the aggregated multimedia sensor data are recovered by the sink according to the data configuration to achieve the privacy-preserving data aggregation and transmission. Our comparative simulation results validate the efficiency and scalability of DCSPDA and demonstrate that the proposed approach can effectively reduce the communication overheads and provide reliable privacy-preserving with low computational complexity for WMSNs.}, pages = {1\u201319}, numpages = {19}, keywords = {distributed compressed sensing, data aggregation, Wireless multimedia sensor networks, privacy-preserving method}}
@inproceedings{10.1145/3274572,title = {Child-computer interaction, ubiquitous technologies, and big data}, author = {Hourcade Juan Pablo , Antle Alissa N. , Anthony Lisa , Fails Jerry Alan , Iversen Ole Sejer , Rubegni Elisa , Skov Mikael , Slovak Petr , Walsh Greg , Zeising Anja },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3274572}, doi = {10.1145/3274572}, abstract = {In this forum we celebrate research that helps to successfully bring the benefits of computing technologies to children, older adults, people with disabilities, and other populations that are often ignored in the design of mass-marketed products. --- Juan Pablo Hourcade, Editor}, pages = {78\u201381}, numpages = {4}}
@inproceedings{10.1145/3465631.3465807,title = {New Sports Fitness Space Based on Big Data}, author = {Liao Wenhao , Ren Yumei },year = {2021}, isbn = {9781450385015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3465631.3465807}, doi = {10.1145/3465631.3465807}, abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.}, location = {Jakarta, Indonesia}, series = {ICIMTECH 21}, pages = {1\u20135}, numpages = {5}}