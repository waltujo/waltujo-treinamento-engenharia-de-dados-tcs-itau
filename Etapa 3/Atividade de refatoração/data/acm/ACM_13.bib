@inproceedings{10.5555/3213032.3213044,title = {Big data, agents, and machine learning: towards a data-driven agent-based modeling approach}, author = {Kavak Hamdi , Padilla Jose J. , Lynch Christopher J. , Diallo Saikou Y. },year = {2018}, isbn = {9781510860148}, publisher = {Society for Computer Simulation International}, address = {San Diego, CA, USA}, abstract = {We have recently witnessed the proliferation of large-scale behavioral data that can be used to empirically develop agent-based models (ABMs). Despite this opportunity, the literature has neglected to offer a structured agent-based modeling approach to produce agents or its parts directly from data. In this paper, we present initial steps towards an agent-based modeling approach that focuses on individual-level data to generate agent behavioral rules and initialize agent attribute values. We present a structured way to integrate Big Data and machine learning techniques at the individual agent-level. We also describe a conceptual use-case study of an urban mobility simulation driven by millions of geo-tagged Twitter social media messages. We believe our approach will advance the-state-of-the-art in developing empirical ABMs and conducting their validation. Further work is needed to assess data suitability, to compare with other approaches, to standardize data collection, and to serve all these features in near-real time.}, location = {Baltimore, Maryland}, series = {ANSS '18}, pages = {1\u201312}, numpages = {12}, keywords = {data-driven modeling, machine learning, agent-based simulation, big data}}
@inproceedings{10.1145/3197091.3205834,title = {Introducing big data analytics in high school and college}, author = {Sooriamurthi Raja },year = {2018}, isbn = {9781450357074}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3197091.3205834}, doi = {10.1145/3197091.3205834}, abstract = {In this teaching tip and courseware note we describe a series of hands on activities and exercises that we've used to introduce the notion of big data analytics to a wide range of audience. These exercises range in complexity from a paper and pencil thought exercise, to using Google Trends for simple explorations, to using a spread sheet to simulate the iterative nature of Google's PageRank algorithm, to programming with a Python based map-reduce framework. These exercises have been used in courses to train high school teachers in data science, full semester university courses (undergraduate and graduate), and CS education outreach efforts. Feedback has been positive as to their efficacy.}, location = {Larnaca, Cyprus}, series = {ITiCSE 2018}, pages = {373\u2013374}, numpages = {2}, keywords = {Map Reduce, Google Trends, Outreac, Big data, PageRank}}
@inproceedings{10.1145/3445945.3445962,title = {A Visual Method for Ship Close Encounter Pattern Recognition based on Fuzzy theory and Big Data Intelligence}, author = {Zhao Liangbin , Fu Xiuju },year = {2020}, isbn = {9781450387750}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3445945.3445962}, doi = {10.1145/3445945.3445962}, abstract = {As an important application of big data technology in the maritime field, big data driven visualization of ship encounter patterns helps to intuitively understand the risk situation in the water traffic. However traditional methods based on fixed thresholds do not consider the fuzziness of classification on ship encounter situations. We proposed a visual method to visualize the risk situations caused by the interaction between vessel traffic flows in more detail based on fuzzy theory and big data intelligence on large scale AIS data. A case study is conducted to verify the applicability based on the AIS data from Singapore Strait. Visualization results of density in grids show that the proposed method can effectively reflect the ship encounter patterns, which are consistent with the real situation and can show more valuable details for the safety assessment of water traffic.}, location = {Tokyo, Japan}, series = {ICBDR 2020}, pages = {94\u2013100}, numpages = {7}, keywords = {Ship encounter, Visualization, Fuzzy theory, AIS data}}
@inproceedings{10.1145/3399205.3399225,title = {Big Data Analytic Architecture for Water Resources Management: A Systematic Review}, author = {Elhassan Jamal , Aniss Moumen , Jamal Chao },year = {2020}, isbn = {9781450375788}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3399205.3399225}, doi = {10.1145/3399205.3399225}, abstract = {The management of hydraulic resources and especially water resources has become a major priority for decision-makers and planners at the international level. The advent of new technologies such as big data analytics and IoT has led to exponential changes in the volume of data generated daily in real-time, this monitoring information has potential significance if it is collected and aggregated effectively. The data collected are increasingly complex and represent a central government issue, developing water resource management strategies. The exploitation of this type of data requires new methods of analysis and knowledge discovery. This work presents a literature review of articles related to big data and water resources. Also, we present our proposition of a new architecture to conduct a big data analytic. In conclusion, we discuss the impact of using this technology in water resource management.}, location = {Al-Hoceima, Morocco}, series = {GEOIT4W-2020}, pages = {1\u20135}, numpages = {5}, keywords = {IoT, real-time, big data analytics, water resources}}
@inproceedings{10.1109/JCDL.2019.00118,title = {Organizing data, information, and knowledge in big data environments}, author = {Chen Jiangping , Lu Wei , Zavalina Oksana },year = {2019}, isbn = {9781728115474}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/JCDL.2019.00118}, doi = {10.1109/JCDL.2019.00118}, abstract = {The explosion of information and the availability of big data provide new significant challenges for digital libraries. For example, libraries face the \"cyberinfrastructural challenge\" and the need to develop better understanding of research data to support curation, sharing and reuse of data generated by data-intensive science [9][6]. The synergy between information science and data science is emerging to address these Big Data environment challenges. The most fruitful collaboration areas for this synergy include those related to information organization: \"big metadata, smart metadata\" the ways to leverage the \"metadata capital\"[4]. The proposed one-day workshop will seek to support the interdisciplinary collaboration in this important area. It will focus on the challenges and opportunities provided by Big Data environment for information and computing professionals to explore innovative strategies and solutions for organizing data, information, and knowledge.}, location = {Champaign, Illinois}, series = {JCDL '19}, pages = {459\u2013460}, numpages = {2}, keywords = {big data analytics, data management, knowledge organization, information organization}}
@inproceedings{10.1145/2168556.2168563,title = {Eye tracker data quality: what it is and how to measure it}, author = {Holmqvist Kenneth , Nystr\u00f6m Marcus , Mulvey Fiona },year = {2012}, isbn = {9781450312219}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2168556.2168563}, doi = {10.1145/2168556.2168563}, abstract = {Data quality is essential to the validity of research results and to the quality of gaze interaction. We argue that the lack of standard measures for eye data quality makes several aspects of manufacturing and using eye trackers, as well as researching eye movements and vision, more difficult than necessary. Uncertainty regarding the comparability of research results is a considerable impediment to progress in the field. In this paper, we illustrate why data quality matters and review previous work on how eye data quality has been measured and reported. The goal is to achieve a common understanding of what data quality is and how it can be defined, measured, evaluated, and reported.}, location = {Santa Barbara, California}, series = {ETRA '12}, pages = {45\u201352}, numpages = {8}, keywords = {eye movements, data quality, eye tracker, precision, latency, accuracy}}
@inproceedings{10.5555/2627435.2638579,title = {Towards ultrahigh dimensional feature selection for big data}, author = {Tan Mingkui , Tsang Ivor W. , Wang Li },year = {2014}, publisher = {JMLR.org}, abstract = {In this paper, we present a new adaptive feature scaling scheme for ultrahigh-dimensional feature selection on Big Data, and then reformulate it as a convex semi-infinite programming (SIP) problem. To address the SIP, we propose an efficient feature generating paradigm. Different from traditional gradient-based approaches that conduct optimization on all input features, the proposed paradigm iteratively activates a group of features, and solves a sequence of multiple kernel learning (MKL) subproblems. To further speed up the training, we propose to solve the MKL subproblems in their primal forms through a modified accelerated proximal gradient approach. Due to such optimization scheme, some efficient cache techniques are also developed. The feature generating paradigm is guaranteed to converge globally under mild conditions, and can achieve lower feature selection bias. Moreover, the proposed method can tackle two challenging tasks in feature selection: 1) group-based feature selection with complex structures, and 2) nonlinear feature selection with explicit feature mappings. Comprehensive experiments on a wide range of synthetic and real-world data sets of tens of million data points with O(1014) features demonstrate the competitive performance of the proposed method over state-of-the-art feature selection methods in terms of generalization performance and training effciency.}, pages = {1371\u20131429}, numpages = {59}, keywords = {big data, multiple kernel learning, nonlinear feature selection, feature generation, feature selection, ultrahigh dimensionality}}
@inproceedings{10.1145/3441369.3441376,title = {Research on Hypertension Prediction and Diagnosis Based on Big Data}, author = {Song Changxin , Ma Ke },year = {2020}, isbn = {9781450389044}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3441369.3441376}, doi = {10.1145/3441369.3441376}, abstract = {With the rapid development of network information technology and big data mining technology, data collection, processing and analysis in different industries have begun to use big data Association rule algorithm, decision tree algorithm, etc, perform processing and display of various data resources. Hypertension, as a common cardiovascular disease in society, should be diagnosed and predicted in time according to the height, weight, smoking, drinking, kidney disease, family history of different individuals, physical exercise and eating habits and other data information, expand the sick population systolic blood pressure, diastolic blood pressure testing, and to predict the future development of hypertension disease, but the traditional data collection scheme through the way of inquiry, obviously, the accuracy of hypertension prediction cannot be guaranteed. Based on this, this paper uses the improved Big Data algorithm to carry out the collection, cleaning, transformation and modeling analysis of various characteristic parameters of hypertension diseases, and obtains the grades of the influence of different data indexes on hypertension, and put forward the coping strategies of hypertension disease prediction and prevention.}, location = {Kyoto, Japan}, series = {DMIP '20}, pages = {40\u201344}, numpages = {5}, keywords = {Algorithm research, Diagnosis, Hypertension prediction, Big data}}
@inproceedings{10.1145/3456887.3457522,title = {Research on the Application Status of University Sports Big Data}, author = {He Xiaohua , Zhang Wei },year = {2021}, isbn = {9781450389969}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456887.3457522}, doi = {10.1145/3456887.3457522}, abstract = {Colleges and universities are the most important positions for personnel training in the new era. With the coverage of China's 5th-Generation network and the construction of an information and intelligent smart campus in internet plus, the application of big data in university sports is bound to become a trend. This paper investigates the application status and influencing factors of big data in college sports in China, aiming to provide countermeasures and suggestions for the construction of smart sports campus in Colleges and universities.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2021}, pages = {1355\u20131359}, numpages = {5}, keywords = {Big data, Status quo, College physical education}}
@inproceedings{10.1145/3460866,title = {Proceedings of the International Workshop on Big Data in Emergent Distributed Environments},year = {2021}, isbn = {9781450384650}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The goal of this workshop is to bring together academic researchers and industry practitioners to address the challenges and report and exchange the research findings in Big Data in emergent distributed environments, including new approaches, techniques and applications, make substantial theoretical and empirical contributions to, and significantly advance the state of the art of Big Data in emergent distributed environments.}, location = {Virtual Event, China}}
@inproceedings{10.1145/3152723.3152743,title = {Prediction of NO2 Emission Concentration via Correlation of Multiple Big Data Sources Using Extreme Learning Machine}, author = {Phing Chen Chai , Kiong Tiong Sieh , Yapandi Md Fauzan K. Mohd , Paw Johnny Koh Siaw },year = {2017}, isbn = {9781450353564}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3152723.3152743}, doi = {10.1145/3152723.3152743}, abstract = {Increase of electricity demand and urbanization process has caused more power plants to be built to meet the demand of electricity. However, development of power plant will cause environmental issue for its surrounding. Necessary measures need to be taken to ensure social and environmental sustainability. Among the requirements in Malaysia, discharge of air pollution emission of a gas- or distillate-fired power plant has to comply with air pollution level as described in the Malaysian Ambient Air Quality Standards ((MAAQS) 2013 and the Environmental Quality (Clean Air) Regulations 2014. Pertaining to the environmental requirements, this paper is to investigate the ability of a regression based artificial intelligence tool, namely Extreme Learning Machine (ELM) in correlating multiple sources of big data sets and subsequently predicting the air pollution emission level from the chimney of a Combined Cycle Gas Turbine (CCGT) power plant. This emission data is later being used to ensure the clean air regulatory requirement is fulfilled. The big data sources that have been used in this work are meteorological data, terrain and land use data, historical emission data and power plant parameters particularly related to the point source emitter. With the correlation of multiple big data sources, Extreme Learning Machine (ELM) is then trained for the prediction of emission rate at certain targeted areas, which are classified as air sensitive receptors (ASR) surrounding the power plant. Nitrogen dioxide (NO2) is the key emission that has been studied in this paper due to its criticality towards environment. A standalone application program has been developed to employ ELM based big data analytics tool for the prediction of NO2 pollution emission. The output of ELM is analyzed to ensure the emission at ground level of ASR is maintained within allowable limit.}, location = {Osaka, Japan}, series = {ICBDR 2017}, pages = {23\u201327}, numpages = {5}, keywords = {Extreme Learning Machine, Terrain Profile, Emission Prediction, Meteorology}}
@inproceedings{10.1145/3079452.3079474,title = {(Bio)medical Publications in the Age of Big Data: Yes, They Are Different}, author = {Altena Allard Jan-Jaap van , Delgado Olabarriaga S\u00edlvia },year = {2017}, isbn = {9781450352499}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3079452.3079474}, doi = {10.1145/3079452.3079474}, abstract = {In 2011 the term \"Big Data\" was introduced by Gartner [5], and since then its use in literature has ever increased, also in the (bio)medical research field [1]. Although the term Big Data is widely used, studies show that its meaning is much debated and many different definitions exist [10]. This variety of definitions may lead to different understandings and therefore difficulties in communication. For example, a researcher that is looking for \"Big Data\" solutions might miss an interesting method that is not tagged as such. In previous work we studied major topics that appear in Big Data literature using a Topic Modelling approach [8]. However, from that study it was not possible to know whether those topics are exclusive to publications self-identified as Big Data (BD), or not. Therefore, here we investigate the research question: What are the differences between topics in BD and non-Big Data (NBD) corpora?}, location = {London, United Kingdom}, series = {DH '17}, pages = {221\u2013222}, numpages = {2}, keywords = {topic modelling, biomedical literature, big data}}
@inproceedings{10.1145/2769458.2769485,title = {Enabling Distributed Simulations Using Big Data and Clouds}, author = {Kacsuk Peter },year = {2015}, isbn = {9781450335836}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2769458.2769485}, doi = {10.1145/2769458.2769485}, abstract = {Large scientific simulation projects should enable the collaboration of large scientific consortia where members are located in different countries and even continents storing their usually very large data set in different kind of storages. Therefore state-of-the-art simulations should process very large set of data stored in a distributed way in different kind of storages located in all over the world. As the data is big its processing time can be intolerably long. To reduce processing time we have to use large infrastructure that enables the exploitation of parallel processing wherever it is possible in the simulation process. Clouds provide the required large set of computing resources and hence we need simulation environments that enable the easy exploitation of cloud resources. This keynote speech introduces a cloud-oriented simulation platform that enables the exploitation of large cloud resources as well as accessing all the major data storage types. This platform called as WS-PGRADE/gUSE is intensively used in many EU FP7 projects among them in CloudSME where the main target is to enable particularly small and medium-sized manufacturing and engineering companies (SMEs), to use state of the art simulation technology as a Service (SaaS, one-stop-shop, pay-per-use) in the cloud.In this talk we will show the main features of WS-PGRADE/gUSE that enable the use of cloud and large data resources to conduct distributed simulations. First, the workflow creation and execution mechanism will be explained. Then the DCI Bridge service will be shown that enables the exploitation of many independent cloud resources in parallel. Finally, the Data Avenue service that enables the access and transfer of large data among various types of data storages will be described. These services together enable the creation of simulation workflows that are easily portable among different distributed computing and data infrastructures including various types of clouds and cloud storages. At the end of the talk some concrete examples from the CloudSME project (www.cloudsme.eu) will highlight the main advantages of using the platform.}, location = {London, United Kingdom}, series = {SIGSIM PADS '15}, pages = {125\u2013126}, numpages = {2}, keywords = {simulation, distributed simulation, big data}}
@inproceedings{10.1145/2757384,title = {Proceedings of the 2015 Workshop on Mobile Big Data},year = {2015}, isbn = {9781450335249}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {It is our great pleasure to welcome you to the 2015 ACM Workshop on Mobile Big Data -- Mobidata'15. This workshop aims to foster the exchange of new ideas in the synergy of mobile computing and big data research. We have selected 13 papers to be presented at the workshop. We hope the workshop will facilitate the discussion of the future research directions in mobile big data.}, location = {Hangzhou, China}}
@inproceedings{10.1145/3368756.3369080,title = {Towards an efficient vehicle traffic management using big data}, author = {Tantaoui Mouad , Laanaoui My Driss , Kabil Mustapha },year = {2019}, isbn = {9781450362894}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3368756.3369080}, doi = {10.1145/3368756.3369080}, abstract = {Big data has become essential given the mass of data generated by different domains and which has become impossible to manage by traditional data management tools; vehicular ad-hoc network is one of those areas that use the tools and technologies of data management of big data. In the present paper, we propose a method that aims to detect anomalies in the road and calculate the time spent in each road section in real time, which will allow us to have a base containing the estimated time spent in all sections in real time, it will help us to send to the vehicles the exact estimated time of arrival all along their way. This base will also allow us to detect an accident or anomaly in a section in real time as well.}, location = {Casablanca, Morocco}, series = {SCA '19}, pages = {1\u20136}, numpages = {6}, keywords = {VANET, traffic congestions prediction, big data, traffic management, intelligent transportation systems (ITS)}}
@inproceedings{10.1145/2460625.2460642,title = {From big data to insights: opportunities and challenges for TEI in genomics}, author = {Shaer Orit , Mazalek Ali , Ullmer Brygg , Konkel Miriam },year = {2013}, isbn = {9781450318983}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2460625.2460642}, doi = {10.1145/2460625.2460642}, abstract = {The combination of advanced genomic technologies and computational tools enables researchers to conduct large-scale experiments that answer biological questions in unprecedented ways. However, interaction tools in this area currently remain immature. We propose that tangible, embedded, and embodied interaction (TEI) offers unique opportunities for enhancing discovery and learning in genomics. Also, designing for problems in genomics can help move forward the theory and practice of TEI. We present challenges and key questions for TEI research in genomics, lessons learned from three case studies, and potential areas of focus for TEI research and design.}, location = {Barcelona, Spain}, series = {TEI '13}, pages = {109\u2013116}, numpages = {8}, keywords = {tangible interaction, scientific discovery, big data, learning, interactive tabletops, computational genomics, biology}}
@inproceedings{10.1145/3389250,title = {Big Data Analysis of Internet of Things System}, author = {Lv Zhihan , Singh Amit Kumar },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3389250}, doi = {10.1145/3389250}, abstract = {The study aims at exploring the Internet of things (IoT) system from the perspective of data and further improving the performance of the IoT system. The IoT data energy collection and information transmission system model is constructed by combining IoT and wireless relay cooperative transmission technology. Moreover, the energy efficiency, outage probability (OP), and accuracy of the model are evaluated by simulation experiments. The results show that, in the energy efficiency analysis, with the increase of power split factor \u03c1, the information transmission ability of the system increases. Whereas, the energy collection ability decreases, so the energy efficiency is reduced. Thus, choosing a more suitable power split factor for the energy efficiency of IoT is important. By analyzing OP and bit error rate (BER), as the values of m (Nakagami, the fading index of the fading distribution) and multi-hop paths increase, the OP and BER are reduced while the system performance is increased. Therefore, this article uses wireless relay cooperative transmission technology to integrate big data analysis into the IoT system. Finally, by adding multi-hop path and other methods to reduce the OP and BER of system, the system performance is improved. It provides experimental basis for the development of IoT systems.}, pages = {1\u201315}, numpages = {15}, keywords = {relay cooperation, Internet of things, big data analysis, energy efficiency}}
@inproceedings{10.1145/3443467.3443749,title = {Optimized Privacy Protection Method for Big Data Traffic}, author = {Qiu Yubing , Sun Lan , Wu Yingjie },year = {2020}, isbn = {9781450387811}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3443467.3443749}, doi = {10.1145/3443467.3443749}, abstract = {Data privacy leakages and accuracy declines are major problems in the field of big data. In this work, an adaptive space partition algorithm based on traffic history data under differential privacy is proposed to meet the privacy demands of big data traffic. This paper first uses the counting values of tree nodes in the quad-tree index structure at the first n moments by considering the spatial and temporal characteristics of traffic data and then applies the weighted moving average filter in exponential decay mode to predict the statistical values of the corresponding regions of the data to be released at the next moment. Then, an adaptive quadtree index structure based on the predicted values of each region and the heuristic judgment strategy is established by using the top-down approach and applied to the real data released at the next moment. Finally, we combine the differential privacy tree structure publishing technology and adjust the consistency constraint. Theoretical analyses and simulation experiments show that our algorithm protects data privacy and improves the accuracy of data query compared with existing algorithms.}, location = {Xiamen, China}, series = {EITCE 2020}, pages = {173\u2013179}, numpages = {7}, keywords = {data availability, differential privacy space division, Big data traffic, adaptive quad-tree, privacy protection}}
@inproceedings{10.1145/2967938.2970374,title = {Big Data Analytics on Flash Storage with Accelerators}, author = {Arvind Arvind },year = {2016}, isbn = {9781450341219}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2967938.2970374}, doi = {10.1145/2967938.2970374}, abstract = {Complex analytics of the vast amount of data collected via social media, cell phones, ubiquitous smart sensors, and satellites is likely to be the biggest economic driver for the IT industry over the next decade. For many \"Big Data\" applications, the limiting factor in performance is often the transportation of large amount of data from hard disks to where it can be processed, i.e. DRAM. We will present BlueDBM, an architecture for a scalable distributed flash store which overcomes this limitation by providing a high-performance, high-capacity, scalable random-access flash storage, and by allowing computation near the data via a FPGA-based programmable flash controller. We will present the preliminary results for two applications, (1) key-value store (KVS) and (2) sparse-matrix accelerator for graph processing, on BlueDBM consisting of 20 nodes and 20TB of flash.}, location = {Haifa, Israel}, series = {PACT '16}, pages = {1}, numpages = {1}, keywords = {in-storage computing, hardware accelerators, big data analytics, nand flash storage}}
@inproceedings{10.1145/3508259.3508284,title = {Research on Influencing Factors of Government Audit Big Data Capability}, author = {Sun Yu , Niu Yanfang , Lu Le },year = {2021}, isbn = {9781450384162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3508259.3508284}, doi = {10.1145/3508259.3508284}, abstract = {The widespread application of big data has had a profound impact on social and economic development. Government auditing is the guarantee for the modernization of national governance, and the development of big data audit capability has become the key to improving national governance capability. This paper summarizes the concept of government audit big data capability, and constructs the influencing factor model of government audit big data capability. This study finds that the construction degree of audit big data platform, big data management ability, big data audit technology and auditors' big data technology ability have a significant positive impact on the government audit big data ability, and the audit organization coordination ability plays a positive moderating effect in the whole impact process. This study provides guidance for the improvement and development of government audit big data capability.}, location = {Kyoto, Japan}, series = {AICCC '21}, pages = {172\u2013178}, numpages = {7}, keywords = {Government audit, Influencing factors, Big data analysis capability}}
@inproceedings{10.1145/2623330.2630811,title = {Sampling for big data: a tutorial}, author = {Cormode Graham , Duffield Nick },year = {2014}, isbn = {9781450329569}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2623330.2630811}, doi = {10.1145/2623330.2630811}, abstract = {One response to the proliferation of large datasets has been to develop ingenious ways to throw resources at the problem, using massive fault tolerant storage architectures, parallel and graphical computation models such as MapReduce, Pregel and Giraph. However, not all environments can support this scale of resources, and not all queries need an exact response. This motivates the use of sampling to generate summary datasets that support rapid queries, and prolong the useful life of the data in storage. To be effective, sampling must mediate the tensions between resource constraints, data characteristics, and the required query accuracy. The state-of-the-art in sampling goes far beyond simple uniform selection of elements, to maximize the usefulness of the resulting sample. This tutorial reviews progress in sample design for large datasets, including streaming and graph-structured data. Applications are discussed to sampling network traffic and social networks.}, location = {New York, New York, USA}, series = {KDD '14}, pages = {1975}, numpages = {1}, keywords = {random sampling}}
@inproceedings{10.1145/3236461.3241967,title = {Identifying Critical Issues in Smart City Big Data Project Implementation}, author = {Barham Husam , Daim Tugrul },year = {2018}, isbn = {9781450357869}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3236461.3241967}, doi = {10.1145/3236461.3241967}, abstract = {Many cities across the globe are adopting smart city initiatives, as smart city holds the promise of better quality of life and equity for city's residents, more efficient use of city's infrastructure, and more effective city planning. Big data analytics is the backbone of smart city and the drive engine to achieve smart city's promises. However, statistics indicate that more than 50% of big data projects fail; they either never finish or do not offer the expected value. Resulting in severe consequences as such projects tends to be expensive and require allocating the organization's best resources while doing the project. This is even more crucial in the case of smart city, as cities usually have limited budget and resources.This paper conducted literature review and perspectives analysis to identify challenges, which can cause big data projects to fail, with focus on smart city related big data projects. The goal is to offer a list of challenges, that a project manager can consider as an initial list of risks for the upcoming project, and evaluate the city's readiness against each of them.}, location = {Portland, OR, USA}, series = {SCC '18}, pages = {1\u20139}, numpages = {9}, keywords = {project management, risk and challenges, Big data, smart cities}}
@inproceedings{10.1145/3251403,title = {Session details: Data quality and security}, author = {Serrano Manuel },year = {2009}, isbn = {9781605588162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3251403}, doi = {10.1145/3251403}, location = {Hong Kong, China}, series = {MoSE+DQS '09}, pages = {}}
@inproceedings{10.1145/3528114.3528119,title = {Research on the Impact of Big Data Technology on Management Accounting}, author = {Zhao Yan , Zhang Weidong , Huang Rong },year = {2022}, isbn = {9781450395724}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3528114.3528119}, doi = {10.1145/3528114.3528119}, abstract = {As the application of big data technology continues to expand. The rational use of big data technology in management accounting. Big data technology makes accounting work efficiency significantly improved. Big data technology has promoted the reform and innovation of accounting work mode. Big data technology has become an important part of management accounting. How to use big data technology to dig out valuable information from massive data for enterprise management decisions is particularly important. On the basis of systematically sorting out the concepts related to big data technology and management accounting, this paper expounds the influence mechanism of big data technology on management accounting, constructs the management accounting system framework under the big data analysis technology, and establishes the management accounting application framework under the big data analysis technology. This paper puts forward the implementation ideas and implementation paths of the application of management accounting system under the big data analysis technology, analyzes the mechanism of the influence of big data mining technology on management accounting, explores the application mechanism of big data mining technology in management accounting, and puts forward the strategy of using big data technology to promote the development of management accounting.}, location = {Sanya, China}, series = {DSDE '22}, pages = {26\u201332}, numpages = {7}, keywords = {big data mining technology, big data technology, management accounting, big data analysis technology}}
@inproceedings{10.5555/2757761,title = {Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing},year = {2014}, isbn = {9781479918973}, publisher = {IEEE Computer Society}, address = {USA}}
@inproceedings{10.1145/2640087.2644170,title = {Identity Tracking in Big Data: Preliminary Research Using In-Memory Data Graph Models for Record Linkage and Probabilistic Signature Hashing for Approximate String Matching in Big Health and Human Services Databases}, author = {Jupin Joseph , Shi Justin Y. },year = {2014}, isbn = {9781450328913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2640087.2644170}, doi = {10.1145/2640087.2644170}, abstract = {Our research explores the practice of Record Linkage (RL), also known as Entity Resolution, Record Matching and the Object Identity Problem, in Big health services databases as is commonly practiced within the domain, and some of the approximate string matching methods used for this purpose. We also propose potential improvements to RL and string matching that have been shown in experiments to increase the quality and efficiency for information systems tasked with this problem. We have developed an in-memory graph-based data model, Aggregate Link and Iterative Match (ALIM), which compresses data by eliminating redundancy and stores alias, approximate and phonetic match links between stored data. We have also developed an enhanced edit-distance optimization, the Probabilistic Signature Hash Filter (PSH), which can perform the Damerau-Levenshtein (DL) edit-distance comparison nearly 6000 times faster than DL alone and produce the same exact approximate match results. Our experiments show significant accuracy and performance gains over a system currently in use by a local health department.}, location = {Beijing, China}, series = {BigDataScience '14}, pages = {1\u20138}, numpages = {8}, keywords = {Hashing, Record Linkage, String Matching, Filtering, Signatures, Graph Models, Entity Resolution, Object Identity Problem, Record Matching}}
@inproceedings{10.1145/3147234.3151010,title = {A Big Data Architecture for Near Real-time Traffic Analytics}, author = {Gong Yikai , Rimba Paul , Sinnott Richard },year = {2017}, isbn = {9781450351959}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3147234.3151010}, doi = {10.1145/3147234.3151010}, abstract = {Big data is a popular research topic that has brought about a range of new IT challenges and opportunities. The transport domain is one area that has much to benefit from big data platforms. It requires capabilities for processing voluminous amounts of heterogeneous data that is often created in near real time and at high velocity from a multitude of distributed sensors. It can also require the application of performance-oriented spatial data processing of such data. In this paper, we present a platform (SMASH) that tackles many of the specific challenges raised by the transport domain. We present a range of case studies applying SMASH to transport and other data used to understand traffic phenomenon across the State of Victoria, Australia. The novelty of this work is that this Cloud-based platform is not designed for a specific type of data or for a specific form of data processing. Rather it supports a range of data flavours with a range of data processing possibilities. In particular we show how the platform can be used for analyzing social media data used for traffic jam identification through spatial and temporal clustering tweets on the road network and compare the results with official real-time traffic data based on the Sydney Coordinated Adaptive Traffic System (SCATS - www.scats.com.au) that has been rolled out across Victoria.}, location = {Austin, Texas, USA}, series = {UCC '17 Companion}, pages = {157\u2013162}, numpages = {6}, keywords = {traffic analysis, big data, cloud}}
@inproceedings{10.1145/2481244.2481247,title = {Scaling big data mining infrastructure: the twitter experience}, author = {Lin Jimmy , Ryaboy Dmitriy },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2481244.2481247}, doi = {10.1145/2481244.2481247}, abstract = {The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on \"big data\". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life \"in the trenches\" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall \"big picture\" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as \"plumbing\". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.}, pages = {6\u201319}, numpages = {14}}
@inproceedings{10.1145/2910896.2925466,title = {Big Data Processing of School Shooting Archives}, author = {Farag Mohamed , Nakate Pranav , Fox Edward A. },year = {2016}, isbn = {9781450342292}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2910896.2925466}, doi = {10.1145/2910896.2925466}, abstract = {Web archives about school shootings consist of webpages that may or may not be relevant to the events of interest. There are 3 main goals of this work; first is to clean the webpages, which involves getting rid of the stop words and non-relevant parts of a webpage. The second goal is to select just webpages relevant to the events of interest. The third goal is to upload the cleaned and relevant webpages to Apache Solr so that they are easily accessible. We show the details of all the steps required to achieve these goals. The results show that representative Web archives are noisy, with 2% - 40% relevant content. By cleaning the archives, we aid researchers to focus on relevant content for their analysis.}, location = {Newark, New Jersey, USA}, series = {JCDL '16}, pages = {271\u2013272}, numpages = {2}, keywords = {classification, big data processing, web archives, digital libraries}}
@inproceedings{10.1145/2536714,title = {Proceedings of First International Workshop on Sensing and Big Data Mining},year = {2013}, isbn = {9781450324304}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {We are glad to welcome you to the First International Workshop on Sensing and Big Data Mining (SenseMine 2013). This workshop presents a new forum, established to bring together researchers, practitioners, and academics in the fields of sensor networks, distributed systems, and big data mining and machine learning, with the goal of driving cross-disciplinary research in support of novel emerging applications in our daily lives.}, location = {Roma, Italy}}
@inproceedings{10.1145/3434581.3434720,title = {Research on Big Data Classification Technology Based on Deep Learning}, author = {Zhang Renjie , Ren Chuanrong , Yang Weishu , Wang Yan , Ding Qian },year = {2020}, isbn = {9781450375764}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3434581.3434720}, doi = {10.1145/3434581.3434720}, abstract = {Based on the current digital industry background, the production processes become centralized and closely related to each other. If industrial production fails, it will bring great damage to it, which makes it more and more important to accurately identify the status of industrial equipment and repair it in real time by collecting time series data with industrial sensors. However, due to the large number, variety and high-frequency sampling of sensors, industrial big data has certain complexity, i.e. high spatial dimension, complex logical relationship, changeable rules, large amount of data and so on. At present, end-to-end algorithms of deep learning are widely used in many fields. From the perspective of application of deep learning, this paper studies its application in industrial big data time series classification, and analyzes the characteristics of industrial data and the challenges of industrial time series classification from three aspects: accurate classification, efficient classification and incremental learning.}, location = {Weihai City, China}, series = {ICASIT 2020}, pages = {718\u2013721}, numpages = {4}, keywords = {Deep learning, Big data, Data classification}}
@inproceedings{10.1145/3383464,title = {SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study}, author = {Zeng Xuezhi , Garg Saurabh , Barika Mutaz , Zomaya Albert Y. , Wang Lizhe , Villari Massimo , Chen Dan , Ranjan Rajiv },year = {2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3383464}, doi = {10.1145/3383464}, abstract = {Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.}, pages = {1\u201340}, numpages = {40}, keywords = {SLA, big data analytics application, SLA metrics, service layer, Big data, service level agreement}}
@inproceedings{10.1145/2345316.2345320,title = {On clusterization of \"big data\" streams}, author = {Berkovich Simon , Liao Duoduo },year = {2012}, isbn = {9781450311137}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2345316.2345320}, doi = {10.1145/2345316.2345320}, abstract = {Big Data refers to the rising flood of digital data from many different sources, including the sensors, digitizers, scanners, mobile phones, cameras, software-based tools, internet, and so on. \"Big\" and \"diverse\" are two important characteristics of Big Data. The diversity of the Big Data, such as text, geometry, image, video, or sound, also increases difficulties of big data processing.Coping with the \"Big Data\" problems requires a radical change in the philosophy of the organization of information processing. Primarily, the Big Data approach has to modify the underlying computational model in order to manage the uncertainty in the access to information items in a huge nebulous environment. As a result, the produced outcomes are directly influenced only by some active part of all information items, while the rest of the available information items just indirectly affect the choice of the active part. An analogous functionality exhibits the organization of the brain featuring the unconsciousness, and a characteristic similarity shows the retrieval process in Google.In this talk, we introduce a novel method for on-the-fly clusterization of amorphous data from diverse sources. The devised construction is based on the previously developed FuzzyFind Dictionary reversing the error-correction scheme of Golay Code. This clusterization involves processing of intensive continuous data streams that can be effectively implemented using multi-core pipelining with forced interrupts. The suggested clusterization is especially suitable for the Big Data computational model as it materializes the requirement of purposeful selection of information items in unsteady framework of cloud computing and stream processing. Furthermore, the uncertainties in relation to the considered method of clusterization are moderated due to the idea of the bounded rationality, an approach that does not require a complete exact knowledge for sensible decision-making.}, location = {Washington, D.C., USA}, series = {COM.Geo '12}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/3323878.3325807,title = {Unlocking the potential of nextGen multi-model databases for semantic big data projects}, author = {Holubov\u00e1 Irena , Scherzinger Stefanie },year = {2019}, isbn = {9781450367660}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3323878.3325807}, doi = {10.1145/3323878.3325807}, abstract = {A new vision in semantic big data processing is to create enterprise data hubs, with a 360\u00b0 view on all data that matters to a corporation. As we discuss in this paper, a new generation of multi-model database systems seems a promising architectural choice for building such scalable, non-native triple stores. In this paper, we first characterize this new generation of multi-model databases. Then, discussing an example scenario, we show how they allow for agile and flexible schema management, spanning a large design space for creative and incremental data modelling. We identify the challenge of generating sound triple-views from data stored in several, interlinked models, for SPARQL querying. We regard this as one of several appealing research challenges where the semantic big data and the database architecture community may join forces.}, location = {Amsterdam, Netherlands}, series = {SBD '19}, pages = {1\u20136}, numpages = {6}, keywords = {semantic data management, schema evolution, multi-model DBMS}}
@inproceedings{10.1145/2507157.2508005,title = {Workshop on recommender systems meet big data & semantic technologies: SeRSy 2013}, author = {de Gemmis Marco , Di Noia Tommaso , Lassila Ora , Lops Pasquale , Lukasiewicz Thomas , Semeraro Giovanni },year = {2013}, isbn = {9781450324090}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2507157.2508005}, doi = {10.1145/2507157.2508005}, abstract = {The primary goal of the workshop is to showcase cutting edge research on the intersection of Recommender Systems and Semantic Technologies, by taking the best of the two worlds. This combination may provide the RecSys community with important scenarios where the potential of Semantic Technologies can be effectively exploited into systems performing complex tasks, such as recommendation engines processing Big Data.}, location = {Hong Kong, China}, series = {RecSys '13}, pages = {483\u2013484}, numpages = {2}, keywords = {big data, linked data, recommendation algorithms, semantic technologies}}
@inproceedings{10.1145/1240616.1240623,title = {Utility-driven assessment of data quality}, author = {Even Adir , Shankaranarayanan G. },year = {2007}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1240616.1240623}, doi = {10.1145/1240616.1240623}, abstract = {Data consumers assess quality within specific business contexts or decision tasks. The same data resource may have an acceptable level of quality for some contexts but this quality may be unacceptable for other contexts. However, existing data quality metrics are mostly derived impartially, disconnected from the specific contextual characteristics. This study argues for the need to revise data quality metrics and measurement techniques to incorporate and better reflect contextual assessment. It contributes to that end by developing new metrics for assessing data quality along commonly used dimensions - completeness, validity, accuracy, and currency. The metrics are driven by data utility, a conceptual measure of the business value that is associated with the data within a specific usage context. The suggested data quality measurement framework uses utility as a scaling factor for calculating quality measurements at different levels of data hierarchy. Examples are used to demonstrate the use of utility-driven assessment in real-world data management scenarios and the broader implications for data management are discussed}, pages = {75\u201393}, numpages = {19}, keywords = {utility, data quality, data management, decision making, database, information products, metadata, information value}}
@inproceedings{10.1145/2932707,title = {Computational Health Informatics in the Big Data Age: A Survey}, author = {Fang Ruogu , Pouyanfar Samira , Yang Yimin , Chen Shu-Ching , Iyengar S. S. },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2932707}, doi = {10.1145/2932707}, abstract = {The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.}, pages = {1\u201336}, numpages = {36}, keywords = {Big data analytics, survey, machine learning, data mining, computational health informatics, clinical decision support, 4V challenges}}
@inproceedings{10.1145/3219104.3229288,title = {Building Big Data Processing and Visualization Pipeline through Apache Zeppelin}, author = {Cheng Yanzhe , Liu Fang Cherry , Jing Shan , Xu Weijia , Chau Duen Horng },year = {2018}, isbn = {9781450364461}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3219104.3229288}, doi = {10.1145/3219104.3229288}, abstract = {Big data analytics pipeline becomes popular for large volume data processing, Apache Zeppelin provides an integrated environment for data ingestion, data discovery, data analytics and data visualization and collaboration with an extended framework which allows different programming languages and data processing back ends to be plugged in. The supported languages include Scala, Python, SQL, and Shell script as well as big data processing back ends including Hadoop, Spark and Hive. With the necessary tool sets, an interactive and dynamic data analysis can be done on the fly with heterogeneous programming interfaces. Although Zeppelin is great for code development and interactive analysis with small scale data set for proof-of-concept or use-case presentations, running the data processing pipeline in the batch mode is still needed for performance, robustness to fit in an automated workflow in some cases. We are developing a tool to convert Zeppelin notebook into a workflow with a set of codes that can run in a batch mode through command line interface without requiring running Zeppelin, so that the prototype code can be seamlessly deployed on the production cluster after demo stage. The entire workflow can be preserved, configured manually and run automatically. Zeppelin also provides a flexible way to integrate the visualization functionality, another contribution of this paper is to extend the Zeppelin's existing built-in visualization component for D3Network. With two added features described above, Zeppelin can help users to develop big data pipeline and visualizing graph data quickly and efficiently.}, location = {Pittsburgh, PA, USA}, series = {PEARC '18}, pages = {1\u20137}, numpages = {7}, keywords = {Visualization, Scala, Batch processing, Spark, Big Data}}
@inproceedings{10.1145/2640087,title = {Proceedings of the 2014 International Conference on Big Data Science and Computing},year = {2014}, isbn = {9781450328913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Beijing, China}}
@inproceedings{10.1145/3507473.3507479,title = {3D Structural Characteristics Used Big Data of Shulu Slope Zone}, author = {Hu Shuang , Chen Zaihe , Wei Jingbin , Zhang Lianchao , Li Haitao , Wu Xiaoyu },year = {2021}, isbn = {9781450385213}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3507473.3507479}, doi = {10.1145/3507473.3507479}, abstract = {Cloud processing provides elastic and scalable infrastructure for big data. Shulu Sag is a typical big data processing system, the slope zone of Shulu sag has good oil and gas exploration prospects in recent years. The faults in this area are well developed, the structure is complex, and the structure interpretation scheme has multiple solutions. In order to solve this problem, this paper carried out 3D structural interpretation of Shulu slope area used big data processing, and studied the structural morphology and fault characteristics of the north, central and south members of the slope zone used Cloud processing, so as to provide a basic basis for the subsequent prediction and exploration and development of favorable oil-gas areas.}, location = {Xiamen, China}, series = {ICSED 2021}, pages = {36\u201339}, numpages = {4}, keywords = {Big data, Structure characteristics, Fault characteristics, 3D structural, Cloud processing}}
@inproceedings{10.1145/3286606.3286841,title = {Customer profiling using CEP architecture in a Big Data context}, author = {Elyusufi Z. , Elyusufi Y. , Aitkbir M. },year = {2018}, isbn = {9781450365628}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3286606.3286841}, doi = {10.1145/3286606.3286841}, abstract = {Today Big Data tools are not just a phenomenon of the massive information collection; they are also the best way to approach a customer target. These technologies allow the profiling of the customers of an organization thanks to the histories of purchases, the products that they consult; the data that they share through the social networks. They also make it possible to anticipate the purchase of actions via behavioral analysis. Therefore, the combination of the power of CRM and the performance of BIG DATA tools brings a great added value for customers profile analysis, especially if it is about events triggered in real time. It is in this context that the present work is positioned. Our goal is to intercept events (customer behaviors) and analyze them in real time. We will use the Complex Events Process (CEP) architecture that perfectly meets this need. In order to successfully implement our CEP architecture, we will use the ontology approach.}, location = {Tetouan, Morocco}, series = {SCA '18}, pages = {1\u20136}, numpages = {6}, keywords = {CEP, CRM, Big Data, Profiling, Ontology}}
@inproceedings{10.1145/2566486.2568002,title = {Test-driven evaluation of linked data quality}, author = {Kontokostas Dimitris , Westphal Patrick , Auer S\u00f6ren , Hellmann Sebastian , Lehmann Jens , Cornelissen Roland , Zaveri Amrapali },year = {2014}, isbn = {9781450327442}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2566486.2568002}, doi = {10.1145/2566486.2568002}, abstract = {Linked Open Data (LOD) comprises an unprecedented volume of structured data on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowdsourced or extracted data of often relatively low quality. We present a methodology for test-driven quality assessment of Linked Data, which is inspired by test-driven software development. We argue that vocabularies, ontologies and knowledge bases should be accompanied by a number of test cases, which help to ensure a basic level of quality. We present a methodology for assessing the quality of linked data resources, based on a formalization of bad smells and data quality problems. Our formalization employs SPARQL query templates, which are instantiated into concrete quality test case queries. Based on an extensive survey, we compile a comprehensive library of data quality test case patterns. We perform automatic test case instantiation based on schema constraints or semi-automatically enriched schemata and allow the user to generate specific test case instantiations that are applicable to a schema or dataset. We provide an extensive evaluation of five LOD datasets, manual test case instantiation for five schemas and automatic test case instantiations for all available schemata registered with Linked Open Vocabularies (LOV). One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics.}, location = {Seoul, Korea}, series = {WWW '14}, pages = {747\u2013758}, numpages = {12}, keywords = {linked data, data quality, dbpedia}}
@inproceedings{10.1145/2554688.2554789,title = {Scalable multi-access flash store for big data analytics}, author = {Jun Sang-Woo , Liu Ming , Fleming Kermin Elliott , Arvind },year = {2014}, isbn = {9781450326711}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2554688.2554789}, doi = {10.1145/2554688.2554789}, abstract = {For many \"Big Data\" applications, the limiting factor in performance is often the transportation of large amount of data from hard disks to where it can be processed, i.e. DRAM. In this paper we examine an architecture for a scalable distributed flash store which aims to overcome this limitation in two ways. First, the architecture provides a high-performance, high-capacity, scalable random-access storage. It achieves high-throughput by sharing large numbers of flash chips across a low-latency, chip-to-chip backplane network managed by the flash controllers. The additional latency for remote data access via this network is negligible as compared to flash access time. Second, it permits some computation near the data via a FPGA-based programmable flash controller. The controller is located in the datapath between the storage and the host, and provides hardware acceleration for applications without any additional latency. We have constructed a small-scale prototype whose network bandwidth scales directly with the number of nodes, and where average latency for user software to access flash store is less than 70mus, including 3.5mus of network overhead.}, location = {Monterey, California, USA}, series = {FPGA '14}, pages = {55\u201364}, numpages = {10}, keywords = {big data, storage system, ssd, fpga networks, flash}}
@inproceedings{10.1145/2539150.2539224,title = {Big Data in Large Scale Intelligent Smart City Installations}, author = {Girtelschmid Sylva , Steinbauer Matthias , Kumar Vikash , Fensel Anna , Kotsis Gabriele },year = {2013}, isbn = {9781450321136}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2539150.2539224}, doi = {10.1145/2539150.2539224}, abstract = {This paper highlights how the domain of Smart Cities is often modeled by ontologies to create applications and services that are highly flexible, (re)configurable, and inter-operable. However, ontology repositories and their accompanying reasoning and rule languages face the disadvantage of bad runtime behavior, especially if the models grow large in size. We propose an architecture that uses tools and methods from the domain of Big Data processing in conjunction with an ontology repository and a rule engine to overcome potential performance bottlenecks that will occur in this scenario.}, location = {Vienna, Austria}, series = {IIWAS '13}, pages = {428\u2013432}, numpages = {5}, keywords = {Big Data, Ontology, Smart City, Semantic Modeling, Energy Efficiency, Real-time Streaming}}
@inproceedings{10.1145/3090354.3090373,title = {Comparative Study: Dependability of Big Data in the Cloud}, author = {Mdarbi Fatima Ezzahra , Afifi Nadia , Hilal Imane },year = {2017}, isbn = {9781450348522}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3090354.3090373}, doi = {10.1145/3090354.3090373}, abstract = {Cloud Computing provides computing resources remotely and on demand. These resources can be infrastructures, platforms, or application software. Whereas Big Data is well known for needing high capacity of both storage and computation's data streams. Thus, dependability must be studied in order to determine the attitude of Cloud systems to complete the features required by Big Data. The objective of this article is to deal with the general context of the Dependability and Cloud environment for Big Data. Thus, we carried out a statistic study of the research work that have been conducted in this context. The state of the art will allow us to approach the problematic of the dependability of Big Data in the Cloud. Our first objective is to know if there are many papers that deal at the same time a three thematics: the Cloud, the Big Data and the Dependability}, location = {Tetouan, Morocco}, series = {BDCA'17}, pages = {1\u20136}, numpages = {6}, keywords = {Volume, Velocity, Dependability, IaaS, Value, Big Data, Variety, Cloud Computing, PaaS, SaaS, Veracity}}
@inproceedings{10.1145/3402569.3409041,title = {Research on Foreign Exchange Management Model Based on Big Data}, author = {Han Ping },year = {2020}, isbn = {9781450377546}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3402569.3409041}, doi = {10.1145/3402569.3409041}, abstract = {The rapid development of Internet technology has promoted the process of economic globalization, and the application of big data technology has injected new vitality into the innovation of foreign exchange management models. Businesses such as foreign exchange deposits and loans and foreign currency exchange have encountered new development opportunities. Under the effect of big data technology, it can quickly process massive amounts of information, respond to various exchange rate changes, and achieve the improvement of foreign exchange business management. Especially under the circumstances of the current RMB marketization, exchange rate reform and the diversification of the international situation, the difficulty of foreign exchange management is gradually increasing. How to better improve the efficiency of foreign exchange management has become a problem that must be solved at present. Therefore, it is of great significance to explore the foreign exchange management model based on the background of big data, build a big data computing mechanism, give full play to its advantages in foreign exchange management, and promote the improvement of foreign exchange management.}, location = {Beijing, China}, series = {ICDEL 2020}, pages = {162\u2013165}, numpages = {4}, keywords = {foreign exchange management, mode, Big data background}}
@inproceedings{10.5555/3213069.3213074,title = {Remote high performance visualization of big data for immersive science}, author = {Abidi Faiz , Polys Nicholas , Rajamohan Srijith , Arsenault Lance , Mohammed Ayat },year = {2018}, isbn = {9781510860162}, publisher = {Society for Computer Simulation International}, address = {San Diego, CA, USA}, abstract = {Remote visualization has emerged as a necessary tool in the analysis of big data. High-performance computing clusters can provide several benefits in scaling to larger data sizes, from parallel file systems to larger RAM profiles to parallel computation among many CPUs and GPUs. For scalable data visualization, remote visualization tools and infrastructure is critical where only pixels and interaction events are sent over the network instead of the data. In this paper, we present our pipeline using VirtualGL, TurboVNC, and ParaView to render over 40 million points using remote HPC clusters and project over 26 million pixels in a CAVE-style system. We benchmark the system by varying the video stream compression parameters supported by TurboVNC and establish some best practices for typical usage scenarios. This work will help research scientists and academicians in scaling their big data visualizations for remote, real-time interaction.}, location = {Baltimore, Maryland}, series = {HPC '18}, pages = {1\u201312}, numpages = {12}, keywords = {HPC, remote rendering, big data, CAVE, paraview}}
@inproceedings{10.1145/3264560.3264562,title = {Fuzzy Cross Language Plagiarism Detection (Arabic-English) using WordNet in a Big Data environment}, author = {Ezzikouri Hanane , Oukessou Mohamed , Youness Madani , Erritali Mohamed },year = {2018}, isbn = {9781450364744}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3264560.3264562}, doi = {10.1145/3264560.3264562}, abstract = {Cross-Language Plagiarism refers to the unacknowledged reuse of a text involving its translation from one natural language to another without proper referencing to the original source. One of the common problems in data processing is efficient large-scale text comparison, especially semantic based similarity due to the increase in the number of publications and the rate of suspicious documents sources of plagiarism. CLPD nature could be more complicated than simple copy+translate and paste, thus the detecting process exposes the need for a vague concept and fuzzy sets techniques in a big data environment to reveal dishonest practices in Arabic documents. In this paper, we propose a new Cross-Language Plagiarism Detection based on fuzzy-semantic similarity using WordNet and two semantic approaches Wu&Palmer and Lin; the work is done in a parallel way using Apache Hadoop with its distributed file system HDFS and the MapReduce programming model. The experimental results show that the Fuzzy Wu & Palmer have high performance than Fuzzy Lin.}, location = {Barcelona, Spain}, series = {ICCBDC'18}, pages = {22\u201327}, numpages = {6}, keywords = {Fuzzy sets, CLPD, Hadoop, HDFS, Semantic Similarity, MapReduce}}
@inproceedings{10.1145/3056662.3056667,title = {Performance and energy efficiency of big data systems: characterization, implication and improvement}, author = {Shi Yingjie , Wang Lei , Du Fang },year = {2017}, isbn = {9781450348577}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3056662.3056667}, doi = {10.1145/3056662.3056667}, abstract = {Large volume of data is produced by various applications in the world, processing such scale of data has great challenges in not only performance but also energy efficiency. Researchers propose various techniques to either improve the performance or the energy efficiency. The techniques of these two trends, however, are significantly different. When both performance and energy efficiency are concerned in the big data systems, how to get balance has become an issuing and challenging problem for data center administrators and hardware designers. In this paper, we conduct comprehensive evaluations on two representative platforms with different types of processors. We quantify the performance and energy efficiency, relating the evaluation results to micro-architectural activities and application characteristics. Two interesting findings are made from our evaluations: (1) the performance and energy efficiency are not only determined by the hardware technology, but also associated with the application characteristics; (2) there is no ever-victorious microprocessor in terms of both performance and energy efficiency in all the big data workloads. Based on the findings and quantified evaluation results, we provide great guidance and implications for both data center administrators and big data system designers, and we argue that a hybrid-core is an efficient way to improve the energy efficiency of big data systems with minimum performance degradation.}, location = {Bangkok, Thailand}, series = {ICSCA '17}, pages = {55\u201361}, numpages = {7}, keywords = {performance, hybrid core, energy efficiency, big data systems}}
@inproceedings{10.1145/2744680.2744687,title = {Generating Actionable Knowledge from Big Data}, author = {Fang Xiu Susie },year = {2015}, isbn = {9781450335294}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2744680.2744687}, doi = {10.1145/2744680.2744687}, abstract = {The last few years have seen a rapid increase of sheer amount of data produced and communicated over the Internet and the Web. While it is widely believed that the availability of such ``Big Data'' holds the potential to revolutionize many aspects of our modern society (e.g., intelligent transportation, environmental monitoring, and energy saving), many challenges need to be addressed before this potential can be realized. This PhD project focuses on one critical challenge, namely extracting actionable knowledge from Big Data. Tremendous efforts have been contributed on mining large-scale data on the Web and constructing comprehensive knowledge bases (KBs). However, existing knowledge extraction systems retrieve data from limited types of Web sources. In addition, data fusion approaches consider very little of the noises produced by those knowledge extraction systems. Consequently, the constructed KBs are far from being comprehensive and accurate. In this paper, we present our initial design of a framework for extracting machine-readable data with high precision and recall from four types of data sources, namely Web texts, Document Object Model (DOM) trees, existing KBs, and query stream. Confidence scores are attached to the resulting knowledge, which can be used to further improve the knowledge fusion results.}, location = {Melbourne, Victoria, Australia}, series = {SIGMOD '15 PhD Symposium}, pages = {3\u20138}, numpages = {6}, keywords = {knowledge fusion, dom tree, knowledge base}}