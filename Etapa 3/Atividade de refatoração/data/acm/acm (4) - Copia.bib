@inproceedings{10.1145/3149572.3149575,
author = {Francisco, Maritza M. C. and Alves-Souza, Solange N. and Campos, Edit G. L. and De Souza, Luiz S.},
title = {Total Data Quality Management and Total Information Quality Management Applied to Costumer Relationship Management},
year = {2017},
isbn = {9781450353373},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149572.3149575},
doi = {10.1145/3149572.3149575},
abstract = {Data quality (DQ) is an important issue for modern organizations, mainly for decision-making based on information, using solutions such as CRM, Business Analytics, and Big Data. In order to obtain quality data, it is necessary to implement methods, processes, and specific techniques that handle information as a product, with well established, controlled, and managed production processes. The literature provides several types of quality data management methodologies that treat structured data, and few treating semi- and non-structured data. Choosing the methodology to be adopted is one the major issues faced by organizations, when challenged to treat the data quality in a systematic manner. This paper makes a comparative analysis between TDQM -- Total Data Quality Management and TIQM -- Total Information Quality Management approaches, focusing on data quality problems in the context of a CRM -- Costumer Relationship Management application. Such analysis identifies the strengths and weaknesses of each methodology and suggests the most suitable for the CRM scenario.},
booktitle = {Proceedings of the 9th International Conference on Information Management and Engineering},
pages = {40–45},
numpages = {6},
keywords = {Data quality, data quality methodology, data quality problems, data quality dimensions, data quality management},
location = {Barcelona, Spain},
series = {ICIME 2017}
}

@inproceedings{10.5555/3320516.3320569,
author = {Newton, David and Pasupathy, Raghu and Yousefian, Farzad},
title = {Recent Trends in Stochastic Gradient Descent for Machine Learning and Big Data},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Stochastic Gradient Descent (SGD), also known as stochastic approximation, refers to certain simple iterative structures used for solving stochastic optimization and root finding problems. The identifying feature of SGD is that, much like in gradient descent for deterministic optimization, each successive iterate in the recursion is determined by adding an appropriately scaled gradient estimate to the prior iterate. Owing to several factors, SGD has become the leading method to solve optimization problems arising within large-scale machine learning and "big data" contexts such as classification and regression. This tutorial covers the basics of SGD with an emphasis on modern developments. The tutorial starts with examples where SGD is applicable, and then details important flavors of SGD and reported complexity calculations.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {366–380},
numpages = {15},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@proceedings{10.1145/3378904,
title = {BDET 2020: Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology},
year = {2020},
isbn = {9781450376839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Big data is an emerging paradigm applied to datasets whose size is beyond the ability of commonly used software tools to capture, manage, and process the data within a tolerable elapsed time. Such datasets are often from various sources (Variety) yet unstructured such as social media, sensors, scientific applications, surveillance, video and image archives, Internet texts and documents, Internet search indexing, medical records, business transactions and web logs; and are of large size (Volume) with fast data in/out (Velocity). 2020 2nd International Conference on Big Data Engineering and Technology is one of the premier events to network and learn from colleagues and other leading international scientific voices from across the world, who is actively engaged in advancing research and raising awareness of the many challenges in the diverse field of Big Data Engineering and Technology.},
location = {Singapore, China}
}

@inproceedings{10.1145/3376897.3379162,
author = {Shi, Laixi and Zhang, Yue and Pan, Shijia and Chi, Yuejie},
title = {Data Quality-Informed Multiple Occupant Localization Using Floor Vibration Sensing},
year = {2020},
isbn = {9781450371162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3376897.3379162},
doi = {10.1145/3376897.3379162},
abstract = {Floor vibration-based sensing provides an alternative approach for multiple occupant localization, enabling various smart building applications such as elderly care. Prior work mainly focuses on detecting onsets of individual footstep from overlapping signals for localization. However, the error rate is higher than that of single footsteps. In this work, we present a data quality-informed time-sequence approach for accurate multi-people localization. The intuition is that when signals overlap, the overlapping part of the signal has a lower SNR, which can be quantified and used as estimation confidence. We conducted real-world experiments to validate our data quality-informed approach for location estimation.},
booktitle = {Proceedings of the 21st International Workshop on Mobile Computing Systems and Applications},
pages = {98},
numpages = {1},
keywords = {data quality quantification, indoor localization, multiple pedestrian localization, signal overlapping, ambient vibration sensing, tdoa},
location = {Austin, TX, USA},
series = {HotMobile '20}
}

@inproceedings{10.1145/3407703.3407711,
author = {Wenxia, Ding and Heping, Li},
title = {Correlation Analysis of Factors Affecting Bridge Health under the Background of Big Data},
year = {2020},
isbn = {9781450377270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407703.3407711},
doi = {10.1145/3407703.3407711},
abstract = {Once the bridge is put into use, in addition to its own material aging, it will also receive damage from human factors such as vehicles, wind, earthquakes, fatigue, and overload. This damage has more or less reduced the service life of the bridge or even destroyed the safety performance of the bridge, causing huge losses to people's lives and property. This requires us to pay attention to the safety, reliability and durability of the bridge at all times during the construction of the bridge and in the later maintenance process. The traditional bridge monitoring work and operation and maintenance have a low degree of automation, bridge condition assessment, and bridge real-time monitoring and comprehensive information management is difficult. This requires real-time monitoring of this information in the context of big data to ensure the healthy use of the bridge. In this paper, a finite element analysis model is established to monitor the sensor network of the bridge and the data detected by the sensor in the context of big data. The optimization analysis results in an optimized layout plan of the identifiable static sensors, taking into account both the economic and structural operating conditions of the bridge.},
booktitle = {Proceedings of the 2020 Artificial Intelligence and Complex Systems Conference},
pages = {34–38},
numpages = {5},
keywords = {reliability, condition evaluation, bridge health, real-time monitoring, Big data, finite element analysis},
location = {Wuhan, China},
series = {AICSconf '20}
}

@article{10.1145/2909493,
author = {Date, Sachin},
title = {Should You Upload or Ship Big Data to the Cloud?},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2909493},
doi = {10.1145/2909493},
abstract = {The accepted wisdom does not always hold true.},
journal = {Commun. ACM},
month = {jun},
pages = {44–51},
numpages = {8}
}

@inproceedings{10.1145/3167132.3167447,
author = {Din, Sadia},
title = {Human Behavior Analysis Based on Big Data Analytics in Cyber-Physical System: Student Research Abstract},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167447},
doi = {10.1145/3167132.3167447},
abstract = {The growing gap between users and the Big Data analytics requires innovative tools that address the challenges faced by big data volume, variety, and velocity. Therefore, it becomes computationally inefficient to analyze such massive volume of data. Moreover, advancements in the field of Big Data application and data science leads toward a new paradigm of human behavior, where various smart devices integrate with each other and establish a relationship. However, majority of the systems are either memoryless or computational inefficient, which are unable to define or predict human behavior. Therefore, keeping in view the aforementioned needs, there is a requirement for a system that can efficiently analyze a stream of Big Data within their requirements. Hence, this paper presents a system architecture that integrates social network with the technical network. We derive a novel notion of 'Smart Socio Network', where a friendship is made based on the geo-location information of the user, and trust index is used based on graphs theory. The proposed graph theory provides a better understanding of extraction knowledge from the data and finding relationship between different users.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {673–674},
numpages = {2},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3338906.3338953,
author = {Gulzar, Muhammad Ali and Mardani, Shaghayegh and Musuvathi, Madanlal and Kim, Miryung},
title = {White-Box Testing of Big Data Analytics with Complex User-Defined Functions},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338953},
doi = {10.1145/3338906.3338953},
abstract = {Data-intensive scalable computing (DISC) systems such as Google’s MapReduce, Apache Hadoop, and Apache Spark are being leveraged to process massive quantities of data in the cloud. Modern DISC applications pose new challenges in exhaustive, automatic testing because they consist of dataflow operators, and complex user-defined functions (UDF) are prevalent unlike SQL queries. We design a new white-box testing approach, called BigTest to reason about the internal semantics of UDFs in tandem with the equivalence classes created by each dataflow and relational operator. Our evaluation shows that, despite ultra-large scale input data size, real world DISC applications are often significantly skewed and inadequate in terms of test coverage, leaving 34% of Joint Dataflow and UDF (JDU) paths untested. BigTest shows the potential to minimize data size for local testing by 10^5 to 10^8 orders of magnitude while revealing 2X more manually-injected faults than the previous approach. Our experiment shows that only few of the data records (order of tens) are actually required to achieve the same JDU coverage as the entire production data. The reduction in test data also provides CPU time saving of 194X on average, demonstrating that interactive and fast local testing is feasible for big data analytics, obviating the need to test applications on huge production data.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {290–301},
numpages = {12},
keywords = {dataflow programs, symbolic execution, map reduce, data intensive scalable computing, test generation},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3018009.3018040,
author = {Xu, Gang and Wu, Shunyu and Xie, Pengfei},
title = {Integration and Exchange Method of Multi-Source Heterogeneous Big Data for Intelligent Power Distribution and Utilization},
year = {2016},
isbn = {9781450348195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018009.3018040},
doi = {10.1145/3018009.3018040},
abstract = {With the development of smart grid and big data technologies, the stability and economy of distribution network operation are enhanced effectively. Intelligent power distribution and utilization (IPDU) big data platform, which exchanges operation data with other related distribution network management systems, makes decisions for demand side management, power system and distributed energy operation strategies by analyzing the big data. In order to solve the data fusion and exchange problems among all information systems, we proposed a kind of general information model for multi-source heterogeneous big data. In addition, a data fusion and exchange mechanism is established based on circle buffer to ensure the data quality. Finally, this paper demonstrates the effective of the method of IPDU big data fusion method by the example of distribution network reconfiguration. The method proposed in this paper can satisfy the data exchanging demands of future smart grid and demand side management, and it also has good confluent and extensible feature.},
booktitle = {Proceedings of the 2nd International Conference on Communication and Information Processing},
pages = {38–42},
numpages = {5},
keywords = {intelligent power distribution and utilization, data fusion and exchange, multi-source and heterogeneous, information model},
location = {Singapore, Singapore},
series = {ICCIP '16}
}

@inproceedings{10.1145/3349567.3351720,
author = {Zhan, Jinyu and Li, Ying and Jiang, Wei and Wu, Junting and Zhu, Jianping},
title = {An Improved Network Interface Card with Query Filter for Big Data Systems: Work-in-Progress},
year = {2019},
isbn = {9781450369237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349567.3351720},
doi = {10.1145/3349567.3351720},
abstract = {In this paper we approach to accelerate the data processing of storage and computing separated big data systems. We propose an improved Network Interface Card with Query Filter (NIC-QF), implemented by FPGA on storage nodes, to accelerate the data queries, which can also reduce the workload and communication overhead on computing nodes. NIC-QF is designed with query filtering accelerator and Network Interface Card (NIC) communicator, which can filter the original data on storage nodes as an implicit coprocessor and directly send the filtered data to computing nodes of big data systems. Filter units in NIC-QF can perform multiple SQL tasks in parallel, and each filter unit is internally pipelined, which can further speed up the data processing. Experiments with two benchmarks demonstrate the efficiency of our approach, which can achieve average up to 65.56% faster than the traditional approach.},
booktitle = {Proceedings of the International Conference on Hardware/Software Codesign and System Synthesis Companion},
articleno = {19},
numpages = {2},
keywords = {FPGA, storage and computing separated big data systems, query filter, network interface card},
location = {New York, New York},
series = {CODES/ISSS '19}
}

@inproceedings{10.1145/3404555.3404601,
author = {Deng, Jianzhi and Zhou, Yuehan and Cheng, Xiaohui and Li, Tianyu and Qin, Chuling},
title = {MRNA Big Data Analysis of Hepatoma Carcinoma Between Different Genders},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404601},
doi = {10.1145/3404555.3404601},
abstract = {In this paper, we did the researches of the directly related differentially expression mRNAs (DEmRNAs) and their gene ontology (GO), Kyoto Encyclopedia of Genes and Genomes (KEGG) signal pathway, COX model and survival analysis. For the purpose, the 87 directly related DEmRNAs (DRmRNAs) to the hepatoma carcinoma illness were selected from the intersectional DEmRNAs of normal-tumor sample matrix and male-female tumor's sample matrix. By the analysis of online databases, DAVID, KOBAS and KEGG, DRmRNAs were enriched in 18 biological process (BP), 5 cellular component (CC), 9 molecular function (MF) and 3 signal pathways (hsa04974, hsa04972 and hsa04080). The co-expression DRmRNAs were analyzed by using the COX model. CHGA was regard as a potential biomarker of hepatoma carcinoma by the proof of survival kmplot analysis and ROC curve analysis.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {84–88},
numpages = {5},
keywords = {DEmRNA, gender difference, CHGA, TCGA, hepatoma carcinoma},
location = {Tianjin, China},
series = {ICCAI '20}
}

@inproceedings{10.1145/3411564.3411612,
author = {Souza, Thiago Vieira de and Farias, Kleinner and Bischoff, Vinicius},
title = {Big Data Analytics Applied in Supply Chain Management: A Systematic Mapping Study},
year = {2020},
isbn = {9781450388733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411564.3411612},
doi = {10.1145/3411564.3411612},
abstract = {In recent years, the capacity of big data analytics (BDA) has attracted the significant attention of researchers linked to academia and industry professionals. This capacity is related to the possibility of managing informations advanced to reach its supply chain. In other words, information technology uses integrated systems, which facilitates innovation and the diffusion of knowledge throughout this supply chain. However, researchers and professionals still need to explore the capacity potential of BDA, in order to improve supply chain operational decision-making skills. This work classifies the state-of-the-art literature that applied BDA to the supply chain management (SCM). A Systematic Mapping Study was elaborated based on literature guidelines. A total of 50 primary studies were selected through a filtering process from initially 5,437 studies. These primary studies were used to answer the six research questions. The result of the classification showed that 64% of the studies are related to supply-chain management; most of the studies carried out empirical research; and approximately 50% of the primary studies investigated models for optimization process. This research provides to academics and industry professionals the gaps and future challenges related to BDA for SCM.},
booktitle = {XVI Brazilian Symposium on Information Systems},
articleno = {14},
numpages = {8},
keywords = {Data Analysis, Information Systems, Supply-Chain, Big Data Analytics},
location = {S\~{a}o Bernardo do Campo, Brazil},
series = {SBSI'20}
}

@article{10.1145/3110218,
author = {Zhang, Qingchen and Yang, Laurence T. and Chen, Zhikui and Li, Peng},
title = {Dependable Deep Computation Model for Feature Learning on Big Data in Cyber-Physical Systems},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3110218},
doi = {10.1145/3110218},
abstract = {With the ongoing development of sensor devices and network techniques, big data are being generated from the cyber-physical systems. Because of sensor equipment occasional failure and network transmission unreliability, a large number of low-quality data, such as noisy data and incomplete data, is collected from the cyber-physical systems. Low-quality data pose a remarkable challenge on deep learning models for big data feature learning. As a novel deep learning model, the deep computation model achieves superior performance for big data feature learning. However, it is difficult for the deep computation model to learn dependable features for low-quality data, since it uses the nonlinear function as the encoder. In this article, a dependable deep computation model is proposed for feature learning on low-quality big data in cyber-physical systems. Specially, a regularity is added into the objective function of the deep computation model to obtain reliable features in the intermediate-level representation space. Furthermore, a learning algorithm based on the back-propagation strategy is devised to train the parameters of the proposed model. Finally, experiments are conducted on three representative datasets and a real dataset to evaluate the effectiveness of the dependable deep computation model for low-quality big data feature learning. Results show that the proposed model achieves a remarkable result for the tasks of classification, restoration, and prediction, proving the potential of this work for practical applications in cyber-physical systems.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {sep},
articleno = {11},
numpages = {17},
keywords = {back-propagation algorithm, feature learning, big data, Cyber-physical systems, dependable deep computation model}
}

@inproceedings{10.1145/3379247.3379270,
author = {Yang, Lan and Chiang, Jason Amaro},
title = {Use Case and Performance Analyses for Missing Data Imputation Methods in Big Data Analytics},
year = {2020},
isbn = {9781450376730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379247.3379270},
doi = {10.1145/3379247.3379270},
abstract = {In big data analytics the phenomenon of missing data is universal due to reasons such as faulty equipment and nonresponses in surveys. Imputation is the process of replacing missing data with substituted values. Proper imputation could greatly improve the accuracy and effectiveness of big data analytics.In this paper, we analyze a rich set of deletion and imputation methods, focusing on strengths, weaknesses, best use cases, implementation strategies, and error-examination based performance analysis. Our goal is to find the best fitted imputation method(s) for each given use case.},
booktitle = {Proceedings of 2020 the 6th International Conference on Computing and Data Engineering},
pages = {107–111},
numpages = {5},
keywords = {Big data analytics, error estimation, missing data, imputation},
location = {Sanya, China},
series = {ICCDE 2020}
}

@inproceedings{10.1145/3341162.3349334,
author = {Wang, Chaofan and Sarsenbayeva, Zhanna and Luo, Chu and Goncalves, Jorge and Kostakos, Vassilis},
title = {Improving Wearable Sensor Data Quality Using Context Markers},
year = {2019},
isbn = {9781450368698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341162.3349334},
doi = {10.1145/3341162.3349334},
abstract = {A major challenge in human activity recognition over long periods with multiple sensors is clock synchronization of independent data streams. Poor clock synchronization can lead to poor data and classifiers. In this paper, we propose a hybrid synchronization approach that combines NTP (Network Time Protocol) and context markers. Our evaluation shows that our approach significantly reduces synchronization error (20 ms) when compared to approaches that rely solely on NTP or sensor events. Our proposed approach can be applied to any wearable sensor where an independent sensor stream requires synchronization.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {598–601},
numpages = {4},
keywords = {clock synchronization, clock drifts, wearable sensors},
location = {London, United Kingdom},
series = {UbiComp/ISWC '19 Adjunct}
}

@inproceedings{10.1145/3159652.3160602,
author = {Teng, Shang-Hua},
title = {Scalable Algorithms in the Age of Big Data and Network Sciences: Characterization, Primitives, and Techniques},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3160602},
doi = {10.1145/3159652.3160602},
abstract = {In the age of network sciences and machine learning, efficient algorithms are now in higher demand more than ever before. Big Data fundamentally challenges the classical notion of efficient algorithms: Algorithms that used to be considered efficient, according to polynomial-time characterization, may no longer be adequate for solving today»s problems. It is not just desirable, but essential, that efficient algorithms should be scalable. In other words, their complexity should be nearly linear or sub-linear with respect to the problem size. Thus, scalability, not just polynomial-time computability, should be elevated as the central complexity notion for characterizing efficient computation. In this talk, I will highlight a family of fundamental algorithmic techniques for designing provably-good scalable algorithms: (1) scalable primitives and scalable reduction, (2) spectral approximation of graphs and matrices, (3) sparsification by multilevel structures, (4) advanced sampling, (5) local network exploration. For the first, I will focus on the emerging Laplacian Paradigm, that has led to breakthroughs in scalable algorithms for several fundamental problems in network analysis, machine learning, and scientific computing. I will then illustrate these algorithmic techniques with four recent applications: (1) sampling from graphic models, (2) network centrality approximation, (3) social-influence analysis (4) local clustering. Mathematical and algorithmic solution to these problems exemplify the fusion of combinatorial, numerical, and statistical thinking in data and network analysis.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {6–7},
numpages = {2},
keywords = {scalable algorithms, big data, machine learning, advanced sampling, graph sparsification, network sciences, local algorithms},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@inproceedings{10.1145/3291801.3291833,
author = {Shaorong, He and Zhifeng, Xie and Jianbo, Huang},
title = {Visual Analysis of Big Data Based on Movies Released in China},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291833},
doi = {10.1145/3291801.3291833},
abstract = {China film market has become the second largest box office market just after North America today, the Chinese film market is less than 1/5 size of the now, in 10 years ago. The Chinese film market is growing so fast, the reasons research on this phenomenon become a hot spot. In this article, we tried to find the rule of films box office market based on the historical data of China released film. Gaining film market rules will help film-makers understand audience preferences and make decisions on future movie projects. We collected the movies data released in China in recent 15 years which contains box office, and basic information (such as release date, language, type, country, etc.), correspondingly, we also collected the movie audience's com-ments on the internet, more than 100000 comments in total, There is no public movies released in China set provided by a inde-pendent agency until now, According to the obtained data, we analyzed the movies data distribution rules in the month, year, genre, using language, We also visualized the proportion of all films in the rating (from douban.com), and analyzed collectively the quality of domestic released films. We conducted word cloud processing on the comments of the highest box office movie Wolf warriors 2, which demonstrate intuitively the public's discussion on this movie.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {80–85},
numpages = {6},
keywords = {data analysis, visual analysis, Movies released in China},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/2534921.2534926,
author = {Baumann, Peter and Dumitru, Alex and Merticariu, Vlad and Misev, Dimitar and Rusu, Mihaela},
title = {Breaking the Big Data Barrier by Enhancing On-Board Sensor Flexibility},
year = {2013},
isbn = {9781450325349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2534921.2534926},
doi = {10.1145/2534921.2534926},
abstract = {Modern sensors, such as hyperspectral cameras, deliver massive amounts of data. On board of satellites, the high volume is paired with low bandwidth and part-time availability, during overpasses. This leads to well-known availability problems and bottlenecks in today's remote sensing.We address this challenge by enhancing the on-board system with flexible filtering and processing capabilities based on the Array Analytics engine, rasdaman. Users then can exact request, which can lead to substantially decreased data traffic. Our project has been accepted for a CubeSat mission for which rasdaman now has been prepared. We present the project setup and core extensions done to rasdaman to this end.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {32–36},
numpages = {5},
keywords = {rasdaman, on-board intelligence, big data, array databases},
location = {Orlando, Florida},
series = {BigSpatial '13}
}

@inproceedings{10.1145/3297280.3297474,
author = {Gonzalez-Aparicio, Maria Teresa and Younas, Muhammad and Tuya, Javier and Casado, Rub\'{e}n},
title = {Evaluation of ACE Properties of Traditional SQL and NoSQL Big Data Systems},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297474},
doi = {10.1145/3297280.3297474},
abstract = {Traditional SQL and NoSQL big data systems are the backbone for managing data in cloud, fog and edge computing. This paper develops a new system and adopts the TPC-DS industry standard benchmark in order to evaluate three key properties, availability, consistency and efficiency (ACE) of SQL and NoSQL systems. The contributions of this work are manifold. It evaluates and analyses the tradeoff between the ACE properties. It provides insight into the NoSQL systems and how they can be improved to be sustainable for a more wide range of applications. The evaluation shows that SQL provides stronger consistency, but at the expense of low efficiency and availability. NoSQL provides better efficiency and availability but lacks support for stronger consistency. In order for NoSQL systems to be more sustainable they need to implement transactional schemes that enforce stronger consistency as well as better efficiency and availability.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1988–1995},
numpages = {8},
keywords = {SQL, Riak, big data, TPC-DS, data consistency, NoSQL},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3102254.3102272,
author = {Weichselbraun, Albert and Kuntschik, Philipp},
title = {Mitigating Linked Data Quality Issues in Knowledge-Intense Information Extraction Methods},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102272},
doi = {10.1145/3102254.3102272},
abstract = {Advances in research areas such as named entity linking and sentiment analysis have triggered the emergence of knowledge-intensive information extraction methods that combine classical information extraction with background knowledge from the Web. Despite data quality concerns, linked data sources such as DBpedia, GeoNames and Wikidata which encode facts in a standardized structured format are particularly attractive for such applications.This paper addresses the problem of data quality by introducing a framework that elaborates on linked data quality issues relevant to different stages of the background knowledge acquisition process, their impact on information extraction performance and applicable mitigation strategies. Applying this framework to named entity linking and data enrichment demonstrates the potential of the introduced mitigation strategies to lessen the impact of different kinds of data quality problems. An industrial use case that aims at the automatic generation of image metadata from image descriptions illustrates the successful deployment of knowledge-intensive information extraction in real-world applications and constraints introduced by data quality concerns.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {17},
numpages = {12},
keywords = {semantic technologies, named entity linking, mitigation strategies, information extraction, linked data quality, applications},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3018896.3018913,
author = {Chaoui, Habiba and Makdoun, Ibtissam},
title = {A New Secure Model for the Use of Cloud Computing in Big Data Analytics},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018913},
doi = {10.1145/3018896.3018913},
abstract = {When Big data and cloud computing join forces together, several domains like: healthcare, disaster prediction and decision making become easier and much more beneficial to users in term of information gathering, although cloud computing will reduce time and cost of analyzing information for big data, it may harm the confidentiality and integrity of the sensitive data, for instance, in healthcare, when analyzing disease's spreading area, the name of the infected people must remain secure, hence the obligation to adopt a secure model that protect sensitive data from malicious users. Several case studies on the integration of big data in cloud computing, urge on how easier it would be to analyze and manage big data in this complex envronement. Companies must consider outsourcing their sensitive data to the cloud to take advantage of its beneficial resources such as huge storage, fast calculation, and availability, yet cloud computing might harm the security of data stored and computed in it (confidentiality, integrity). Therefore, strict paradigm must be adopted by organization to obviate their outsourced data from being stolen, damaged or lost. In this paper, we compare between the existing models to secure big data implementation in the cloud computing. Then, we propose our own model to secure Big Data on the cloud computing environement, considering the lifecycle of data from uploading, storage, calculation to its destruction.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {18},
numpages = {11},
keywords = {safe data destruction, search over encrypted data, functional encryption, big data, cloud computing, authentication protocols},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/2818950.2818983,
author = {Choi, I. Stephen and Kee, Yang-Suk},
title = {Energy Efficient Scale-In Clusters with In-Storage Processing for Big-Data Analytics},
year = {2015},
isbn = {9781450336048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818950.2818983},
doi = {10.1145/2818950.2818983},
abstract = {Big data drives a computing paradigm shift. Due to enormous data volumes, data-intensive programming frameworks are pervasive and scale-out clusters are widespread. As a result, data-movement energy dominates overall energy consumption and this will get worse with a technology scaling. We propose scale-in clusters with In-Storage Processing (ISP) devices that would enable energy efficient computing for big-data analytics. ISP devices eliminate/reduce data movements towards CPUs and execute tasks more energy-efficiently. Thus, with energy efficient computing near data and higher throughput enabled, clusters with ISP can achieve more than quadruple energy efficiency with fewer number of nodes as compared to the energy efficiency of similarly performing its counter-part scale-out clusters.},
booktitle = {Proceedings of the 2015 International Symposium on Memory Systems},
pages = {265–273},
numpages = {9},
location = {Washington DC, DC, USA},
series = {MEMSYS '15}
}

@inproceedings{10.1145/3075564.3078884,
author = {Fiore, Sandro and Palazzo, Cosimo and D'Anca, Alessandro and Elia, Donatello and Londero, Elisa and Knapic, Cristina and Monna, Stephen and Marcucci, Nicola M. and Aguilar, Fernando and P\l{}\'{o}ciennik, Marcin and De Lucas, Jes\'{u}s E. Marco and Aloisio, Giovanni},
title = {Big Data Analytics on Large-Scale Scientific Datasets in the INDIGO-DataCloud Project},
year = {2017},
isbn = {9781450344876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3075564.3078884},
doi = {10.1145/3075564.3078884},
abstract = {In the context of the EU H2020 INDIGO-DataCloud project several use case on large scale scientific data analysis regarding different research communities have been implemented. All of them require the availability of large amount of data related to either output of simulations or observed data from sensors and need scientific (big) data solutions to run data analysis experiments. More specifically, the paper presents the case studies related to the following research communities: (i) the European Multidisciplinary Seafloor and water column Observatory (INGV-EMSO), (ii) the Large Binocular Telescope, (iii) LifeWatch, and (iv) the European Network for Earth System Modelling (ENES).},
booktitle = {Proceedings of the Computing Frontiers Conference},
pages = {343–348},
numpages = {6},
keywords = {Workflow, scientific use case, ensemble analysis, big data},
location = {Siena, Italy},
series = {CF'17}
}

@inproceedings{10.1145/3366030.3366103,
author = {Januzaj, Eshref and Januzaj, Visar and Mandl, Peter},
title = {An Application of Distributed Data Mining to Identify Data Quality Problems},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366103},
doi = {10.1145/3366030.3366103},
abstract = {When dealing with huge data sets, during the integration process of distributed data into a single data warehouse, one is not only confronted with time and security factors but with the well known problem of low data quality as well. In order to cope with such issues that the integration of distributed data often is faced with, we present in this paper an approach that applies distributed data mining, to facilitate a data quality analysis of the data in their distributed state. Data quality problems are identified by a classifier, which uses the knowledge gained from the clustering (subspace clustering) process performed on the distributed data. Experiments on real data show that the distributed analysis results are comparable to those conducted on the central data warehouse using classical data mining.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {418–422},
numpages = {5},
keywords = {Data Quality, Data Mining, Distributed Clustering},
location = {Munich, Germany},
series = {iiWAS2019}
}

@proceedings{10.1145/3396452,
title = {ICBDE '20: Proceedings of the 2020 The 3rd International Conference on Big Data and Education},
year = {2020},
isbn = {9781450374989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Due to the outbreak of COVID-19, and considering the participants' healthy, the 2020 3rd International Conference on Big Data and Education (ICBDE 2020) was held successfully online during April 01-03, 2020. Online conference is a good scientific platform for both local and international scientists, managers, business leaders, educators, scholars, engineers and technologists who work in all aspects of Big Data and Education to exchange and share their experiences, new ideas, and discuss the practical challenges encountered and the solutions adopted.},
location = {London, United Kingdom}
}

@inproceedings{10.1145/3286606.3286793,
author = {El Haourani, Lamia and Elkalam, Anas Abou and Ouahman, Abdelah Ait},
title = {Knowledge Based Access Control a Model for Security and Privacy in the Big Data},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286793},
doi = {10.1145/3286606.3286793},
abstract = {The most popular features of Big Data revolve around the so-called "3V" criterion: Volume, Variety and Velocity. Big Data is based on the massive collection and in-depth analysis of personal data, with a view to profiling, or even marketing and commercialization, thus violating citizens' privacy and the security of their data.In this article we discuss security and privacy solutions in the context of Big Data. We then focus on access control and present our new model called Knowledge-based Access Control (KBAC); this strengthens the access control already deployed in the target company (e.g., based on "RBAC" role or "ABAC" attributes for example) by adding a semantic access control layer. KBAC offers thinner access control, tailored to Big Data, with effective protection against intrusion attempts and unauthorized data inferences.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {16},
numpages = {8},
keywords = {Big Data, security, privacy, KBAC, access control model},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.1145/2903150.2906141,
author = {Pilato, Christian and Xu, Qirui and Mantovani, Paolo and Di Guglielmo, Giuseppe and Carloni, Luca P.},
title = {On the Design of Scalable and Reusable Accelerators for Big Data Applications},
year = {2016},
isbn = {9781450341288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2903150.2906141},
doi = {10.1145/2903150.2906141},
abstract = {Accelerators are becoming key elements of computing platforms for both data centers and mobile devices as they deliver energy-efficient high performance for key computational kernels. However, the design and integration of such components is complex, especially for Big Data applications where they have very large workloads to elaborate. Properly customizing the accelerators' private local memories (PLMs) is of critical importance. To analyze this problem we design an accelerator for Collaborative Filtering by applying a system-level design methodology that allows us to synthesize many alternative micro-architectures as we vary the PLM sizes. We then evaluate the resulting accelerators in terms of resource requirements for both embedded architectures and data centers as we vary the size and density of the workloads.},
booktitle = {Proceedings of the ACM International Conference on Computing Frontiers},
pages = {406–411},
numpages = {6},
location = {Como, Italy},
series = {CF '16}
}

@inproceedings{10.1145/3269206.3269232,
author = {Bereta, Konstantina and Caumont, Herv\'{e} and Goor, Erwin and Koubarakis, Manolis and Pantazi, Despina-Athanasia and Stamoulis, George and Ubels, Sam and Venus, Valentijn and Wahyudi, Firman},
title = {From Copernicus Big Data to Big Information and Big Knowledge: A Demo from the Copernicus App Lab Project},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269232},
doi = {10.1145/3269206.3269232},
abstract = {Copernicus is the European program for monitoring the Earth. It consists of a set of complex systems that collect data from satellites and in-situ sensors, process this data and provide users with reliable and up-to-date information on a range of environmental and security issues. The data collected by Copernicus is made available freely following an open access policy. Information extracted from Copernicus data is disseminated to users through the Copernicus services which address six thematic areas: land, marine, atmosphere, climate, emergency and security. We present a demo from the Horizon 2020 Copernicus App Lab project which takes big data from the Copernicus land service, makes it available on the Web as linked geospatial data and interlinks it with other useful public data to aid the development of applications by developers that might not be Earth Observation experts. Our demo targets a scenario where we want to study the "greenness" of Paris.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1911–1914},
numpages = {4},
keywords = {satellite data, linked data, copernicus},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3378393.3403823,
author = {ZeMicheal, Tadesse and Dietterich, Thomas G.},
title = {Conditional Mixture Models for Precipitation Data Quality Control},
year = {2020},
isbn = {9781450371292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378393.3403823},
doi = {10.1145/3378393.3403823},
abstract = {Rainfall is a very important weather variable, especially for agriculture. Unfortunately, rain gauges fail frequently. This paper describes a conditional mixture model for predicting the presence and amount of rain at a weather station based on measurements at nearby stations. The model is evaluated on simulated faults (blocked rain gauges) inserted into observations from the Oklahoma Mesonet. Using the negative log-likelihood as an anomaly score, we evaluate the area under the ROC and precision-recall curves for detecting these faults. The results show very good performance.},
booktitle = {Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {13–21},
numpages = {9},
location = {Ecuador},
series = {COMPASS '20}
}

@inproceedings{10.1145/2791347.2791377,
author = {Cuzzocrea, Alfredo},
title = {Aggregation and Multidimensional Analysis of Big Data for Large-Scale Scientific Applications: Models, Issues, Analytics, and Beyond},
year = {2015},
isbn = {9781450337090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791347.2791377},
doi = {10.1145/2791347.2791377},
abstract = {Aggregation and multidimensional analysis are well-known powerful tools for extracting useful knowledge, shaped in a summarized manner, which are being successfully applied to the annoying problem of managing and mining big data produced by large-scale scientific applications. Indeed, in the context of big data analytics, aggregation approaches allow us to provide meaningful descriptions of these data, otherwise impossible for alternative data-intensive analysis tools. On the other hand, multidimensional analysis methodologies introduce fortunate metaphors that significantly empathize the knowledge discovery phase from such huge amounts of data. Following this main trend, several big data aggregation and multidimensional analysis approaches have been proposed recently. The goal of this paper is to (i) provide a comprehensive overview of state-of-the-art techniques and (ii) depict open research challenges and future directions adhering to the reference scientific field.},
booktitle = {Proceedings of the 27th International Conference on Scientific and Statistical Database Management},
articleno = {23},
numpages = {6},
keywords = {big data aggregation, multidimensional analysis of big data, large-scale scientific applications, big data analytics},
location = {La Jolla, California},
series = {SSDBM '15}
}

@inproceedings{10.1145/3422713.3422715,
author = {Han, Bing and Chen, Zhenxiang and Liu, Cong and Shang, Mingyue},
title = {Design and Implementation of Big Data Management Platform for Android Applications},
year = {2020},
isbn = {9781450387859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422713.3422715},
doi = {10.1145/3422713.3422715},
abstract = {In recent years, the number of Android malicious applications has grown rapidly. In the field of network security, the detection of Android malicious applications has been a hot spot. Traditional Android malicious application detection has two methods: dynamic detection and static detection. With the development of network technology, network traffic has increased dramatically, analysis and researchers pay attention to malware detection based on network traffic. As the number of applications increases, application data management becomes particularly important. This paper proposes a method of the collection, store, analysis and visualization of Android applications. This platform provides a simple way to access data, which has broad application prospects.},
booktitle = {Proceedings of the 2020 3rd International Conference on Big Data Technologies},
pages = {36–40},
numpages = {5},
keywords = {Android, Malicious applications, Network traffic, Data management},
location = {Qingdao, China},
series = {ICBDT 2020}
}

@inproceedings{10.5555/2857070.2857072,
author = {Chen, Hsin-liang and Doty, Philip and Mollman, Carol and Niu, Xi and Yu, Jen-chien and Zhang, Tao},
title = {Library Assessment and Data Analytics in the Big Data Era: Practice and Policies},
year = {2015},
isbn = {087715547X},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Emerging technologies have offered libraries and librarians new ways and methods to collect and analyze data in the era of accountability to justify their value and contributions. For example, Gallagher, Bauer and Dollar (2005) analyzed the paper and online journal usage from all possible data sources and discovered that users at the Yale Medical Library preferred the electronic format of articles to the print version. After this discovery, they were able to take necessary steps to adjust their journal subscriptions. Many library professionals advocate such data-driven library management to strengthen and specify library budget proposals.},
booktitle = {Proceedings of the 78th ASIS&amp;T Annual Meeting: Information Science with Impact: Research in and for the Community},
articleno = {2},
numpages = {4},
keywords = {information privacy, big data, information policy, data analytics, library assessment},
location = {St. Louis, Missouri},
series = {ASIST '15}
}

@inproceedings{10.1145/1651415.1651417,
author = {Farinha, Jos\'{e} and Trigueiros, Maria Jos\'{e} and Belo, Orlando},
title = {Using Inheritance in a Metadata Based Approach to Data Quality Assessment},
year = {2009},
isbn = {9781605588162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1651415.1651417},
doi = {10.1145/1651415.1651417},
abstract = {Currently available data quality tools provide development environments that significantly decrease the effort in dealing with common data problems, such as those related with attribute domain validation, syntax checking, or value matching against a reference master data repository. On the contrary, more complex and specific data quality functionalities, whose requirements usually derive from application domain business rules, have to be developed from scratch, usually leading to high costs of development and maintenance. This paper introduces the concept of inheritance in a metadata-driven approach to simplified data quality rule management. The approach is based on the belief that even complex data quality rules very often adhere to recurring patterns that can be encoded and encapsulated as reusable, abstract templates. The approach is supported by a metamodel developed on top of OMG's Common Warehouse Metamodel, herein extended with the ability to derive new rule patterns from existing ones, through inheritance. The inheritance metamodel is presented in UML and its application is illustrated with a running example.},
booktitle = {Proceedings of the First International Workshop on Model Driven Service Engineering and Data Quality and Security},
pages = {1–8},
numpages = {8},
keywords = {patterns, data quality, metadata, cwm, conceptual modeling, inheritance},
location = {Hong Kong, China},
series = {MoSE+DQS '09}
}

@inproceedings{10.1145/3378904.3378919,
author = {Pane, Murty Magda and Siregar, Christian and Lake, Silverius C.J.M.},
title = {The Role of Big Data in Enhancing Student's Sustainable Development Awareness: A Case Study in Higher Education},
year = {2020},
isbn = {9781450376839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378904.3378919},
doi = {10.1145/3378904.3378919},
abstract = {This study objective is to find the general picture of the knowledge, attitude and behavior of students for sustainable development. It used the quantitative method with 160 students as the research subjects with the data collection technique of questionnaire distribution. The questionnaires are valid (the corrected item total correlations for knowledge, attitudes and behavior are: 0.745, 0.878 and 0.726) and reliable (α for knowledge, attitudes and behavior are: 0.985, 0.978 and 0.959). The positive nuances questionnaire's results are analyzed through a correlative analysis. The results showed scale 4 were chosen most for knowledge and attitude (41.40% and 42.93%), only in behavior the scale 3 was the most chosen (34.63%). The results concluded that the students tend to have positive knowledge, attitude and behavior towards the sustainable development, and the results will give strong support the sustainable future for human security. The results were connected with the big data, especially in social media and instant messaging where they can learn and get educated about sustainable development.},
booktitle = {Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology},
pages = {13–17},
numpages = {5},
keywords = {Knowledge, instant messaging, sustainable development, big data, attitude, behavior, social media},
location = {Singapore, China},
series = {BDET 2020}
}

@inproceedings{10.1145/1966901.1966903,
author = {F\"{u}rber, Christian and Hepp, Martin},
title = {Towards a Vocabulary for Data Quality Management in Semantic Web Architectures},
year = {2011},
isbn = {9781450306089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1966901.1966903},
doi = {10.1145/1966901.1966903},
abstract = {Reliable decision-making and reliable information based on Semantic Web data requires methodologies and techniques for managing the quality of the published data. To make things more complicated, the judgment of what is "good" data will often depend on the task at hand or the subjective requirements of data owners or data consumers. Some data quality requirements can be modeled using data quality rules, i.e. executable definitions that allow the identification and measurement of data quality problems. In this paper, we provide a conceptual model that allows the representation of such rules and other quality-related knowledge using the Resource Description Framework (RDF) and the Web Ontology Language (OWL). Based on our model, it is possible to monitor and assess the quality of data sources and to automate data cleansing tasks. The use of a generic conceptual model based on Semantic Web formalisms supports the definition of reusable, broadly applicable SPARQL queries and portable applications for data quality management (DQM). Furthermore, the explicit representation of rules in RDF/OWL facilitates rule management tasks, e.g. for analyzing consistency among the rules, and allows to collaborate and create a shared understanding.},
booktitle = {Proceedings of the 1st International Workshop on Linked Web Data Management},
pages = {1–8},
numpages = {8},
keywords = {SPARQL, information quality, linked data management, data quality management, knowledge representation, semantic web, ontology, trust},
location = {Uppsala, Sweden},
series = {LWDM '11}
}

@article{10.1145/3374751,
author = {Ding, Weilong and Zhao, Zhuofeng and Wang, Jianwu and Li, Han},
title = {Task Allocation in Hybrid Big Data Analytics for Urban IoT Applications},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {2691-1922},
url = {https://doi.org/10.1145/3374751},
doi = {10.1145/3374751},
abstract = {In urban Internet of Things (IoT) environments, data generated in real time could be processed by analytical applications in online or offline mode. In the management perspective of runtime environments, such modes can hardly be supported in a unified framework under multiple restrictions such as latency, utility, and QoS (quality of service). Meanwhile in the optimization perspective of specific applications, it is difficult for current infrastructure to efficiently allocate sufficient resources to tasks of an application, simultaneously considering multiple factors such as data size, velocity, and locality. In this article, two task allocation methods are proposed for batch and stream analytics to improve resource utility with auto-scaling guarantee when an analytical application is submitted or sudden workloads appear. Taking the highway domain as an example, the task allocation methods are implemented in a novel combined framework accordingly. Using both real-world and simulated data, extensive experiments show that our methods can improve utility efficiency with effective offload support.},
journal = {ACM/IMS Trans. Data Sci.},
month = {sep},
articleno = {18},
numpages = {22},
keywords = {analytical framework, spatio-temporal data, urban computing, task allocation, Internet of Things}
}

@proceedings{10.1145/3418688,
title = {ICCBD '20: 2020 the 3rd International Conference on Computing and Big Data},
year = {2020},
isbn = {9781450387866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Taichung, Taiwan}
}

@inproceedings{10.1145/2743065.2743097,
author = {Karthikeyan, P. and Amudhavel, J. and Abraham, A. and Sathian, D. and Raghav, R. S. and Dhavachelvan, P.},
title = {A Comprehensive Survey on Variants And Its Extensions Of Big Data In Cloud Environment},
year = {2015},
isbn = {9781450334419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2743065.2743097},
doi = {10.1145/2743065.2743097},
abstract = {As technology grows very fast with trendy outcome applications like Social networking, web analysis, bio-informatics network analysis, product analysis, etc., a huge amount of heterogeneous data is delivered in a wide range. Effective management of this huge data is interesting but faces many challenges in accuracy and processing. When a term huge data arrives then a recent and growing field namely BIG DATA comes into the act as it becomes a mass attracter of industry, academia and government for efficient processing of variety of huge data. This paper surveys a various technologies and the different areas where big data is implemented currently with a help of cloud environment [1] and its complete architecture [13]. Following it also explains about the different map reduce techniques and the framework that is being implanted for processing such huge data. Finally we discuss the future on big data processing with the cloud environment and the challenges [28] faced at these areas.},
booktitle = {Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering &amp; Technology (ICARCSET 2015)},
articleno = {32},
numpages = {5},
keywords = {Cloud computing, Security, Big data, Hadoop},
location = {Unnao, India},
series = {ICARCSET '15}
}

@inproceedings{10.1145/2723372.2742784,
author = {G.C., Paul Suganthan and Sun, Chong and K., Krishna Gayatri and Zhang, Haojun and Yang, Frank and Rampalli, Narasimhan and Prasad, Shishir and Arcaute, Esteban and Krishnan, Ganesh and Deep, Rohit and Raghavendra, Vijay and Doan, AnHai},
title = {Why Big Data Industrial Systems Need Rules and What We Can Do About It},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742784},
doi = {10.1145/2723372.2742784},
abstract = {Big Data industrial systems that address problems such as classification, information extraction, and entity matching very commonly use hand-crafted rules. Today, however, little is understood about the usage of such rules. In this paper we explore this issue. We discuss how these systems differ from those considered in academia. We describe default solutions, their limitations, and reasons for using rules. We show examples of extensive rule usage in industry. Contrary to popular perceptions, we show that there is a rich set of research challenges in rule generation, evaluation, execution, optimization, and maintenance. We discuss ongoing work at WalmartLabs and UW-Madison that illustrate these challenges. Our main conclusions are (1) using rules (together with techniques such as learning and crowdsourcing) is fundamental to building semantics-intensive Big Data systems, and (2) it is increasingly critical to address rule management, given the tens of thousands of rules industrial systems often manage today in an ad-hoc fashion.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {265–276},
numpages = {12},
keywords = {rule management, classification, big data},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/3424978.3425002,
author = {Liu, Jianhua and Gao, Taotao and Du, Yunxia},
title = {Research on Equilibrium Control Method of Urban Road Network Based on Big Data},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425002},
doi = {10.1145/3424978.3425002},
abstract = {Because the traffic flow on the urban road network continues to gather to the congested area, which exceeds the regional traffic carrying capacity, causing the traffic condition to deteriorate gradually. Based on the spatial proximity characteristics of the urban road network and relying on big data analysis technology, a traffic balance control method based on the urban regional capacity is designed and implemented. Through the collected data of urban vehicle travel, the correlation between regional capacity and traffic state is analyzed, the maximum bearing capacity of the region is determined, and the key adjacent areas that affect the regional state are identified. In the process of actual traffic control, real-time traffic flow data of urban road network is collected. When the regional traffic volume is close to the capacity threshold, the traffic flow in key adjacent areas is controlled and dynamically allocated to effectively prevent the continuous accumulation and state deterioration of regional traffic flow. Finally, the algorithm simulation test based on big data analysis platform is conducted. The results show that the variance of traffic volume in each area is reduced by 15% and the average congestion duration is reduced by 12%.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {24},
numpages = {5},
keywords = {Big data analysis, Equilibrium control, Traffic state, Dynamic distribution},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.1145/3360901.3364438,
author = {Hoque, Md Reshad Ul and Bradley, Dash and Kwan, Chiman and Chiatti, Agnese and Li, Jiang and Wu, Jian},
title = {Searching for Evidence of Scientific News in Scholarly Big Data},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364438},
doi = {10.1145/3360901.3364438},
abstract = {Public digital media can often mix factual information with fake scientific news, which is typically difficult to pinpoint, especially for non-professionals. These scientific news articles create illusions and misconceptions, thus ultimately influence the public opinion, with serious consequences at a broader social scale. Yet, existing solutions aiming at automatically verifying the credibility of news articles are still unsatisfactory. We propose to verify scientific news by retrieving and analyzing its most relevant source papers from an academic digital library (DL), e.g., arXiv. Instead of querying keywords or regular named entities extracted from news articles, we query domain knowledge entities (DKEs) extracted from the text. By querying each DKE, we retrieve a list of candidate scholarly papers. We then design a function to rank them and select the most relevant scholarly paper. After exploring various representations, experiments indicate that the term frequency-inverse document frequency (TF-IDF) representation with cosine similarity outperforms baseline models based on word embedding. This result demonstrates the efficacy of using DKEs to retrieve scientific papers which are relevant to a specific news article. It also indicates that word embedding may not be the best document representation for domain specific document retrieval tasks. Our method is fully automated and can be effectively applied to facilitating fake and misinformed news detection across many scientific domains.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {251–254},
numpages = {4},
keywords = {embedding, web api, domain knowledge entity, fake news},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1109/UCC.2014.46,
author = {Bellavista, Paolo and Corradi, Antonio and Reale, Andrea and Ticca, Nicola},
title = {Priority-Based Resource Scheduling in Distributed Stream Processing Systems for Big Data Applications},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.46},
doi = {10.1109/UCC.2014.46},
abstract = {Distributed Stream Processing Systems (DSPSs) are attracting increasing industrial and academic interest as flexible tools to implement scalable and cost-effective on-line analytics applications over Big Data streams. Often hosted in private/public cloud deployment environments, DSPSs offer data stream processing services that transparently exploit the distributed computing resources made available to them at runtime. Given the volume of data of interest, possible (hard/soft) real-time processing requirements, and the time-variable characteristics of input data streams, it is very important for DSPSs to use smart and innovative scheduling techniques that allocate computing resources properly and avoid static over-provisioning. In this paper, we originally investigate the suitability of exploiting application-level indications about differentiated priorities of different stream processing tasks to enable application-specific DSPS resource scheduling, e.g., Capable of re-shaping processing resources in order to dynamically follow input data peaks of prioritized tasks, with no static over-provisioning. We originally propose a general and simple technique to design and implement priority-based resource scheduling in flow-graph-based DSPSs, by allowing application developers to augment DSPS graphs with priority metadata and by introducing an extensible set of priority schemas to be automatically handled by the extended DSPS. In addition, we show the effectiveness of our approach via its implementation and integration in our Quasit DSPS and through experimental evaluation of this prototype on a real-world stream processing application of Big Data vehicular traffic analysis.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {363–370},
numpages = {8},
keywords = {Cloud Computing Optimization, Big Data, Priority-based Resource Scheduling, Distributed Stream Processing, Application-level and Application-specific Scheduling, Vehicular Traffic Analysis},
series = {UCC '14}
}

@inproceedings{10.1145/3278229.3278235,
author = {Li, Hui and Cheng, Yibo and Li, Yinghui and Ma, Xiaochang and Li, Delong},
title = {Health Assessment System Based on Big Data Analysis of Meridian Electrical Potential},
year = {2018},
isbn = {9781450364362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278229.3278235},
doi = {10.1145/3278229.3278235},
abstract = {This paper develops a health management and assessment system named Jinluo Kangbao Health Management System, which is based on big data analysis of meridian potential value. The main purpose is to achieve fast and low-cost diagnosis of disease, and to alleviate problems like unequal distribution of medical resources. The system contains three major modules, the meridian detector, the client software and the central database. During test process, the electrode of meridian detector contacts 24 acupuncture points of the body in particular order, and collects potential value of each acupuncture point. By constructing a calculation matrix, the potential values are corresponded to certain regions of a 24-dimensional space. Within this space, the positions are used to judge the status of the client's health condition, including at low-risk, at medium-risk, at high risk, in subclinical state or in clinical state. The collected data are analyzed by the central database, which also provides reliable medical advice. In this paper, we use Jinluo Kangbao system to check a client. We list the detailed results concerning major aspects of his health condition, and give him relevant medical advice. The output of this system shows high accuracy compared to the results provided by hospital.},
booktitle = {Proceedings of the 3rd International Conference on Biomedical Signal and Image Processing},
pages = {75–80},
numpages = {6},
keywords = {Health assessment system, Acupuncture point, Medical advice, Meridian detector, Big data analysis},
location = {Seoul, Republic of Korea},
series = {ICBIP '18}
}

@inproceedings{10.1145/3210604.3210644,
author = {Whitman, Madisson and Hsiang, Chien-yi and Roark, Kendall},
title = {Potential for Participatory Big Data Ethics and Algorithm Design: A Scoping Mapping Review},
year = {2018},
isbn = {9781450355742},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210604.3210644},
doi = {10.1145/3210604.3210644},
abstract = {Ubiquitous networked data collection and algorithm-based information systems have the potential to disparately impact lives around the planet and pose a host of emerging ethical challenges. One response has been a call for more transparency and democratic control over the design and implementation of such systems. This scoping mapping review focuses on participatory approaches to the design, governance, and future of these systems across a wide variety of contexts and domains.1},
booktitle = {Proceedings of the 15th Participatory Design Conference: Short Papers, Situated Actions, Workshops and Tutorial - Volume 2},
articleno = {5},
numpages = {6},
keywords = {speculative design, algorithm design, research through design},
location = {Hasselt and Genk, Belgium},
series = {PDC '18}
}

@inproceedings{10.5555/645926.671870,
author = {Caruso, Francesco and Cochinwala, Munir and Ganapathy, Uma and Lalk, Gail and Missier, Paolo},
title = {Telcordia's Database Reconciliation and Data Quality Analysis Tool},
year = {2000},
isbn = {1558607153},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 26th International Conference on Very Large Data Bases},
pages = {615–618},
numpages = {4},
series = {VLDB '00}
}

@article{10.5555/2382887.2382903,
author = {Carter, Thomas and Hauselt, Peggy and Martin, Melanie and Thomas, Megan},
title = {Building a Big Data Research Program at a Small University},
year = {2012},
issue_date = {December 2012},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {28},
number = {2},
issn = {1937-4771},
abstract = {In the 2010-2011 school year we received an Army High Performance Computing Research Center grant whose goal was increasing the number of Hispanic engineers with expertise in complex systems, simulations and large data sets. Our university is a medium-sized, public, Hispanic-serving institution; our department is small. Our goals were to improve the social support available to our Hispanic students, encourage them to complete their degrees, and give them a valid research experience to provide a basis for informed decisions about whether or not they want to go to graduate school. This paper will cover how we structured our program to accomplish our goals, including how we factored in results from prior research on minority student experiences. In the second year, we expanded our program to include geography and give a new cohort of students a multi-disciplinary experience. We will discuss how a small computer science department successfully built an undergraduate research program organized around the theme of large data sets, what we have accomplished so far and how we hope to continue.},
journal = {J. Comput. Sci. Coll.},
month = {dec},
pages = {95–102},
numpages = {8}
}

@inproceedings{10.1145/3378936.3378950,
author = {Tosson, A. and Shokr, M. and Pietsch, U.},
title = {Application of Cloud Computing for Big Data in the X-Ray Crystallography Community},
year = {2020},
isbn = {9781450376907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378936.3378950},
doi = {10.1145/3378936.3378950},
abstract = {The X-ray crystallography community has recently been affected by a significant increase in data volume caused by the use of advanced detector technologies and the new generation of high brilliance light sources. The fact that forced the decision makers to implement Big Data analytics, aiming to achieve a suitable environment for scientists at experimental and post-experimental phases. This paper demonstrates an extension of our approach towards a compact platform which provides the scientists with the digital ecosystem for the systematic harvest of data. It introduces an innovative solution to use warehousing and cloud computing to manage datasets collected by 2D energy-dispersive detectors, for an example. Moreover, it suggests that, deploying a Software as a Service (SaaS) cloud model, a public cloud data center, and cloud-based in-memory warehousing architecture, it is possible to dramatically reduce both hardware and processing costs.},
booktitle = {Proceedings of the 3rd International Conference on Software Engineering and Information Management},
pages = {1–4},
numpages = {4},
keywords = {Cloud computing, Crystallography, Big Data, In-Memory warehousing},
location = {Sydney, NSW, Australia},
series = {ICSIM '20}
}

@inproceedings{10.1145/2808797.2809372,
author = {Wang, Kun and Sun, Duoyong},
title = {Research on the Shanghai Cooperation Organization Network Architecture from the Big Data Perspective},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2809372},
doi = {10.1145/2808797.2809372},
abstract = {The Shanghai Cooperation Organization (SCO) is playing an increasingly important role in many respects, such as the economic, political and security cooperation. Research results aiming at this regional organization have gained lots of attentions for a long time. The Social Network Analysis (SNA) is considered to be an effective method in the studies of international relation, especially from the Big Data perspective. Our research is mainly based on the economic and trade data among the member states of SCO in 2012. All the results have significantly shown that both Russia and China have occupied the central roles, no matter in the tables or in the figures. Due to the limitation of essential data, the importance of Russia has not been fully reflected. Based on the results, we suggests that China should pay more attention to the affairs of the region. And then the Chinese government may enhance her influence in the SCO.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {1208–1211},
numpages = {4},
keywords = {Networks Architecture, Social Networks Analysis, Big Data, Shanghai Cooperation Organization},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/3127479.3129248,
author = {Sikdar, Sourav and Teymourian, Kia and Jermaine, Chris},
title = {An Experimental Comparison of Complex Object Implementations for Big Data Systems},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3129248},
doi = {10.1145/3127479.3129248},
abstract = {Many cloud-based data management and analytics systems support complex objects. Dataflow platforms such as Spark and Flink allow programmers to manipulate sets consisting of objects from a host programming language (often Java). Document databases such as MongoDB make use of hierarchical interchange formats---most popularly JSON---which embody a data model where individual records can themselves contain sets of records. Systems such as Dremel and AsterixDB allow complex nesting of data structures.Clearly, no system designer would expect a system that stores JSON objects as text to perform at the same level as a system based upon a custom-built physical data model. The question we ask is: How significant is the performance hit associated with choosing a particular physical implementation? Is the choice going to result in a negligible performance cost, or one that is debilitating? Unfortunately, there does not exist a scientific study of the effect of physical complex model implementation on system performance in the literature. Hence it is difficult for a system designer to fully understand performance implications of such choices. This paper is an attempt to remedy that.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {432–444},
numpages = {13},
keywords = {big data management, experimental comparison, complex objects implementation, data serialization},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/2808719.2816981,
author = {Deng, Xin and Wu, Donghui},
title = {Senior Health Management through Internet of Things and Real-Time Big Data Analytics},
year = {2015},
isbn = {9781450338530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808719.2816981},
doi = {10.1145/2808719.2816981},
abstract = {In 10 years, one third of the population in developed countries will be 60-years or older. 90% of seniors want to age in their own homes, not a facility. Families want to know that their aging parents are healthy and safe. It is extremely challenging for seniors with chronic conditions to manage their day-to-day medical needs and prevent medical emergencies, e.g. stroke, fall, etc. With the rapid growth of Internet of Things, increasing popularity of wearable medical devices or monitor devices, and other sensors at home, the opportunity for senior health management and prevention of medical emergency at home has never been better with big data platform, integration of massive and diverse data sources, e.g. medical charts from doctor's office, prescription information from pharmacies, claims data from insurance companies, more importantly, real-time data stream from Internet-of-things, wearable devices, and other vital and sensor data at home; and even more critically, real-time alerts of various risks from predictive analytics connected with providers, care givers and family members. In this paper, we will analyze the scope of the problem, and present current status and challenges in this area. Secondly we will propose a prototype schema to address this problem through Internet of things and real-time big data predictive analytics. Finally we will discuss some technical and non-technical challenges observed.},
booktitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {674},
numpages = {1},
keywords = {health informatics, big data platform, real-time alerts, predictive analytics, real-time data collection, data integration},
location = {Atlanta, Georgia},
series = {BCB '15}
}

