@inproceedings{10.1145/3127479.3132565,
author = {Chen, Wei and Pi, Aidi and Rao, Jia and Zhou, Xiaobo},
title = {MBalloon: Enabling Elastic Memory Management for Big Data Processing},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3132565},
doi = {10.1145/3127479.3132565},
abstract = {Big Data processing often suffers from significant memory pressure, resulting in excessive garbage collection (GC) and out-of-memory (OOM) errors, harming system performance and reliability. Therefore, users tend to give an excessive heap size to applications to avoid job failure, causing low cluster utilization.In this paper, we demonstrate that lightweight virtualization, such as OS containers, opens up opportunities to address memory pressure: 1) tasks running in a container can be set to a large heap size to avoid OOM errors without worrying about thrashing the host machine; 2) tasks that are under memory pressure and incur significant GC activities can be temporarily "suspended" by depriving the hosting container's resources, and can be "resumed" later when other tasks complete and release their resources. We propose and develop mBalloon, an elastic memory manager, that leverages containers to flexibly and precisely control the memory usage of big data tasks. Applications running with mBalloon can survive from memory pressure, incur less GC overhead and help improve cluster utilization.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {654},
numpages = {1},
keywords = {memory management, big data, virtualization},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/2818869.2818889,
author = {Dubey, Harishchandra and Yang, Jing and Constant, Nick and Amiri, Amir Mohammad and Yang, Qing and Makodiya, Kunal},
title = {Fog Data: Enhancing Telehealth Big Data Through Fog Computing},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818889},
doi = {10.1145/2818869.2818889},
abstract = {The size of multi-modal, heterogeneous data collected through various sensors is growing exponentially. It demands intelligent data reduction, data mining and analytics at edge devices. Data compression can reduce the network bandwidth and transmission power consumed by edge devices. This paper proposes, validates and evaluates Fog Data, a service-oriented architecture for Fog computing. The center piece of the proposed architecture is a low power embedded computer that carries out data mining and data analytics on raw data collected from various wearable sensors used for telehealth applications. The embedded computer collects the sensed data as time series, analyzes it, and finds similar patterns present. Patterns are stored, and unique patterns are transmited. Also, the embedded computer extracts clinically relevant information that is sent to the cloud. A working prototype of the proposed architecture was built and used to carry out case studies on telehealth big data applications. Specifically, our case studies used the data from the sensors worn by patients with either speech motor disorders or cardiovascular problems. We implemented and evaluated both generic and application specific data mining techniques to show orders of magnitude data reduction and hence transmission power savings. Quantitative evaluations were conducted for comparing various data mining techniques and standard data compression techniques. The obtained results showed substantial improvement in system efficiency using the Fog Data architecture.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {14},
numpages = {6},
keywords = {Internet of Things, Wearable Devices, Body Area Network, Cyber-physical Systems, Edge Computing, Fog Computing, Big Data},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@inproceedings{10.5555/2888619.2888711,
author = {Sanchez, Susan M.},
title = {Simulation Experiments: Better Data, Not Just Big Data},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {Data mining tools have been around for several decades, but the term "big data" has only recently captured widespread attention. Numerous success stories have been promulgated as organizations have sifted through massive volumes of data to find interesting patterns that are, in turn, transformed into actionable information. Yet a key drawback to the big data paradigm is that it relies on observational data---limiting the types of insights that can be gained. The simulation world is different. A "data farming" metaphor captures the notion of purposeful data generation from simulation models. Large-scale designed experiments let us grow the simulation output efficiently and effectively. We can explore massive input spaces, uncover interesting features of complex simulation response surfaces, and explicitly identify cause-and-effect relationships. With this new mindset, we can achieve quantum leaps in the breadth, depth, and timeliness of the insights yielded by simulation models.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {800–811},
numpages = {12},
location = {Huntington Beach, California},
series = {WSC '15}
}

@inproceedings{10.5555/2693848.2693957,
author = {Sanchez, Susan M.},
title = {Simulation Experiments: Better Data, Not Just Big Data},
year = {2014},
publisher = {IEEE Press},
abstract = {Data mining tools have been around for several decades, but the term "big data" has only recently captured widespread attention. Numerous success stories have been promulgated as organizations have sifted through massive volumes of data to find interesting patterns that are, in turn, transformed into actionable information. Yet a key drawback to the big data paradigm is that it relies on observational data---limiting the types of insights that can be gained. The simulation world is different. A "data farming" metaphor captures the notion of purposeful data generation from simulation models. Large-scale designed experiments let us grow the simulation output efficiently and effectively. We can explore massive input spaces, uncover interesting features of complex simulation response surfaces, and explicitly identify cause-and-effect relationships. With this new mindset, we can achieve quantum leaps in the breadth, depth, and timeliness of the insights yielded by simulation models.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {805–816},
numpages = {12},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/3098593.3098597,
author = {Blenk, Andreas and Kalmbach, Patrick and Kellerer, Wolfgang and Schmid, Stefan},
title = {O'zapft is: Tap Your Network Algorithm's Big Data!},
year = {2017},
isbn = {9781450350549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098593.3098597},
doi = {10.1145/3098593.3098597},
abstract = {At the heart of many computer network planning, deployment, and operational tasks lie hard algorithmic problems. Accordingly, over the last decades, we have witnessed a continuous pursuit for ever more accurate and faster algorithms. We propose an approach to design network algorithms which is radically different from most existing algorithms. Our approach is motivated by the observation that most existing algorithms to solve a given hard computer networking problem overlook a simple yet very powerful optimization opportunity in practice: many network algorithms are executed repeatedly (e.g., for each virtual network request or in reaction to user mobility), and hence with each execution, generate interesting data: (problem,solution)-pairs. We make the case for leveraging the potentially big data of an algorithm's past executions to improve and speed up future, similar solutions, by reducing the algorithm's search space. We study the applicability of machine learning to network algorithm design, identify challenges and discuss limitations. We empirically demonstrate the potential of machine learning network algorithms in two case studies, namely the embedding of virtual networks (a packing optimization problem) and k-center facility location (a covering optimization problem), using a prototype implementation.},
booktitle = {Proceedings of the Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},
pages = {19–24},
numpages = {6},
keywords = {Machine Learning, Computer Networks, Big Data, Algorithms},
location = {Los Angeles, CA, USA},
series = {Big-DAMA '17}
}

@inproceedings{10.1145/3318299.3318395,
author = {Zhan, Ge},
title = {Online Forum Authenticity: Big Data Analytics in Healthcare},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318395},
doi = {10.1145/3318299.3318395},
abstract = {It is difficult to discern the authenticity online reviews, which is critical particularly in a setting of patient-doctor online forum. In this paper, a model on the detection of doctor quality has been developed and tested with online big data. In this study, a database with 31,646 online reviews was compiled. Text mining and word-cloud analysis results indicate that the model provides an effective solution to assess the quality of doctors registered in online forum, the quality of doctor-patient online interaction, and patients' overall perception. A guideline has been provided to evaluate doctor authenticity.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {290–294},
numpages = {5},
keywords = {authenticity, patient review, Doctor quality, doctor-patient interaction},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1145/3297280.3297504,
author = {Pittaras, Nikiforos and Papadakis, George and Stamoulis, George and Argyriou, Giorgos and Taniskidou, Efi Karra and Thanos, Emmanouil and Giannakopoulos, George and Tsekouras, Leonidas and Koubarakis, Manolis},
title = {GeoSensor: Semantifying Change and Event Detection over Big Data},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297504},
doi = {10.1145/3297280.3297504},
abstract = {GeoSensor is a novel, open-source system that enriches change detection over satellite images with event detection over news items and social media content. GeoSensor combines these two orthogonal operations through state-of-the-art Semantic Web technologies. At its core lies the open-source, semantics-enabled Big Data infrastructure developed by the EU H2020 BigDataEurope project. This allows GeoSensor to offer an on-line functionality, despite facing three major challenges of Big Data: Volume (a single satellite image typically occupies a few GBs), Variety (its data sources include two different types of satellite images and various types of user-generated content) and Veracity, as the accuracy of the end result is crucial for the usefulness of our system. We present GeoSensor's architecture in detail, highlighting the advantages of using semantics for taking the most of the knowledge extracted from news items and Earth Observation products. We also verify GeoSensor's efficiency through a preliminary experimental study.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2259–2266},
numpages = {8},
keywords = {change detection, linked data, satellite data, big data, event detection},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.5555/2755753.2757017,
author = {Wang, Chao and Li, Xi and Zhou, Xuehai},
title = {SODA: Software Defined FPGA Based Accelerators for Big Data},
year = {2015},
isbn = {9783981537048},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {FPGA has been an emerging field in novel big data architectures and systems, due to its high efficiency and low power consumption. It enables the researchers to deploy massive accelerators within one single chip. In this paper, we present a software defined FPGA based accelerators for big data, named SODA, which could reconstruct and reorganize the acceleration engines according to the requirement of the various data-intensive applications. SODA decomposes large and complex applications into coarse grained single-purpose RTL code libraries that perform specialized tasks in out-of-order hardware. We built a prototyping system with constrained shortest path Finding (CSPF) case studies to evaluate SODA framework. SODA is able to achieve up to 43.75X speedup at 128 node application. Furthermore, hardware cost of the SODA framework demonstrates that it can achieve high speedup with moderate hardware utilization.},
booktitle = {Proceedings of the 2015 Design, Automation &amp; Test in Europe Conference &amp; Exhibition},
pages = {884–887},
numpages = {4},
keywords = {acceleratioin, big data, software-defined, FPGA},
location = {Grenoble, France},
series = {DATE '15}
}

@inproceedings{10.1145/2378356.2378367,
author = {Bear, Chuck and Lamb, Andrew and Tran, Nga},
title = {The Vertica Database: SQL RDBMS for Managing Big Data},
year = {2012},
isbn = {9781450317528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2378356.2378367},
doi = {10.1145/2378356.2378367},
abstract = {In this presentation, we describe the architecture of the Vertica Analytic Database (Vertica), with an emphasis on the management features. Vertica combines a scale-out design, commodity hardware, and the RDBMS data management paradigm to keep SQL relevant to structured "Big Data".},
booktitle = {Proceedings of the 2012 Workshop on Management of Big Data Systems},
pages = {37–38},
numpages = {2},
keywords = {vertica},
location = {San Jose, California, USA},
series = {MBDS '12}
}

@inproceedings{10.1145/3219819.3219867,
author = {Lin, Qingwei and Ke, Weichen and Lou, Jian-Guang and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Zhou, Ziyi and Qiao, Bo and Zhang, Dongmei},
title = {BigIN4: Instant, Interactive Insight Identification for Multi-Dimensional Big Data},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219867},
doi = {10.1145/3219819.3219867},
abstract = {The ability to identify insights from multi-dimensional big data is important for business intelligence. To enable interactive identification of insights, a large number of dimension combinations need to be searched and a series of aggregation queries need to be quickly answered. The existing approaches answer interactive queries on big data through data cubes or approximate query processing. However, these approaches can hardly satisfy the performance or accuracy requirements for ad-hoc queries demanded by interactive exploration. In this paper, we present BigIN4, a system for instant, interactive identification of insights from multi-dimensional big data. BigIN4 gives insight suggestions by enumerating subspaces and answers queries by combining data cube and approximate query processing techniques. If a query cannot be answered by the cubes, BigIN4 decomposes it into several low dimensional queries that can be directly answered by the cubes through an online constructed Bayesian Network and gives an approximate answer within a statistical interval. Unlike the related works, BigIN4 does not require any prior knowledge of queries and does not assume a certain data distribution. Our experiments on ten real-world large-scale datasets show that BigIN4 can successfully identify insights from big data. Furthermore, BigIN4 can provide approximate answers to aggregation queries effectively (with less than 10% error on average) and efficiently (50x faster than sampling-based methods).},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {547–555},
numpages = {9},
keywords = {data cube, insight identification, approximate query processing, interactive data analytics},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/2987550.2987576,
author = {Yan, Ying and Gao, Yanjie and Chen, Yang and Guo, Zhongxin and Chen, Bole and Moscibroda, Thomas},
title = {TR-Spark: Transient Computing for Big Data Analytics},
year = {2016},
isbn = {9781450345255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987550.2987576},
doi = {10.1145/2987550.2987576},
abstract = {Large-scale public cloud providers invest billions of dollars into their cloud infrastructure and operate hundreds of thousands of servers across the globe. For various reasons, much of this provisioned server capacity runs at low average utilization, and there is tremendous competitive pressure to increase utilization. Conceptually, the way to increase utilization is clear: Run time-insensitive batch-job workloads as secondary background tasks whenever server capacity is underutilized; and evict these workloads when the server's primary task requires more resources. Big data analytic tasks would seem to be an ideal fit to run opportunistically on such transient resources in the cloud. In reality, however, modern distributed data processing systems such as MapReduce or Spark are designed to run as the primary task on dedicated hardware, and they perform badly on transiently available resources because of the excessive cost of cascading re-computations in case of evictions.In this paper, we propose a new framework for big data analytics on transient resources. Specifically, we design and implement TR-Spark, a version of Spark that can run highly efficiently as a secondary background task on transient (evictable) resources. The design of TR-Spark is based on two principles: resource stability and data size reduction-aware scheduling and lineage-aware checkpointing. The combination of these principles allows TR-Spark to naturally adapt to the stability characteristics of the underlying compute infrastructure. Evaluation results show that while regular Spark effectively fails to finish a job in clusters of even moderate instability, TR-Spark performs nearly as well as Spark running on stable resources.},
booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
pages = {484–496},
numpages = {13},
keywords = {Checkpointing, Spark, Transient computing},
location = {Santa Clara, CA, USA},
series = {SoCC '16}
}

@article{10.1145/3369738,
author = {Sivathanu, Muthian and Vuppalapati, Midhul and Gulavani, Bhargav S. and Rajan, Kaushik and Leeka, Jyoti and Mohan, Jayashree and Kedia, Piyus},
title = {INSTalytics: Cluster Filesystem Co-Design for Big-Data Analytics},
year = {2020},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1553-3077},
url = {https://doi.org/10.1145/3369738},
doi = {10.1145/3369738},
abstract = {We present the design, implementation, and evaluation of INSTalytics, a co-designed stack of a cluster file system and the compute layer, for efficient big-data analytics in large-scale data centers. INSTalytics amplifies the well-known benefits of data partitioning in analytics systems; instead of traditional partitioning on one dimension, INSTalytics enables data to be simultaneously partitioned on four different dimensions at the same storage cost, enabling a larger fraction of queries to benefit from partition filtering and joins without network shuffle. To achieve this, INSTalytics uses compute-awareness to customize the three-way replication that the cluster file system employs for availability. A new heterogeneous replication layout enables INSTalytics to preserve the same recovery cost and availability as traditional replication. INSTalytics also uses compute-awareness to expose a new sliced-read API that improves performance of joins by enabling multiple compute nodes to read slices of a data block efficiently via co-ordinated request scheduling and selective caching at the storage nodes. We have built a prototype implementation of INSTalytics in a production analytics stack, and we show that recovery performance and availability is similar to physical replication, while providing significant improvements in query performance, suggesting a new approach to designing cloud-scale big-data analytics systems.},
journal = {ACM Trans. Storage},
month = {jan},
articleno = {23},
numpages = {30},
keywords = {data center storage, Storage replication, big data query processing}
}

@inproceedings{10.1145/3357223.3362707,
author = {Dai, Jason Jinquan and Wang, Yiheng and Qiu, Xin and Ding, Ding and Zhang, Yao and Wang, Yanzhang and Jia, Xianyan and Zhang, Cherry Li and Wan, Yan and Li, Zhichao and Wang, Jiao and Huang, Shengsheng and Wu, Zhongyuan and Wang, Yang and Yang, Yuhao and She, Bowen and Shi, Dongjie and Lu, Qi and Huang, Kai and Song, Guoqiong},
title = {BigDL: A Distributed Deep Learning Framework for Big Data},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3362707},
doi = {10.1145/3357223.3362707},
abstract = {ThispaperpresentsBigDL (adistributeddeeplearning framework for Apache Spark), which has been used by a variety of users in the industry for building deep learning applications on production big data platforms. It allows deep learning applications to run on the Apache Hadoop/Spark cluster so as to directly process the production data, and as a part of the end-to-end data analysis pipeline for deployment and management. Unlike existing deep learning frameworks, BigDL implements distributed, data parallel training directly on top of the functional compute model (with copy-on-write and coarse-grained operations) of Spark. We also share real-world experience and "war stories" of users that havead-optedBigDLtoaddresstheirchallenges(i.e., howtoeasilybuildend-to-enddataanalysisanddeep learning pipelines for their production data).},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {50–60},
numpages = {11},
keywords = {big data, end-to-end data pipeline, Apache Spark, distributed deep learning},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings{10.1145/3229598.3229600,
author = {Wassermann, Sarah and Casas, Pedro},
title = {BIGMOMAL: Big Data Analytics for Mobile Malware Detection},
year = {2018},
isbn = {9781450359108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229598.3229600},
doi = {10.1145/3229598.3229600},
abstract = {Mobile malware is on the rise. Indeed, due to their popularity, smartphones represent an attractive target for cybercriminals, especially because of private user data, as these devices incorporate a lot of sensitive information about users, even more than a personal computer. As a matter of fact, besides personal information such as documents, accounts, passwords, and contacts, smartphone sensors centralise other sensitive data including user location and physical activities. In this paper, we study the problem of malware detection in smartphones, relying on supervised-machine-learning models and big-data analytics frameworks. Using the SherLock dataset, a large, publicly available dataset for smartphone-data analysis, we train and benchmark tree-based models to identify running applications and to detect malware activity. We verify their accuracy, and initial results suggest that decision trees are capable of identifying running apps and malware activity with high accuracy.},
booktitle = {Proceedings of the 2018 Workshop on Traffic Measurements for Cybersecurity},
pages = {33–39},
numpages = {7},
keywords = {High-Dimensional Data, Machine Learning, Mobile-Malware Detection, Big-Data Analytics},
location = {Budapest, Hungary},
series = {WTMC '18}
}

@inproceedings{10.1145/2668930.2688199,
author = {Singhal, Rekha and Chahal, Dheeraj},
title = {PABS 2015: 1st Workshop on Performance Analysis of Big Data Systems},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688199},
doi = {10.1145/2668930.2688199},
abstract = {The first ACM international workshop on performance analysis of big data system is held in Austin, Texas, USA on February 1, 2015 and co-located with the ACM fifth International Conference on Performance Engineering (ICPE). The main objective of the workshop is to discuss the performance challenges imposed by big data systems and the different state-of-the-art solutions proposed to overcome these challenges. The workshop aims at providing a platform for scientific researchers, academicians and practitioners to discuss techniques, models, benchmarks, tools and experiences while dealing with performance issues in big data systems. We have constructed an exciting program of one big data expert keynote talk, one invited talk and two refereed papers that will give participants a full dose of emerging research.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {347–348},
numpages = {2},
keywords = {prediction, performance, case study, big data, architectures, analytics},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@inproceedings{10.1145/2976749.2989034,
author = {Volgushev, Nikolaj and Schwarzkopf, Malte and Lapets, Andrei and Varia, Mayank and Bestavros, Azer},
title = {DEMO: Integrating MPC in Big Data Workflows},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2989034},
doi = {10.1145/2976749.2989034},
abstract = {Secure multi-party computation (MPC) allows multiple parties to perform a joint computation without disclosing their private inputs. Many real-world joint computation use cases, however, involve data analyses on very large data sets, and are implemented by software engineers who lack MPC knowledge. Moreover, the collaborating parties -- e.g., several companies -- often deploy different data analytics stacks internally. These restrictions hamper the real-world usability of MPC. To address these challenges, we combine existing MPC frameworks with data-parallel analytics frameworks by extending the Musketeer big data workflow manager [4]. Musketeer automatically generates code for both the sensitive parts of a workflow, which are executed in MPC, and the remainder of the computation, which runs on scalable, widely-deployed analytics systems. In a prototype use case, we compute the Herfindahl-Hirschman Index (HHI), an index of market concentration used in antitrust regulation, on an aggregate 156GB of taxi trip data over five transportation companies. Our implementation computes the HHI in about 20 minutes using a combination of Hadoop and VIFF [1], while even "mixed mode" MPC with VIFF alone would have taken many hours. Finally, we discuss future research questions that we seek to address using our approach.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1844–1846},
numpages = {3},
keywords = {mapreduce, distributed systems, hadoop, multi-party computation, MPC},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/2702613.2732933,
author = {Linder, Rhema and Koh, Eunyee},
title = {Quarry: Picking From Examples to Explore Big Data},
year = {2015},
isbn = {9781450331463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702613.2732933},
doi = {10.1145/2702613.2732933},
abstract = {Analysts use scripts, visualization tools, and spreadsheets as they process and understand data. We focus on two phases of analysts' work: discovery, where the field definitions are understood, and profiling, where assumptions are tested by searching, observing, and running counts on data. Lack of data exploration and understanding can lead to faulty assumptions and misinterpretation. In practice, analysts use SQL queries and scripts to subset big data, reducing it for visualization or spreadsheet pivots. However, due to large-size and high-dimensional data, it is challenging to determine precise subsets of interest without thorough data exploration and discovery. We reduce the cost of previewing subsets by combining search with an information rich visualization of high-dimensional data. To enable discovery and profiling, Quarry supports (1) rapid query generation and visualized search; and (2) defining and previewing subsets of data for potential export for further processing. This work presents the design of Quarry and results from a formative study involving 11 analysts/data scientists and a dataset with 80 columns and 15 million rows.},
booktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1869–1874},
numpages = {6},
keywords = {search, big data, query generation, visualization},
location = {Seoul, Republic of Korea},
series = {CHI EA '15}
}

@inproceedings{10.1145/3361149.3361166,
author = {Moreno, Julio and Fernandez, Eduardo B. and Fernandez-Medina, Eduardo and Serrano, Manuel A.},
title = {BlockBD: A Security Pattern to Incorporate Blockchain in Big Data Ecosystems},
year = {2019},
isbn = {9781450362061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361149.3361166},
doi = {10.1145/3361149.3361166},
abstract = {Big Data is changing the perspective on how to obtain valuable information from data stored by organizations of all kinds. By using these insights, companies can make better decisions and thus achieve their business goals. However, each new technology can create new security problems, and Big Data is no exception. One of the major security issues in a Big Data ecosystem is what level of trust in data and data sources stakeholders can have: without reliable data, the results of data analysis lose value. In this paper, we propose a security pattern to improve traceability and veracity of data through the use of Blockchain technologies. In this pattern, Blockchain will be used as a distributed ledger where all operations performed on the data will be registered. Therefore, the veracity of the data will increase, as will the confidence in the insights obtained from the analysis. The purpose of this paper is to help Chief Security Officer and Big Data architects incorporate this mechanism to improve the security of their environment.},
booktitle = {Proceedings of the 24th European Conference on Pattern Languages of Programs},
articleno = {17},
numpages = {8},
keywords = {blockchain, big data, security pattern},
location = {Irsee, Germany},
series = {EuroPLop '19}
}

@article{10.1145/2996198,
author = {Shankaranarayanan, G. and Blake, Roger},
title = {From Content to Context: The Evolution and Growth of Data Quality Research},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2996198},
doi = {10.1145/2996198},
abstract = {Research in data and information quality has made significant strides over the last 20 years. It has become a unified body of knowledge incorporating techniques, methods, and applications from a variety of disciplines including information systems, computer science, operations management, organizational behavior, psychology, and statistics. With organizations viewing “Big Data”, social media data, data-driven decision-making, and analytics as critical, data quality has never been more important. We believe that data quality research is reaching the threshold of significant growth and a metamorphosis from focusing on measuring and assessing data quality—content—toward a focus on usage and context. At this stage, it is vital to understand the identity of this research area in order to recognize its current state and to effectively identify an increasing number of research opportunities within. Using Latent Semantic Analysis (LSA) to analyze the abstracts of 972 peer-reviewed journal and conference articles published over the past 20 years, this article contributes by identifying the core topics and themes that define the identity of data quality research. It further explores their trends over time, pointing to the data quality dimensions that have—and have not—been well-studied, and offering insights into topics that may provide significant opportunities in this area.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {9},
numpages = {28},
keywords = {Data quality, text mining, information quality}
}

@inproceedings{10.1145/2559206.2560476,
author = {Tanenbaum, Karen and Tanenbaum, Theresa Jean and Williams, Amanda M. and Ratto, Matt and Resch, Gabriel and Gamba Bari, Antonio},
title = {Critical Making Hackathon: Situated Hacking, Surveillance and Big Data Proposal},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2560476},
doi = {10.1145/2559206.2560476},
abstract = {In this workshop we propose to explore issues around big data, data privacy, visualization, sensing, surveillance, and counter-surveillance, through a team-based Critical Making hackathon.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {17–20},
numpages = {4},
keywords = {hardware, open data, sousveillance, surveillance, counter-surveillance, critical making, social issues, big data, data visualization, software, hacking},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}

@inproceedings{10.1145/2749469.2750412,
author = {Jun, Sang-Woo and Liu, Ming and Lee, Sungjin and Hicks, Jamey and Ankcorn, John and King, Myron and Xu, Shuotao and Arvind},
title = {BlueDBM: An Appliance for Big Data Analytics},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2750412},
doi = {10.1145/2749469.2750412},
abstract = {Complex data queries, because of their need for random accesses, have proven to be slow unless all the data can be accommodated in DRAM. There are many domains, such as genomics, geological data and daily twitter feeds where the datasets of interest are 5TB to 20 TB. For such a dataset, one would need a cluster with 100 servers, each with 128GB to 256GBs of DRAM, to accommodate all the data in DRAM. On the other hand, such datasets could be stored easily in the flash memory of a rack-sized cluster. Flash storage has much better random access performance than hard disks, which makes it desirable for analytics workloads. In this paper we present BlueDBM, a new system architecture which has flash-based storage with in-store processing capability and a low-latency high-throughput inter-controller network. We show that BlueDBM outperforms a flash-based system without these features by a factor of 10 for some important applications. While the performance of a ram-cloud system falls sharply even if only 5%~10% of the references are to the secondary storage, this sharp performance degradation is not an issue in BlueDBM. BlueDBM presents an attractive point in the cost-performance trade-off for Big Data analytics.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {1–13},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}

@article{10.1145/2872887.2750412,
author = {Jun, Sang-Woo and Liu, Ming and Lee, Sungjin and Hicks, Jamey and Ankcorn, John and King, Myron and Xu, Shuotao and Arvind},
title = {BlueDBM: An Appliance for Big Data Analytics},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3S},
issn = {0163-5964},
url = {https://doi.org/10.1145/2872887.2750412},
doi = {10.1145/2872887.2750412},
abstract = {Complex data queries, because of their need for random accesses, have proven to be slow unless all the data can be accommodated in DRAM. There are many domains, such as genomics, geological data and daily twitter feeds where the datasets of interest are 5TB to 20 TB. For such a dataset, one would need a cluster with 100 servers, each with 128GB to 256GBs of DRAM, to accommodate all the data in DRAM. On the other hand, such datasets could be stored easily in the flash memory of a rack-sized cluster. Flash storage has much better random access performance than hard disks, which makes it desirable for analytics workloads. In this paper we present BlueDBM, a new system architecture which has flash-based storage with in-store processing capability and a low-latency high-throughput inter-controller network. We show that BlueDBM outperforms a flash-based system without these features by a factor of 10 for some important applications. While the performance of a ram-cloud system falls sharply even if only 5%~10% of the references are to the secondary storage, this sharp performance degradation is not an issue in BlueDBM. BlueDBM presents an attractive point in the cost-performance trade-off for Big Data analytics.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {1–13},
numpages = {13}
}

@article{10.1145/2934664,
author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
title = {Apache Spark: A Unified Engine for Big Data Processing},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/2934664},
doi = {10.1145/2934664},
abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
journal = {Commun. ACM},
month = {oct},
pages = {56–65},
numpages = {10}
}

@article{10.14778/3137765.3137781,
author = {Zhang, Mingming and Wo, Tianyu and Xie, Tao and Lin, Xuelian and Liu, Yaxiao},
title = {CarStream: An Industrial System of Big Data Processing for Internet-of-Vehicles},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137781},
doi = {10.14778/3137765.3137781},
abstract = {As the Internet-of-Vehicles (IoV) technology becomes an increasingly important trend for future transportation, designing large-scale IoV systems has become a critical task that aims to process big data uploaded by fleet vehicles and to provide data-driven services. The IoV data, especially high-frequency vehicle statuses (e.g., location, engine parameters), are characterized as large volume with a low density of value and low data quality. Such characteristics pose challenges for developing real-time applications based on such data. In this paper, we address the challenges in designing a scalable IoV system by describing CarStream, an industrial system of big data processing for chauffeured car services. Connected with over 30,000 vehicles, CarStream collects and processes multiple types of driving data including vehicle status, driver activity, and passenger-trip information. Multiple services are provided based on the collected data. CarStream has been deployed and maintained for three years in industrial usage, collecting over 40 terabytes of driving data. This paper shares our experiences on designing CarStream based on large-scale driving-data streams, and the lessons learned from the process of addressing the challenges in designing and maintaining CarStream.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1766–1777},
numpages = {12}
}

@inproceedings{10.1145/3299869.3314045,
author = {Camacho-Rodr\'{\i}guez, Jes\'{u}s and Chauhan, Ashutosh and Gates, Alan and Koifman, Eugene and O'Malley, Owen and Garg, Vineet and Haindrich, Zoltan and Shelukhin, Sergey and Jayachandran, Prasanth and Seth, Siddharth and Jaiswal, Deepak and Bouguerra, Slim and Bangarwa, Nishant and Hariappan, Sankar and Agarwal, Anishek and Dere, Jason and Dai, Daniel and Nair, Thejas and Dembla, Nita and Vijayaraghavan, Gopal and Hagleitner, G\"{u}nther},
title = {Apache Hive: From MapReduce to Enterprise-Grade Big Data Warehousing},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3314045},
doi = {10.1145/3299869.3314045},
abstract = {Apache Hive is an open-source relational database system for analytic big-data workloads. In this paper we describe the key innovations on the journey from batch tool to fully fledged enterprise data warehousing system. We present a hybrid architecture that combines traditional MPP techniques with more recent big data and cloud concepts to achieve the scale and performance required by today's analytic applications. We explore the system by detailing enhancements along four main axis: Transactions, optimizer, runtime, and federation. We then provide experimental results to demonstrate the performance of the system for typical workloads and conclude with a look at the community roadmap.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1773–1786},
numpages = {14},
keywords = {databases, hadoop, hive, data warehouses},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.5555/3191835.3191963,
author = {Jia, Yantao and Wang, Yuanzhuo and Cheng, Xueqi and Jin, Xiaolong and Guo, Jiafeng},
title = {OpenKN: An Open Knowledge Computational Engine for Network Big Data},
year = {2014},
isbn = {9781479958764},
publisher = {IEEE Press},
abstract = {With the coming of the era of big data, it is most urgent to establish the knowledge computational engine for the purpose of discovering implicit and valuable knowledge from the huge, rapidly dynamic, and complex network data. In this paper, we first survey the mainstream knowledge computational engines from four aspects and point out their deficiency. To cover these shortages, we propose the open knowledge network (OpenKN), which is a self-adaptive and evolutionable knowledge computational engine for network big data. To the best of our knowledge, this is the first work of designing the end-to-end and holistic knowledge processing pipeline in regard with the network big data. Moreover, to capture the evolutionable computing capability of OpenKN, we present the evolutionable knowledge network for knowledge representation. A case study demonstrates the effectiveness of the evolutionable computing of OpenKN.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {657–664},
numpages = {8},
keywords = {knowledge computational engine, open knowledge network, evolutionable knowledge network, evolutionable computing},
location = {Beijing, China},
series = {ASONAM '14}
}

@inproceedings{10.1145/1869790.1869884,
author = {Lassoued, Yassine and Bouadjenek, Mohamed Reda and Boucelma, Omar and Lemos, Fernando and Bouzeghoub, Mokrane},
title = {GQBox: Geospatial Data Quality Assessment},
year = {2010},
isbn = {9781450304283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869790.1869884},
doi = {10.1145/1869790.1869884},
abstract = {In order to measure and assess the quality of GIS, there exist a sparse offer of tools, providing specific functions with their own interest but are not sufficient to deal with broader user's requirements. Interoperability of these tools remains a technical challenge because of the heterogeneity of their models and access patterns. On the other side, quality analysts require more and more integration facilities that allow them to consolidate and aggregate multiple quality measures acquired from different observations or data sources, in using/combining seamlessly different quality tools. Clearly, there is a gap between users's requirements and the spatial data quality market. This demo paper will illustrate GQBox, a geographic quality (tool)box. GQBox supplies a standards-based generic meta model that supports the definition of quality goals and metrics, and it provides a service-based infrastructure that allows interoperability among several quality tools.},
booktitle = {Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {534–535},
numpages = {2},
keywords = {web services, geospatial data quality},
location = {San Jose, California},
series = {GIS '10}
}

@inproceedings{10.1145/2609876.2609883,
author = {Thomson, Robert and Lebiere, Christian and Bennati, Stefano},
title = {Human, Model and Machine: A Complementary Approach to Big Data},
year = {2014},
isbn = {9781450329385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609876.2609883},
doi = {10.1145/2609876.2609883},
abstract = {In this paper, we describe a framework for processing big data that maximizing the efficiency of human data scientists by having them primarily operate over information that is best structured to human processing demands. We accomplish this through the use of cognitive models as an intermediary between machine learning algorithms and human data scientists. The ACT-R cognitive architecture is a computational implementation of a unified theory of cognition. ACT-R cognitive models can take weakly structured data and learn to filter information and make accurate inferences orders of magnitude faster than machine learning, and then present these well-structured inferences to human data scientists. The role for human data scientists is both oversight and feedback; one complementary piece of a hierarchy of cognitive and machine learning techniques that are computationally appropriate for their level of information complexity.},
booktitle = {Proceedings of the 2014 Workshop on Human Centered Big Data Research},
pages = {27–31},
numpages = {5},
keywords = {Cognitive architectures, deep learning, Big Data, ACT-R},
location = {Raleigh, NC, USA},
series = {HCBDR '14}
}

@inproceedings{10.5555/3400397.3400442,
author = {Vieira, Ant\'{o}nio A. C. and Dias, Lu\'{\i}s M. S. and Santos, Maribel Y. and Pereira, Guilherme A. B. and Oliveira, Jos\'{e} A.},
title = {Real-Time Supply Chain Simulation: A Big Data-Driven Approach},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Simulation of Supply Chains comprises huge amounts of data, resulting in numerous entities flowing in the model. These networks are highly dynamic systems, where entities' relationships and other elements evolve with time, paving the way for real-time Supply Chain decision-support tools capable of using real data. In light of this, a solution comprising of a Big Data Warehouse to store relevant data and a simulation model of an automotive plant, are being developed. The purpose of this paper is to address the modelling approach, which allowed the simulation model to automatically adapt to the data stored in a Big Data Warehouse and thus adapt to new scenarios without manual intervention. The main characteristics of the conceived solution were demonstrated, with emphasis to the real-time and the ability to allow the model to load the state of the system from the Big Data Warehouse.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {548–559},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/2940116.2940117,
author = {Casas, Pedro and D'Alconzo, Alessandro and Zseby, Tanja and Mellia, Marco},
title = {Big-DAMA: Big Data Analytics for Network Traffic Monitoring and Analysis},
year = {2016},
isbn = {9781450344265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2940116.2940117},
doi = {10.1145/2940116.2940117},
abstract = {The complexity of the Internet has dramatically increased in the last few years, making it more important and challenging to design scalable Network Traffic Monitoring and Analysis (NTMA) applications and tools. Critical NTMA applications such as the detection of anomalies, network attacks and intrusions, require fast mechanisms for online analysis of thousands of events per second, as well as efficient techniques for offline analysis of massive historical data. We are witnessing a major development in Big Data Analysis Frameworks (BDAFs), but the application of BDAFs and scalable analysis techniques to the NTMA domain remains poorly understood and only in-house and difficult to benchmark solutions are conceived. In this position paper we describe the basis of the Big-DAMA research project, which aims at tackling this growing need by benchmarking and developing novel scalable techniques and frameworks capable to analyze both online network traffic data streams and offline massive traffic datasets.},
booktitle = {Proceedings of the 2016 Workshop on Fostering Latin-American Research in Data Communication Networks},
pages = {1–3},
numpages = {3},
keywords = {Data Stream Processing, Data Mining, Network Traffic Monitoring and Analysis, Machine Learning, Big Data},
location = {Florianopolis, Brazil},
series = {LANCOMM '16}
}

@inproceedings{10.1145/3340964.3340988,
author = {Petrou, Petros and Nikitopoulos, Panagiotis and Tampakis, Panagiotis and Glenis, Apostolos and Koutroumanis, Nikolaos and Santipantakis, Georgios M. and Patroumpas, Kostas and Vlachou, Akrivi and Georgiou, Harris and Chondrodima, Eva and Doulkeridis, Christos and Pelekis, Nikos and Andrienko, Gennady L. and Patterson, Fabian and Fuchs, Georg and Theodoridis, Yannis and Vouros, George A.},
title = {ARGO: A Big Data Framework for Online Trajectory Prediction},
year = {2019},
isbn = {9781450362801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340964.3340988},
doi = {10.1145/3340964.3340988},
abstract = {We present a big data framework for the prediction of streaming trajectory data, enriched from other data sources and exploiting mined patterns of trajectories, allowing accurate long-term predictions with low latency. To meet this goal, we follow a multi-step methodology. First, we efficiently compress surveillance data in an online fashion, by constructing trajectory synopses that are spatio-temporally linked with streaming and archival data from a variety of diverse and heterogeneous data sources. The enriched stream of trajectory synopses is stored in a distributed RDF store, supporting data exploration via SPARQL queries. The enriched stream of synopses along with the raw data is consumed by trajectory prediction algorithms that exploit mined patterns from the RDF store, namely medoids of (sub-) trajectory clusters, which prolong the horizon of useful predictions. The framework is extended with offline and online interactive visual analytics tool to facilitate real world analysis in the maritime and the aviation domains.},
booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
pages = {194–197},
numpages = {4},
keywords = {trajectories, mobility events, location prediction, geostreaming},
location = {Vienna, Austria},
series = {SSTD '19}
}

@inproceedings{10.1145/2559206.2567828,
author = {Chau, Duen Horng (Polo)},
title = {HCI Meets Data Mining: Principles and Tools for Big Data Analytics},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2567828},
doi = {10.1145/2559206.2567828},
abstract = {This two-part course takes a practical approach to introduce you to the principles, tools and pitfalls in big data analytics. Part 1: A non-technical introduction illustrating where HCI and data mining as fields of research and practice can benefit from each other with illustrative case studies, followed by a review of tools for analyzing datasets from small to huge. Part 2: A more technical discussion of how to "do it right", such as: How to choose a "big data" platform for your work (or do you need one at all)? How to find an algorithm that is right for your data? How to evaluate your approach appropriately? And more... Audience: HCI researchers, practitioners, and students. No prior knowledge of data mining or machine learning is required. Teaching Methods: Lecture and videos. Instructor Background: Prof. Polo Chau has been working at the intersection of HCI and data mining for over 9 years, to create scalable, interactive tools for big data analytics. Now a professor at Georgia Tech's College of Computing, Polo holds a Ph.D. in Machine Learning and a Masters in HCI, both from Carnegie Mellon. His thesis on bridging HCI and data mining for making sense of large network data won received Carnegie Mellon's Distinguished Computer Science Dissertation Award, Honorable Mention. He teaches the "Data and Visual Analytics" course at Georgia Tech. Polo is the only two-time Symantec fellow. He contributes to the PEGASUS peta-scale graph mining that won an Open Source Software World Challenge Silver Award. Polo's NetProbe auction fraud detection research appeared on The Wall Street Journal, CNN, TV and radio. His Polonium malware detection technology protects 120 million people worldwide.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {1039–1040},
numpages = {2},
keywords = {CHI course},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}

@inproceedings{10.1145/2897839.2927468,
author = {Root, Christopher and Mostak, Todd},
title = {MapD: A GPU-Powered Big Data Analytics and Visualization Platform},
year = {2016},
isbn = {9781450342827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897839.2927468},
doi = {10.1145/2897839.2927468},
abstract = {MapD, or "Massively Parallel Database", is a big data analytics platform that can query and visualize big data up to 100x faster than other systems. It leverages the massive parallelism of commodity GPUs to execute SQL queries over multi-billion row datasets with millisecond response times, and optionally render the results using the GPU's native graphics pipeline. Depending on the use case, MapD can be used as a standalone SQL database or as a data visualization suite by using its own visualization frontend (see Fig. 1) or by integrating it with other third-party toolkits.},
booktitle = {ACM SIGGRAPH 2016 Talks},
articleno = {73},
numpages = {2},
keywords = {database, data visualization, GPU, CUDA, analytics, OpenGL},
location = {Anaheim, California},
series = {SIGGRAPH '16}
}

@article{10.1145/2693208.2693228,
author = {Caldarola, Enrico Giacinto and Picariello, Antonio and Castelluccia, Daniela},
title = {Modern Enterprises in the Bubble: Why Big Data Matters},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2693208.2693228},
doi = {10.1145/2693208.2693228},
abstract = {In the past few years, a massive amount of data has been delivered by an increased and ubiquitous use of Information and Communication Technologies (ICTs) in human activities and a propagation of smart devices or smart sensors, which continuously connect people and things in cyberspace. This huge bubble of data is a gold mine: it is an unlimited source of knowledge and insights about habits and preferences of people and has captured the attention of modern enterprises. Companies look at this data with interest and purpose to gain competitive advantage by applying analytics tools over them. In this context, a new approach is required for mastering data without the risk of ending up in the bubble and collecting a huge meaningless pile of junk data. Starting from a definition of new approaches, this work outlines Big Data strategies for modern enterprises and highlights challenges, emergent solutions and open issues.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {1–4},
numpages = {4},
keywords = {Business Intelligence, data storage, data integration, Big Data, Knowledge management, data analytics, Enterprise 2.0}
}

@inproceedings{10.1145/3322431.3326448,
author = {Awaysheh, Feras M. and Cabaleiro, Jos\'{e} C. and Pena, Tom\'{a}s F. and Alazab, Mamoun},
title = {Poster: A Pluggable Authentication Module for Big Data Federation Architecture},
year = {2019},
isbn = {9781450367530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322431.3326448},
doi = {10.1145/3322431.3326448},
abstract = {This paper intends to propose a trustworthy model for authenticating users and services over a Big Data Federation deployment architecture. The main goal of this model is to provide a Single-Sign-on (SSO) approach for the latest Hadoop 3.x platform. To achieve this, a conceptual model is proposed combining Hadoop access control primitives and the Apache Knox framework. The paper provides various insights regarding the latest ongoing developments and open challenges in this domain.},
booktitle = {Proceedings of the 24th ACM Symposium on Access Control Models and Technologies},
pages = {223–225},
numpages = {3},
keywords = {sso reference model, authentication, big data, hdfs federation},
location = {Toronto ON, Canada},
series = {SACMAT '19}
}

@inproceedings{10.5555/3026877.3026905,
author = {Nguyen, Khanh and Fang, Lu and Xu, Guoqing and Demsky, Brian and Lu, Shan and Alamian, Sanazsadat and Mutlu, Onur},
title = {Yak: A High-Performance Big-Data-Friendly Garbage Collector},
year = {2016},
isbn = {9781931971331},
publisher = {USENIX Association},
address = {USA},
abstract = {Most "Big Data" systems are written in managed languages, such as Java, C#, or Scala. These systems suffer from severe memory problems due to the massive volume of objects created to process input data. Allocating and deallocating a sea of data objects puts a severe strain on existing garbage collectors (GC), leading to high memory management overheads and reduced performance.This paper describes the design and implementation of Yak, a "Big Data" friendly garbage collector that provides high throughput and low latency for all JVM-based languages. Yak divides the managed heap into a control space (CS) and a data space (DS), based on the observation that a typical data-intensive system has a clear distinction between a control path and a data path. Objects created in the control path are allocated in the CS and subject to regular tracing GC. The lifetimes of objects in the data path often align with epochs creating them. They are thus allocated in the DS and subject to region-based memory management. Our evaluation with three large systems shows very positive results.},
booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
pages = {349–365},
numpages = {17},
location = {Savannah, GA, USA},
series = {OSDI'16}
}

@inproceedings{10.1145/3299869.3323670,
author = {Groppe, Sven and Gruenwald, Le},
title = {SBD'19: Fourth Edition of the International Workshop on Semantic Big Data},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3323670},
doi = {10.1145/3299869.3323670},
abstract = {The International Workshop on Semantic Big Data (SBD) is already in the fourth year in conjunction with the ACM SIGMOD Conference. By focusing on topics related to Semantic Web and Big Data, it highlights these important research areas to all participants of the SBD workshop and the main conference. Hence, the workshop offers an ideal forum for open discussions and establishment of professional relationships between academic researchers and industrial members of the Semantic Web and database communities.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {2075–2076},
numpages = {2},
keywords = {semantic web, big data},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/2536146.2536149,
author = {Soylu, Ahmet and Giese, Martin and Jimenez-Ruiz, Ernesto and Kharlamov, Evgeny and Zheleznyakov, Dmitry and Horrocks, Ian},
title = {OptiqueVQS: Towards an Ontology-Based Visual Query System for Big Data},
year = {2013},
isbn = {9781450320047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536146.2536149},
doi = {10.1145/2536146.2536149},
abstract = {A recent EU project, named Optique, with a strong industrial perspective, strives to enable scalable end-user access to Big Data. To this end, Optique employs an ontology-based approach, along with other techniques such as query optimisation and parallelisation, for scalable query formulation and evaluation. In this paper, we specifically focus on end-user visual query formulation, demonstrate our preliminary ontology-based visual query system (i.e., interface), and discuss initial insights for alleviating the affects of Big Data.},
booktitle = {Proceedings of the Fifth International Conference on Management of Emergent Digital EcoSystems},
pages = {119–126},
numpages = {8},
keywords = {visual query systems, big data, ontologies},
location = {Luxembourg, Luxembourg},
series = {MEDES '13}
}

@inproceedings{10.1145/3173162.3173200,
author = {Nguyen, Khanh and Fang, Lu and Navasca, Christian and Xu, Guoqing and Demsky, Brian and Lu, Shan},
title = {Skyway: Connecting Managed Heaps in Distributed Big Data Systems},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173200},
doi = {10.1145/3173162.3173200},
abstract = {Managed languages such as Java and Scala are prevalently used in development of large-scale distributed systems. Under the managed runtime, when performing data transfer across machines, a task frequently conducted in a Big Data system, the system needs to serialize a sea of objects into a byte sequence before sending them over the network. The remote node receiving the bytes then deserializes them back into objects. This process is both performance-inefficient and labor-intensive: (1) object serialization/deserialization makes heavy use of reflection, an expensive runtime operation and/or (2) serialization/deserialization functions need to be hand-written and are error-prone. This paper presents Skyway, a JVM-based technique that can directly connect managed heaps of different (local or remote) JVM processes. Under Skyway, objects in the source heap can be directly written into a remote heap without changing their formats. Skyway provides performance benefits to any JVM-based system by completely eliminating the need (1) of invoking serialization/deserialization functions, thus saving CPU time, and (2) of requiring developers to hand-write serialization functions.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {56–69},
numpages = {14},
keywords = {distributed systems, big data, serialization and deserialization, data transfer},
location = {Williamsburg, VA, USA},
series = {ASPLOS '18}
}

@article{10.1145/3296957.3173200,
author = {Nguyen, Khanh and Fang, Lu and Navasca, Christian and Xu, Guoqing and Demsky, Brian and Lu, Shan},
title = {Skyway: Connecting Managed Heaps in Distributed Big Data Systems},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296957.3173200},
doi = {10.1145/3296957.3173200},
abstract = {Managed languages such as Java and Scala are prevalently used in development of large-scale distributed systems. Under the managed runtime, when performing data transfer across machines, a task frequently conducted in a Big Data system, the system needs to serialize a sea of objects into a byte sequence before sending them over the network. The remote node receiving the bytes then deserializes them back into objects. This process is both performance-inefficient and labor-intensive: (1) object serialization/deserialization makes heavy use of reflection, an expensive runtime operation and/or (2) serialization/deserialization functions need to be hand-written and are error-prone. This paper presents Skyway, a JVM-based technique that can directly connect managed heaps of different (local or remote) JVM processes. Under Skyway, objects in the source heap can be directly written into a remote heap without changing their formats. Skyway provides performance benefits to any JVM-based system by completely eliminating the need (1) of invoking serialization/deserialization functions, thus saving CPU time, and (2) of requiring developers to hand-write serialization functions.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {56–69},
numpages = {14},
keywords = {serialization and deserialization, big data, distributed systems, data transfer}
}

@inproceedings{10.1145/2597917.2597921,
author = {Pan, Fengfeng and Yue, Yinliang and Xiong, Jin},
title = {Chameleon: A Data Organization Transformation Scheme for Big Data Systems},
year = {2014},
isbn = {9781450328708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597917.2597921},
doi = {10.1145/2597917.2597921},
abstract = {Big data system requires multiple types of data organizations to efficiently support various operations. It is well known that in-place update index, unordered log structured index and ordered log structured index are three typical data organizations which are designed to meet different workload requirements respectively. Differentiated workload requirements in different phase of the data life-cycle lead to data organization transformation. However, typical sequential data organization transformation not only incurs extremely long time, but also significant energy consumption.In this paper, we propose Chameleon, a novel data organization transformation scheme for replication based big data system. The goal of Chameleon is to significantly shorten the data organization transformation process and improve the write performance and the subsequent read performance through data organization transformation, meanwhile eliminate the additional hardware and energy costs by reusing the mirrored disks. For each put request, Chameleon keeps two copies of the key-value pair. One in its normal place and organized in ordered log structured index, and the other in relatively high performance log disk and organized in unordered log structured index. By spreading destaging I/O activities among short idle time slots, key-value pairs are transformed from write-optimized index to read-optimized index. Extensive experimental evaluation based on our prototype shows that Chameleon can shorten the time of data organization transformation and enhance energy efficiency and performance.},
booktitle = {Proceedings of the 11th ACM Conference on Computing Frontiers},
articleno = {19},
numpages = {10},
keywords = {in-place update index, data organization transformation, ordered log structured index, data translation, big data system, unordered log structured index, data organization},
location = {Cagliari, Italy},
series = {CF '14}
}

@inproceedings{10.1145/2640087.2644169,
author = {Shi, Justin Y.},
title = {Symposium: CAP-Plus for Big Data},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644169},
doi = {10.1145/2640087.2644169},
abstract = {Data intensive parallel applications are harder to protect against transient software and hardware failures compared to traditional parallel applications. Due to the need for distributed data replication, the CAP Conjecture and Theorem define the ultimate limits for data intensive application's reliability, availability and overall scalability. This paper examines the two assumptions in the proof of CAP Theorem and proposes a statistic multiplexing paradigm for eliminating the reliability, availability and scalability limits of data intensive parallel applications.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {19},
numpages = {5},
keywords = {Statistic Multiplexed Computing, CAP Theorem, Unlimited Scalability of Extreme Scale Data Intensive Application, CAP Conjectur},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/3184558.3186984,
author = {Argyriou, Giorgos and Papadakis, George and Stamoulis, George and Karra Taniskidou, Efi and Pittaras, Nikiforos and Giannakopoulos, George and Albani, Sergio and Lazzarini, Michele and Angiuli, Emanuele and Popescu, Anca and Argyridis, Argyros and Koubarakis, Manolis},
title = {GeoSensor: On-Line Scalable Change and Event Detection over Big Data},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186984},
doi = {10.1145/3184558.3186984},
abstract = {GeoSensor is a novel system that enriches change detection over satellite images with event detection over news items and social media content. GeoSensor faces the major challenges of Big Data: volume (a single satellite image may be a few GBs), variety (its data sources include two different types of satellite images and various types of user-generated content) and veracity, as the accuracy of the end result is crucial for the usefulness of our system. To overcome these three challenges, while offering on-line functionality, GeoSensor comprises a complex architecture that is based on the open-source platform developed in the H2020 project Big Data Europe. Through the presented demonstration, both the effectiveness and the efficiency of GeoSensor's functionalities are highlighted.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {223–226},
numpages = {4},
keywords = {big data, satellite image processing, event detection, change detection, semantic web},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.5555/3330299.3330317,
author = {de Almeida, Leandro Batista and de Almeida, Eduardo Cunha and Murphy, John and De Grande, Robson E. and Ventresque, Anthony},
title = {BigDataNetSim: A Simulator for Data and Process Placement in Large Big Data Platforms},
year = {2018},
isbn = {9781538650486},
publisher = {IEEE Press},
abstract = {Big Data platforms are convoluted distributed systems which commonly comprise skill- and labour-intensive solution development to treat inherent Big Data application challenges. Several tools have been proposed to help developers and engineers to overcome the involved complexities in coordinating the execution of plenty processes/threads on multiple machines. However, no work so far has been able to combine both an accurate representation of Big Data jobs and realistic modeling of the behaviour of Big Data platforms at scale, including networking elements and data and job placement. In this paper, we propose BigDataNetSim, the first simulator which models accurately all the main components of the data movements in Big Data platforms (e.g., HDFS, YARN/MapReduce, network topologies, switching/routing protocols) in a large scale system. BigDataNetSim can serve as a valuable tool for engineering Big Data solutions, which includes set-up of systems, prototyping of jobs, and improvement of components/algorithms for Big Data platforms. We also demonstrate that BigDataNetSim can simulate a real Hadoop cluster with a high degree of accuracy in terms of data and job placements, being able to scale up to very large systems.},
booktitle = {Proceedings of the 22nd International Symposium on Distributed Simulation and Real Time Applications},
pages = {145–154},
numpages = {10},
keywords = {simulation, Hadoop, big data},
location = {Madrid, Spain},
series = {DS-RT '18}
}

@inproceedings{10.1145/3362789.3362841,
author = {Navarro, Joan and Zaballos, Agust\'{\i}n and Fonseca, David and Torres-Kompen, Ricardo},
title = {Master as a Service: A Multidisciplinary Approach to Big Data Teaching},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362841},
doi = {10.1145/3362789.3362841},
abstract = {The recent and rapid growth of data-driven applications, fostered by the advent of enhanced Information and Communication Technologies (ICTs) together with the broad availability of modern high-performance storage and computing infrastructures, has created a considerable gap of experts in this new field. The quick evolution of these technologies, their dissimilarities with traditional approaches, and the broad skills set required to master them, might prevent existing professionals working in industry to gain high quality knowledge and experience in Big Data related areas. Therefore, universities and teaching professionals must propose feasible and effective alternatives to train professionals and students in these topics. The purpose of this paper is to present the Master as a Service (MaaS) approach, that is currently being used to train students in Big Data- related areas (e.g., eHealth, Digital Transformation, etc.), following a multidisciplinary, Project Based Learning strategy. More specifically, students coming from different master's degrees and undergraduate backgrounds (ranging from management studies to computer engineering, including architects, social and physical sciences) are trained to address latent and future challenges in Big Data and High-Performance Computing technologies by combining their profiles, and exposing them to real-world challenges that require the very best of each different profile. The results obtained from the implementation of the MaaS approach during the last two years in terms of both student satisfaction and employability rate, confirm the benefits of this method and encourage practitioners to keep working in this direction.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {534–538},
numpages = {5},
keywords = {Multidisciplinary teaching, Cloud computing, Data analytics, Big data},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@inproceedings{10.1145/3297280.3297564,
author = {de Aquino, Gabriel R. Caldas and de Farias, Claudio M. and Pirmez, Luci},
title = {Hygieia: Data Quality Assessment for Smart Sensor Network},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297564},
doi = {10.1145/3297280.3297564},
abstract = {Smart Sensor Networks (SSN) are the core data transmission infrastructure of the Internet of Things. However, SSN devices are resource-constrained and the environment in which the SSN is deployed impose data quality challenges. Data quality refers to the fitness of the data for the data consumer. In this case, mechanisms to assess the data quality for IoT data in network are demanded. In this work we propose Hygieia: a data quality assessment mechanism for constrained SSN devices. Hygieia is designed to impose low memory overhead, low network communication overhead and to not impose a significant delay from its data input to its data quality assessment output. We conducted real node experiments.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {889–891},
numpages = {3},
keywords = {smart sensor networks, internet of things, data quality},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/2538862.2538949,
author = {Buffum, Philip Sheridan and Martinez-Arocho, Allison G. and Frankosky, Megan Hardy and Rodriguez, Fernando J. and Wiebe, Eric N. and Boyer, Kristy Elizabeth},
title = {CS Principles Goes to Middle School: Learning How to Teach "Big Data"},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538862.2538949},
doi = {10.1145/2538862.2538949},
abstract = {Spurred by evidence that students' future studies are highly influenced during middle school, recent efforts have seen a growing emphasis on introducing computer science to middle school learners. This paper reports on the in-progress development of a new middle school curricular module for Big Data, situated as part of a new CS Principles-based middle school curriculum. Big Data is of widespread societal importance and holds increasing implications for the computer science workforce. It also has appeal as a focus for middle school computer science because of its rich interplay with other important computer science principles. This paper examines three key aspects of a Big Data unit for middle school: its alignment with emerging curricular standards; the perspectives of middle school classroom teachers in mathematics, science, and language arts; and student feedback as explored during a middle school pilot study with a small subset of the planned curriculum. The results indicate that a Big Data unit holds great promise as part of a middle school computer science curriculum.},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
pages = {151–156},
numpages = {6},
keywords = {big data, middle school, computer science education},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1145/2764947.2764954,
author = {Capot\u{a}, Mihai and Hegeman, Tim and Iosup, Alexandru and Prat-P\'{e}rez, Arnau and Erling, Orri and Boncz, Peter},
title = {Graphalytics: A Big Data Benchmark for Graph-Processing Platforms},
year = {2015},
isbn = {9781450336116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2764947.2764954},
doi = {10.1145/2764947.2764954},
abstract = {Graphs are increasingly used in industry, governance, and science. This has stimulated the appearance of many and diverse graph-processing platforms. Although platform diversity is beneficial, it also makes it very challenging to select the best platform for an application domain or one of its important applications, and to design new and tune existing platforms. Continuing a long tradition of using benchmarking to address such challenges, in this work we present our vision for Graphalytics, a big data benchmark for graph-processing platforms. We have already benchmarked with Graphalytics a variety of popular platforms, such as Giraph, GraphX, and Neo4j.},
booktitle = {Proceedings of the GRADES'15},
articleno = {7},
numpages = {6},
location = {Melbourne, VIC, Australia},
series = {GRADES'15}
}

@inproceedings{10.5555/2874916.2874964,
author = {Tolk, Andreas},
title = {The next Generation of Modeling &amp; Simulation: Integrating Big Data and Deep Learning},
year = {2015},
isbn = {9781510810594},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Big data allows users to cope with data that are huge in regards to volume, velocity, variety, and veracity. It provides methods and tools to extract aggregates and new information out of heterogeneously structured data or even completely unstructured data. Deep learning is a collection of algorithms that allows us to discover correlations and learn --- supervised and unsupervised --- from information provided. This contribution introduces the main ideas and methods of big data and deep learning and shows how they can be applied to various phases of the traditional modeling and simulation process. Big data supports obtaining data for the initialization as well as evaluating the results of the simulation experiment. Deep learning can help with the conceptual modeling phase as well as with the discovery of correlations in the results. Examples of existing applications will be given to prove the feasibility of such ideas. This leads to the observation that big data, deep learning, and modeling and simulation have the potential to lead to a new generation of modeling and simulation applications that provide computational scientific support on a new scale beyond the current capabilities.},
booktitle = {Proceedings of the Conference on Summer Computer Simulation},
pages = {1–8},
numpages = {8},
keywords = {big data, simulation, deep learning, artificial intelligence, modeling},
location = {Chicago, Illinois},
series = {SummerSim '15}
}

@inproceedings{10.1145/2649387.2649439,
author = {Phan, John H. and Kothari, Sonal and Wang, May D.},
title = {OmniClassifier: A Desktop Grid Computing System for Big Data Prediction Modeling},
year = {2014},
isbn = {9781450328944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2649387.2649439},
doi = {10.1145/2649387.2649439},
abstract = {Robust prediction models are important for numerous science, engineering, and biomedical applications. However, best-practice procedures for optimizing prediction models can be computationally complex, especially when choosing models from among hundreds or thousands of parameter choices. Computational complexity has further increased with the growth of data in these fields, concurrent with the era of "Big Data". Grid computing is a potential solution to the computational challenges of Big Data. Desktop grid computing, which uses idle CPU cycles of commodity desktop machines, coupled with commercial cloud computing resources can enable research labs to gain easier and more cost effective access to vast computing resources. We have developed omniClassifier, a multi-purpose prediction modeling application that provides researchers with a tool for conducting machine learning research within the guidelines of recommended best-practices. omniClassifier is implemented as a desktop grid computing system using the Berkeley Open Infrastructure for Network Computing (BOINC) middleware. In addition to describing implementation details, we use various gene expression datasets to demonstrate the potential scalability of omniClassifier for efficient and robust Big Data prediction modeling. A prototype of omniClassifier can be accessed at http://omniclassifier.bme.gatech.edu/.},
booktitle = {Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {514–523},
numpages = {10},
keywords = {big data, nested cross validation, desktop grid computing, prediction modeling},
location = {Newport Beach, California},
series = {BCB '14}
}

@inproceedings{10.1145/3299874.3319483,
author = {Imani, Mohsen and Gupta, Saransh and Kim, Yeseong and Zhou, Minxuan and Rosing, Tajana},
title = {DigitalPIM: Digital-Based Processing In-Memory for Big Data Acceleration},
year = {2019},
isbn = {9781450362528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299874.3319483},
doi = {10.1145/3299874.3319483},
abstract = {In this work, we design, DigitalPIM, a Digital-based Processing In-Memory platform capable of accelerating fundamental big data algorithms in real time with orders of magnitude more energy efficient operation. Unlike the existing near-data processing approach such as HMC 2.0, which utilizes additional low-power processing cores next to memory blocks, the proposed platform implements the entire algorithm directly in memory blocks without using extra processing units. In our platform, each memory block supports the essential operations including: bitwise operation, addition/multiplication, and search operation internally in memory without reading any values out of the block. This significantly mitigates the processing costs of the new architecture, while providing high scalability and parallelism for performing the extensive computations. We exploit these essential operations to accelerate popular big data applications entirely in memory such as machine learning algorithms, query processing, and graph processing. Our evaluations show that for all tested applications, the performance can be accelerated significantly by eliminating the memory access bottleneck},
booktitle = {Proceedings of the 2019 on Great Lakes Symposium on VLSI},
pages = {429–434},
numpages = {6},
keywords = {non-volatile memories, processing in memory, big data acceleration, energy efficiency},
location = {Tysons Corner, VA, USA},
series = {GLSVLSI '19}
}

@inproceedings{10.1145/3093338.3093375,
author = {DeYoung, Mark E. and Salman, Mohammed and Bedi, Himanshu and Raymond, David and Tront, Joseph G.},
title = {Spark on the ARC: Big Data Analytics Frameworks on HPC Clusters},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3093375},
doi = {10.1145/3093338.3093375},
abstract = {In this paper we document our approach to overcoming service discovery and configuration of Apache Hadoop and Spark frameworks with dynamic resource allocations in a batch oriented Advanced Research Computing (ARC) High Performance Computing (HPC) environment. ARC efforts have produced a wide variety of HPC architectures. A common HPC architectural pattern is multi-node compute clusters with low-latency, high-performance interconnect fabrics and shared central storage. This pattern enables processing of workloads with high data co-dependency, frequently solved with message passing interface (MPI) programming models, and then executed as batch jobs. Unfortunately, many HPC programming paradigms are not well suited to big data workloads which are often easily separable. Our approach lowers barriers of entry to HPC environments by enabling end users to utilize Apache Hadoop and Spark frameworks that support big data oriented programming paradigms appropriate for separable workloads in batch oriented HPC environments.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {34},
numpages = {6},
keywords = {Advanced Research Computing, Distributed Computing, Big Data, Applied Computing, High Performance Computing},
location = {New Orleans, LA, USA},
series = {PEARC17}
}

