@inproceedings{10.1145/2463676.2463724,
author = {Suchanek, Fabian and Weikum, Gerhard},
title = {Knowledge Harvesting in the Big-Data Era},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2463724},
doi = {10.1145/2463676.2463724},
abstract = {The proliferation of knowledge-sharing communities such as Wikipedia and the progress in scalable information extraction from Web and text sources have enabled the automatic construction of very large knowledge bases. Endeavors of this kind include projects such as DBpedia, Freebase, KnowItAll, ReadTheWeb, and YAGO. These projects provide automatically constructed knowledge bases of facts about named entities, their semantic classes, and their mutual relationships. They contain millions of entities and hundreds of millions of facts about them. Such world knowledge in turn enables cognitive applications and knowledge-centric services like disambiguating natural-language text, semantic search for entities and relations in Web and enterprise data, and entity-oriented analytics over unstructured contents. Prominent examples of how knowledge bases can be harnessed include the Google Knowledge Graph and the IBM Watson question answering system. This tutorial presents state-of-the-art methods, recent advances, research opportunities, and open challenges along this avenue of knowledge harvesting and its applications. Particular emphasis will be on the twofold role of knowledge bases for big-data analytics: using scalable distributed algorithms for harvesting knowledge from Web and text sources, and leveraging entity-centric knowledge for deeper interpretation of and better intelligence with Big Data.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {933–938},
numpages = {6},
keywords = {entity recognition, information extraction, knowledge base, ontology, web contents, big data},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2612733.2612780,
author = {Harrison, Teresa M.},
title = {Using Big Data for Digital Government Research},
year = {2014},
isbn = {9781450329019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2612733.2612780},
doi = {10.1145/2612733.2612780},
abstract = {"Big data" has captured the imagination of e-government researchers as the source of potential advances in government innovation, strategy, and policy, and as the basis for entirely new approaches to research investigations across the disciplines. Digital data is everywhere, and, it is thought, considerable value may be obtained in analyzing the digital traces generated by internet users as they traverse social, political, economic and material spaces. However, once past the seminal anecdotes (e.g, estimates of flu outbreaks based on Google searches), generating knowledge from big data presents complexities along with its potentials. In this panel, we bring together researchers to share their experiences with big data projects in e-government, considering the research questions they are asking, the opportunities and complications they are encountering, and the new strategies or responses they are creating in response.},
booktitle = {Proceedings of the 15th Annual International Conference on Digital Government Research},
pages = {309–310},
numpages = {2},
keywords = {policy informatics, social media, automated linguistic analysis, political science, social networks, big data, project X haren, Facebook, communication, 2008 financial crisis},
location = {Aguascalientes, Mexico},
series = {dg.o '14}
}

@inproceedings{10.1145/3423603.3424056,
author = {Tekaya, Balkiss and Feki, Sirine El and Tekaya, Tasnim and Masri, Hela},
title = {Recent Applications of Big Data in Finance},
year = {2020},
isbn = {9781405377539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423603.3424056},
doi = {10.1145/3423603.3424056},
abstract = {The financial sector generates tremendous amounts of data daily. Consequently, more attention is being focused on transforming that data into actionable knowledge. Big data and data science techniques have revolutionized the business world; the focus of this paper is to study the extent of this transformation in the financial field by considering research works discussing big data in financial markets, banking, credit risk management, fraud detection and insurance and illustrating some real life applications and challenges of this technology.},
booktitle = {Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress},
articleno = {19},
numpages = {6},
keywords = {data science, big data, finance, banking},
location = {Virtual Event, Tunisia},
series = {DTUC '20}
}

@inproceedings{10.1145/2487575.2487588,
author = {Raman, Karthik and Swaminathan, Adith and Gehrke, Johannes and Joachims, Thorsten},
title = {Beyond Myopic Inference in Big Data Pipelines},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487588},
doi = {10.1145/2487575.2487588},
abstract = {Big Data Pipelines decompose complex analyses of large data sets into a series of simpler tasks, with independently tuned components for each task. This modular setup allows re-use of components across several different pipelines. However, the interaction of independently tuned pipeline components yields poor end-to-end performance as errors introduced by one component cascade through the whole pipeline, affecting overall accuracy. We propose a novel model for reasoning across components of Big Data Pipelines in a probabilistically well-founded manner. Our key idea is to view the interaction of components as dependencies on an underlying graphical model. Different message passing schemes on this graphical model provide various inference algorithms to trade-off end-to-end performance and computational cost. We instantiate our framework with an efficient beam search algorithm, and demonstrate its efficiency on two Big Data Pipelines: parsing and relation extraction.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {86–94},
numpages = {9},
keywords = {probabilistic inference, modular design, big data pipelines},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.5555/1325851.1325890,
author = {Cong, Gao and Fan, Wenfei and Geerts, Floris and Jia, Xibei and Ma, Shuai},
title = {Improving Data Quality: Consistency and Accuracy},
year = {2007},
isbn = {9781595936493},
publisher = {VLDB Endowment},
abstract = {Two central criteria for data quality are consistency and accuracy. Inconsistencies and errors in a database often emerge as violations of integrity constraints. Given a dirty database D, one needs automated methods to make it consistent, i.e., find a repair D' that satisfies the constraints and "minimally" differs from D. Equally important is to ensure that the automatically-generated repair D' is accurate, or makes sense, i.e., D' differs from the "correct" data within a predefined bound. This paper studies effective methods for improving both data consistency and accuracy. We employ a class of conditional functional dependencies (CFDs) proposed in [6] to specify the consistency of the data, which are able to capture inconsistencies and errors beyond what their traditional counterparts can catch. To improve the consistency of the data, we propose two algorithms: one for automatically computing a repair D' that satisfies a given set of CFDs, and the other for incrementally finding a repair in response to updates to a clean database. We show that both problems are intractable. Although our algorithms are necessarily heuristic, we experimentally verify that the methods are effective and efficient. Moreover, we develop a statistical method that guarantees that the repairs found by the algorithms are accurate above a predefined rate without incurring excessive user interaction.},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
pages = {315–326},
numpages = {12},
location = {Vienna, Austria},
series = {VLDB '07}
}

@inproceedings{10.1145/2691195.2691196,
author = {Ramasamy, Ramachandran},
title = {Towards Big Data Analytics Framework: ICT Professionals Salary Profile Compilation Perspective},
year = {2014},
isbn = {9781605586113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2691195.2691196},
doi = {10.1145/2691195.2691196},
abstract = {This paper elucidates the opportunity on expanding the on-going preparation of salary profile of information communications technology (ICT) professionals of Malaysia into a big data analytics (BDA) activity. The current activity is based on structured database provided by the online job service providers. In essence, BDA framework entailing 5 Vs namely, volume, variety, velocity, veracity and value used in gauging the gaps and potential areas to consider in next stages.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance},
pages = {450–451},
numpages = {2},
keywords = {big data analytics, business intelligence, ICT salary profile},
location = {Guimaraes, Portugal},
series = {ICEGOV '14}
}

@article{10.1145/2601074,
author = {CACM Staff},
title = {Visualizations Make Big Data Meaningful},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/2601074},
doi = {10.1145/2601074},
abstract = {New techniques are designed to translate "invisible numbers" into visible images.},
journal = {Commun. ACM},
month = {jun},
pages = {19–21},
numpages = {3}
}

@inproceedings{10.1145/2948992.2949024,
author = {Santos, Maribel Yasmina and Costa, Carlos},
title = {Data Warehousing in Big Data: From Multidimensional to Tabular Data Models},
year = {2016},
isbn = {9781450340755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948992.2949024},
doi = {10.1145/2948992.2949024},
abstract = {Data warehouses are central pieces in business intelligence and analytics as these repositories ensure proper data storage and querying, being supported by data models that allow the analysis of data by different perspectives. Those perspectives support users and organizations in the decision-making process. In Big Data environments, Hive is used as a distributed storage mechanism that provides data warehousing capabilities. Its data schemas are defined attending to the analytical requirements specified by the users. In this work, multidimensional data models are used as the source of those requirements, allowing the automatic transformation of a multidimensional schema into a tabular schema suited to be implemented in Hive. To achieve this objective, a set of rules is proposed and tested in a demonstration case, showing the applicability and usefulness of the proposed approach.},
booktitle = {Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering},
pages = {51–60},
numpages = {10},
keywords = {Analytical Data Model, Big Data, Hive, Data Warehousing},
location = {Porto, Portugal},
series = {C3S2E '16}
}

@inproceedings{10.1145/3207677.3278000,
author = {Ke, Changwen and Wang, Kuisheng},
title = {Research and Application of Enterprise Big Data Governance},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3278000},
doi = {10.1145/3207677.3278000},
abstract = {With1 the further development of information technology, data has become one of the core resources of enterprises. In the current era of big data, data governance has gradually become an important means for enterprises to make intelligent decisions, helping enterprises to occupy a favorable position in a highly competitive market environment. This paper briefly describes the current situation of enterprise big data governance, and analyzes the problems of current data governance from the perspectives of enterprise management and data itself. To deal with these problems, this paper proposes several suggestions and strategies from the perspectives of organization and management system construction, construction of a data standard system, improve the level of data quality management, data technology surpport, etc. Fully combining the advanced theories and methods of domestic data governance, this paper designs a data governance framework from the perspective of data application and innovation. The framework includes supervision and control modules, data governance staff organization modules, application and service modules, and data processing and integration modules. And the framework is applied to the data governance of electric power enterprises, which has important reference significance and value for enterprises to carry out data governance.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {29},
numpages = {5},
keywords = {Data governance, data quality, governance framework},
location = {Hohhot, China},
series = {CSAE '18}
}

@article{10.1145/2331042.2331061,
author = {Chan, Cliburn},
title = {Big Data in Computational Biology},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331061},
doi = {10.1145/2331042.2331061},
abstract = {An invitation to the digital science of life.},
journal = {XRDS},
month = {sep},
pages = {64–68},
numpages = {5}
}

@article{10.1145/2436256.2436263,
author = {Hoffmann, Leah},
title = {Looking Back at Big Data},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/2436256.2436263},
doi = {10.1145/2436256.2436263},
abstract = {As computational tools open up new ways of understanding history, historians and computer scientists are working together to explore the possibilities.},
journal = {Commun. ACM},
month = {apr},
pages = {21–23},
numpages = {3}
}

@inproceedings{10.1145/2484028.2494492,
author = {Smith, John R.},
title = {Riding the Multimedia Big Data Wave},
year = {2013},
isbn = {9781450320344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484028.2494492},
doi = {10.1145/2484028.2494492},
abstract = {In this talk we present a perspective across multiple industry problems, including safety and security, medical, Web, social and mobile media, and motivate the need for large-scale analysis and retrieval of multimedia data. We describe a multi-layer architecture that incorporates capabilities for audio-visual feature extraction, machine learning and semantic modeling and provides a powerful framework for learning and classifying contents of multimedia data. We discuss the role semantic ontologies for representing audio-visual concepts and relationships, which are essential for training semantic classifiers. We discuss the importance of using faceted classification schemes in particular for organizing multimedia semantic concepts in order to achieve effective learning and retrieval. We also show how training and scoring of multimedia semantics can be implemented on big data distributed computing platforms to address both massive-scale analysis and low-latency processing. We describe multiple efforts at IBM on image and video analysis and retrieval, including IBM Multimedia Analysis and Retrieval System (IMARS), and show recent results for semantic-based classification and retrieval. We conclude with future directions for improving analysis of multimedia through interactive and curriculum-based techniques for multimedia semantics-based learning and retrieval.},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1–2},
numpages = {2},
keywords = {semantic modeling, video analysis, multimedia information retrieval, machine learning, content-based search},
location = {Dublin, Ireland},
series = {SIGIR '13}
}

@article{10.1145/2500873,
author = {Kim, Gang-Hoon and Trimi, Silvana and Chung, Ji-Hyong},
title = {Big-Data Applications in the Government Sector},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/2500873},
doi = {10.1145/2500873},
abstract = {In the same way businesses use big data to pursue profits, governments use it to promote the public good.},
journal = {Commun. ACM},
month = {mar},
pages = {78–85},
numpages = {8}
}

@inproceedings{10.1145/3183713.3183757,
author = {Qiu, Disheng and Barbosa, Luciano and Crescenzi, Valter and Merialdo, Paolo and Srivastava, Divesh},
title = {Big Data Linkage for Product Specification Pages},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183757},
doi = {10.1145/3183713.3183757},
abstract = {An increasing number of product pages are available from thousands of web sources, each page associated with a product, containing its attributes and one or more product identifiers. The sources provide overlapping information about the products, using diverse schemas, making web-scale integration extremely challenging. In this paper, we take advantage of the opportunity that sources publish product identifiers to perform big data linkage across sources at the beginning of the data integration pipeline, before schema alignment. To realize this opportunity, several challenges need to be addressed: identifiers need to be discovered on product pages, made difficult by the diversity of identifiers; the main product identifier on the page needs to be identified, made difficult by the many related products presented on the page; and identifiers across pages need to beresolved, made difficult by the ambiguity between identifiers across product categories. We present our RaF (Redundancy as Friend) solution to the problem of big data linkage for product specification pages, which takes advantage of the redundancy of identifiers at a global level, and the homogeneity of structure and semantics at the local source level, to effectively and efficiently link millions of pages of head and tail products across thousands of head and tail sources. We perform a thorough empirical evaluation of our RaF approach using the publicly available Dexter dataset consisting of 1.9M product pages from 7.1k sources of 3.5k websites, and demonstrate its effectiveness in practice.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {67–81},
numpages = {15},
keywords = {data linkage, big data, data extraction, data integration},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/2632320.2632325,
author = {Grillenberger, Andreas},
title = {Big Data and Data Management: A Topic for Secondary Computing Education},
year = {2014},
isbn = {9781450327558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2632320.2632325},
doi = {10.1145/2632320.2632325},
abstract = {The topics data management and data analysis are currently discussed in various contexts, e.g. in Computer Science but also in daily life and society. The recent developments in this field, which are often summarized under the term Big Data, did not only lead to the emergence of new database models, but also comprise new threats, e.g. for data privacy. These topics include many aspects that are important for everyone, but they only gain in relevance slowly in higher education and hardly in secondary education. Hence, I will evaluate data management as a topic for secondary education, with a view on the long-lasting concepts and aspects in this field.},
booktitle = {Proceedings of the Tenth Annual Conference on International Computing Education Research},
pages = {147–148},
numpages = {2},
keywords = {daily life, data analysis, data privacy, databases, data management, secondary schools, big data, nosql},
location = {Glasgow, Scotland, United Kingdom},
series = {ICER '14}
}

@article{10.14778/2733004.2733015,
author = {Yu, Meng-Chieh and Yu, Tong and Wang, Shao-Chen and Lin, Chih-Jen and Chang, Edward Y.},
title = {Big Data Small Footprint: The Design of a Low-Power Classifier for Detecting Transportation Modes},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733015},
doi = {10.14778/2733004.2733015},
abstract = {Sensors on mobile phones and wearables, and in general sensors on IoT (Internet of Things), bring forth a couple of new challenges to big data research. First, the power consumption for analyzing sensor data must be low, since most wearables and portable devices are power-strapped. Second, the velocity of analyzing big data on these devices must be high, otherwise the limited local storage may overflow.This paper presents our hardware-software co-design of a classifier for wearables to detect a person's transportation mode (i.e., still, walking, running, biking, and on a vehicle). We particularly focus on addressing the big-data small-footprint requirement by designing a classifier that is low in both computational complexity and memory requirement. Together with a sensor-hub configuration, we are able to drastically reduce power consumption by 99%, while maintaining competitive mode-detection accuracy. The data used in the paper is made publicly available for conducting research.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1429–1440},
numpages = {12},
keywords = {transportation mode, support vector machines, context-aware computing, sensor hub, classification, big data small footprint}
}

@inproceedings{10.1145/3172871.3172886,
author = {Srinivasan, Madhan Kumar and Revathy, P.},
title = {State-of-the-Art Big Data Security Taxonomies},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172886},
doi = {10.1145/3172871.3172886},
abstract = {Today's businesses accumulate an astonishing amount of digital data, which can be leveraged to unlock new sources of economic value and provide fresh insights into business trends. The real challenge in this process is the design of computing, storage infrastructure and algorithms needed to handle this "Big Data". Hence, organizations are looking at different ways in which they can make use of Big Data in their business. There's no doubt that the creation of a Hadoop-powered Data Lake can provide a robust foundation for a new generation of analytics and intuitive results. At the same time, it is also very necessary to consider security before launching or expanding a Hadoop initiative. As we move towards a stage where Hadoop is considered for real-time production scenarios rather than just experimentation levels, a major chunk of production data is normally sensitive, or subject to many industry regulations and governance controls. This paper analyzes the current security challenges in big data implementations based on state-of-the-art big data security taxonomies.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {16},
numpages = {7},
keywords = {Identity Management, Hadoop Security, Hadoop Framework Security, Big Data Analytics, Big Data, Hadoop, Cloud Security, Big Data Security Taxonomies, Big Data Security, NoSQL Concerns},
location = {Hyderabad, India},
series = {ISEC '18}
}

@inproceedings{10.1145/3090354.3090376,
author = {Sebaa, Abderrazak and Nouicer, Amina and Chikh, Fatima and Tari, Abdelkamel},
title = {Big Data Technologies to Improve Medical Data Warehousing},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090376},
doi = {10.1145/3090354.3090376},
abstract = {The purpose of this review is to explore how the use of big data technology improves the performance of medical data warehousing. Indeed, traditional data warehousing tools can no longer be used to handle the volume, variety, and velocity of today's data-centric medical applications. Moreover, Big data technologies can be used to process the streams of medical data, they increase performance by using a cluster of existing networked nodes through powerful processing of these streams. In this paper, we provide an overview of state-of-the-art research issues of data warehousing technologies in the medical field and the opportunities presented by big data technologies. Whereas, an appropriate use of the current technology of big data especially Hadoop could help to overcome issues of medical data warehousing.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {21},
numpages = {5},
keywords = {Hadoop, Medical Data Warehouse, Medical Informatics, Big Data, Big Data Analytic},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@article{10.1145/2737817.2737829,
author = {Ellis, Jason and Fokoue, Achille and Hassanzadeh, Oktie and Kementsietsidis, Anastasios and Srinivas, Kavitha and Ward, Michael J.},
title = {Exploring Big Data with Helix: Finding Needles in a Big Haystack},
year = {2015},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2737817.2737829},
doi = {10.1145/2737817.2737829},
abstract = {While much work has focused on efficient processing of Big Data, little work considers how to understand them. In this paper, we describe Helix, a system for guided exploration of Big Data. Helix provides a unified view of sources, ranging from spreadsheets and XML files with no schema, all the way to RDF graphs and relational data with well-defined schemas. Helix users explore these heterogeneous data sources through a combination of keyword searches and navigation of linked web pages that include information about the schemas, as well as data and semantic links within and across sources. At a technical level, the paper describes the research challenges involved in developing Helix, along with a set of real-world usage scenarios and the lessons learned.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {43–54},
numpages = {12}
}

@inproceedings{10.1145/3063955.3063968,
author = {Wang, Hongzhi and Gao, Hong and Yin, Shenjun and Zhu, Jie},
title = {The Design of Course Architecture for Big Data},
year = {2017},
isbn = {9781450348737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3063955.3063968},
doi = {10.1145/3063955.3063968},
abstract = {Big data is one of the hottest topic in not only academic but also enterprise, which provide grate requirements for the people with knowledge and experiences of big data. However, current education architecture of computer science could not provide sufficient training for big data. For the education for people suitable for big data era, we attempt to design a novel course architecture. Such course architecture will not change the skeleton of traditional course architecture of computer science but just add content and subjects that is adaptive for big data. In this paper, we discuss the goal, architecture and content of the course architecture.},
booktitle = {Proceedings of the ACM Turing 50th Celebration Conference - China},
articleno = {13},
numpages = {6},
keywords = {course architecture, big data, data science},
location = {Shanghai, China},
series = {ACM TUR-C '17}
}

@inproceedings{10.1145/3129757.3129762,
author = {Plekhanov, Dmitriy},
title = {Official Statistics Embrace Big Data: A Review of Current and Developing International Practice},
year = {2017},
isbn = {9781450354127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129757.3129762},
doi = {10.1145/3129757.3129762},
abstract = {Big Data is a phenomenon brought about by the rapid spread of digital devices and massive increase in information flow in the modern world. The digitalization of society provides new sources of information about our society in general. The main aim of the official statistics is to provide the public with relevant information on a timely basis. Therefore it is actually the duty of statisticians to explore opportunities provided by Big Data sources and try to find ways to generate information for the public good. This paper provides a brief overview of pilot projects carried out by national statistical organizations around the world and analyses main challenges related to the use of Big Data sources in the realm of official statistics.},
booktitle = {Proceedings of the Internationsl Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {22–26},
numpages = {5},
keywords = {data analysis, big data, official statistics},
location = {St. Petersburg, Russia},
series = {eGose '17}
}

@article{10.1145/2168931.2168943,
author = {Fisher, Danyel and DeLine, Rob and Czerwinski, Mary and Drucker, Steven},
title = {Interactions with Big Data Analytics},
year = {2012},
issue_date = {May + June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1072-5520},
url = {https://doi.org/10.1145/2168931.2168943},
doi = {10.1145/2168931.2168943},
journal = {Interactions},
month = {may},
pages = {50–59},
numpages = {10}
}

@inproceedings{10.1145/3368691.3368717,
author = {Mohammed, Tareq Abed and Ghareeb, Ahmed and Al-bayaty, Hussein and Aljawarneh, Shadi},
title = {Big Data Challenges and Achievements: Applications on Smart Cities and Energy Sector},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368717},
doi = {10.1145/3368691.3368717},
abstract = {In this paper, the Big Data challenges and the processing is analyzed, recently great attention has been paid to the challenges for great data, largely due to the wide spread of applications and systems used in real life, such as presentation, modeling, processing and large (often unlimited) data storage. Mass Data Survey, OLAP Mass Data, Mass Data Dissemination and Mass Data Protection. Consequently, we focus on further research trends and, as a default, we will explore a future research challenge research project in this area of research.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {26},
numpages = {5},
keywords = {data processing, OLAP, data mining, machin learning, big data},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@article{10.1145/2094114.2094129,
author = {Bizer, Christian and Boncz, Peter and Brodie, Michael L. and Erling, Orri},
title = {The Meaningful Use of Big Data: Four Perspectives -- Four Challenges},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2094114.2094129},
doi = {10.1145/2094114.2094129},
abstract = {Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.},
journal = {SIGMOD Rec.},
month = {jan},
pages = {56–60},
numpages = {5}
}

@inproceedings{10.1145/3246336,
author = {Chen, Lei},
title = {Session Details: Towards Big Data},
year = {2013},
isbn = {9781450321556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246336},
doi = {10.1145/3246336},
booktitle = {Proceedings of the 2013 SIGMOD/PODS Ph.D. Symposium},
location = {New York, New York, USA},
series = {SIGMOD'13 PhD Symposium}
}

@inproceedings{10.1145/2983323.2983345,
author = {Zhu, Fangzhou and Luo, Chen and Yuan, Mingxuan and Zhu, Yijian and Zhang, Zhengqing and Gu, Tao and Deng, Ke and Rao, Weixiong and Zeng, Jia},
title = {City-Scale Localization with Telco Big Data},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983345},
doi = {10.1145/2983323.2983345},
abstract = {It is still challenging in telecommunication (telco) industry to accurately locate mobile devices (MDs) at city-scale using the measurement report (MR) data, which measure parameters of radio signal strengths when MDs connect with base stations (BSs) in telco networks for making/receiving calls or mobile broadband (MBB) services. In this paper, we find that the widely-used location based services (LBSs) have accumulated lots of over-the-top (OTT) global positioning system (GPS) data in telco networks, which can be automatically used as training labels for learning accurate MR-based positioning systems. Benefiting from these telco big data, we deploy a context-aware coarse-to-fine regression (CCR) model in Spark/Hadoop-based telco big data platform for city-scale localization of MDs with two novel contributions. First, we design map-matching and interpolation algorithms to encode contextual information of road networks. Second, we build a two-layer regression model to capture coarse-to-fine contextual features in a short time window for improved localization performance. In our experiments, we collect 108 GPS-associated MR records in the centroid of Shanghai city with 12 x 11 square kilometers for 30 days, and measure four important properties of real-world MR data related to localization errors: stability, sensitivity, uncertainty and missing values. The proposed CCR works well under different properties of MR data and achieves a mean error of 110m and a median error of $80m$, outperforming the state-of-art range-based and fingerprinting localization methods.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {439–448},
numpages = {10},
keywords = {regression models, localization, telco big data},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.5555/2483628.2483630,
author = {Abbass, Hussein},
title = {Mining Big Data Streams: The Fallacy of Blind Correlation and the Importance of Models},
year = {2011},
isbn = {9781921770029},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Big data streams mark a new era in artificial intelligence and the data mining literature. Video and voice streams have grown rapidly in recent years. A single lab--based human--computer interaction experiment with one human subject collecting Cognitive, Physiological, and other data can easily generate a few terabytes of data in a single hour; growing rapidly to a Petabyte within a timeframe less than a month. In an article in the Wired Magazine, 2008, by Chris Anderson, he wrote "the data deluge makes the scientific method obsolete" He predicted that in the age of Petabyte and beyond, a meaningful correlation analysis is enough! Chris comment was provocative; but some started believing it. So was Chris right or wrong? Why? What can we do to face the outburst of big data? Do we have the data mining tools to manage these data? Where is the future of data mining heading? In this talk, I will discuss the above questions and demonstrate some answers using examples of my work and analysis.},
booktitle = {Proceedings of the Ninth Australasian Data Mining Conference - Volume 121},
pages = {5–6},
numpages = {2},
location = {Ballarat, Australia},
series = {AusDM '11}
}

@inproceedings{10.1145/2479724.2479764,
author = {Harrison, Teresa M. and Hrdinova, Jana},
title = {The Complexities of "Big Data": The Opportunities and Challenges for e-Government},
year = {2013},
isbn = {9781450320573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479724.2479764},
doi = {10.1145/2479724.2479764},
abstract = {In this plenary panel, speakers from academia and government consider the technical, scientific, and organizational implications of Big Data and offer observations about what it means for research and practice related to e-government.},
booktitle = {Proceedings of the 14th Annual International Conference on Digital Government Research},
pages = {263–264},
numpages = {2},
keywords = {data stewardship, privacy, security, large-scale data systems, data integration, big data, research data alliance, social media, data analytics, performance and benchmarking of big data systems, scientific data management},
location = {Quebec, Canada},
series = {dg.o '13}
}

@inproceedings{10.1145/2559206.2580093,
author = {Eagle, Nathan},
title = {Big Data for Social Good},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2580093},
doi = {10.1145/2559206.2580093},
abstract = {Petabytes of data about human movements, transactions, and communication patterns are continuously being generated by everyday technologies such as mobile phones and credit cards. This unprecedented volume of information facilitates a novel set of research questions applicable to a wide range of development issues.In a collaboration involving 237 mobile operators across 102 countries, Jana's mobile technology platform can instantly poll and compensate 3.48 billion active mobile subscriptions. This talk will discuss how insights gained from living in Kenya became the genesis of a technology company currently working with global clients in over 50 countries, including P&amp;G, Google, Unilever, Danone, General Mills, Nestle, Johnson &amp; Johnson, Microsoft, the World Bank, and the United Nations. After providing an overview of the mobile and social media landscapes in emerging markets, it will conclude by emphasizing the value of consumer data in underserved and understudied regions of the world.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {11–12},
numpages = {2},
keywords = {keynote/invited talk},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}

@inproceedings{10.1145/2745754.2745782,
author = {Jordan, Michael I.},
title = {Computational Thinking, Inferential Thinking and "Big Data"},
year = {2015},
isbn = {9781450327572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745754.2745782},
doi = {10.1145/2745754.2745782},
abstract = {The phenomenon of "Big Data" is creating a need for research perspectives that blend computational thinking (with its focus on, e.g., abstractions, algorithms and scalability) with inferential thinking (with its focus on, e.g., underlying populations, sampling patterns, error bars and predictions). Database researchers and statistical machine learning researchers are centrally involved in the creation of this blend, and research that incorporates perspectives from both databases and machine learning will be of particular value in the bigger picture. This is true both for methodology and for theory. I present highlights of several research initiatives that draw jointly on database and statistical foundations, including work on concurrency control and distributed inference, subsampling, time/data tradeoffs and inference/privacy tradeoffs.},
booktitle = {Proceedings of the 34th ACM SIGMOD-SIGACT-SIGAI  Symposium on Principles of Database Systems},
pages = {1},
numpages = {1},
keywords = {computational thinking, big data, statistical machine learning, inferential thinking},
location = {Melbourne, Victoria, Australia},
series = {PODS '15}
}

@article{10.1145/2935753,
author = {Berti-Equille, Laure and Ba, Mouhamadou Lamine},
title = {Veracity of Big Data: Challenges of Cross-Modal Truth Discovery},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2935753},
doi = {10.1145/2935753},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {12},
numpages = {3},
keywords = {Truth discovery, fact checking, data fusion, information extraction, data quality}
}

@inproceedings{10.1145/2666310.2666481,
author = {Lee, Kisung and Ganti, Raghu K. and Srivatsa, Mudhakar and Liu, Ling},
title = {Efficient Spatial Query Processing for Big Data},
year = {2014},
isbn = {9781450331319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666310.2666481},
doi = {10.1145/2666310.2666481},
abstract = {Spatial queries are widely used in many data mining and analytics applications. However, a huge and growing size of spatial data makes it challenging to process the spatial queries efficiently. In this paper we present a lightweight and scalable spatial index for big data stored in distributed storage systems. Experimental results show the efficiency and effectiveness of our spatial indexing technique for different spatial queries.},
booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {469–472},
numpages = {4},
keywords = {spatial query, spatial indexing, big data},
location = {Dallas, Texas},
series = {SIGSPATIAL '14}
}

@inproceedings{10.1145/2837060.2837067,
author = {Rathore, M. Mazhar and Ahmad, Awais and Paul, Anand},
title = {Big Data and Internet of Things: An Asset for Urban Planning},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837067},
doi = {10.1145/2837060.2837067},
abstract = {The growing city population demands the delivery of services and infrastructure. The use of Internet of Things (IoT) devices, such as sensors, actuators, and smartphones, etc., and the smart system is the valuable source in order to meet increasing demands. However, thousands of interconnecting IoT devices effects in producing an enormous volume of data, termed as Big Data. To integrate IoT services and processing Big Data in an efficient way to achieve smooth urban planning is a challenging task. In this paper, we propose an IoT-based urban planning system using Big Data Analytics. The system consists of various types of IoT-based smart system including smart home, vehicular networking, weather and water system, smart parking, and surveillance objects, etc. A four-tier architecture is proposed that includes 1) Bottom Tier-1: responsible for IoT data generation, and collections 2) Intermediate Tier-1: responsible for all type of communication between sensors, relays, base stations, Internet, etc. 3) Intermediate Tier 2: it is responsible for data management and processing using Hadoop framework, and 4) Top Tier: is responsible for application and usage of the data analysis and results generated. The proposed system is implemented in Hadoop ecosystem environment with MapReduce programming. The system produced results with higher throughput and low processing time, which proves the scalability and efficiency of the system.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {58–65},
numpages = {8},
keywords = {Smart City, IoT, Urban Planning, Big Data},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1109/CCGrid.2015.138,
author = {Ros\`{a}, Andrea and Chen, Lydia Y. and Binder, Walter},
title = {Understanding Unsuccessful Executions in Big-Data Systems},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.138},
doi = {10.1109/CCGrid.2015.138},
abstract = {Big-data applications are being increasingly used in today's large-scale datacenters for a large variety of purposes, such as solving scientific problems, running enterprise services, and computing data-intensive tasks. Due to the growing scale of these systems and the complexity of running applications, jobs running in big-data systems experience unsuccessful terminations of different nature. While a large body of existing studies sheds light on failures occurred in large-scale datacenters, the current literature overlooks the characteristics and the performance impairment of a broader class of unsuccessful executions which can arise due to application failures, dependency violations, machine constraints, job kills, and task preemption. Nonetheless, deepening our understanding in this field is of paramount importance, as unsuccessful executions can lower user satisfaction, impair reliability, and lead to a high resource waste. In this paper, we describe the problem of unsuccessful executions in big-data systems, and highlight the critical importance of improving our knowledge on this subject. We review the existing literature on this field, discuss its limitations, and present our own contributions to the problem, along with our research plan for the future.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {741–744},
numpages = {4},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/2320765.2320830,
author = {Held, Johannes and Lenz, Richard},
title = {Towards Measuring Test Data Quality},
year = {2012},
isbn = {9781450311434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2320765.2320830},
doi = {10.1145/2320765.2320830},
abstract = {In order to enable proper system and integration testing, it is often necessary to have huge test data inventories, reflecting the heterogeneous live system. Although the maintenance of large data stores can be guided by advice obtained from data quality evaluations, this technique can be only partly applied to test data inventories. Assessing test data quality is difficult, as the well-known data quality dimensions are not applicable in an easy fashion. For example, an otherwise good value of 100% for correctness would not allow to store erroneous test data items. The need for data quality dimensions dedicated to assessing test data quality can't be satisfied by well-known data quality dimensions. In this paper, we present our thesis approach to identify and validate new quality dimensions applicable for test data quality and develop quantification methods. We propose proximity to reality and degree of coverage as two new test data quality dimension and sketch quantification approach to measures, specifically suited for test data.},
booktitle = {Proceedings of the 2012 Joint EDBT/ICDT Workshops},
pages = {233–238},
numpages = {6},
keywords = {test data quality, data quality dimensions, testing},
location = {Berlin, Germany},
series = {EDBT-ICDT '12}
}

@inproceedings{10.1145/3438872.3439086,
author = {Guo, Naiwang and Shen, Quanjiang and Yang, Hongshan},
title = {Design and Implementation of Componentized Big Data Platform},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439086},
doi = {10.1145/3438872.3439086},
abstract = {The rapid development of the Internet has produced massive amounts of data, and has promoted the development of various big data components in the big data field, such as Hadoop, Spark and other big data components. However, many big data components face many problems in actual industrial production, make the construction and maintenance cost of the big data platform high. Although there is an automated deployment platform for big data components on the market, it lacks comprehensive management and monitoring functions for the entire big data platform, resulting in users not being able to fully pay attention to all aspects of the big data platform, and the service and resource management is not flexible enough. Therefore, we have designed and implemented an open componentized big data platform, so that users can not only easily install and deploy big data components in the cluster, but also manage the resources and services of big data components conveniently and flexibly. Besides, complete monitoring information also provides the necessary guarantee for the stable running of the big data platform. The open componentized big data platform provides users with one-stop big data services.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {234–238},
numpages = {5},
keywords = {Comprehensive monitoring, Big data platform, Convenient management, Automated deployment, Big data components},
location = {Shanghai, China},
series = {RICAI 2020}
}

@article{10.1145/3262388,
author = {Fan, Wei and Bifet, Albert},
title = {Session Details: Mining Big Data},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3262388},
doi = {10.1145/3262388},
journal = {SIGKDD Explor. Newsl.},
month = {apr}
}

@inproceedings{10.1145/3372938.3372939,
author = {Jernejcic, Thomas and Kettani, Houssain},
title = {On the Intersection of Big Data and Privacy},
year = {2020},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3372939},
doi = {10.1145/3372938.3372939},
abstract = {A struggle has emerged in relation to the sacredness of one's private information and the importance of moving forward in a digital world of social media, smart devices, and Big Data -- an era known as the Age of Context. The purpose of this paper is to make a clear case for concern regarding the seriousness of keeping data private while facilitating efforts to encourage and support emerging technologies. This investigative process included the pursuit of relevant articles and published works that provided a clear and relevant picture of the current state of affairs concerning Big Data and privacy. After a review of the literature, an analysis of data collection methods, a discovery of Big Data processes and purposes, and the identification of risks pertaining to the individual, the military, and the country, it was determined that significant concerns do exist pertaining to data collection, Big Data, and privacy. These concerns not only pertain to the individual, but with military effectiveness and national security.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {1},
numpages = {4},
keywords = {Cyber Security, Big Data, Data Privacy},
location = {Rabat, Morocco},
series = {BDIoT'19}
}

@inproceedings{10.1145/2745754.2745771,
author = {Fan, Wenfei and Geerts, Floris and Cao, Yang and Deng, Ting and Lu, Ping},
title = {Querying Big Data by Accessing Small Data},
year = {2015},
isbn = {9781450327572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745754.2745771},
doi = {10.1145/2745754.2745771},
abstract = {This paper investigates the feasibility of querying big data by accessing a bounded amount of the data. We study boundedly evaluable queries under a form of access constraints, when their evaluation cost is determined by the queries and constraints only. While it is undecidable to determine whether FO queries are boundedly evaluable, we show that for several classes of FO queries, the bounded evaluability problem is decidable. We also provide characterization and effective syntax for their boundedly evaluable queries.When a query Q is not boundedly evaluable, we study two approaches to approximately answering Q under access constraints. (1) We search for upper and lower envelopes of Q that are boundedly evaluable and warrant a constant accuracy bound. (2) We instantiate a minimum set of variables (parameters) in Q such that the specialized query is boundedly evaluable. We study problems for deciding the existence of envelopes and bounded specialized queries, and establish their complexity for various classes of FO queries.},
booktitle = {Proceedings of the 34th ACM SIGMOD-SIGACT-SIGAI  Symposium on Principles of Database Systems},
pages = {173–184},
numpages = {12},
keywords = {big data, complexity, query answering},
location = {Melbourne, Victoria, Australia},
series = {PODS '15}
}

@inproceedings{10.1145/3335484.3335534,
author = {Ye, Zilong},
title = {Green Optical Networking for Big Data Transfer},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335534},
doi = {10.1145/3335484.3335534},
abstract = {Recent advances in computing and sensing techniques lead to the generation of tons of data that needs to be processed in distributed computing infrastructures (e.g., datacenters). In this paper, we focus on applying optical networking techniques for supporting the big data transfer between datacenters. In particular, we propose a green optical networking scheme, namely predictive and incremental grooming (PI-grooming), which makes the traffic grooming decision based on the existing flow assignment, the current traffic demands, and the expected future traffic demands to achieve energy-efficient communication. We conduct large-scale simulations to evaluate the performance of the proposed PI-grooming scheme. The results show that PI-grooming can save energy by up to 25% compared to existing baseline solutions.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {64–67},
numpages = {4},
keywords = {traffic grooming, predictive, optical networking, Big data transfer},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@article{10.14778/2367502.2367562,
author = {Dittrich, Jens and Quian\'{e}-Ruiz, Jorge-Arnulfo},
title = {Efficient Big Data Processing in Hadoop MapReduce},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367562},
doi = {10.14778/2367502.2367562},
abstract = {This tutorial is motivated by the clear need of many organizations, companies, and researchers to deal with big data volumes efficiently. Examples include web analytics applications, scientific applications, and social networks. A popular data processing engine for big data is Hadoop MapReduce. Early versions of Hadoop MapReduce suffered from severe performance problems. Today, this is becoming history. There are many techniques that can be used with Hadoop MapReduce jobs to boost performance by orders of magnitude. In this tutorial we teach such techniques. First, we will briefly familiarize the audience with Hadoop MapReduce and motivate its use for big data processing. Then, we will focus on different data management techniques, going from job optimization to physical data organization like data layouts and indexes. Throughout this tutorial, we will highlight the similarities and differences between Hadoop MapReduce and Parallel DBMS. Furthermore, we will point out unresolved research problems and open issues.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2014–2015},
numpages = {2}
}

@inproceedings{10.1145/3003733.3003767,
author = {Petrou, Charilaos and Paraskevas, Michael},
title = {Signal Processing Techniques Restructure The Big Data Era},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003767},
doi = {10.1145/3003733.3003767},
abstract = {Big data science has been developed into a topic that attracts attention from industry, academia and governments. The main objective in Big Data science is to recognize and extract meaningful information from huge amounts of heterogeneous data and unstructured data (which constitute 95% of big data). Signal Processing (SP) techniques and related statistical learning (SL) tools such as Principal Component Analysis (PCA), R-PCA (Robust PCA), Compressive Sampling (CS), convex optimization (CO), stochastic approximation (SA), kernel based learning (KBL) tasks are used for robustness, compression and dimensionality reduction in Big Data arising challenges. This review paper introduces Big Data related SP techniques and presents applications of this emerging field.},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {52},
numpages = {6},
keywords = {stochastic approximation, convex optimization, statistical learning tools, big data, signal processing techniques},
location = {Patras, Greece},
series = {PCI '16}
}

@inproceedings{10.1109/CCGRID.2017.73,
author = {Wu, Dongyao and Sakr, Sherif and Zhu, Liming and Wu, Huijun},
title = {Towards Big Data Analytics across Multiple Clusters},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.73},
doi = {10.1109/CCGRID.2017.73},
abstract = {Big data are increasingly collected and stored in a highly distributed infrastructures due to the development of sensor network, cloud computing, IoT and mobile computing among many other emerging technologies. In practice, the majority of existing big-data-processing frameworks (e.g., Hadoop and Spark) are designed based on the single-cluster setup with the assumptions of centralized management and homogeneous connectivity which makes them sub-optimal and sometimes infeasible to apply for scenarios that require implementing data analytics jobs on highly distributed data sets (across racks, data centers or multi-organizations). In order to tackle this challenge, we present HDM-MC, a multi-cluster big data processing framework which is designed to enable the capability of performing large scale data analytics across multi-clusters with minimum extra overhead due to additional scheduling requirements. In this paper, we present the architecture and realization of the system. In addition, we evaluate the performance of our framework in comparison to other state-of-art single cluster big data processing frameworks.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {218–227},
numpages = {10},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3253877,
author = {Lempel, Ronny},
title = {Session Details: Big Data Algorithms},
year = {2016},
isbn = {9781450337168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253877},
doi = {10.1145/3253877},
booktitle = {Proceedings of the Ninth ACM International Conference on Web Search and Data Mining},
location = {San Francisco, California, USA},
series = {WSDM '16}
}

@inproceedings{10.1145/3231830.3231840,
author = {Kaloyanova, Kalinka and Hristov, Tsvetomir and Naydenova, Ina and Kovacheva, Zlatinka},
title = {Information Management Technologies for Big Data: A Case of Oracle},
year = {2017},
isbn = {9781450353106},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3231830.3231840},
doi = {10.1145/3231830.3231840},
abstract = {During the last decade the volume, the rate of accumulation and the diversity of data in general have been steadily increasing, which leads to the rapid development of Big data and technological enhancements associated with it. Many leading companies in the area took the challenge and provided different solutions to manage Big data. New hardware and software technologies are introduced for information management. The paper analyzes the main Big data technologies provided by Oracle as well their implementation in several specific cases.},
booktitle = {Proceedings of the Second International Conference on Advanced Wireless Information, Data, and Communication Technologies},
articleno = {10},
numpages = {4},
keywords = {Database, NoSQL, RDBMS, MapReduce},
location = {Paris, France},
series = {AWICT 2017}
}

@inproceedings{10.1145/2588555.2610512,
author = {LeFevre, Jeff and Sankaranarayanan, Jagan and Hacigumus, Hakan and Tatemura, Junichi and Polyzotis, Neoklis and Carey, Michael J.},
title = {Opportunistic Physical Design for Big Data Analytics},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2610512},
doi = {10.1145/2588555.2610512},
abstract = {Big data analytical systems, such as MapReduce, perform aggressive materialization of intermediate job results in order to support fault tolerance. When jobs correspond to exploratory queries submitted by data analysts, these materializations yield a large set of materialized views that we propose to treat as an opportunistic physical design. We present a semantic model for UDFs that enables effective reuse of views containing UDFs along with a rewrite algorithm that provably finds the minimum-cost rewrite under certain assumptions. An experimental study on real-world datasets using our prototype based on Hive shows that our approach can result in dramatic performance improvements.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {851–862},
numpages = {12},
keywords = {exploratory analysis, opportunistic views, query processing, opportunistic physical design, query rewriting, UDFs, big data},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/956750.956844,
author = {Dasu, Tamraparni and Vesonder, Gregg T. and Wright, Jon R.},
title = {Data Quality through Knowledge Engineering},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956844},
doi = {10.1145/956750.956844},
abstract = {Traditionally, data quality programs have acted as a preprocessing stage to make data suitable for a data mining or analysis operation. Recently, data quality concepts have been applied to databases that support business operations such as provisioning and billing. Incorporating business rules that drive operations and their associated data processes is critically important to the success of such projects. However, there are many practical complications. For example, documentation on business rules is often meager. Rules change frequently. Domain knowledge is often fragmented across experts, and those experts do not always agree. Typically, rules have to be gathered from subject matter experts iteratively, and are discovered out of logical or procedural sequence, like a jigsaw puzzle. Our approach is to impement business rules as constraints on data in a classical expert system formalism sometimes called production rules. Our system works by allowing good data to pass through a system of constraints unchecked. Bad data violate constraints and are flagged, and then fed back after correction. Constraints are added incrementally as better understanding of the business rules is gained. We include a real-life case study.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {705–710},
numpages = {6},
keywords = {data quality, static and dynamic constraints, business operations databases},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/2872518.2890583,
author = {Maret, Pierre and Akerkar, Rajendra and Vercouter, Laurent},
title = {Web Communities in Big Data Era. Editorial},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890583},
doi = {10.1145/2872518.2890583},
abstract = {Web-based community is a self-defined web-based network of interactive communication organized around a shared interest or purpose. It provides the means of interactions among people in which they create, share, and exchange information and ideas in virtual space and networks. Working with big data often requires querying and reasoning that data to isolate information of interest and manipulate it in various ways. This editorial paper explores recent big data research topics -- stream querying and reasoning -- over data from web based communities. It combines aspects from some well-studied research domains, such as, social network analysis, graph databases, and data streams. We provide a brief synopsis of some research issues in supporting reasoning and querying tasks. This editorial also presents the WI&amp;C-16 workshop's goal and programme.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {945–947},
numpages = {3},
keywords = {web communities, querying, datastream, and reasoning},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/3206505.3206556,
author = {Kammer, Dietrich and Keck, Mandy and Gr\"{u}nder, Thomas and Groh, Rainer},
title = {Big Data Landscapes: Improving the Visualization of Machine Learning-Based Clustering Algorithms},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206556},
doi = {10.1145/3206505.3206556},
abstract = {With the internet, massively heterogeneous data sources need to be understood and classified to provide suitable services to users such as content observation, data exploration, e-commerce, or adaptive learning environments. The key to providing these services is applying machine learning (ML) in order to generate structures via clustering and classification. Due to the intricate processes involved in ML, visual tools are needed to support designing and evaluating the ML pipelines. In this contribution, we propose a comprehensive tool that facilitates the analysis and design of ML-based clustering algorithms using multiple visualization features such as semantic zoom, glyphs, and histograms.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {66},
numpages = {3},
keywords = {clustering, glyphs, machine learning, visualization, big data landscapes},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1109/CHASE.2017.81,
author = {Katsis, Yannis and Balac, Natasha and Chapman, Derek and Kapoor, Madhur and Block, Jessica and Griswold, William G. and Huang, Jeannie and Koulouris, Nikos and Menarini, Massimiliano and Nandigam, Viswanath and Ngo, Mandy and Ong, Kian Win and Papakonstantinou, Yannis and Smith, Besa and Zarifis, Konstantinos and Woolf, Steven and Patrick, Kevin},
title = {Big Data Techniques for Public Health: A Case Study},
year = {2017},
isbn = {9781509047215},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CHASE.2017.81},
doi = {10.1109/CHASE.2017.81},
abstract = {Public health researchers increasingly recognize that to advance their field they must grapple with the availability of increasingly large (i.e., thousands of variables) traditional population-level datasets (e.g., electronic medical records), while at the same time integrating additional large datasets (e.g., data on genomics, the microbiome, environmental exposures, socioeconomic factors, and health behaviors). Leveraging these multiple forms of data might well provide unique and unexpected discoveries about the determinants of health and wellbeing. However, we are in the very early stages of advancing the techniques required to understand and analyze big population-level data for public health research.To address this problem, this paper describes how we propose that big data can be efficiently used for public health discoveries. We show that data analytics techniques traditionally employed in public health studies are not up to the task of the data we now have in hand. Instead we present techniques adapted from big data visualization and analytics approaches used in other domains that can be used to answer important public health questions utilizing these existing and new datasets. Our findings are based on an exploratory big data case study carried out in San Diego County, California where we analyzed thousands of variables related to health to gain interesting insights on the determinants of several health outcomes, including life expectancy and anxiety disorders. These findings provide a promising early indication that public health research will benefit from the larger set of activities in contemporary big data research.},
booktitle = {Proceedings of the Second IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies},
pages = {222–231},
numpages = {10},
location = {Philadelphia, Pennsylvania},
series = {CHASE '17}
}

