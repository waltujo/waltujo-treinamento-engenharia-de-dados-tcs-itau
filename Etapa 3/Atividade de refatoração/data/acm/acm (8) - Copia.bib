@inproceedings{10.1145/2379436.2379437,
author = {Regola, Nathan and Cieslak, David A. and Chawla, Nitesh V.},
title = {The Constraints of Magnetic versus Flash Disk Capabilities in Big Data Analysis},
year = {2012},
isbn = {9781450314442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2379436.2379437},
doi = {10.1145/2379436.2379437},
abstract = {Solid state disks (or flash disks) are decreasing in cost per gigabyte and are being incorporated into many appliances, such as the Oracle Database Appliance [8]. Databases--and more specifically data warehouses--are often utilized to support large scale data analysis and decision support systems. Decision makers prefer information in real time. Traditional storage systems that are based on magnetic disks achieve high performance by utilizing many disks for parallel operations in RAID arrays. However, this performance is only possible if requests represent a reasonable fraction of the RAID stripe size, or I/O transactions will suffer from high overhead. Solid state disks have the potential to increase the speed of data retrieval for mission critical workloads that require real time applications, such as analytic dashboards. However, solid state disks behave differently than magnetic hard disks due to the limitations of rewriting NAND flash based blocks. Therefore, this work presents benchmark results for a modern relational database that stores data on solid state disks, and contrasts this performance to a ten disk RAID 10 array, a traditional storage design for high performance database data blocks. The preliminary results show that a single solid state disk is able to outperform the array for queries summarizing a data set for a variety of OLAP cube dimensions. Future work will explore the low level database performance in more detail.},
booktitle = {Proceedings of the 2nd Workshop on Architectures and Systems for Big Data},
pages = {4–9},
numpages = {6},
keywords = {solid state disk, query performance},
location = {Portland, Oregon, USA},
series = {ASBD '12}
}

@inproceedings{10.1145/3349341.3349375,
author = {Liu, Yongfu and Zhao, Xin},
title = {Research on the Application of "MOOC" in Modern Distance Education under the Background of Big Data},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349375},
doi = {10.1145/3349341.3349375},
abstract = {Starting from the characteristics of the big data era, the researcher reveals the trend of information-based teaching in the big data era, and the changes of resource view, teaching view and teacher development view brought about by the information-based teaching reform, which is of great significance for deepening the reform of modern distance education curriculum and building a new teaching team. Combining with the actual situation of information-based teaching in the era of big data change, it is considered that flipping classroom, MOOC and micro-course are the first wave of data change education. The significant feature of flipping classroom and micro-course is the innovation of information-based teaching in the field of education in the era of cloud computing and big data.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {93–96},
numpages = {4},
keywords = {MOOC, Big Data, Distance Education},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3047273.3047274,
author = {Shrivastava, Swapnil and Pal, Supriya N.},
title = {A Big Data Analytics Framework for Enterprise Service Ecosystems in an E-Governance Scenario},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047274},
doi = {10.1145/3047273.3047274},
abstract = {In the recent times we have been seeing a fundamental shift from Enterprise Applications towards large scale Enterprise Service Ecosystems. Enterprise Service Ecosystems are developed by modularizing and bundling of individual business rules and functions in the form of services. These services are loosely coupled, distributed and heterogeneous components which orchestrate amongst themselves in a seamless manner. Ecosystem components record the events that are related to the activities performed by them. These components could span across Data Centre, Cloud Infrastructure and Internet of Things. Aadhaar Authentication Ecosystem and e-Governance Service Exchange are examples of Enterprise Service Ecosystems which recently emerged in national e-Governance scenario. A Big Data Analytics Framework for comprehensive mining and analyzing event data of Enterprise Service Ecosystems is proposed in this paper. The offered framework facilitates interesting real time analytics (e.g. Process Conformance Checking, Bottleneck Detection) as well as performing offline analytics (e.g. Process Discovery). The application of the proposed framework for real time analytics is explained using Aadhaar (Unique Identity) Authentication Ecosystem case study.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {5–11},
numpages = {7},
keywords = {Complex Event Processing, Event Data, Enterprise Service Ecosystem, Aadhaar Authentication Ecosystem, Graph Analytics, Big Data Analytics, Process Mining, e-Governance},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/2994539.2994546,
author = {Sillaber, Christian and Sauerwein, Clemens and Mussmann, Andrea and Breu, Ruth},
title = {Data Quality Challenges and Future Research Directions in Threat Intelligence Sharing Practice},
year = {2016},
isbn = {9781450345651},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2994539.2994546},
doi = {10.1145/2994539.2994546},
abstract = {In the last couple of years, organizations have demonstrated an increased willingness to participate in threat intelligence sharing platforms. The open exchange of information and knowledge regarding threats, vulnerabilities, incidents and mitigation strategies results from the organizations' growing need to protect against today's sophisticated cyber attacks. To investigate data quality challenges that might arise in threat intelligence sharing, we conducted focus group discussions with ten expert stakeholders from security operations centers of various globally operating organizations. The study addresses several factors affecting shared threat intelligence data quality at multiple levels, including collecting, processing, sharing and storing data. As expected, the study finds that the main factors that affect shared threat intelligence data stem from the limitations and complexities associated with integrating and consolidating shared threat intelligence from different sources while ensuring the data's usefulness for an inhomogeneous group of participants.Data quality is extremely important for shared threat intelligence. As our study has shown, there are no fundamentally new data quality issues in threat intelligence sharing. However, as threat intelligence sharing is an emerging domain and a large number of threat intelligence sharing tools are currently being rushed to market, several data quality issues -- particularly related to scalability and data source integration -- deserve particular attention.},
booktitle = {Proceedings of the 2016 ACM on Workshop on Information Sharing and Collaborative Security},
pages = {65–70},
numpages = {6},
keywords = {threat intelligence sharing data quality, threat intelligence data, data quality challenges, cyber security operations center},
location = {Vienna, Austria},
series = {WISCS '16}
}

@article{10.1145/3373086,
author = {Guo, Yuan and Sun, Yu and Wu, Kai and Jiang, Kerong},
title = {New Algorithms of Feature Selection and Big Data Assignment for CBR System Integrated by Bayesian Network},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3373086},
doi = {10.1145/3373086},
abstract = {Under big data, the integrated system of case-based reasoning and Bayesian network has exhibited great advantage in implementing the intelligence of engineering application in many domains. To further improve the performance of the hybrid system, this article proposes Probability Change Measurement of Solution Parameters (PCMSP)–Half-Division-Cross (HDC) method, which includes two algorithms, namely PCMSP and HDC algorithm. PCMSP algorithm can select principal problem features according to their effects upon all solution features measured by calculating the weighted relative probability (RP) change of all solution features caused by each problem feature. PCMSP algorithm can perfectly work under big data no matter how complex the data types are and how huge the data size is. HDC algorithm is used to assign the computation task of big data to enhance the efficiency of the integrated system. HDC algorithm assigns big data by grouping all the problem parameters into many small sub-groups and then distributing the data which covers the same sub-group of problem parameters to a slave node. HDC algorithm can guarantee enough efficiency of the integrated system under big data no matter how large the number of problem parameters is. Finally, lots of experiments are executed to validate the proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {feb},
articleno = {23},
numpages = {20},
keywords = {CBR, big data, integrated system, Feature selection}
}

@proceedings{10.1145/3006299,
title = {BDCAT '16: Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Rapid advances in digital sensors, networks, storage, and computation along with their availability at low cost is leading to the creation of huge collections of data---dubbed "Big Data." As a result, a Big Data computing paradigm has emerged, enabling new insights that can change the way business, science, and governments deliver services to their consumers, and can impact society as a whole. BDCAT provides an international forum for researchers and practitioners to present and discuss new discoveries, developments, and results, as well as the latest trends in big data computing, technologies, and applications.},
location = {Shanghai, China}
}

@inproceedings{10.1145/3417188.3417214,
author = {Shu, Wei and Sun, Fuliang and Li, Yueen},
title = {The Development Trend of Design Methodology under the Influence of Artificial Intelligence and Big Data},
year = {2020},
isbn = {9781450375481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417188.3417214},
doi = {10.1145/3417188.3417214},
abstract = {Traditional design methods are inspired by introverted self-salvation or creativity-driven design. In the era of big data, they are gradually driven by vast data. Design innovation without data is increasingly lacking in persuasion. The design of data participation increasingly faces market risks. Moreover, with the progress of artificial intelligence, such a technological innovation will eventually deconstruct the existing field of design innovation, its impact will continue, and it may fundamentally spawn new design ideas and methods.},
booktitle = {Proceedings of the 2020 4th International Conference on Deep Learning Technologies},
pages = {104–108},
numpages = {5},
keywords = {design methodology, innovation, big data, Artificial intelligence},
location = {Beijing, China},
series = {ICDLT '20}
}

@inproceedings{10.1145/3352411.3352417,
author = {Yahya, Farashazillah and Fazli, Bashirah Mohd and Abdullah, Mohd Fikri and Zulkifli, Harlisa},
title = {Extending the National Lake Database of Malaysia (MyLake) as a Central Data Exchange Using Big Data Integration},
year = {2019},
isbn = {9781450371414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352411.3352417},
doi = {10.1145/3352411.3352417},
abstract = {With the rise of heterogeneous large spatial and non-spatial data, systems are developed to manage these data sets. Big data emphasizes heterogeneity among systems leading to data integration issues due to the nature of big data which includes volume, variety and velocity. MyLake is a National Lake Database to manage information and knowledge sharing on lakes in Malaysia. At the moment, data are uploaded by each agency using MyLake as a platform. Nevertheless, this is carried out manually and require timely human effort. Each agency does one-to-one data integration (in-silo) where the integration is developed according to the agency-specific needs resulting from a possibility of integration issue across agencies. Therefore, this paper introduces a big data integration approach that extends the MyLake repository. The big data integration platform is a preliminary idea of how data can be shared, integrated, retrieved, and disseminated within a reliable and authenticated environment. The proposed centralized platform consisting of a set of standards, tools, repository and registry that enable multiple integrations between different agencies. The platform offers the potential to provide a reliable platform that acts as data retriever and disseminator.},
booktitle = {Proceedings of the 2019 2nd International Conference on Data Science and Information Technology},
pages = {30–35},
numpages = {6},
keywords = {Database, Lake, Big Data Integration, Big Data, National Lake, Data Integration},
location = {Seoul, Republic of Korea},
series = {DSIT 2019}
}

@article{10.5555/2675327.2675342,
author = {Mahadev, Aparna and Wurst, Karl R.},
title = {Developing Concentrations in Big Data Analytics and Software Development at a Small Liberal Arts University},
year = {2015},
issue_date = {January 2015},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {30},
number = {3},
issn = {1937-4771},
abstract = {In this paper, the authors share their experiences with the creation and implementation of two concentrations in an undergraduate Computer Science degree program -- one in Big Data Analytics and one in Software Development. We describe the reasoning behind these two concentrations and how these two concentrations came to exist. All Computer Science majors must choose one of these 12 credit hour concentrations after completing the core Computer Science courses. Though there have been many model undergraduate programs and graduate programs in Data Science in the nation [14], [15], [16] we believe that our concentration in Big Data Analytics is a unique offering, especially in our state. The concentration in Big Data Analytics covers predictive analytics, cloud and distributed computing and data mining. The concentration in Software Development follows a modified version of the IEEE/ACM software engineering curriculum recommendations [3]. Both concentrations finish with a semester-long capstone course.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {92–98},
numpages = {7}
}

@proceedings{10.1145/3365109,
title = {BDCAT '19: Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
year = {2019},
isbn = {9781450370165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the program committee, it is our pleasure to welcome you to the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies, being held in Auckland, New Zealand.Rapid advances in digital sensors, networks, storage, and computation along with their availability at low cost is leading to the creation of huge collections of data - dubbed as Big Data. As a result, Big Data Computing paradigm has emerged, enabling new insights that can change the way business, science, and governments deliver services to their consumers, and can impact society as a whole. The IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT) is an annual conference series aiming to provide a platform for researchers from both academia and industry to present new discoveries in the broad area of big data computing and applications.BDCAT 2019 received 47 submissions from 17 countries. The conference accepted 13 papers as regular papers, leading to acceptance rate of 27.7%. The conference also accepted an additional 10 papers as short papers. For this we would like to acknowledge the dedication and tremendous efforts of the program committee and reviewers, who provided nearly 200 reviews in a 3-week turnaround time.},
location = {Auckland, New Zealand}
}

@article{10.1145/3229049,
author = {Malik, Maria and Rafatirad, Setareh and Homayoun, Houman},
title = {System and Architecture Level Characterization of Big Data Applications on Big and Little Core Server Architectures},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2376-3639},
url = {https://doi.org/10.1145/3229049},
doi = {10.1145/3229049},
abstract = {The rapid growth in data yields challenges to process data efficiently using current high-performance server architectures such as big Xeon cores. Furthermore, physical design constraints, such as power and density, have become the dominant limiting factor for scaling out servers.&nbsp;Low-power embedded cores in servers such as little Atom&nbsp;have emerged as a promising solution to enhance energy-efficiency&nbsp;to address these challenges.&nbsp;Therefore, the question of whether to&nbsp;process the big data&nbsp;applications on big Xeon- or Little Atom-based servers becomes important.&nbsp;In this work, through methodical investigation of power and performance measurements, and comprehensive application-level, system-level, and micro-architectural level analysis, we characterize dominant big data applications on big Xeon- and little Atom-based server architectures. The characterization results across a wide range of real-world big data applications, and various software stacks demonstrate how the choice of big- versus little-core-based server for energy-efficiency is significantly influenced by the size of data, performance constraints, and presence of accelerator.&nbsp;In addition, we analyze processor resource utilization of this important class of applications,&nbsp;such as&nbsp;memory footprints, CPU&nbsp;utilization, and disk bandwidth,&nbsp;to understand their run-time behavior.&nbsp;Furthermore, we perform micro-architecture-level analysis to highlight where improvement is needed in big- and little-core microarchitectures to address their performance bottlenecks.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {jul},
articleno = {14},
numpages = {32},
keywords = {Performance, big data, characterization, low-power server, high-performance server, power, accelerator}
}

@inproceedings{10.1145/1146598.1146688,
author = {Karr, Alan F.},
title = {Data Confidentiality, Data Quality and Data Integration for Federal Databases},
year = {2006},
publisher = {Digital Government Society of North America},
url = {https://doi.org/10.1145/1146598.1146688},
doi = {10.1145/1146598.1146688},
abstract = {The high-level goal of the research is to develop abstractions, theory and methodology and software tools that allow federal statistical agencies to disseminate useful information derived from confidential data but protect the privacy of data subjects---individuals and establishments.},
booktitle = {Proceedings of the 2006 International Conference on Digital Government Research},
pages = {332–333},
numpages = {2},
location = {San Diego, California, USA},
series = {dg.o '06}
}

@inproceedings{10.1145/3288599.3295594,
author = {Valluripally, Samaikya and Raju, Murugesan and Calyam, Prasad and Chisholm, Matthew and Sivarathri, Sai Swathi and Mosa, Abu and Joshi, Trupti},
title = {Community Cloud Architecture to Improve Use Accessibility with Security Compliance in Health Big Data Applications},
year = {2019},
isbn = {9781450360944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3288599.3295594},
doi = {10.1145/3288599.3295594},
abstract = {The adoption of big data analytics in healthcare applications is overwhelming not only because of the huge volume of data being analyzed, but also because of the heterogeneity and sensitivity of the data. Effective and efficient analysis and visualization of secure patient health records are needed to e.g., find new trends in disease management, determining risk factors for diseases, and personalized medicine. In this paper, we propose a novel community cloud architecture to help clinicians and researchers to have easy/increased accessibility to data sets from multiple sources, while also ensuring security compliance of data providers is not compromised. Our cloud-based system design configuration with cloudlet principles ensures application performance has high-speed processing, and data analytics is sufficiently scalable while adhering to security standards (e.g., HIPAA, NIST). Through a case study, we show how our community cloud architecture can be implemented along with best practices in an ophthalmology case study which includes health big data (i.e., Health Facts database, I2B2, Millennium) hosted in a campus cloud infrastructure featuring virtual desktop thin-clients and relevant Data Classification Levels in storage.},
booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
pages = {377–380},
numpages = {4},
keywords = {electronic health records, cloud architecture, security standard compliance, smart healthcare, big data application},
location = {Bangalore, India},
series = {ICDCN '19}
}

@inproceedings{10.1145/3152723.3152743,
author = {Phing, Chen Chai and Kiong, Tiong Sieh and Yapandi, Md Fauzan K. Mohd and Paw, Johnny Koh Siaw},
title = {Prediction of NO2 Emission Concentration via Correlation of Multiple Big Data Sources Using Extreme Learning Machine},
year = {2017},
isbn = {9781450353564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152723.3152743},
doi = {10.1145/3152723.3152743},
abstract = {Increase of electricity demand and urbanization process has caused more power plants to be built to meet the demand of electricity. However, development of power plant will cause environmental issue for its surrounding. Necessary measures need to be taken to ensure social and environmental sustainability. Among the requirements in Malaysia, discharge of air pollution emission of a gas- or distillate-fired power plant has to comply with air pollution level as described in the Malaysian Ambient Air Quality Standards ((MAAQS) 2013 and the Environmental Quality (Clean Air) Regulations 2014. Pertaining to the environmental requirements, this paper is to investigate the ability of a regression based artificial intelligence tool, namely Extreme Learning Machine (ELM) in correlating multiple sources of big data sets and subsequently predicting the air pollution emission level from the chimney of a Combined Cycle Gas Turbine (CCGT) power plant. This emission data is later being used to ensure the clean air regulatory requirement is fulfilled. The big data sources that have been used in this work are meteorological data, terrain and land use data, historical emission data and power plant parameters particularly related to the point source emitter. With the correlation of multiple big data sources, Extreme Learning Machine (ELM) is then trained for the prediction of emission rate at certain targeted areas, which are classified as air sensitive receptors (ASR) surrounding the power plant. Nitrogen dioxide (NO2) is the key emission that has been studied in this paper due to its criticality towards environment. A standalone application program has been developed to employ ELM based big data analytics tool for the prediction of NO2 pollution emission. The output of ELM is analyzed to ensure the emission at ground level of ASR is maintained within allowable limit.},
booktitle = {Proceedings of the 2017 International Conference on Big Data Research},
pages = {23–27},
numpages = {5},
keywords = {Emission Prediction, Terrain Profile, Extreme Learning Machine, Meteorology},
location = {Osaka, Japan},
series = {ICBDR 2017}
}

@inproceedings{10.1145/1851476.1851559,
author = {Bui, Hoang and Wright, Diane and Helm, Clarence and Witty, Rachel and Flynn, Patrick and Thain, Douglas},
title = {Towards Long Term Data Quality in a Large Scale Biometrics Experiment},
year = {2010},
isbn = {9781605589428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851476.1851559},
doi = {10.1145/1851476.1851559},
abstract = {Quality of data plays a very important role in any scientific research. In this paper we present some of the challenges that we face in managing and maintaining data quality for a terabyte scale biometrics repository. We have developed a step by step model to capture, ingest, validate, and prepare data for biometrics research. During these processes, there are many hidden errors which can be introduced into the data. Those errors can affect the overall quality of data, and thus can skew the results of biometrics research. We discuss necessary steps we have taken to reduce and eliminate the errors. Steps such as data replication, automated data validation, and logging metadata changes are both necessary and crucial to improve the quality and reliability of our data.},
booktitle = {Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
pages = {565–572},
numpages = {8},
location = {Chicago, Illinois},
series = {HPDC '10}
}

@inproceedings{10.1145/2909132.2927471,
author = {Bornschlegl, Marco X. and Manieri, Andrea and Walsh, Paul and Catarci, Tiziana and Hemmje, Matthias L.},
title = {Road Mapping Infrastructures for Advanced Visual Interfaces Supporting Big Data Applications in Virtual Research Environments},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2927471},
doi = {10.1145/2909132.2927471},
abstract = {Handling the complexity of relevant data requires new techniques about data access, visualization, perception, and interaction for innovative and successful strategies. In order to address human-computer interaction, cognitive eficiency, and interoperability problems, a generic information visualization, user empowerment, as well as service integration and mediation approach based on the existing state-of-the-art in the relevant areas of computer science has to be achieved.This workshop will address these issues with a special focus on supporting distributed Big Data analysis in Virtual Research Environments (VREs). In this way, the overall scope and goal of the workshop is to bring together researchers in these areas to achieve a road map, which can support the acceleration in research activities by means of transforming, enriching, and deploying advanced visual user interfaces for managing and using e-Science infrastructures. Advancements in this fields of research can i.e. support the, creation, configuration, management, and usage of distributed Big Data analysis in VREs.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {363–367},
numpages = {5},
keywords = {User Empowerment, Virtual Research Environments, Distributed Big Data Analysis, Advanced Visual User Interfaces, Information Visualization},
location = {Bari, Italy},
series = {AVI '16}
}

@article{10.1145/269012.269025,
author = {Redman, Thomas C.},
title = {The Impact of Poor Data Quality on the Typical Enterprise},
year = {1998},
issue_date = {Feb. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/269012.269025},
doi = {10.1145/269012.269025},
journal = {Commun. ACM},
month = {feb},
pages = {79–82},
numpages = {4}
}

@inproceedings{10.5555/3199700.3199793,
author = {Lee, Dajung and Althoff, Alric and Richmond, Dustin and Kastner, Ryan},
title = {A Streaming Clustering Approach Using a Heterogeneous System for Big Data Analysis},
year = {2017},
publisher = {IEEE Press},
abstract = {Data clustering is a fundamental challenge in data analytics. It is the main task in exploratory data mining and a core technique in machine learning. As the volume, variety, velocity, and variability of data grows, we need more efficient data analysis methods that can scale towards increasingly large and high dimensional data sets. We develop a streaming clustering algorithm that is highly amenable to hardware acceleration. Our algorithm eliminates the need to store the data objects, which removes limits on the size of the data that we can analyze. Our algorithm is highly parameterizable, which allows it to fit to the characteristics of the data set, and scale towards the available hardware resources. Our streaming hardware core can handle more than 40 Msamples/s when processing 3-dimensional streaming data and up to 1.78 Msamples/s for 70-dimensional data. To validate the accuracy and performance of our algorithms we compare it with several common clustering techniques on several different applications. The experimental result shows that it outperforms other prior hardware accelerated clustering systems.},
booktitle = {Proceedings of the 36th International Conference on Computer-Aided Design},
pages = {699–706},
numpages = {8},
keywords = {FPGA, hardware-software codesign, hardware acceleration, online clustering, streaming architecture, vector quantization},
location = {Irvine, California},
series = {ICCAD '17}
}

@inproceedings{10.5555/2648668.2648748,
author = {Wang, Yuhao and Yu, Hao},
title = {An Ultralow-Power Memory-Based Big-Data Computing Platform by Nonvolatile Domain-Wall Nanowire Devices},
year = {2013},
isbn = {9781479912353},
publisher = {IEEE Press},
abstract = {As one recently introduced non-volatile memory (NVM) device, domain-wall nanowire (or race-track) has shown potential for main memory storage but also computing capability. In this paper, the domain-wall nanowire is studied for a memory-based computing platform towards ultra-low-power big-data processing. One domain-wall nanowire based logic-in-memory architecture is proposed for big-data processing, where the domain-wall nanowire memory is deployed as main memory for data storage as well as XOR-logic for comparison and addition operations. The domain-wall nanowire based logic-in-memory circuits are evaluated by SPICE-level verifications. Further evaluated by applications of general-purpose SPEC2006 benchmark and also web-searching oriented Phoenix benchmark, the proposed computing platform can exhibit a significant power saving on both main memory and ALU under the similar performance when compared to CMOS based designs.},
booktitle = {Proceedings of the 2013 International Symposium on Low Power Electronics and Design},
pages = {329–334},
numpages = {6},
location = {Beijing, China},
series = {ISLPED '13}
}

@inproceedings{10.1145/3335484.3335509,
author = {Liao, Han-Teng and Wang, Zijia and Wu, Xue},
title = {Developing a Minimum Viable Product for Big Data and AI Education: Action Research Based on a Two-Year Reform of an Undergraduate Program of Internet and New Media},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335509},
doi = {10.1145/3335484.3335509},
abstract = {The advancement in Big Data and Artificial Intelligence has posed challenges and opportunities for undergraduate education. It raises several questions regarding what is desirable and viable for preparing undergraduate students for future success. This paper presents the rationales and outcomes of a two-year reform of an undergraduate program of Internet and New Media in China, summarizing the ways in which the curriculum design can incorporate Big Data and Artificial Intelligence education for future Internet product managers and HCI professionals. Using the notion of "minimum viable products" to frame the action research of education reform, it first describes the ways in which we identify the job market need for Internet product managers and HCI professionals, and explores the learning pathways, departing from the conventional Internet and New Media programs in China. It then documents the results of the minimum required changes to deliver such a viable learning product and the initial evidence from students that validates the designed learning product. The overall findings demonstrate the usefulness and challenges in preparing students for careers in a data-intensive or data-driven world.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {42–47},
numpages = {6},
keywords = {Sustainable HCI, Big Data Education, Sustainable Design, Knowledge Brokerage, ICT4D, Ecological Design},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@article{10.1109/TCBB.2016.2581460,
author = {Wang, Chao and Yu, Hong and Wang, Aili and Xia, Kai},
title = {Guest Editorial for Special Section on Big Data Computing and Processing in Computational Biology and Bioinformatics},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2581460},
doi = {10.1109/TCBB.2016.2581460},
abstract = {The papers in this special section focus on big data computing in the field of bioinformatics and biocomputing. Big data has emerged as an important application field which has shown its huge impact in different scientific research domains. In particular, the big data bioinformatics applications such as DNA sequence analysis have posed significant challenges to the state-of-the-art processing and computing systems. With the growing explosive data scale, the collection, storage, retrieval, processing, scheduling, and visualization are key big data issues to be tackled. Up to now, many researchers have been seeking high-level parallelism using novel big data computing architectures and processing mechanisms.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {sep},
pages = {810–811},
numpages = {2}
}

@article{10.1145/2983642,
author = {Wu, Taotao and Dou, Wanchun and Wu, Fan and Tang, Shaojie and Hu, Chunhua and Chen, Jinjun},
title = {A Deployment Optimization Scheme Over Multimedia Big Data for Large-Scale Media Streaming Application},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2983642},
doi = {10.1145/2983642},
abstract = {With the prosperity of media streaming applications over the Internet in the past decades, multimedia data has sharply increased (categorized as multimedia big data), which exerts more pressure on the infrastructure, such as networking of the application provider. In order to move this hurdle, an increasing number of traditional media streaming applications have migrated from a private server cluster onto the cloud. With the elastic resource provisioning and centralized management of the cloud, the operational costs of media streaming application providers can decrease dramatically. However, to the best of our knowledge, existing migration solutions do not fully take viewer information such as hardware condition into consideration. In this article, we consider the deployment optimization problem named ODP by leveraging local memories at each viewer. Considering the NP-hardness of calculating the optimal solution, we turn to propose computationally tractable algorithms. Specifically, we unfold the original problem into two interactive subproblems: coarse-grained migration subproblem and fine-grained scheduling subproblem. Then, the corresponding offline approximation algorithms with performance guarantee and computational efficiency are given. The results of extensive evaluation show that compared with the baseline algorithm without leveraging local memories at viewers, our proposed algorithms and their online versions can decrease total bandwidth reservation and enhance the utilization of bandwidth reservation dramatically.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {oct},
articleno = {73},
numpages = {23},
keywords = {deployment optimization, cloud, Large-scale media streaming application, multimedia big data, local memory}
}

@inproceedings{10.5555/1124191.1124210,
author = {Karr, Alan F.},
title = {Data Confidentiality, Data Quality and Data Integration for Federal Databases},
year = {2004},
publisher = {Digital Government Society of North America},
abstract = {The principal high-level goal of the research is to develop abstractions, theory and methodology and software tools that allow federal statistical agencies to disseminate useful information derived from confidential data but protect the privacy of data subjects---individuals and establishments.},
booktitle = {Proceedings of the 2004 Annual National Conference on Digital Government Research},
articleno = {19},
numpages = {2},
location = {Seattle, WA, USA},
series = {dg.o '04}
}

@inproceedings{10.5555/3021955.3021960,
author = {Paiva, Eduardo and Revoredo, Kate},
title = {Big Data and Transparency: Using MapReduce Functions to Increase Public Expenditure Transparency},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {Nowadays all government entity must maintain transparency portals that shows the all revenue and expenditure carried out daily. However, the mere availability of such information in government portals does not ensure an effective increase in the degree of transparency of these entities, because the large volume of data combined with the lack of standards makes it impossible any systematic monitoring of such data. This paper suggests the application of parallel programming techniques based on mapreduce programming paradigm to the identification of a predetermined set of products purchased by the Public Administration. It also proposes a way to consolidate this information to make easy viewing of disparities found in the large volume of data presented. The proposed solution was tested in a case study performed in the Transparency Portal of the Federal Government. The results suggest that the presented techniques constitute a promising approach to issues related to transparency areas, which normally handles large volumes of data, but it does not always provide quality information.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {25–32},
numpages = {8},
keywords = {text mining, Big data, Public transparency},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI 2016}
}

@inproceedings{10.1109/CCGRID.2017.147,
author = {Runsewe, Olubisi and Samaan, Nancy},
title = {Cloud Resource Scaling for Big Data Streaming Applications Using A Layered Multi-Dimensional Hidden Markov Model},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.147},
doi = {10.1109/CCGRID.2017.147},
abstract = {Recent advancements in technology have led to a deluge of data that require real-time analysis with strict latency constraints. A major challenge, however, is determining the amount of resources required by big data stream processing applications in response to heterogeneous data sources, streaming events, unpredictable data volume and velocity changes. Over-provisioning of resources for peak loads can be wasteful while under-provisioning can have a huge impact on the performance of the streaming applications. The majority of research efforts on resource scaling in the cloud are investigated from the cloud provider's perspective, they focus on web applications and do not consider multiple resource bottlenecks. We aim at analyzing the resource scaling problem from a big data streaming application provider's point of view such that efficient scaling decisions can be made for future resource utilization. This paper proposes a Layered Multi-dimensional Hidden Markov Model (LMD-HMM) for facilitating the management of resource auto-scaling for big data streaming applications in the cloud. Our detailed experimental evaluation shows that LMD-HMM performs best with an accuracy of 98%, outperforming the single-layer hidden markov model.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {848–857},
numpages = {10},
keywords = {Stream Processing, Cloud Computing, Big Data, Resource Prediction, Layered Hidden Markov Model, Resource Scaling},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3352460.3358266,
author = {Chen, Renhai and Shao, Zili and Liu, Duo and Feng, Zhiyong and Li, Tao},
title = {Towards Efficient NVDIMM-Based Heterogeneous Storage Hierarchy Management for Big Data Workloads},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358266},
doi = {10.1145/3352460.3358266},
abstract = {In this paper, we propose a holistic solution to address several important and challenging issues in storage data management in light of emerging NVDIMM-based architecture: namely, new performance modeling, NVDIMM-based migration, and architectural support for NVDIMMs on migration optimization. In particular, a novel NVDIMM-based heterogeneous storage performance model is proposed to effectively address bus contention issues caused by placing NVDIMMs on the memory bus. We also develop an NVDIMM-based lazy migration scheme to effectively minimize adverse effects caused by memory traffic interferences during storage data management processes. Finally, the NVDIMM-based architectural support for migration optimization is proposed to increase channel parallelism in the destination NVDIMMs and bypass buffer caches in the source NVDIMMs, so that the impact of memory traffic can be alleviated. We present detailed evaluation and analysis to quantify how well our techniques can enhance the I/O performances of big workloads via efficient heterogeneous storage hierarchy management. Our experimental results show that overall the proposed techniques yield up to 98% performance improvement over the state-of-the-art techniques.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {849–860},
numpages = {12},
keywords = {bus contention, NVDIMM, machine learning, heterogeneous storage},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@inproceedings{10.1145/3175684.3175717,
author = {Xue, Jiarui and Chen, Xiangzhou and Ding, Huixia and He, Xiao},
title = {Research on Real Time Processing and Intelligent Analysis Technology of Power Big Data},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175717},
doi = {10.1145/3175684.3175717},
abstract = {This paper focuses on the research of all kinds of data in the power grid business system, and carries on the research of intelligent analysis technology. It focuses on breaking the technical difficulties of intelligent and efficient analysis and mining, distributed multi-stream real-time processing, developing large-scale stock data for power and high-frequency incremental data efficient analysis system prototype, for wide-area distributed multi-stream real-time computing, from the data quick access to valuable information to solve the problems in the grid business system to improve the overall business performance, to achieve support power Data real-time processing and other technology applications.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {43–47},
numpages = {5},
keywords = {data flow, data information, distributed, intelligence},
location = {London, United Kingdom},
series = {BDIOT2017}
}

@inproceedings{10.1145/2896825.2896831,
author = {Scavuzzo, Marco and Tamburri, Damian A. and Di Nitto, Elisabetta},
title = {Providing Big Data Applications with Fault-Tolerant Data Migration across Heterogeneous NoSQL Databases},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896831},
doi = {10.1145/2896825.2896831},
abstract = {The recent growing interest on highly-available data-intensive applications sparked the need for flexible and portable storage technologies, e.g., NoSQL databases. Unfortunately, the lack of standard interfaces and architectures for NoSQLs makes it difficult and expensive to create portable applications, which results in vendor lock-in. Building on previous work, we aim at providing guaranteed fault-tolerant techniques and supporting architectures to port or migrate data to and across heterogeneous NoSQL technology. To prove the effectiveness of our approach we evaluate it on an industrial case-study. We conclude that our method and supporting architecture offer an efficient and fault-tolerant mechanism for NoSQL portability and interoperation.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {26–32},
numpages = {7},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/3343413.3377979,
author = {Borgman, Christine L.},
title = {Big Data, Little Data, or No Data? Why Human Interaction with Data is a Hard Problem},
year = {2020},
isbn = {9781450368926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343413.3377979},
doi = {10.1145/3343413.3377979},
abstract = {Enthusiasm for big data is obscuring the complexity and diversity of data in scholarship and the challenges of human interaction and retrieval. Data practices are local, varying from field to field, individual to individual, and country to country. As the number and variety of research partners expands, so do the difficulties of sharing, reusing, and sustaining access to data. Information retrieval is hindered by the lack of agreement on what are "data." Complexities of human interaction with data will be illustrated with empirical examples from environmental sciences, sensor networks, astronomy, biomedicine, and other fields. Unless larger questions of knowledge infrastructures and stewardship are addressed by research communities, "no data" often becomes the norm. Implications for policy and practice in the information sciences will be explored, drawing upon the presenter's book, Big Data, Little Data, No Data: Scholarship in the Networked World (MIT Press, 2015), and subsequent research.},
booktitle = {Proceedings of the 2020 Conference on Human Information Interaction and Retrieval},
pages = {1},
numpages = {1},
keywords = {data science, research, knowledge infrastructures, data, information policy, collaboration, scholarly communication, science},
location = {Vancouver BC, Canada},
series = {CHIIR '20}
}

@inproceedings{10.1145/3102254.3102281,
author = {Jamil, Hasan M. and Rivero, Carlos R.},
title = {A Novel Model for Distributed Big Data Service Composition Using Stratified Functional Graph Matching},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102281},
doi = {10.1145/3102254.3102281},
abstract = {A significant number of current industrial applications rely on web services. A cornerstone task in these applications is discovering a suitable service that meets the threshold of some user needs. Then, those services can be composed to perform specific functionalities. We argue that the prevailing approach to compose services based on the "all or nothing" paradigm is limiting and leads to exceedingly high rejection of potentially suitable services. Furthermore, contemporary models do not allow "mix and match" composition from atomic services of different composite services when binary matching is not possible or desired. In this paper, we propose a new model for service composition based on "stratified graph summarization" and "service stitching". We discuss the limitations of existing approaches with a motivating example, present our approach to overcome these limitations, and outline a possible architecture for service composition from atomic services. Our thesis is that, with the advent of Big Data, our approach will reduce latency in service discovery, and will improve efficiency and accuracy of matchmaking and composition of services.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {34},
numpages = {8},
keywords = {graph matching, web services, service stitching, service discovery, semantic graph matching, graph summarization},
location = {Amantea, Italy},
series = {WIMS '17}
}

@proceedings{10.1145/3090354,
title = {BDCA'17: Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tetouan, Morocco}
}

@proceedings{10.1145/2640087,
title = {BigDataScience '14: Proceedings of the 2014 International Conference on Big Data Science and Computing},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@proceedings{10.1145/2377978,
title = {ASBD '11: Proceedings of the 1st Workshop on Architectures and Systems for Big Data},
year = {2011},
isbn = {9781450314398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Galveston Island, Texas, USA}
}

@inproceedings{10.1145/3208040.3225055,
author = {Matsuoka, Satoshi},
title = {Cambrian Explosion of Computing and Big Data in the Post-Moore Era},
year = {2018},
isbn = {9781450357852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208040.3225055},
doi = {10.1145/3208040.3225055},
abstract = {The so-called "Moore's Law", by which the performance of the processors will increase exponentially by factor of 4 every 3 years or so, is slated to be ending in 10--15 year timeframe due to the lithography of VLSIs reaching its limits around that time, and combined with other physical factors. We are now embarking on a project to revolutionize the total system architectural stack in a holistic fashion in the Post-Moore era, from devices and hardware, abstracted by system software and programming models and languages, and optimized according to the device characteristics with new algorithms and applications that exploit them. Such systems will have multitudes of varieties according to the matching characteristics of applications to the underlying architecture, leading to what can be metaphorically described as Cambrian Explosion of computing systems. The diverse elements of such systems will be interconnected with next-generation terabit optics and networks, allowing metropolitan-scale computing infrastructure that would truly realize high performance parallel and distributed computing. However, which algorithms and applications would benefit the most from such future computing, given that some physical constants, e.g., communication latency, cannot be improved? We speculate on some of the scenarios that would change the nature of current Cloud-centric infrastructures towards the Post-Moore era.},
booktitle = {Proceedings of the 27th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {105},
numpages = {1},
location = {Tempe, Arizona},
series = {HPDC '18}
}

@inproceedings{10.1145/3134847.3134858,
author = {Wan, Jian and Chen, BinBin and Si, Huayou},
title = {Mining and Measurement of Vocational Skills and Their Association Rules Based on Big Data},
year = {2017},
isbn = {9781450352833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134847.3134858},
doi = {10.1145/3134847.3134858},
abstract = {In recent years, researches on vocational skills are very extensive, most of them concentrated on vocational skills training, effect of vocational skill competition, vocational education and vocational skill identification and so on. But, there is very little research on the inner relationship among vocational skill based on big data. To address the issue, from LinkedIn we first collect tens of thousands of member profiles with vocational skills as empirical data. And then, we dig out high-frequency vocational skills and apply correlation analysis method to explore correlation characteristics of vocational skills. According to our studying, we figure out that there exist a large number of association rules, some of which have very high degrees of confidence, lift, and support. This research reveals the intrinsic association among different vocational skills. These relationships can reflect the general behavior and cognitive rules of human beings. We hope that our research results can provide a theoretical base for other research areas of vocational skills, such as vocational skill training, vocational skill mining, and identification of vocational skills.},
booktitle = {Proceedings of the International Conference on Digital Technology in Education},
pages = {59–63},
numpages = {5},
keywords = {Vocational Skill, LinkedIn, Correlation Analysis, Association Rules},
location = {Taipei, Taiwan},
series = {ICDTE '17}
}

@inproceedings{10.1145/2757384.2757390,
author = {Yin, Bo and Shen, Wenlong and Cai, Lin X. and Cheng, Yu},
title = {A Mobile Cloud Computing Middleware for Low Latency Offloading of Big Data},
year = {2015},
isbn = {9781450335249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757384.2757390},
doi = {10.1145/2757384.2757390},
abstract = {Recent years have witnessed an explosive growth of mobile applications. Thanks to improved network connectivity, it becomes a promising enabling solution to offload computation-intensive applications to the resource abundant public cloud to further augment the capacity of resource-constrained devices. As mobile applications usually have QoS requirements, it is critical to provide low latency services to the mobile users while maintain low leasing cost of cloud resources. However, the resources offered by cloud vendors are usually charged based on a time quanta while the offloading demand for heavy-lifting computation may occur infrequently on mobile devices. This mismatch would demotivate users to resort to public cloud for computation offloading. In this paper, we design a computation offloading middleware which bridges the aforementioned gap between cloud vendors and mobile clients, providing offloading service to multiple users with low cost and delay. The proposed middleware has two key components: Task Scheduler and Instance Manager. The Task Scheduler dispatches the received offloading tasks to execute in the instances reserved by the Instance Manager. Based on the arrival pattern of offloading tasks, the Instance Manager dynamically changes the number of instances to ensure certain service grade of mobile users. Our proposed mechanisms are validated through numerical results. It is shown that a lower average delay can be achieved through proposed scheduling heuristic, and the number of reserved instances well adapts to the offloading demands.},
booktitle = {Proceedings of the 2015 Workshop on Mobile Big Data},
pages = {31–35},
numpages = {5},
keywords = {computation offloading, mobile cloud computing},
location = {Hangzhou, China},
series = {Mobidata '15}
}

@inproceedings{10.1145/2751205.2751230,
author = {Zhou, Ruijin and Chen, Huixiang and Li, Tao},
title = {Towards Lightweight and Swift Storage Resource Management in Big Data Cloud Era},
year = {2015},
isbn = {9781450335591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2751205.2751230},
doi = {10.1145/2751205.2751230},
abstract = {Workload IO behavior in modern data centers is fluctuating and unpredictable due to the rapidly adopted, public cloud environment. Nevertheless, existing storage resource management systems, such as VMware SDRS, are incapable of performing real time policy-based storage management due to the high cost of migrating large size virtual disks. Hence, the traditional storage management schemes become ineffective due to the lack of quick response to the frequent IO bursts and the inaccurate storage latency prediction in the light of a highly fluctuating environment. To address the aforementioned issues, we propose LightSRM, which can work properly in a time-variant cloud environment. To mitigate the storage migration cost, we leverage copy-on-write/read snapshots to redirect the IO requests without moving the virtual disk. To support snapshots in storage management, we also build a performance model specifically for snapshots. We employ exponentially weighted moving average with adjustable sliding window to provide quick and accurate performance prediction. Furthermore, we propose a hybrid management scheme, which can dynamically choose either snapshot or migration for fastest performance tuning. We build our prototype in a QEMU/KVM based virtualized environment. Our empirical evaluation results show that snapshot can redirect IO requests in a faster manner than migration can do when the virtual disk size is large. Besides, snapshot method has less disk performance impact on the applications. By employing hybrid snapshot/migration method, LightSRM yields less overall latency, better load balance, and less IO traffic overhead.},
booktitle = {Proceedings of the 29th ACM on International Conference on Supercomputing},
pages = {133–142},
numpages = {10},
keywords = {distributed storage management, storage virtualization, storage migration, snapshot},
location = {Newport Beach, California, USA},
series = {ICS '15}
}

@inproceedings{10.1145/3357223.3362720,
author = {Li, Rui and Guo, Peizhen and Hu, Bo and Hu, Wenjun},
title = {Libra and the Art of Task Sizing in Big-Data Analytic Systems},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3362720},
doi = {10.1145/3357223.3362720},
abstract = {Despite extensive investigation of job scheduling in data-intensive computation frameworks, less consideration has been given to optimizing job partitioning for resource utilization and efficient processing. Instead, partitioning and job sizing are a form of dark art, typically left to developer intuition and trial-and-error style experimentation.In this work, we propose that just as job scheduling and resource allocation are out-sourced to a trusted mechanism external to the workload, so too should be the responsibility for partitioning data as a determinant for task size. Job partitioning essentially involves determining the partition sizes to match the resource allocation at the finest granularity. This is a complex, multi-dimensional problem that is highly application specific: resource allocation, computational runtime, shuffle and reduce communication requirements, and task startup overheads all have strong influence on the most effective task size for efficient processing. Depending on the partition size, the job completion time can differ by as much as 10 times!Fortunately, we observe a general trend underlying the tradeoff between full resource utilization and system overhead across different settings. The optimal job partition size balances these two conflicting forces. Given this trend, we design Libra to automate job partitioning as a framework extension. We integrate Libra with Spark and evaluate its performance on EC2. Compared to state-of-the-art techniques, Libra can reduce the individual job execution time by 25% to 70%.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {364–376},
numpages = {13},
keywords = {Data-analytic Systems, Automatic Task Sizing, Big Data Systems},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings{10.1109/CCGrid.2016.94,
author = {Zhang, Yiqun and Ordonez, Carlos and Cabrera, Wellington},
title = {Big Data Analytics Integrating a Parallel Columnar DBMS and the R Language},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.94},
doi = {10.1109/CCGrid.2016.94},
abstract = {Most research has proposed scalable and parallel analytic algorithms that work outside a DBMS. On the other hand, R has become a very popular system to perform machine learning analysis, but it is limited by main memory and single-threaded processing. Recently, novel columnar DBMSs have shown to provide orders of magnitude improvement in SQL query processing speed, preserving the parallel speedup of row-based parallel DBMSs. With that motivation in mind, we present COLUMNAR, a system integrating a parallel columnar DBMS and R, that can directly compute models on large data sets stored as relational tables. Our algorithms are based on a combination of SQL queries, user-defined functions (UDFs) and R calls, where SQL queries and UDFs compute data set summaries that are sent to R to compute models in RAM. Since our hybrid algorithms exploit the DBMS for the most demanding computations involving the data set, they show linear scalability and are highly parallel. Our algorithms generally require one pass on the data set or a few passes otherwise (i.e. fewer passes than traditional methods). Our system can analyze data sets faster than R even when they fit in RAM and it also eliminates memory limitations in R when data sets exceed RAM size. On the other hand, it is an order of magnitude faster than Spark (a prominent Hadoop system) and a traditional row-based DBMS.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {627–630},
numpages = {4},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/3368640.3368658,
author = {Gerakidis, Sergios and Mamalis, Basilis},
title = {Utilizing the Buckshot Algorithm for Efficient Big Data Clustering in the MapReduce Model},
year = {2019},
isbn = {9781450372923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368640.3368658},
doi = {10.1145/3368640.3368658},
abstract = {Clustering is an efficient data mining as well as machine-learning method when we need to get an insight of the objects of a dataset that could be grouped together. The K-Means algorithm and the Hierarchical Agglomerative Clustering (HAC) algorithm are two of the most known and commonly used methods of clustering; the former due to its low time cost and the latter due to its accuracy. However, even the use of K-Means in document clustering over large-scale collections can lead to unpredictable time costs. In this paper, towards the direction of the efficient handling of big text data, we present a hybrid clustering approach based on a customized version of the Buckshot algorithm, which first applies a hierarchical clustering procedure on a sample of the input dataset and then uses the results as the initial centers for a K-Means based assignment of the remaining documents, with very few iterations. We also give a highly efficient adaptation of the proposed Buckshot-based approach in the MapReduce model which is then experimentally tested using Apache Hadoop over a real cluster environment. As it comes out of the experiments, it leads to acceptable clustering quality as well as to significant execution time improvements. Preliminary results drawn from relevant experiments using the Spark framework are also presented.},
booktitle = {Proceedings of the 23rd Pan-Hellenic Conference on Informatics},
pages = {112–117},
numpages = {6},
keywords = {big data, spark, hierarchical agglomerative clustering, buckshot algorithm, K-Means, MapReduce},
location = {Nicosia, Cyprus},
series = {PCI '19}
}

@article{10.1145/3130972,
author = {van Berkel, Niels and Goncalves, Jorge and Hosio, Simo and Kostakos, Vassilis},
title = {Gamification of Mobile Experience Sampling Improves Data Quality and Quantity},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130972},
doi = {10.1145/3130972},
abstract = {The Experience Sampling Method is used to capture high-quality in situ data from study participants. This method has become popular in studies involving smartphones, where it is often adapted to motivate participation through the use of gamification techniques. However, no work to date has evaluated whether gamification actually affects the quality and quantity of data collected through Experience Sampling. Our study systematically investigates the effect of gamification on the quantity and quality of experience sampling responses on smartphones. In a field study, we combine event contingent and interval contingent triggers to ask participants to describe their location. Subsequently, participants rate the quality of these entries by playing a game with a purpose. Our results indicate that participants using the gamified version of our ESM software provided significantly higher quality responses, slightly increased their response rate, and provided significantly more data on their own accord. Our findings suggest that gamifying experience sampling can improve data collection and quality in mobile settings.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {107},
numpages = {21},
keywords = {labeling, location, experience sampling method, human behavior, ESM, EMA, CSCW, crowdsensing, motivation, sensing}
}

@inproceedings{10.5555/1065226.1065289,
author = {Karr, Alan F.},
title = {Data Confidentiality, Data Quality and Data Integration for Federal Databases},
year = {2005},
publisher = {Digital Government Society of North America},
abstract = {The high-level goal of the research is to develop abstractions, theory and methodology and software tools that allow federal statistical agencies to disseminate useful information derived from confidential data but protect the privacy of data subjects---individuals and establishments.},
booktitle = {Proceedings of the 2005 National Conference on Digital Government Research},
pages = {217–218},
numpages = {2},
location = {Atlanta, Georgia, USA},
series = {dg.o '05}
}

@article{10.14778/2536222.2536242,
author = {Bedini, Ivan and Elser, Benedikt and Velegrakis, Yannis},
title = {The Trento Big Data Platform for Public Administration and Large Companies: Use Cases and Opportunities},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536222.2536242},
doi = {10.14778/2536222.2536242},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1166–1167},
numpages = {2}
}

@inproceedings{10.1145/2811411.2811481,
author = {Han, Youngsub and Lee, Hyeoncheol and Kim, Yanggon},
title = {A Real-Time Knowledge Extracting System from Social Big Data Using Distributed Architecture},
year = {2015},
isbn = {9781450337380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811411.2811481},
doi = {10.1145/2811411.2811481},
abstract = {A huge amount of data is being generated by social media in real time. Accordingly, demands for extracting meaningful information from the social data have been dramatically increased. However, most of the previous research encompasses potential problems with data processing, management and analysis in real time. In this paper, we propose a distributed system architecture for generating meaningful information from text-based social data. The system collects data from multi-source channels, such as Twitter, YouTube, and The New York Times. Also, the system extracts terms and sentiment from each document using data mining technologies. In addition, the system uses HDFS, Map-reduce, and message service to handle the huge data. By analyzing keywords in texts and user account information, the system generates a summary of results including terms, sentiments and data variations for further analysis, including reputation, social trends, and customer reactions. The experiment results show that our approach is able to effectively process the social data in real time.},
booktitle = {Proceedings of the 2015 Conference on Research in Adaptive and Convergent Systems},
pages = {74–79},
numpages = {6},
keywords = {big data, message driven processing, distributed computing, crawling, sentiment analysis, data mining, natural language processing, Hadoop},
location = {Prague, Czech Republic},
series = {RACS}
}

@proceedings{10.1145/2379436,
title = {ASBD '12: Proceedings of the 2nd Workshop on Architectures and Systems for Big Data},
year = {2012},
isbn = {9781450314442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Portland, Oregon, USA}
}

@inproceedings{10.1145/3219104.3229279,
author = {Li, Yu and Zhang, Xiaohong and Srinath, Ashwin and Getman, Rachel B. and Ngo, Linh B.},
title = {Combining HPC and Big Data Infrastructures in Large-Scale Post-Processing of Simulation Data: A Case Study},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3229279},
doi = {10.1145/3219104.3229279},
abstract = {Advances in scientific software and computing infrastructure have enabled researchers across disciplines to simulate and model highly complex systems. At the same time, these increases in simulation duration and scale have led to significant growths in the sizes of output data, which can be as much as hundreds of gigabytes or more. While there exist solutions to assist with most standard post-simulation analytics, researchers must develop their own code to support customized analytical tasks. Given the nature of these output data, most naive in-house sequential codes end up being inefficient, and in most cases, time-consuming. In this paper, we propose a solution to this issue by transparently combining the strengths of a high-performance computing cluster and a big data infrastructure to support an end-to-end scientific workflow. More specifically, we present a case study around the design of a research computing environment at Clemson University where these two computing systems are integrated and accessible from one another. This environment allows simulation data to be automatically transferred across systems and complex analytical tasks on these data to be developed using the Hadoop/Spark frameworks. Results show that a hybrid workflow for molecular dynamics simulation can provide significant performance improvements over a traditional workflow. Furthermore, code complexity of Hadoop/Spark solutions is shown to be less than that of a traditional solution.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {41},
numpages = {7},
keywords = {Apache Hadoop, Apache Spark, HPC, Big Data, Molecular Dynamics Simulation},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@inproceedings{10.1145/2428736.2428779,
author = {Tyukov, Anton and Brebels, Adriaan and Shcherbakov, Maxim and Kamaev, Valeriy},
title = {A Concept of Web-Based Energy Data Quality Assurance and Control System},
year = {2012},
isbn = {9781450313063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2428736.2428779},
doi = {10.1145/2428736.2428779},
abstract = {Development both of sensor and data communication technologies made a serious impact in development of SCADA systems in various domains. Possibility of access to sensor data in real time mode changed methods of monitoring and control. Data quality control is essential for next generation monitoring and control systems which acts on issues in data and performs model predictive control of equipment connected to data acquisition devices. Authors offer a concept of web-based real-time data quality evaluation and improvement system for energy domain. The system evaluates data in real-time based on a set of rules applied by the operator (identifying gaps, outliers, anomalies etc) and classifies cases into categories: errors, warnings and statistics. All cases are registered and tracked by the system. The system creates reports, improves data automatically (performs data imputation using curtain interpolation algorithm) or reacts on manual data modification. domain specific query language allows to create user-defined rules. It contains advanced data checks based on statistics and machine learning methods. The system is designed to be domain independent and to be able to process data from different data sources. The concept was designed and implemented in program code. The functionality of it can be extended by plug-ins written according to special requirements.},
booktitle = {Proceedings of the 14th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {267–271},
numpages = {5},
keywords = {forecasting, SCADA, data quality assurance and control, energy, monitoring system, data mining, data management system},
location = {Bali, Indonesia},
series = {IIWAS '12}
}

@article{10.1145/3392050,
author = {Walker, Henry M.},
title = {CLASSROOM VIGNETTES Bias in Algorithms and the Misuse of Big Data Sets},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2153-2184},
url = {https://doi.org/10.1145/3392050},
doi = {10.1145/3392050},
journal = {ACM Inroads},
month = {may},
pages = {12–17},
numpages = {6}
}

@article{10.1145/2639988.2661641,
author = {Daries, Jon P. and Reich, Justin and Waldo, Jim and Young, Elise M. and Whittinghill, Jonathan and Seaton, Daniel Thomas and Ho, Andrew Dean and Chuang, Isaac},
title = {Privacy, Anonymity, and Big Data in the Social Sciences: Quality Social Science Research and the Privacy of Human Subjects Requires Trust.},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {7},
issn = {1542-7730},
url = {https://doi.org/10.1145/2639988.2661641},
doi = {10.1145/2639988.2661641},
abstract = {Open data has tremendous potential for science, but, in human subjects research, there is a tension between privacy and releasing high-quality open data. Federal law governing student privacy and the release of student records suggests that anonymizing student data protects student privacy. Guided by this standard, we de-identified and released a data set from 16 MOOCs (massive open online courses) from MITx and HarvardX on the edX platform. In this article, we show that these and other de-identification procedures necessitate changes to data sets that threaten replication and extension of baseline analyses. To balance student privacy and the benefits of open data, we suggest focusing on protecting privacy without anonymizing data by instead expanding policies that compel researchers to uphold the privacy of the subjects in open data sets. If we want to have high-quality social science research and also protect the privacy of human subjects, we must eventually have trust in researchers. Otherwise, we’ll always have the strict tradeoff between anonymity and science illustrated here.},
journal = {Queue},
month = {jul},
pages = {30–41},
numpages = {12}
}

@article{10.1145/5465.5466,
author = {Laudon, Kenneth C.},
title = {Data Quality and Due Process in Large Interorganizational Record Systems},
year = {1986},
issue_date = {Jan. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/5465.5466},
doi = {10.1145/5465.5466},
abstract = {As societies have become more dependent on information systems to conduct and record transactions between organizations and individuals, interorganizational computer systems have become a widely used method of coordinating the actions of independent organizations. This article examines the quality of data in one important interorganizational system—the criminal-record system of the United States.},
journal = {Commun. ACM},
month = {jan},
pages = {4–11},
numpages = {8}
}

