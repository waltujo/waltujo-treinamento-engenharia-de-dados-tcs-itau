@inproceedings{10.1145/3132747.3132773,
author = {Schlaipfer, Matthias and Rajan, Kaushik and Lal, Akash and Samak, Malavika},
title = {Optimizing Big-Data Queries Using Program Synthesis},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132773},
doi = {10.1145/3132747.3132773},
abstract = {Classical query optimization relies on a predefined set of rewrite rules to re-order and substitute SQL operators at a logical level. This paper proposes Blitz, a system that can synthesize efficient query-specific operators using automated program reasoning. Blitz uses static analysis to identify sub-queries as potential targets for optimization. For each sub-query, it constructs a template that defines a large space of possible operator implementations, all restricted to have linear time and space complexity. Blitz then employs program synthesis to instantiate the template and obtain a data-parallel operator implementation that is functionally equivalent to the original sub-query up to a bound on the input size.Program synthesis is an undecidable problem in general and often difficult to scale, even for bounded inputs. Blitz therefore uses a series of analyses to judiciously use program synthesis and incrementally construct complex operators.We integrated Blitz with existing big-data query languages by embedding the synthesized operators back into the query as User Defined Operators. We evaluated Blitz on several production queries from Microsoft running on two state-of-the-art query engines: SparkSQL as well as Scope, the big-data engine of Microsoft. Blitz produces correct optimizations despite the synthesis being bounded. The resulting queries have much more succinct query plans and demonstrate significant performance improvements on both big-data systems (1.3x --- 4.7x).},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {631–646},
numpages = {16},
keywords = {Query Optimization, Program Synthesis, User-Defined Operators},
location = {Shanghai, China},
series = {SOSP '17}
}

@article{10.5555/3204979.3204985,
author = {Mackey, Andrew and Cuevas, Israel},
title = {Automatic Text Summarization within Big Data Frameworks},
year = {2018},
issue_date = {May 2018},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {33},
number = {5},
issn = {1937-4771},
abstract = {Data increasing at unabated rates will prove to be challenging for individuals trying to effectively leverage it. This paper describes an approach to automatically summarize text documents by utilizing statistical and information retrieval methodologies within big data frameworks that support the distributed parallel processing of data. A pedagogical approach to the incorporation of automatic summarization and big data topics within data mining, machine learning, or natural language processing courses is detailed.},
journal = {J. Comput. Sci. Coll.},
month = {may},
pages = {26–32},
numpages = {7}
}

@inproceedings{10.1145/2591971.2611389,
author = {Stoica, Ion},
title = {Conquering Big Data with Spark and BDAS},
year = {2014},
isbn = {9781450327893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591971.2611389},
doi = {10.1145/2591971.2611389},
abstract = {Today, big and small organizations alike collect huge amounts of data, and they do so with one goal in mind: extract "value" through sophisticated exploratory analysis, and use it as the basis to make decisions as varied as personalized treatment and ad targeting. Unfortunately, existing data analytics tools are slow in answering queries, as they typically require to sift through huge amounts of data stored on disk, and are even less suitable for complex computations, such as machine learning algorithms. These limitations leave the potential of extracting value of big data unfulfilled.To address this challenge, we are developing Berkeley Data Analytics Stack (BDAS), an open source data analytics stack that provides interactive response times for complex computations on massive data. To achieve this goal, BDAS supports efficient, large-scale in-memory data processing, and allows users and applications to trade between query accuracy, time, and cost. In this talk, I'll present the architecture, challenges, results, and our experience with developing BDAS, with a focus on Apache Spark, an in-memory cluster computing engine that provides support for a variety of workloads, including batch, streaming, and iterative computations. In a relatively short time, Spark has become the most active big data project in the open source community, and is already being used by over one hundred of companies and research institutions.},
booktitle = {The 2014 ACM International Conference on Measurement and Modeling of Computer Systems},
pages = {193},
numpages = {1},
keywords = {distributed algorithms, big data, cluster computing},
location = {Austin, Texas, USA},
series = {SIGMETRICS '14}
}

@article{10.1145/2637364.2611389,
author = {Stoica, Ion},
title = {Conquering Big Data with Spark and BDAS},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2637364.2611389},
doi = {10.1145/2637364.2611389},
abstract = {Today, big and small organizations alike collect huge amounts of data, and they do so with one goal in mind: extract "value" through sophisticated exploratory analysis, and use it as the basis to make decisions as varied as personalized treatment and ad targeting. Unfortunately, existing data analytics tools are slow in answering queries, as they typically require to sift through huge amounts of data stored on disk, and are even less suitable for complex computations, such as machine learning algorithms. These limitations leave the potential of extracting value of big data unfulfilled.To address this challenge, we are developing Berkeley Data Analytics Stack (BDAS), an open source data analytics stack that provides interactive response times for complex computations on massive data. To achieve this goal, BDAS supports efficient, large-scale in-memory data processing, and allows users and applications to trade between query accuracy, time, and cost. In this talk, I'll present the architecture, challenges, results, and our experience with developing BDAS, with a focus on Apache Spark, an in-memory cluster computing engine that provides support for a variety of workloads, including batch, streaming, and iterative computations. In a relatively short time, Spark has become the most active big data project in the open source community, and is already being used by over one hundred of companies and research institutions.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {193},
numpages = {1},
keywords = {cluster computing, big data, distributed algorithms}
}

@inproceedings{10.1145/2513190.2513198,
author = {Ordonez, Carlos},
title = {Can We Analyze Big Data inside a DBMS?},
year = {2013},
isbn = {9781450324120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513190.2513198},
doi = {10.1145/2513190.2513198},
abstract = {Relational DBMSs remain the main data management technology, despite the big data analytics and no-SQL waves. On the other hand, for data analytics in a broad sense, there are plenty of non-DBMS tools including statistical languages, matrix packages, generic data mining programs and large-scale parallel systems, being the main technology for big data analytics. Such large-scale systems are mostly based on the Hadoop distributed file system and MapReduce. Thus it would seem a DBMS is not a good technology to analyze big data, going beyond SQL queries, acting just as a reliable and fast data repository. In this survey, we argue that is not the case, explaining important research that has enabled analytics on large databases inside a DBMS. However, we also argue DBMSs cannot compete with parallel systems like MapReduce to analyze web-scale text data. Therefore, each technology will keep influencing each other. We conclude with a proposal of long-term research issues, considering the "big data analytics" trend.},
booktitle = {Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP},
pages = {85–92},
numpages = {8},
keywords = {mapreduce, parallel algorithms, big data, dbms, sql},
location = {San Francisco, California, USA},
series = {DOLAP '13}
}

@inproceedings{10.1109/CCGrid.2016.107,
author = {Leung, Carson K. and Zhang, Hao},
title = {Management of Distributed Big Data for Social Networks},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.107},
doi = {10.1109/CCGrid.2016.107},
abstract = {In the current era of big data, high volumes of a wide variety of valuable data can be easily collected and generated from a broad range of data sources of different veracities at a high velocity. Due to the well-known 5V's of these big data, many traditional data management approaches may not be suitable for handling the big data. Over the past few years, several applications and systems have developed to use cluster, cloud or grid computing to manage big data so as to support data science, big data analytics, as well as knowledge discovery and data mining. In this paper, we focus on distributed big data management. Specifically, we present our method for big data representation and management of distributed big data from social networks. We represent such big graph data in distributed settings so as to support big data mining of frequently occurring patterns from social networks.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {639–648},
numpages = {10},
keywords = {distributed data, data analytics, big data, data representation, graph data, big data management, distributed computing},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/2903150.2917755,
author = {Girone, Maria},
title = {Big Data Analytics and the LHC},
year = {2016},
isbn = {9781450341288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2903150.2917755},
doi = {10.1145/2903150.2917755},
abstract = {The Large Hadron Collider is one of the largest and most complicated pieces of scientific apparatus ever constructed. The detectors along the LHC ring see as many as 800 million proton-proton collisions per second. An event in 10 to the 11th power is new physics and there is a hierarchical series of steps to extract a tiny signal from an enormous background. High energy physics (HEP) has long been a driver in managing and processing enormous scientific datasets and the largest scale high throughput computing centers. HEP developed one of the first scientific computing grids that now regularly operates 500k processor cores and half of an exabyte of disk storage located on 5 continents including hundred of connected facilities. In this presentation I will discuss the techniques used to extract scientific discovery from a large and complicated dataset. While HEP has developed many tools and techniques for handling big datasets, there is an increasing desire within the field to make more effective use of additional industry developments. I will discuss some of the ongoing work to adopt industry techniques in big data analytics to improve the discovery potential of the LHC and the effectiveness of the scientists who work on it.},
booktitle = {Proceedings of the ACM International Conference on Computing Frontiers},
pages = {ii},
location = {Como, Italy},
series = {CF '16}
}

@inproceedings{10.1145/3168390.3168425,
author = {Samosir, Ridha Sefina and Hendric, Harco Leslie and Gaol, Ford Lumban and Abdurachman, Edi and Soewito, Benfano},
title = {Measurement Metric Proposed For Big Data Analytics System},
year = {2017},
isbn = {9781450353922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168390.3168425},
doi = {10.1145/3168390.3168425},
abstract = {Big data is defined as a very large data set (volume), velocity and variety. Big data analytics systems must be supports for parallel processing and large storage. The problem of this research is how to identify measurement metric based on big data analytics system characteristic. One device that support big data platform is Hadoop. Measurement is a process for assigning values or symbols to the attributes of an entity. The purpose of measurement is to distinguish between entities one to another. Indicator for software measurement represented with a metric. The aim of this research is to proposes some measurement metric for big data analytics system. This research using UML exactly a class diagram in system modelling to identify the measurement metric. Both of dynamic and static metric is proposed as solution to measure big data analytics system. Result for this researh are some measurement ndicator both of dynamic and static metric based on class diagram for big data analytics.},
booktitle = {Proceedings of the 2017 International Conference on Computer Science and Artificial Intelligence},
pages = {265–269},
numpages = {5},
keywords = {Metric, Measurement, Big Data Analytics, Software},
location = {Jakarta, Indonesia},
series = {CSAI 2017}
}

@inproceedings{10.1145/3305160.3305211,
author = {Sun, Zhaohao and Huo, Yanxia},
title = {A Managerial Framework for Intelligent Big Data Analytics},
year = {2019},
isbn = {9781450366427},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3305160.3305211},
doi = {10.1145/3305160.3305211},
abstract = {Intelligent big data analytics is an emerging paradigm for integrating big data, analytics, and artificial intelligence. The objective of this paper is to provide a managerial framework of intelligent big data analytics. More specifically, this paper proposes a managerial framework of intelligent big data analytics, which consists of intelligent big data analytics as a science, technology, system, service and management for improving business decision making. Then it elaborates intelligent big data analytics for management taking into account main managerial functions: planning, organising, leading and controlling. The proposed approach in this paper might facilitate the research and development of business analytics, big data analytics, business intelligence, artificial intelligence and data science.},
booktitle = {Proceedings of the 2nd International Conference on Software Engineering and Information Management},
pages = {152–156},
numpages = {5},
keywords = {artificial intelligence, intelligent analytics, intelligent big data analytics, management analytics},
location = {Bali, Indonesia},
series = {ICSIM 2019}
}

@inproceedings{10.1145/3106237.3117774,
author = {Garbervetsky, Diego and Pavlinovic, Zvonimir and Barnett, Michael and Musuvathi, Madanlal and Mytkowicz, Todd and Zoppi, Edgardo},
title = {Static Analysis for Optimizing Big Data Queries},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3117774},
doi = {10.1145/3106237.3117774},
abstract = {Query languages for big data analysis provide user extensibility through a mechanism of user-defined operators (UDOs). These operators allow programmers to write proprietary functionalities on top of a relational query skeleton. However, achieving effective query optimization for such languages is extremely challenging since the optimizer needs to understand data dependencies induced by UDOs. SCOPE, the query language from Microsoft, allows for hand coded declarations of UDO data dependencies. Unfortunately, most programmers avoid using this facility since writing and maintaining the declarations is tedious and error-prone. In this work, we designed and implemented two sound and robust static analyses for computing UDO data dependencies. The analyses can detect what columns of an input table are never used or pass-through a UDO unchanged. This information can be used to significantly improve execution of SCOPE scripts. We evaluate our analyses on thousands of real-world queries and show we can catch many unused and pass-through columns automatically without relying on any manually provided declarations.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {932–937},
numpages = {6},
keywords = {Static analysis, Big Data, UDOs, Query optimization},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/2896825.2896828,
author = {Zareian, Saeed and Fokaefs, Marios and Khazaei, Hamzeh and Litoiu, Marin and Zhang, Xi},
title = {A Big Data Framework for Cloud Monitoring},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896828},
doi = {10.1145/2896825.2896828},
abstract = {Elasticity is a key component of modern cloud environments and monitoring is an essential part of this process. Monitoring demonstrates several challenges including gathering metrics from a variety of layers (infrastructure, platform, application), the need for fast processing of this data to enable efficient elasticity and the proper management of this data in order to facilitate analysis of current and past data and future predictions. In this work, we classify monitoring as a big data problem and propose appropriate solutions in a layered, pluggable and extendable architecture for a monitoring component. More specifically, we propose the use of NoSQL databases as the back-end and BigQueue as a write buffer to achieve high throughput. Our evaluation shows that our monitoring is capable of achieving response time of a few hundreds of milliseconds for the insertion of hundreds of rows regardless of the underlying NoSQL database.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {58–64},
numpages = {7},
keywords = {NoSQL datastores, performance analysis, monitoring system, cloud applications, big data},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/3084381.3084422,
author = {Eachempati, Prajwal and Srivastava, Praveen Ranjan},
title = {Systematic Literature Review of Big Data Analytics},
year = {2017},
isbn = {9781450350372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084381.3084422},
doi = {10.1145/3084381.3084422},
abstract = {This paper aims to identify some emerging sectors that apply big-data analytics through a systematic literature review conducted by capturing the existing work done in this subject area by academicians and industry experts worldwide and specifically in India backed by a detailed domain-wise, nation-wise and within India, an institute-wise analysis of the contributions made. Based on the existing work, the need for applying analytics in Banking and Finance is emphasized through the paper and a premise is provided for conducting future research in this domain.},
booktitle = {Proceedings of the 2017 ACM SIGMIS Conference on Computers and People Research},
pages = {177–178},
numpages = {2},
keywords = {scopus, big-data, analytics, databases, business, finance, domains, tools},
location = {Bangalore, India},
series = {SIGMIS-CPR '17}
}

@inproceedings{10.1145/2663715.2669614,
author = {Cuzzocrea, Alfredo},
title = {Privacy and Security of Big Data: Current Challenges and Future Research Perspectives},
year = {2014},
isbn = {9781450315838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663715.2669614},
doi = {10.1145/2663715.2669614},
abstract = {Privacy and security of Big Data is gaining momentum in the research community, also due to emerging technologies like Cloud Computing, analytics engines and social networks. In response of this novel research challenge, several privacy and security of big data models, techniques and algorithms have been proposed recently, mostly adhering to algorithmic paradigms or model-oriented paradigms. Following this major trend, in this paper we provide an overview of state-of-the-art research issues and achievements in the field of privacy and security of big data, by highlighting open problems and actual research trends, and drawing novel research directions in this field.},
booktitle = {Proceedings of the First International Workshop on Privacy and Secuirty of Big Data},
pages = {45–47},
numpages = {3},
keywords = {secure query processing over big data, privacy-preserving analytics over big data, security of big data, privacy of big data},
location = {Shanghai, China},
series = {PSBD '14}
}

@inproceedings{10.1145/3290420.3290466,
author = {Jia, Shuli and Ma, Liyong and Zhang, Shuting},
title = {Big Data Prototype Practice for Unmanned Surface Vehicle},
year = {2018},
isbn = {9781450365345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290420.3290466},
doi = {10.1145/3290420.3290466},
abstract = {In recent years, unmanned surface vehicle (USV) has made great progress with the development of communication and electronic technologies. In the future, USVs will be widely used in marine tasks. USV has begun to offer a wide variety of large heterogeneous data sources. The architecture of USV big data is discussed. A prototype system of USV big data is established, and a preliminary anomaly detection application based on isolation forest method is developed. The effective anomaly detection results show that the USV big data prototype system and the anomaly detection application is feasible. The establishment of USV big data system is meaningful and the system can provide effective application services.},
booktitle = {Proceedings of the 4th International Conference on Communication and Information Processing},
pages = {43–47},
numpages = {5},
keywords = {isolation forest, big data, unmanned surface vehicle (USV)},
location = {Qingdao, China},
series = {ICCIP '18}
}

@inproceedings{10.1145/3312614.3312623,
author = {Khan, Nawsher and Naim, Arshi and Hussain, Mohammad Rashid and Naveed, Quadri Noorulhasan and Ahmad, Naim and Qamar, Shamimul},
title = {The 51 V's Of Big Data: Survey, Technologies, Characteristics, Opportunities, Issues and Challenges},
year = {2019},
isbn = {9781450366403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312614.3312623},
doi = {10.1145/3312614.3312623},
abstract = {Currently Big Data is the biggest buzzword, and definitely, we believe that Big Data is changing the world. Some researchers say Big Data will be even bigger buzzword than the Internet. With fast-growing computing resources, information and knowledge a new digital globe has emerged. Information is being created and stored at a fast rate and is being accessed by a vast range of applications through scientific computing, commercial workloads, and social media. In 2018, over 28 billion devices globally, are connected to the internet. In 2020, more than 50 billion smart appliances will be connected worldwide and internet traffic flow will be 92 times greater than it was in 2005. The usage of such a massive number of connected devices not only increase the data volume but also the velocity of data addition with speed of light on fiber optic and various wireless networks. This fast generation of enormous data creates numerous threats and challenges. There exist various approaches that are addressing issues and challenges of Big Data with the theory of Vs such as 3 V's, 5 V's, 7 V's etc. The objective of this work is to explore and investigate the status of the current Big Data domain. Further, a comprehensive overview of Big Data, its characteristics, opportunities, issues, and challenges have been explored and described with the help of 51 V's. The outcome of this research will help in understanding the Big Data in a systematic way.},
booktitle = {Proceedings of the International Conference on Omni-Layer Intelligent Systems},
pages = {19–24},
numpages = {6},
keywords = {data characteristics, data storage, data generation, Big Data},
location = {Crete, Greece},
series = {COINS '19}
}

@inproceedings{10.1145/3386415.3386964,
author = {Chen, Lijun and Pan, Zhengjun and Yuan, Lina},
title = {Study on Clustering Computing Methods of Big Data},
year = {2020},
isbn = {9781450372930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386415.3386964},
doi = {10.1145/3386415.3386964},
abstract = {In the past few years, the rapidly developing technology in the field of information technology is "big data". Clustering is one of the key tasks in a wide range of areas dealing with large amounts of data. This survey introduces various clustering methods used for effective big data clustering. Therefore, this review paper reviewed 15 research papers, which proposed various methods for effective big data clustering, such as k-means clustering, k-means variant clustering, fuzzy c-means clustering, possibility c-means clustering, collaborative filtering and optimization based clustering. In addition, detailed analysis is carried out by referring to the implementation tools used, the data sets used and the big data clustering framework adopted. Then, an effective solution must be developed to go beyond the existing technology to the special management of big data. Finally, the research problems and gaps of various big data clustering technologies are proposed to enable researchers to start with better big data clustering.},
booktitle = {Proceedings of the 2nd International Conference on Information Technologies and Electrical Engineering},
articleno = {17},
numpages = {5},
keywords = {Reduce, C-mean, Cluster, Big data, Map, K-mean},
location = {Zhuzhou, Hunan, China},
series = {ICITEE-2019}
}

@inproceedings{10.1145/2694730.2694733,
author = {Panda, Dhabaleswar K.},
title = {Accelerating Big Data Processing on Modern Clusters},
year = {2015},
isbn = {9781450333382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694730.2694733},
doi = {10.1145/2694730.2694733},
abstract = {Modern clusters are having multi-/many-core architectures, high-performance rdma-enabled interconnects and SSD-based storage devices. Hadoop framework is extensively being used these days for Big Data processing. Spark framework is emerging for real-time analytics. Similarly, Memcached is being used in data centers with Web 2.0 environment. This talk will provide an overview of challenges in accelerating Hadoop, Spark and Memcached on modern clusters. An overview of RDMA-based designs for multiple components of Hadoop (HDFS, MapReduce, RPC and HBase), Spark and Memcached will be presented. Performance benefits of these designs on various cluster configurations will be shown. The talk will also address the need for designing benchmarks using a multi-layered and systematic approach, which can be used to evaluate the performance of these middleware.},
booktitle = {Proceedings of the 1st Workshop on Performance Analysis of Big Data Systems},
pages = {1},
numpages = {1},
keywords = {hpc, big data, acceleration},
location = {Austin, Texas, USA},
series = {PABS '15}
}

@article{10.1007/s00778-014-0357-y,
author = {Alexandrov, Alexander and Bergmann, Rico and Ewen, Stephan and Freytag, Johann-Christoph and Hueske, Fabian and Heise, Arvid and Kao, Odej and Leich, Marcus and Leser, Ulf and Markl, Volker and Naumann, Felix and Peters, Mathias and Rheinl\"{a}nder, Astrid and Sax, Matthias J. and Schelter, Sebastian and H\"{o}ger, Mareike and Tzoumas, Kostas and Warneke, Daniel},
title = {The Stratosphere Platform for Big Data Analytics},
year = {2014},
issue_date = {December  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-014-0357-y},
doi = {10.1007/s00778-014-0357-y},
abstract = {We present Stratosphere, an open-source software stack for parallel data analysis. Stratosphere brings together a unique set of features that allow the expressive, easy, and efficient programming of analytical applications at very large scale. Stratosphere's features include "in situ" data processing, a declarative query language, treatment of user-defined functions as first-class citizens, automatic program parallelization and optimization, support for iterative programs, and a scalable and efficient execution engine. Stratosphere covers a variety of "Big Data" use cases, such as data warehousing, information extraction and integration, data cleansing, graph analysis, and statistical analysis applications. In this paper, we present the overall system architecture design decisions, introduce Stratosphere through example queries, and then dive into the internal workings of the system's components that relate to extensibility, programming model, optimization, and query execution. We experimentally compare Stratosphere against popular open-source alternatives, and we conclude with a research outlook for the next years.},
journal = {The VLDB Journal},
month = {dec},
pages = {939–964},
numpages = {26},
keywords = {Data cleansing, Text mining, Query processing, Big data, Graph processing, Distributed systems, Query Optimization, Parallel databases}
}

@inproceedings{10.1145/3134472.3134516,
author = {Nguyen, Quang Vinh and Engelke, Ulrich},
title = {Big Data Visual Analytics: Fundamentals, Techniques, and Tools},
year = {2017},
isbn = {9781450354035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134472.3134516},
doi = {10.1145/3134472.3134516},
booktitle = {SIGGRAPH Asia 2017 Courses},
articleno = {2},
numpages = {203},
location = {Bangkok, Thailand},
series = {SA '17}
}

@inproceedings{10.1145/3264437.3264474,
author = {Dauda, Ahmed and Mclean, Scott and Almehmadi, Abdulaziz and El-Khatib, Khalil},
title = {Big Data Analytics Architecture for Security Intelligence},
year = {2018},
isbn = {9781450366083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264437.3264474},
doi = {10.1145/3264437.3264474},
abstract = {The need for security1 continues to grow in distributed computing. Today's security solutions require greater scalability and convenience in cloud-computing architectures, in addition to the ability to store and process larger volumes of data to address very sophisticated attacks. This paper explores some of the existing architectures for big data intelligence analytics, and proposes an architecture that promises to provide greater security for data intensive environments. The architecture is designed to leverage the wealth in the multi-source information for security intelligence.},
booktitle = {Proceedings of the 11th International Conference on Security of Information and Networks},
articleno = {19},
numpages = {4},
keywords = {analytics architecture, Big Data, Security Intelligence},
location = {Cardiff, United Kingdom},
series = {SIN '18}
}

@inproceedings{10.1145/3130218.3130236,
author = {Doppa, Janardhan Rao and Kim, Ryan Gary and Isakov, Mihailo and Kinsy, Michel A. and Kwon, Hyouk Jun and Krishna, Tushar},
title = {Adaptive Manycore Architectures for Big Data Computing},
year = {2017},
isbn = {9781450349840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3130218.3130236},
doi = {10.1145/3130218.3130236},
abstract = {This work presents a cross-layer design of an adaptive manycore architecture to address the computational needs of emerging big data applications within the technological constraints of power and reliability. From the circuits end, we present links with reconfigurable repeaters that allow single-cycle traversals across multiple hops, creating fast single-cycle paths on demand. At the microarchitecture end, we present a router with bi-directional links, unified virtual channel (VC) structure, and the ability to perform self-monitoring and self-configuration around faults. We present our vision for self-aware manycore architectures and argue that machine learning techniques are very appropriate to efficiently control various configurable on-chip resources in order to realize this vision. We provide concrete learning algorithms for core and NoC reconfiguration; and dynamic power management to improve the performance, energy-efficiency, and reliability over static designs to meet the demands of big data computing. We also discuss future challenges to push the state-of-the-art on fully adaptive manycore architectures.},
booktitle = {Proceedings of the Eleventh IEEE/ACM International Symposium on Networks-on-Chip},
articleno = {20},
numpages = {8},
keywords = {Big data computing, Power management, Interconnect networks, Adaptive manycore architectures, Machine learning},
location = {Seoul, Republic of Korea},
series = {NOCS '17}
}

@inproceedings{10.1145/2663715.2669615,
author = {Christen, Peter},
title = {Privacy Aspects in Big Data Integration: Challenges and Opportunities},
year = {2014},
isbn = {9781450315838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663715.2669615},
doi = {10.1145/2663715.2669615},
abstract = {Big Data projects often require data from several sources to be integrated before they can be used for analysis. Once data have been integrated, they allow more detailed analysis that would otherwise not be possible. Accordingly, recent years have seen an increasing interest in techniques that facilitate the integration of data from diverse sources. Whenever data about individuals, or otherwise sensitive data, are to be integrated across organizations, privacy and confidentiality have to be considered. Domains where privacy preservation during data integration is of importance include business collaborations, health research, national censuses, the social sciences, crime and fraud detection, and homeland security. Increasingly, applications in these domains require data from diverse sources (both internal and external to an organization) to be integrated.Consequently, in the past decade, various techniques have been developed that aim to facilitate data integration without revealing any private or confidential information about the databases and records that are integrated. These techniques either provably prevent leakage of any private information, or they provide some empirical numerical measure of the risk of disclosure of private information.In the first part of this presentation we provide a background on data integration, and illustrate the importance of preserving privacy during data integration with several application scenarios. We then given an overview of the main concepts and techniques that have been developed to facilitate data integration in such ways that no private or confidential information is being revealed. We focus on privacy-preserving record linkage (PPRL), where so far most research has been conducted. We describe the basic protocols used in PPRL, and several key technologies employed in these protocols. Finally, we discuss the challenges privacy poses to data integration in the era of Big Data, and we discuss directions and opportunities in this research area.},
booktitle = {Proceedings of the First International Workshop on Privacy and Secuirty of Big Data},
pages = {1},
numpages = {1},
keywords = {scalability, privacy-preserving record linkage, privacy techniques, multi-party, data matching},
location = {Shanghai, China},
series = {PSBD '14}
}

@article{10.14778/3007263.3007324,
author = {Stoica, Ion},
title = {Trends and Challenges in Big Data Processing},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007324},
doi = {10.14778/3007263.3007324},
abstract = {Almost six years ago we started the Spark project at UC Berkeley. Spark is a cluster computing engine that is optimized for in-memory processing, and unifies support for a variety of workloads, including batch, interactive querying, streaming, and iterative computations. Spark is now the most active big data project in the open source community, and is already being used by over one thousand organizations.One of the reasons behind Spark's success has been our early bet on the continuous increase in the memory capacity and the feasibility to fit many realistic workloads in the aggregate memory of typical production clusters. Today, we are witnessing new trends, such as Moore's law slowing down, and the emergence of a variety of computation and storage technologies, such as GPUs, FPGAs, and 3D Xpoint. In this talk, I'll discuss some of the lessons we learned in developing Spark as a unified computation platform, and the implications of today's hardware and software trends on the development of the next generation of big data processing systems.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {1619},
numpages = {1}
}

@inproceedings{10.1145/2647908.2655958,
author = {Eichelberger, Holger and Schmid, Klaus},
title = {Resource-Optimizing Adaptation for Big Data Applications},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655958},
doi = {10.1145/2647908.2655958},
abstract = {The resource requirements of Big Data applications may vary dramatically over time, depending on changes in the context. If resources should not be defined for the maximum case, but available resources are mostly static, there is a need to adapt resource usage by modifying the processing behavior. The QualiMaster project researches such an approach for the analysis of systemic risks in the financial markets.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {10–11},
numpages = {2},
keywords = {systematic-risks, financial markets, QualiMaster, resource adaptation, adaptive systems, stream-processing},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2567948.2577274,
author = {Wang, Lidan and Lin, Jimmy and Metzler, Donald and Han, Jiawei},
title = {Learning to Efficiently Rank on Big Data},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2577274},
doi = {10.1145/2567948.2577274},
abstract = {Ranking in response to user queries is a central problem in information retrieval, data mining, and machine learning. In the era of "Big data", traditional effectiveness-centric ranking techniques tend to get more and more costly (requiring additional hardware and energy costs) to sustain reasonable ranking speed on large data. The mentality of combating big data by throwing in more hardware/machines will quickly become highly expensive since data is growing at an extremely fast rate oblivious to any cost concerns from us. "Learning to efficiently rank" offers a cost-effective solution to ranking on large data (e.g., billions of documents). That is, it addresses a critically important question -- whether it is possible to improve ranking effectiveness on large data without incurring (too much) additional cost?},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {209–210},
numpages = {2},
keywords = {effectiveness, efficiency},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.1145/2775292.2775319,
author = {Evans, Alun and Agenjo, Javi and Blat, Josep},
title = {Hybrid Visualisation of Digital Production Big Data},
year = {2015},
isbn = {9781450336475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2775292.2775319},
doi = {10.1145/2775292.2775319},
abstract = {In this paper, we present a web application for the hybrid visualisation of digital production Big Data. In a typical film or television production, several terabytes of data can be recorded per day, such as film footage from multiple cameras or background information regarding the set. Interactive visualisation of this multimodal data, integrating 2D (image and video) and 3D graphics modes, would result in enhanced use. A browser-based context is capable of this integration in a seamless and powerful manner, but faces significant challenges related to data transfer and compression which must be overcome. This paper presents an application designed to harness the power of a hybrid web context while attempting to overcome or compensate for the difficulties of data transfer limitations and rendering power. Results are presented from three, publicly available test datasets, which represent a realistic sample of data recorded on a typical high-budget production set.},
booktitle = {Proceedings of the 20th International Conference on 3D Web Technology},
pages = {69–72},
numpages = {4},
keywords = {pointclouds, WebGL, big data, video, hybrid, visualisation, web},
location = {Heraklion, Crete, Greece},
series = {Web3D '15}
}

@inproceedings{10.1145/3407666,
author = {Lofstead, Jay},
title = {Session Details: Big Data Management},
year = {2020},
isbn = {9781450370523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407666},
doi = {10.1145/3407666},
booktitle = {Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing},
location = {Stockholm, Sweden},
series = {HPDC '20}
}

@inproceedings{10.1145/3183440.3183458,
author = {Gousios, Georgios},
title = {Big Data Software Analytics with Apache Spark},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183458},
doi = {10.1145/3183440.3183458},
abstract = {At the beginning of every research effort, researchers in empirical software engineering have to go through the processes of extracting data from raw data sources and transforming them to what their tools expect as inputs. This step is time consuming and error prone, while the produced artifacts (code, intermediate datasets) are usually not of scientific value. In the recent years, Apache Spark has emerged as a solid foundation for data science and has taken the big data analytics domain by storm. We believe that the primitives exposed by Apache Spark can help software engineering researchers create and share reproducible, high-performance data analysis pipelines.In our technical briefing, we discuss how researchers can profit from Apache Spark, through a hands-on case study.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {542–543},
numpages = {2},
keywords = {data analytics, Apache Spark, big data},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3090354.3090370,
author = {Zerabi, Soumeya and Meshoul, Souham and Merniz, Amina and Melal, Radia},
title = {Towards Clustering Validation in Big Data Context},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090370},
doi = {10.1145/3090354.3090370},
abstract = {Clustering1is an essential task in many areas such as machine learning, data mining and computer vision among others. Cluster validation aims to assess the quality of partitions obtained by clustering algorithms. Several indexes have been developed for cluster validation purpose. They can be external or internal depending on the availability of ground truth clustering. This paper deals with the issue of cluster validation of large data set. Indeed, in the era of big data this task becomes even more difficult to handle and requires parallel and distributed approaches. In this work, we are interested in external validation indexes. More specifically, this paper proposes a model for purity based cluster validation in parallel and distributed manner using Map-Reduce paradigm in order to be able to scale with increasing dataset sizes.The experimental results show that our proposed model is valid and achieves properly cluster validation of large datasets.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {16},
numpages = {6},
keywords = {parallel computing, distributed computing, cluster validation index, purity index, partitioned clustering},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3127942.3127961,
author = {Al-Qirim, Nabeel and Tarhini, Ali and Rouibah, Kamel},
title = {Determinants of Big Data Adoption and Success},
year = {2017},
isbn = {9781450352840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127942.3127961},
doi = {10.1145/3127942.3127961},
abstract = {This research investigates the large hype surrounding big data (BD) and Analytics (BDA) in both academia and the business world. Initial insights pointed to large and complex amalgamations of different fields, techniques and tools. Above all, BD as a research field and as a business tool found to be under developing and is fraught with many challenges. The intention here in this research is to develop an adoption model of BD that could detect key success predictors. The research finds a great interest and optimism about BD value that fueled this current buzz behind this novel phenomenon. Like any disruptive innovation, its assimilation in organizations oppressed with many challenges at various contextual levels. BD would provide different advantages to organizations that would seriously consider all its perspectives alongside its lifecycle in the pre-adoption or adoption or implementation phases. The research attempts to delineate the different facets of BD as a technology and as a management tool highlighting different contributions, implications and recommendations. This is of great interest to researchers, professional and policy makers.},
booktitle = {Proceedings of the International Conference on Algorithms, Computing and Systems},
pages = {88–92},
numpages = {5},
keywords = {big data challenges, big data strategy, Big data analytics, big data success factors},
location = {Jeju Island, Republic of Korea},
series = {ICACS '17}
}

@inproceedings{10.1145/2429376.2429382,
author = {Acar, Umut A. and Chen, Yan},
title = {Streaming Big Data with Self-Adjusting Computation},
year = {2013},
isbn = {9781450318716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2429376.2429382},
doi = {10.1145/2429376.2429382},
abstract = {Many big data computations involve processing data that changes incrementally or dynamically over time. Using existing techniques, such computations quickly become impractical. For example, computing the frequency of words in the first ten thousand paragraphs of a publicly available Wikipedia data set in a streaming fashion using MapReduce can take as much as a full day. In this paper, we propose an approach based on self-adjusting computation that can dramatically improve the efficiency of such computations. As an example, we can perform the aforementioned streaming computation in just a couple of minutes.},
booktitle = {Proceedings of the 2013 Workshop on Data Driven Functional Programming},
pages = {15–18},
numpages = {4},
keywords = {incremental mapreduce, self-adjusting computation},
location = {Rome, Italy},
series = {DDFP '13}
}

@inproceedings{10.1145/3097983.3105814,
author = {Mazumdar, Mainak},
title = {Addressing Challenges with Big Data for Media Measurement},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3105814},
doi = {10.1145/3097983.3105814},
abstract = {The digital media and TV - which is increasingly digitized, have amassed and generating enormous amount of data. While extremely useful, the big data generated by these platforms poses unique challenges for Data Scientists working on developing measurement framework and metrics. Most practitioners optimize speed and scale at the expense of accuracy, which is critical for any measurement. And, the trade-off between bias and variance is not in consideration. In this paper, we will demonstrate how Nielsen is combining proprietary ground truth data and methodologies with Big Data to address the accuracy and bias/variance challenges. We argue that high quality ground truth or training set is pre-requisite to deploying Big Data for high quality media measurement. To illustrate the point, we will share how Nielsen is combining its proprietary high quality panels with Set Top Box for TV measurement in the U.S.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {23},
numpages = {1},
keywords = {tv, digital media, proprietary data, media measurement, big data, data mining},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/2968219.2968282,
author = {Matekenya, Dunstan and Ito, Masaki and Shibasaki, Ryosuke and Sezaki, Kaoru},
title = {Enhancing Location Prediction with Big Data: Evidence from Dhaka},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968282},
doi = {10.1145/2968219.2968282},
abstract = {In recent years, the study of location prediction has received heightened attention due to its applications in LBS and other areas. However, most of the techniques and subsequent conclusions drawn from previous research works are specific to the data used in the study. For instance, resolution of location data and inclusion of external data (e.g., from social networks) may limit application of previous techniques to new situations. Therefore, we explore ways of enhancing location prediction techniques which leverage big data without the need for external data sources. To this end, we study a large CDR dataset with more than 3.5 billion calls from a leading cellular network provider in Dhaka, Bangladesh. The research question we tackle is how we can leverage big data to enhance performance of location predictors? Based on spatio-temporal analysis of call activity, we devise a scheme to compute prior probabilities from cell call activity. With this reasoning, we develop an enhanced Bayes predictor which uses a distance threshold and the users' regular location to improve generation of prior probabilities. Experimental results show that overall the enhanced Bayes predictor improves accuracy by 17 percentage points.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {753–762},
numpages = {10},
keywords = {location prediction. supervised classification, human mobility, data mining},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3252644,
author = {Graus, Mark},
title = {Session Details: Exploiting Big Data},
year = {2017},
isbn = {9781450349055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3252644},
doi = {10.1145/3252644},
booktitle = {Proceedings of the 2017 ACM Workshop on Theory-Informed User Modeling for Tailoring and Personalizing Interfaces},
location = {Limassol, Cyprus},
series = {HUMANIZE '17}
}

@inproceedings{10.1145/2938503.2938540,
author = {McClatchey, Richard and Branson, Andrew and Shamdasani, Jetendr},
title = {Provenance Support for Biomedical Big Data Analytics},
year = {2016},
isbn = {9781450341189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938503.2938540},
doi = {10.1145/2938503.2938540},
abstract = {One essential requirement for supporting analytics for Big Medical Data systems is the provision of a suitable level of traceability to data or processes ('Items') in large volumes of data. Systems should be designed from the outset to support usage of such Items across the spectrum of medical use and over time in order to promote traceability, to simplify maintenance and to assist analytics. The philosophy proposed in this paper is to design medical data systems using a 'description-driven' approach in which meta-data and the description of medical items are saved alongside the data, simplifying item re-use over time and thereby enabling the traceability of these items over time and their use in analytics. Details are given of a big data system in neuroimaging to demonstrate aspects of provenance data capture, collaborative analysis and longitudinal information traceability. Evidence is presented that the description-driven approach leads to simplicity of design and ease of maintenance following the adoption of a unified approach to Item management.},
booktitle = {Proceedings of the 20th International Database Engineering &amp; Applications Symposium},
pages = {386–391},
numpages = {6},
keywords = {provenance data, Big Data, medical analytics, Description-driven systems, traceability},
location = {Montreal, QC, Canada},
series = {IDEAS '16}
}

@inproceedings{10.5555/3192424.3192613,
author = {Yu, Yuan-Chih and Tsai, Dwen-Ren},
title = {A Privacy Weaving Pipeline for Open Big Data},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {The power of big data gives us an unprecedented chance to understand, analyze, and recreate the world, while open data ensures that power be shared and widely exploited. Open and big data has become the emerging topics for researchers and governments. Thus, the related privacy issues also become an emerging urgent problem. In this work, we propose a conceptual framework of privacy weaving pipeline dedicated for producing open and big data while preserving privacy. Within the processing pipeline, each step of the process flow considers the privacy assurance to manipulate datasets. However, the complexity of process flow is the same as normal data pipeline. The experimental prototype confirms the feasibility of framework design. We hope this work will facilitate the development of open and big data industry.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {997–998},
numpages = {2},
keywords = {big data, privacy breach, open data, data pipeline},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.1145/3319647.3325854,
author = {Feder, Oshrit and Khazma, Guy and Lushi, Gal and Moatti, Yosef and Ta-Shma, Paula},
title = {Big Data Skipping in the Cloud},
year = {2019},
isbn = {9781450367493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319647.3325854},
doi = {10.1145/3319647.3325854},
abstract = {According to today's best practices, cloud compute and storage services should be deployed and managed independently. However, this generates a problem for big data analytics in the cloud: potentially huge datasets need to be shipped from the storage service to the compute service to analyse the data. To address this, minimizing the amount of data sent across the network is critical to achieve good performance and low cost. Data skipping is a technique which achieves this for SQL style analytics on structured data.Data skipping stores summary metadata for each object (or file) in a dataset. For each column in the object, the summary might include minimum and maximum values, a list or bloom filter of the appearing values, or other metadata which succinctly represents the data in that column. This metadata can then be indexed to support efficient retrieval, although since it can be orders of magnitude smaller than the data itself, this step may not be essential. This metadata can be used during query evaluation to skip over objects which have no relevant data. False positives for object relevance are acceptable since the query execution engine will ultimately filter the data at the row level. However false negatives must be avoided to ensure correctness of query results.Unlike fully inverted database indexes, data skipping indexes are much smaller than the data itself. This property is critical in the cloud, since otherwise a full index scan could increase the amount of data sent across the network instead of reducing it. In the context of database systems, data skipping is used as an additional technique which complements classical indexes. It is referred to as synopsis in DB2 [6] and zone maps in Oracle [9], where in both cases it is limited to min/max metadata. Data skipping and the associated topic of data layout, has been addressed in recent research papers [7, 8] and is also used in cloud analytics platforms [3,4]. Data skipping can also be built into specific data formats [1].We implemented data skipping support for Apache Spark SQL [2] without changing core Spark, in the form of an addon Scala library which can be added to the classpath and used in Spark applications. Our work applies to storage systems which implement the Hadoop FileSystem API, which includes various object storage systems as well as HDFS. Metadata is stored in Elasticsearch (ES) [5], and additional metadata stores can be supported in future using a pluggable API. Our approach prunes the list of candidate objects for any given Spark SQL query according to the associated data skipping metadata, stored and indexed in ES. Our technique applies to all Spark supported native formats e.g. JSON, CSV, Avro, Parquet, ORC, and can benefit from the latest optimizations built in to those formats in Spark. Unlike approaches which embed data skipping metadata inside the data format itself [1], which require reading at least part of the object, our approach avoids touching irrelevant objects altogether.},
booktitle = {Proceedings of the 12th ACM International Conference on Systems and Storage},
pages = {193},
numpages = {1},
location = {Haifa, Israel},
series = {SYSTOR '19}
}

@inproceedings{10.1145/2857218.2857256,
author = {Agrawal, Rajeev and Kadadi, Anirudh and Dai, Xiangfeng and Andres, Frederic},
title = {Challenges and Opportunities with Big Data Visualization},
year = {2015},
isbn = {9781450334808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857218.2857256},
doi = {10.1145/2857218.2857256},
abstract = {In this big data era, huge amount data are continuously acquired for a variety of purposes. Advanced computing, imaging, and sensing technologies enable scientists to study natural and physical phenomena at unprecedented precision, resulting in an explosive growth of data. It is a huge challenge to visualize this growing data in static or in dynamic form. Most traditional data visualization approaches and tools can't support at "big" scale. In this paper, we identified the challenges and opportunities in big data visualization and review some current approaches and visualization tools.},
booktitle = {Proceedings of the 7th International Conference on Management of Computational and Collective IntElligence in Digital EcoSystems},
pages = {169–173},
numpages = {5},
keywords = {data reduction, big data, scalability, visualization},
location = {Caraguatatuba, Brazil},
series = {MEDES '15}
}

@inproceedings{10.1145/1526993.1526999,
author = {Spaniol, Marc and Denev, Dimitar and Mazeika, Arturas and Weikum, Gerhard and Senellart, Pierre},
title = {Data Quality in Web Archiving},
year = {2009},
isbn = {9781605584881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1526993.1526999},
doi = {10.1145/1526993.1526999},
abstract = {Web archives preserve the history of Web sites and have high long-term value for media and business analysts. Such archives are maintained by periodically re-crawling entire Web sites of interest. From an archivist's point of view, the ideal case to ensure highest possible data quality of the archive would be to "freeze" the complete contents of an entire Web site during the time span of crawling and capturing the site. Of course, this is practically infeasible. To comply with the politeness specification of a Web site, the crawler needs to pause between subsequent http requests in order to avoid unduly high load on the site's http server. As a consequence, capturing a large Web site may span hours or even days, which increases the risk that contents collected so far are incoherent with the parts that are still to be crawled. This paper introduces a model for identifying coherent sections of an archive and, thus, measuring the data quality in Web archiving. Additionally, we present a crawling strategy that aims to ensure archive coherence by minimizing the diffusion of Web site captures. Preliminary experiments demonstrate the usefulness of the model and the effectiveness of the strategy.},
booktitle = {Proceedings of the 3rd Workshop on Information Credibility on the Web},
pages = {19–26},
numpages = {8},
keywords = {temporal coherence, data quality, web archiving},
location = {Madrid, Spain},
series = {WICOW '09}
}

@inproceedings{10.1145/2905055.2905202,
author = {Kumar, Sunil and Shekhar, Jayant and Gupta, Himanshu},
title = {Agent Based Security Model for Cloud Big Data},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905202},
doi = {10.1145/2905055.2905202},
abstract = {As we know that digitization is one of boon of 21st century technologies. With the massstorage of digital information and development of internet based technologies like cloud computing, researcher interest has been increased in Big Data and its security. The term Big Data refers to the huge amount of digital information. Actually, Big Data is not a fully new technology; but it is the expansion of data mining technique. In this paper, we propose an agent based security model for cloud big data. The main objective of this security model is to facilitate the IT companies in term of data protection; those are using Cloud Big Data for the analyzing purpose.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {142},
numpages = {5},
keywords = {Cloud Security, NoSQL Security, Agent Based Security, Big Data Security},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1109/CCGrid.2013.53,
author = {Ghi\c{t}, Bogdan and Iosup, Alexandru and Epema, Dick},
title = {Towards an Optimized Big Data Processing System},
year = {2013},
isbn = {9780768549965},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2013.53},
doi = {10.1109/CCGrid.2013.53},
abstract = {Scalable by design to very large computing systems such as grids and clouds, MapReduce is currently a major big data processing paradigm. Nevertheless, existing performance models for MapReduce only comply with specific workloads that process a small fraction of the entire data set, thus failing to assess the capabilities of the MapReduce paradigm under heavy workloads that process exponentially increasing data volumes. The goal of my PhD is to build and analyze a scalable and dynamic big data processing system, including storage (distributed file system), execution engine (MapReduce), and query language (Pig). My contributions for the first two years of PhD research are the following: 1) the design and implementation of a resource management system part of a MapReduce-based processing system for deploying and resizing MapReduce clusters over multicluster systems, 2) the design and implementation of a benchmarking tool for the MapReduce processing system, and 3) the evaluation and modeling of MapReduce using workloads with very large data sets. Furthermore, based on the first two years research, we will optimize the MapReduce system to efficiently process terabytes of data.},
booktitle = {Proceedings of the 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {83–86},
numpages = {4},
location = {Delft, Netherlands},
series = {CCGRID '13}
}

@inproceedings{10.1109/MET.2019.00019,
author = {Auer, Florian and Felderer, Michael},
title = {Addressing Data Quality Problems with Metamorphic Data Relations},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MET.2019.00019},
doi = {10.1109/MET.2019.00019},
abstract = {In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.},
booktitle = {Proceedings of the 4th International Workshop on Metamorphic Testing},
pages = {76–83},
numpages = {8},
keywords = {big data, data quality, metamorphic testing, quality assessment, metamorphic data relations},
location = {Montreal, Quebec, Canada},
series = {MET '19}
}

@inproceedings{10.1145/2378975.2378987,
author = {Deng, Chao and Qian, Ling and Xu, Meng and Du, Yujian and Luo, Zhiguo and Sun, Shaoling},
title = {Federated Cloud-Based Big Data Platform in Telecommunications},
year = {2012},
isbn = {9781450317542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2378975.2378987},
doi = {10.1145/2378975.2378987},
abstract = {China Mobile is the biggest telecommunication operator in the world, with more than 600 million customers and an ever increasing information technology (IT). To provide better service to 600 million customers and reduce the cost of IT systems, China Mobile adopted a centralized IT strategy based on cloud computing. The big data issue becomes the most significant challenge to the cloud computing based China Mobile IT structure. This paper presents the China Mobile's big data platform based on the cloud. This platform integrates the big data storage, the development and deployment of big data ETL (Extract, Transfer, Load) and DM (Data Mining) into a unified framework. This big data analysis platform can effectively support the analytical tasks in telecommunications, but it can also help China Mobile provide public cloud computing service. In this paper, we introduce the detailed architecture of China Mobile's platform and discuss its performance.},
booktitle = {Proceedings of the 2012 Workshop on Cloud Services, Federation, and the 8th Open Cirrus Summit},
pages = {44–48},
numpages = {5},
keywords = {hadoop, SaaS, big data, telecommunication, cloud computing},
location = {San Jose, California, USA},
series = {FederatedClouds '12}
}

@inproceedings{10.1145/3361525.3361545,
author = {Khorasani, Sobhan Omranian and Rellermeyer, Jan S. and Epema, Dick},
title = {Self-Adaptive Executors for Big Data Processing},
year = {2019},
isbn = {9781450370097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361525.3361545},
doi = {10.1145/3361525.3361545},
abstract = {The demand for additional performance due to the rapid increase in the size and importance of data-intensive applications has considerably elevated the complexity of computer architecture. In response, systems offer pre-determined behaviors based on heuristics and then expose a large number of configuration parameters for operators to adjust them to their particular infrastructure. Unfortunately, in practice this leads to a substantial manual tuning effort. In this work, we focus on one of the most impactful tuning decisions in big data systems: the number of executor threads. We first show the impact of I/O contention on the runtime of workloads and a simple static solution to reduce the number of threads for I/O-bound phases. We then present a more elaborate solution in the form of self-adaptive executors which are able to continuously monitor the underlying system resources and detect contentions. This enables the executors to tune their thread pool size dynamically at runtime in order to achieve the best performance. Our experimental results show that being adaptive can significantly reduce the execution time especially in I/O intensive applications such as Terasort and PageRank which see a 34% and 54% reduction in runtime.},
booktitle = {Proceedings of the 20th International Middleware Conference},
pages = {176–188},
numpages = {13},
keywords = {Apache Spark, Self-Adaptive Executors, Big Data},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@article{10.1145/2744700.2744705,
author = {Ravada, Siva},
title = {Big Data Spatial Analytics for Enterprise Applications},
year = {2015},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/2744700.2744705},
doi = {10.1145/2744700.2744705},
abstract = {Big data is present everywhere and it can help organization in any industry in many different ways. One of the main sources of this data is the increased digitization in every aspect of life. In general terms, digitization is the process of making something digital. That is, use computer technology in the middle of an activity that used to be done without computers. For example, people used to shoot pictures on film, but now most of the pictures are digital. People used to pay tolls with cash, and now it is digital. People used to drive cars with rack-and-pinion steering, now they are all drive-by-wire, fully digital rolling computers.},
journal = {SIGSPATIAL Special},
month = {mar},
pages = {34–41},
numpages = {8}
}

@inproceedings{10.1145/3239283.3239322,
author = {Liu, Zhuang and Gong, Lin},
title = {Study on Big Data Framework for Product Design},
year = {2018},
isbn = {9781450365215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239283.3239322},
doi = {10.1145/3239283.3239322},
abstract = {With the arrival of the intelligent era, Big Data will play an increasingly important role in all aspects of product design. In the product design stage, comprehensive evaluations such as product quality assessment and social demand exploration are directly associated with various data and information. The advantages of big data in data mining and model prediction can be used to provide new ideas for process optimization and problem improvement of design innovation. Based on this situation, this paper proposes a big data framework based on Hadoop for product design. The framework contains the complete flow of big data analysis, and these processes are efficiently connected with good efficiency and practicality. This paper selects the data sets of product quality design and product demand design respectively, mining the design factors in the product design process through data mining technology, thus providing new ideas for product design.},
booktitle = {Proceedings of the 2018 International Conference on Data Science and Information Technology},
pages = {105–110},
numpages = {6},
keywords = {big data, data mining, Hadoop, product design},
location = {Singapore, Singapore},
series = {DSIT '18}
}

@inproceedings{10.1145/2447481.2447491,
author = {Ayhan, Samet and Pesce, Johnathan and Comitz, Paul and Gerberick, Gary and Bliesner, Steve},
title = {Predictive Analytics with Surveillance Big Data},
year = {2012},
isbn = {9781450316927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2447481.2447491},
doi = {10.1145/2447481.2447491},
abstract = {In this paper, we describe a novel analytics system that enables query processing and predictive analytics over streams of aviation data. As part of an Internal Research and Development project, Boeing Research and Technology (BR&amp;T) Advanced Air Traffic Management (AATM) built a system that makes predictions based upon descriptive patterns of archived aviation data. Boeing AATM has been receiving live Aircraft Situation Display to Industry (ASDI) data and archiving it for over two years. At the present time, there is not an easy mechanism to perform analytics on the data. The incoming ASDI data is large, compressed, and requires correlation with other flight data before it can be analyzed.The service exposes this data once it has been uncompressed, correlated, and stored in a data warehouse for further analysis using a variety of descriptive, predictive, and possibly prescriptive analytics tools. The service is being built partially in response to requests from Boeing Commercial Aviation (BCA) for analysis of capacity and flow in the US National Airspace System (NAS). The service utilizes a custom tool for correlating the raw ASDI feed, IBM Warehouse with DB2 for data management, WebSphere Message Broker for real-time message brokering, SPSS Modeler for statistical analysis, and Cognos BI for front-end business intelligence (BI) visualization. This paper describes a scalable service architecture, implementation and the value it adds to the aviation domain.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {81–90},
numpages = {10},
keywords = {data warehouse, big data, data stream management, data analytics},
location = {Redondo Beach, California},
series = {BigSpatial '12}
}

@inproceedings{10.1145/2911451.2911550,
author = {Kumar, Vipin},
title = {Big Data in Climate: Opportunities and Challenges for Machine Learning},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2911550},
doi = {10.1145/2911451.2911550},
abstract = {This talk will present an overview of research being done in a large interdisciplinary project on the development of novel data mining and machine learning approaches for analyzing massive amount of climate and ecosystem data now available from satellite and ground-based sensors, and physics-based climate model simulations. These information-rich data sets offer huge potential for monitoring, understanding, and predicting the behavior of the Earth's ecosystem and for advancing the science of global change. This talk will discuss challenges in analyzing such data sets and some of our research results in mapping the dynamics of surface water globally as well as detecting deforestation and fires in tropical forests using data from Earth observing satellites.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3},
numpages = {1},
keywords = {incomplete, rare class detection, spatio-temporal data, predictive models for imperfect, identifying tropical forest fires., big data, monitoring global water dynamics, and heterogeneous data},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{10.1145/3097983.3105810,
author = {Karpatne, Anuj and Kumar, Vipin},
title = {Big Data in Climate: Opportunities and Challenges for Machine Learning},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3105810},
doi = {10.1145/3097983.3105810},
abstract = {The climate and Earth sciences have recently undergone a rapid transformation from a data-poor to a data-rich environment. In particular, massive amount of data about Earth and its environment is now continuously being generated by a large number of Earth observing satellites as well as physics-based earth system models running on large-scale computational platforms. These massive and information-rich datasets offer huge potential for understanding how the Earth's climate and ecosystem have been changing and how they are being impacted by humans actions. We discuss the challenges involved in analyzing these massive data sets as well as opportunities they present for both advancing machine learning as well as the science of climate change.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {21–22},
numpages = {2},
keywords = {earth observation data, climate science, machine learning},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/2851141.2851187,
author = {Rabozzi, Marco and Mazzucchelli, Matteo and Cordone, Roberto and Fumarola, Giovanni Matteo and Santambrogio, Marco D.},
title = {Preemption-Aware Planning on Big-Data Systems},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851187},
doi = {10.1145/2851141.2851187},
abstract = {Recent developments in Big Data frameworks are moving towards reservation based approaches as a mean to manage the increasingly complex mix of computations, whereas preemption techniques are employed to meet strict jobs deadlines. Within this work we propose and evaluate a new planning algorithm in the context of reservation based scheduling. Our approach is able to achieve high cluster utilization while minimizing the need for preemption that causes system overheads and planning mispredictions.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {48},
numpages = {2},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851187,
author = {Rabozzi, Marco and Mazzucchelli, Matteo and Cordone, Roberto and Fumarola, Giovanni Matteo and Santambrogio, Marco D.},
title = {Preemption-Aware Planning on Big-Data Systems},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851187},
doi = {10.1145/3016078.2851187},
abstract = {Recent developments in Big Data frameworks are moving towards reservation based approaches as a mean to manage the increasingly complex mix of computations, whereas preemption techniques are employed to meet strict jobs deadlines. Within this work we propose and evaluate a new planning algorithm in the context of reservation based scheduling. Our approach is able to achieve high cluster utilization while minimizing the need for preemption that causes system overheads and planning mispredictions.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {48},
numpages = {2}
}

@article{10.1145/2448917.2448923,
author = {McNely, Brian},
title = {Big Data, Situated People: Humane Approaches to Communication Design},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/2448917.2448923},
doi = {10.1145/2448917.2448923},
abstract = {In his 2005 book Ambient Findability, Peter Morville argued that what we find changes who we become. In 2012 and beyond---in an information environment of filter bubbles, contextual advertising, and friend-of-friend chains that push ordinary folks well beyond the Dunbar number---perhaps Morville is in need of some updating: what finds us changes who we become.},
journal = {Commun. Des. Q. Rev},
month = {sep},
pages = {27–30},
numpages = {4}
}

