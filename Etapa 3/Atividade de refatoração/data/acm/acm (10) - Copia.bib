@proceedings{10.1145/3372938,
title = {BDIoT'19: Proceedings of the 4th International Conference on Big Data and Internet of Things},
year = {2019},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rabat, Morocco}
}

@article{10.14778/2536274.2536330,
author = {Hoppe, Anett and Nicolle, C. and Roxin, A.},
title = {Automatic Ontology-Based User Profile Learning from Heterogeneous Web Resources in a Big Data Context},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536274.2536330},
doi = {10.14778/2536274.2536330},
abstract = {The Web has developed to the biggest source of information and entertainment in the world. By its size, its adaptability and flexibility, it challenged our current paradigms on information sharing in several areas. By offering everybody the opportunity to release own contents in a fast and cheap way, the Web already led to a revolution of the traditional publishing world and just now, it commences to change the perspective on advertisements. With the possibility to adapt the contents displayed on a page dynamically based on the viewer's context, campaigns launched to target rough customer groups will become an element of the past. However, this new ecosystem, that relates advertisements with the user, heavily relies on the quality of the underlying user profile. This profile has to be able to model any combination of user characteristics, the relations between its composing elements and the uncertainty that stems from the automated processing of real-world data. The work at hand describes the beginnings of a PhD project that aims to tackle those issues using a combination of data analysis, ontology engineering and processing of big data resources provided by an industrial partner. The final goal is to automatically construct and populate a profile ontology for each user identified by the system. This allows to associate these users to high-value audience segments in order to drive digital marketing.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1428–1433},
numpages = {6}
}

@inproceedings{10.1145/2600821.2600841,
author = {Shirai, Yasutaka and Nichols, William and Kasunic, Mark},
title = {Initial Evaluation of Data Quality in a TSP Software Engineering Project Data Repository},
year = {2014},
isbn = {9781450327541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600821.2600841},
doi = {10.1145/2600821.2600841},
abstract = {To meet critical business challenges, software development teams need data to effectively manage product quality, cost, and schedule. The Team Software ProcessSM (TSPSM) provides a framework that teams use to collect software process data in real time, using a defined disciplined process. This data holds promise for use in software engineering research. We combined data from 109 industrial projects into a database to support performance benchmarking and model development. But is the data of sufficient quality to draw conclusions? We applied various tests and techniques to identify data anomalies that affect the quality of the data in several dimensions. In this paper, we report some initial results of our analysis, describing the amount and the rates of identified anomalies and suspect data, including incorrectness, inconsistency, and credibility. To illustrate the types of data available for analysis, we provide three examples. The preliminary results of this empirical study suggest that some aspects of the data quality are good and the data are generally credible, but size data are often missing.},
booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
pages = {25–29},
numpages = {5},
keywords = {TSP, Team Software Process, Database, Data Quality},
location = {Nanjing, China},
series = {ICSSP 2014}
}

@inproceedings{10.1145/2691195.2691200,
author = {Peipeng, Luo and Sim, Rita Tse Tan},
title = {Research Experience of Big Data Analytics: The Tools for Government: A Case Using Social Network in Mining Preferences of Tourists},
year = {2014},
isbn = {9781605586113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2691195.2691200},
doi = {10.1145/2691195.2691200},
abstract = {This research analyzes messages from Sina Weibo and extracts preferences of tourists using data mining tools. The data mining process, presented in this report, illustrates a good approach for the government in mining information and knowledge from the social network. Discovering association rules is one of the most important tasks in data mining. In this report, Apriori algorithm is used to find the association rules among keywords within topics resulting from the Topic Modeling Algorithm - LDA model. The results and further analysis of data collected from the social network can be used to help the government in Macao and Hong Kong to learn more about their tourists for their policy making in support of the tourism industry.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance},
pages = {312–315},
numpages = {4},
keywords = {preferences of tourists, topic modeling, social network, chinese segmentation, apriori algorithm},
location = {Guimaraes, Portugal},
series = {ICEGOV '14}
}

@inproceedings{10.1145/2724660.2728690,
author = {MacHardy, Zachary and Pardos, Zachary A.},
title = {Toward the Evaluation of Educational Videos Using Bayesian Knowledge Tracing and Big Data},
year = {2015},
isbn = {9781450334112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2724660.2728690},
doi = {10.1145/2724660.2728690},
abstract = {Along with the advent of MOOCs and other online learning platforms such as Khan Academy, the role of online education has continued to grow in relation to that of traditional on-campus instruction. Rather than tackle the problem of evaluating large educational units such as entire online courses, this paper approaches a smaller problem: exploring a framework for evaluating more granular educational units, in this case, short educational videos. We have chosen to leverage an adaptation of traditional Bayesian Knowledge Tracing (BKT), intended to incorporate the usage of video content in addition to assessment activity. By exploring the change in predictive error when alternately including or omitting video activity, we suggest a metric for determining the relevance of videos to associated assessments. To validate our hypothesis and demonstrate the application of our proposed methods we use data obtained from the popular Khan Academy website.},
booktitle = {Proceedings of the Second (2015) ACM Conference on Learning @ Scale},
pages = {347–350},
numpages = {4},
keywords = {instructional technology, online education, educational videos, knowledge tracing, bayesian inference},
location = {Vancouver, BC, Canada},
series = {L@S '15}
}

@inproceedings{10.5555/2399776.2399818,
author = {Joshi, Karuna and Yesha, Yelena},
title = {Workshop on Analytics for Big Data Generated by Healthcare and Personalized Medicine Domain},
year = {2012},
publisher = {IBM Corp.},
address = {USA},
abstract = {Healthcare data presents a trove of information, which when combined with genomic data about a patient can be analyzed and lead to significantly improved and personalized delivery of healthcare. This data at present is very large in volume running to the order of terabytes. With the increasing adoption of digitized patient records and physician's notes, it has the potential of reaching peta (1015) or even exa (1018) bytes of data which in itself will be difficult to manage and analyze. Further, much of this data is in separate silos, which prevents it from being correlated and analyzed. However, very few providers can afford the infrastructure; both hardware and software are needed to collect, clean, curate, and analyze this data. As such, cloud-based healthcare services provide an important technique with which to make analytics driven personalized medicine services available to practitioners at the point of care. This however raises serious concerns around patient privacy, and also issues of regulatory compliance, as the data would reside with the cloud provider and outside of the confines of the physician's control.},
booktitle = {Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {267–269},
numpages = {3},
location = {Toronto, Ontario, Canada},
series = {CASCON '12}
}

@inproceedings{10.1145/564691.564719,
author = {Dasu, Tamraparni and Johnson, Theodore and Muthukrishnan, S. and Shkapenyuk, Vladislav},
title = {Mining Database Structure; or, How to Build a Data Quality Browser},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564719},
doi = {10.1145/564691.564719},
abstract = {Data mining research typically assumes that the data to be analyzed has been identified, gathered, cleaned, and processed into a convenient form. While data mining tools greatly enhance the ability of the analyst to make data-driven discoveries, most of the time spent in performing an analysis is spent in data identification, gathering, cleaning and processing the data. Similarly, schema mapping tools have been developed to help automate the task of using legacy or federated data sources for a new purpose, but assume that the structure of the data sources is well understood. However the data sets to be federated may come from dozens of databases containing thousands of tables and tens of thousands of fields, with little reliable documentation about primary keys or foreign keys.We are developing a system, Bellman, which performs data mining on the structure of the database. In this paper, we present techniques for quickly identifying which fields have similar values, identifying join paths, estimating join directions and sizes, and identifying structures in the database. The results of the database structure mining allow the analyst to make sense of the database content. This information can be used to e.g., prepare data for data mining, find foreign key joins for schema mapping, or identify steps to be taken to prevent the database from collapsing under the weight of its complexity.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {240–251},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/2799979.2799995,
author = {Stepanova, Taiana and Pechenkin, Alexander and Lavrova, Daria},
title = {Ontology-Based Big Data Approach to Automated Penetration Testing of Large-Scale Heterogeneous Systems},
year = {2015},
isbn = {9781450334532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2799979.2799995},
doi = {10.1145/2799979.2799995},
abstract = {Global corporations and government organizations are nowadays represented in cyberspace in the form of numerous large-scale heterogeneous information systems, which implement corresponding business, technological and other types of processes. This extends the set of security analysis tasks, stated for these infrastructures, and tangles already existing tasks. This paper addresses the challenge of increasing penetration testing automation level through the adoption of semi-automatic knowledge extraction from the huge amounts of heterogeneous regularly updated data. The proposed solution is based on the novel penetration testing ontology, which gives a holistic view on the results of security analysis. Designed ontology is evaluated within the penetration testing framework prototype and binds together the conceptual (process) abstraction level, addressed by security experts, and technical abstraction level, employed in modern security analysis tools and methods.},
booktitle = {Proceedings of the 8th International Conference on Security of Information and Networks},
pages = {142–149},
numpages = {8},
keywords = {ontology, large-scale systems, penetration testing, big data},
location = {Sochi, Russia},
series = {SIN '15}
}

@inproceedings{10.1145/3152723.3152732,
author = {Back, Bong-Hyun and Ha, Il-Kyu},
title = {A Platform for Supporting Automatic Data Storing and Visualization of Public and Private Big Data},
year = {2017},
isbn = {9781450353564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152723.3152732},
doi = {10.1145/3152723.3152732},
abstract = {With the arrival of the Fourth Industrial Revolution, large amounts of data are being produced every day in various fields such as industry, culture, politics, education, medicine, and meteorology. In recent years, as the importance of data has increased, data that have been provided free of charge, such as existing social networks, have become available, and the supply period is being limited. Commercialization of data is a continuous process in all over the world; therefore, the data industry, which collects, processes, analyzes, predicts, and provides data is expected to be more active. In this research, using the universal resource locator (URL)and application programming interface (API) of the dataset provided by various public and personal data providers, we propose an automatic database building and visualization platform that can automatically convert the collected data into database (DB), regardless of the data format. Particularly, it is possible to restructure the dataset on the proposed platform, which allows the user to select only the necessary items in the collected dataset and visualize the data set by fusing it.},
booktitle = {Proceedings of the 2017 International Conference on Big Data Research},
pages = {12–17},
numpages = {6},
keywords = {Public Big Data, Big Data Processing, Distributed Processing, Big Data Visualization},
location = {Osaka, Japan},
series = {ICBDR 2017}
}

@article{10.1145/3058750,
author = {St-Maurice, Justin and Burns, Catherine},
title = {An Exploratory Case Study to Understand Primary Care Users and Their Data Quality Tradeoffs},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3–4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3058750},
doi = {10.1145/3058750},
abstract = {Primary care data is an important part of the evolving healthcare ecosystem. Generally, users in primary care are expected to provide excellent patient care and record high-quality data. In practice, users must balance sets of priorities regarding care and data. The goal of this study was to understand data quality tradeoffs between timeliness, validity, completeness, and use among primary care users. As a case study, data quality measures and metrics are developed through a focus group session with managers. After calculating and extracting measurements of data quality from six years of historic data, each measure was modeled with logit binomial regression to show correlations, characterize tradeoffs, and investigate data quality interactions. Measures and correlations for completeness, use, and timeliness were calculated for 196,967 patient encounters. Based on the analysis, there was a positive relationship between validity and completeness, and a negative relationship between timeliness and use. Use of data and reductions in entry delay were positively associated with completeness and validity. Our results suggest that if users are not provided with sufficient time to record data as part of their regular workflow, they will prioritize spending available time with patients. As a measurement of a primary care system's effectiveness, the negative correlation between use and timeliness points to a self-reinforcing relationship that provides users with little external value. In the future, additional data can be generated from comparable organizations to test several new hypotheses about primary care users.},
journal = {J. Data and Information Quality},
month = {jul},
articleno = {15},
numpages = {24},
keywords = {Case study, data mining, primary care, statistical analysis, tradeoffs}
}

@proceedings{10.1145/3361758,
title = {BDIOT 2019: Proceedings of the 3rd International Conference on Big Data and Internet of Things},
year = {2019},
isbn = {9781450372466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We like to start by first acknowledging the traditional custodians of the lands, where La Trobe University campuses are located in Victoria. We recognize their continuing connection to land, waters and culture and we pay our respects to their Elders past, present and emerging.},
location = {Melbourn, VIC, Australia}
}

@inproceedings{10.1145/3264560.3264564,
author = {Chai, Yuanyuan and Zhu, Mengyao and Zhu, Yilong and Zhang, Zizhou and Zhang, Zundong},
title = {On Two-Dimensional Structural Information of Beijing Transportation Networks Based on Traffic Big Data},
year = {2018},
isbn = {9781450364744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264560.3264564},
doi = {10.1145/3264560.3264564},
abstract = {Hierarchy is a fundamental characteristic of many complex systems. The methods of structural information have been taken as a prospective way for quantifying dynamical network complexity. This paper is based on the study of the high-dimensional natural structural information entropy in networks. And then we propose a new similarity District Structural Information (DSI) index, which takes the characteristics of network districts into consideration, to analyze the complexity of dynamical network districts. Based on the method, this paper applies the district structural information to explain the equilibrium problem in real-world networks. And taking Beijing traffic network and its districts to complete experiments demonstrates that the DSI index can reflect the equilibrium of the network and the districts effectively.},
booktitle = {Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing},
pages = {28–32},
numpages = {5},
keywords = {Traffic Big Data, Network Districts, Structural Information, Dynamical Complexity of Networks},
location = {Barcelona, Spain},
series = {ICCBDC'18}
}

@inproceedings{10.1145/1401890.1401965,
author = {Sheng, Victor S. and Provost, Foster and Ipeirotis, Panagiotis G.},
title = {Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers},
year = {2008},
isbn = {9781605581934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401890.1401965},
doi = {10.1145/1401890.1401965},
abstract = {This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect. We examine the improvement (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction. With the outsourcing of small tasks becoming easier, for example via Rent-A-Coder or Amazon's Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. With low-cost labeling, preparing the unlabeled part of the data can become considerably more expensive than labeling. We present repeated-labeling strategies of increasing complexity, and show several main results. (i) Repeated-labeling can improve label quality and model quality, but not always. (ii) When labels are noisy, repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap. (iii) As soon as the cost of processing the unlabeled data is not free, even the simple strategy of labeling everything multiple times can give considerable advantage. (iv) Repeatedly labeling a carefully chosen set of points is generally preferable, and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved. The bottom line: the results show clearly that when labeling is not perfect, selective acquisition of multiple labels is a strategy that data miners should have in their repertoire; for certain label-quality/cost regimes, the benefit is substantial.},
booktitle = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {614–622},
numpages = {9},
keywords = {data selection, data preprocessing},
location = {Las Vegas, Nevada, USA},
series = {KDD '08}
}

@inproceedings{10.1145/3349341.3349502,
author = {Dai, Jianhua and Jin, Libo and Wang, Xinyang},
title = {Factors Affecting the Box Office of Chinese Main-Melody Films Based on Big Data},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349502},
doi = {10.1145/3349341.3349502},
abstract = {With the popularity of the Main-melody films such as "Wolf 2" and "Mekong River Action" in 2017, "Red Sea Action" in 2018, a small wave of development climax of Chinese main-melody films has been set off. Film box office is an important index to measure the success of a film. The analysis and study of the factors affecting the box office of the film provides an indispensable theoretical basis for the development of the film industry. This survey selected 100 main-melody films which were the top box office films in 2013-2018 as the research data. Scores, number of commentaries, actor influence and director influence were taken as independent variables, while box office was taken as a dependent variable for correlation test and regression analysis.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {741–744},
numpages = {4},
keywords = {film box office, regression analysis, big data, main-melody film},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@proceedings{10.1145/2949550,
title = {XSEDE16: Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale},
year = {2016},
isbn = {9781450347556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Miami, USA}
}

@article{10.1145/1577840.1577842,
author = {Naumann, Felix and Raschid, Louiqa},
title = {Guest Editorial for the Special Issue on Data Quality in Databases},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1577840.1577842},
doi = {10.1145/1577840.1577842},
journal = {J. Data and Information Quality},
month = {sep},
articleno = {7},
numpages = {3}
}

@inproceedings{10.1145/3246861,
author = {Castillo, Carlos},
title = {Session Details: Session 6 -- Big Data Analytics and Crowdsourcing for Public Health 2},
year = {2015},
isbn = {9781450334921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246861},
doi = {10.1145/3246861},
booktitle = {Proceedings of the 5th International Conference on Digital Health 2015},
location = {Florence, Italy},
series = {DH '15}
}

@inproceedings{10.1145/2949550.2949651,
author = {Schmidt, Drew and Chen, Wei-Chen and Ostrouchov, George},
title = {Introducing a New Client/Server Framework for Big Data Analytics with the R Language},
year = {2016},
isbn = {9781450347556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2949550.2949651},
doi = {10.1145/2949550.2949651},
abstract = {Historically, large scale computing and interactivity have been at odds. This is a particularly sore spot for data analytics applications, which are typically interactive in nature. To help address this problem, we introduce a new client/server framework for the R language. This framework allows the R programmer to remotely control anywhere from one to thousands of batch servers running as cooperating instances of R. And all of this is done from the user's local R session. Additionally, no specialized software environment is needed; the framework is a series of R packages, available from CRAN. The communication between client and server(s) is handled by the well-known ZeroMQ library. To handle server side computations, we use our established pbdR packages for large scale distributed computing. These packages utilize HPC standards like MPI and ScaLAPACK to handle complex, tightly-coupled computations on large datasets. In this paper, we outline the new client/server architecture components, discuss the pros and cons to this approach, and provide several example workflows that bring interactivity to potentially terabyte size computations.},
booktitle = {Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale},
articleno = {38},
numpages = {9},
keywords = {Big Data, Analytics, Remote Computing, R},
location = {Miami, USA},
series = {XSEDE16}
}

@inproceedings{10.1145/3194164.3194170,
author = {Albarak, Mashel and Bahsoon, Rami},
title = {Prioritizing Technical Debt in Database Normalization Using Portfolio Theory and Data Quality Metrics},
year = {2018},
isbn = {9781450357135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194164.3194170},
doi = {10.1145/3194164.3194170},
abstract = {Database normalization is the one of main principles for designing relational databases. The benefits of normalization can be observed through improving data quality and performance, among the other qualities. We explore a new context of technical debt manifestation, which is linked to ill-normalized databases. This debt can have long-term impact causing systematic degradation of database qualities. Such degradation can be liken to accumulated interest on a debt. We claim that debts are likely to materialize for tables below the fourth normal form. Practically, achieving fourth normal form for all the tables in the database is a costly and idealistic exercise. Therefore, we propose a pragmatic approach to prioritize tables that should be normalized to the fourth normal form based on the metaphoric debt and interest of the ill-normalized tables, observed on data quality and performance. For data quality, tables are prioritized using the risk of data inconsistency metric. Unlike data quality, a suitable metric to estimate the impact of weakly or un-normalized tables on performance is not available. We estimate performance degradation and its costs using Input/Output (I/O) cost of the operations performed on the tables and we propose a model to estimate this cost for each table. We make use of Modern Portfolio Theory to prioritize tables that should be normalized based on the estimated I/O cost and the likely risk of cost accumulation in the future. To evaluate our methods, we use a case study from Microsoft, AdventureWorks. The results show that our methods can be effective in reducing normalization debt and improving the quality of the database.},
booktitle = {Proceedings of the 2018 International Conference on Technical Debt},
pages = {31–40},
numpages = {10},
keywords = {database design, normalization, technical debt},
location = {Gothenburg, Sweden},
series = {TechDebt '18}
}

@inproceedings{10.1145/2839509.2844631,
author = {Fields, Deborah A. and Quirke, Lisa and Amely, Janell and Maughan, Jason},
title = {Combining Big Data and Thick Data Analyses for Understanding Youth Learning Trajectories in a Summer Coding Camp},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2844631},
doi = {10.1145/2839509.2844631},
abstract = {In this paper we explore how to assess novice youths' learning of programming in an open-ended, project-based learning environment. Our goal is to combine analysis of frequent, automated snapshots of programming (e.g., "big" data) within the "thick" social context of kids? learning for deeper insights into their programming trajectories. This paper focuses on the first stage of this endeavor: the development of exploratory quantitative measures of youths? learning of computer science concepts. Analyses focus on kids? learning in a series of three Scratch Camps where 64 campers aged 10-13 used Scratch 2.0 to make a series of creative projects over 30 hours in five days. In the discussion we consider the highlights of the insights-and blind spots-of each data source with regard to youths' learning.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {150–155},
numpages = {6},
keywords = {constructionism, novice programmers, computer science education, big data, scratch, assessment},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/3264560.3264567,
author = {Gaona-Garc\'{\i}a, Paulo Alonso and Mart\'{\i}n-Moncunill, David and Gaona-Garc\'{\i}a, Elvis Eduardo and G\'{o}mez-Acosta, Adriana and Monenegro-Marin, Carlos},
title = {Usability of Big Data Resources in Visual Search Interfaces of Repositories Based on KOS},
year = {2018},
isbn = {9781450364744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264560.3264567},
doi = {10.1145/3264560.3264567},
abstract = {Digital repositories allow storage and manage digital resources and collections of museums, libraries, archives in order to be use in educational context. Unfortunately, several deficiencies in user interfaces based on resource discovery, user-centered design, and strategy of search, among others, prevent the widespread use of the valuable services that data of repository offers. Having the intuition that some deficiencies are reflected in usability problems associated with interfaces, we conducted a research from the Human Computer Interaction (HCI) perspective in order to present a novel framework to evaluate usability of different types of visual interfaces based on visualization techniques and Knowledge Organization System (KOS). Our study analyzed the efficacy of a framework in order to allow repository creators the assessment and selection of appropriate user interfaces according to the needs and demands of the data collection of learning objects. The preliminary results show that framework could improve the select of appropriate visualization techniques after to development of them in a digital repository. Although, some problems associated with the limited computational capabilities for information visualization are difficult to overcome.},
booktitle = {Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing},
pages = {33–37},
numpages = {5},
keywords = {human computer interface, user interfaces, information visualization, visual search interfaces, visualization techniques, Digital repositories},
location = {Barcelona, Spain},
series = {ICCBDC'18}
}

@inproceedings{10.1145/3246859,
author = {Abdelmalik, Philip},
title = {Session Details: Session 4 -- Big Data Analytics and Crowdsourcing for Public Health 1},
year = {2015},
isbn = {9781450334921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246859},
doi = {10.1145/3246859},
booktitle = {Proceedings of the 5th International Conference on Digital Health 2015},
location = {Florence, Italy},
series = {DH '15}
}

@inproceedings{10.1145/3415958.3433072,
author = {Shahoud, Shadi and Khalloof, Hatem and Winter, Moritz and Duepmeier, Clemens and Hagenmeyer, Veit},
title = {A Meta Learning Approach for Automating Model Selection in Big Data Environments Using Microservice and Container Virtualization Technologies},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433072},
doi = {10.1145/3415958.3433072},
abstract = {For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming, since a variety of learning algorithms are proposed in literature and the non-expert users do not know which one to use in order to obtain good performance results. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new generic microservice-based framework for realizing the concept of meta learning in Big Data environments is introduced. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture for a fully manageable and highly scalable solution. In this demonstration and for evaluation purpose, time series model selection is taken into account. The performance and usability of the new framework is evaluated on state-of-the-art machine learning algorithms for time series forecasting: it is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, the recommendation of the most appropriate forecasting model results in a well acceptable low overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in context of Big Data.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {84–91},
numpages = {8},
keywords = {Web-based Applications, Big Data, Machine Learning, Microservice, Meta Learning},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/3398329.3398330,
author = {Qin, Yana and Yang, Haolin and Guo, Mengjie and Guo, Meicheng},
title = {An Advanced Data Science Model Based on Big Data Analytics for Urban Driving Cycle Construction in China},
year = {2020},
isbn = {9781450377713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3398329.3398330},
doi = {10.1145/3398329.3398330},
abstract = {In recent years, with the rapid growth of car ownership, Chinese road traffic conditions have changed a lot. Governments, enterprises, and the public are increasingly finding that the increasing deviation between the actual fuel consumption and the results of the regulatory certification based on NEDC (New European Driving Cycle). In addition, this deviation has seriously affected the credibility of the government, energy saving and emission reduction of automobiles and environmental pollution. Thus, need to improve urban driving cycle construction methods to adapt the Chinese traffic and automobiles driving cycles.This paper proposes an advanced data science model based on big data analysis for accurate urban driving cycle construction in Chinese cities. In addition, we conduct a lot of data analysis and statistics. Then we design a data preprocessing method for cleaning the noise data to use in driving cycle construction. Extensive experiments and analysis on real-world datasets demonstrate that the proposed methods can significantly reduce the impact of missing and abnormal data on microtrips segmentation, and thus the proposed methods can be used for driving cycle construction in China more accurately.},
booktitle = {Proceedings of the 2020 International Conference on Computing, Networks and Internet of Things},
pages = {1–7},
numpages = {7},
keywords = {feature engineering, urban driving cycle construction, Big data analytics, PCA, data preprocessing},
location = {Sanya, China},
series = {CNIOT2020}
}

@article{10.1145/3371041.3371045,
author = {Velichety, Srikar},
title = {Quality Assessment of Peer-Produced Content in Knowledge Repositories Using Big Data and Social Networks: The Case of Implicit Collaboration in Wikipedia},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0095-0033},
url = {https://doi.org/10.1145/3371041.3371045},
doi = {10.1145/3371041.3371045},
abstract = {This research provides a method for quality assessment of peer-produced content in knowledge repositories using a complementary view of collaboration. Using the definition of collaboration as the action of working with someone to produce something, we identify the aspects of collaboration that the present research on online communities does not consider. To this end, we introduce and define the concept of implicit collaboration and then identify two dimensions and four possible areas of collaboration. In each area, we identify the relevant social network that captures collaboration. Using customized measures on each of the networks that capture various aspects of collaboration, we quantify the utility of implicit collaboration in assessing article quality. Experiments conducted on the complete population of graded English language Wikipedia articles show that all the identified measures improve the predictive accuracy of the existing models by 11.89 percent while improving the class-wise precision by 9-18 percent and the class-wise recall by 5-26 percent. We also find that our method complements the existing quality assessment approaches well. Our research has implications for developing automated quality assessment methods for peer-produced content using big data and social networks.},
journal = {SIGMIS Database},
month = {nov},
pages = {28–51},
numpages = {24},
keywords = {discussions, wikipedia, edits, social networks, implicit collaboration}
}

@article{10.1145/3404995,
author = {Wang, Wei and Gong, Zhiguo and Ren, Jing and Xia, Feng and Lv, Zhihan and Wei, Wei},
title = {Venue Topic Model–Enhanced Joint Graph Modelling for Citation Recommendation in Scholarly Big Data},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3404995},
doi = {10.1145/3404995},
abstract = {Natural language processing technologies, such as topic models, have been proven to be effective for scholarly recommendation tasks with the ability to deal with content information. Recently, venue recommendation is becoming an increasingly important research task due to the unprecedented number of publication venues. However, traditional methods focus on either the author’s local network or author-venue similarity, where the multiple relationships between scholars and venues are overlooked, especially the venue–venue interaction. To solve this problem, we propose an author topic model–enhanced joint graph modeling approach that consists of venue topic modeling, venue-specific topic influence modeling, and scholar preference modeling. We first model the venue topic with Latent Dirichlet Allocation. Then, we model the venue-specific topic influence in an asymmetric and low-dimensional way by considering the topic similarity between venues, the top-influence of venues, and the top-susceptibility of venues. The top-influence characterizes venues’ capacity of exerting topic influence on other venues. The top-susceptibility captures venues’ propensity of being topically influenced by other venues. Extensive experiments on two real-world datasets show that our proposed joint graph modeling approach outperforms the state-of-the-art methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
articleno = {4},
numpages = {15},
keywords = {Network embedding, scientific collaboration, academic information retrieval, natural language processing}
}

@inproceedings{10.1145/3416921.3416936,
author = {Gotsev, Lyubomir and Shoikova, Elena},
title = {An Analysis of Scientific Production in Big Data Knowledge Domain on Google Books, YouTube and IEEE Explore® Digital Library},
year = {2020},
isbn = {9781450375382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416921.3416936},
doi = {10.1145/3416921.3416936},
abstract = {The paper aims to reveal the current state of book, video and article production in Big Data Knowledge Domain on particular platforms by examining the capabilities of Application Programming Interface (API) technology in conducting scientific data-driven research. Queries append public records from Google Books, YouTube and IEEE Explore® Digital Library to two research paradigms (sets of data sets): Big Data (incl. Analysis, Engineering, Architecture, Governance, Management, Frameworks) and Big Data interdisciplinary fields (Data Science, Data Mining, Deep Learning, Machine Learning, Artificial Intelligence). Metadata from more than 25 000 conference papers, 2000 books over the past 50 years, and 4 000 videos for the last 12 years, matching the searching criteria, has been stored and analyzed. The outputs are summarized in statistics, forecasting, rating key findings by various attributes: title, author, publisher, research field, category, subject, publication year, description, view count, and a combination of mentioned metadata in cross-tables. Nearly a half of billion video views; a half of million article reference count; a twofold increase in the number of papers in Machine learning over past three years compared to the total number in the same field for entire 1988-2016 period; 1:2:12 overall books-to-videos-to conference papers ratio; 61.3% of last year's video production just in a month (Jan-2020); the earliest found usage of "Artificial Intelligence" expression in a printed law document dated 1848 are few curious examples of analysis findings. The paper presents non-commercial research and retrieved data is collected entirely from public records.},
booktitle = {Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing},
pages = {10–14},
numpages = {5},
keywords = {Data Analysis, Big Data, Scientific Publishing, API},
location = {Virtual, United Kingdom},
series = {ICCBDC '20}
}

@proceedings{10.1145/2658840,
title = {Data4U '14: Proceedings of the First International Workshop on Bringing the Value of "Big Data" to Users (Data4U 2014)},
year = {2014},
isbn = {9781450331869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The trend of bigger and bigger data---in terms of volume, velocity, and variety---is inevitable. Ultimately, how "big data" will impact the broad population of users rests on what value we can bring to them. Historically, the database community has focused primarily on efficient processing of structured queries posed by expert users on preorganized data. But this focus only addresses one of the many different challenges in bringing the value of big data to users. Besides making queries and analysis faster and more scalable, we must address the pain points before and after analytics---i.e., how to put together data from diverse sources and "wrangle" it into representations appropriate for analyses, and how to communicate results and insights effectively. To broaden the impact of big data, we must also move beyond our traditional notions of "users," such as programmers and analysts, to a much wider range of new user archetypes, such as nonexpert users who want to "get something" from their data, or ordinary citizens who wish to play a more active role in understanding public data.},
location = {Hangzhou, China}
}

@inproceedings{10.1145/2967938.2967944,
author = {Song, Mingcong and Hu, Yang and Xu, Yunlong and Li, Chao and Chen, Huixiang and Yuan, Jingling and Li, Tao},
title = {Bridging the Semantic Gaps of GPU Acceleration for Scale-out CNN-Based Big Data Processing: Think Big, See Small},
year = {2016},
isbn = {9781450341219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2967938.2967944},
doi = {10.1145/2967938.2967944},
abstract = {Convolutional Neural Networks (CNNs) have substantially advanced the state-of-the-art accuracies of object recognition, which is the core function of a myriad of modern multimedia processing techniques such as image/video processing, speech recognition, and natural language processing. GPU-based accelerators gained increasing attention because a large amount of highly parallel neurons in CNN naturally matches the GPU computation pattern. In this work, we perform comprehensive experiments to investigate the performance bottlenecks and overheads of current GPU acceleration platform for scale-out CNN-based big data processing.In our characterization, we observe two significant semantic gaps: framework gap that lies between CNN-based data processing workflow and data processing manner in distributed framework; and the standalone gap that lies between the uneven computation loads at different CNN layers and fixed computing capacity provisioning of current GPU acceleration library. To bridge these gaps, we propose D3NN, a Distributed, Decoupled, and Dynamically tuned GPU acceleration framework for modern CNN architectures. In particular, D3NN features a novel analytical model that enables accurate time estimation of GPU accelerated CNN processing with only 5-10% error. Our evaluation results show the throughput of standalone processing node using D3NN gains up to 3.7X performance improvement over current standalone GPU acceleration platform. Our CNN-oriented GPU acceleration library with built-in dynamic batching scheme achieves up to 1.5X performance improvement over the non-batching scheme and outperforms the state-of-the-art deep learning library by up to 28% (performance mode) ~ 67% (memory-efficient mode).},
booktitle = {Proceedings of the 2016 International Conference on Parallel Architectures and Compilation},
pages = {315–326},
numpages = {12},
keywords = {big data, deep learning, distributed system, gpu},
location = {Haifa, Israel},
series = {PACT '16}
}

@inproceedings{10.1145/3257765,
author = {Abdelmalik, Philip},
title = {Session Details: Big Data and Social Media for Early-Warning and Emerging Epidemics},
year = {2016},
isbn = {9781450342247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257765},
doi = {10.1145/3257765},
booktitle = {Proceedings of the 6th International Conference on Digital Health Conference},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {DH '16}
}

@article{10.1145/2854006.2854018,
author = {Chen, Shimin and Luo, Qiong and Meng, Xiaofeng},
title = {Report on the International Workshop on Big Data Management on Emerging Hardware (HardBD 2015)},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2854006.2854018},
doi = {10.1145/2854006.2854018},
journal = {SIGMOD Rec.},
month = {dec},
pages = {60–61},
numpages = {2}
}

@proceedings{10.1145/2513549,
title = {UnstructureNLP '13: Proceedings of the 2013 International Workshop on Mining Unstructured Big Data Using Natural Language Processing},
year = {2013},
isbn = {9781450324151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2013 ACM International Workshop on Mining Unstructured Big Data using Natural Language Processing, which will be held at ACM International Conference on Information and Knowledge Management, CIKM 2013.Unstructured text data is heterogeneous and available in different formats, such as text document, scientific publication, web page, and customer comment. The availability of many big unstructured text datasets enables, while also challenges researchers to discover and explore valuable information/knowledge via different techniques.Mining semantics by using Natural Language Processing (NLP) methodologies is an important approach to uncover the "latent knowledge/semantic" of the unstructured text data. In the past decade, while a number of NLP based features already successfully used to enhance the performance of the text mining or information retrieval systems, we are also facing some challenges. For instance, most NLP algorithms' computational cost is high, and we can hardly employ them directly to large-scale text data for online systems.In this workshop, we aggregate different but highly related research communities, i.e., "NLP", "Text Mining" and "IR" researchers, to investigate the possible opportunities and challenges in semantic mining problem. Nine very interesting papers, covering semantic analysis, social media mining, real-time information extraction, and etc., will be presented in this workshop. For this workshop, an opportunity is offered to both NLP and text mining research communities to better clarify the opportunities and challenges in NLP based semantic mining for big unstructured text data with their research experience.We also encourage attendees to attend the keynote presentation - "HathiTrust Data, Opportunities and Challenges for Text Mining and NLP" by Dr. Beth A. Plale, Director of Data to Insight Center, and Professor at School of Informatics and Computing, Indiana University. HathiTrust is a partnership of academic &amp; research institutions, offering a collection of millions of digitized from libraries around the world plus effective API access.We hope that you will find this program interesting and thought-provoking and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.},
location = {San Francisco, California, USA}
}

@inproceedings{10.1145/3397056.3397058,
author = {He, Zong and Ye, Sheng and Jia, Yahui and Liu, Jian},
title = {Analysis of Spatial Distribution Characteristics and Format Changes of Community Business Based on Internet Big Data},
year = {2020},
isbn = {9781450377416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397056.3397058},
doi = {10.1145/3397056.3397058},
abstract = {Community business is the foundation of urban commerce, an important carrier to meet the comprehensive consumption of residents, and an important element of urban quality improvement. Based on the collection and processing of internet big data, combined with the understanding of the connotation of community business, this paper uses GIS spatial analysis technology and nuclear density analysis technology to analyze the spatial distribution characteristics and format changes of community business in the Chongqing. The results show that: (1) The community business in the Chongqing shows a state of "the south is more important than the north". (2) The community business in the Chongqing shows the form of "multi-core + multi-node". In addition to the obvious clustering of community business in the surrounding city business area, there are also many strong community businesses co-existing, and its nuclear density value is comparable to that of the community business in the surrounding city business area, which is in line with the multi-group spatial layout in the Chongqing. (3) There is a strong spatial correlation between the community business and the resident population as a whole, the correlation coefficient is 0.82. (4) From 2014 to 2019, the growth of community business space in the Chongqing is mainly within the inner ring, while showing the trend of diffusion from the inner ring to the outer ring. (5) From 2014 to 2019, the number of commercial shopping facilities in the Chongqing increased, while the number of catering facilities decreased.},
booktitle = {Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis},
pages = {19–23},
numpages = {5},
keywords = {Community business, POI, GIS, Spatial characteristics, Business format},
location = {Marseille, France},
series = {ICGDA 2020}
}

@inproceedings{10.1145/3178158.3178190,
author = {Tsujioka, Keiko},
title = {A Case Study of ICT Used by Big Data Processing in Education: Discuss on Visualization of RE Research Paper},
year = {2018},
isbn = {9781450353595},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178158.3178190},
doi = {10.1145/3178158.3178190},
abstract = {The latest technologies and information have been introduced one after another at the present field of education. On the other hand, however, it is questionable, whether those technologies and information are understood conveniently. In the case of research papers, for example, is really it true that the paper has been written well about her/his idea, in which the author would like to express? From those questions, we have carried out a survey of research papers.In this paper, we have an awareness especially concerning with academic writing as our core research question. As one of the problem solving, we have extracted the main problems of writing and found out the solution by referring to visualized RE research paper in the field of mechanical technologies.We consider that the contents of the visualized RE (Rotary Engine) research paper are written clearly and logically about the latest technologies and information.},
booktitle = {Proceedings of the 6th International Conference on Information and Education Technology},
pages = {160–164},
numpages = {5},
keywords = {human information processing, big data processing, methodology of clear writing paper, conceptual metaphor, visualization, cognitive processing},
location = {Osaka, Japan},
series = {ICIET '18}
}

@inproceedings{10.1145/3203217.3205336,
author = {Arvind},
title = {Low-Power Appliances for Big-Data Analytics Using Flash Storage and Hardware Accelerators},
year = {2018},
isbn = {9781450357616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3203217.3205336},
doi = {10.1145/3203217.3205336},
abstract = {We live in an age where enormous amount of data is being collected constantly because of smart phones, ubiquitous presence of sensors and the wide-spread use of social media. Useful and cost-effective analysis of this data is the biggest economic driver for the IT industry. Such analyses are often done in data centers or on cluster of machines because they involve applying sophisticated algorithms to terabyte-size graphs, which are extremely irregular and sparse. We will show how low-power appliances for such analyses can be built using flash storage and hardware accelerators. Such appliances are likely to be 10X cheaper than 16-32 node server clusters and will come in the form factor of an SSD to be plugged into your laptop.This work has been done by Sang-Woo Jun and Andy Wright under my supervision},
booktitle = {Proceedings of the 15th ACM International Conference on Computing Frontiers},
pages = {i},
location = {Ischia, Italy},
series = {CF '18}
}

@inproceedings{10.5555/3201607.3258534,
author = {Chen, Deming},
title = {Session Details: (SS-1) Deep Learning for Applications That Live on Big Data},
year = {2018},
publisher = {IEEE Press},
booktitle = {Proceedings of the 23rd Asia and South Pacific Design Automation Conference},
location = {Jeju, Republic of Korea},
series = {ASPDAC '18}
}

@inproceedings{10.1145/3209914.3234639,
author = {Song, Zhendong},
title = {Application of Big Data and Intelligent Processing Technology in Modern Chinese Multi-Category Words Part of Speech Tagging Corpus},
year = {2018},
isbn = {9781450364218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209914.3234639},
doi = {10.1145/3209914.3234639},
abstract = {The application of modern Chinese multi-category words corpus is very wide. With the development of the Internet, data from the corpus is getting bigger and bigger during collection. The data gradually develops so big that the current relational database is difficult to deal with them. This article analyzes the important role of the big data technology in corpu},
booktitle = {Proceedings of the 2018 International Conference on Information Science and System},
pages = {107–111},
numpages = {5},
keywords = {Multi-category words, Corpus, Tagging, Big data, Intelligent processing},
location = {Jeju, Republic of Korea},
series = {ICISS '18}
}

@inproceedings{10.1109/WI-IAT.2009.252,
author = {Romero, Francisco P. and Caballero, Ismael and Olivas, Jose A. and Verbo, Eugenio and Serrano-Guerrero, Jesus},
title = {Web-Based Personal Health Records Filtering Using Fuzzy Prototypes and Data Quality Criteria},
year = {2009},
isbn = {9780769538013},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2009.252},
doi = {10.1109/WI-IAT.2009.252},
abstract = {Due to the Internet, new ways to access to and manage health care information are continuously appearing. All of them allow that data stores can grow with more and more data, which has not always the adequate levels of quality. Manage such amount of data makes that typical tools do not succeed any longer to satisfy users' information needs. It has been demonstrated that some kind of information filtering, like the one based on conceptual categories, can improve the performance of these tools. This kind of filtering is specially suited for domains where there is a constant flow of new information like Web-based Personal Health Records. So, although existing information filtering techniques are valid, new and more efficient approaches are still needed. In this work, we propose a new kind of information filtering based on categories which have been defined by means of conceptual prototypes taking into account profiles created on data quality requirements.},
booktitle = {Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 03},
pages = {159–162},
numpages = {4},
keywords = {Medical information systems, Information filtering},
series = {WI-IAT '09}
}

@inproceedings{10.1145/3423603.3424059,
author = {Bjaoui, Mohamed and Sakly, Houneida and Said, Mourad and Kraiem, Naoufel and Bouhlel, Mohamed Salim},
title = {Depth Insight for Data Scientist with RapidMiner « an Innovative Tool for AI and Big Data towards Medical Applications»},
year = {2020},
isbn = {9781405377539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423603.3424059},
doi = {10.1145/3423603.3424059},
abstract = {RapidMiner tool is considered among the advanced analytics and powerful platform services in the field of artificial intelligence besides the Big Data storage. This solution has emerged towards several industries such as Financial Services, Energy, Logistics, Life Science and Healthcare and has shown a crucial impact for predictive decisions in this area. This work seeks to describe the solution strategy of this tool for data scientist by depicting a depth insight of this concept which contains more 1500 native algorithms, data preparation and data science functions. This features allows professionals to support any machine learning libraries and integrate python and R codes. RapidMiner offers three different modalities to access to their products which are the main Platform, the automated data science and the AI cloud.},
booktitle = {Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress},
articleno = {21},
numpages = {6},
keywords = {RapidMiner tool, artificial intelligence, advanced analytics, data science, security policies, big data},
location = {Virtual Event, Tunisia},
series = {DTUC '20}
}

@inproceedings{10.1145/3318299.3318349,
author = {Liu, Jian and Ji, Ke and Sun, Runyuan and Ma, Kun and Chen, Zhenxiang and Wang, Lin},
title = {Abnormal Phone Analysis Based on Learning to Rank and Ensemble Learning in Environment of Telecom Big Data},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318349},
doi = {10.1145/3318299.3318349},
abstract = {With the rapid development of Telecom era, the number of telecom users has increased dramatically. User phone have been widely recognized as user identities and are registered in a large number of Internet applications. Due to the leakage of user information, a series of social problems have arisen. Abnormal telephone has become a social problem to be solved. Current methods are mostly passive detection methods, and some of them are extremely expensive and do not meet the requirements of practical application. Our current situation of lack of effective control measures and active detection methods for abnormal phones. Based on the existing telecommunication big data, an abnormal phone active detection method is designed based on learning to rank and ensemble learning algorithm. The experimental results on the real dataset show that the proposed method has higher accuracy than the experimental results obtained by a single learning algorithm.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {301–305},
numpages = {5},
keywords = {Telecom big data, abnormal phone, ensemble learning, learning to rank},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1145/2457276.2457292,
author = {Mequanint, Dessalegn and Brunie, Lionel and Libsie, Mulugeta and Coquil, David},
title = {A Latency Hiding Framework for Enhanced Ubiquitous Access to Big Data in a Constrained Digital Ecosystem: Application to Digital Medical Archives},
year = {2012},
isbn = {9781450317559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2457276.2457292},
doi = {10.1145/2457276.2457292},
abstract = {This paper presents our latency hiding framework for access to big data in a constrained digital ecosystem with application to digital medical archives. Aiming to enhance ubiquitous access of big data such as patient-oriented access of medical archives, we apply complex/multi-context prefetching to reduce latency thereby improving response time. We propose a formal model for prefetch requests rate and network workload or stress bound that takes into account a diverse set of constraints a digital ecosystem could be in. In addition to that, components of our latency hiding framework such as a generic multi-context functional architecture, use case model, medical database model with emphasis on API (abstracted patient information) and a high-level system architecture have been designed. The development of a complex or multi-context prefetch algorithm that uses a patient's chief complaints, slackness sensitivity, popular content tag, user specified contexts and constraints is underway. A prototype system will also be developed to validate the proposed solutions. Moreover, input and output metrics will be developed to gauge the efficiency and effectiveness of the prefetch algorithm under development.},
booktitle = {Proceedings of the International Conference on Management of Emergent Digital EcoSystems},
pages = {80–87},
numpages = {8},
keywords = {ubiquitous access to big data, multi-context prefetching, latency hiding, digital medical archives},
location = {Addis Ababa, Ethiopia},
series = {MEDES '12}
}

@proceedings{10.1145/3352740,
title = {EBDIT 2019: Proceedings of the 2019 3rd International Workshop on Education, Big Data and Information Technology},
year = {2019},
isbn = {9781450372053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guilin, China}
}

@inproceedings{10.1145/3209914.3226157,
author = {Zou, Mi},
title = {Study on Recommend Model of Online Shopping for Music and Dance Majors under the Background of Big Data},
year = {2018},
isbn = {9781450364218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209914.3226157},
doi = {10.1145/3209914.3226157},
abstract = {With the rapid development of Internet technology and e-commerce, online shopping has become the main mode of consumption nowadays. Music and dance majors have become the main group of online shopping because of their particularity. Based on the theory of big data and Bayesian networks, this paper collects data of college students' online shopping apparel by means of questionnaire survey and builds the style recommendation model and the clothing's main color recommendation model of online shopping.},
booktitle = {Proceedings of the 2018 International Conference on Information Science and System},
pages = {98–101},
numpages = {4},
keywords = {Music and dance majors, Big data, Online shopping, Bayesian networks},
location = {Jeju, Republic of Korea},
series = {ICISS '18}
}

@inproceedings{10.1145/3254603,
author = {Singhal, Rekha and Chahal, Dheeraj},
title = {Session Details: Third International Workshop on Performance Analysis of Big Data Systems (PABS'17)},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254603},
doi = {10.1145/3254603},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/3375998.3376039,
author = {Adinew, Deleli Mesay and Shijie, Zhou and Liao, Yongjian},
title = {Spark Performance Optimization Analysis in Memory Tuning On GC Overhead for Big Data Analytics},
year = {2020},
isbn = {9781450377027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375998.3376039},
doi = {10.1145/3375998.3376039},
abstract = {Apache spark is one of the high speed "in-memory computing" that run over the JVM. Due to increasing data in volume, it needs performance optimization mechanism that requires management of JVM heap space. To Manage JVM heap space it needs management of garbage collector pause time that affects application performance. There are different parameters to pass to spark to control JVM heap space and GC time overhead to increase application performance. Passing appropriate heap size with appropriate types of GC as a parameter is one of performance optimization which is known as Spark Garbage collection tuning. To reduce GC overhead, an experiment was done by adjusting certain parameters for loading and dataframe creation and data retrieval process. The result shows 3.23% improvement in Latency and 1.62% improvement in Throughput as compared to default parameter configuration in garbage collection tuning approach.},
booktitle = {Proceedings of the 2019 8th International Conference on Networks, Communication and Computing},
pages = {75–78},
numpages = {4},
keywords = {Garbage Collection Tuning, JVM heap space, GC Events, HeapRegionSize},
location = {Luoyang, China},
series = {ICNCC 2019}
}

@inproceedings{10.1145/3159450.3159553,
author = {Dryer, Amber and Walia, Nicole and Chattopadhyay, Ankur},
title = {A Middle-School Module for Introducing Data-Mining, Big-Data, Ethics and Privacy Using RapidMiner and a Hollywood Theme},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3159553},
doi = {10.1145/3159450.3159553},
abstract = {Today's organizations, including online businesses, use the art of data-driven decision-making i.e. business-intelligence (BI) to benefit from all the data out in the open. Given the current market demand for BI skill-sets, including the knowledge of different sources and tools for data-collection plus processing, today's youth need a basic understanding of data-driven intelligence, and an awareness of big-data related ethics and privacy. However, there has been limited research and development work towards designing an effective educational module in this regard at the K-12 level. We intend to address this particular limitation by presenting a uniquely engaging middle-school learning module based upon a combination of useful topics, like data-mining, predictive-analytics, data-visualization, big-data, ethics and privacy, using the free RapidMiner software-tool. The novelty of our module lies in the use of a GUI-based visual hands-on platform (RapidMiner), a Hollywood movie-theme based educational activity, as well as an added focus on big-data ethics and privacy, and its conceptual mapping to the NSA-GenCyber security-first principles. We discuss and analyze the survey data obtained from over hundred participants through several offerings of our module as an educational workshop through our Google-IgniteCS and NSA-GenCyber programs. The collected learning-analytics data indicate that our module can become a simple yet effective means for introducing data-mining, big-data, ethical and privacy issues, and GenCyber security-first principles at the middle-school level. Our results show prospects of motivating middle-school participants towards further learning of topics in data-science, data-ethics and data-security, which is necessary today in a variety of professions.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {753–758},
numpages = {6},
keywords = {hollywood movie, middle-school, data-driven decision-making, data-driven intelligence, NSA-gencyber, security-first principles, ethics, K-12 learning module, big-data, GUI, rapidminer, Google-ignitecs, privacy, data-mining, BI},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/2837060.2837091,
author = {Kim, Ji-Dae and Chi, Su-Young and Song, Young-Wook and Cho, Wan-Sup and Yoo, Kwan-Hee},
title = {Design Strategy for Enhancing Adoption of Manufacturing Big Data System (MBDS) in Korean Small and Medium-Sized Manufacturing Firms (SMMFs)},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837091},
doi = {10.1145/2837060.2837091},
abstract = {The adoption of manufacturing big data system (MBDS) is anticipated to enhance the competitiveness of small and medium-sized manufacturing firms (SMMFs), as massive amount of big data proliferate in the field of manufacturing industries. Several professionals argue that it is necessary to take an attentive approach in designing appropriate MBDS for SMMFs. This study examines appropriate design options, based on the information design theory, product-service system theory, and contingency theory, which would elevate the adoption probability of MBDS of Korean SMMFs. An empirical analysis of 191 Korean SMMFs reveals the following results. First, SMMFs put similar degree of importance on the entire design attributes of MBDS from big data gathering to information visualization. Second, SMMFs prefer payment per use of low-cost MBDS that provides more simplified information (e.g., on-spot monitoring and 2 dimension visualization) and utilizes open cloud as well as open telecommunication network. However, the firms want to have an independent and sophisticated MBDS that includes professional big data analysis and visualization software tool.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {184–188},
numpages = {5},
keywords = {Utility, Small and Medium-Sized Manufacturing Firms, Design, Manufacturing Type, Manufacturing Big Data System},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/3131085.3131124,
author = {Kozlov, Stanislav},
title = {Big Data Visualization as an Auxiliary Tool in Designing a Distribution Network of a Company},
year = {2017},
isbn = {9781450354264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131085.3131124},
doi = {10.1145/3131085.3131124},
abstract = {This paper offers an original way of improving the interpretation of the results of designing a distribution network via a set of various visualization techniques. First, using clustering algorithms allows us to automatically sort through a indeterminate number of warehouses and offer the recommended number of warehouses (clusters) depending on some incoming parameters. Visualization of the results of the algorithm can be illustrated on a geographical map as a model of distribution network. Second, using interpolation algorithms allows us to visualize the problem areas of a region in a model of previously designed network. Such visualizations provide a deep understanding of the properties of a distribution network.},
booktitle = {Proceedings of the 21st International Academic Mindtrek Conference},
pages = {247–250},
numpages = {4},
keywords = {express delivery, clustering, distribution network, warehouse coordinates, visualization, big data},
location = {Tampere, Finland},
series = {AcademicMindtrek '17}
}

@inproceedings{10.1145/3230348.3230424,
author = {Mu, Min and Liu, Chengshan},
title = {Research on the Construction of O2O E-Commerce and Express Industry Collaborative Development Model in Big Data Environment},
year = {2018},
isbn = {9781450363754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230348.3230424},
doi = {10.1145/3230348.3230424},
abstract = {Based on the current situation of O2O development, this paper makes a brief discussion on the research of collaborative development both at home and abroad. This paper discusses the coordinated development mode of O2O e-commerce and express delivery industry under the era of big data from the strategic coordination, operation coordination, knowledge collaboration, technology cooperation four aspects. It also discusses the application of the model to O2O merchants and express delivery industries.},
booktitle = {Proceedings of the 2018 1st International Conference on Internet and E-Business},
pages = {29–33},
numpages = {5},
keywords = {express delivery industries, large data environment, O2O e-commerce, collaborative development model},
location = {Singapore, Singapore},
series = {ICIEB '18}
}

@article{10.14778/2536222.2536244,
author = {Franceschini, Monica},
title = {How to Maximize the Value of Big Data with the Open Source SpagoBI Suite through a Comprehensive Approach},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536222.2536244},
doi = {10.14778/2536222.2536244},
abstract = {This paper describes the approach adopted by SpagoBI suite (www.spagobi.org) to manage large volumes of heterogeneous structured and unstructured data, to perform real-time Business Intelligence on Big Data streaming and to give meaning to data through the semantic analysis. SpagoBI supplies meaningful data insights through the main concept of persistable and schedulable datasets, and using tools such as self-service BI, ad-hoc reporting, interactive dashboards and explorative analysis.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1170–1171},
numpages = {2}
}

