@inproceedings{10.1145/3436209.3436888,
author = {Yumin, Gui},
title = {Analysis of Financial Risk Management of E-Commerce Enterprises Based on Big Data},
year = {2021},
isbn = {9781450388573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436209.3436888},
doi = {10.1145/3436209.3436888},
abstract = {As a new type of transaction model in the Internet + era, e-commerce has been integrated into people's lives and plays an important role. E-commerce business transactions will inevitably be affected by payment behavior, taxation and other factors. With the rapid development of e-commerce companies, the current financial risk management model of e-commerce companies is no longer able to meet the development requirements of the company. Their financial management and financial status are facing major tests, which directly affect the normal operation of the company. It is increasingly important to strengthen the financial risk management of e-commerce companies. This article takes e-commerce companies as the research object and analyzes financial risk management on the basis of big data. It mainly explores how e-commerce companies do a good job in security management, how to establish and improve corporate financial risk management systems, and then improve corporate financial risk identification and management and control capabilities to ensure the healthy and sustainable development of the company. With the emergence of big data, e-commerce companies have risen sharply, so the competition between them has become increasingly fierce. Therefore, their own financial risks are more strictly controlled. Only by strengthening corporate financial risk management can the company be able to promote stable and long-term development. In the context of Internet+, strengthening the research on financial risk management of e-commerce companies has become a key concern of all sectors of society. This is of great significance for preventing corporate financial risks, improving financial management capabilities and comprehensive competitiveness.},
booktitle = {2020 The 4th International Conference on E-Business and Internet},
pages = {37–41},
numpages = {5},
location = {Singapore, Singapore},
series = {ICEBI 2020}
}

@inproceedings{10.1145/2538862.2544296,
author = {Mart\'{\i}nez-Arocho, Allison G. and Buffum, Philip Sheridan and Boyer, Kristy Elizabeth},
title = {Developing a Game-Based Learning Curriculum for "Big Data" in Middle School (Abstract Only)},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538862.2544296},
doi = {10.1145/2538862.2544296},
abstract = {Exposing students early to computer science may influence their choice of career, and there is increasing recognition that even for students who do not pursue computer science careers, computational literacy is important. This poster reports on a project targeting the development of a new middle school computer science curriculum. This research aims to highlight the role of computation in Big Data in the context of middle school computer science education, which serves as a catalyst to keep students engaged in computer science through middle school via the ENGAGE narrative game-based learning environment. This poster discusses steps taken to validate one activity meant to highlight the role of computation in the context of Big Data: skip list manipulation. While we found that most of the middle school students performed poorly in assessments after the skip list activities, several students showed they were capable of completing the activity successfully, implying that a repetition of the revised skip list study and additional pilot studies for other Big Data activities are needed to pave the way for the development of this Big Data curriculum. This activity will be just one part of a broader curriculum designed to showcase the social relevance and power of Big Data.},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
pages = {712},
numpages = {1},
keywords = {middle school, game-based learning, engage, big data},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1145/3194188.3194204,
author = {Wang, Siyu and Xue, Heru and Jiang, Xinhua and Zhou, Yanqing and Duan, Xiaodong and Bai, Mingyue},
title = {Construction of Information Management System of Steppe-Watershed Multiple Water Resources Based on Big Data},
year = {2018},
isbn = {9781450363686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194188.3194204},
doi = {10.1145/3194188.3194204},
abstract = {Aiming at the water shortage and Inefficient water resources management of Xilin River Basin. we put forward the Management System of Steppe-Watershed Multiple Water Resources Based on Big Data. In this paper, we mainly did two things, the first one is the build of the big data storage platform. The sensors sent back the real-time data, which will be stored in the MySQL temporary database. Every week, these data will be synchronized to the big data platform for permanent storage and backup. The second is the visualization of the current data. Based on the data we have got, we designed different diagrams accomplished by using Echarts for the visualization of the current data. Additional, for the later analysis of the history data, we designed a data processing flow in the big platform for processing enormous amounts of data.},
booktitle = {Proceedings of the 2018 International Conference on E-Business and Applications},
pages = {78–81},
numpages = {4},
keywords = {Visualization, water resources management, Big data, data storage},
location = {Da Nang, Viet Nam},
series = {ICEBA 2018}
}

@inproceedings{10.1145/3361821.3361825,
author = {Podhoranyi, Michal and Vojacek, Lukas},
title = {Social Media Data Processing Infrastructure by Using Apache Spark Big Data Platform: Twitter Data Analysis},
year = {2019},
isbn = {9781450372411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361821.3361825},
doi = {10.1145/3361821.3361825},
abstract = {Social media provide continuous data streams that contain information with different level of sensitivity, validity and accuracy. Therefore, this type of information has to be properly filtered, extracted and processed to avoid noisy and inaccurate results. The main goal of this work is to propose architecture and workflow able to process Twitter social network data in near real-time. The primary design of the introduced modern architecture covers all processing aspects from data ingestion and storing to data processing and analysing. This paper presents Apache Spark and Hadoop implementation. The secondary objective is to analyse tweets with the defined topic --- floods. The word frequency method (Word Clouds) is shown as a major tool to analyse the content of the input dataset. The experimental architecture confirmed the usefulness of many well-known functions of Spark and Hadoop in the social data domain. The platforms which were used provided effective tools for optimal data ingesting, storing as well as processing and analysing. Based on the analytical part, it was observed that the word frequency method (n-grams) can effectively reveal the tweets content. According to the results of this study, the tweets proved their high informative potential regarding data quality and quantity.},
booktitle = {Proceedings of the 2019 4th International Conference on Cloud Computing and Internet of Things},
pages = {1–6},
numpages = {6},
keywords = {Twitter, Apache Spark, data processing architecture, social network data},
location = {Tokyo, Japan},
series = {CCIOT 2019}
}

@inproceedings{10.1145/3055635.3056660,
author = {Cahyani, Anggita Dian and Budiharto, Widodo},
title = {Modeling Intelligent Human Resources Systems (IRHS) Using Big Data and Support Vector Machine (SVM)},
year = {2017},
isbn = {9781450348171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055635.3056660},
doi = {10.1145/3055635.3056660},
abstract = {The information available in the form of data in Human Resources (HR) to be analyzed will vary greatly depending on the type of organization. Fast and effective Human Resources Systems facilitates the success of an organization. Data science in HR can help determine whether there are patterns of churn in our data that could help predict future churn and find the right candidate of employee. In this paper, we describe the novel model of Intelligent Human Resources Systems (IHRS) using Big Data to analyze and predict about future status of the employees based on the Support Vector Machine (SVM).},
booktitle = {Proceedings of the 9th International Conference on Machine Learning and Computing},
pages = {137–140},
numpages = {4},
keywords = {Human resources, SVM, big data, data science, Psychology},
location = {Singapore, Singapore},
series = {ICMLC 2017}
}

@inproceedings{10.1145/3090354.3090372,
author = {Samadi, Yassir and Zbakh, Mostapha},
title = {Threshold-Based Load Balancing Algorithm for Big Data on a Cloud Environment},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090372},
doi = {10.1145/3090354.3090372},
abstract = {In this paper, we discuss a load balancing strategy in heterogeneous cloud environments. Load balancing of data processing takes a very important place in cloud computing for several years. Load balancing on cloud systems is critical problem that is difficult to cope with, especially on the emerging heterogeneous clusters. In this aspect, we propose a threshold-based load balancing algorithm that balances the load among datacenters in cloud environments as well as minimizing remote communication among datacenters. The proposed approach is divided into two phases. Firstly, we specify the load threshold of each datacenter based on its processing speed and storage capacity. Secondly, we maintain load balancing among datacenters based on this threshold while the proposed approach takes into consideration the heterogeneity of datacenters. The results show that our approach improve efficiently the load balancing among datacenters.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {18},
numpages = {6},
keywords = {big data, Load balancing, HPC Challenge benchmark, cloud computing},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3075564.3078883,
author = {Gkorou, Dimitra and Ypma, Alexander and Tsirogiannis, George and Giollo, Manuel and Sonntag, Dag and Vinken, Geert and van Haren, Richard and van Wijk, Robert Jan and Nije, Jelle and Hoogenboom, Tom},
title = {Towards Big Data Visualization for Monitoring and Diagnostics of High Volume Semiconductor Manufacturing},
year = {2017},
isbn = {9781450344876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3075564.3078883},
doi = {10.1145/3075564.3078883},
abstract = {In semiconductor manufacturing, continuous on-line monitoring prevents production stop and yield loss. The challenges towards this accomplishment are: 1) the complexity of lithography machines which are composed of hundreds of mechanical and optical components, 2) the high rate and volume data acquisition from different lithography and metrology machines, and 3) the scarcity of performance measurements due to their cost. This paper addresses these challenges by 1) visualizing and ranking the most relevant factors to a performance metric, 2) organizing efficiently Big Data from different sources and 3) predicting the performance with machine learning when measurements are lacking. Even though this project targets semiconductor manufacturing, its methodology is applicable to any case of monitoring complex systems, with many potentially interesting features, and imbalanced datasets.},
booktitle = {Proceedings of the Computing Frontiers Conference},
pages = {338–342},
numpages = {5},
keywords = {analytics, continuous monitoring of high volume manufacturing, machine learning, anomaly detection, data science, visualization of high dimensional data},
location = {Siena, Italy},
series = {CF'17}
}

@inproceedings{10.1109/CCGrid.2014.129,
author = {Cuzzocrea, Alfredo and Moussa, Rim},
title = {A Cloud-Based Framework for Supporting Effective and Efficient OLAP in Big Data Environments},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.129},
doi = {10.1109/CCGrid.2014.129},
abstract = {Inspired by the emerging Big Data challenge, in this paper we provide the description of OLAP*, a Cloud-based framework for supporting effective and efficient OLAP in Big Data environments. OLAP* combines data warehouse partitioning techniques with Cloud Computing paradigms, and provides a suitable implementation on top of the well-known ROLAP server Mondrian where the main task consists in applying meaningful transformation of multidimensional database schemas. We complement our analytical contribution by mean of a case study showing the effectiveness of our framework in a practical setting.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {680–684},
numpages = {5},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/2801948.2801963,
author = {Doulkeridis, Christos and Vlachou, Akrivi and Nikitopoulos, Panagiotis and Tampakis, Panagiotis and Saouk, Mei},
title = {The RoadRunner Framework for Efficient and Scalable Processing of Big Data},
year = {2015},
isbn = {9781450335515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801948.2801963},
doi = {10.1145/2801948.2801963},
abstract = {In this paper, we present the overall architecture of RoadRunner, a Hadoop-based framework that enhances the efficiency of rank-aware query processing by introducing various optimizations to Hadoop, without changing its internal operation. RoadRunner focuses on a specific class of queries that involve ranking, such as top-k queries and top-k joins, as well as on preference-aware queries, such as skyline queries, which are tightly related. For this class of queries, we identify improvements on various stages of MapReduce processing, which result in improved performance without sacrificing scalability. We describe the RoadRunner framework, along with individual modules and their roles, and we demonstrate the merits of the proposed framework by means of showcase query examples.},
booktitle = {Proceedings of the 19th Panhellenic Conference on Informatics},
pages = {215–220},
numpages = {6},
keywords = {ranking, MapReduce, top-k queries},
location = {Athens, Greece},
series = {PCI '15}
}

@article{10.1145/2978575,
author = {Li, Yibin and Gai, Keke and Ming, Zhong and Zhao, Hui and Qiu, Meikang},
title = {Intercrossed Access Controls for Secure Financial Services on Multimedia Big Data in Cloud Systems},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2978575},
doi = {10.1145/2978575},
abstract = {The dramatically growing demand of Cyber Physical and Social Computing (CPSC) has enabled a variety of novel channels to reach services in the financial industry. Combining cloud systems with multimedia big data is a novel approach for Financial Service Institutions (FSIs) to diversify service offerings in an efficient manner. However, the security issue is still a great issue in which the service availability often conflicts with the security constraints when the service media channels are varied. This paper focuses on this problem and proposes a novel approach using the Semantic-Based Access Control (SBAC) techniques for acquiring secure financial services on multimedia big data in cloud computing. The proposed approach is entitled IntercroSsed Secure Big Multimedia Model (2SBM), which is designed to secure accesses between various media through the multiple cloud platforms. The main algorithms supporting the proposed model include the Ontology-Based Access Recognition (OBAR) Algorithm and the Semantic Information Matching (SIM) Algorithm. We implement an experimental evaluation to prove the correctness and adoptability of our proposed scheme.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {sep},
articleno = {67},
numpages = {18},
keywords = {secure financial services, cloud computing, multimedia big data, Intercrossed access control, trust management}
}

@inproceedings{10.1145/3396452.3396454,
author = {Baranova, T. A. and Trostinskaya, I. R. and Kobicheva, A. M. and Tokareva, E. Y.},
title = {Improving the Students' Big Data Era Skills through the Online International Project X-Culture},
year = {2020},
isbn = {9781450374989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396452.3396454},
doi = {10.1145/3396452.3396454},
abstract = {Among the most crucial for the specialist of the era of Big Data skills are critical-thinking, problem-solving, collaborative and digital skills. In this paper, we investigate how international online project "X-culture" that was implemented into the course influenced such vital students' skills. To evaluate the impact we used information from the X-culture project database that was collected through various surveys and conducted an interview with students. According to the results received students skills developed significantly after the participation in the project. Also we implemented a correlation analysis to determine weather students' critical-thinking, problem-solving, collaborative and digital skills affected the final reports scores on the international project. In our case collaborative skills played the most substantial role for getting the highest final reports scores while the relationship between digital skills and reports' results was the weakest.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Big Data and Education},
pages = {15–20},
numpages = {6},
keywords = {collaborative skills, critical thinking skills, big data, digital skills, problem-solving skills},
location = {London, United Kingdom},
series = {ICBDE '20}
}

@inproceedings{10.1145/1966883.1966892,
author = {Guerra-Garc\'{\i}a, C\'{e}sar and Caballero, Ismael and Piattini, Mario},
title = {Capturing Data Quality Requirements for Web Applications by Means of DQ_WebRE},
year = {2011},
isbn = {9781450306102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1966883.1966892},
doi = {10.1145/1966883.1966892},
abstract = {The number and complexity of Web applications which are part of Business Intelligence (BI) applications had grown exponentially in recent years. The amount of data used in these applications has consequently also grown. Managing data with an acceptable level of quality is paramount to success in any organizational business process. In order to raise and maintain the adequate levels of Data Quality (DQ) it is indispensable for Web applications to be able to satisfy specific DQ requirements. In order to achieve this goal, DQ requirements should be captured and introduced into the development process together with the other software requirements needed in the applications. However, in the field of Web application development, and to the best of our knowledge, no proposals exist with regard to the way in which to manage specific DQ software requirements. This paper considers the MDA (Model Driven Architecture) approach and, principally, the benefits provided by Model Driven Web Engineering (MDWE) in order to put forward a proposal for two artifacts. These two artifacts are a metamodel and a UML profile for the management of Data Quality Software Requirements for Web Applications (DQ_WebRE).},
booktitle = {Proceedings of the 2nd International Workshop on Business IntelligencE and the WEB},
pages = {28–35},
numpages = {8},
keywords = {model driven Web engineering, data quality, requirements modeling, Web engineering, requirements engineering},
location = {Uppsala, Sweden},
series = {BEWEB '11}
}

@inproceedings{10.1145/2568088.2576096,
author = {Suzumura, Toyotaro},
title = {Extreme Big Data Processing in Large-Scale Graph Analytics and Billion-Scale Social Simulation},
year = {2014},
isbn = {9781450327336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568088.2576096},
doi = {10.1145/2568088.2576096},
abstract = {This paper introduces some of the example applications handling extremely big data with supercomputers such as large-scale network analysis, X10-based large-scale graph analytics library, Graph500 benchmark, and billion-scale social simulation.},
booktitle = {Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
pages = {1–2},
numpages = {2},
keywords = {social simulation, supercomputer, big data, graph},
location = {Dublin, Ireland},
series = {ICPE '14}
}

@proceedings{10.1145/3358505,
title = {ICCBDC 2019: Proceedings of the 2019 3rd International Conference on Cloud and Big Data Computing},
year = {2019},
isbn = {9781450371650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our greatest pleasure to welcome you to the 2019 3rd International Conference on Cloud and Big Data Computing (ICCBDC 2019) held at St Anne's College in University of Oxford, Oxford, United Kingdom. In this conference, you can expect to meet fellow researchers from both academia and industry, and to hear and discuss the latest technological advances in the field. In our previous runs of this conference series in London (2017) and Barcelona (2018), we have seen novel ideas and developments being shared and collaborations being established.},
location = {Oxford, United Kingdom}
}

@inproceedings{10.1145/3128128.3128157,
author = {Das, Sajal K. and Yamana, Hayato},
title = {Securing Big Data and IoT Networks in Smart Cyber-Physical Environments},
year = {2017},
isbn = {9781450352819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128128.3128157},
doi = {10.1145/3128128.3128157},
abstract = {This position paper highlights security and privacy issues in smart environments based on cyber-physical systems. It also summarizes some of our recent research activities and projects in this area.},
booktitle = {Proceedings of the 2017 International Conference on Smart Digital Environment},
pages = {189–194},
numpages = {6},
location = {Rabat, Morocco},
series = {ICSDE '17}
}

@proceedings{10.1145/3289430,
title = {BDIOT 2018: Proceedings of the 2018 2nd International Conference on Big Data and Internet of Things},
year = {2018},
isbn = {9781450365192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {With the sustainable development on information technology, and the widespread use of emerging technology, such as: cloud computing, internet of things and social network, the variety of Big Data is increasing day by day, and the scale of big date have expanded dramatically in recent years, which means the era of big data is coming silently. In order to keep up with the development of network technology, BDIOT2018 provide a platform to all scholars on relevant research area gather together on North China University of Technology, Beijing, China to share and discuss their experimental fruits or learn the most advanced innovative technologies from the outstanding experts on big data and internet of things. This conference is technically sponsored by North China University of Technology, China; University of Macau, Macau SAR; Dongguk University, The Republic of South Korea.},
location = {Beijing, China}
}

@inproceedings{10.1145/2733373.2807985,
author = {Rehman, Faizan Ur and Lbath, Ahmed and Murad, Abdullah and Rahman, Md. Abdur and Sadiq, Bilal and Ahmad, Akhlaq and Qamar, Ahmad and Basalamah, Saleh},
title = {A Semantic Geo-Tagged Multimedia-Based Routing in a Crowdsourced Big Data Environment},
year = {2015},
isbn = {9781450334594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2733373.2807985},
doi = {10.1145/2733373.2807985},
abstract = {Traditional routing algorithms for calculating the fastest or shortest path become ineffective or difficult to use when both source and destination are dynamic or unknown. To solve the problem, we propose a novel semantic routing system that leverages geo-tagged rich crowdsourced multimedia information such as images, audio, video and text to add semantics to the conventional routing. Our proposed system includes a Semantic Multimedia Routing Algorithm (SMRA) that uses an indexed spatial big data environment to answer multimedia spatio-temporal queries in real-time. The results are customized to the users' smartphone bandwidth and resolution requirements. The system has been designed to be able to handle a very large number of multimedia spatio-temporal requests at any given moment. A proof of concept of the system will be demonstrated through two scenarios. These are 1) multimedia enhanced routing and 2) finding lost individuals in a large crowd using multimedia. We plan to test the system's performance and usability during Hajj 2015, where over four million pilgrims from all over the world gather to perform their rituals.},
booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
pages = {759–760},
numpages = {2},
keywords = {semantic multimedia routing, geo-tagged multimedia, spatio-temporal multimedia queries, crowdsourcing},
location = {Brisbane, Australia},
series = {MM '15}
}

@inproceedings{10.1145/3419635.3419734,
author = {Qiu, Chenxia},
title = {Empirical Study of Big Data Mining Technology in English Teaching Integration and Optimization Analysis},
year = {2020},
isbn = {9781450387729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419635.3419734},
doi = {10.1145/3419635.3419734},
abstract = {In recent years, teaching staff has increased the frequency of application of multimedia teaching equipment in subject teaching. The scientific application of the teaching staff to the database can gain efficiency of teaching for English. According to situation, we describe the role of English teaching. This article uses big data, takes college English teaching as the research object, applies data mining technology, analyses from the teaching environment, faculty, teaching process and students' learning methods, learning motivation, learning time and other factors, statistics of college English learning behaviour data. Combined with questionnaire survey and interview, the validity of the model in learning activities, knowledge acquisition and other aspects are verified. Results show the English teaching model can improve teaching effect under the big data environment.},
booktitle = {Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education},
pages = {495–499},
numpages = {5},
keywords = {Empirical research, Teaching, Data mining, Optimization analysis},
location = {Ottawa, ON, Canada},
series = {CIPAE 2020}
}

@inproceedings{10.1145/1449814.1449820,
author = {Lowry, Sonya J. and Warner, Phillip B. and Deaubl, Evan},
title = {NOAO Imaging Meta Data Quality Improvement: A Case Study of the Evolution of a Service Oriented System},
year = {2008},
isbn = {9781605582207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449814.1449820},
doi = {10.1145/1449814.1449820},
abstract = {Due to the natures of legacy astronomical imaging meta data acquisition and storage technologies and techniques at the National Optical Astronomy Observatory, the challenge of methodically improving the quality of such information has been insurmountable until now. The diversity of the sources and lack of cohesive effort along with technologies that enable silo style efforts have contributed to the current, disorganized state of the collection of astronomical imaging information. By meeting these issues with a solution that combines policy and technology support for implementing master data management, use of transformations that enable continued improvements and a history of changes made, and a modular architecture based upon services that are federated over a flexible, robust communications architecture, the NOAO Archive System can assure improvements in the quality of the imaging meta data now and into the future.},
booktitle = {Companion to the 23rd ACM SIGPLAN Conference on Object-Oriented Programming Systems Languages and Applications},
pages = {675–684},
numpages = {10},
keywords = {astronomy, service-oriented architecture, data quality, soa, transformation, virtual observatory, meta data, service platform, integration},
location = {Nashville, TN, USA},
series = {OOPSLA Companion '08}
}

@inproceedings{10.1145/3383583.3398560,
author = {Liao, Longwen and Li, Qi and Chen, Junyan},
title = {Research on the Development of Library and Information Science in the Era of Big Data},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398560},
doi = {10.1145/3383583.3398560},
abstract = {To explore the key research technology knowledge graphs related to Library and Information Science(LIS), articles between 1998 and 2020 were collected from the "Web of Science TM". By using the visualization software CiteSpace, the pivotal literature related to big data in the field of LIS, as well as countries, institutions, and keywords, were visualized and recognized. The results show that the research hot spots in this field mainly include: big data brings influences and challenges to LIS, big data analysis technology, and data management and user privacy.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {453–454},
numpages = {2},
keywords = {big data, LIS, citespace},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3331453.3362042,
author = {Lin, Ching-Lung and Lin, Huang-Liang and Lin, Shu-Chi and Liu, Yung-Te},
title = {Long Term Healthcare System for Elders by Using Internet of Things with Big Data},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3362042},
doi = {10.1145/3331453.3362042},
abstract = {This paper proposes an elder care system that develops a multi-solution for Taiwan's current Long-Term Care 2.0. The system is designed by using the Internet of Things, Big data, cloud database, application of various sensors, and the integration of the experiences of the Long-Term Care Centers. We find a way to create the maximum effectiveness with the least resources, so that elders in long-term care centers can keep their ability of daily living activities (ADLs) and instrumental activities of daily living (IADLs), and it alleviates internal pressures and costs inside country under the continuing aging society.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {18},
numpages = {5},
keywords = {Long Term Healthcare 2.0, Internet of Things, Aging country, Big Data},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/2749469.2750381,
author = {Li, Chao and Hu, Yang and Liu, Longjun and Gu, Juncheng and Song, Mingcong and Liang, Xiaoyao and Yuan, Jingling and Li, Tao},
title = {Towards Sustainable In-Situ Server Systems in the Big Data Era},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2750381},
doi = {10.1145/2749469.2750381},
abstract = {Recent years have seen an explosion of data volumes from a myriad of distributed sources such as ubiquitous cameras and various sensors. The challenges of analyzing these geographically dispersed datasets are increasing due to the significant data movement overhead, time-consuming data aggregation, and escalating energy needs. Rather than constantly move a tremendous amount of raw data to remote warehouse-scale computing systems for processing, it would be beneficial to leverage in-situ server systems (InS) to pre-process data, i.e., bringing computation to where the data is located.This paper takes the first step towards designing server clusters for data processing in the field. We investigate two representative in-situ computing applications, where data is normally generated from environmentally sensitive areas or remote places that lack established utility infrastructure. These very special operating environments of in-situ servers urge us to explore standalone (i.e., off-grid) systems that offer the opportunity to benefit from local, self-generated energy sources. In this work we implement a heavily instrumented proof-of-concept prototype called InSURE: in-situ server systems using renewable energy. We develop a novel energy buffering mechanism and a unique joint spatio-temporal power management strategy to coordinate standalone power supplies and in-situ servers. We present detailed deployment experiences to quantify how our design fits with in-situ processing in the real world. Overall, InSURE yields 20%~60% improvements over a state-of-the-art baseline. It maintains impressive control effectiveness in under-provisioned environment and can economically scale along with the data processing needs. The proposed design is well complementary to today's grid-connected cloud data centers and provides competitive cost-effectiveness.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {14–26},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}

@article{10.1145/2872887.2750381,
author = {Li, Chao and Hu, Yang and Liu, Longjun and Gu, Juncheng and Song, Mingcong and Liang, Xiaoyao and Yuan, Jingling and Li, Tao},
title = {Towards Sustainable In-Situ Server Systems in the Big Data Era},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3S},
issn = {0163-5964},
url = {https://doi.org/10.1145/2872887.2750381},
doi = {10.1145/2872887.2750381},
abstract = {Recent years have seen an explosion of data volumes from a myriad of distributed sources such as ubiquitous cameras and various sensors. The challenges of analyzing these geographically dispersed datasets are increasing due to the significant data movement overhead, time-consuming data aggregation, and escalating energy needs. Rather than constantly move a tremendous amount of raw data to remote warehouse-scale computing systems for processing, it would be beneficial to leverage in-situ server systems (InS) to pre-process data, i.e., bringing computation to where the data is located.This paper takes the first step towards designing server clusters for data processing in the field. We investigate two representative in-situ computing applications, where data is normally generated from environmentally sensitive areas or remote places that lack established utility infrastructure. These very special operating environments of in-situ servers urge us to explore standalone (i.e., off-grid) systems that offer the opportunity to benefit from local, self-generated energy sources. In this work we implement a heavily instrumented proof-of-concept prototype called InSURE: in-situ server systems using renewable energy. We develop a novel energy buffering mechanism and a unique joint spatio-temporal power management strategy to coordinate standalone power supplies and in-situ servers. We present detailed deployment experiences to quantify how our design fits with in-situ processing in the real world. Overall, InSURE yields 20%~60% improvements over a state-of-the-art baseline. It maintains impressive control effectiveness in under-provisioned environment and can economically scale along with the data processing needs. The proposed design is well complementary to today's grid-connected cloud data centers and provides competitive cost-effectiveness.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {14–26},
numpages = {13}
}

@inproceedings{10.1145/2783258.2783344,
author = {Hsieh, Hsun-Ping and Lin, Shou-De and Zheng, Yu},
title = {Inferring Air Quality for Station Location Recommendation Based on Urban Big Data},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783344},
doi = {10.1145/2783258.2783344},
abstract = {This paper tries to answer two questions. First, how to infer real-time air quality of any arbitrary location given environmental data and historical air quality data from very sparse monitoring locations. Second, if one needs to establish few new monitoring stations to improve the inference quality, how to determine the best locations for such purpose? The problems are challenging since for most of the locations (&gt;99%) in a city we do not have any air quality data to train a model from. We design a semi-supervised inference model utilizing existing monitoring data together with heterogeneous city dynamics, including meteorology, human mobility, structure of road networks, and point of interests (POIs). We also propose an entropy-minimization model to suggest the best locations to establish new monitoring stations. We evaluate the proposed approach using Beijing air quality data, resulting in clear advantages over a series of state-of-the-art and commonly used methods.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {437–446},
numpages = {10},
keywords = {monitoring station, location recommendation, city dynamics, sensor placement, air quality, semi-supervised inference},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@article{10.1109/TNET.2019.2934026,
author = {Gong, Xiaowen and Shroff, Ness B.},
title = {Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2934026},
doi = {10.1109/TNET.2019.2934026},
abstract = {Mobile crowdsensing has found a variety of applications e.g., spectrum sensing, environmental monitoring by leveraging the “wisdom” of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users’ data e.g., users’ received SNRs for measuring a transmitter’s transmit signal strength. However, the quality of a user can be its private information which, e.g., may depend on the user’s location that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data’s accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data’s accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation QEE, which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user’s data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester’s optimal RO effort assignment assigns effort only to the best user that has the smallest “virtual valuation”, which depends on the user’s quality and the quality’s distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.},
journal = {IEEE/ACM Trans. Netw.},
month = {oct},
pages = {1959–1972},
numpages = {14}
}

@article{10.1145/3040934,
author = {Ji, Rongrong and Liu, Wei and Xie, Xing and Chen, Yiqiang and Luo, Jiebo},
title = {Mobile Social Multimedia Analytics in the Big Data Era: An Introduction to the Special Issue},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3040934},
doi = {10.1145/3040934},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {34},
numpages = {3}
}

@article{10.1145/2788402.2788404,
author = {Ardagna, Danilo and Squillante, Mark S.},
title = {Special Issue on Performance and Resource Management in Big Data Applications},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2788402.2788404},
doi = {10.1145/2788402.2788404},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {2},
numpages = {1}
}

@inproceedings{10.1145/2831244.2831253,
author = {Xuan, Pengfei and Denton, Jeffrey and Srimani, Pradip K. and Ge, Rong and Luo, Feng},
title = {Big Data Analytics on Traditional HPC Infrastructure Using Two-Level Storage},
year = {2015},
isbn = {9781450339933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2831244.2831253},
doi = {10.1145/2831244.2831253},
abstract = {Data-intensive computing has become one of the major workloads on traditional high-performance computing (HPC) clusters. Currently, deploying data-intensive computing software framework on HPC clusters still faces performance and scalability issues. In this paper, we develop a new two-level storage system by integrating Tachyon, an in-memory file system with OrangeFS, a parallel file system. We model the I/O throughputs of four storage structures: HDFS, OrangeFS, Tachyon and two-level storage. We conduct computational experiments to characterize I/O throughput behavior of two-level storage and compare its performance to that of HDFS and OrangeFS, using TeraSort benchmark. Theoretical models and experimental tests both show that the two-level storage system can increase the aggregate I/O throughputs. This work lays a solid foundation for future work in designing and building HPC systems that can provide a better support on I/O intensive workloads with preserving existing computing resources.},
booktitle = {Proceedings of the 2015 International Workshop on Data-Intensive Scalable Computing Systems},
articleno = {4},
numpages = {8},
keywords = {high performance computing, file system design, data-intensive computing, OrangeFS, two-level storage, hadoop, HPC, tachyon},
location = {Austin, Texas},
series = {DISCS '15}
}

@proceedings{10.1145/3404512,
title = {BDE 2020: Proceedings of the 2020 2nd International Conference on Big Data Engineering},
year = {2020},
isbn = {9781450377225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {At this conference, I served as Conference Program Chair. Prof. Kai Hwang from Chinese University of Hong Kong, China, Prof. Yao Liang from Purdue University School of Science, Indiana University Purdue University, USA and I shared our speeches as keynote speakers. Dr Wei Li from Central Queensland University, Australia, Dr Ka-Chun Wong from City University of Hong Kong, Hong Kong and Dr. Gabriella Casalino from University of Bari, Italy served as the Invited Speakers. They also shared their research during the conference.},
location = {Shanghai, China}
}

@inproceedings{10.1145/3077839.3081670,
author = {Lee, Junghoon and Park, Gyung-Leen and Han, Yeonju and Yoo, Seunghee},
title = {Big Data Analysis for an Electric Vehicle Charging Infrastructure Using Open Data and Software},
year = {2017},
isbn = {9781450350365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077839.3081670},
doi = {10.1145/3077839.3081670},
abstract = {This paper describes a big data analysis strategy for electric vehicle charging infrastructure, mainly built upon open data sets and open software components. The data acquisition module periodically retrieves the real-time status information of each charger from the public data portal, while the downloaded XML files are parsed to extract fields of interest. At this stage, we present the distribution of charging facilities in Jeju City based on our own map viewer implementation, the city-wide dynamics of the number of chargers in operation based on MySQL queries, and the visualization of regional occupancy rates based on the R GISTools library. After combining a variety of statistical and machine learning techniques to understand the demand pattern of electric vehicle charging, we will integrate renewable energy to charging-intensive power grids as much as possible.},
booktitle = {Proceedings of the Eighth International Conference on Future Energy Systems},
pages = {252–253},
numpages = {2},
keywords = {big data analysis, Charging infrastructure, open software, occupancy rate, data acquisition},
location = {Shatin, Hong Kong},
series = {e-Energy '17}
}

@inproceedings{10.1145/2818869.2818871,
author = {Jian, Ming-Shen and Chen, Jun-Lin and Fang, Yi-Chi and Li, Siang-Jyun},
title = {Cloud Based Dynamical Digital Game Learning Scenario Corresponding to Individual Learner Big Data},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818871},
doi = {10.1145/2818869.2818871},
abstract = {In this paper, we propose a Cloud Based Dynamical Digital Game Learning Scenario Corresponding to Individual Learner Big Data according to the proposed dynamic assigning algorithm to separate individual learner into different levels. In dynamic assigning algorithm, student's answering times and answering correct rate for each education scenario in cloud based digital game learning system is considered. Suitable teaching scenario will be provided for individual user corresponding to personal learning ability. Each designed modular teaching materials can be embedded into the game platform on the virtual machine in cloud. The information about users in the individual game learning system can be stored in the cloud database. In addition, the whole learning system is established as a virtual machine. System maintainer can configure the learning system easily and quickly. Based on cloud, different remote devices can connect to server for learning.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {44},
numpages = {6},
keywords = {Virtualization, Dynamic assigning algorithm, Cloud Computing, Game-Learning, Digital game, RPG, Virtual Machine},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@inproceedings{10.1109/CCGrid.2012.143,
author = {Esswein, Sam and Goasguen, Sebastien and Post, Chris and Hallstrom, Jason and White, David and Eidson, Gene},
title = {Towards Ontology-Based Data Quality Inference in Large-Scale Sensor Networks},
year = {2012},
isbn = {9780769546919},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CCGrid.2012.143},
doi = {10.1109/CCGrid.2012.143},
abstract = {This paper presents an ontology-based approach for data quality inference on streaming observation data originating from large-scale sensor networks. We evaluate this approach in the context of an existing river basin monitoring program called the Intelligent River®. Our current methods for data quality evaluation are compared with the ontology-based inference methods described in this paper. We present an architecture that incorporates semantic inference into a publish/subscribe messaging middleware, allowing data quality inference to occur on real-time data streams. Our preliminary benchmark results indicate delays of 100ms for basic data quality checks based on an existing semantic web software framework. We demonstrate how these results can be maintained under increasing sensor data traffic rates by allowing inference software agents to work in parallel. These results indicate that data quality inference using the semantic sensor network paradigm is viable solution for data intensive, large-scale sensor networks.},
booktitle = {Proceedings of the 2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (Ccgrid 2012)},
pages = {898–903},
numpages = {6},
keywords = {Distributed Computing, Semantic Web, Wireless Sensor Networks},
series = {CCGRID '12}
}

@inproceedings{10.1145/3377672.3378054,
author = {Liqiang, Hao and Quan, Liu},
title = {Design of Resource Recommendation Model for Personalized Learning in the Era of Big Data},
year = {2020},
isbn = {9781450362481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377672.3378054},
doi = {10.1145/3377672.3378054},
abstract = {This paper proposes a personalized learning resource recommendation model based on big data. The design of the model consists of data storage, data analysis, resource matching, and the resource recommendation. In order to provide a suitable resource, data analysis is a more critical procedure that involves the analyses of basic information, learning style, learning status, learning behavior, and learning interest, which can be successfully analyzed by means of kafka and flume. Through an experiment, it shows that personalized resource recommendation platform really plays a positive role in improving students learning.},
booktitle = {Proceedings of the 2019 Annual Meeting on Management Engineering},
pages = {181–187},
numpages = {7},
keywords = {recommendation model, big data, personalized learning},
location = {Kuala Lumpur, Malaysia},
series = {AMME 2019}
}

@inproceedings{10.1145/3209415.3209427,
author = {Androutsopoulou, Aggeliki and Charalabidis, Yannis},
title = {A Framework for Evidence Based Policy Making Combining Big Data, Dynamic Modelling and Machine Intelligence},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209427},
doi = {10.1145/3209415.3209427},
abstract = {Governments and policy makers are striving to respond to contemporary socio-economic challenges, however, often neglecting the human factor and the multidimensionality of policy implications. In this chapter, a framework for evidence based policy making is proposed, which integrates the usage of open big data coming from a multiplicity of sources with policy simulations. It encompasses the application of dynamic modelling methodologies and data mining techniques to extract knowledge from two types of data. On the one hand, objective data such as governmental and statistical data, are used to capture the interlinked policy domains and their underlying casual mechanisms. On the other hand, behavioural patterns and citizens' opinions are extracted from Web 2.0 sources, social media posts, polls and statistical surveys. To combine this multimodal information, our approach suggests a modelling methodology that bases on big data acquisition and processing for the identification of significant factors and counterintuitive interrelations between them, which can be applied in any policy domain. Then, to allow the practical application of the framework an ICT architecture is designed, with the aim to overcome challenges related with big data management and processing. Finally, validation of the approach for driving policy design and implementation in the future in diverse policy domains, is suggested.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {575–583},
numpages = {9},
keywords = {Big data, behavioural patterns, dynamic simulation, data mining, evidence based policy making, impact assessment, policy Modelling},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@article{10.1145/3381343.3381345,
author = {Langdon, W. B.},
title = {Big Data Driven Genetic Improvement for Maintenance of Legacy Software Systems},
year = {2020},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
url = {https://doi.org/10.1145/3381343.3381345},
doi = {10.1145/3381343.3381345},
abstract = {Software is vital to modern life, yet much of it is old and suffers from bit-rot. There are not and never will be enough software experts to keep it all up to date by hand. Instead we suggest combining data driven learning with evolutionary search to maintain computer systems. @RE: &lt;1&gt;N. Alshahwan. Industrial experience of genetic improvement in Facebook. In J. Petke, S. H. Tan, W. B. Langdon, and W. Weimer, editors, GI-2019, ICSE workshops proceedings, page 1, Montreal, 28 May 2019. IEEE. Invited Keynote. &lt;2&gt;W. Banzhaf, P. Nordin, R. E. Keller, and F. D. Francone. Genetic Programming - An Introduction; On the Automatic Evolution of Computer Programs and its Applications. Morgan Kaufmann, San Francisco, CA, USA, Jan. 1998. &lt;3&gt;N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary Computation, 9(2):159--195, Summer 2001. &lt;4&gt;S. Haraldsson, A. Brownlee, and J. R. Woodward. Computers will soon be able to fix themselves - are IT departments for the chop? The Conversation, page 3.29pm BST, Oct. 12 2017. &lt;5&gt;S. O. Haraldsson, J. R. Woodward, A. E. I. Brownlee, and K. Siggeirsdottir. Fixing bugs in your sleep: How genetic improvement became an overnight success. In J. Petke, D. R. White, W. B. Langdon, and W. Weimer, editors, GI-2017, pages 1513--1520, Berlin, 15--19 July 2017. ACM. Best paper. &lt;6&gt;M. Harman and B. F. Jones. Search based software engineering. Information and Software Technology, 43(14):833--839, Dec. 2001. &lt;7&gt;F. Hutter, H. H. Hoos, K. Leyton-Brown, and T. Stuetzle. ParamILS: An automatic algorithm configuration framework. JAIR, 36:267--306, 2009. &lt;8&gt;G. Kendall. Evolutionary computation has been promising self-programming machines for 60 years - so where are they? The Conversation, page 8.54am BST, Mar. 27 2018. &lt;9&gt;J. R. Koza. Genetic Programming: On the Programming of Computers by Natural Selection. MIT press, 1992. &lt;10&gt;W. B. Langdon. Genetic improvement of programs. In R. Matousek, editor, 18th International Con- ference on Soft Computing, MENDEL 2012, Brno, Czech Republic, 27--29 June 2012. Brno University of Technology. Invited keynote. &lt;11&gt;W. B. Langdon and M. Harman. Optimising existing software with genetic programming. IEEE Transactions on Evolutionary Computation, 19(1):118--135, Feb. 2015. &lt;12&gt;W. B. Langdon, B. Y. H. Lam, J. Petke, and M. Harman. Improving CUDA DNA analysis soft- ware with genetic programming. In S. Silva, A. I. Esparcia-Alcazar, M. Lopez-Ibanez, S. Mostaghim, J. Timmis, C. Zarges, L. Correia, T. Soule, M. Giacobini, R. Urbanowicz, Y. Akimoto, T. Glasmach- ers, F. Fernandez de Vega, A. Hoover, P. Larranaga, M. Soto, C. Cotta, F. B. Pereira, J. Handl, J. Koutnik, A. Gaspar-Cunha, H. Trautmann, J.-B. Mouret, S. Risi, E. Costa, O. Schuetze, K. Kraw- iec, A. Moraglio, J. F. Miller, P. Widera, S. Cagnoni, J. Merelo, E. Hart, L. Trujillo, M. Kessentini, G. Ochoa, F. Chicano, and C. Doerr, editors, GECCO '15: Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, pages 1063--1070, Madrid, 11--15 July 2015. ACM. &lt;13&gt;W. B. Langdon and R. Lorenz. Improving SSE parallel code with grow and graft genetic programming. In J. Petke, D. R. White, W. B. Langdon, and W. Weimer, editors, GI-2017, pages 1537--1538, Berlin, 15--19 July 2017. ACM. &lt;14&gt;W. B. Langdon and J. Petke. Evolving better software parameters. In T. E. Colanzi and P. McMinn, editors, SSBSE 2018 Hot off the Press Track, volume 11036 of LNCS, pages 363--369, Montpellier, France, 8--9 Sept. 2018. Springer. &lt;15&gt;W. B. Langdon, J. Petke, and R. Lorenz. Evolving better RNAfold structure prediction. In M. Castelli, L. Sekanina, and M. Zhang, editors, EuroGP 2018: Proceedings of the 21st European Conference on Genetic Programming, volume 10781 of LNCS, pages 220--236, Parma, Italy, 4--6 Apr. 2018. Springer Verlag. &lt;16&gt;C. Le Goues, M. Pradel, and A. Roychoudhury. Automated program repair. Communications of the ACM. To appear. &lt;17&gt;M. Orlov. Evolving software building blocks with FINCH. In J. Petke, D. R. White, W. B. Langdon, and W. Weimer, editors, GI-2017, pages 1539--1540, Berlin, 15--19 July 2017. ACM. &lt;18&gt;L. Perez Caceres, M. Lopez-Ibanez, H. Hoos, and T. Stuetzle. An experimental study of adaptive capping in irace. In R. Battiti, D. E. Kvasov, and Y. D. Sergeyev, editors, Learning and Intelligent Optimization - 11th International Conference, LION 11, Nizhny Novgorod, Russia, June 19--21, 2017, Revised Selected Papers, volume 10556 of Lecture Notes in Computer Science, pages 235--250. Springer, 2017. &lt;19&gt;J. Petke. Constraints: The future of combinatorial interaction testing. In 2015 IEEE/ACM 8th International Workshop on Search-Based Software Testing, pages 17--18, Florence, May 2015. &lt;20&gt;J. Petke, S. O. Haraldsson, M. Harman, W. B. Langdon, D. R. White, and J. R. Woodward. Genetic improvement of software: a comprehensive survey. IEEE Transactions on Evolutionary Computation, 22(3):415--432, June 2018. &lt;21&gt;J. Petke, M. Harman, W. B. Langdon, and W. Weimer. Specialising software for different downstream applications using genetic improvement and code transplantation. IEEE Transactions on Software Engineering, 44(6):574--594, June 2018. &lt;22&gt;R. Poli, W. B. Langdon, and N. F. McPhee. A field guide to genetic programming. Published via http://lulu.com and freely available at http://www.gp-field-guide.org.uk, 2008. (With contri- butions by J. R. Koza). &lt;23&gt;E. Schulte, S. Forrest, and W. Weimer. Automated program repair through the evolution of assembly code. In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering, pages 313--316, Antwerp, 20--24 Sept. 2010. ACM. &lt;24&gt;E. Schulte, W. Weimer, and S. Forrest. Repairing COTS router firmware without access to source code or test suites: A case study in evolutionary software repair. In W. B. Langdon, J. Petke, and D. R. White, editors, Genetic Improvement 2015 Workshop, pages 847--854, Madrid, 11--15 July 2015. ACM. Best Paper. &lt;25&gt;J. R. Woodward, J. Petke, and W. Langdon. How computers are learning to make human software work more efficiently. The Conversation, page 10.08am BST, June 25 2015. &lt;26&gt;K. Yeboah-Antwi and B. Baudry. Online genetic improvement on the java virtual machine with ECSELR. Genetic Programming and Evolvable Machines, 18(1):83--109, Mar. 2017.},
journal = {SIGEVOlution},
month = {jan},
pages = {6–9},
numpages = {4}
}

@proceedings{10.1145/3305275,
title = {ISBDAI '18: Proceedings of the International Symposium on Big Data and Artificial Intelligence},
year = {2018},
isbn = {9781450365703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hong Kong, Hong Kong}
}

@proceedings{10.1145/3416921,
title = {ICCBDC '20: Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing},
year = {2020},
isbn = {9781450375382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The main goal and feature of the conference are to bring together scientists, engineers and industry researchers together to exchange and share their experiences, research results and discuss emerging practical problems and solutions. ICCBDC 2020 received 39 submissions of research papers. After a strict reviewing process, 23 of them have been accepted for presentation at the conference and publication in the proceedings. The conference program included invited talks, six research paper presentation sessions and one industry session. It covered recent trends and advances made in the fields of cloud and big data computing. All accepted papers were presented online in 15 minutes followed by discussions. This conference proceedings consists of the research papers presented at ICCBDC 2020.},
location = {Virtual, United Kingdom}
}

@inproceedings{10.1145/2632168.2638835,
author = {Xu, Guoqing},
title = {Language, Compiler, and Runtime System Support towards Highly Scalable Big Data Application (Invited Talk Abstract)},
year = {2014},
isbn = {9781450329347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2632168.2638835},
doi = {10.1145/2632168.2638835},
abstract = {Modern computing has entered the era of Big Data. Analyzing data from Twitter, Google, Facebook, Wikipedia, or the Human Genome Project requires the development of scalable platforms that can quickly extract useful information from an ocean of records collected from customers, clinical trial participants, program execution logs, or the Internet. Most of the existing Big Data applications, including Hadoop, Giraph, Hive, Pig, Mahout, or Hyracks are written managed, object-oriented languages such as Java. While the use of such languages simplifies development tasks, the (memory and execution) inefficiencies inherent in these languages can have large impact on the application performance and scalability. When object-orientation meets Big Data, performance problems are significantly magnified, making data-intensive computing systems fail to scale to large datasets. I will talk about several projects we are currently working on to scale Big Data applications by reducing the cost of a managed runtime. Particularly, I will talk about Facade, a compiler and runtime system we have developed to transform a Big Data application into an almost object-bounded application which has been shown to be much more efficient and scale to much larger datasets. I will also briefly mention two other projects, one attempting to provide a memory-oblivious programming model for developers to allow them to write a program without worrying about how to create threads and use memory, and second aiming to trim a big dataset with probabilistic guarantees to facilitate debugging/testing of a Big Data application.},
booktitle = {Proceedings of the 2014 Joint International Workshop on Dynamic Analysis (WODA) and Software and System Performance Testing, Debugging, and Analytics (PERTEA)},
pages = {13},
numpages = {1},
keywords = {Highly Scalable Big Data Application, System Support},
location = {San Jose, CA, USA},
series = {WODA+PERTEA 2014}
}

@inproceedings{10.1145/2493190.2499469,
author = {Rost, Mattias and Morrison, Alistair and Cramer, Henriette and Bentley, Frank},
title = {Informing Future Design via Large-Scale Research Methods and Big Data},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2499469},
doi = {10.1145/2493190.2499469},
abstract = {With the launch of 'app stores' on several mobile platforms and the great uptake of smartphones among the general population, researchers have begun utilising these distribution channels to deploy research software to large numbers of users. Previous Research In The Large workshops have sought to establish base-line practice in this area. We have seen the use of app stores as being successful as a methodology for gathering large amounts of data, leading to design implications, but we have yet to explore the full potential for this data's use and interpretation. How is it possible to leverage the practices of large-scale research, beyond the current approaches, to more directly inform future designs? We propose that the time is right to re-energise discussions on large-scale research, looking further than the basic methodological issues and assessing the potential for informing the design of new mobile software.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {612–615},
numpages = {4},
keywords = {app stores, research in the large, design, user study, large-scale mobile hci, mass participation},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2790755.2790762,
author = {Ceci, Michelangelo and Corizzo, Roberto and Fumarola, Fabio and Ianni, Michele and Malerba, Donato and Maria, Gaspare and Masciari, Elio and Oliverio, Marco and Rashkovska, Aleksandra},
title = {Big Data Techniques For Supporting Accurate Predictions of Energy Production From Renewable Sources},
year = {2015},
isbn = {9781450334143},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790755.2790762},
doi = {10.1145/2790755.2790762},
abstract = {Predicting the output power of renewable energy production plants distributed on a wide territory is a really valuable goal, both for marketing and energy management purposes. Vi-POC (Virtual Power Operating Center) project aims at designing and implementing a prototype which is able to achieve this goal. Due to the heterogeneity and the high volume of data, it is necessary to exploit suitable Big Data analysis techniques in order to perform a quick and secure access to data that cannot be obtained with traditional approaches for data management. In this paper, we describe Vi-POC -- a distributed system for storing huge amounts of data, gathered from energy production plants and weather prediction services. We use HBase over Hadoop framework on a cluster of commodity servers in order to provide a system that can be used as a basis for running machine learning algorithms. Indeed, we perform one-day ahead forecast of PV energy production based on Artificial Neural Networks in two learning settings, that is, structured and non-structured output prediction. Preliminary experimental results confirm the validity of the approach, also when compared with a baseline approach.},
booktitle = {Proceedings of the 19th International Database Engineering &amp; Applications Symposium},
pages = {62–71},
numpages = {10},
location = {Yokohama, Japan},
series = {IDEAS '15}
}

@inproceedings{10.1145/2630729.2630741,
author = {Brammertz, Willi and Mendelowitz, Allan I.},
title = {Limits and Opportunities of Big Data For Macro-Prudential Modeling of Financial Systemic Risk},
year = {2014},
isbn = {9781450330121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2630729.2630741},
doi = {10.1145/2630729.2630741},
abstract = {We explore the use of "big data", i.e. large unstructured data sets, within financial risk analysis. We conclude it has value, but structured data remain critical. We show that forward-looking financial analysis on the systemic level needs a data structure that represents financial contracts as algorithms that produce state contingent cash flows. Currently the industry lacks such a standard, which precludes meaningful systemic forward-looking analysis. We introduce ACTUS as an emerging standard that will enable consistent analysis on all levels. This standard will also create an infrastructure for macro financial analysis.},
booktitle = {Proceedings of the International Workshop on Data Science for Macro-Modeling},
pages = {1–6},
numpages = {6},
keywords = {Systemic Level, Financial Modeling, Data Standard for Financial Contracts},
location = {Snowbird, UT, USA},
series = {DSMM'14}
}

@inproceedings{10.1145/3404555.3404642,
author = {Xueyong, Hu and Bei, Wang and Lei, Zhao and Yang, Yang and Aiyu, Hu and Ge, Pan and Baoxian, Zhou},
title = {Research on Search Intent Prediction for Big Data of National Grid System Standards},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404642},
doi = {10.1145/3404555.3404642},
abstract = {Smart grids are becoming more complex due to the development of big data., and technical documents and institutional standards are constantly updated. As a result, It is difficult for workers in different positions to obtain the required information and data. This thesis is oriented towards this problem, and combined with deep learning algorithms to build a user intent prediction model based on the existing knowledge map. By extracting user characteristics and using a dynamic matching algorithm, the purpose of intent prediction is achieved. In this way, the required standards and requirements can be found faster and more directly in the work process, which effectively improves the working efficiency of employees and reduces the difficulty of learning and training.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {89–93},
numpages = {5},
keywords = {convolutional neural network, search intent prediction, Deep learning, personalization, dynamic matching, knowledge map},
location = {Tianjin, China},
series = {ICCAI '20}
}

@article{10.1145/1891879.1891881,
author = {Blake, Roger and Mangiameli, Paul},
title = {The Effects and Interactions of Data Quality and Problem Complexity on Classification},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1891879.1891881},
doi = {10.1145/1891879.1891881},
abstract = {Data quality remains a persistent problem in practice and a challenge for research. In this study we focus on the four dimensions of data quality noted as the most important to information consumers, namely accuracy, completeness, consistency, and timeliness. These dimensions are of particular concern for operational systems, and most importantly for data warehouses, which are often used as the primary data source for analyses such as classification, a general type of data mining. However, the definitions and conceptual models of these dimensions have not been collectively considered with respect to data mining in general or classification in particular. Nor have they been considered for problem complexity. Conversely, these four dimensions of data quality have only been indirectly addressed by data mining research. Using definitions and constructs of data quality dimensions, our research evaluates the effects of both data quality and problem complexity on generated data and tests the results in a real-world case. Six different classification outcomes selected from the spectrum of classification algorithms show that data quality and problem complexity have significant main and interaction effects. From the findings of significant effects, the economics of higher data quality are evaluated for a frequent application of classification and illustrated by the real-world case.},
journal = {J. Data and Information Quality},
month = {feb},
articleno = {8},
numpages = {28},
keywords = {Data quality, data quality metrics and measurements, data mining, information quality}
}

@inproceedings{10.1145/3291801.3291817,
author = {Li, Dezhi and Wang, Xiaohui and Cai, Wei and Zhu, Linsen and Guo, Chunsheng},
title = {Application of Big Data Collection Based on Self-Powered Technology in Intelligent Transportation System},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291817},
doi = {10.1145/3291801.3291817},
abstract = {Intelligent transportation system is an important part of smart city. The wireless sensor network (WSN) system can provide real-time and reliable big data for the intelligent transportation system, but the power supply of the wireless sensor node is a difficult point that restricts its wide application. This study will use the piezoceramic to converts vibration energy in the surroundings into voltage output. The power management module can meet the energy requirements of MEMS devices such as wireless sensor nodes, wireless monitoring modules, accelerometers and so on. Using piezoceramic as energy supply, the total mass, speed and wheelbase of vehicles can be potentially measured through systematic layout, which is to achieve self-powered demand. The acceleration and temperature sensors can be used to record acceleration information and surface temperature data of the vehicle respectively. A wireless sensor node is established in unit of a wireless transmission module, and a wireless sensor network is further formed. The data obtained by the wireless sensor network is processed in the server to obtain traffic information such as traffic volume, vehicle type and overspeed in different sections in real time, which provides powerful and accurate data support for the planning and decision-making of the intelligent transportation system.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {47–51},
numpages = {5},
keywords = {intelligent transportation system, sensor node, big data, wireless sensor network (WSN), self-powered},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/3414274.3414511,
author = {Zhu, Jiahui and Chen, Yuepeng and Fu, Qingwen and Zhang, Jiawen},
title = {A Real-Time Anomaly Detection Algorithm for Taxis Based on Trajectory Big Data},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414511},
doi = {10.1145/3414274.3414511},
abstract = {In order to prevent taxi drivers from deliberately detouring or other fraudulent behaviors, a real-time anomaly trajectory detection algorithm based on spatial-temporal big data of taxi is proposed to obtain the real-time detection results with fast feedback. Firstly, the road network map is divided into grid and numbered, and the feature points obtained with direction angle, then transform original trajectory into feature grid trajectory sequence by preprocessing such as matching and completion. Then, the common sequence of the trajectory is obtained from a large number of taxi historical data, and the standard K trajectory between the start point and the end point is obtained by clustering. When the real-time trajectory detection, it is matched with the standard K trajectory, only edit distance of two trajectory need to be calculated considering the spatial-temporal threshold. Finally, based on the real data set of taxi GPS trajectory in Beijing from March to May 2011, a large number of experiments are carried out, and verified the effectiveness and efficiency of the proposed algorithm.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {227–231},
numpages = {5},
keywords = {cluster trajectory, anomaly detection, GPS trajectory, spatial temporal mining},
location = {Xiamen, China},
series = {DSIT 2020}
}

@article{10.14778/3137628.3137634,
author = {Mehta, Parmita and Dorkenwald, Sven and Zhao, Dongfang and Kaftan, Tomer and Cheung, Alvin and Balazinska, Magdalena and Rokem, Ariel and Connolly, Andrew and Vanderplas, Jacob and AlSayyad, Yusra},
title = {Comparative Evaluation of Big-Data Systems on Scientific Image Analytics Workloads},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137628.3137634},
doi = {10.14778/3137628.3137634},
abstract = {Scientific discoveries are increasingly driven by analyzing large volumes of image data. Many new libraries and specialized database management systems (DBMSs) have emerged to support such tasks. It is unclear how well these systems support real-world image analysis use cases, and how performant the image analytics tasks implemented on top of such systems are. In this paper, we present the first comprehensive evaluation of large-scale image analysis systems using two real-world scientific image data processing use cases. We evaluate five representative systems (SciDB, Myria, Spark, Dask, and TensorFlow) and find that each of them has shortcomings that complicate implementation or hurt performance. Such shortcomings lead to new research opportunities in making large-scale image analysis both efficient and easy to use.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1226–1237},
numpages = {12}
}

@inproceedings{10.1145/2955129.2955190,
author = {Zhang, Fuquan and Mao, Zijing and Ding, Gangyi},
title = {Simulation and Analysis of Classification Optimization Model of Temperature Sensing Big Data in Intelligent Building},
year = {2016},
isbn = {9781450341295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2955129.2955190},
doi = {10.1145/2955129.2955190},
abstract = {The temperature sensor network in intelligent building classified collection of big data processing has the problem of big data redundancy interference, which results in unable to determine the fixed filter thresholds. This paper proposed Chaos differential disturbance based fuzzy C-means clustering model for big temperature sensing data classification tasks. It requires to analyze temperature sensor in the intelligent building big distributed structure model of data in a database storage system, the big data information flow feature fusion and time series analysis. Based on traditional fuzzy c-means clustering processing, we introduced chaos disturbance to avoid the classification into local convergence and local optimum, and therefore improve the performance of data clustering. The testing results show that our proposed classification method effectively reduces the error rate for classification tasks of temperature data in intelligent building and have achieved the best performance among the existing algorithms.},
booktitle = {Proceedings of the The 3rd Multidisciplinary International Social Networks Conference on SocialInformatics 2016, Data Science 2016},
articleno = {52},
numpages = {5},
keywords = {Intelligent building, Classification, Big data, Temperature sensor},
location = {Union, NJ, USA},
series = {MISNC, SI, DS 2016}
}

@inproceedings{10.5555/2693848.2693962,
author = {de C. Gatti, Ma\'{\i}ra A. and Vieira, Marcos R. and de Melo, Jo\~{a}o Paulo F. and Cavalin, Paulo Rodrigo and Pinhanez, Claudio Santos},
title = {Handling Big Data on Agent-Based Modeling of Online Social Networks with Mapreduce},
year = {2014},
publisher = {IEEE Press},
abstract = {There is an increasing interest on using Online Social Networks (OSNs) in a wide range of applications. Two interesting problems that have received a lot of attention in OSNs is how to provide effective ways to understand and predict how users behave, and how to build accurate models for specific domains (e.g., marketing campaigns). In this context, stochastic multi-agent based simulation can be employed to reproduce the behavior observed in OSNs. Nevertheless, the first step to build an accurate behavior model is to create an agent-based system. Hence, a modeler needs not only to be effective, but also to scale up given the huge volume of streaming graph data. To tackle the above challenges, this paper proposes a MapReduce-based method to build a modeler to handle big data. We demonstrate in our experiments how efficient and effective our proposal is using the Obama's Twitter network on the 2012 U.S. presidential election.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {851–862},
numpages = {12},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/3419635.3419736,
author = {Guo, Jianliang},
title = {Empirical Analysis for English Teaching Integration and Optimization Based on Big Data Mining Technology},
year = {2020},
isbn = {9781450387729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419635.3419736},
doi = {10.1145/3419635.3419736},
abstract = {This article takes English teaching behavior analysis as a breakthrough point, combines big data mining technology, optimizes English teaching plan analysis methods, and uses data mining methods on the basis of inductive analysis to compare the K nearest neighbour algorithm and the support vector machine algorithm. The algorithm is combined with the support vector machine algorithm to obtain the advantages and disadvantages of the new algorithm. A classification prediction model is built. Through the built prediction model, the prediction of English teaching integration is optimized. The classifier's prediction results for accuracy, precision, and recall are 89.63%, 90.63%, and 71.01% respectively. This can provide a basis for educators to optimize decision-making and optimize teaching methods.},
booktitle = {Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education},
pages = {504–508},
numpages = {5},
keywords = {Big data, Integration and optimization, English teaching, Data mining},
location = {Ottawa, ON, Canada},
series = {CIPAE 2020}
}

@inproceedings{10.1145/3396452.3396463,
author = {Panqiu, Jiang and Li, Lin},
title = {Research on Methods of Psychological Health Education of University Students in the Context of Big Data},
year = {2020},
isbn = {9781450374989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396452.3396463},
doi = {10.1145/3396452.3396463},
abstract = {With the further development of big data, the psychological health education of university students confronts with new challenges and opportunities. However, so far, no research has been conducted to construct a new methods of psychological health education of University Students through combining big data from the perspective of Internet. This paper takes the psychological health education of university students as research object and the idea of positive psychology is referenced as guidance in order to explore the methods of psychological health education of university students in the context of big data. This work builds the mixed curriculum system of psychological health education combining online and offline in university to stimulate students' potential self-help capabilities and improve the system of psychological health education of university students.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Big Data and Education},
pages = {10–14},
numpages = {5},
keywords = {psychological health education, big data, university students, positive psychology},
location = {London, United Kingdom},
series = {ICBDE '20}
}

@article{10.5555/2831432.2831475,
author = {Gruss, Richard and Farag, Mohamed and Kanan, Tarek and English, Mary C. and Zhang, Xuan and Fox, Edward A.},
title = {Teaching Big Data through Project-Based Learning in Computational Linguistics and Information Retrieval},
year = {2015},
issue_date = {December 2015},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {31},
number = {2},
issn = {1937-4771},
abstract = {In Project Based Learning (PBL), students acquire knowledge just-in-time while completing a large project driven by a particular question. We demonstrate that this approach is particularly well suited to courses in two computer science (CS) domains pertaining to Big Data: Computational Linguistics and Information Retrieval. The courses presented here proved successful, as evidenced by both the high quality of the student projects and the positive responses on end-of-semester surveys.},
journal = {J. Comput. Sci. Coll.},
month = {dec},
pages = {260–270},
numpages = {11}
}

