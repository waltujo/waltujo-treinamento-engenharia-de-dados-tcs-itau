@inproceedings{10.1145/3281375.3281386,title = {A semantic-based model to represent multimedia big data}, author = {Rinaldi Antonio M. , Russo Cristiano },year = {2018}, isbn = {9781450356220}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3281375.3281386}, doi = {10.1145/3281375.3281386}, abstract = {The use of formal representation is a key task in the era of big data. In the context of multimedia big data this issue is stressed due to the intrinsic complexity nature of this kind of data. Moreover, the relations among objects should be clearly expressed and formalized to give the right meaning of data correlation. For this reason the design of formal models to represent and manage information is a necessary task to implement intelligent information systems. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of big data applications. Using these models the visualization of data and information become an intrinsic and strategic task for the analysis and exploration of multimedia BigData. In this paper we propose the use of a semantic approach to formalize the model structure of multimedia BigData. In addition, the recognition of multimodal features to represent concepts and linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. The proposed model has been implemented in a NoSQL graph database populated from different knowledge sources and a visualization of this very large knowledge base has been presented and discussed as a case study.}, location = {Tokyo, Japan}, series = {MEDES '18}, pages = {31\u201338}, numpages = {8}, keywords = {semantics, semantic bigdata, multimedia ontologies}}
@inproceedings{10.1145/3529570.3529580,title = {Inference and Prediction in Big Data Using Sparse Gaussian Process Method}, author = {Yobsan Bayisa Leta , Wang Weidong , Wang Qing-xian , Meseret Debele Gurmu , Bona Debela Lamessa },year = {2022}, isbn = {9781450395809}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3529570.3529580}, doi = {10.1145/3529570.3529580}, abstract = {Gaussian process is one of computationally expensive algorithm for large datasets and lack of the flexibility to model different datasets is a common problem for modeling it. We introduce sparse Gaussian regression with the combination of designed kernels to solve the computational complexity of a traditional Gaussian process by taking pseudo input from large datasets and developing a model with better accuracy which enables Gaussian process application. We design a better combination of the kernel that can catch up with most of our data points. We demonstrate the approach on a large weather dataset and sales record dataset. Both are open source big datasets available online. Numerous experiments and comparisons with traditional Gaussian process methods using both large datasets demonstrate the efficiency and accuracy of sparse Gaussian processes.}, location = {Chengdu, China}, series = {ICDSP '22}, pages = {54\u201362}, numpages = {9}, keywords = {Big data, Pseudo-input, Kernel, Gaussian Process, Inference}}
@inproceedings{10.1145/2744700.2744703,title = {Spatial big data for eco-routing services: computational challenges and accomplishments}, author = {Ali Reem Y. , Gunturi Venkata M. V. , Shekhar Shashi },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2744700.2744703}, doi = {10.1145/2744700.2744703}, abstract = {The size, variety, and update rate of spatial datasets are increasingly exceeding the capacity of commonly used spatial computing technologies to learn, manage, and process the data with reasonable effort. We refer to these datasets as Spatial Big Data (SBD). Examples of emerging SBD datasets include temporally detailed (TD) roadmaps that provide speeds every minute for every road-segment, GPS track data from cell-phones, and engine measurements of fuel consumption, greenhouse gas (GHG) emissions, etc. Harnessing SBD has a transformative potential. For example, a 2011 McKinsey Global Institute report estimates savings of \"about $600 billion annually by 2020\" in terms of fuel and time saved by helping vehicles avoid congestion and reduce idling at red lights or left turns. In this paper, we discuss the challenges posed by SBD for a next generation of routing services and we present our work towards addressing these challenges.}, pages = {19\u201325}, numpages = {7}}
@inproceedings{10.1145/2608020.2612731,title = {Big data challenges in simulation-based science}, author = {Parashar Manish },year = {2014}, isbn = {9781450329132}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2608020.2612731}, doi = {10.1145/2608020.2612731}, abstract = {Data-related challenges are quickly dominating computational and data-enabled sciences, and are limiting the potential impact of scientific applications enabled by current and emerging high-performance distributed computing environments. These data-intensive application workflows involve dynamic coordination, interactions and data coupling between multiple application process that run at scale on different resources, and with services for monitoring, analysis and visualization and archiving. In this talk I will explore data grand challenges in simulation-based science and investigate how solutions based on data sharing abstractions, managed data pipelines, in-memory data-staging, in-situ placement and execution, and in-transit data processing can be used to address these data challenges at extreme scales.}, location = {Vancouver, BC, Canada}, series = {DIDC '14}, pages = {1\u20132}, numpages = {2}, keywords = {dataspaces, data-intensive computing, in-situ data processing, data staging, extreme-scale computing}}
@inproceedings{10.1145/3393527.3393532,title = {Research on Copyright Protection Method of Big Data Based on Nash Equilibrium and Constraint Optimization}, author = {Shi Bin , YabinXu },year = {2020}, isbn = {9781450375344}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3393527.3393532}, doi = {10.1145/3393527.3393532}, abstract = {Data watermarking technology is an effective means to protect the copyright of big data. In order to embed robust and highly available data watermarks, firstly, based on the game theory, a Nash equilibrium model between watermark robustness and data quality is established to solve the optimal number of data partitioning. Then, the mapping relationship between data partitioning and watermark bit is established by using secure hash algorithm. Finally, under the constraint of data usability, the improved particle swarm optimization algorithm is used to calculate the optimal solution of data change for each data partitioning, and then the data is changed accordingly to complete the embedding of watermark bit. In order to verify the copyright ownership of big data, this paper also gives the corresponding watermark extraction method. Watermark extraction is the inverse process of watermark embedding. First, traverse all partitions and extract the possible embedded bit values in each data partitioning. Then, the actual embedded watermark bit is finally determined by majority voting strategies. The experimental results show that our proposed method can not only detect watermarks under different attack conditions, ensure the robustness of big data watermarks, but also achieve better data quality, and the comprehensive effect of data watermarks is better than the existing methods.}, location = {Hefei, China}, series = {ACM TURC'20}, pages = {21\u201325}, numpages = {5}, keywords = {Data watermarking, Particle swarm optimization algorithm, Nash equilibrium, Constrained optimization, Majority voting strategy, Copyright protection, Big data}}
@inproceedings{10.14778/3229863.3229867,title = {Automating large-scale data quality verification}, author = {Schelter Sebastian , Lange Dustin , Schmidt Philipp , Celikel Meltem , Biessmann Felix , Grafberger Andreas },year = {2018}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3229863.3229867}, doi = {10.14778/3229863.3229867}, abstract = {Modern companies and institutions rely on data to guide every single business process and decision. Missing or incorrect information seriously compromises any decision process downstream. Therefore, a crucial, but tedious task for everyone involved in data processing is to verify the quality of their data. We present a system for automating the verification of data quality at scale, which meets the requirements of production use cases. Our system provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables 'unit tests' for data. We efficiently execute the resulting constraint validation workload by translating it to aggregation queries on Apache Spark. Our platform supports the incremental validation of data quality on growing datasets, and leverages machine learning, e.g., for enhancing constraint suggestions, for estimating the 'predictability' of a column, and for detecting anomalies in historic data quality time series. We discuss our design decisions, describe the resulting system architecture, and present an experimental evaluation on various datasets.}, pages = {1781\u20131794}, numpages = {14}}
@inproceedings{10.1145/3362121,title = {Ethical Dimensions for Data Quality}, author = {Firmani Donatella , Tanca Letizia , Torlone Riccardo },year = {2019}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3362121}, doi = {10.1145/3362121}, pages = {1\u20135}, numpages = {5}, keywords = {source selection, Data integration, knowledge extraction}}
@inproceedings{10.1145/3501409.3501630,title = {Research on the application of cloud computing technology in computer big data analysis}, author = {Chen Genjin },year = {2021}, isbn = {9781450384322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3501409.3501630}, doi = {10.1145/3501409.3501630}, abstract = {With the development of the times, computer technology is making continuous progress. People's every move is generating data. In the face of such a large amount of data information, the traditional data processing technology has been unable to meet people's needs. Big data analysis and cloud computing technology came into being. Cloud computing and big data technology have far exceeded the traditional data processing technology in terms of accuracy and speed. Now cloud computing and big data analysis have become hot words in the society, and relevant experts and scholars are constantly studying these two technologies. Starting from the elaboration of cloud computing technology and big data technology, this paper explores the advantages and problems of cloud computing, deeply analyzes the application of cloud computing technology in computer big data, imagines the future of cloud computer technology, and provides some references for the future development of cloud computing technology.}, location = {Xiamen, China}, series = {EITCE 2021}, pages = {1257\u20131262}, numpages = {6}, keywords = {Research, Cloud computing technology, Big data technology, Computer technology}}
@inproceedings{10.1145/2685553.2685558,title = {Ethics for Studying Online Sociotechnical Systems in a Big Data World}, author = {Fiesler Casey , Young Alyson , Peyton Tamara , Bruckman Amy S. , Gray Mary , Hancock Jeff , Lutters Wayne },year = {2015}, isbn = {9781450329460}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2685553.2685558}, doi = {10.1145/2685553.2685558}, abstract = {The evolution of social technology and research methods present ongoing challenges to studying people online. Recent high-profile cases have prompted discussion among both the research community and the general public about the ethical implications of researching humans, their information, and their activities in large-scale digital contexts. Examples of scientific and market research involving Facebook users and OKCupid clients exemplify the ethical complexities of both studying and manipulating online user behavior. When does data science become human subjects research, and what are our obligations to these subjects as researchers' Drawing from previous work around the ethics of digital research, one goal of this workshop is to work towards a set of guiding principles for CSCW scholars doing research online.}, location = {Vancouver, BC, Canada}, series = {CSCW'15 Companion}, pages = {289\u2013292}, numpages = {4}, keywords = {sociotechnical systems, online communities, research ethics, big data}}
@inproceedings{10.1145/2851613.2851881,title = {Virtualization technologies for the big data environment}, author = {Jlassi Aymen , Martineau Patrick },year = {2016}, isbn = {9781450337397}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2851613.2851881}, doi = {10.1145/2851613.2851881}, abstract = {Today, consumers request virtual resources like CPU, RAM, disk (etc.) supplied by the service providers (like Amazon) and they pay on a \"pay-as-you-go\" basis. Generally, the supervisors adopt virtualization technologies, which optimize resources usage and limit the operating cost. The virtualization technologies are classified in two categories. The first one concerns the heavy virtualization. Each virtual machines (VM) emulates hardware and embeds its own operating system (OS) that is completely isolated from the host OS. The second one concerns the light virtualization, which is based on the management of containers. The containers share the host OS kernel [5] while ensuring isolation. In this paper, we benchmark the performance and the energy consumption of an infrastructure that is based on the software Hadoop regarding the two technologies of virtualization. At first, we will identify the points to be improved concerning Hadoop performances and then we will reduce the deployment cost on the cloud. Second, the Hadoop community finds an in-depth study of the resources consumption depending on the environment of deployment. Our experiments are based on the comparison of the Docker technology (light virtualization) and VMware technology\u00ae (heavy virtualization). We come to the point that in most experiments the light technology offers better performances in completion time of workloads and it is more adapted to be used with the Hadoop software.}, location = {Pisa, Italy}, series = {SAC '16}, pages = {542\u2013545}, numpages = {4}, keywords = {resources consumption, virtualization, Hadoop, benchmarks}}
@inproceedings{10.1145/3297720,title = {Augmenting Data Quality through High-Precision Gender Categorization}, author = {M\u00fcller Daniel , Jain Pratiksha , Te Yieh-Funk },year = {2019}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297720}, doi = {10.1145/3297720}, abstract = {Mappings of first name to gender have been widely recognized as a critical tool for the completion, study, and validation of data records in a range of areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 4 million people, which was provided by a car insurance company. Then, we study how naming conventions have changed over time and how they differ by nationality. Next, we build a probabilistic first-name-to-gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in two-label and three-label settings and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies\u2019 outcomes and find that our mapping produces high-precision results. We validate that the additional information of nationality and year of birth improve the precision scores of name-to-gender mappings. Therefore, the proposed approach constitutes an efficient process for improving the data quality of organizations\u2019 records, if the gender attribute is missing or unreliable.}, pages = {1\u201318}, numpages = {18}, keywords = {gender name mapping, patenting, record completion, Data quality improvement}}
@inproceedings{10.1145/2487575.2487677,title = {Big data analytics with small footprint: squaring the cloud}, author = {Canny John , Zhao Huasha },year = {2013}, isbn = {9781450321747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2487575.2487677}, doi = {10.1145/2487575.2487677}, abstract = {This paper describes the BID Data Suite, a collection of hardware, software and design patterns that enable fast, large-scale data mining at very low cost. By co-designing all of these elements we achieve single-machine performance levels that equal or exceed reported cluster implementations for common benchmark problems. A key design criterion is rapid exploration of models, hence the system is interactive and primarily single-user. The elements of the suite are: (i) the data engine, a hardware design pattern that balances storage, CPU and GPU acceleration for typical data mining workloads, (ii) BIDMat, an interactive matrix library that integrates CPU and GPU acceleration and novel computational kernels (iii), BIDMach, a machine learning system that includes very efficient model optimizers, (iv) Butterfly mixing, a communication strategy that hides the latency of frequent model updates needed by fast optimizers and (v) Design patterns to improve performance of iterative update algorithms. We present several benchmark problems to show how the above elements combine to yield multiple orders-of-magnitude improvements for each problem.}, location = {Chicago, Illinois, USA}, series = {KDD '13}, pages = {95\u2013103}, numpages = {9}, keywords = {machine learning, toolkit, data mining, cluster, gpu}}
@inproceedings{10.1145/2588555.2618215,title = {Are we experiencing a big data bubble?}, author = {\u00d6zcan Fatma , Tatbul Nesime , Abadi Daniel J. , Kornacker Marcel , Mohan C. , Ramasamy Karthik , Wiener Janet },year = {2014}, isbn = {9781450323765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2588555.2618215}, doi = {10.1145/2588555.2618215}, location = {Snowbird, Utah, USA}, series = {SIGMOD '14}, pages = {1407\u20131408}, numpages = {2}, keywords = {SQL-on-HADOOP, NewSQL, NoSQL}}
@inproceedings{10.1145/3178461.3178464,title = {A Smart City Environmental Monitoring Network and Analysis Relying on Big Data Techniques}, author = {Tahat Ashraf , Aburub Ruba , Al-Zyoude Aseel , Talhi Chamseddine },year = {2018}, isbn = {9781450354387}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3178461.3178464}, doi = {10.1145/3178461.3178464}, abstract = {A new integrated environmental monitoring system to carry-out real-time measurements on board a moving vehicle is presented. It is composed of an arbitrary number of Electronic Measurements Units (EMU), a smart phone application to relay collected data, and a cloud Central Processing Platform (CPP) to perform analysis utilizing big data techniques and algorithms. Each EMU consists of an electric circuit that incorporates an ultra violet (UV) sensor, an air particles concentration sensor, a temperature sensor and a humidity sensor that all interface to a microcontroller. Bluetooth is employed for communication between the EMU and the smart phone application, while a 3G/4G cellular communications network furnishes the wireless connectivity to the remote CPP. When the collected data reaches the designated cloud server (CPP), it is immediately stored for subsequent analysis. Finally, big data statistical analysis (clustering and classification), mapping and plotting are performed to deduce correlations and to facilitate inferencing. Moreover, the scalability and low-cost of selected components of this realistic system makes it very feasible for large scale deployments in the context of smart cities initiatives, ad-hoc designs, or educational projects.}, location = {Casablanca, Morocco}, series = {ICSIM2018}, pages = {82\u201386}, numpages = {5}, keywords = {air particles, UV index, smart phone, temperature sensor, environment, Big data, telemetry}}
@inproceedings{10.14778/2752939.2752945,title = {A performance study of big data on small nodes}, author = {Loghin Dumitrel , Tudor Bogdan Marius , Zhang Hao , Ooi Beng Chin , Teo Yong Meng },year = {2015}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2752939.2752945}, doi = {10.14778/2752939.2752945}, abstract = {The continuous increase in volume, variety and velocity of Big Data exposes datacenter resource scaling to an energy utilization problem. Traditionally, datacenters employ x86-64 (big) server nodes with power usage of tens to hundreds of Watts. But lately, low-power (small) systems originally developed for mobile devices have seen significant improvements in performance. These improvements could lead to the adoption of such small systems in servers, as announced by major industry players. In this context, we systematically conduct a performance study of Big Data execution on small nodes in comparison with traditional big nodes, and present insights that would be useful for future development. We run Hadoop MapReduce, MySQL and in-memory Shark workloads on clusters of ARM big. LITTLE boards and Intel Xeon server systems. We evaluate execution time, energy usage and total cost of running the workloads on self-hosted ARM and Xeon nodes. Our study shows that there is no one size fits all rule for judging the efficiency of executing Big Data workloads on small and big nodes. But small memory size, low memory and I/O bandwidths, and software immaturity concur in canceling the lower-power advantage of ARM servers. We show that I/O-intensive MapReduce workloads are more energy-efficient to run on Xeon nodes. In contrast, database query processing is always more energy-efficient on ARM servers, at the cost of slightly lower throughput. With minor software modifications, CPU-intensive MapReduce workloads are almost four times cheaper to execute on ARM servers.}, pages = {762\u2013773}, numpages = {12}}
@inproceedings{10.1145/2568088.2576096,title = {Extreme big data processing in large-scale graph analytics and billion-scale social simulation}, author = {Suzumura Toyotaro },year = {2014}, isbn = {9781450327336}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2568088.2576096}, doi = {10.1145/2568088.2576096}, abstract = {This paper introduces some of the example applications handling extremely big data with supercomputers such as large-scale network analysis, X10-based large-scale graph analytics library, Graph500 benchmark, and billion-scale social simulation.}, location = {Dublin, Ireland}, series = {ICPE '14}, pages = {1\u20132}, numpages = {2}, keywords = {big data, graph, social simulation, supercomputer}}
@inproceedings{10.1145/3377817.3377836,title = {A Systematic Review on Big Data Analytics Frameworks for Higher Education - Tools and Algorithms}, author = {Otoo-Arthur David , Van Zyl Terence },year = {2019}, isbn = {9781450366496}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377817.3377836}, doi = {10.1145/3377817.3377836}, abstract = {The development of Big Data applications in education has drawn much attention in the last few years due to the enormous benefits it brings to improving teaching and learning. The integration of these Big Data applications in education generates massive data that put new demands to available processing technologies of data and extraction of useful information. Primarily, several higher educational institutions depend on the knowledge mined from these vast volumes of data to optimise the teaching and learning environment. However, Big Data in the higher education context has relied on traditional data techniques and platforms that are less efficient. This paper, therefore, conducts a Systematic Literature Review (SLR) that examines Big Data framework technologies in higher education outlining gaps that need a solution in Big Educational Data Analytics. We achieved this by summarising the current knowledge on the topic and recommend areas where educational institutions could focus on exploring the potential of Big Data Analytics. To this end, we reviewed 55 related articles out of 1543 selected from Six (6) accessible Computer Science databases between the period of 2007 and 2018, focusing on the development of the Big Data framework and its applicability in education for academic purposes. Our results show that very few researchers have tried to address the integrative use of Big Data framework and learning analytics in higher education. The review further suggests that there is an emerging best practice in applying Big Data Analytics to improve teaching and learning. However, this information does not appear to have been thoroughly examined in higher education. Hence, there is the need for a complete investigation to come up with comprehensive Big Data frameworks that build effective learning systems for instructors, learners, course designers and educational administrators.}, location = {Kuala Lumpur, Malaysia}, series = {EBIMCS '19}, pages = {1\u20139}, numpages = {9}, keywords = {Big Educational Data, MS, Learning Analytics, Data Mining, Big Data, Higher Education}}
@inproceedings{10.1145/3378936.3378950,title = {Application of Cloud Computing for Big Data in the X-Ray Crystallography Community}, author = {Tosson A. , Shokr M. , Pietsch U. },year = {2020}, isbn = {9781450376907}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3378936.3378950}, doi = {10.1145/3378936.3378950}, abstract = {The X-ray crystallography community has recently been affected by a significant increase in data volume caused by the use of advanced detector technologies and the new generation of high brilliance light sources. The fact that forced the decision makers to implement Big Data analytics, aiming to achieve a suitable environment for scientists at experimental and post-experimental phases. This paper demonstrates an extension of our approach towards a compact platform which provides the scientists with the digital ecosystem for the systematic harvest of data. It introduces an innovative solution to use warehousing and cloud computing to manage datasets collected by 2D energy-dispersive detectors, for an example. Moreover, it suggests that, deploying a Software as a Service (SaaS) cloud model, a public cloud data center, and cloud-based in-memory warehousing architecture, it is possible to dramatically reduce both hardware and processing costs.}, location = {Sydney, NSW, Australia}, series = {ICSIM '20}, pages = {1\u20134}, numpages = {4}, keywords = {Big Data, Crystallography, Cloud computing, In-Memory warehousing}}
@inproceedings{10.1145/3297280.3297474,title = {Evaluation of ACE properties of traditional SQL and NoSQL big data systems}, author = {Gonzalez-Aparicio Maria Teresa , Younas Muhammad , Tuya Javier , Casado Rub\u00e9n },year = {2019}, isbn = {9781450359337}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297280.3297474}, doi = {10.1145/3297280.3297474}, abstract = {Traditional SQL and NoSQL big data systems are the backbone for managing data in cloud, fog and edge computing. This paper develops a new system and adopts the TPC-DS industry standard benchmark in order to evaluate three key properties, availability, consistency and efficiency (ACE) of SQL and NoSQL systems. The contributions of this work are manifold. It evaluates and analyses the tradeoff between the ACE properties. It provides insight into the NoSQL systems and how they can be improved to be sustainable for a more wide range of applications. The evaluation shows that SQL provides stronger consistency, but at the expense of low efficiency and availability. NoSQL provides better efficiency and availability but lacks support for stronger consistency. In order for NoSQL systems to be more sustainable they need to implement transactional schemes that enforce stronger consistency as well as better efficiency and availability.}, location = {Limassol, Cyprus}, series = {SAC '19}, pages = {1988\u20131995}, numpages = {8}, keywords = {big data, data consistency, SQL, TPC-DS, NoSQL, Riak}}
@inproceedings{10.1145/3093338.3093372,title = {Optimizing High Performance Big Data Cancer Workflows}, author = {Jimenez-Ruiz Ivan , Gonzalez-Mendez Ricardo , Ropelewski Alexander },year = {2017}, isbn = {9781450352727}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3093338.3093372}, doi = {10.1145/3093338.3093372}, abstract = {Appropriate optimization of bioinformatics workflows is vital to improve the timely discovery of variants implicated in cancer genomics. Sequenced human brain tumor data was assembled to optimize tool implementations and run various components of RNA sequence (RNA-seq) workflows. The measurable information produced by these tools account for the success rate and overall efficiency of a standardized and simultaneous analysis. We used the National Center for Biotechnology Information) Sequence Read Archive (NCBI-SRA) database to retrieve two transcriptomic datasets containing over 104 million reads as input data. We used these datasets to benchmark various file systems on the Bridges supercomputer to improve overall workflow throughput. Based on program and job timings, we report critical recommendations on selections of appropriate file systems and node types to efficiently execute these workflows.}, location = {New Orleans, LA, USA}, series = {PEARC17}, pages = {1\u20134}, numpages = {4}, keywords = {Bioinformatics, Supercomputing, File Systems, Timings, Memory, ACM proceedings, Transcriptome, Performance, Workflows, Genome}}
@inproceedings{10.1145/2771299,title = {Bringing big data to the big tent}, author = {Goth Gregory },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2771299}, doi = {10.1145/2771299}, abstract = {Open source tools assist data science.}, pages = {17\u201319}, numpages = {3}}
@inproceedings{10.1145/3511211,title = {Unified Holistic Memory Management Supporting Multiple Big Data Processing Frameworks over Hybrid Memories}, author = {Chen Lei , Zhao Jiacheng , Wang Chenxi , Cao Ting , Zigman John , Volos Haris , Mutlu Onur , Lv Fang , Feng Xiaobing , Xu Guoqing Harry , Cui Huimin },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3511211}, doi = {10.1145/3511211}, abstract = {To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy inefficient. Emerging non-volatile memory (NVM) technologies offer high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages and executed on top of a managed runtime that already performs various dimensions of memory management. Supporting hybrid physical memories adds a new dimension, creating unique challenges in data replacement. This article proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division information is accurate enough to guide the GC for data layout, which hardly incurs overhead in data monitoring and moving. We implemented Panthera in OpenJDK and Apache Spark. Based on Big Data applications\u2019 memory access pattern, we also implemented a new profiling-guided optimization strategy, which is transparent to applications. With this optimization, our extensive evaluation demonstrates that Panthera reduces energy by 32\u201353% at less than 1% time overhead on average. To show Panthera\u2019s applicability, we extend it to QuickCached, a pure Java implementation of Memcached. Our evaluation results show that Panthera reduces energy by 28.7% at 5.2% time overhead on average.}, pages = {1\u201338}, numpages = {38}, keywords = {garbage collection, Hybrid memories, Big Data systems, memory management}}
@inproceedings{10.1145/3093241.3093265,title = {Social Login and Data Storage in the Big Data File System HDFS}, author = {Madani Youness , Bengourram Jemaa , Erritali Mohammed },year = {2017}, isbn = {9781450352413}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3093241.3093265}, doi = {10.1145/3093241.3093265}, abstract = {Studies have shown that the registration forms on Websites are ineffective because many people give false data, forget their login information to the site or just refuse to register, to overcome these problems a new type of authentication is born is the social authentication or social login which is a type of SSO(Single Sign-On),due to this type of authentication enrollment increases to a platform because the user registered to the platform with a simple click of a button authentication without passing by the step of filling a form, choose a username and a secure password. In this article, we will study the social authentication how it works, and how after the authorization of the user we can Retrieve personal data to complete registration, we can also use its social authorization on our facebook application to register its data on HDFS in a Big data system to analyze them and personalize its member space in the platform, using the Hadoop framework based on the MapReduce programming.}, location = {Lakeland, FL, USA}, series = {ICCDA '17}, pages = {91\u201397}, numpages = {7}, keywords = {authorization, MapReduce, social authentication, SSO(Single Sign-On), Hadoop, Big Data, HDFS, Authentication}}
@inproceedings{10.1145/2786752,title = {Phonetic analytics technology and big data: real-world cases}, author = {Shim J. P. , Koh J. , Fister S. , Seo H. Y. },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2786752}, doi = {10.1145/2786752}, abstract = {Data from phone interactions can help address customers' complaints, and predict their future purchasing behavior.}, pages = {84\u201390}, numpages = {7}}
@inproceedings{10.1145/2939672.2939859,title = {Parallel Lasso Screening for Big Data Optimization}, author = {Li Qingyang , Qiu Shuang , Ji Shuiwang , Thompson Paul M. , Ye Jieping , Wang Jie },year = {2016}, isbn = {9781450342322}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2939672.2939859}, doi = {10.1145/2939672.2939859}, abstract = {Lasso regression is a widely used technique in data mining for model selection and feature extraction. In many applications, it remains challenging to apply the regression model to large-scale problems that have massive data samples with high-dimensional features. One popular and promising strategy is to solve the Lasso problem in parallel. Parallel solvers run multiple cores in parallel on a shared memory system to speedup the computation, while the practical usage is limited by the huge dimension in the feature space. Screening is a promising method to solve the problem of high dimensionality by discarding the inactive features and removing them from optimization. However, when integrating screening methods with parallel solvers, most of solvers cannot guarantee the convergence on the reduced feature matrix. In this paper, we propose a novel parallel framework by parallelizing screening methods and integrating it with our proposed parallel solver. We propose two parallel screening algorithms: Parallel Strong Rule (PSR) and Parallel Dual Polytope Projection (PDPP). For the parallel solver, we proposed an Asynchronous Grouped Coordinate Descent method (AGCD) to optimize the regression problem in parallel on the reduced feature matrix. AGCD is based on a grouped selection strategy to select the coordinate that has the maximum descent for the objective function in a group of candidates. Empirical studies on the real-world datasets demonstrate that the proposed parallel framework has a superior performance compared to the state-of-the-art parallel solvers.}, location = {San Francisco, California, USA}, series = {KDD '16}, pages = {1705\u20131714}, numpages = {10}, keywords = {screening rules, coordinate descent, parallel computing, aynchronized coordinate descent, lasso regression}}
@inproceedings{10.1145/3383455.3422529,title = {Quantifying ESG alpha using scholar big data: an automated machine learning approach}, author = {Chen Qian , Liu Xiao-Yang },year = {2020}, isbn = {9781450375849}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3383455.3422529}, doi = {10.1145/3383455.3422529}, abstract = {ESG (Environmental, social and governance) alpha strategy that makes sustainable investment has gained popularity among investors. The ESG fields of study in scholar big data is a valuable alternative data that reflects a company's long-term ESG commitment. However, it is considered a difficulty to quantitatively measure a company's ESG premium and its impact to the company's stock price using scholar big data. In this paper, we utilize ESG scholar data as alternative data to develop an automatic trading strategy and propose a practical machine learning approach to quantify the ESG premium of a company and capture the ESG alpha. First, we construct our ESG investment universe and apply feature engineering on the companies' ESG scholar data from the Microsoft Academic Graph database. Then, we train six complementary machine learning models using a combination of financial indicators and ESG scholar data features and employ an ensemble method to predict stock prices and automatically set up portfolio allocation. Finally, we manage our portfolio, trade and rebalance the portfolio allocation monthly using predicted stock prices. We backtest our ESG alpha strategy and compare its performance with benchmarks. The proposed ESG alpha strategy achieves a cumulative return of 2,154.4% during the backtesting period of ten years, which significantly outperforms the NASDAQ-100 index's 397.4% and S&P 500's 226.9%. The traditional financial indicators results in only 1,443.7%, thus our scholar data-based ESG alpha strategy is better at capturing ESG premium than traditional financial indicators.}, location = {New York, New York}, series = {ICAIF '20}, pages = {1\u20138}, numpages = {8}, keywords = {AI in finance, scholar data, quantitative investment, ESG alpha, alternative data}}
@inproceedings{10.1145/2983323.2983841,title = {Scalable Local-Recoding Anonymization using Locality Sensitive Hashing for Big Data Privacy Preservation}, author = {Zhang Xuyun , Leckie Christopher , Dou Wanchun , Chen Jinjun , Kotagiri Ramamohanarao , Salcic Zoran },year = {2016}, isbn = {9781450340731}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2983323.2983841}, doi = {10.1145/2983323.2983841}, abstract = {While cloud computing has become an attractive platform for supporting data intensive applications, a major obstacle to the adoption of cloud computing in sectors such as health and defense is the privacy risk associated with releasing datasets to third-parties in the cloud for analysis. A widely-adopted technique for data privacy preservation is to anonymize data via local recoding. However, most existing local-recoding techniques are either serial or distributed without directly optimizing scalability, thus rendering them unsuitable for big data applications. In this paper, we propose a highly scalable approach to local-recoding anonymization in cloud computing, based on Locality Sensitive Hashing (LSH). Specifically, a novel semantic distance metric is presented for use with LSH to measure the similarity between two data records. Then, LSH with the MinHash function family can be employed to divide datasets into multiple partitions for use with MapReduce to parallelize computation while preserving similarity. By using our efficient LSH-based scheme, we can anonymize each partition through the use of a recursive agglomerative $k$-member clustering algorithm. Extensive experiments on real-life datasets show that our approach significantly improves the scalability and time-efficiency of local-recoding anonymization by orders of magnitude over existing approaches.}, location = {Indianapolis, Indiana, USA}, series = {CIKM '16}, pages = {1793\u20131802}, numpages = {10}, keywords = {mapreduce, privacy preservation, big data, cloud, LSH}}
@inproceedings{10.1145/3194452.3194458,title = {Probabilistic Time Context Framework for Big Data Collaborative Recommendation}, author = {Aboagye Emelia Opoku , James Gee C. , Jianbin Gao , Kumar Rajesh , Khan Riaz Ullah },year = {2018}, isbn = {9781450364195}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3194452.3194458}, doi = {10.1145/3194452.3194458}, abstract = {A parallel scheme based on Probabilistic Tensor Factorization which addresses the scalability problem of Collaborative Filtering (CF) is proposed for big data processing. Parallel algorithms for large scale recommendation problems have witnessed advancements in the big data era in recent times. Matrix Factorization models have been enormously used to tackle such constraints, which we see as not scalable and does not converge easily unless numerous iterations making it computationally expensive. This study proposes a novel coordinate descent based probabilistic Tensor factorization method; Scalable Probabilistic Time Context Tensor Factorization (SPTTF) for collaborative recommendation. Our experiments with natural datasets show its efficiency.}, location = {Chengdu, China}, series = {ICCAI 2018}, pages = {118\u2013121}, numpages = {4}, keywords = {tensor, algorithm integration, SPTTF, Time contest}}
@inproceedings{10.1145/3474880.3474882,title = {Application of Big Data Marketing in Customer Relationship Management}, author = {Yin Pengzhi , Huang Hao , Zhao Mengxuan , Zhu Ying },year = {2021}, isbn = {9781450389600}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3474880.3474882}, doi = {10.1145/3474880.3474882}, abstract = {Policy decisions and marketing models have a great impact on the specific application of customer relationship management. In various fields of marketing, big data marketing is gradually popularized as a combined product. The so-called big data marketing refers to collecting a large amount of behavior data through the Internet, primarily helping advertisers find out the target audience to analyze the content, time and form of advertising and finally complete the marketing process of advertising. The big data marketing can also be used for effective customer relationship management, which not only effectively enhance customer stickiness and reduce enterprise operating costs, but also has great significance for deeply mining the potential value of existing customers, predicting the future demand trend of customers, discovering new market growth points and developing new customer groups. Taking the large FMCG company A as the research subject, its sales is grim and stagnant: the purchase quantity of products is low; the price is lower than the cost; simple reproduction is difficult to continue. In order to change the current situation of company A, first of all, we cooperate with the marketing team to determine the business needs, using SQL and python to conduct exploratory data analysis (EDA) on 110k + e-commerce transaction data, and generate Data-Driven Insights on purchasing behavior and product sales; secondly, we use Python to establish RFM model (recent degree, frequency, currency), classify customers and calculate. Finally, according to the above customer analysis, a feasible listing strategy is proposed, which is expected to increase the retention rate by 3.5% and the monthly sales by 6%. Through big data marketing calculation, we come to a solution for the current situation of company A, which will have a good influence on many other companies.}, location = {Beijing, China}, series = {ICEBT '21}, pages = {1\u20130}}
@inproceedings{10.1145/3098593.3098597,title = {o'zapft is: Tap Your Network Algorithm's Big Data!}, author = {Blenk Andreas , Kalmbach Patrick , Kellerer Wolfgang , Schmid Stefan },year = {2017}, isbn = {9781450350549}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3098593.3098597}, doi = {10.1145/3098593.3098597}, abstract = {At the heart of many computer network planning, deployment, and operational tasks lie hard algorithmic problems. Accordingly, over the last decades, we have witnessed a continuous pursuit for ever more accurate and faster algorithms. We propose an approach to design network algorithms which is radically different from most existing algorithms. Our approach is motivated by the observation that most existing algorithms to solve a given hard computer networking problem overlook a simple yet very powerful optimization opportunity in practice: many network algorithms are executed repeatedly (e.g., for each virtual network request or in reaction to user mobility), and hence with each execution, generate interesting data: (problem,solution)-pairs. We make the case for leveraging the potentially big data of an algorithm's past executions to improve and speed up future, similar solutions, by reducing the algorithm's search space. We study the applicability of machine learning to network algorithm design, identify challenges and discuss limitations. We empirically demonstrate the potential of machine learning network algorithms in two case studies, namely the embedding of virtual networks (a packing optimization problem) and k-center facility location (a covering optimization problem), using a prototype implementation.}, location = {Los Angeles, CA, USA}, series = {Big-DAMA '17}, pages = {19\u201324}, numpages = {6}, keywords = {Machine Learning, Computer Networks, Algorithms, Big Data}}
@inproceedings{10.1145/3344948.3344986,title = {Measuring performance quality scenarios in big data analytics applications: a DevOps and domain-specific model approach}, author = {Castellanos Camilo , Varela Carlos A. , Correal Dario },year = {2019}, isbn = {9781450371421}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3344948.3344986}, doi = {10.1145/3344948.3344986}, abstract = {Big data analytics (BDA) applications use advanced analysis algorithms to extract valuable insights from large, fast, and heterogeneous data sources. These complex BDA applications require software design, development, and deployment strategies to deal with volume, velocity, and variety (3vs) while sustaining expected performance levels. BDA software complexity frequently leads to delayed deployments, longer development cycles and challenging performance monitoring. This paper proposes a DevOps and Domain Specific Model (DSM) approach to design, deploy, and monitor performance Quality Scenarios (QS) in BDA applications. This approach uses high-level abstractions to describe deployment strategies and QS enabling performance monitoring. Our experimentation compares the effort of development, deployment and QS monitoring of BDA applications with two use cases of near mid-air collisions (NMAC) detection. The use cases include different performance QS, processing models, and deployment strategies. Our results show shorter (re)deployment cycles and the fulfillment of latency and deadline QS for micro-batch and batch processing.}, location = {Paris, France}, series = {ECSA '19}, pages = {165\u2013172}, numpages = {8}, keywords = {big data analytics, software architecture, domain specific model, DevOps, performance quality scenarios}}
@inproceedings{10.1145/3093338.3104155,title = {Dynamically Creating Custom SDN High-Speed Network Paths for Big Data Science Flows}, author = {Rivera Sergio , Hayashida Mami , Griffioen James , Fei Zongming },year = {2017}, isbn = {9781450352727}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3093338.3104155}, doi = {10.1145/3093338.3104155}, abstract = {Existing campus network infrastructure is not designed to effectively handle the transmission of big data sets. Performance degradation in these networks is often caused by middleboxes -- appliances that enforce campus-wide policies by deeply inspecting all traffic going through the network (including big data transmissions). We are developing a Software-Defined Networking (SDN) solution for our campus network that grants privilege to science flows by dynamically calculating routes that bypass certain middleboxes to avoid the bottlenecks they create. Using the global network information provided by an SDN controller, we are developing graph databases approaches to compute custom paths that not only bypass middleboxes to achieve certain requirements (e.g., latency, bandwidth, hop-count) but also insert rules that modify packets hop-by-hop to create the illusion of standard routing/forward despite the fact that packets are being rerouted. In some cases, additional functionality needs to be added to the path using network function virtualization (NFV) techniques (e.g., NAT). To ensure that path computations are run on an up-to-date snapshot of the topology, we introduce a versioning mechanism that allows for lazy topology updates that occur only when \"important\" network changes take place and are requested by big data flows.}, location = {New Orleans, LA, USA}, series = {PEARC17}, pages = {1\u20134}, numpages = {4}, keywords = {Software-Defined Networks, Big Data Flows, Path Calculation}}
@inproceedings{10.1145/2818869.2818881,title = {Large LDPC Codes for Big Data Storage}, author = {Yongmei Wei , Fengmin Chen , Cher Lim Khai },year = {2015}, isbn = {9781450337359}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2818869.2818881}, doi = {10.1145/2818869.2818881}, abstract = {Current distributed storage systems mainly rely on data replication to ensure certain level of data availability and reliability. But in scenarios, like data archiving, replication is not cost effective and does not provides a robust solution to prevent data loss. A recent trend is to introduce erasure codes into the distributed storage. Inspired by the RAID system, early attempts have been focused on designing Reed-Solomon (R-S) based solutions with small block sizes. This paper investigates in details about repair traffic to apply Low Density Parity Check (LDPC) codes with relatively large block sizes. It has been demonstrated that the LDPC codes have unique advantages over R-S based solutions including low repair traffic for multiple erasures and parity erasures. The LDPC-based method is integrated with the Hadoop system with various configurations. Both theoretical analysis and simulations show that significant improvement in reliability can be achieved through using large LDPC codes without increasing the repair latency and network traffic especially for multiple erasures. Simulations also show great improvement in terms repairing latencies compared with Reed-Solomon codes. The latency is further improved through parallelism by engaging map-reduce processes from Hadoop.}, location = {Kaohsiung, Taiwan}, series = {ASE BD&amp;SI '15}, pages = {1\u20136}, numpages = {6}, keywords = {Reed-Solomon codes, Erasure codes, Reliability, LDPC codes, Distributed storage, Redundancy}}
@inproceedings{10.1145/2641225,title = {Weathering a new era of big data}, author = {Greengard Samuel },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2641225}, doi = {10.1145/2641225}, abstract = {Increased computing power combined with new and more advanced models are changing weather forecasting.}, pages = {12\u201314}, numpages = {3}}
@inproceedings{10.14778/2733004.2733071,title = {Enterprise search in the big data era: recent developments and open challenges}, author = {Li Yunyao , Liu Ziyang , Zhu Huaiyu },year = {2014}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2733004.2733071}, doi = {10.14778/2733004.2733071}, abstract = {Enterprise search allows users in an enterprise to retrieve desired information through a simple search interface. It is widely viewed as an important productivity tool within an enterprise. While Internet search engines have been highly successful, enterprise search remains notoriously challenging due to a variety of unique challenges, and is being made more so by the increasing heterogeneity and volume of enterprise data. On the other hand, enterprise search also presents opportunities to succeed in ways beyond current Internet search capabilities. This tutorial presents an organized overview of these challenges and opportunities, and reviews the state-of-the-art techniques for building a reliable and high quality enterprise search engine, in the context of the rise of big data.}, pages = {1717\u20131718}, numpages = {2}}
@inproceedings{10.1145/3380688.3380705,title = {Crop Knowledge Discovery Based on Agricultural Big Data Integration}, author = {Ngo Vuong M. , Kechadi M-Tahar },year = {2020}, isbn = {9781450376310}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3380688.3380705}, doi = {10.1145/3380688.3380705}, abstract = {Nowadays, the agricultural data can be generated through various sources, such as: Internet of Thing (IoT), sensors, satellites, weather stations, robots, farm equipment, agricultural laboratories, farmers, government agencies and agribusinesses. The analysis of this big data enables farmers, companies and agronomists to extract high business and scientific knowledge, improving their operational processes and product quality. However, before analysing this data, different data sources need to be normalised, homogenised and integrated into a unified data representation. In this paper, we propose an agricultural data integration method using a constellation schema which is designed to be flexible enough to incorporate other datasets and big data models. We also apply some methods to extract knowledge with the view to improve crop yield; these include finding suitable quantities of soil properties, herbicides and insecticides for both increasing crop yield and protecting the environment.}, location = {Haiphong City, Viet Nam}, series = {ICMLSC 2020}, pages = {46\u201350}, numpages = {5}, keywords = {Decision support, insecticides, herbicides, crop yield, soil properties}}
@inproceedings{10.1145/3468264.3468532,title = {TaintStream: fine-grained taint tracking for big data platforms through dynamic code translation}, author = {Yang Chengxu , Li Yuanchun , Xu Mengwei , Chen Zhenpeng , Liu Yunxin , Huang Gang , Liu Xuanzhe },year = {2021}, isbn = {9781450385626}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3468264.3468532}, doi = {10.1145/3468264.3468532}, abstract = {Big data has become valuable property for enterprises and enabled various intelligent applications. Today, it is common to host data in big data platforms (e.g., Spark), where developers can submit scripts to process the original and intermediate data tables. Meanwhile, it is highly desirable to manage the data to comply with various privacy requirements. To enable flexible and automated privacy policy enforcement, we propose TaintStream, a fine-grained taint tracking framework for Spark-like big data platforms. TaintStream works by automatically injecting taint tracking logic into the data processing scripts, and the injected scripts are dynamically translated to maintain a taint tag for each cell during execution. The dynamic translation rules are carefully designed to guarantee non-interference in the original data operation. By defining different semantics of taint tags, TaintStream can enable various data management applications such as access control, data retention, and user data erasure. Our experiments on a self-crafted benchmarksuite show that TaintStream is able to achieve accurate cell-level taint tracking with a precision of 93.0% and less than 15% overhead. We also demonstrate the usefulness of TaintStream through several real-world use cases of privacy policy enforcement.}, location = {Athens, Greece}, series = {ESEC/FSE 2021}, pages = {806\u2013817}, numpages = {12}, keywords = {big data platform, privacy compliance, Taint tracking, GDPR}}
@inproceedings{10.1145/3407703.3407711,title = {Correlation analysis of factors affecting bridge health under the background of big data}, author = {Wenxia Ding , Heping Li },year = {2020}, isbn = {9781450377270}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3407703.3407711}, doi = {10.1145/3407703.3407711}, abstract = {Once the bridge is put into use, in addition to its own material aging, it will also receive damage from human factors such as vehicles, wind, earthquakes, fatigue, and overload. This damage has more or less reduced the service life of the bridge or even destroyed the safety performance of the bridge, causing huge losses to people's lives and property. This requires us to pay attention to the safety, reliability and durability of the bridge at all times during the construction of the bridge and in the later maintenance process. The traditional bridge monitoring work and operation and maintenance have a low degree of automation, bridge condition assessment, and bridge real-time monitoring and comprehensive information management is difficult. This requires real-time monitoring of this information in the context of big data to ensure the healthy use of the bridge. In this paper, a finite element analysis model is established to monitor the sensor network of the bridge and the data detected by the sensor in the context of big data. The optimization analysis results in an optimized layout plan of the identifiable static sensors, taking into account both the economic and structural operating conditions of the bridge.}, location = {Wuhan, China}, series = {AICSconf '20}, pages = {34\u201338}, numpages = {5}, keywords = {finite element analysis, reliability, bridge health, real-time monitoring, condition evaluation, Big data}}
@inproceedings{10.1145/3377672.3378054,title = {Design of Resource Recommendation Model for Personalized Learning in the Era of Big Data}, author = {Liqiang Hao , Quan Liu },year = {2019}, isbn = {9781450362481}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377672.3378054}, doi = {10.1145/3377672.3378054}, abstract = {This paper proposes a personalized learning resource recommendation model based on big data. The design of the model consists of data storage, data analysis, resource matching, and the resource recommendation. In order to provide a suitable resource, data analysis is a more critical procedure that involves the analyses of basic information, learning style, learning status, learning behavior, and learning interest, which can be successfully analyzed by means of kafka and flume. Through an experiment, it shows that personalized resource recommendation platform really plays a positive role in improving students learning.}, location = {Kuala Lumpur, Malaysia}, series = {AMME 2019}, pages = {181\u2013187}, numpages = {7}, keywords = {big data, recommendation model, personalized learning}}
@inproceedings{10.5555/2857070.2857072,title = {Library assessment and data analytics in the big data era: practice and policies}, author = {Chen Hsin-liang , Doty Philip , Mollman Carol , Niu Xi , Yu Jen-chien , Zhang Tao },year = {2015}, isbn = {087715547X}, publisher = {American Society for Information Science}, address = {USA}, abstract = {Emerging technologies have offered libraries and librarians new ways and methods to collect and analyze data in the era of accountability to justify their value and contributions. For example, Gallagher, Bauer and Dollar (2005) analyzed the paper and online journal usage from all possible data sources and discovered that users at the Yale Medical Library preferred the electronic format of articles to the print version. After this discovery, they were able to take necessary steps to adjust their journal subscriptions. Many library professionals advocate such data-driven library management to strengthen and specify library budget proposals.}, location = {St. Louis, Missouri}, series = {ASIST '15}, pages = {1\u20134}, numpages = {4}, keywords = {information privacy, big data, library assessment, information policy, data analytics}}
@inproceedings{10.1145/3394486.3406477,title = {Overview and Importance of Data Quality for Machine Learning Tasks}, author = {Jain Abhinav , Patel Hima , Nagalapatti Lokesh , Gupta Nitin , Mehta Sameep , Guttula Shanmukha , Mujumdar Shashank , Afzal Shazia , Sharma Mittal Ruhi , Munigala Vitobha },year = {2020}, isbn = {9781450379984}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3394486.3406477}, doi = {10.1145/3394486.3406477}, abstract = {It is well understood from literature that the performance of a machine learning (ML) model is upper bounded by the quality of the data. While researchers and practitioners have focused on improving the quality of models (such as neural architecture search and automated feature selection), there are limited efforts towards improving the data quality. One of the crucial requirements before consuming datasets for any application is to understand the dataset at hand and failure to do so can result in inaccurate analytics and unreliable decisions. Assessing the quality of the data across intelligently designed metrics and developing corresponding transformation operations to address the quality gaps helps to reduce the effort of a data scientist for iterative debugging of the ML pipeline to improve model performance. This tutorial highlights the importance of analysing data quality in terms of its value for machine learning applications. This tutorial surveys all the important data quality related approaches discussed in literature, focusing on the intuition behind them, highlighting their strengths and similarities, and illustrates their applicability to real-world problems. Finally we will discuss the interesting work IBM Research is doing in this space.}, location = {Virtual Event, CA, USA}, series = {KDD '20}, pages = {3561\u20133562}, numpages = {2}, keywords = {quality metrics, data quality, machine learning}}
@inproceedings{10.1145/3178442.3178447,title = {Supporting Fine-grained Dataflow Parallelism in Big Data Systems}, author = {Ertel Sebastian , Adam Justus , Castrillon Jeronimo },year = {2018}, isbn = {9781450356459}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3178442.3178447}, doi = {10.1145/3178442.3178447}, abstract = {Big data systems scale with the number of cores in a cluster for the parts of an application that can be executed in data parallel fashion. It has been recently reported, however, that these systems fail to translate hardware improvements, such as increased network bandwidth, into a higher throughput. This is particularly the case for applications that have inherent sequential, computationally intensive phases. In this paper, we analyze the data processing cores of state-of-the-art big data systems to find the cause for these scalability problems. We identify design patterns in the code that are suitable for pipeline and task-level parallelism, potentially increasing application performance. As a proof of concept, we rewrite parts of the Hadoop MapReduce framework in an implicit parallel language that exploits this parallelism without adding code complexity. Our experiments on a data analytics workload show throughput speedups of up to 3.5x.}, location = {Vienna, Austria}, series = {PMAM'18}, pages = {41\u201350}, numpages = {10}}
@inproceedings{10.1145/3537693.3537710,title = {A method for Vietnamese Hotel Online Rating based on Big Data Analysis: Vietnames Hotel Rating based on Big Data analysis}, author = {Nguyen Thi Thu Ha , Nguyen Binh Giang , Trung Nguyen Xuan , Ho Ngoc Vinh },year = {2022}, isbn = {9781450396523}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3537693.3537710}, doi = {10.1145/3537693.3537710}, abstract = {The rapid growth of online booking websites has created a new trend in hotel star rating based on customer reviews. Therefore, there is a discrepancy between hotel ratings by traveler on the Internet and hotel ratings according to national standards, especially for 4\u20135 stars hotels. In recent years, a number of hotel rating organizations on the world have incorporated Internet star rating standards to update their hotel star rating standards. In Vietnam, the hotel star rating standards have been updated since 2015 and have not yet approached online hotel star ratings. In this study, a new hotel rating method is proposed using Internet traveler reviews for rating. Data was collected from TripAdvisor about hotels in Vietnam from 4-5 stars of 5 major cities. Deep neural network model is used to classify hotels from 3 to 5 stars. The results shown that, the deviation between online rating and actual star rating is 0.6. This is also a suggestion for hotel managers to understand about their customers and improve the quality of their hotels to match the common standards of many different customers around the world.}, location = {Plymouth, United Kingdom}, series = {ICEEG '22}, pages = {108\u2013114}, numpages = {7}, keywords = {Vietnamese hotel, overall rating, online review, neural network, hotel management, data analysis}}
@inproceedings{10.1145/3528416.3530868,title = {Anomaly detection to improve security of big data analytics}, author = {Slooff Tom , Regazzoni Francesco , Brocheton Fabien , Parodi Antonio , Cmar Radim },year = {2022}, isbn = {9781450393386}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3528416.3530868}, doi = {10.1145/3528416.3530868}, abstract = {Big data analytics largely rely on data. Because of their central role, it is fundamental to ensure the security and correctness of data used in these applications. Anomaly detection could help to increase the security of big data analytics applications. However, these applications are very diverse both for the properties of the data analyzed and for the computations to be carried out on them. As a result, the selection of the most appropriate anomaly detection method is a challenging and time consuming task for designers. Hierarchical Temporal Memory (HTM) is as an anomaly detection technique sufficiently generic to achieve satisfactory performance on a wide range of applications, thus suitable to ease the burden of selecting the anomaly detection method. To confirm this, in this paper we explore the performance of HTM on a dataset used for air quality prediction. Our preliminary results show that HTM achieves excellent performance when compared to other popular anomaly detection methods.}, location = {Turin, Italy}, series = {CF '22}, pages = {205\u2013206}, numpages = {2}}
@inproceedings{10.1145/3404512.3404527,title = {Gene Big Data Analysis of Differentially Expressed lncRNA and MiRNA in Liver Cancer with Different Gender}, author = {Deng Jianzhi , Zhou Yuehan , Tang Weixian },year = {2020}, isbn = {9781450377225}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3404512.3404527}, doi = {10.1145/3404512.3404527}, abstract = {In this paper, we try to find the differentially expressed lncRNA and differentially expressed miRNA of Liver cancer, especially between the different gender's patients. The differentially expressed genes were screened from TCGA liver data. Based on the extracted differentially expressed lncRNAs and differentially expressed miRNAs, we reveal an 8-lncRNA (TTTY14, UCA1, LINC00162, TTTY10, XIST, ERVH48-1, ZFY-AS1 and TTTY15) to 3-miRNA (hsa-mir-506, hsa-mir-508, hsa-mir-205) regulatory network of 13 pairs inter-regulatory between male and female. The 8 differentially expressed lncRNAs in the lncRNA-miRNA regulatory network were analyzed by the multivariable COX regression model, and LINC00162 and TTTY10 were found as the co-expression differentially expressed lncRNAs. After survival kmplot analysis and receiver operating characteristic analysis of the co-expression differentially expressed lncRNAs, TTTY10 was selected and proved as the potential biomarker of liver cancer for the diagnose and therapy.}, location = {Shanghai, China}, series = {BDE 2020}, pages = {24\u201328}, numpages = {5}, keywords = {regulatory network, gender, TCGA, liver cancer, COX model, TTTY10}}
@inproceedings{10.1145/3219104.3229276,title = {Navigating the Unexpected Realities of Big Data Transfers in a Cloud-based World}, author = {Rivera Sergio , Griffioen James , Fei Zongming , Hayashida Mami , Shi Pinyi , Chitre Bhushan , Chappell Jacob , Song Yongwook , Pike Lowell , Carpenter Charles , Nasir Hussamuddin },year = {2018}, isbn = {9781450364461}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3219104.3229276}, doi = {10.1145/3219104.3229276}, abstract = {The emergence of big data has created new challenges for researchers transmitting big data sets across campus networks to local (HPC) cloud resources, or over wide area networks to public cloud services. Unlike conventional HPC systems where the network is carefully architected (e.g., a high speed local interconnect, or a wide area connection between Data Transfer Nodes), today's big data communication often occurs over shared network infrastructures with many external and uncontrolled factors influencing performance.This paper describes our efforts to understand and characterize the performance of various big data transfer tools such as rclone, cyberduck, and other provider-specific CLI tools when moving data to/from public and private cloud resources. We analyze the various parameter settings available on each of these tools and their impact on performance. Our experimental results give insights into the performance of cloud providers and transfer tools, and provide guidance for parameter settings when using cloud transfer tools. We also explore performance when coming from HPC DTN nodes as well as researcher machines located deep in the campus network, and show that emerging SDN approaches such as the VIP Lanes system can deliver excellent performance even from researchers' machines.}, location = {Pittsburgh, PA, USA}, series = {PEARC '18}, pages = {1\u20138}, numpages = {8}, keywords = {Software-Defined Networks, Data Transfer Tools, Big Data Flows}}
@inproceedings{10.1145/3206157.3206164,title = {A Method for Big Data Analysis of the impact of Economic and Social Events on Japanese Stock Prices}, author = {Kyo Koki },year = {2018}, isbn = {9781450363587}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3206157.3206164}, doi = {10.1145/3206157.3206164}, abstract = {In this paper, we propose an approach for isolating the effects of economic and social events on stock prices. Using a newly-proposed Bayesian modeling technique, we decompose the daily time series of stock price data into three components: a trend component, a cyclical component, and an irregular component. We can then analyze the behavior of each estimated component in relation to economic and social events. As an empirical example, we analyze the daily time series for closing values of the Nikkei Stock Average (NSA) from January 4, 2000 to November 28, 2017, and examine relationships between the estimated components of NSA and significant events together with variations in the economic and social.}, location = {Honolulu, HI, USA}, series = {ICBDE '18}, pages = {5\u201310}, numpages = {6}, keywords = {Bayesian modeling, Nikkei Stock Average, daily stock price data, state space model}}
@inproceedings{10.1145/3183519.3183528,title = {Cross-language optimizations in big data systems: a case study of SCOPE}, author = {Selakovic Marija , Barnett Michael , Musuvathi Madan , Mytkowicz Todd },year = {2018}, isbn = {9781450356596}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3183519.3183528}, doi = {10.1145/3183519.3183528}, abstract = {Building scalable big data programs currently requires programmers to combine relational (SQL) with non-relational code (Java, C#, Scala). Relational code is declarative - a program describes what the computation is and the compiler decides how to distribute the program. SQL query optimization has enjoyed a rich and fruitful history, however, most research and commercial optimization engines treat non-relational code as a black-box and thus are unable to optimize it.This paper empirically studies over 3 million SCOPE programs across five data centers within Microsoft and finds programs with non-relational code take between 45-70% of data center CPU time. We further explore the potential for SCOPE optimization by generating more native code from the non-relational part. Finally, we present 6 case studies showing that triggering more generation of native code in these jobs yields significant performance improvement: optimizing just one portion resulted in as much as 25% improvement for an entire program.}, location = {Gothenburg, Sweden}, series = {ICSE-SEIP '18}, pages = {45\u201354}, numpages = {10}}
@inproceedings{10.1145/3349341.3349431,title = {A User Profile Analysis Framework Driven by Distributed Machine Learning for Big Data}, author = {Wang Xiaodong , Wang Qing , Tao Ye },year = {2019}, isbn = {9781450371506}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3349341.3349431}, doi = {10.1145/3349341.3349431}, abstract = {In recent years, big data has become the new focus of attention from all walks of life. The valuable information contained in big data becomes the driving force for people to process and analyze big data. Big data analytics helps enterprises to take better decisions to improve business output. As a user description tool, user profile is widely used in various fields. However, it is difficult to deal with large-scale datasets using traditional methods since the established processes was not designed to handle large volumes of data. In this paper, we propose a user profile analysis framework using machine learning approach which apply advanced machine learning programs to solve industrial scale problems. And this approach can be effective to speculate real and potential needs of various groups of users and precisely extract individual characteristics and group generality. By introducing high-level data parallel framework, the process of large-scale data processing can be executed efficiently. We use real-world data to validate the effectiveness of the proposed framework.}, location = {Wuhan, Hubei, China}, series = {AICS 2019}, pages = {358\u2013363}, numpages = {6}, keywords = {Distributed Machine Learning, Computing Framework, Big Data Analysis, User Profile}}
@inproceedings{10.1145/2808797.2809372,title = {Research on the Shanghai Cooperation Organization Network Architecture from the Big Data Perspective}, author = {Wang Kun , Sun Duoyong },year = {2015}, isbn = {9781450338547}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2808797.2809372}, doi = {10.1145/2808797.2809372}, abstract = {The Shanghai Cooperation Organization (SCO) is playing an increasingly important role in many respects, such as the economic, political and security cooperation. Research results aiming at this regional organization have gained lots of attentions for a long time. The Social Network Analysis (SNA) is considered to be an effective method in the studies of international relation, especially from the Big Data perspective. Our research is mainly based on the economic and trade data among the member states of SCO in 2012. All the results have significantly shown that both Russia and China have occupied the central roles, no matter in the tables or in the figures. Due to the limitation of essential data, the importance of Russia has not been fully reflected. Based on the results, we suggests that China should pay more attention to the affairs of the region. And then the Chinese government may enhance her influence in the SCO.}, location = {Paris, France}, series = {ASONAM '15}, pages = {1208\u20131211}, numpages = {4}, keywords = {Shanghai Cooperation Organization, Networks Architecture, Social Networks Analysis, Big Data}}