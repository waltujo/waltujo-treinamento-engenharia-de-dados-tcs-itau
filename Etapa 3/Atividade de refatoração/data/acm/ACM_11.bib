@inproceedings{10.1145/3358505,title = {Proceedings of the 2019 3rd International Conference on Cloud and Big Data Computing},year = {2019}, isbn = {9781450371650}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {It is our greatest pleasure to welcome you to the 2019 3rd International Conference on Cloud and Big Data Computing (ICCBDC 2019) held at St Anne's College in University of Oxford, Oxford, United Kingdom. In this conference, you can expect to meet fellow researchers from both academia and industry, and to hear and discuss the latest technological advances in the field. In our previous runs of this conference series in London (2017) and Barcelona (2018), we have seen novel ideas and developments being shared and collaborations being established.}, location = {Oxford, United Kingdom}}
@inproceedings{10.1145/3006386.3006387,title = {Analytics on public transport delays with spatial big data}, author = {Raghothama Jayanth , Shreenath Vinutha Magal , Meijer Sebastiaan },year = {2016}, isbn = {9781450345811}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3006386.3006387}, doi = {10.1145/3006386.3006387}, abstract = {The increasing pervasiveness of location-aware technologies is leading to the rise of large, spatio-temporal datasets and to the opportunity of discovering usable knowledge about the behaviors of people and objects. Applied extensively in transportation, spatial big data and its analytics can deliver useful insights on a number of different issues such as congestion, delays, public transport reliability and so on. Predominantly studied for its use in operational management, spatial big data can be used to provide insight in strategic applications as well, from planning and design to evaluation and management. Such large scale, streaming spatial big data can be used in the improvement of public transport, for example the design of public transport networks and reliability. In this paper, we analyze GTFS data from the cities of Stockholm and Rome to gain insight on the sources and factors influencing public transport delays in the cities. The analysis is performed on a combination of GTFS data with data from other sources. The paper points to key issues in the analysis of real time data, driven by the contextual setting in the two cities.}, location = {Burlingame, California}, series = {BigSpatial '16}, pages = {28\u201333}, numpages = {6}, keywords = {public transport, big data, decision making}}
@inproceedings{10.1007/s00778-018-0515-8,title = {Adaptive correlation exploitation in big data query optimization}, author = {Liu Yuchen , Liu Hai , Xiao Dongqing , Eltabakh Mohamed Y. },year = {2018}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg}, url = {https://doi.org/10.1007/s00778-018-0515-8}, doi = {10.1007/s00778-018-0515-8}, abstract = {Correlations among the data attributes are abundant and inherent in most application domains. These correlations, if managed in systematic and efficient ways, would enable various optimization opportunities. Unfortunately, the state-of-art techniques are all heavily tailored toward optimizing factors intrinsic to relational databases, e.g., predicate selectivity, random I/O accesses, and secondary indexes, which are mostly not applicable to the modern big data infrastructures, e.g., Hadoop and Spark. In this paper, we propose the EXORD$$^+$$+ system for exploiting the data's correlations in big data query optimization. EXORD$$^+$$+ supports two types of correlations; hard (which does not allow for exceptions) and soft (which allows for exceptions). We introduce a three-phase approach for managing soft correlations including: (1) validating and judging the worthiness of soft correlations, (2) selecting and preparing the soft correlations for deployment, and (3) exploiting the correlations in query optimization. EXORD$$^+$$+ introduces a novel cost-benefit model for adaptively selecting the most beneficial soft correlations given a query workload. We show the complexity of this problem (NP-Hard) and propose a heuristic to efficiently solve it in a polynomial time. Moreover, we present incremental maintenance algorithms for efficiently updating the system's state under data appends and workload changes. EXORD$$^+$$+ prototype is implemented as an extension to the Hive engine on top of Hadoop. The experimental evaluation shows the potential of EXORD$$^+$$+ in achieving more than 10x speedup while introducing minimal storage overheads.}, pages = {873\u2013898}, numpages = {26}, keywords = {Query optimization, Incremental maintenance, Data correlations, Soft and hard correlations, Big data}}
@inproceedings{10.1145/2594538.2594551,title = {On scale independence for querying big data}, author = {Fan Wenfei , Geerts Floris , Libkin Leonid },year = {2014}, isbn = {9781450323758}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2594538.2594551}, doi = {10.1145/2594538.2594551}, abstract = {To make query answering feasible in big datasets, practitioners have been looking into the notion of scale independence of queries. Intuitively, such queries require only a relatively small subset of the data, whose size is determined by the query and access methods rather than the size of the dataset itself. This paper aims to formalize this notion and study its properties. We start by defining what it means to be scale-independent, and provide matching upper and lower bounds for checking scale independence, for queries in various languages, and for combined and data complexity. Since the complexity turns out to be rather high, and since scale-independent queries cannot be captured syntactically, we develop sufficient conditions for scale independence. We formulate them based on access schemas, which combine indexing and constraints together with bounds on the sizes of retrieved data sets. We then study two variations of scale-independent query answering, inspired by existing practical systems. One concerns incremental query answering: we check when query answers can be maintained in response to updates scale-independently. The other explores scale-independent query rewriting using views.}, location = {Snowbird, Utah, USA}, series = {PODS '14}, pages = {51\u201362}, numpages = {12}, keywords = {query answering, big data, scale independence}}
@inproceedings{10.1145/2464157.2466485,title = {A bloat-aware design for big data applications}, author = {Bu Yingyi , Borkar Vinayak , Xu Guoqing , Carey Michael J. },year = {2013}, isbn = {9781450321006}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2464157.2466485}, doi = {10.1145/2464157.2466485}, abstract = {Over the past decade, the increasing demands on data-driven business intelligence have led to the proliferation of large-scale, data-intensive applications that often have huge amounts of data (often at terabyte or petabyte scale) to process. An object-oriented programming language such as Java is often the developer's choice for implementing such applications, primarily due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, significant performance problems can often be seen --- the combination of the inefficiencies inherent in a managed run-time system and the impact of the huge amount of data to be processed in the limited memory space often leads to memory bloat and performance degradation at a surprisingly early stage.This paper proposes a bloat-aware design paradigm towards the development of efficient and scalable Big Data applications in object-oriented GC enabled languages. To motivate this work, we first perform a study on the impact of several typical memory bloat patterns. These patterns are summarized from the user complaints on the mailing lists of two widely-used open-source Big Data applications. Next, we discuss our design paradigm to eliminate bloat. Using examples and real-world experience, we demonstrate that programming under this paradigm does not incur significant programming burden. We have implemented a few common data processing tasks both using this design and using the conventional object-oriented design. Our experimental results show that this new design paradigm is extremely effective in improving performance --- even for the moderate-size data sets processed, we have observed 2.5x+ performance gains, and the improvement grows substantially with the size of the data set.}, location = {Seattle, Washington, USA}, series = {ISMM '13}, pages = {119\u2013130}, numpages = {12}, keywords = {memory bloat, big data applications, design}}
@inproceedings{10.1145/2820783.2820854,title = {Grooming big data from afar}, author = {Baumann Peter , Dumitru Alex Mircea , Merticariu Vlad },year = {2015}, isbn = {9781450339674}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2820783.2820854}, doi = {10.1145/2820783.2820854}, abstract = {Flexible, scalable services on massive geo data receive much attention today. In particular, the OGC Web Coverage Service (WCS) standards suite has established a best practice for versatile access and retrieval on spatio-temporal \"Big Data\". Fewer efforts have been devoted, though, to an easy-to-use, standardized way of maintaining a service's offering. Our experience from supporting a series of heterogeneous, large-scale services reveals that this can become tedious indeed, due to heterogeneity and incompleteness of incoming data, operator-less transformation and ingest of large amounts of files, as well as the need for narrowly focused manual corrections and updates sometimes.In this contribution, we present the WCS-T (for \"Transaction\") specification which enables users and machines to perform atomic insertion, updating, and deletions through simple Web requests. WCS-T has been implemented and is actively being used in projects; recently, it has entered the adoption process in OGC to become part of the WCS suite.}, location = {Seattle, Washington}, series = {SIGSPATIAL '15}, pages = {1\u20134}, numpages = {4}, keywords = {coverage, standards, OGC, geo services, datacube, web coverage service}}
@inproceedings{10.1145/3341620.3341636,title = {A Secure Cloud-Assisted Certificateless Group Authentication Scheme for VANETs in Big Data Environment}, author = {Tan Haowen , Chung Ilyong },year = {2019}, isbn = {9781450360913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341620.3341636}, doi = {10.1145/3341620.3341636}, abstract = {Nowadays, the construction of efficient intelligent transportation system (ITS) has become a new trend for metropolitan cities with increasingly large populations. As one of the most significant component of ITS, the vehicular ad hoc networks (VANETs) are capable of building temporary vehicular sensor networks for efficient and dynamic information exchange between vehicles and road side units (RSUs). As a matter of fact, the traditional VANETs have limited computing and storing capabilities, which restrict the rapid development VANETs services provided to the drivers. Hence, with the rapid development of big data facilities, the cloud-assisted VANETs structure is proposed in order to enhance the capabilities of VANETs. In addition, due to the inherent wireless communication characteristics, data transmissions of VANETs suffer from charted and uncharted security risks and attacks. Thus proper security strategies should be adopted to guarantee secure communication and driver privacy. Emphasizing on the above issues, we develop an efficient cloud-assisted certificateless grouping authentication scheme for VANETs. In our design, vehicle anonymity is provided during the entire communication process. Note that most of the current authenticating schemes assume the secure channel between the RSU and vehicles in order for initial key message transmission, which is not necessary in our scheme.}, location = {Hong Kong, Hong Kong}, series = {BDE 2019}, pages = {107\u2013113}, numpages = {7}, keywords = {conditional privacy, anonymous identity, group authentication, VANETs, certificateless}}
@inproceedings{10.1145/3378936.3378957,title = {Big Data and its Effect on the Music Industry}, author = {Hujran Omar , Alikaj Ahmad , Durrani Usman Khan , Al-Dmour Nidal },year = {2020}, isbn = {9781450376907}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3378936.3378957}, doi = {10.1145/3378936.3378957}, abstract = {This research discusses the effect of big data and Internet technologies on the music industry. Specifically, this paper addresses two research questions; (1) how do modern businesses in the music industry implement the use of Internet technologies and big data to ensure their success in the market, and (2) what are the advantages and drawbacks of implementing digital business models in the music industry? To answer the research questions, two real-life cases (i.e. Shazam and Spotify) were analyzed to show how modern businesses in the music industry implement big data and Internet technologies to ensure their success in the market. Furthermore, previous literature and secondary resources were used to explain the development of traditional business models into digital business models in the music industry. In addition to discussing the benefits and drawbacks of implementing the modern digital business model.}, location = {Sydney, NSW, Australia}, series = {ICSIM '20}, pages = {5\u20139}, numpages = {5}, keywords = {Big Data, Music, Business Analytics, Spotify, Shazam}}
@inproceedings{10.1145/2513190.2517828,title = {Data warehousing and OLAP over big data: current challenges and future research directions}, author = {Cuzzocrea Alfredo , Bellatreche Ladjel , Song Il-Yeol },year = {2013}, isbn = {9781450324120}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2513190.2517828}, doi = {10.1145/2513190.2517828}, abstract = {In this paper, we highlight open problems and actual research trends in the field of Data Warehousing and OLAP over Big Data, an emerging term in Data Warehousing and OLAP research. We also derive several novel research directions arising in this field, and put emphasis on possible contributions to be achieved by future research efforts.}, location = {San Francisco, California, USA}, series = {DOLAP '13}, pages = {67\u201370}, numpages = {4}, keywords = {big multidimensional data, olap, big data, data warehousing}}
@inproceedings{10.1145/2990473,title = {Real-Time Load Reduction in Multimedia Big Data for Mobile Internet}, author = {Wang Kun , Mi Jun , Xu Chenhan , Zhu Qingquan , Shu Lei , Deng Der-Jiunn },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2990473}, doi = {10.1145/2990473}, abstract = {In the age of multimedia big data, the popularity of mobile devices has been in an unprecedented growth, the speed of data increasing is faster than ever before, and Internet traffic is rapidly increasing, not only in volume but also in heterogeneity. Therefore, data processing and network overload have become two urgent problems. To address these problems, extensive papers have been published on image analysis using deep learning, but only a few works have exploited this approach for video analysis. In this article, a hybrid-stream model is proposed to solve these problems for video analysis. Functionality of this model covers Data Preprocessing, Data Classification, and Data-Load-Reduction Processing. Specifically, an improved Convolutional Neural Networks (CNN) classification algorithm is designed to evaluate the importance of each video frame and video clip to enhance classification precision. Then, a reliable keyframe extraction mechanism will recognize the importance of each frame or clip, and decide whether to abandon it automatically by a series of correlation operations. The model will reduce data load to a dynamic threshold changed by \u03c3, control the input size of the video in mobile Internet, and thus reduce network overload. Through experimental simulations, we find that the size of processed video has been effectively reduced and the quality of experience (QoE) has not been lowered due to a suitably selected parameter \u03b7. The simulation also shows that the model has a steady performance and is powerful enough for continuously growing multimedia big data.}, pages = {1\u201320}, numpages = {20}, keywords = {big data, real-time, load reduction, mobile Internet, Multimedia, networking}}
@inproceedings{10.1145/3183440.3190334,title = {Interactive and automated debugging for big data analytics}, author = {Gulzar Muhammad Ali },year = {2018}, isbn = {9781450356633}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3183440.3190334}, doi = {10.1145/3183440.3190334}, abstract = {An abundance of data in many disciplines of science, engineering, national security, health care, and business has led to the emerging field of Big Data Analytics that run in a cloud computing environment. To process massive quantities of data in the cloud, developers leverage Data-Intensive Scalable Computing (DISC) systems such as Google's MapReduce, Hadoop, and Spark.Currently, developers do not have easy means to debug DISC applications. The use of cloud computing makes application development feel more like batch jobs and the nature of debugging is therefore post-mortem. Developers of big data applications write code that implements a data processing pipeline and test it on their local workstation with a small sample data, downloaded from a TB-scale data warehouse. They cross fingers and hope that the program works in the expensive production cloud. When a job fails or they get a suspicious result, data scientists spend hours guessing at the source of the error, digging through post-mortem logs. In such cases, the data scientists may want to pinpoint the root cause of errors by investigating a subset of corresponding input records.The vision of my work is to provide interactive, real-time and automated debugging services for big data processing programs in modern DISC systems with minimum performance impact. My work investigates the following research questions in the context of big data analytics: (1) What are the necessary debugging primitives for interactive big data processing? (2) What scalable fault localization algorithms are needed to help the user to localize and characterize the root causes of errors? (3) How can we improve testing efficiency during iterative development of DISC applications by reasoning the semantics of dataflow operators and user-defined functions used inside dataflow operators in tandem?To answer these questions, we synthesize and innovate ideas from software engineering, big data systems, and program analysis, and coordinate innovations across the software stack from the user-facing API all the way down to the systems infrastructure.}, location = {Gothenburg, Sweden}, series = {ICSE '18}, pages = {509\u2013511}, numpages = {3}, keywords = {big data, debugging and testing, and data cleaning, fault localization, data provenance, automated debugging, data-intensive scalable computing (DISC), test minimization}}
@inproceedings{10.1145/3372938.3373003,title = {An approach for the implementation of semantic Big Data Analytics in the Social Business Intelligence process on distributed environments (Cloud computing)}, author = {Alcabnani Sara , Oubezza Mohamed , Elkafi Jamal },year = {2019}, isbn = {9781450372404}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3372938.3373003}, doi = {10.1145/3372938.3373003}, abstract = {Managing and extracting useful knowledge from social media sources is a challenge. It has attracted a lot of attention from universities and industry. To meet this challenge, semantic analysis of textual data is the subject matter.Today, with the connection present everywhere and at any time, considerable data is born. These data or data become a key player for understanding, analyzing, anticipating and solving major economic, political, social and scientific problems. Data also changes our working procedures, our cultural environment, even restructuring our way of thinking. And just as the scientific, managerial and financial world is interested in Big Data, a new discipline is growing: Fast Data. In addition to the salient volume of data; another variant becomes decisive, the ability to efficiently process data in all their diversity, transforming it into knowledge by providing the right information to the right person at the right time, or even using it to predict the future.The exploitation of Big Data requires the proposition of new adapted mathematical and IT approaches but also a reengineering of managerial approaches for the control of the informational environment of a public or private organization. While basing itself on a strategic information management approach such as Economic Intelligence (EI). The latter combines and encompasses Business Intelligence techniques for internal data management and business intelligence techniques for monitoring and controlling external information flows. However, Big Data, as a boundless source of information for EI, has upset the traditional EI process, which requires a reengineering of the EI approach. My research works perfectly in this context characterized by an uncertain and unpredictable environment.We ask to propose an ontology-based, service-oriented, agile and scalable Social Business Intelligence approach to extract the semantics of textual data and define the domain of massive data. In other words, we semantically analyze social data at two levels, namely the level of the entity and the level of the domain.}, location = {Rabat, Morocco}, series = {BDIoT'19}, pages = {1\u20136}, numpages = {6}, keywords = {Big Data, Social BI, Fast Data, Cloud, Ontology, Distributed Processing}}
@inproceedings{10.1145/3387168.3387220,title = {Integrated Retrieval System Based on Medical Big Data}, author = {Yu Song },year = {2019}, isbn = {9781450376259}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3387168.3387220}, doi = {10.1145/3387168.3387220}, abstract = {This paper presented an integrated retrieval system based on medical big data, aiming at the characteristics of medical data, such as diversity, large quantity and complex structure. The data center is built with data extraction module and data storage module in the system. The index module is built based on Solr and the data is presented in a webpage. The system keeps proper loose coupling among modules with high availability and extension. In a hospital application environment, this system realizes the function of real-time retrieval, and provides effective support for clinical decision.}, location = {Vancouver, BC, Canada}, series = {ICVISP 2019}, pages = {1\u20135}, numpages = {5}, keywords = {Solr, Medical Big Data, Data storage}}
@inproceedings{10.1145/3022227.3022232,title = {Big data analysis on secure VoIP services}, author = {Saad Amna , Amran Ahmad Roshidi , Phillips Iain William , Salagean Ana M. },year = {2017}, isbn = {9781450348881}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3022227.3022232}, doi = {10.1145/3022227.3022232}, abstract = {VoIP users increase each day. However, the documentation on the behavior of VoIP applications is still lacking. The needs to understand and generalize the behavior of applications like Skype, GoogleTalk and SIP based applications grow each day. There are many factors that influenced the performance of a VoIP application such as bandwidth, packet loss rate, delay, jitter, codec type and CPU power of the end devices. The user experience of the service is important since, VoIP is a real time application running over the best effort internet. Since VoIP data co-exist with other data on the internet, extracting, transforming, loading and analyzing the selected VoIP application is a challenge. We design an instrument to do data collections, data massaging, data analysis and data interpretation of large amounts of network packet. The result shows that GoogleTalk, Skype and Express Talk are more sensitive to the impairments due to packet loss rate and jitter rather than to the impairment due to delay. Bandwidth and other resources like a de-jitter buffer and a gateway's CPU and memory are important in order to produce a good quality VoIP service. The lack of these resources would result in several packets lost before they reach a destination or the packets arrive too late to join the other packets in the de-jitter buffer at the destination gateway. The gateway would drop these packets if the de-jitter buffer is full or not enough memory or CPU powers to process the packets. A gateway closer to the receiver end decapsulates IPSec or TLS packets. The gateway also decodes the voice packets before the packets entering the receiving machine. High throughputs do not imply high Perceptual Evaluation of Speech Quality for Wideband (PESQ-WB) scores. The throughput size is determined by the codec type and the security features that are implemented on the infrastructure.}, location = {Beppu, Japan}, series = {IMCOM '17}, pages = {1\u20138}, numpages = {8}, keywords = {resources, VoIP protocol, impairments, PESQ, CVSS, big data}}
@inproceedings{10.1145/3396956.3398253,title = {Big Data, Anonymisation and Governance to Personal Data Protection}, author = {Potiguara Carvalho Artur , Potiguara Carvalho Fernanda , Dias Canedo Edna , Potiguara Carvalho Pedro Henrique },year = {2020}, isbn = {9781450387910}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3396956.3398253}, doi = {10.1145/3396956.3398253}, abstract = {In a massive processing data era, an emerging impasse has taking scenario: privacy. In this context, personal data receive particular attention, witch its laws and guidelines that ensure better and legal use of data. The General Data Protection Regulation (GDPR) - in the European Union - and the Brazilian General Data Protection Law (LGPD) - in Brazil - lead to anonymisation (and its processes and techniques) as a way to reach secure use of personal data. However, expectations placed on this tool must be reconsidered according to risks and limits of its use, mainly when this technique is applied to Big Data. We discussed whether anonymisation used in conjunction with good data governance practices could provide greater protection for privacy. We conclude that good governance practices can strengthen privacy in anonymous data belonging to a Big Data, and we present a suggestive governance framework aimed at privacy.}, location = {Seoul, Republic of Korea}, series = {dg.o '20}, pages = {185\u2013195}, numpages = {11}, keywords = {Personal Data Protection, Big Data, Privacy, Governance, Anonymisation}}
@inproceedings{10.5555/2819289.2819295,title = {Mining big data for detecting, extracting and recommending architectural design concepts}, author = {Mirakhorli Mehdi , Chen Hong-Mei , Kazman Rick },year = {2015}, publisher = {IEEE Press}, abstract = {An architecture recommender system can help programmers make better design choices to address their architectural quality attribute concerns while doing their daily programming tasks. We mine big data to detect and extract a large set of architectural design concepts, such as design patterns, design tactics, architecture styles, etc., to be used in our architecture recommender system called ARS. However, mining big data poses many practical challenges for system implementation. The volume, velocity and variety of our data set, like all other big data systems, requires careful planning. This first challenge is to select appropriate technologies from the large number of available products for our system implementation. Building on these technologies our greatest challenge is to custom-fit our algorithms to the parallel processing platform we have selected for ARS, to meet our performance goals.}, location = {Florence, Italy}, series = {BIGDSE '15}, pages = {15\u201318}, numpages = {4}, keywords = {patterns, mining internet scale software repositories, tactics, open architecture, design knowledge}}
@inproceedings{10.1145/2837060.2837091,title = {Design Strategy for Enhancing Adoption of Manufacturing Big Data System (MBDS) in Korean Small and Medium-Sized Manufacturing Firms (SMMFs)}, author = {Kim Ji-Dae , Chi Su-Young , Song Young-Wook , Cho Wan-Sup , Yoo Kwan-Hee },year = {2015}, isbn = {9781450338462}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2837060.2837091}, doi = {10.1145/2837060.2837091}, abstract = {The adoption of manufacturing big data system (MBDS) is anticipated to enhance the competitiveness of small and medium-sized manufacturing firms (SMMFs), as massive amount of big data proliferate in the field of manufacturing industries. Several professionals argue that it is necessary to take an attentive approach in designing appropriate MBDS for SMMFs. This study examines appropriate design options, based on the information design theory, product-service system theory, and contingency theory, which would elevate the adoption probability of MBDS of Korean SMMFs. An empirical analysis of 191 Korean SMMFs reveals the following results. First, SMMFs put similar degree of importance on the entire design attributes of MBDS from big data gathering to information visualization. Second, SMMFs prefer payment per use of low-cost MBDS that provides more simplified information (e.g., on-spot monitoring and 2 dimension visualization) and utilizes open cloud as well as open telecommunication network. However, the firms want to have an independent and sophisticated MBDS that includes professional big data analysis and visualization software tool.}, location = {Jeju Island, Republic of Korea}, series = {BigDAS '15}, pages = {184\u2013188}, numpages = {5}, keywords = {Manufacturing Type, Manufacturing Big Data System, Design, Utility, Small and Medium-Sized Manufacturing Firms}}
@inproceedings{10.1145/2491894.2466485,title = {A bloat-aware design for big data applications}, author = {Bu Yingyi , Borkar Vinayak , Xu Guoqing , Carey Michael J. },year = {2013}, isbn = {9781450321006}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2491894.2466485}, doi = {10.1145/2491894.2466485}, abstract = {Over the past decade, the increasing demands on data-driven business intelligence have led to the proliferation of large-scale, data-intensive applications that often have huge amounts of data (often at terabyte or petabyte scale) to process. An object-oriented programming language such as Java is often the developer's choice for implementing such applications, primarily due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, significant performance problems can often be seen --- the combination of the inefficiencies inherent in a managed run-time system and the impact of the huge amount of data to be processed in the limited memory space often leads to memory bloat and performance degradation at a surprisingly early stage.This paper proposes a bloat-aware design paradigm towards the development of efficient and scalable Big Data applications in object-oriented GC enabled languages. To motivate this work, we first perform a study on the impact of several typical memory bloat patterns. These patterns are summarized from the user complaints on the mailing lists of two widely-used open-source Big Data applications. Next, we discuss our design paradigm to eliminate bloat. Using examples and real-world experience, we demonstrate that programming under this paradigm does not incur significant programming burden. We have implemented a few common data processing tasks both using this design and using the conventional object-oriented design. Our experimental results show that this new design paradigm is extremely effective in improving performance --- even for the moderate-size data sets processed, we have observed 2.5x+ performance gains, and the improvement grows substantially with the size of the data set.}, location = {Seattle, Washington, USA}, series = {ISMM '13}, pages = {119\u2013130}, numpages = {12}, keywords = {memory bloat, big data applications, design}}
@inproceedings{10.1145/2555670.2466485,title = {A bloat-aware design for big data applications}, author = {Bu Yingyi , Borkar Vinayak , Xu Guoqing , Carey Michael J. },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2555670.2466485}, doi = {10.1145/2555670.2466485}, abstract = {Over the past decade, the increasing demands on data-driven business intelligence have led to the proliferation of large-scale, data-intensive applications that often have huge amounts of data (often at terabyte or petabyte scale) to process. An object-oriented programming language such as Java is often the developer's choice for implementing such applications, primarily due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, significant performance problems can often be seen --- the combination of the inefficiencies inherent in a managed run-time system and the impact of the huge amount of data to be processed in the limited memory space often leads to memory bloat and performance degradation at a surprisingly early stage.This paper proposes a bloat-aware design paradigm towards the development of efficient and scalable Big Data applications in object-oriented GC enabled languages. To motivate this work, we first perform a study on the impact of several typical memory bloat patterns. These patterns are summarized from the user complaints on the mailing lists of two widely-used open-source Big Data applications. Next, we discuss our design paradigm to eliminate bloat. Using examples and real-world experience, we demonstrate that programming under this paradigm does not incur significant programming burden. We have implemented a few common data processing tasks both using this design and using the conventional object-oriented design. Our experimental results show that this new design paradigm is extremely effective in improving performance --- even for the moderate-size data sets processed, we have observed 2.5x+ performance gains, and the improvement grows substantially with the size of the data set.}, pages = {119\u2013130}, numpages = {12}, keywords = {big data applications, memory bloat, design}}
@inproceedings{10.1145/2524224.2524227,title = {Using big data for more dependability: a cellular network tale}, author = {Theera-Ampornpunt Nawanol , Bagchi Saurabh , Joshi Kaustubh R. , Panta Rajesh K. },year = {2013}, isbn = {9781450324571}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2524224.2524227}, doi = {10.1145/2524224.2524227}, abstract = {There are many large infrastructures that instrument everything from network performance metrics to user activities. However, the collected data are generally used for long-term planning instead of improving reliability and user experience in real time. In this paper, we present our vision of how such collections of data can be used in real time to enhance the dependability of cellular network services. We first discuss mitigation mechanisms that can be used to improve reliability, but incur a high cost which prohibit them to be used except in certain conditions. We present two case studies where analyses of real cellular network traffic data show that we can identify these conditions.}, location = {Farmington, Pennsylvania}, series = {HotDep '13}, pages = {1\u20135}, numpages = {5}, keywords = {big data, data mining, cellular, wireless}}
@inproceedings{10.1145/3386723.3387886,title = {Real-time Prediction of Accident using Big Data System}, author = {Tantaoui Mouad , Laanaoui My Driss , Kabil Mustapha },year = {2020}, isbn = {9781450376341}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3386723.3387886}, doi = {10.1145/3386723.3387886}, abstract = {In this paper, we propose a new prediction system in real time using Big Data to improve the VANET network. Firstly, The Traffic density and average speed are calculated in each section of road, and then the risk of vehicle accident is predicted in instantaneous manner with parallel data processing, which makes execution faster.}, location = {Marrakech, Morocco}, series = {NISS2020}, pages = {1\u20135}, numpages = {5}, keywords = {Lambda architecture, Traffic density, Big Data, VANET}}
@inproceedings{10.1145/3417990.3422004,title = {User-centred tooling for modelling of big data applications}, author = {Khalajzadeh Hourieh , Verma Tarun , Simmons Andrew J. , Grundy John , Abdelrazek Mohamed , Hosking John },year = {2020}, isbn = {9781450381352}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3417990.3422004}, doi = {10.1145/3417990.3422004}, abstract = {We outline the key requirements for a Big Data modelling recommender tool. Our web-based tool is suitable for capturing system requirements in big data analytics applications involving diverse stakeholders. It promotes awareness of the datasets and algorithm implementations that are available to leverage in the design of the solution. We implement these ideas in BiDaML-web, a proof of concept recommender system for Big Data applications, and evaluate the tool using an empirical study with a group of 16 target end-users. Participants found the integrated recommender and technique suggestion tools helpful and highly rated the overall BiDaML web-based modelling experience. BiDaML-web is available at https://bidaml.web.app/ and the source code can be accessed at https://github.com/tarunverma23/bidaml.}, location = {Virtual Event, Canada}, series = {MODELS '20}, pages = {1\u20135}, numpages = {5}, keywords = {BiDaML, big data applications, recommender}}
@inproceedings{10.1145/2351316.2351327,title = {Subscriber classification within telecom networks utilizing big data technologies and machine learning}, author = {Magnusson Jonathan , Kvernvik Tor },year = {2012}, isbn = {9781450315470}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2351316.2351327}, doi = {10.1145/2351316.2351327}, abstract = {This paper describes a scalable solution for identifying influential subscribers in for example telecom networks. The solution estimates one weighted value of influence out of several Social Network Analysis(SNA) metrics. The novel method for aggregation of several metrics utilizes machine learning to train models. A prototype solution has been implemented on a Hadoop platform to support scalability and to reduce hard ware cost by enabling the usage of commodity computers. The SNA algorithms have been adapted to efficiently execute on the MapReduce distributed platform. The prototype solution has been tested on a Hadoop cluster. The tests have verified that the solution can scale to support networks with millions of subscribers. Both real data from a telecom network operator with 2.4 million subscribers and synthetic data for networks up to 100 million subscribers have been used to verify the scalability and accuracy of the solution. The correlation between metrics have been analyzed to identify the information gain from each metric.}, location = {Beijing, China}, series = {BigMine '12}, pages = {77\u201384}, numpages = {8}, keywords = {machine learning, telecommunication, scalability, big data, social network analysis}}
@inproceedings{10.1145/3147213.3155013,title = {NIST Big Data Reference Architecture for Analytics and Beyond}, author = {Chang Wo },year = {2017}, isbn = {9781450351492}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3147213.3155013}, doi = {10.1145/3147213.3155013}, abstract = {Big Data is the term used to describe the deluge of data in our networked, digitized, sensor-laden, information driven world. There is a broad agreement among commercial, academic, and government leaders about the remarkable potential of \"Big Data\" to spark innovation, fuel commerce, and drive progress. The availability of vast data resources carries the potential to answer questions previously out of reach. However, there is also broad agreement on the ability of Big Data to overwhelm traditional approaches. Big Data architectures come in many shapes and forms ranging from academic research settings to product-oriented workflows. With massive-scale dynamic data being generate from social media, Internet of Things, Smart Cities, and others, it is critical to analyze these data in real-time and provide proactive decision. With the advancement of computer architecture in multi-cores and GPUs, and fast communication between CPUs and GPUs, parallel processing utilizes these platforms could optimize resources at a reduced time. This presentation will provide the past, current, and future activities of the NIST Big Data Public Working Group (NBD-PWG) and how the NIST Reference Architecture may address the rate at which data volumes, speeds, and complexity are growing requires new forms of computing infrastructure to enable Big Data analytics interoperability such that analytics tools can be re-usable, deployable, and operational. The focus of NBD-PWG is to form a community of interest from industry, academia, and government, with the goal of developing consensus definitions, taxonomies, secure reference architectures, and standards roadmap which would create vendor-neutral, technology and infrastructure agnostic framework. The aim is to enable Big Data stakeholders to pick-and-choose best analytics tools for their processing under the most suitable computing platforms and clusters while allowing value-additions from Big Data service providers and flow of data between the stakeholders in a cohesive and secure manner.}, location = {Austin, Texas, USA}, series = {UCC '17}, pages = {3}, numpages = {1}, keywords = {many cpus/cores/gpus, big data analytics, big data reference architecture, high-performance computing}}
@inproceedings{10.1145/1651415.1651417,title = {Using inheritance in a metadata based approach to data quality assessment}, author = {Farinha Jos\u00e9 , Trigueiros Maria Jos\u00e9 , Belo Orlando },year = {2009}, isbn = {9781605588162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1651415.1651417}, doi = {10.1145/1651415.1651417}, abstract = {Currently available data quality tools provide development environments that significantly decrease the effort in dealing with common data problems, such as those related with attribute domain validation, syntax checking, or value matching against a reference master data repository. On the contrary, more complex and specific data quality functionalities, whose requirements usually derive from application domain business rules, have to be developed from scratch, usually leading to high costs of development and maintenance. This paper introduces the concept of inheritance in a metadata-driven approach to simplified data quality rule management. The approach is based on the belief that even complex data quality rules very often adhere to recurring patterns that can be encoded and encapsulated as reusable, abstract templates. The approach is supported by a metamodel developed on top of OMG's Common Warehouse Metamodel, herein extended with the ability to derive new rule patterns from existing ones, through inheritance. The inheritance metamodel is presented in UML and its application is illustrated with a running example.}, location = {Hong Kong, China}, series = {MoSE+DQS '09}, pages = {1\u20138}, numpages = {8}, keywords = {cwm, metadata, inheritance, conceptual modeling, data quality, patterns}}
@inproceedings{10.1145/3426020.3426052,title = {The Relationships between Capabilities and Values of Big Data Analytics}, author = {Park Byeonghwa , Noh Mijin , Lee Choong Kwon },year = {2020}, isbn = {9781450389259}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3426020.3426052}, doi = {10.1145/3426020.3426052}, abstract = {This study aims to investigate the effect of big data analytics capability on big data values and business performance from the organizations performing big data analytics, which is one of the leading technologies in the fourth industrial revolution. For this study, the values of big data include transactional, strategic, transformational, and informational values. We conducted a survey and analyzed data from 200 professionals in organizations who had the experience of performing big data analytics. Structural equation modeling is used to test the research hypotheses. The results suggest that big data analytics capability has positive relationships with values of big data analytics and business performance. Of the values of big data analytics, however, the informational value of big data does not affect business performance. The results of this research are expected to provide researchers and practitioners who are interested in big data with useful information.}, location = {Jeju, Republic of Korea}, series = {SMA 2020}, pages = {132\u2013134}, numpages = {3}, keywords = {Big data analytics, big data value, business performance}}
@inproceedings{10.1145/2792838.2799670,title = {Data Quality Matters in Recommender Systems}, author = {Sar Shalom Oren , Berkovsky Shlomo , Ronen Royi , Ziklik Elad , Amihood Amir },year = {2015}, isbn = {9781450336925}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2792838.2799670}, doi = {10.1145/2792838.2799670}, abstract = {Although data quality has been recognized as an important factor in the broad information systems research, it has received little attention in recommender systems. Data quality matters are typically addressed in recommenders by ad-hoc cleansing methods, which prune noisy or unreliable records from the data. However, the setting of the cleansing parameters is often done arbitrarily, without thorough consideration of the data characteristics. In this work, we turn to two central data quality problems in recommender systems: sparsity and redundancy. We devise models for setting data-dependent thresholds and sampling levels, and evaluate these using a collection of public and proprietary datasets. We observe that the models accurately predict data cleansing parameters, while having minor effect on the accuracy of the generated recommendations.}, location = {Vienna, Austria}, series = {RecSys '15}, pages = {257\u2013260}, numpages = {4}, keywords = {recommender systems, sparsity, redundancy, data quality}}
@inproceedings{10.1145/2640087.2644149,title = {FAST: Fast Anonymization of Big Data Streams}, author = {Mohammadian Esmaeil , Noferesti Morteza , Jalili Rasool },year = {2014}, isbn = {9781450328913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2640087.2644149}, doi = {10.1145/2640087.2644149}, abstract = {This paper proposes an anonymization algorithm (FAST) to speed up anonymization of big data streams. The proposed parallel algorithm provides an efficient big data anonymization by a multithread technique. A proactive time-expiration heuristic is applied to publish data before they are being expired. Our simulation results indicate significant improvement in big data stream anonymization in terms of information loss and cost metric.}, location = {Beijing, China}, series = {BigDataScience '14}, pages = {1\u20138}, numpages = {8}, keywords = {Stream, Privacy, Anonymization, Big data}}
@inproceedings{10.14778/2367502.2367572,title = {Challenges and opportunities with big data}, author = {Labrinidis Alexandros , Jagadish H. V. },year = {2012}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2367502.2367572}, doi = {10.14778/2367502.2367572}, abstract = {The promise of data-driven decision-making is now being recognized broadly, and there is growing enthusiasm for the notion of \"Big Data,\" including the recent announcement from the White House about new funding initiatives across different agencies, that target research for Big Data. While the promise of Big Data is real -- for example, it is estimated that Google alone contributed 54 billion dollars to the US economy in 2009 -- there is no clear consensus on what is Big Data. In fact, there have been many controversial statements about Big Data, such as \"Size is the only thing that matters.\" In this panel we will try to explore the controversies and debunk the myths surrounding Big Data.}, pages = {2032\u20132033}, numpages = {2}}
@inproceedings{10.1145/2976744,title = {Scalable and Accurate Online Feature Selection for Big Data}, author = {Yu Kui , Wu Xindong , Ding Wei , Pei Jian },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2976744}, doi = {10.1145/2976744}, abstract = {Feature selection is important in many big data applications. Two critical challenges closely associate with big data. First, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Second, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a <underline>S</underline>calable and <underline>A</underline>ccurate <underline>O</underline>n<underline>L</underline>ine <underline>A</underline>pproach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintains a parsimonious model over time in an online manner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real datasets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on datasets of extremely high dimensionality and have superior performance over the state-of-the-art feature selection methods.}, pages = {1\u201339}, numpages = {39}, keywords = {big data, extremely high dimensionality, Online feature selection, group features}}
@inproceedings{10.1145/3447548.3470817,title = {Data Quality for Machine Learning Tasks}, author = {Gupta Nitin , Mujumdar Shashank , Patel Hima , Masuda Satoshi , Panwar Naveen , Bandyopadhyay Sambaran , Mehta Sameep , Guttula Shanmukha , Afzal Shazia , Sharma Mittal Ruhi , Munigala Vitobha },year = {2021}, isbn = {9781450383325}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3447548.3470817}, doi = {10.1145/3447548.3470817}, abstract = {The quality of training data has a huge impact on the efficiency, accuracy and complexity of machine learning tasks. Data remains susceptible to errors or irregularities that may be introduced during collection, aggregation or annotation stage. This necessitates profiling and assessment of data to understand its suitability for machine learning tasks and failure to do so can result in inaccurate analytics and unreliable decisions. While researchers and practitioners have focused on improving the quality of models, there are limited efforts towards improving the data quality.Assessing the quality of the data across intelligently designed metrics and developing corresponding transformation operations to address the quality gaps helps to reduce the effort of a data scientist for iterative debugging of the ML pipeline to improve model performance. This tutorial highlights the importance of analysing data quality in terms of its value for ML applications. Finding the data quality issues in data helps different personas like data stewards, data scientists, subject matter experts, or machine learning scientists to get relevant data insights and take remedial actions to rectify any issue. This tutorial surveys all the important data quality related approaches for structured, unstructured and spatio-temporal domains discussed in literature, focusing on the intuition behind them, highlighting their strengths and similarities, and illustrates their applicability to real-world problems. Finally we will discuss the interesting work IBM Research is doing in this space.}, location = {Virtual Event, Singapore}, series = {KDD '21}, pages = {4040\u20134041}, numpages = {2}, keywords = {machine learning, data quality, quality metrics}}
@inproceedings{10.1145/2508859.2512502,title = {POSTER: Data quality evaluation: integrating security and accuracy}, author = {Reznik Leon , Bertino Elisa },year = {2013}, isbn = {9781450324779}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2508859.2512502}, doi = {10.1145/2508859.2512502}, abstract = {Data quality (DQ) is essential to achieve data trustworthiness, as it assures that data is free of errors, complete, and consistent. This paper proposes an approach to evaluate DQ in multichannel sensor networks and systems with heterogeneous data sources. The approach integrates various DQ indicators ranging from traditional data accuracy metrics to network security and business performance measures. It demonstrates the advantage of including security metrics into the DQ evaluation for the design optimization of data fusion procedures and even the whole data collection and communication systems. The DQ metrics composition and calculus are discussed. However, the major attention is paid to the analysis of the relationship between conventional data accuracy metrics and network security indicators.}, location = {Berlin, Germany}, series = {CCS '13}, pages = {1367\u20131370}, numpages = {4}, keywords = {data accuracy, data quality, data fusion., computer security evaluation}}
@inproceedings{10.1145/3053600.3053630,title = {Capacity Allocation for Big Data Applications in the Cloud}, author = {Ciavotta Michele , Gianniti Eugenio , Ardagna Danilo },year = {2017}, isbn = {9781450348997}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3053600.3053630}, doi = {10.1145/3053600.3053630}, abstract = {The aim of this work is to present the problem of Capacity Allocation for multiple classes of Big Data applications running in the Cloud. The objective is the minimization of the renting out costs subject to the fulfillment of QoS requirements expressed in terms of application deadlines. We propose a preliminary version of a tool embedding a local-search-based algorithm exploiting also an integer nonlinear mathematical formulation and a queueing network simulation to solve the problem.}, location = {L&apos;Aquila, Italy}, series = {ICPE '17 Companion}, pages = {175\u2013176}, numpages = {2}, keywords = {qos, big data, cloud, capacity allocation.}}
@inproceedings{10.1145/2627534.2627556,title = {Attack tolerant architecture for big data file systems}, author = {Madan Bharat B. , Banik Manoj },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2627534.2627556}, doi = {10.1145/2627534.2627556}, abstract = {Data driven decisions derived from big data have become critical in many application domains, fueling the demand for collection, transportation, storage and processing of massive volumes of data. Such applications have made data a valuable resource that needs to be provided appropriate security. High value associated with big data sets has rendered big data storage systems attractive targets for cyber attackers, whose goal is to compromise the Confidentiality, Integrity and Availability of data and information. Common defense strategy for protecting cyber assets has been to first take preventive measures, and if these fail, detecting intrusions and finally recovery. Unfortunately, attackers have developed tremendous technical sophistication to defeat most defensive mechanisms. Alternative strategy is to design architectures which are intrinsically attack tolerant. This paper describes a technique that involves eliminating single point of security failures through fragmentation, coding, dispersion and reassembly. It is shown that this technique can be successfully applied to routing, networked storage systems, and big data file systems to make them attack tolerant.}, pages = {65\u201369}, numpages = {5}, keywords = {data confidentiality, big-data security, secure storage, data availability, attack tolerance, data integrity}}
@inproceedings{10.1145/2378356,title = {Proceedings of the 2012 workshop on Management of big data systems},year = {2012}, isbn = {9781450317528}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {Data is growing at an exponential rate and several systems have emerged to store and analyze such large amounts of data. These systems, termed \u201cBig data systems\u201d are fast evolving Examples include the NoSQL storage systems, Hadoop Map-Reduce, data analytics platforms, search and indexing platforms, and messaging infrastructures. These systems address needs for structured and unstructured data across a wide spectrum of domains such as web, social networks, enterprise, cloud, mobile, sensor networks, multimedia/streaming, cyberphysical and high performance systems, and for multiple application verticals such as biosciences, healthcare, transportation, public sector, energy utilities, oil & gas, and scientific computing.With increasing scale and complexity, managing these big data systems to cope with failures and performance problems is becoming non-trivial. New resource management and scheduling mechanisms are also needed for such systems, and so are mechanisms for tuning and support from platform layers. Several open source and proprietary solutions have been proposed to address these requirements, with extensive contributions from industry and academia. However, there remain substantial challenges, including those that pertain to such systems' autonomic and self-management capabilities.The objective of the MBDS workshop is to bring together researchers, practitioners, system administrators, system programmers, and others interested in sharing and presenting their perspectives on the effective management of big data systems. The focus of the workshop is on novel and practical, systems-oriented work. MBDS offers an opportunity for researchers and practitioners from industry, academia, and national labs to showcase the latest advances in this area and to also discuss and identify future directions and challenges in all aspects on autonomic management of big data systems.Papers are solicited on all aspects of big data management. Specific topics of interest include, but are not limited, to the following: Autonomic and self-managing techniques Application-level resource management and scheduling mechanisms System tuning/auto-tuning and configuration management Performance management, fault management, and power management Scalability challenges Complexity challenges, as for composite, cross-tier systems with multiple control loops Unified management of 'data in motion' and 'data at rest' Dealing with both structured and unstructured data Monitoring, diagnosis, and automated behaviour detection System-level principles and support for resource management Holistic management across hardware and software Implications of emerging hardware technologies such as non-volatile memory Domain specific challenges in web, cloud, social networks, mobile, sensor networks, streaming analytics, cyber-physical systems System building and experience papers for specific industry verticals}, location = {San Jose, California, USA}}
@inproceedings{10.1109/WI-IAT.2014.15,title = {Big Data - Characterizing an Emerging Research Field Using Topic Models}, author = {Hansmann Thomas , Niemeyer Peter },year = {2014}, isbn = {9781479941438}, publisher = {IEEE Computer Society}, address = {USA}, url = {https://doi.org/10.1109/WI-IAT.2014.15}, doi = {10.1109/WI-IAT.2014.15}, abstract = {Big Data is one of the latest emerging topics in the field of business information systems, and is marketed as being the key for companies' future success. Many analytic solutions are offered by IT companies to help other businesses with the flood of data that is generated within and outside of a company. Despite the extensive use of the notion Big Data for marketing purposes, there is no common understanding of how to characterize the elements of the Big Data concept. The authors contribute to the clarification of this concept with a methodologically enriched literature review by deriving characteristic dimensions from existing definitions of Big Data. These dimensions are validated and enriched with a two-step approach by applying topic models on 248 publications relevant to Big Data. The authors propose that the concept of Big Data can be described by the dimensions of data, IT infrastructure, applied methods, and an applications perspective. The assignment of the results to a generic data analysis process reveals that recent publications focus on data analysis and processing, and less attention is given to the initial data selection or the visualization and utilization of the analysis results.}, series = {WI-IAT '14}, pages = {43\u201351}, numpages = {9}, keywords = {literature review, Big Data, topic models}}
@inproceedings{10.1145/2628071.2671429,title = {Processing Big Data Graphs on Memory-Restricted Systems}, author = {Harshvardhan , Amato Nancy M. , Rauchweger Lawrence },year = {2014}, isbn = {9781450328098}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2628071.2671429}, doi = {10.1145/2628071.2671429}, abstract = {With the advent of big-data, processing large graphs quickly has become increasingly important. Most existing approaches either utilize in-memory processing techniques, which can only process graphs that fit completely in RAM, or disk-based techniques that sacrifice performance.In this work, we propose a novel RAM-Disk hybrid approach to graph processing that can scale well from a single shared-memory node to large distributed-memory systems. It works by partitioning the graph into subgraphs that fit in RAM and uses a paging-like technique to load subgraphs. We show that without modifying the algorithms, this approach can scale from small memory-constrained systems (such as tablets) to large-scale distributed machines with 16,000+ cores.}, location = {Edmonton, AB, Canada}, series = {PACT '14}, pages = {517\u2013518}, numpages = {2}, keywords = {distributed computing, big data, parallel graph processing, out-of-core graph algorithms, graph analytics}}
@inproceedings{10.1145/3257818,title = {Session details: Big data}, author = {Kelly Terence },year = {2015}, isbn = {9781450338349}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3257818}, doi = {10.1145/3257818}, location = {Monterey, California}, series = {SOSP '15}, pages = {}}
@inproceedings{10.1145/2640087.2644187,title = {FAST: Fast Anonymization of Big Data Streams}, author = {Mohammadian Esmaeil , Noferesti Morteza , Jalili Rasool },year = {2014}, isbn = {9781450328913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2640087.2644187}, doi = {10.1145/2640087.2644187}, abstract = {This paper proposes an anonymization algorithm (FAST) to speed up anonymization of big data streams. The proposed parallel algorithm provides an efficient big data anonymization by a multithread technique. A proactive time-expiration heuristic is applied to publish data before they are being expired. Our simulation results indicate significant improvement in big data stream anonymization in terms of information loss and cost metric.}, location = {Beijing, China}, series = {BigDataScience '14}, pages = {1\u20132}, numpages = {2}, keywords = {Stream, Big data, Privacy, Anonymization}}
@inproceedings{10.1145/3415958.3433082,title = {Scalable Execution of Big Data Workflows using Software Containers}, author = {Dessalk Yared Dejene , Nikolov Nikolay , Matskin Mihhail , Soylu Ahmet , Roman Dumitru },year = {2020}, isbn = {9781450381154}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3415958.3433082}, doi = {10.1145/3415958.3433082}, abstract = {Big Data processing involves handling large and complex data sets, incorporating different tools and frameworks as well as other processes that help organisations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, require taking advantage of the elasticity of cloud infrastructures for scalability. In this paper, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies and message-oriented middleware (MOM) to enable highly scalable workflow execution. The approach is demonstrated in a use case together with a set of experiments that demonstrate the practical applicability of the proposed approach for the scalable execution of Big Data workflows. Furthermore, we present a scalability comparison of our proposed approach with that of Argo Workflows - one of the most prominent tools in the area of Big Data workflows.}, location = {Virtual Event, United Arab Emirates}, series = {MEDES '20}, pages = {76\u201383}, numpages = {8}, keywords = {Domain-specific languages, Software containers, Big Data workflows}}
@inproceedings{10.1145/3416921,title = {Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing},year = {2020}, isbn = {9781450375382}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The main goal and feature of the conference are to bring together scientists, engineers and industry researchers together to exchange and share their experiences, research results and discuss emerging practical problems and solutions. ICCBDC 2020 received 39 submissions of research papers. After a strict reviewing process, 23 of them have been accepted for presentation at the conference and publication in the proceedings. The conference program included invited talks, six research paper presentation sessions and one industry session. It covered recent trends and advances made in the fields of cloud and big data computing. All accepted papers were presented online in 15 minutes followed by discussions. This conference proceedings consists of the research papers presented at ICCBDC 2020.}, location = {Virtual, United Kingdom}}
@inproceedings{10.1145/2808719.2816984,title = {Big data and predictive modeling topics in healthcare}, author = {Deng Xin , Wu Donghui },year = {2015}, isbn = {9781450338530}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2808719.2816984}, doi = {10.1145/2808719.2816984}, abstract = {This panel discussion will first review a few current big data and predictive modeling topics in healthcare in both providers and payers marketspace, including the problems, current status and challenges, and opportunities for big data and future trends. Follow the opening remarks, the panelists will provide their own insights and point of views on some of the topics, and interactive discussion among themselves and audience. Among many of the topics, a few examples are: Improve quality outcomes through analytics and predictive modeling; Reduce and prevent hospital re-admission through risk predictions and case management; Discover Abuse, Waste and Fraud in Healthcare Providers; Improve population health through personalized interventions; Understand the population signed through Health Insurance Exchange Marketplace (HIX); Population Risk Adjustment; Improve CMS STARS rating; Provider Quality Measurement and Regulator Reporting; Provider Risk and Revenue management; Unified Patient Record, Big Data Challenges for Providers and Payers, etc.}, location = {Atlanta, Georgia}, series = {BCB '15}, pages = {677}, numpages = {1}, keywords = {data integration, big data platform, predictive analytics, population health management, health informatics}}
@inproceedings{10.1145/3158344,title = {Developing an Open Source 'Big Data' Cognitive Computing Platform: Big Data (Ubiquity symposium)}, author = {Kowolenko Michael , Vouk Mladen A. },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3158344}, doi = {10.1145/3158344}, abstract = {The ability to leverage diverse data types requires a robust and dynamic approach to systems design. The needs of a data scientist are as varied as the questions being explored. Compute systems have focused on the management and analysis of structured data as the driving force of analytics in business. As open source platforms have evolved, the ability to apply compute to unstructured information has exposed an array of platforms and tools available to the business and technical community. We have developed a platform that meets the needs of the analytics user requirements of both structured and unstructured data. This analytics workbench is based on acquisition, transformation, and analysis using open source tools such as Nutch, Tika, Elastic, Python, PostgreSQL, and Django to implement a cognitive compute environment that can handle widely diverse data, and can leverage the ever-expanding capabilities of infrastructure in order to provide intelligence augmentation.}, pages = {1\u201315}, numpages = {15}}
@inproceedings{10.1145/3402569.3409038,title = {Research on Big Data Mining Method of Bioinformatics}, author = {Changxin Song , Ke Ma },year = {2020}, isbn = {9781450377546}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3402569.3409038}, doi = {10.1145/3402569.3409038}, abstract = {With the updating and upgrading of hospital network information diagnosis and treatment, in the medical disease diagnosis, data processing and scientific research, we usually use a variety of big data and cloud computing technology, combined with clinical biology, medicine and other diseases diagnosis and treatment plan, and carry out biological information data collection, mining, analysis, processing and storage. In order to meet the needs of data variables for different medical treatment activities, this article focuses on the analysis of the causes of hypertension diseases in different regions around the essential hypertension (EH) by using the improved ant colony optimization algorithm (IACO), including the external environment, racial differences, genetic heterogeneity, and the single nucleotide polymorphism (SNP) polymorphism data and SNP-SNP combination in each gene coding region. Information mining and parallel computing analysis are carried out to obtain more objective and accurate results of complex disease assessment.}, location = {Beijing, China}, series = {ICDEL 2020}, pages = {166\u2013169}, numpages = {4}, keywords = {Bioinformatics, big data mining, research, methods}}
@inproceedings{10.1145/3524383.3524399,title = {The Path of Cultivating Applied Accounting Talents Based on DES Model under Big Data, Intelligentization, Mobile Internet and Cloud Computing}, author = {Yang Xuxin },year = {2022}, isbn = {9781450395793}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3524383.3524399}, doi = {10.1145/3524383.3524399}, abstract = {Under the Big Data, Intelligentization, Mobile Internet and Cloud Computing, the accounting and the accounting profession face new opportunities and difficulties, and the new standards for accounting expertise are established. Because of the imbalance between supply and demand, there is currently an excess and oversupply of accounting skills in China. In order to deal with this challenge, this paper proposes the path of training applied accounting talents based on the DES model. On this basis, it examines the current situation of insufficient accounting talent cultivated by accounting higher education in China from a perspective of supply, and analyzes the society's demand for accounting talent under Big Data, Intelligentization, Mobile Internet and Cloud Computing from a perspective of demand. Then, it is found that the supply cannot match the demand effectively. Thus, in the Big Data, Intelligentization, Mobile Internet and Cloud Computing environment, this paper proposes the paths of reconstructing accounting talent cultivation goals, systematically adding information-based curriculum, optimizing teaching evaluation mechanism, focusing on faculty construction, establishing teaching modes of student participation, and stressing cross-border capability cultivation.}, location = {Shanghai, China}, series = {ICBDE '22}, pages = {183\u2013188}, numpages = {6}, keywords = {Accounting Ability, Applied Accounting Talents, DES Model, Big Data, Intelligentization, Mobile Internet and Cloud Computing, Cultivation Path}}
@inproceedings{10.1145/2911975,title = {What happens when big data blunders?}, author = {Kugler Logan },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2911975}, doi = {10.1145/2911975}, abstract = {Big data is touted as a cure-all for challenges in business, government, and healthcare, but as disease outbreak predictions show, big data often fails.}, pages = {15\u201316}, numpages = {2}}
@inproceedings{10.1145/3085228.3085275,title = {Enterprise Architectures for Supporting the Adoption of Big Data}, author = {Gong Yiwei , Janssen Marijn },year = {2017}, isbn = {9781450353175}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3085228.3085275}, doi = {10.1145/3085228.3085275}, abstract = {Governments from all over the world are struggling to take advantage of big data developments. Enterprise Architecture (EA) can be used as an instrument to integrate big data (BD) in the existing business processes and ICT-landscape. In this policy paper, we explore the role of EA in the adoption of BD. For this, we adopted a qualitative case study approach and investigated a large administrative organization that was in the process of adopting BD. We found in our case study that the first attempts were focused on integrating big data in the current landscape, but this encountered too many challenges that halt progress. To overcome the challenges, a separate BD department and accompanying infrastructure was created. The strategy was first to reap the benefits of BD and to understand what should be done, and thereafter integrating the working systems in the existing landscape. The findings suggest that current infrastructures might not be suitable for integrating BD and substantial changes are needed first. In the case the role of BD needed to be first clarified before EA could play a role in adopting BD. EA should deal with the uncertainties and complexities by ensuring a configurable landscape, by providing an incremental approach for adapting the infrastructure step-by-step, before the benefits of big data can be gained. Developing an incremental migration plan was found to be a key aspect for the adoption of BD.}, location = {Staten Island, NY, USA}, series = {dg.o '17}, pages = {505\u2013510}, numpages = {6}, keywords = {enterprise architecture, BOLD, big data, e-government, open data, infrastructure, ICT-architecture}}
@inproceedings{10.1145/2487575.2506179,title = {Entity resolution for big data}, author = {Getoor Lise , Machanavajjhala Ashwin },year = {2013}, isbn = {9781450321747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2487575.2506179}, doi = {10.1145/2487575.2506179}, abstract = {Entity resolution (ER), the problem of extracting, matching and resolving entity mentions in structured and unstructured data, is a long-standing challenge in database management, information retrieval, machine learning, natural language processing and statistics. Accurate and fast entity resolution has huge practical implications in a wide variety of commercial, scientific and security domains. Despite the long history of work on entity resolution, there is still a surprising diversity of approaches, and lack of guiding theory. Meanwhile, in the age of big data, the need for high quality entity resolution is growing, as we are inundated with more and more data, all of which needs to be integrated, aligned and matched, before further utility can be extracted. In this tutorial, we bring together perspectives on entity resolution from a variety of fields, including databases, information retrieval, natural language processing and machine learning, to provide, in one setting, a survey of a large body of work. We discuss both the practical aspects and theoretical underpinnings of ER. We describe existing solutions, current challenges and open research problems. In addition to giving attendees a thorough understanding of existing ER models, algorithms and evaluation methods, the tutorial will cover important research topics such as scalable ER, active and lightly supervised ER, and query-driven ER.}, location = {Chicago, Illinois, USA}, series = {KDD '13}, pages = {1527}, numpages = {1}}
@inproceedings{10.1145/1851476.1851558,title = {Monitoring data quality in Kepler}, author = {Na'im Aisa , Crawl Daniel , Indrawan Maria , Altintas Ilkay , Sun Shulei },year = {2010}, isbn = {9781605589428}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1851476.1851558}, doi = {10.1145/1851476.1851558}, abstract = {Data quality is an important component of modern scientific discovery. Many scientific discovery processes consume data from a diverse array of resources such as streaming sensor networks, web services, and databases. The validity of a scientific computation's results is highly dependent on the quality of these input data. Scientific workflow systems are being increasingly used to automate scientific computations by facilitating experiment design, data capture, integration, processing, and analysis. These workflows may execute in grid or cloud environments, and if the data produced during workflow execution is deemed unusable or low in quality, execution should stop to prevent wasting these valuable resources. We propose an approach in the Kepler scientific workflow system for monitoring data quality and demonstrate its use for oceanography and bioinformatics domains.}, location = {Chicago, Illinois}, series = {HPDC '10}, pages = {560\u2013564}, numpages = {5}, keywords = {scientific workflow, grid computing, data quality management}}
@inproceedings{10.1145/3027385.3029445,title = {Reproducibility of findings from educational big data: a preliminary study}, author = {Oi Misato , Yamada Masanori , Okubo Fumiya , Shimada Atsushi , Ogata Hiroaki },year = {2017}, isbn = {9781450348706}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3027385.3029445}, doi = {10.1145/3027385.3029445}, abstract = {In this paper, we examined whether previous findings on educational big data consisting of e-book logs from a given academic course can be reproduced with different data from other academic courses. The previous findings showed that (1) students who attained consistently good achievement more frequently browsed different e-books and their pages than low achievers and that (2) this difference was found only for logs of preparation for course sessions (preview), not for reviewing material (review). Preliminarily, we analyzed e-book logs from four courses. The results were reproduced in only one course and only partially, that is, (1) high achievers more frequently changed e-books than low achievers (2) for preview. This finding suggests that to allow effective usage of learning and teaching analyses, we need to carefully construct an educational environment to ensure reproducibility.}, location = {Vancouver, British Columbia, Canada}, series = {LAK '17}, pages = {536\u2013537}, numpages = {2}, keywords = {reproducibility, educational big data, e-book}}
@inproceedings{10.1145/3090354,title = {Proceedings of the 2nd international Conference on Big Data, Cloud and Applications},year = {2017}, isbn = {9781450348522}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Tetouan, Morocco}}