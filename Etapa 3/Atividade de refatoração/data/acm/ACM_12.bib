@inproceedings{10.1145/2588555.2594530,title = {Demonstration of the Myria big data management service}, author = {Halperin Daniel , Teixeira de Almeida Victor , Choo Lee Lee , Chu Shumo , Koutris Paraschos , Moritz Dominik , Ortiz Jennifer , Ruamviboonsuk Vaspol , Wang Jingjing , Whitaker Andrew , Xu Shengliang , Balazinska Magdalena , Howe Bill , Suciu Dan },year = {2014}, isbn = {9781450323765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2588555.2594530}, doi = {10.1145/2588555.2594530}, abstract = {In this demonstration, we will showcase Myria, our novel cloud service for big data management and analytics designed to improve productivity. Myria's goal is for users to simply upload their data and for the system to help them be self-sufficient data science experts on their data -- self-serve analytics. Using a web browser, Myria users can upload data, author efficient queries to process and explore the data, and debug correctness and performance issues. Myria queries are executed on a scalable, parallel cluster that uses both state-of-the-art and novel methods for distributed query processing. Our interactive demonstration will guide visitors through an exploration of several key Myria features by interfacing with the live system to analyze big datasets over the web.}, location = {Snowbird, Utah, USA}, series = {SIGMOD '14}, pages = {881\u2013884}, numpages = {4}, keywords = {data management service, big data management, Myria}}
@inproceedings{10.1145/3105971.3105979,title = {Towards Glyph-based visualizations for big data clustering}, author = {Keck Mandy , Kammer Dietrich , Gr\u00fcnder Thomas , Thom Thomas , Kleinsteuber Martin , Maasch Alexander , Groh Rainer },year = {2017}, isbn = {9781450352925}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3105971.3105979}, doi = {10.1145/3105971.3105979}, abstract = {Data Analysts have to deal with an ever-growing amount of data resources. One way to make sense of this data is to extract features and use clustering algorithms to group items according to a similarity measure. Algorithm developers are challenged when evaluating the performance of the algorithm since it is hard to identify features that influence the clustering. Moreover, many algorithms can be trained using a semi-supervised approach, where human users provide ground truth samples by manually grouping single items. Hence, visualization techniques are needed that help data analysts achieve their goal in evaluating Big data clustering algorithms. In this context, Multidimensional Scaling (MDS) has become a prominent visualization tool. In this paper, we propose a combination with glyphs that can provide a detailed view of specific features involved in MDS. In consequence, human users can understand, adjust, and ultimately improve clustering algorithms. We present a thorough glyph design, which is founded in a comprehensive survey of related work and report the results of a controlled experiments, where participants solved data analysis tasks with both glyphs and a traditional textual display of data values.}, location = {Bangkok, Thailand}, series = {VINCI '17}, pages = {129\u2013136}, numpages = {8}, keywords = {big data, Glyph-based visualization techniques, multidimensional scaling, visual cluster analysis}}
@inproceedings{10.1145/2905055.2905197,title = {Big-data Transportation in Orchestrated Bioinformatics Workflows: An Analysis and Hypothesis}, author = {Nunes Rickey T. P. , Desphande Santosh L. , Subramanian Sattanathan },year = {2016}, isbn = {9781450339629}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2905055.2905197}, doi = {10.1145/2905055.2905197}, abstract = {Rapid growth of biological-data size brings the field of bioinformatics into the era of big-data and challenges the process of coordinating biological data sources and tool services. One of the challenges is how to transfer big-data among the components of a workflow in a faster and efficient manner. In this paper, we therefore (i) show the state-of-the-art approaches and analyze big-data handling in orchestrated bioinformatics workflows, (ii) provide a hypothesis based on the analysis and show that the hypothesis performs better than the state-of-the-art approaches. Our analysis shows that none of the existing approaches can retain the execution time of a workflow stable with respect to the size of the data involved. The main reason is that all the approaches attempt to move data which is varying in size, but not the tool (or the computation associated with it) which is fixed in size. Considering this as a hypothesis, we have moved computation instead of data in a workflow and shown that the workflow performs much better than other approaches in terms of execution time.}, location = {Udaipur, India}, series = {ICTCS '16}, pages = {1\u20137}, numpages = {7}, keywords = {moving computation, Big-data, bioinformatics, moving data, orchestration, workflows}}
@inproceedings{10.1145/3156818,title = {A Study on Garbage Collection Algorithms for Big Data Environments}, author = {Bruno Rodrigo , Ferreira Paulo },year = {2018}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3156818}, doi = {10.1145/3156818}, abstract = {The need to process and store massive amounts of data\u2014Big Data\u2014is a reality. In areas such as scientific experiments, social networks management, credit card fraud detection, targeted advertisement, and financial analysis, massive amounts of information are generated and processed daily to extract valuable, summarized information. Due to its fast development cycle (i.e., less expensive to develop), mainly because of automatic memory management, and rich community resources, managed object-oriented programming languages (e.g., Java) are the first choice to develop Big Data platforms (e.g., Cassandra, Spark) on which such Big Data applications are executed.However, automatic memory management comes at a cost. This cost is introduced by the garbage collector, which is responsible for collecting objects that are no longer being used. Although current (classic) garbage collection algorithms may be applicable to small-scale applications, these algorithms are not appropriate for large-scale Big Data environments, as they do not scale in terms of throughput and pause times.In this work, current Big Data platforms and their memory profiles are studied to understand why classic algorithms (which are still the most commonly used) are not appropriate, and also to analyze recently proposed and relevant memory management algorithms, targeted to Big Data environments. The scalability of recent memory management algorithms is characterized in terms of throughput (improves the throughput of the application) and pause time (reduces the latency of the application) when compared to classic algorithms. The study is concluded by presenting a taxonomy of the described works and some open problems, with regard to Big Data memory management, that could be addressed in future works.}, pages = {1\u201335}, numpages = {35}, keywords = {memory managed runtime, Java, scalability, Big Data, processing platforms, storage platform, Garbage collection, Big Data environment}}
@inproceedings{10.1145/3278607,title = {Tensor Completion Algorithms in Big Data Analytics}, author = {Song Qingquan , Ge Hancheng , Caverlee James , Hu Xia },year = {2019}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3278607}, doi = {10.1145/3278607}, abstract = {Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.}, pages = {1\u201348}, numpages = {48}, keywords = {Tensor, tensor completion, big data analytics, tensor factorization, multilinear data analysis, tensor decomposition, dynamic data analysis}}
@inproceedings{10.1145/2938503.2938539,title = {A Big Data Approach For Querying Data in EHR Systems}, author = {Cassavia Nunziato , Ciampi Mario , De Pietro Giuseppe , Masciari Elio },year = {2016}, isbn = {9781450341189}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2938503.2938539}, doi = {10.1145/2938503.2938539}, abstract = {Information management in healthcare is nowadays experiencing a great revolution. After the impressive progress in digitizing medical data by private organizations, also the federal government and other public stakeholders have also started to make use of healthcare data for data analysis purposes in order to extract actionable knowledge. In this paper, we propose an architecture for supporting interoperability in healthcare systems by exploiting Big Data techniques. In particular, we describe a proposal based on big data techniques to implement a nationwide system able to improve EHR data access efficiency and reduce costs.}, location = {Montreal, QC, Canada}, series = {IDEAS '16}, pages = {212\u2013217}, numpages = {6}, keywords = {Interoperability, Healthcare, Big data}}
@inproceedings{10.1145/3369555.3369573,title = {IoT-based big data analytics issues in healthcare}, author = {Ntehelang Gomotsegang , Isong Bassey , Lugayizi Francis , Dladlu Nosipho },year = {2019}, isbn = {9781450371803}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3369555.3369573}, doi = {10.1145/3369555.3369573}, abstract = {Internet of Things devices constantly generate big data which when analyzed reveals hidden patterns, information and trends, thus, enabling decision making. Several organization today utilized data analytics to improve organizational performance and deliver high quality of service and experience. Healthcare is not an exception and uses data analytics for some of its capabilities such as detecting diseases and diagnose patients at early stages, identifying high-risk patients and providing them treatment to reduce unnecessary hospitalization or readmission. However, the existing and expected increase of connected devices poses several challenges for data analysis, especially with little work being done to address them. Therefore, this paper brings together some of these challenges that needs to be addressed and some of the proposed solutions. We performed a review of some of the studies in the literature with a view of providing research directions for researchers.}, location = {Tokyo, Japan}, series = {ICTCE '19}, pages = {16\u201321}, numpages = {6}, keywords = {healthcare, internet of things, big data analytics}}
@inproceedings{10.14778/2367502.2367563,title = {MapReduce algorithms for big data analysis}, author = {Shim Kyuseok },year = {2012}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2367502.2367563}, doi = {10.14778/2367502.2367563}, abstract = {There is a growing trend of applications that should handle big data. However, analyzing big data is a very challenging problem today. For such applications, the MapReduce framework has recently attracted a lot of attention. Google's MapReduce or its open-source equivalent Hadoop is a powerful tool for building such applications. In this tutorial, we will introduce the MapReduce framework based on Hadoop, discuss how to design efficient MapReduce algorithms and present the state-of-the-art in MapReduce algorithms for data mining, machine learning and similarity joins. The intended audience of this tutorial is professionals who plan to design and develop MapReduce algorithms and researchers who should be aware of the state-of-the-art in MapReduce algorithms available today for big data analysis.}, pages = {2016\u20132017}, numpages = {2}}
@inproceedings{10.1145/3274005.3274015,title = {Using Big Data Value Chain to Create Government Education Policies}, author = {Petrova-Antonova Dessislava , Ilieva Sylvia },year = {2018}, isbn = {9781450364256}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3274005.3274015}, doi = {10.1145/3274005.3274015}, abstract = {The Big Data Value Chain aims to discover patterns, correlations, and pattern deviations hidden in a dataset. This paper investigates (1) the possible approaches to applying Big Data Value Chain to decision making in the Public sector, specifically in Education; and (2) the ways in which such activities can be automated. The models created can be customized depending on a school's dropout rate and number of students with learning difficulties. This research is part of a current project at the Ministry of Education and Science (MES) of Bulgaria, funded by the Operational Programme Science and Education for Smart Growth which aims to increase student engagement.}, location = {Ruse, Bulgaria}, series = {CompSysTech'18}, pages = {42\u201349}, numpages = {8}, keywords = {Big Data, Big Data Value Chain, Data model, Decision support, Education}}
@inproceedings{10.1145/2377978,title = {Proceedings of the 1st Workshop on Architectures and Systems for Big Data},year = {2011}, isbn = {9781450314398}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Galveston Island, Texas, USA}}
@inproceedings{10.1145/3240117.3240139,title = {Les Archives \u00e0 l'\u00c8re des Big Data: Les Enjeux de l'Archivage des Donn\u00e9es Num\u00e9riques Massives}, author = {Ben Amor Fatma , Mkadmi Abderrazak },year = {2018}, isbn = {9781450364515}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3240117.3240139}, doi = {10.1145/3240117.3240139}, abstract = {Big Data is now a cross-cutting research topic in all disciplines related to digital as content and as technology too. They lie in the intersection between all the massive data captured, obtained, created by different means and of various origins. They represent an advanced step in the re-development of information, particularly concerning data management and data retention issues. This upheaval due to these massive data has touched all sectors, particularly the archives. Indeed, given their volume, their speed of creation and their importance to the social, economic, scientific and cultural actors, the sorting, the treatment and the conservation of these data Massive e orts require memory capacity, new methods and techniques for processing, analyzing and managing particular flows. We will try in this article to bring some elements of primary answers on the modalities of generative multiplication. exponential of numerical data and archive in a mass of numeric data, data that are in flux and that occur in a sup speed.}, location = {Paris, France}, series = {DTUC '18}, pages = {1\u20136}, numpages = {6}, keywords = {Digital archiving, Big Data, long-term conservation, big data access}}
@inproceedings{10.1145/3453187.3453340,title = {Research on Smart Education Service Platform Based on Big Data}, author = {Hu Zhifeng , Zhao Feng , Zhao Xiaona },year = {2020}, isbn = {9781450389099}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3453187.3453340}, doi = {10.1145/3453187.3453340}, abstract = {The big data technology can be applied to build the education service platforms and construct the big data analysis and application system as well as the multi-dimensional perception system. The big data analysis assists in the teaching process and breaks the temporal and spatial restrictions of educational resources, to realize the diversification of educational resources and improve the effectiveness of teaching feedback. This paper proposes a smart education service platform based on big data, which can promote the organic integration of educational communication, educational research, learning activities, teaching affairs administration, and information infrastructures. At the same time, the platform provides smarter, more efficient, and accurate services for teaching.}, location = {Wuhan, China}, series = {EBIMCS 2020}, pages = {228\u2013233}, numpages = {6}, keywords = {Smart education, Big data, Information-oriented education}}
@inproceedings{10.1145/2339530.2339534,title = {Divide-and-conquer and statistical inference for big data}, author = {Jordan Michael I. },year = {2012}, isbn = {9781450314626}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2339530.2339534}, doi = {10.1145/2339530.2339534}, abstract = {I present some recent work on statistical inference for Big Data. Divide-and-conquer is a natural computational paradigm for approaching Big Data problems, particularly given recent developments in distributed and parallel computing, but some interesting challenges arise when applying divide-and-conquer algorithms to statistical inference problems. One interesting issue is that of obtaining confidence intervals in massive datasets.The bootstrap principle suggests resampling data to obtain fluctuations in the values of estimators, and thereby confidence intervals, but this is infeasible with massive data. Subsampling the data yields fluctuations on the wrong scale, which have to be corrected to provide calibrated statistical inferences. I present a new procedure, the \"bag of little bootstraps,\" which circumvents this problem, inheriting the favorable theoretical properties of the bootstrap but also having a much more favorable computational profile. Another issue that I discuss is the problem of large-scale matrix completion. Here divide-and-conquer is a natural heuristic that works well in practice, but new theoretical problems arise when attempting to characterize the statistical performance of divide-and-conquer algorithms. Here the theoretical support is provided by concentration theorems for random matrices, and I present a new approach to this problem based on Stein's method1.}, location = {Beijing, China}, series = {KDD '12}, pages = {4}, numpages = {1}, keywords = {divide-and-conquer, subsampling, confidence intervals, big data}}
@inproceedings{10.1145/3358528.3358566,title = {An Efficient Annotation Method for Big Data Sets of High-Resolution Earth Observation Images}, author = {Lu Zeshan , Liu Kun , Liu Zhen , Wang Cong , Shen Maoxin , Xu Tao },year = {2019}, isbn = {9781450371926}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3358528.3358566}, doi = {10.1145/3358528.3358566}, abstract = {High-resolution earth observation images have increased dramatically because of the increasing of remote sensing satellites. Researchers must do large-scale target annotations to meet the training needs of deep neural network based model. However, most existing datasets contain an insufficient number of annotated samples, due to the inefficient manual annotation process which reason lies in the large number of remote sensing images, huge size, numerous targets, and high accuracy requirements. This paper proposed an efficient annotation method for big data sets of high-resolution earth observation images, in which the annotation process is divided into two parallel sub-processes, fast panchromatic image labeling and multi-spectral image fusion. Automatic scale transform is utilized for annotation of fused imagery. Experimental results show that the proposed method could improve the accuracy and efficiency of target labeling. Mask-RCNN and Faster-RCNN based target detection results demonstrate the validity of the big dataset annotated via our method.}, location = {Jinan, China}, series = {ICBDT2019}, pages = {240\u2013243}, numpages = {4}, keywords = {Mask-RCNN, target detection, Annotation method, high-resolution images}}
@inproceedings{10.1145/2837060,title = {Proceedings of the 2015 International Conference on Big Data Applications and Services},year = {2015}, isbn = {9781450338462}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Jeju Island, Republic of Korea}}
@inproceedings{10.1145/3017680.3017705,title = {Teaching Big Data and Cloud Computing with a Physical Cluster}, author = {Eickholt Jesse , Shrestha Sharad },year = {2017}, isbn = {9781450346986}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3017680.3017705}, doi = {10.1145/3017680.3017705}, abstract = {Cloud Computing and Big Data continue to be disruptive forces in computing and have made inroads in the Computer Science curriculum, with courses in Cloud Computing and Big Data being routinely offered at the graduate and undergraduate level. One major challenge in offering courses in Big Data and Cloud Computing is resources. The question is how to provide students with authentic experiences making use of current Cloud and Big Data resources and tools and do so in a cost effective manner. Historically, three options, namely physical clusters, virtual clusters and cloud-based clusters, have been used to support Big Data and Cloud Computing courses. Virtual clusters and cloud-based options are those that institutions have typically adopted and many arguments in favor of these options exist in the literature, citing cost and performance. Here we argue that teaching Big Data and Cloud Computing courses can be done making use of a physical cluster and that many of the existing arguments fail to take into account many important factors in their calculations. These factors include the flexibility and control of a physical cluster in responding to changes in industry, the ability to work with much larger datasets, and the synergy and broad applicability of an appropriately equipped physical cluster for courses such as Cloud Computing, Big Data and Data Mining. We present three possible configurations of a physical cluster which span the spectrum in terms of cost and provide cost comparisons of these configurations against virtual and cloud-based options, taking into account the unique requirements of an academic setting. While limitations do exist with a physical cluster and it is not an option for all situations, our analysis and experience indicates that there is great value in using a physical cluster to support teaching Cloud Computing and Big Data courses and it should not be dismissed.}, location = {Seattle, Washington, USA}, series = {SIGCSE '17}, pages = {177\u2013181}, numpages = {5}, keywords = {computing cluster, cloud computing, big data}}
@inproceedings{10.1145/3259177,title = {Session details: Big data}, author = {\u00d6zsu M. Tamer },year = {2012}, isbn = {9781450313261}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3259177}, doi = {10.1145/3259177}, location = {Scottsdale, Arizona, USA}, series = {PhD '12}, pages = {}}
@inproceedings{10.1145/3265689.3265706,title = {Impacts of Big Data on Data Mining Research: An Empirical Study of Chinese Journals}, author = {Huang Yue },year = {2018}, isbn = {9781450365871}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3265689.3265706}, doi = {10.1145/3265689.3265706}, abstract = {With the advent of big data, data mining theories and methods face new challenges. This paper tries to find the impacts of big data on data mining research through 23377 data mining-related papers published in Chinese academic journals during 1996--2016. By utilization of various methods of bibliometrics, this study conducts three different levels of analysis to gradually dig deeper into the contents of literature. For the macro-level, paper amount analysis results show that big data-related research began in 2012 and has brought new growth to data mining area. For the meso-level, journal distribution analysis results indicate that many other disciplines, such as arts and agriculture science, began to apply data mining techniques with the wide spread of big data. For the micro-level, co-word-based research topic clustering results imply that new topics emerged due to the easy access of big data, such as 'clouding computing' and 'teaching and learning analysis'.}, location = {Singapore, Singapore}, series = {ICCSE'18}, pages = {1\u20135}, numpages = {5}, keywords = {Bibliometrics, Data mining, Big data, Impact}}
@inproceedings{10.1145/3152723.3152741,title = {Big Data Analysis for Spatio-Temporal Earthquake Risk-Mapping System in Indonesia with Automatic Clustering}, author = {Barakbah Ali Ridho , Harsono Tri , Sudarsono Amang , Aliefyan Roy Advandy },year = {2017}, isbn = {9781450353564}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3152723.3152741}, doi = {10.1145/3152723.3152741}, abstract = {Earthquake is one of nature disaster types. Almost all regions in Indonesia are often earthquakes ranging from small magnitude to large. Anticipation and mitigation of earthquake victims is one of the important points in preventing the occurrence of earthquake victims in large numbers. One of the important information in anticipating and handling earthquake victims is to provide information about earthquake risk mapping in a region (province). This information is given by big data processing of the earthquake distribution and big data analytics of spatio-temporal earthquake data. This paperpresented a big data analysis for earthquake risk mapping system based on earthquake density projected to provinces in Indonesia. This system has 4 main features: (1) Data acquisation and preprocessing, (2) Automatic clustering using our Valley Tracing algorithm, (3) Density measurement of earthquake data distribution, and (4) Risk-mapping visualization projected to provinces. For experimental study, earthquake data is obtained from Advanced National Seismic System(ANSS) year 1963-2016 in location of Indonesia. We made a series of experiments in the places hit by big earthquake in Andaman (Banca Aceh), West Sumatra, and Papua. Based on the big data processing of the earthquake distribution and big data analytics of spatio-temporal earthquake data, it performed that the high seismic density value affected the risk of earthquake occurrence in the next year in the area concerned.}, location = {Osaka, Japan}, series = {ICBDR 2017}, pages = {33\u201337}, numpages = {5}, keywords = {Earthquake Data Distribution, Risk-Mapping Visualization, Automatic Clustering, Spatio-Temporal Earthquake Data Distribution}}
@inproceedings{10.1145/3230348.3230368,title = {Twitter Data Classification Using Big Data Technologies}, author = {Youness Madani , Mohammed Erritali , Jamaa Bengourram },year = {2018}, isbn = {9781450363754}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3230348.3230368}, doi = {10.1145/3230348.3230368}, abstract = {Tweets classification or in general the classification of the social network's data is a recent field of scientific research, where researchers look for new methods to classify users data (tweets, Facebook's post...) into classes (positive, negative, neutral).This type of scientific research called sentiment analysis (SA) or opinion mining and it allows to extract the feelings, opinions or attitudes expressed in a tweet or a facebook post ...In this article, we describe how we can collect and store a large volume of data, which is in the form of tweets, in Hadoop Distributed File System (HDFS), and how we can classify these tweets using different classification methods, making a comparison between the well-known machine learning algorithms and a dictionary based-approach using the AFINN dictionary. The experimental results show that the AFINN dictionary outperforms the well-known machine learning algorithms.}, location = {Singapore, Singapore}, series = {ICIEB '18}, pages = {124\u2013129}, numpages = {6}, keywords = {data mining, Hadoop, big data, opinion mining, sentiment analysis, Twitter}}
@inproceedings{10.5555/2819009.2819232,title = {1st international workshop on big data software engineering (BIGDSE 2015)}, author = {Baresi Luciano , Menzies Tim , Metzger Andreas , Zimmermann Thomas },year = {2015}, publisher = {IEEE Press}, abstract = {Big Data is about extracting valuable information from data in order to use it in intelligent ways such as to revolutionize decision-making in businesses, science and society. BIGDSE 2015 discusses the link between Big Data and software engineering and critically looks into issues such as cost-benefit of big data.}, location = {Florence, Italy}, series = {ICSE '15}, pages = {965\u2013966}, numpages = {2}, keywords = {software engineering, big data, software analytics}}
@inproceedings{10.1145/3474944,title = {2021 the 3rd International Conference on Big Data Engineering and Technology (BDET)},year = {2021}, isbn = {9781450389280}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Singapore, Singapore}}
@inproceedings{10.1145/3230348.3230425,title = {Application Research of Big Data E-commerce in Closed Community}, author = {Xia Huan , Tang Shiqi , Li Shuang , Yu Xiaomin },year = {2018}, isbn = {9781450363754}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3230348.3230425}, doi = {10.1145/3230348.3230425}, abstract = {Big data e-commerce is melting into our lives with an overwhelming trend, the establishment of big data e-commerce in closed communities is much easier to meet the consumer demand of community residents. This article is based on the analysis of the development of big data electronic commerce, presented establish big data electronic commerce in a closed community, to take Guizhou University of Finance and Economics \"Trust Me\" platform as an example. It introduces the functional requirements, platform structure, idea, operation mechanism and operation mode of the platform, and summarizes the advantages and improvement.}, location = {Singapore, Singapore}, series = {ICIEB '18}, pages = {43\u201346}, numpages = {4}, keywords = {closed community, e-commerce, big data}}
@inproceedings{10.1145/2812428.2812429,title = {Database technologies in the world of big data}, author = {Pokorn\u00fd Jaroslav },year = {2015}, isbn = {9781450333573}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2812428.2812429}, doi = {10.1145/2812428.2812429}, abstract = {Now we have a number of database technologies called usually NoSQL, like key-value, column-oriented, and document stores as well as search engines and graph databases. Whereas SQL software vendors offer advanced products with the capability to handle highly complex queries and transactions, NoSQL databases share rather characteristics concerning scaling and performance, as e.g. auto-sharding, distributed query support, and integrated caching. Their drawbacks can be a lack of schema or data consistency, difficulty in testing and maintaining, and absence of a higher query language. Complex data modelling and the SQL language as the only access tool to data are missing here. On the other hand, last studies show that both SQL and NoSQL databases have value for both for transactional and analytical Big Data. Top databases providers offer rearchitected database technologies combining row data stores with columnar in-memory compression enabling processing large data sets and analytical querying, often over massive, continuous data streams. The technological progress led to development of massively parallel processing analytic databases. The paper presents some details of current database technologies, their pros and cons in different application environments, and emerging trends in this area.}, location = {Dublin, Ireland}, series = {CompSysTech '15}, pages = {1\u201312}, numpages = {12}, keywords = {database technologies, big data, transaction processing, NewSQL databases, data distribution, NoSQL databases, big analytics}}
@inproceedings{10.1145/3424978.3425001,title = {Research on University Security Big Data and Emergency Collaborative Disposal}, author = {Wang Guotian , Liu Xu },year = {2020}, isbn = {9781450377720}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3424978.3425001}, doi = {10.1145/3424978.3425001}, abstract = {With the frequent occurrence of university security accidents in recent years, university security issues have become a hot topic of concern to society and research. Currently for university security, the most serious problems are the inability to response to emergencies timely due to the short of real-time data acquisition and processing devices. For these existing issues on the aspects of university security mentioned above, this paper focuses research on the university security big data and emergency collaborative disposal, the university big data integration system is established firstly for university security big data real-time collection, processing, analysis and storage, then the UML based multi-department emergency collaborative disposal organization structure is proposed to realize the efficient and coordinated emergency response operations of multi-department when emergencies occur, meanwhile, the emergency disposal and supplies dispatch scheme is designed to optimize relief supplies distribution and emergency response results. Lastly, the university security big data and emergency coordination system is implemented, and accuracy experiment of the system is conducted, the results of system experiment and implementation are not only indicate the effectiveness and feasibility of the methods proposed by this paper, but also proved to be capable of general applicability to university security management.}, location = {Sanya, China}, series = {CSAE 2020}, pages = {1\u20138}, numpages = {8}, keywords = {Collaborative Disposal, Big Data, University Security}}
@inproceedings{10.1145/2379436.2379437,title = {The constraints of magnetic versus flash disk capabilities in big data analysis}, author = {Regola Nathan , Cieslak David A. , Chawla Nitesh V. },year = {2012}, isbn = {9781450314442}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2379436.2379437}, doi = {10.1145/2379436.2379437}, abstract = {Solid state disks (or flash disks) are decreasing in cost per gigabyte and are being incorporated into many appliances, such as the Oracle Database Appliance [8]. Databases--and more specifically data warehouses--are often utilized to support large scale data analysis and decision support systems. Decision makers prefer information in real time. Traditional storage systems that are based on magnetic disks achieve high performance by utilizing many disks for parallel operations in RAID arrays. However, this performance is only possible if requests represent a reasonable fraction of the RAID stripe size, or I/O transactions will suffer from high overhead. Solid state disks have the potential to increase the speed of data retrieval for mission critical workloads that require real time applications, such as analytic dashboards. However, solid state disks behave differently than magnetic hard disks due to the limitations of rewriting NAND flash based blocks. Therefore, this work presents benchmark results for a modern relational database that stores data on solid state disks, and contrasts this performance to a ten disk RAID 10 array, a traditional storage design for high performance database data blocks. The preliminary results show that a single solid state disk is able to outperform the array for queries summarizing a data set for a variety of OLAP cube dimensions. Future work will explore the low level database performance in more detail.}, location = {Portland, Oregon, USA}, series = {ASBD '12}, pages = {4\u20139}, numpages = {6}, keywords = {solid state disk, query performance}}
@inproceedings{10.5555/2840819.2840927,title = {Modern Big Data Analytics for \"Old-fashioned\" Semiconductor Industry Applications}, author = {Zhu Yada , Xiong Jinjun },year = {2015}, isbn = {9781467383899}, publisher = {IEEE Press}, abstract = {Big data analytics is the latest spotlight with all the glare of fame ranging from media coverage to booming start-up companies to eye-catching merges and acquisitions. On the contrary, the $336 billion industry of semiconductor was seen as an \"old-fashioned\" business, with fading interests from the best and brightest among young graduates and engineers. How will modern big data analytics help the semiconductor industry walk through this transition? This paper answers this question via a number of practical but challenging problems arising from semiconductor manufacturing process. We show that many existing machine learning algorithms are not well positioned to solve these problems, and novel techniques involving temporal, structural and hierarchical properties need to be developed to solve these problems.}, location = {Austin, TX, USA}, series = {ICCAD '15}, pages = {776\u2013780}, numpages = {5}, keywords = {manufacturing, semiconductor, analytics, Big data}}
@inproceedings{10.1145/3255001,title = {Session details: Big data}, author = {Narayanan Krish },year = {2014}, isbn = {9781450326056}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3255001}, doi = {10.1145/3255001}, location = {Atlanta, Georgia, USA}, series = {SIGCSE '14}, pages = {}}
@inproceedings{10.1145/2379436,title = {Proceedings of the 2nd Workshop on Architectures and Systems for Big Data},year = {2012}, isbn = {9781450314442}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Portland, Oregon, USA}}
@inproceedings{10.1145/3433996.3434027,title = {The Planning and Construction of Healthcare Big Data Platform}, author = {Ding Shifu , Liu Yan , Zhang Jianjun , Tan Yaqi , Li Xiaoxia , Tang RuiChun },year = {2020}, isbn = {9781450388641}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3433996.3434027}, doi = {10.1145/3433996.3434027}, abstract = {Healthcare Big Data Platform is the important content in the process of medical information industry. To a certain extent, it represents the overall level of the regional informatization. It is also a data exchange and sharing platform connecting the basic systems of local various medical and health institutions, and it is also the base and carrier to integrate the regional information system. This paper introduces the local regional medical informatization construction, planning architecture, data center construction mode and technical realization methods. Through this project, the informatization level of basic health agencies and all hospitals will have been greatly improved. It can provide more convenient and high-quality medical service for patients, alleviates \"difficulty and expensive\" problem effectively.}, location = {Taiyuan, China}, series = {CAIH2020}, pages = {170\u2013176}, numpages = {7}, keywords = {Big Data, SOA, Healthcare, Electronic Health Record, EMR}}
@inproceedings{10.1145/3411681.3412951,title = {Online Teaching Platform Based on Big Data Recommendation System}, author = {Liao TongXin , Feng XinHui , Sun YuanLi , Wang HongTing , Liao Cong , Li YuanBing },year = {2020}, isbn = {9781450375757}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411681.3412951}, doi = {10.1145/3411681.3412951}, abstract = {The essence of big data education is the combination of network education and traditional education. The development of big data and Internet industry has brought new changes to the traditional learning model. The way to acquire knowledge is not limited to books and classrooms, and the Internet era is the era of information explosion, so the Internet has gradually become the most important channel for people to acquire knowledge. Therefore, the big data learning platform has gradually become a new mode of teaching adopted by schools. In the big data teaching mode, personalized recommendation system plays an indispensable role in online education.}, location = {London, United Kingdom}, series = {ICIEI '20}, pages = {35\u201339}, numpages = {5}, keywords = {Keywords Big data, Online teaching and cloud platform, recommended}}
@inproceedings{10.1145/3220228.3220238,title = {An innovative methodology for big data visualization in oceanographic domain}, author = {Galletta Antonino , Allam Salma , Carnevale Lorenzo , Bekri Moulay Ali , Ouahbi Rachid El , Villari Massimo },year = {2018}, isbn = {9781450364454}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3220228.3220238}, doi = {10.1145/3220228.3220238}, abstract = {Nowadays, thanks to new technologies, we are observing an explosion of data in different fields such as clinical, environmental and so on. In this context, a typical example of the well-known Big Data problem is represented by visualization. In this work, we propose an innovative platform for managing the oceanographic acquisitions. More specifically, we present two innovative visualization techniques: general overview and site specific observation. Experiments prove the goodness of the proposed system in terms both of performance and user experience.}, location = {Prague, Czech Republic}, series = {ICGDA '18}, pages = {103\u2013107}, numpages = {5}, keywords = {oceanography, big data visualization, big data, IoT, acidification, geolocation, microservices}}
@inproceedings{10.1145/1541880.1541883,title = {Methodologies for data quality assessment and improvement}, author = {Batini Carlo , Cappiello Cinzia , Francalanci Chiara , Maurino Andrea },year = {2009}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1541880.1541883}, doi = {10.1145/1541880.1541883}, abstract = {The literature provides a wide range of techniques to assess and improve the quality of data. Due to the diversity and complexity of these techniques, research has recently focused on defining methodologies that help the selection, customization, and application of data quality assessment and improvement techniques. The goal of this article is to provide a systematic and comparative description of such methodologies. Methodologies are compared along several dimensions, including the methodological phases and steps, the strategies and techniques, the data quality dimensions, the types of data, and, finally, the types of information systems addressed by each methodology. The article concludes with a summary description of each methodology.}, pages = {1\u201352}, numpages = {52}, keywords = {quality dimension, data quality measurement, data quality assessment, Data quality, methodology, information system, data quality improvement}}
@inproceedings{10.1145/2676536.2676538,title = {High performance integrated spatial big data analytics}, author = {Chen Xin , Vo Hoang , Aji Ablimit , Wang Fusheng },year = {2014}, isbn = {9781450331326}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2676536.2676538}, doi = {10.1145/2676536.2676538}, abstract = {The growth of spatial big data has been explosive thanks to cost-effective and ubiquitous positioning technologies, and the generation of data from multiple sources in multi-forms. Such emerging spatial data has high potential to create new insights and values for our life through spatial analytics. However, spatial data analytics faces two major challenges. First, spatial data is both data-and compute-intensive due to the massive amounts of data and the multi-dimensional nature, which requires high performance spatial computing infrastructure and methods. Second, spatial big data sources are often isolated, for example, OpenStreetMap, census data and Twitter tweets are independent data sources. This leads to incompleteness of information and sometimes limited data accuracy, thus limited values from the data. Integrating spatial big data analytics by consolidating multiple data sources provides significant potential for data quality improvement in terms of completeness and accuracy, and much increased values derived from the data. In this paper, we present our vision of a high performance integrated spatial big data analytics framework. We provide a scalable spatial query based data integration engine with MapReduce, and demonstrate integrated spatial data analytics through a few use cases in our preliminary work. We then present our future plan on integrated spatial big data analytics for improving public health research and applications.}, location = {Dallas, Texas}, series = {BigSpatial '14}, pages = {11\u201314}, numpages = {4}, keywords = {database, GIS, MapReduce, data warehouse, spatial analytics}}
@inproceedings{10.1145/3544538.3544635,title = {Low-cost clusters on big data - A systematic study}, author = {Alves Neto Antonio Jose , Carneiro Neto Jose Aprigio , Moreno Ordonez Edward David },year = {2022}, isbn = {9781450397384}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544538.3544635}, doi = {10.1145/3544538.3544635}, abstract = {The growing gap between users and the Big Data analytics requires innovative tools that address the challenges faced by big data such as volume, variety, and velocity. Therefore, it becomes computationally inefficient to analyze this massive volume of data. Moreover, advancements in the field of Big Data applications and data science poses additional challenges, where High-Performance Computing (HPC) solution has become a key issue and has attracted attention in recent years. Because of the high costs to obtain a HPC, the researchers are looking for a solution that copes with the increasing demand on processing power due to the expanding amount of the data produced. Eventually, they have been trying to implement big data clusters based on low-cost and low-energy hardware. The goal of this paper is to identify how is possible to develop a big data cluster using hardware structures of low cost, exposing the studies found in literature. In order to fulfill this, a Systematic Literature Mapping (SLM) was realized, resulting in several relevant papers which are able to response three research questions. The SLM identified Single Board Computers (SBC) as hardware structure most used, being the Raspberry PI with major citations, and Apache Hadoop and Apache Spark as big data platforms used in these clusters. The validation of these clusters it was done with some popular algorithms, where the algorithms K-Means and Map-Reduce were the most quoted in the selected studies, this last based on its original approach, unlike the approach from Hadoop Map-Reduce.}, location = {Aveiro, Portugal}, series = {EATIS '22}, pages = {1\u20137}, numpages = {7}, keywords = {Single Board Card, Systematic Literature Mapping, Beowulf, Big Data, Cluster, Low Cost, Raspberry Pi}}
@inproceedings{10.1145/3424978.3425010,title = {Research on Scientific Data Management in Big Data Era}, author = {Man Rui , Zhou Guomin , Fan Jingchao },year = {2020}, isbn = {9781450377720}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3424978.3425010}, doi = {10.1145/3424978.3425010}, abstract = {Scientific data is an important strategic resource in the era of big data. Efficient management and wide circulation are the key ways to enhance the value of scientific data resources. With the transformation of the industrial society into the information society, the importance of scientific data management is also increasing all over the world, which continuously promotes the maturity of scientific data management and sharing. In this article, through comprehensive research of scientific data management ideas, policies, practices and results, the analysis summarizes the advanced experience of international scientific data management, for the similar problems and challenges existing in the research in China, puts forward the future a period of time the direction and suggestions on the development of scientific data management: 1. the specification of various kinds of degree of the standardization of scientific data resources; 2. To strengthen data mining capacity; 3. To strengthen the cultivation of talents in data science; 4. To strengthen international cooperation and enhance core competitiveness in the big data era.}, location = {Sanya, China}, series = {CSAE 2020}, pages = {1\u20136}, numpages = {6}, keywords = {Scientific data, Big data, Scientific data management, Opening and sharing of data resource}}
@inproceedings{10.1145/3281375.3281382,title = {A knowledge discovery in community contributions of big data technologies}, author = {Bieh-Zimmert Oliver , Felden Carsten },year = {2018}, isbn = {9781450356220}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3281375.3281382}, doi = {10.1145/3281375.3281382}, abstract = {The increasing variety of big data technologies in open source communities is challenging organizations to generate value from those advancements. The technology landscape is missing an overall perspective that clarifies the fragmented understanding of technologies, unpredictable lifecycles, and the unknown adoption for organizations to enable their business with useful technologies. More than one million contributions of features, bugs, and changes were pushed on public available code repositories to develop big data technologies with hidden understanding of the underlying data basis. Using this source could help to identify insights about technological domains as well as their adoption process of contributors to new uprising big data technologies. A knowledge discovery process provided the potential to analyze 269 big data technologies regarding their contribution behavior of over 21,000 contributors. As a result, investigations show an ecosystem of structuring big data technologies based on dynamic contributor networks that have implications on organizations adoption.}, location = {Tokyo, Japan}, series = {MEDES '18}, pages = {55\u201358}, numpages = {4}, keywords = {adoption, open source, big data, knowledge discovery}}
@inproceedings{10.1145/3289402.3289525,title = {Big Data Processing using Machine Learning algorithms: MLlib and Mahout Use Case}, author = {Aziz Khadija , Zaidouni Dounia , Bellafkih Mostafa },year = {2018}, isbn = {9781450364621}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3289402.3289525}, doi = {10.1145/3289402.3289525}, abstract = {Machine learning is a field within artificial intelligence that allows machines to learn on their own from existing information to make predictions or/and decisions. There are three main categories of machine learning techniques: Collaborative filtering (for making recommendations), Clustering (for discovering structure in collections of data) and Classification (form of supervised learning). Machine learning helps users to make better decisions, Machine learning algorithms create patterns based on previous information and use them to design predictive models, then, use this models to obtain predictions about future data. A huge amount of data from several sources need methods and techniques to be processed correctly, in order to exploit this data efficiently, machine learning is a great technology for exploiting the needs in big data analysis. This paper describes the implementation of Apache Spark MLlib and Apache Mahout in order to process Big Data using Machine Learning algorithms. Furthermore, we conduct experimental simulations to show the difference between this two Machine Learning frameworks. Subsequently, we discuss the most striking observations that emerge from the comparison of these technologies through several experimental studies.}, location = {Rabat, Morocco}, series = {SITA'18}, pages = {1\u20136}, numpages = {6}, keywords = {MLlib, Classification, Collaborative Filtering, Big Data, Spark, Hadoop, Machine Learning, Clustering, Mahout}}
@inproceedings{10.1145/2448917.2448925,title = {Visual rhetoric and big data: design of future communication}, author = {Salvo Michael J. },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2448917.2448925}, doi = {10.1145/2448917.2448925}, abstract = {The hype machine---media, corporate communications, and futurist prognosticators---are hard at work promoting Big Data. There are computing and storage resources that, like the \"dark fiber\" installed at the turn of the millennium that now carries streaming video, are looking for huge data sets that require the powerful processing and tremendous storage capacity of the new infrastructure. And there is no better confluence than that provided by the impetus to rearticulate Communication Design Quarterly in an age of Big Data. The New York Times has been running articles about Big Data for some time:\"Big data is all about exploration without preconceived notions.\"}, pages = {37\u201340}, numpages = {4}}
@inproceedings{10.1145/3234664.3234677,title = {Research on Clothing and Accouterment Security Based on Big Data}, author = {Fei Xian-hong , Zhai Cheng-gong , Qiao Han },year = {2018}, isbn = {9781450364850}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3234664.3234677}, doi = {10.1145/3234664.3234677}, abstract = {The research on the Clothing and Accouterment security based on big data plays an important role in deepening the army's reform and strengthening the scientific management of the army. This paper introduces the main influence of big data on the installation security, analyzes the application status of data in the installation security, and puts forward the application assumption of the big data in the installation security.}, location = {Beijing, China}, series = {HPCCT '18}, pages = {19\u201323}, numpages = {5}, keywords = {clothing and accouterment security, data analysis, Big data}}
@inproceedings{10.1145/3421537,title = {Proceedings of the 2020 4th International Conference on Big Data and Internet of Things},year = {2020}, isbn = {9781450375504}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {On behalf of the conference committees, it is my pleasure to address a warm welcome to all delegates to participate in2020 the 4th International Conference on Big Data and Internet of Things(BDIOT2020). This conference is technically sponsored by University of Macau, La trobe University and Universita Di Pisa, and also attracts some media partners.}, location = {Singapore, Singapore}}
@inproceedings{10.1145/2538862.2544280,title = {Towards engaging big data for CS1/2 (abstract only)}, author = {Hamid Nadeem Abdul , Benzel Steven },year = {2014}, isbn = {9781450326056}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2538862.2544280}, doi = {10.1145/2538862.2544280}, abstract = {A number of contextualized approaches to teaching introductory Computer Science (CS) courses have been developed in the past few years, catering to students with different interests and learning styles. For instance, entire courses have been developed around media computation or robots (real and virtual). There is however one context which, to our knowledge, has not been exploited in a systematic fashion - that of \"big data,\" by which we mean massive, openly accessible online datasets from a wide variety of sources. We present progress on a code framework and methodology to facilitate the incorporation of large, online data sets into traditional CS1 and CS2 courses. The goal of our project is to develop a way to provide students a library that relieves them from low-level issues of reading and parsing raw data from web-based data sources and that interfaces with data structures and representations defined by students themselves. In addition, the library requires minimal syntactic overhead to use its functionality and allows students and instructors to focus on algorithmic exercises involving processing live and large data obtained from the Internet. At a minimum, the library should serve to create drop-in replacements for traditional programming exercises in introductory courses - raising the engagement level by having students deal with \"real\" data rather than artificial data provided through standard input.}, location = {Atlanta, Georgia, USA}, series = {SIGCSE '14}, pages = {710}, numpages = {1}, keywords = {CS1, big data}}
@inproceedings{10.1145/2743065.2743110,title = {Demystifying Challenges, Opportunities and Issues of Big Data Frameworks}, author = {Padmapriya V. , Amudhavel J. , Gowri V. , Lakshmipriya K. , Vinothini S. , Kumar K. Prem },year = {2015}, isbn = {9781450334419}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2743065.2743110}, doi = {10.1145/2743065.2743110}, abstract = {The Big data has a huge volume of data sets, which is used to process the conventional data processing applications. In this paper, the Flex Analytics framework is used to enhance the scalability and flexibility of analyzing and processing the data and provide visualization of data. The MapReduce framework of the fuzzy logic model is used in big data for the degree of uncertainty and to reduce the imbalance of data. The MapReduce framework for large-scale extreme learning machine for massive and dispersed data and machine learning approach is used to find the bottleneck in the peer-peer network. The emerging distributed cloud data centers with the security framework using Hadoop technology is used to process larger datasets. The financial data standard is utilized to eliminate the data redundancy and noise.}, location = {Unnao, India}, series = {ICARCSET '15}, pages = {1\u20135}, numpages = {5}, keywords = {Data analytics, application, exploration, infringement, monitor, unstructured data, Big data}}
@inproceedings{10.1145/2998575,title = {An Introduction to Dynamic Data Quality Challenges}, author = {Labouseur Alan G. , Matheus Carolyn C. },year = {2017}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2998575}, doi = {10.1145/2998575}, pages = {1\u20133}, numpages = {3}, keywords = {relational systems, graph systems, internet of things, Dynamic data quality, big data}}
@inproceedings{10.1145/2361999.2362038,title = {Unified analytics platform for big data}, author = {Miner Donald },year = {2012}, isbn = {9781450315685}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2361999.2362038}, doi = {10.1145/2361999.2362038}, abstract = {Greenplum is using Hadoop and several other open source tools in interesting ways as part of a big data architecture with their Greenplum Database (a scale-out MPP SQL database).}, location = {Helsinki, Finland}, series = {WICSA/ECSA '12}, pages = {176}, numpages = {1}, keywords = {EMC, R, data science, database, Hadoop, Greenplum, geospatial, MADlib, MPP, PostGIS, Solr, Mahout}}
@inproceedings{10.1145/2656434.2657486,title = {Big data trends and evolution: a human perspective}, author = {Villanustre Flavio G. },year = {2014}, isbn = {9781450327114}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2656434.2657486}, doi = {10.1145/2656434.2657486}, abstract = {The Big Data revolution has already happened and, through it, organizations started realizing the potential of using data to take better informed decisions, mitigate risks and overall better control their destiny. With all the benefits that Big Data brings, it also creates new challenges; the growing talent gap possibly being the most representative of them all. In order to effectively leverage Big Data, a new profession is emerging: the data scientist. Tasked with understanding the methodologies to process and analyze vast and complex data, this professional must possess knowledge in a broad spectrum of domains, including mathematics (calculus, linear algebra, statistics, probabilities and even possibly category theory), programming languages (Python and R being frequently cited), data processing and analysis expertise (profiling, parsing, cleansing, linking), machine learning techniques (supervised and unsupervised learning, dimensionality reduction, feature selection, etc.) and business domain knowledge. While it is conceivable to identify individuals that can achieve this breadth of knowledge with significant depth, it is unreasonable to expect this to be the norm, so these individuals fall usually far into the upper tail of the population distribution. To make things worse, the current toolsets available to the data scientist tend to be very involved and require considerable amounts of time to develop applications, reducing the overall effectiveness of these experts. The solution to this talent gap is certainly not to try and breed a new step up the evolutionary ladder that can cope with this vast knowledge, but to create radically different abstractions as part of the toolsets that data scientists use, to increase efficiency and reduce the scope of the basic knowledge required to build Big Data applications. During this presentation we will explore this challenge and provide a new perspective on more efficient toolsets for Big Data applications.}, location = {Atlanta, Georgia, USA}, series = {RIIT '14}, pages = {1\u20132}, numpages = {2}, keywords = {HPCC, KEL, data science, declarative programming, ECL, data analysis, dataflow programming}}
@inproceedings{10.1145/3386723.3387826,title = {Big Data Solutions Proposed for Cluster Computing Systems Challenges: A survey}, author = {Es-Sabery Fatima , Hair Abdellatif },year = {2020}, isbn = {9781450376341}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3386723.3387826}, doi = {10.1145/3386723.3387826}, abstract = {CCS (Cluster Computing System) is coming to solve the problems of standard technology. Whose, objective is to improve the performance/power efficiency of a single processor for storing and mining the large data sets, using the parallel programming to read and process the massive data sets on multiple disks and CPUs. The thing which makes these systems somewhat performant than the standard technology is the physical organization of computing nodes in the cluster. Currently, this kind of cluster does not entirely solve the problem because it comes with its challenges, which are Node failures, Computations, Network Bottleneck, and Distributed programming. All these problems are coming when we are mining and storing the massive volume of data using cluster computing. To solve these challenges, Google invented a new Big Data framework of data processing called MapReduce, to manage large scale data processing across large clusters of commodity servers. The paper outlines the running of CCS and presents its challenges in this era of Big Data. Moreover, it introduces the most popular Big Data solutions proposed to overcome the CCS challenges. Also, it shows how Big Data technologies solve CCS issues. Generally, the main goal of this work is to provide a better understanding of the challenges of CCS and identify the essential big data solutions in this increasingly important area.}, location = {Marrakech, Morocco}, series = {NISS2020}, pages = {1\u20137}, numpages = {7}, keywords = {CCS, MapReduce, Distributed File System, Big Data, Challenges}}
@inproceedings{10.1145/3469968,title = {Proceedings of the 6th International Conference on Big Data and Computing},year = {2021}, isbn = {9781450389808}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Shenzhen, China}}
@inproceedings{10.1145/3035918.3058737,title = {Debugging Big Data Analytics in Spark with BigDebug}, author = {Gulzar Muhammad Ali , Interlandi Matteo , Condie Tyson , Kim Miryung },year = {2017}, isbn = {9781450341974}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3035918.3058737}, doi = {10.1145/3035918.3058737}, abstract = {To process massive quantities of data, developers leverage Data-Intensive Scalable Computing (DISC) systems such as Apache Spark. In terms of debugging, DISC systems support only post-mortem log analysis and do not provide any debugging functionality. This demonstration paper showcases BigDebug: a tool enhancing Apache Spark with a set of interactive debugging features that can help users in debug their Big Data Applications.}, location = {Chicago, Illinois, USA}, series = {SIGMOD '17}, pages = {1627\u20131630}, numpages = {4}, keywords = {big data analytics, automatic fault localization, disc, debugging, data-intensive scalable computing, interactive tools}}
@inproceedings{10.1145/3408127.3408180,title = {Research on Big Data Parallel Processing Platform Based on Postal Industry}, author = {Yang Pinglin , Guo Gaizhi },year = {2020}, isbn = {9781450376877}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3408127.3408180}, doi = {10.1145/3408127.3408180}, abstract = {With the development of cloud computing, big data, and the Internet of Things, for the data collection and daily drama of the postal express industry, in the face of such a large-scale data set, the traditional storage and calculation related theories and methods can no longer meet the massive and multi-source access and processing of heterogeneous data. The article analyzes the characteristics of postal data, focuses on the Hadoop and Spark platform architectures, and compares the performance differences between the two platforms through experiments. At the same time, according to the characteristics of Hadoop and Spark big data platforms, they learn from each other's strengths and apply them to different stages of the postal big data system.}, location = {Chengdu, China}, series = {ICDSP 2020}, pages = {305\u2013309}, numpages = {5}, keywords = {Postal, Spark, Big data platform, Hadoop, Big data}}