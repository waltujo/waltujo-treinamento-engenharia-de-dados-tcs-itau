@inproceedings{10.1145/2968456.2976765,title = {Big data analytics on heterogeneous accelerator architectures}, author = {Neshatpour Katayoun , Sasan Avesta , Homayoun Houman },year = {2016}, isbn = {9781450344838}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2968456.2976765}, doi = {10.1145/2968456.2976765}, abstract = {In this paper, we present the implementation of big data analytics applications in a heterogeneous CPU+FPGA accelerator architecture. We develop the MapReduce implementation of K-means, K nearest neighbor, support vector machine and Naive Bayes in a Hadoop Streaming environment that allows developing mapper/reducer functions in a non-Java based language suited for interfacing with FPGA-based hardware accelerating environment. We present a full implementation of the HW+SW mappers on the Zynq FPGA platform. A promising speedup as well as energy-efficiency gains of upto 4.5X and 22X is achieved, respectively, in an end-to-end Hadoop implementation.}, location = {Pittsburgh, Pennsylvania}, series = {CODES '16}, pages = {1\u20133}, numpages = {3}}
@inproceedings{10.14778/2824032.2824067,title = {Differential privacy in telco big data platform}, author = {Hu Xueyang , Yuan Mingxuan , Yao Jianguo , Deng Yu , Chen Lei , Yang Qiang , Guan Haibing , Zeng Jia },year = {2015}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2824032.2824067}, doi = {10.14778/2824032.2824067}, abstract = {Differential privacy (DP) has been widely explored in academia recently but less so in industry possibly due to its strong privacy guarantee. This paper makes the first attempt to implement three basic DP architectures in the deployed telecommunication (telco) big data platform for data mining applications. We find that all DP architectures have less than 5% loss of prediction accuracy when the weak privacy guarantee is adopted (e.g., privacy budget parameter \u03b5 \u2265 3). However, when the strong privacy guarantee is assumed (e.g., privacy budget parameter \u03b5 \u2264 0:1), all DP architectures lead to 15% ~ 30% accuracy loss, which implies that real-word industrial data mining systems cannot work well under such a strong privacy guarantee recommended by previous research works. Among the three basic DP architectures, the Hybridized DM (Data Mining) and DB (Database) architecture performs the best because of its complicated privacy protection design for the specific data mining algorithm. Through extensive experiments on big data, we also observe that the accuracy loss increases by increasing the variety of features, but decreases by increasing the volume of training data. Therefore, to make DP practically usable in large-scale industrial systems, our observations suggest that we may explore three possible research directions in future: (1) Relaxing the privacy guarantee (e.g., increasing privacy budget \u03b5) and studying its effectiveness on specific industrial applications; (2) Designing specific privacy scheme for specific data mining algorithms; and (3) Using large volume of data but with low variety for training the classification models.}, pages = {1692\u20131703}, numpages = {12}}
@inproceedings{10.1145/3318464.3384677,title = {RASQL: A Powerful Language and its System for Big Data Applications}, author = {Wang Jin , Xiao Guorui , Gu Jiaqi , Wu Jiacheng , Zaniolo Carlo },year = {2020}, isbn = {9781450367356}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3318464.3384677}, doi = {10.1145/3318464.3384677}, abstract = {There is a growing interest in supporting advanced Big Data applications on distributed data processing platforms. Most of these systems support SQL or its dialect as the query interface due to its portability and declarative nature. However, current SQL standard cannot effectively express advanced analytical queries due to its limitation in supporting recursive queries. In this demonstration, we show that this problem can be resolved via a simple SQL extension that delivers greater expressive power by allowing aggregates in recursion. To this end, we propose the Recursive-aggregate-SQL (RASQL) language and its system on top of Apache Spark to express and execute complex queries and declarative algorithms in many applications, such as graph search and machine learning. With a variety of examples, we will (i) show how complicated analytic queries can be expressed with RASQL; (ii) illustrate formal semantics of the powerful new constructs; and (iii) present a user-friendly interface to interact with the RASQL system and monitor the query results.}, location = {Portland, OR, USA}, series = {SIGMOD '20}, pages = {2673\u20132676}, numpages = {4}, keywords = {recursive query, query language, big data}}
@inproceedings{10.14778/2733004.2733045,title = {Interactive outlier exploration in big data streams}, author = {Cao Lei , Wang Qingyang , Rundensteiner Elke A. },year = {2014}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2733004.2733045}, doi = {10.14778/2733004.2733045}, abstract = {We demonstrate our VSOutlier system for supporting interactive exploration of outliers in big data streams. VSOutlier not only supports a rich variety of outlier types supported by innovative and efficient outlier detection strategies, but also provides a rich set of interactive interfaces to explore outliers in real time. Using the stock transactions dataset from the US stock market and the moving objects dataset from MITRE, we demonstrate that the VSOutlier system enables analysts to more efficiently identify, understand, and respond to phenomena of interest in near real-time even when applied to high volume streams.}, pages = {1621\u20131624}, numpages = {4}}
@inproceedings{10.1145/3379247.3379282,title = {Evaluation Method of Equipment Combat Effectiveness Based On Big Data Mining}, author = {Liyao Zhou , Xiaofang Liu , Chunyu Hu },year = {2020}, isbn = {9781450376730}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3379247.3379282}, doi = {10.1145/3379247.3379282}, abstract = {In the evaluation of equipment combat effectiveness, it is necessary to comprehensively analyze the data of outfield test and infield test, including a variety of audio-visual, image and other combat test data. These data can be classified, extracted, stored and managed by building data model through big data mining technology. The evaluation method of equipment combat effectiveness based on big data mining is based on massive data, through machine learning, statistical analysis, neural network, database and other methods to analyze and process the data, mining the correlation between test data, evaluation index and evaluation conclusion, and extracting useful information and finding new knowledge from it to realize the evaluation of the combat effectiveness of the tested system.}, location = {Sanya, China}, series = {ICCDE 2020}, pages = {131\u2013135}, numpages = {5}, keywords = {combat test, combat effectiveness evaluation, Big data mining}}
@inproceedings{10.1145/2674026.2674031,title = {Change detection in streaming data in the era of big data: models and issues}, author = {Tran Dang-Hoan , Gaber Mohamed Medhat , Sattler Kai-Uwe },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2674026.2674031}, doi = {10.1145/2674026.2674031}, abstract = {Big Data is identified by its three Vs, namely velocity, volume, and variety. The area of data stream processing has long dealt with the former two Vs velocity and volume. Over a decade of intensive research, the community has provided many important research discoveries in the area. The third V of Big Data has been the result of social media and the large unstructured data it generates. Streaming techniques have also been proposed recently addressing this emerging need. However, a hidden factor can represent an important fourth V, that is variability or change. Our world is changing rapidly, and accounting to variability is a crucial success factor. This paper provides a survey of change detection techniques as applied to streaming data. The review is timely with the rise of Big Data technologies, and the need to have this important aspect highlighted and its techniques categorized and detailed.}, pages = {30\u201338}, numpages = {9}}
@inproceedings{10.14778/2733004.2733037,title = {Redoop infrastructure for recurring big data queries}, author = {Lei Chuan , Zhuang Zhongfang , Rundensteiner Elke A. , Eltabakh Mohamed Y. },year = {2014}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2733004.2733037}, doi = {10.14778/2733004.2733037}, abstract = {This demonstration presents the Redoop infrastructure, the first full-fledged MapReduce framework with native support for recurring big data queries. Recurring queries, repeatedly being executed for long periods of time over evolving high-volume data, have become a bedrock component in most large-scale data analytic applications. Redoop is a comprehensive extension to Hadoop that pushes the support and optimization of recurring queries into Hadoop's core functionality. While backward compatible with regular MapReduce jobs, Redoop achieves an order of magnitude better performance than Hadoop for recurring workloads. Redoop employs innovative window-aware optimization techniques for such recurring workloads including adaptive window-aware data partitioning, cache-aware task scheduling, and inter-window caching mechanisms. We will demonstrate Redoop's capabilities on a compute cluster against real life workloads including click-stream and sensor data analysis.}, pages = {1589\u20131592}, numpages = {4}}
@inproceedings{10.1145/2342441.2342462,title = {Programming your network at run-time for big data applications}, author = {Wang Guohui , Ng T.S. Eugene , Shaikh Anees },year = {2012}, isbn = {9781450314770}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2342441.2342462}, doi = {10.1145/2342441.2342462}, abstract = {Recent advances of software defined networking and optical switching technology make it possible to program the network stack all the way from physical topology to flow level traffic control. In this paper, we leverage the combination of SDN controller with optical switching to explore the tight integration of application and network control. We particularly study the run-time network configuration for big data applications to jointly optimize application performance and network utilization. We use Hadoop as an example to discuss the integrated network control architecture, job scheduling, topology and routing configuration mechanisms for Hadoop jobs. Our analysis suggests that such an integrated control has great potential to improve application performance with relatively small configuration overhead. We believe our study shows early promise of achieving the long-term goal of tight network and application integration using SDN.}, location = {Helsinki, Finland}, series = {HotSDN '12}, pages = {103\u2013108}, numpages = {6}, keywords = {big data applications, optical circuit switching, software defined networking}}
@inproceedings{10.1145/3265007.3265015,title = {Research on the Innovation of Trajectory Big Data in Social Governance}, author = {Zhang Bin , Zhu Guobin , Yu Riji , Wei Shaoyan , Peng Ling , Fei Dingzhou , Yu Xuesong , Pan Peiwen },year = {2018}, isbn = {9781450365741}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3265007.3265015}, doi = {10.1145/3265007.3265015}, abstract = {With the development of modern society. The unprecedented prosperity of science & technology and finance. Objects formed a huge amount of track data in its movement. The large amount of track data contains rich spatio-temporal characteristics information, it exposes the privacy information such as the behavior characteristics, interests and social habits of mobile objects. Through trajectory data processing technology. It can excavate information such as human activity pattern and behavior characteristic, urban vehicle movement characteristic, atmospheric environment change law and so on. The large amount of track data also reveals the privacy information, such as the behavior characteristics, interests and social habits of mobile objects, which is rich in spatio-temporal characteristics information. This paper begins with the significance of the study of trajectory big data. Introducing track big data acquisition mode and social application in various fields, In the specific application With the development of modern society. The unprecedented prosperity of science & technology and finance. Objects formed a huge amount of track data in its movement. The large amount of track data contains rich spatio-temporal characteristics information, it exposes the privacy information such as the behavior characteristics, interests and social habits of mobile objects. Through trajectory data processing technology. It can excavate information such as human activity pattern and behavior characteristic, urban vehicle movement characteristic, atmospheric environment change law and so on. The large amount of track data also reveals the privacy information, such as the behavior characteristics, interests and social habits of mobile objects, which is rich in spatio-temporal characteristics information. This paper begins with the significance of the study of trajectory big data. Introducing track big data acquisition mode and social application in various fields, In the specific application, we pay more attention to the object's trajectory privacy protection. Applying the big data of trajectory to social governance; In addition, the application of big data in social governance is summarized and the future work prospect is discussed. We pay more attention to the object's trajectory privacy protection. Applying the big data of trajectory to social governance; In addition, the application of big data in social governance is summarized and the future work prospect is discussed.}, location = {Kunming, China}, series = {ACIT 2018}, pages = {38\u201342}, numpages = {5}, keywords = {Social Computing, Social Governance, Trajectory Big Data, Privacy Protection}}
@inproceedings{10.1145/2968332,title = {Ontology-Based Data Quality Management for Data Streams}, author = {Geisler Sandra , Quix Christoph , Weber Sven , Jarke Matthias },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2968332}, doi = {10.1145/2968332}, abstract = {Data Stream Management Systems (DSMS) provide real-time data processing in an effective way, but there is always a tradeoff between data quality (DQ) and performance. We propose an ontology-based data quality framework for relational DSMS that includes DQ measurement and monitoring in a transparent, modular, and flexible way. We follow a threefold approach that takes the characteristics of relational data stream management for DQ metrics into account. While (1) Query Metrics respect changes in data quality due to query operations, (2) Content Metrics allow the semantic evaluation of data in the streams. Finally, (3) Application Metrics allow easy user-defined computation of data quality values to account for application specifics. Additionally, a quality monitor allows us to observe data quality values and take counteractions to balance data quality and performance. The framework has been designed along a DQ management methodology suited for data streams. It has been evaluated in the domains of transportation systems and health monitoring.}, pages = {1\u201334}, numpages = {34}, keywords = {data quality assessment, ontologies, data quality control, Data streams}}
@inproceedings{10.1145/3469213.3470409,title = {Application of big data technology in blockchain computing}, author = {Cai Liya , Yao Shuchun },year = {2021}, isbn = {9781450390200}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3469213.3470409}, doi = {10.1145/3469213.3470409}, abstract = {Under the information age, a large number of structurally complicated and unmanageable data generated by all walks of life is increasingly multiplied. As a newly emerging technology, compared with the traditional data management methods, blockchain is characterized in advantages such as decentralization, de-trustness, and data encryption, which enables it to solve data management problems existing in big data applications in a more efficient way. Since government big data involves huge economic and social values, it is of great significance these curity sharing of government big data for the transformation of the government and social demand patterns. Taking government big data as an example, this paper has analyzed the feasibility of big data security sharing that is based on blockchain, and proposed the demand model and support plan of big data security sharing. Last but not least, it expounded the characteristics of big data security sharing based on blockchain technology, so as to provide beneficial references for the e-government big data security sharing of the government.}, location = {Chongqing, China}, series = {ICAIIS 2021}, pages = {1\u20133}, numpages = {3}}
@inproceedings{10.1145/2903220.2903255,title = {Efficient MapReduce Kernel k-Means for Big Data Clustering}, author = {Tsapanos Nikolaos , Tefas Anastasios , Nikolaidis Nikolaos , Pitas Ioannis },year = {2016}, isbn = {9781450337342}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2903220.2903255}, doi = {10.1145/2903220.2903255}, abstract = {Data clustering is an unsupervised learning task that has found many applications in various scientific fields. The goal is to find subgroups of closely related data samples (clusters) in a set of unlabeled data. A classic clustering algorithm is the so-called k-Means. It is very popular, however, it is also unable to handle cases in which the clusters are not linearly separable. Kernel k-Means is a state of the art clustering algorithm, which employs the kernel trick, in order to perform clustering on a higher dimensionality space, thus overcoming the limitations of classic k-Means regarding the non linear separability of the input data. It has recently received a distributed implementation, named Trimmed Kernel k-Means, following the MapReduce distributed computing model. In addition to performing the computations in a distributed manner, Trimmed Kernel k-Means also trims the kernel matrix, in order to reduce the memory requirements and improve performance. The trimming of each row of the kernel matrix is achieved by attempting to estimate the cardinality of the cluster that the corresponding sample belongs to, and removing the kernel matrix entries connecting the sample to samples that probably belong to another cluster. The Spark cluster computing framework was used for the distributed implementation. In this paper, we present a distributed clustering scheme that is based on Trimmed Kernel k-Means, which employs subsampling, in order to be able to efficiently perform clustering on an extremely large dataset. The results indicate that the proposed method run much faster than the original Trimmed Kernel k-Means, while still providing clustering performance competitive with other state of the art kernel approaches.}, location = {Thessaloniki, Greece}, series = {SETN '16}, pages = {1\u20135}, numpages = {5}, keywords = {Kernel k-Means, clustering, Big Data, MapReduce, distributed computing}}
@inproceedings{10.1145/3127479.3132685,title = {Revisiting performance in big data systems: an resource decoupling approach}, author = {Yang Chen , Guo Qi , Meng Xiaofeng , Xin Rihui , Wang Chunkai },year = {2017}, isbn = {9781450350280}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3127479.3132685}, doi = {10.1145/3127479.3132685}, abstract = {Big data systems for large-scale data processing are now in widespread use. To improve their performance, both academia and industry have expended a great deal of effort in the analysis of performance bottlenecks. Most big data systems, as Hadoop and Spark, allow distributed computing across clusters. As a result, the execution of systems always parallelizes the use of the CPU, memory, disk and network. If a given resource has the greatest limiting impact on performance, systems will be bottlenecked on it. For a system designer, it is effective for the improvement of performance to tune the bottleneck resource. The key point for the aforementioned scenario is how to determine the bottleneck resource. The nature clue is to quantify the impact of the four major components and identify one causing the greatest impact factor as the bottleneck resource.}, location = {Santa Clara, California}, series = {SoCC '17}, pages = {639}, numpages = {1}}
@inproceedings{10.1145/3093338.3093351,title = {Insights into Research Computing Operations using Big Data-Powered Log Analysis}, author = {Liu Fang Cherry , Xu Weijia , Belgin Mehmet , Huang Ruizhu , Fleischer Blake C. },year = {2017}, isbn = {9781450352727}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3093338.3093351}, doi = {10.1145/3093338.3093351}, abstract = {Research computing centers provide a wide variety of services including large-scale computing resources, data storage, high-speed interconnect and scientific software repositories to facilitate continuous competitive research. Efficient management of these complex resources and services, as well as ensuring their fair use by a large number of researchers from different scientific domains are key to a center's success. Almost all research centers use monitoring services based on real time data gathered from systems and services, but often lack tools to perform a deeper analysis on large volumes of historical logs for identifying insightful trends from recurring events. The size of collected data can be massive, posing significant challenges for the use of conventional tools for this kind of analysis. This paper describes a big data pipeline based on Hadoop and Spark technologies, developed in close collaboration between TACC and Georgia Tech. This data pipeline is capable of processing large volumes of data collected from schedulers using PBSTools, making it possible to run a deep analysis in minutes as opposed to hours with conventional tools. Our component-based pipeline design adds the flexibility of plugging in different components, as well as promotes data reuse. Using this data pipeline, we demonstrate the process of formulating several critical operational questions around researcher behavior, systems health, operational aspects and software usage trends, all of which are critical factors in determining solutions and strategies for efficient management of research computing centers.}, location = {New Orleans, LA, USA}, series = {PEARC17}, pages = {1\u20138}, numpages = {8}, keywords = {log analysis, hadoop, big data, spark}}
@inproceedings{10.1145/3491204.3527473,title = {Analysis of Garbage Collection Patterns to Extend Microbenchmarks for Big Data Workloads}, author = {Sarnayak Samyak S. , Ahuja Aditi , Kesavarapu Pranav , Naik Aayush , Kumar V. Santhosh , Kalambur Subramaniam },year = {2022}, isbn = {9781450391597}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3491204.3527473}, doi = {10.1145/3491204.3527473}, abstract = {Java uses automatic memory allocation where the user does not have to explicitly free used memory. This is done by the garbage collector. Garbage Collection (GC) can take up a significant amount of time, especially in Big Data applications running large workloads where garbage collection can take up to 50 percent of the application's run time. Although benchmarks have been designed to trace garbage collection events, these are not specifically suited for Big Data workloads, due to their unique memory usage patterns. We have developed a free and open source pipeline to extract and analyze object-level details from any Java program including benchmarks and Big Data applications such as Hadoop. The data contains information such as lifetime, class and allocation site of every object allocated by the program. Through the analysis of this data, we propose a small set of benchmarks designed to emulate some of the patterns observed in Big Data applications. These benchmarks also allow us to experiment and compare some Java programming patterns.}, location = {Bejing, China}, series = {ICPE '22}, pages = {121\u2013128}, numpages = {8}, keywords = {garbage collection, java, hadoop, java virtual machine, big data}}
@inproceedings{10.1145/3097983.3105810,title = {Big Data in Climate: Opportunities and Challenges for Machine Learning}, author = {Karpatne Anuj , Kumar Vipin },year = {2017}, isbn = {9781450348874}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3097983.3105810}, doi = {10.1145/3097983.3105810}, abstract = {The climate and Earth sciences have recently undergone a rapid transformation from a data-poor to a data-rich environment. In particular, massive amount of data about Earth and its environment is now continuously being generated by a large number of Earth observing satellites as well as physics-based earth system models running on large-scale computational platforms. These massive and information-rich datasets offer huge potential for understanding how the Earth's climate and ecosystem have been changing and how they are being impacted by humans actions. We discuss the challenges involved in analyzing these massive data sets as well as opportunities they present for both advancing machine learning as well as the science of climate change.}, location = {Halifax, NS, Canada}, series = {KDD '17}, pages = {21\u201322}, numpages = {2}, keywords = {machine learning, earth observation data, climate science}}
@inproceedings{10.1145/3436286.3436400,title = {Research on the Influence of the WeChat Official Account in Wuhan Garden Expo Park Scenic Area in the Era of Big Data}, author = {Lu Wanting },year = {2020}, isbn = {9781450376457}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3436286.3436400}, doi = {10.1145/3436286.3436400}, abstract = {With the rapid development of the Internet and the popularity of smart mobile terminals, individuals can publish and access data anytime, anywhere, accurately and quickly, which makes the scale and type of data grow rapidly, and the era of big data has come quietly. At present, WeChat Official Account has become a platform for various industries to carry out marketing activities by virtue of its advantages of accuracy, speed and effectiveness, and tourist attractions have also opened WeChat Official Account to carry out marketing publicity activities. Wuhan Garden Expo Park officially opened on September 26, 2015, is one of the 12 Garden Expo Parks in China, its WeChat Official Account has been opened and operated so far, although it has a certain social influence, but there is still room for improvement. This paper, with Wuhan Garden Expo Park WeChat Official Account as the research object, first introduced the overview of Wuhan Garden Expo Park WeChat Official Account, based on the WeChat Communication Index WCI of Wuhan Park Expo Park influence analysis of the influence of WeChat Official Account, explore the constraints to enhance its influence, finally proposed the design of iconic avatar, stable tweet frequency, adjusting the content and boosting quantity of tweets, in order to further enhance the influence of WeChat Official Account of Wuhan Garden Expo Park Scenic Area and better serve users.}, location = {Johannesburg, South Africa}, series = {ISBDAI '20}, pages = {249\u2013254}, numpages = {6}, keywords = {Wuhan Garden Expo Park Scenic Area, WeChat Official Account, WeChat Communication Index, Influence, Big Data Era}}
@inproceedings{10.1145/3284103.3284123,title = {Using Big Data Analytics to Build Prosperity Index of Transportation Market}, author = {Li Tao },year = {2018}, isbn = {9781450360449}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3284103.3284123}, doi = {10.1145/3284103.3284123}, abstract = {As the transportation services represented by DiDi have entered the mobile Internet, the data volume of transportation services on various network platforms and social media has increased dramatically, which indicates that the era of big data of transportation system has come. This paper proposes a transportation market prosperity index system based on big data analytics. First, the construction principle of the prosperity index is established; second, the quintessential prosperity indexes are selected; third, we formulate the index data acquisition method, data processing and index calculation method; fourth, the calculation of transportation diffusion index and traffic composite index is developed. The contribution of this paper is that we can grasp the degree of economic development of the transportation market from both qualitative and quantitative perspectives and show its development trend based on the mining and analysis of big data on the Internet. Then, we can provide foundation for the government to formulate relative policies and decision-making for relevant enterprises.}, location = {Seattle, WA, USA}, series = {Safety and Resilience'18}, pages = {1\u20136}, numpages = {6}, keywords = {Big data analytics, Prosperity index, Transportation market}}
@inproceedings{10.1145/3264560.3264567,title = {Usability of Big Data Resources in Visual Search Interfaces of Repositories Based on KOS}, author = {Gaona-Garc\u00eda Paulo Alonso , Mart\u00edn-Moncunill David , Gaona-Garc\u00eda Elvis Eduardo , G\u00f3mez-Acosta Adriana , Monenegro-Marin Carlos },year = {2018}, isbn = {9781450364744}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3264560.3264567}, doi = {10.1145/3264560.3264567}, abstract = {Digital repositories allow storage and manage digital resources and collections of museums, libraries, archives in order to be use in educational context. Unfortunately, several deficiencies in user interfaces based on resource discovery, user-centered design, and strategy of search, among others, prevent the widespread use of the valuable services that data of repository offers. Having the intuition that some deficiencies are reflected in usability problems associated with interfaces, we conducted a research from the Human Computer Interaction (HCI) perspective in order to present a novel framework to evaluate usability of different types of visual interfaces based on visualization techniques and Knowledge Organization System (KOS). Our study analyzed the efficacy of a framework in order to allow repository creators the assessment and selection of appropriate user interfaces according to the needs and demands of the data collection of learning objects. The preliminary results show that framework could improve the select of appropriate visualization techniques after to development of them in a digital repository. Although, some problems associated with the limited computational capabilities for information visualization are difficult to overcome.}, location = {Barcelona, Spain}, series = {ICCBDC'18}, pages = {33\u201337}, numpages = {5}, keywords = {user interfaces, information visualization, human computer interface, Digital repositories, visualization techniques, visual search interfaces}}
@inproceedings{10.1145/2213598.2213606,title = {Data quality and integration in collaborative environments}, author = {Endler Gregor },year = {2012}, isbn = {9781450313261}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2213598.2213606}, doi = {10.1145/2213598.2213606}, abstract = {The trend to merge medical practices into cooperatively operating networks and organizational units like Medical Supply Centers generates new challenges for an adequate IT support. In particular, new use cases for common economic planning, controlling and treatment coordination arise. This requires consolidation of data originating from heterogeneous and autonomous software systems. Heterogeneity and autonomy are core reasons for low data quality. The intuitive approach of initially integrating heterogeneous systems into a federated system creates a very high upfront effort before the system can become operable and does not adequately consider the fact that data quality requirements might change over time. To remedy this, we propose an approach for continuous data quality improvement which enables a demand driven step by step system integration. By adapting the generic Total Data Quality Management process to healthcare specific use cases, we are developing an extended model for continuous data quality management in cooperative healthcare settings. The IT tools which are needed to provide the information that drives this process are currently in development within a government supported project involving both industry and academia.}, location = {Scottsdale, Arizona, USA}, series = {PhD '12}, pages = {21\u201326}, numpages = {6}, keywords = {data quality management, demand-driven improvement, co-operative healthcare delivery}}
@inproceedings{10.1145/3344948.3344987,title = {Towards an architecture for big data analytics leveraging edge/fog paradigms}, author = {D\u00edaz-de-Arcaya Josu , Mi\u00f1on Ra\u00fcl , Torre-Bastida Ana I. },year = {2019}, isbn = {9781450371421}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3344948.3344987}, doi = {10.1145/3344948.3344987}, abstract = {An industry transformation is being boosted by Big Data and Cloud technologies. We present a Big Data architecture, which expands the life cycle of data processing through the Edge, Fog and Cloud computing layers. The proposed architecture takes advantage of the strengths of each: the Cloud layer executes heavy analytical processes, the Fog is responsible for the ingestion and performing aggregations, and the Edge manages devices and actuators. The proposed architecture tackles two main goals, 1) latencies and response times can be reduced by bringing the analytics closer to where the data is generated and 2) the use of computing resources is optimised. In order to conceptualise this architecture, an orchestration module is proposed with the goal of optimising the deployment of analytical workloads across the three layers, by evaluating their computing resources. In addition to this, another module is designed to monitor the performance of such workloads allowing the redistribution of tasks assigned to each node. These modules will be implemented in a real case scenario in the train domain.}, location = {Paris, France}, series = {ECSA '19}, pages = {173\u2013176}, numpages = {4}, keywords = {data analytics, predictive maintenance, cloud computing, fog computing, IoT, big data}}
@inproceedings{10.1145/3341069.3342996,title = {Research on Knowledge Management Technology of Aerospace Engineering Based on Big Data}, author = {Liu Jun },year = {2019}, isbn = {9781450371858}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341069.3342996}, doi = {10.1145/3341069.3342996}, abstract = {In the era of big data, mass production, analysis and application of data have become a new trend. In the long-term design, production, operation and testing process of aerospace enterprises, a large number of valuable data have been generated. Collection and analysis of these data can improve the management of aerospace enterprises and gain competitive advantages. With the increase of semi-structured and unstructured data produced by aerospace enterprises year by year, how to store and analyze data, how to mine and share knowledge has become a major problem. The existing knowledge management system cannot meet the diversified needs of users only by traditional database technology. It also needs to combine distributed computing and storage technology to solve the problems of knowledge storage, knowledge sharing, knowledge mining, knowledge retrieval and recommendation in big data environment. Aerospace enterprises need to build a knowledge management system based on big data technology to support knowledge innovation and knowledge application. From the perspective of data operation and relying on Hadoop ecosystem related big data technology, this paper constructs a knowledge management framework model for aerospace enterprises based on Hadoop.}, location = {Guangzhou, China}, series = {HPCCT '19}, pages = {172\u2013176}, numpages = {5}, keywords = {Hadoop, knowledge management system, knowledge management, Big data}}
@inproceedings{10.1145/3314221.3314650,title = {Panthera: holistic memory management for big data processing over hybrid memories}, author = {Wang Chenxi , Cui Huimin , Cao Ting , Zigman John , Volos Haris , Mutlu Onur , Lv Fang , Feng Xiaobing , Xu Guoqing Harry },year = {2019}, isbn = {9781450367127}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3314221.3314650}, doi = {10.1145/3314221.3314650}, abstract = {Modern data-parallel systems such as Spark rely increasingly on in-memory computing that can significantly improve the efficiency of iterative algorithms. To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy-inefficient. Emerging non-volatile memory (NVM) technologies offers high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages (e.g., Scala and Java) and executed on top of a managed runtime (e.g., the Java Virtual Machine) that already performs various dimensions of memory management. Supporting hybrid physical memories adds in a new dimension, creating unique challenges in data replacement and migration. This paper proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed down to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division is accurate enough to guide GC for data layout, which hardly incurs data monitoring and moving overhead. We have implemented Panthera in OpenJDK and Apache Spark. An extensive evaluation with various datasets and applications demonstrates that Panthera reduces energy by 32 \u2013 52% at only a 1 \u2013 9% execution time overhead.}, location = {Phoenix, AZ, USA}, series = {PLDI 2019}, pages = {347\u2013362}, numpages = {16}, keywords = {memory management, garbage collection, Big Data systems, hybrid memories}}
@inproceedings{10.1145/3070607.3075961,title = {Towards minimal algorithms for big data analytics with spreadsheets}, author = {Sroka Jacek , Le\u015bniewski Artur , Kowaluk Miros\u0142aw , Stencel Krzysztof , Tyszkiewicz Jerzy },year = {2017}, isbn = {9781450350198}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3070607.3075961}, doi = {10.1145/3070607.3075961}, abstract = {The motivation for our research is the need for a simple and accessible method to process large datasets available to the public. We aim to significantly lower the technological barrier, which currently prevents average users from analyzing big datasets available as CSV files. Spreadsheets are perfectly suited for this task, being the most popular and accessible data analysis tool. We present ideally balanced algorithms for specifying MapReduce computations of high number of range queries. With our algorithms, it will be possible to automatically perform analysis defined with a spreadsheet on data of size exceeding the dimensions of the spreadsheet grid by orders of magnitude.}, location = {Chicago, IL, USA}, series = {BeyondMR'17}, pages = {1\u20134}, numpages = {4}, keywords = {minimal algorithms, MapReduce, range queries, Big data, spreadsheet, Hadoop}}
@inproceedings{10.1145/2908216.2908222,title = {Big data and social netbanks: what happens when tech companies become financial companies?}, author = {Geslevich-Packin Nizan , Lev-Aretz Yafit },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2908216.2908222}, doi = {10.1145/2908216.2908222}, abstract = {Social netbanks hold much potential as alternative financial institutions that could better serve marginalized populations. But they bring with them a number of regulatory complications and informational risks. We explore the rise of big data and social netbanks, and describe some of the challenges they present.}, pages = {36\u201340}, numpages = {5}, keywords = {social networks, banks, netbanks, regulation}}
@inproceedings{10.1145/3341620.3341624,title = {Study on Standard System of Aerospace Quality Data Resources Integration under the Background of Big Data}, author = {Jia Fengsheng , Gao Yang , Wang Yuming },year = {2019}, isbn = {9781450360913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3341620.3341624}, doi = {10.1145/3341620.3341624}, abstract = {The integration and application of aerospace product quality data resources is an important way to carry out quality improvement, quality evaluation and precise management. Standardization is the basis for promoting quality data resources integration. The unified and normative standard system is the guarantee for efficient development of integration standards. Firstly, we analyzed the features of quality data resources according to the status quo of integration. Integration structure of quality data resources in terms of vertical and horizontal integration was proposed by adopting the methods of \"decomposition-integration\" and \"classification-association\". Secondly, we constructed a three-dimensions architecture of quality data resource integration using the method of system engineering methodology, from the layer dimension (basis, common, special), technical dimension (description, collection, storage, transmission, processing, comprehensive management) and category dimension (rocket, spacecraft). Thirdly, we worked out 20 lists about basis and common standard by adopting the top-down approach. Some standard development suggestions are proposed based on the characteristics of quality data resources and standard research strategies. Finally, we applied the quality problem data resource standard construction and application to verify the proposed method.}, location = {Hong Kong, Hong Kong}, series = {BDE 2019}, pages = {16\u201322}, numpages = {7}, keywords = {aerospace products, system planning, quality data resource integration, standard system}}
@inproceedings{10.1145/3468264.3473135,title = {Learning type annotation: is big data enough?}, author = {Jesse Kevin , Devanbu Premkumar T. , Ahmed Toufique },year = {2021}, isbn = {9781450385626}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3468264.3473135}, doi = {10.1145/3468264.3473135}, abstract = {TypeScript is a widely used optionally-typed language where developers can adopt \u201cpay as you go\u201d typing: they can add types as desired, and benefit from static typing. The \u201ctype annotation tax\u201d or manual effort required to annotate new or existing TypeScript can be reduced by a variety of automatic methods. Probabilistic machine-learning (ML) approaches work quite well. ML approaches use different inductive biases, ranging from simple token sequences to complex graphical neural network (GNN) models capturing syntax and semantic relations. More sophisticated inductive biases are hand-engineered to exploit the formal nature of software. Rather than deploying fancy inductive biases for code, can we just use \u201cbig data\u201d to learn natural patterns relevant to typing? We find evidence suggesting that this is the case. We present TypeBert, demonstrating that even with simple token-sequence inductive bias used in BERT-style models and enough data, type-annotation performance of the most sophisticated models can be surpassed.}, location = {Athens, Greece}, series = {ESEC/FSE 2021}, pages = {1483\u20131486}, numpages = {4}, keywords = {TypeScript, transfer learning, Type inference, deep learning}}
@inproceedings{10.1145/3424978.3425000,title = {Research on Multi-source Heterogeneous Big Data in Extra-large Enterprises}, author = {Yuan Lufeng , Gao Xiaoxin , Wang Sining , Wang Jun },year = {2020}, isbn = {9781450377720}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3424978.3425000}, doi = {10.1145/3424978.3425000}, abstract = {Extra-large enterprises, due to their huge scale and complex businesses, face serious challenges in the big data time. This paper introduces the Operating and Monitoring Information System (OMIS) in the State Grid Corporation of China to try to use big data in the extra-large enterprises. OMIS consists of full coverage data flow path, compound general library of model and algorithm, multi-mode computing platform and interface components. It solves a series of key problems that are data barrier, transmission, analysis, computing, usability in extra large enterprises. OMIS has connected headquarters, provinces and cities, covered 27 provinces. Benefited from interface components, a programme for data extraction, model train and parallel computing can be implemented by several codes conveniently in OMIS. In the experiments, OMIS can extract 1.73TB line loss data in about 93 hours from 27 provinces, provide multiple algorithms to detect abnormal low-voltage substation areas, support high performance computing by parallelization. Finally, OMIS reaches 3.71 speedup ratio on five nodes.}, location = {Sanya, China}, series = {CSAE 2020}, pages = {1\u20135}, numpages = {5}, keywords = {Parallelization, Multi-source, Operating, Monitoring, Big data, Data extraction, Heterogeneous, Extra-large enterprise}}
@inproceedings{10.1145/3510858.3510975,title = {Application Analysis of Hadoop in Big Data Processing}, author = {Zheng Zhiming },year = {2021}, isbn = {9781450390422}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510858.3510975}, doi = {10.1145/3510858.3510975}, abstract = {How to use big data technology to effectively excavate and identify data information contained in user behaviors and further innovate services has become a development trend of the Internet big data. With the further increase of the amount of data, the configuration parameters involved further increase, and the optimization of configuration parameters has become the main bottleneck limiting the performance of MapReduce. Hadoop configuration involves many parameters, which have a great impact on the running jobs. These parameters just determine the overall performance of the cluster. This paper uses Hadoop technology to the Internet big data combining the optimization model. The construction process of Hadoop cluster environment is described in detail. Hadoop is applied to a file publishing system. For files of different orders of magnitude, the time-consuming operation of file upload is compared when the number of clusters is different. The experimental results show that the larger the amount of data and the number of cluster nodes, the stronger the ability of Hadoop cluster to process data. The results prove that this method effectively can solve the problems of the complex information of big data and improve the service efficiency for big data processing.}, location = {Changsha, China}, series = {ICASIT 2021}, pages = {405\u2013408}, numpages = {4}}
@inproceedings{10.1109/UCC.2014.101,title = {Governance Strategies for the Cloud, Big Data, and Other Technologies in Education}, author = {Self Richard J. },year = {2014}, isbn = {9781479978816}, publisher = {IEEE Computer Society}, address = {USA}, url = {https://doi.org/10.1109/UCC.2014.101}, doi = {10.1109/UCC.2014.101}, abstract = {The Cloud, Big Data and many emerging technologies are now being considered by many educational establishments as candidates for deriving benefits for both students in their learning and also for the organisation in terms of more effective and efficient operation. This paper considers the governance strategies which need to be developed and implemented in order to ensure that the technologies can be safely incorporated into the technical and operational infrastructure. It demonstrates that synthesising ISO 27002 with a new framework of 12 Vs of Big Data provides an effective approach to identifying some important aspects of new technologies that do not naturally arise from traditional frameworks.}, series = {UCC '14}, pages = {630\u2013635}, numpages = {6}, keywords = {governance, big data, emerging technology, strategy, education, cloud}}
@inproceedings{10.1145/3383913.3383917,title = {Research on Copyright Protection of Digital Publishing in the Era of Big Data}, author = {Ruoyu Ren , Shulin Yang , Qi Zhang },year = {2019}, isbn = {9781450372879}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3383913.3383917}, doi = {10.1145/3383913.3383917}, abstract = {With the development of network information technology, the era of big data brings new opportunities for digital publishing industry. Big data publishing has become one of the trends of future publishing, and has become the core competitiveness of publishing industry, and also brings new challenges for digital publishing. Due to the increasing popularity of data sharing in the era of big data, the problem of network infringement has also been gradually paid attention to, so the research on copyright protection is very important. Based on the characteristics and current situation of the development of digital publishing in the big data environment, this paper analyzes in detail the main problems and difficulties of copyright protection in the big data era. On this basis, this paper puts forward the era of big data is copyright protection of digital publishing strategy, in order to further improve the construction of copyright protection of digital publishing system.}, location = {London, United Kingdom}, series = {IHIP 2019}, pages = {39\u201343}, numpages = {5}, keywords = {copyright protection, big data, digital publishing}}
@inproceedings{10.1145/2896825.2896831,title = {Providing big data applications with fault-tolerant data migration across heterogeneous NoSQL databases}, author = {Scavuzzo Marco , Tamburri Damian A. , Di Nitto Elisabetta },year = {2016}, isbn = {9781450341523}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2896825.2896831}, doi = {10.1145/2896825.2896831}, abstract = {The recent growing interest on highly-available data-intensive applications sparked the need for flexible and portable storage technologies, e.g., NoSQL databases. Unfortunately, the lack of standard interfaces and architectures for NoSQLs makes it difficult and expensive to create portable applications, which results in vendor lock-in. Building on previous work, we aim at providing guaranteed fault-tolerant techniques and supporting architectures to port or migrate data to and across heterogeneous NoSQL technology. To prove the effectiveness of our approach we evaluate it on an industrial case-study. We conclude that our method and supporting architecture offer an efficient and fault-tolerant mechanism for NoSQL portability and interoperation.}, location = {Austin, Texas}, series = {BIGDSE '16}, pages = {26\u201332}, numpages = {7}}
@inproceedings{10.1145/2688072,title = {Big data needs approximate computing: technical perspective}, author = {Nair Ravi },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2688072}, doi = {10.1145/2688072}, pages = {104}, numpages = {1}}
@inproceedings{10.1145/3469213.3470709,title = {Optimal Quantization for Big Data Based on the Dynamic Programming}, author = {Li Punan },year = {2021}, isbn = {9781450390200}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3469213.3470709}, doi = {10.1145/3469213.3470709}, abstract = {This paper presents an optimal source data quantization approach. The corresponding algorithm in this paper can obtain the optimal result for source symbols by considering the correlation between source symbol values, and overcome the defect that the context quantization of minimum description length is only applicable to binary cases. We use a tree structure to represent the context quantization process. The similar measure is description length which is used to present the conditional probability model for judgment. For merging of some conditional probability distributions, the optimal tree structure with the smallest description length can be found. With the help of dynamic programming, our algorithm can get the optimized models. The experimental results are: under the reasonable computational complexity, the results of this algorithm are similar to or better than those of previous algorithms.}, location = {Chongqing, China}, series = {ICAIIS 2021}, pages = {1\u20134}, numpages = {4}, keywords = {DP Programming, quantization, Big data}}
@inproceedings{10.1145/2025528.2025537,title = {Big data from the built environment}, author = {Khan Azam , Hornb\u00e6k Kasper },year = {2011}, isbn = {9781450309240}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2025528.2025537}, doi = {10.1145/2025528.2025537}, abstract = {As sensor networks in buildings continue to grow in number and heterogeneity, occupants can become empowered to better control their environment for comfort maximization and energy minimization. Since buildings are the primary consumers of energy and are the dominant cause of greenhouse gases, apps that help occupants to understand and control their interactions with a building could be extremely beneficial to society. However, the massive raw data sets that could be collected must be aggregated and visualized to be usable which presents significant data handling, information visualization, and interaction challenges. In the context of Project Dasher, a prototype building site for exploring these issues, we discuss lessons learned and challenges ahead to develop ubiquitous computing support for sustainability.}, location = {Beijing, China}, series = {LARGE '11}, pages = {29\u201332}, numpages = {4}, keywords = {massive data sets, app, sustainability, augmented reality, building information model, data aggregation}}
@inproceedings{10.1145/3175628.3175647,title = {Towards a big data analytics framework for smart cities}, author = {Abbad Hicham , Bouchaib Radi },year = {2017}, isbn = {9781450352116}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3175628.3175647}, doi = {10.1145/3175628.3175647}, abstract = {Due to the emergence of the Internet of Things, social network and the mobile applications, the data produced is very important and diversified. These data are very useful to design a smart city based on machine learning and statistical algorithms used by real time analytics or predictive analytics. This paper presents a framework headlines that helps cities to take profit from data to improve the life quality of citizens in several areas: mobility and transportation, economy, environment and energy, public safety, health, education and others.This framework describe different data sources (IoT, software applications, sensors,..) used to generate the information. It also describes, the processing steps that make data usable, the different machine learning statistical algorithms and data mining techniques used to extract the insights. This framework, combine two complementary approaches: Techno-centric approach based on a city populated by sensors that collect a massive data to drive all the urban services, and the collaborative approach that addresses citizens more directly.}, location = {Tangier, Morocco}, series = {SCAMS '17}, pages = {1\u20135}, numpages = {5}, keywords = {smart cities, internet of things (IoT), machine learning, big data, analytics}}
@inproceedings{10.1145/3319619.3322045,title = {Multi-GPU approach for big data mining: global induction of decision trees}, author = {Jurczuk Krzysztof , Czajkowski Marcin , Kretowski Marek },year = {2019}, isbn = {9781450367486}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3319619.3322045}, doi = {10.1145/3319619.3322045}, abstract = {This paper identifies scalability bounds of the evolutionary induced decision trees (DT)s. In order to conquer the barriers concerning the large-scale data we propose a novel multi-GPU approach. It incorporates the knowledge of the global DT induction and EA parallelization. The search for a tree structure and tests is performed sequentially by a CPU, while the fitness calculations are delegated to GPUs, thus the core evolution is unchanged. The results show that the evolutionary induction is accelerated several thousand times by using up to 4 GPUs on datasets with up to 1 billion objects.}, location = {Prague, Czech Republic}, series = {GECCO '19}, pages = {175\u2013176}, numpages = {2}, keywords = {graphics processing unit (GPU), parallel computing, evolutionary data mining, big data, decision trees, scalability bounds, CUDA}}
@inproceedings{10.1145/3318265.3318268,title = {A unified scaling model in the era of big data analytics}, author = {Li Zhongwei , Duan Feng , Che Hao },year = {2019}, isbn = {9781450366380}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3318265.3318268}, doi = {10.1145/3318265.3318268}, abstract = {As scale-out execution of big data analytics has become predominate datacenter workloads, it is of paramount importance to faithfully characterize the scaling properties for such workloads. To date, the most widely cited scaling laws for big data analytics is the traditional Amdahl's law, which was discovered well before the era of big data analytics. A key observation made in this paper is that both the system and workload models underlying the traditional scaling laws are too simplistic to fully characterize the scaling properties for big data analytics workloads. In this paper, we put forward a Unified Scaling model for Big data Analytics (USBA), based on a multi-stage system model and a discretized workload model. USBA allows for flexible workload scaling unifying the fixed-size and fixed-time workload models underlying Amdahl's and Gustafson's laws, respectively, and flexible system scaling in terms of both number of stages and degree of parallelism per stage. Moreover, to faithfully characterize the scaling properties for big data analytics workloads, USBA accounts for variabilities of task response times and barrier synchronization. Finally, application of USBA to the scaling analysis of four Spark-based data mining and graph benchmarks demonstrates that USBA is able to adequately characterize the scaling design space and predict the scaling properties of real-world big data analytics workloads. This makes it possible to use USBA as a useful tool to facilitate job resource provisioning for big data analytics in datacenters.}, location = {Xi&apos;an, China}, series = {HP3C '19}, pages = {67\u201377}, numpages = {11}, keywords = {spark, MapReduce, performance modeling, big data analytics, Amdahl's law, Gustafson's law}}
@inproceedings{10.1145/2663204.2669985,title = {Smart Multimodal Interaction through Big Data}, author = {Tosun Cafer },year = {2014}, isbn = {9781450328852}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2663204.2669985}, doi = {10.1145/2663204.2669985}, abstract = {Smart phones and mobile technologies have changed software usage dramatically. Ease of use and simplicity has made software accessible to a huge number of users. In addition, technological advancements in multimodal interaction are opening new frontiers in software. Users are interacting with software systems through multiple channels such as gestures and speech. Touch screens, cameras, sensors, and wearable devices are enablers of this interaction. The user expectation is that the interaction with business software also becomes as simple as the interaction with consumer software. In particular, through the usage of mobile devices, consumer and business software is coming closer together. Next generation software systems and applications will have to enable smart, seamless and contextual multimodal interaction capabilities. New tools, technologies and solutions will be required to increase the ease of use and to build the user experience of the future.}, location = {Istanbul, Turkey}, series = {ICMI '14}, pages = {282}, numpages = {1}, keywords = {augmented reality, user experience, smart city, human-computer interaction, connected living, interactive software, user-centered software, sap hana, internet of things}}
@inproceedings{10.1145/3456887.3457506,title = {Construction of Big Data Security Early Warning Visualization Model for Smart Tourism}, author = {Sun Yan , Zhao Jing , Yin Weiwei },year = {2021}, isbn = {9781450389969}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456887.3457506}, doi = {10.1145/3456887.3457506}, abstract = {With the rapid development of Internet of things and communication technology and the arrival of big data era, the construction of smart tourism becomes possible. The construction of tourism destination smart tourism platform is in line with the current strategic goal of China's tourism development. Based on the background of big data, this paper expounds the connotation of big data and smart tourism. This paper analyzes the development and existing problems of the existing smart tourism platform, and constructs a data service platform for the purpose of smart tourism prediction and feedback. According to the data platform, this paper proposes to build a security early warning mechanism for scenic spots based on Tourism big data, which can realize the interconnection between inside and outside the scenic spots. The system can predict the hot degree and tourist saturation of scenic spots in advance, and expand the space of safety warning mechanism. It can further build the safety emergency mechanism, rescue mechanism and security mechanism of the scenic spot, so as to realize the safe and normal operation of the scenic spot.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2021}, pages = {1273\u20131277}, numpages = {5}, keywords = {Smart Tourism Platform, Security Early Warning, Big Data, Hot Spots}}
@inproceedings{10.1145/3419635.3419675,title = {Research on Humanistic Value of Labor Practice Education under Big Data Technology}, author = {Liu Yan },year = {2020}, isbn = {9781450387729}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3419635.3419675}, doi = {10.1145/3419635.3419675}, abstract = {The goal of humanistic value of big data technology is to promote the development of individual freedom, inner balance, comprehensive education and cultural harmony. According to the present situation of big data technology, this paper makes a preliminary exploration on the way to realize the humanistic value in the big data labor practice education. In other words, the humanistic value of big data technology can be realized through three ways: persisting in putting people first, optimizing the technological environment of big data, and integrating the technological resources of big data. Through the above research on the humanistic value of big data technology, this paper applies the application of big data algorithm in labor practice to make it better serve human beings.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2020}, pages = {359\u2013362}, numpages = {4}, keywords = {Humanistic value, Labor practice, Big data technology}}
@inproceedings{10.1145/2389686.2389688,title = {When big data leads to lost data}, author = {Megler V. M. , Maier David },year = {2012}, isbn = {9781450317191}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2389686.2389688}, doi = {10.1145/2389686.2389688}, abstract = {For decades, scientists bemoaned the scarcity of observational data to analyze and against which to test their models. Exponential growth in data volumes from ever-cheaper environmental sensors has provided scientists with the answer to their prayers: \"big data\". Now, scientists face a new challenge: with terabytes, petabytes or exabytes of data at hand, stored in thousands of heterogeneous datasets, how can scientists find the datasets most relevant to their research interests? If they cannot find the data, then they may as well never have collected it; that data is lost to them. Our research addresses this challenge, using an existing scientific archive as our test-bed. We approach this problem in a new way: by adapting Information Retrieval techniques, developed for searching text documents, into the world of (primarily numeric) scientific data. We propose an approach that uses a blend of automated and \"semi-curated\" methods to extract metadata from large archives of scientific data. We then perform searches over the extracted metadata, returning results ranked by similarity to the query terms. We briefly describe an implementation performed at an ocean observatory to validate the proposed approach. We propose performance and scalability research to explore how continued archive growth will affect our goal of interactive response, no matter the scale.}, location = {Maui, Hawaii, USA}, series = {PIKM '12}, pages = {1\u20138}, numpages = {8}, keywords = {ranked data search, scientific data}}
@inproceedings{10.1145/3357384.3358124,title = {A Sampling-Based System for Approximate Big Data Analysis on Computing Clusters}, author = {Salloum Salman , Wu Yinxu , Huang Joshua Zhexue },year = {2019}, isbn = {9781450369763}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3357384.3358124}, doi = {10.1145/3357384.3358124}, abstract = {To break the in-memory bottleneck and facilitate online sampling in cluster computing frameworks, we propose a new sampling-based system for approximate big data analysis on computing clusters. We address both computational and statistical aspects of big data across the main layers of cluster computing frameworks: big data storage, big data management, big data online sampling, big data processing, and big data exploration and analysis. We use the new Random Sample Partition (RSP) distributed data model to store a big data set as a set of ready-to-use random sample data blocks in Hadoop Distributed File System (HDFS), called RSP blocks. With this system, only a few RSP blocks are selected and processed using a sequential algorithm in a distributed data-parallel manner to produce approximate results for the entire data set. In this paper, we present a prototype RSP-based system and demonstrate its advantages. Our experiments show that RSP blocks can be used to get approximate models and summary statistics as well as estimate the proportions of inconsistent values without computing the entire data or running expensive online sampling operations. This new system enables big data exploration and analysis where the entire data set cannot be computed.}, location = {Beijing, China}, series = {CIKM '19}, pages = {2481\u20132484}, numpages = {4}, keywords = {approximate computing, block-level sampling, big data, random sample partition, cluster computing}}
@inproceedings{10.1145/3014812.3014869,title = {A big data analytic framework for investigating streaming educational data}, author = {Yang Jie , Ma Jun , Howard Sarah K. , Ciao Matthew , Srikhanta Rangan },year = {2017}, isbn = {9781450347686}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3014812.3014869}, doi = {10.1145/3014812.3014869}, abstract = {Last decade has witnessed the dramatic expansion of online user-generated content. Making full use of this data to discover behaviour patterns has become an increasingly appealing research topic. In this pilot study, a big data analytic framework is proposed, particularly taking streaming data from students' activity on their laptop usage as an illustrative example. Three modules are implemented to harvest raw streaming records, storage heterogeneous data, and apply the fuzzy representation and rule-mining algorithm for a modelling purpose.The efficiency of the proposed framework is then evaluated using a nationwide streaming dataset. The exploratory simulation of results demonstrates the flexibility and applicability of the proposed framework for processing complex streaming data, and revealing patterns from digital engagement which be used to inform decision makers.}, location = {Geelong, Australia}, series = {ACSW '17}, pages = {1\u20134}, numpages = {4}, keywords = {big data analytics, rule mining, streaming data, user-generated content}}
@inproceedings{10.1145/3377170.3377278,title = {Design of Integrated Platform for Clothing and Accouterment Support Based on Big Data}, author = {Zhai Chenggong , Fei Xiande , Yang Zhiwei },year = {2019}, isbn = {9781450376631}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377170.3377278}, doi = {10.1145/3377170.3377278}, abstract = {Based on the in-depth analysis of the necessity and feasibility of the optimization of big data-based quilt support, this paper puts forward the overall framework and implementation concept of the optimization of big data-based quilt support, and describes how to build the basic matching of the optimization of big data-based quilt support and the concept of big data information system system. Design of integrated platform for clothing and accouterment support based on big data that collects, stores, processes, analyzes and mines the supply data of conscription bedding by using big data, cloud computing and other information technologies, which plays an important role in deepening the reform of military bedding and strengthening the scientific management of bedding.}, location = {Shanghai, China}, series = {ICIT 2019}, pages = {54\u201358}, numpages = {5}, keywords = {Big Data, Clothing and Accouterment, Integrated Platform}}
@inproceedings{10.1145/3437963.3441654,title = {Harnessing Big Data for Personalized Medicine}, author = {Segal Eran },year = {2021}, isbn = {9781450382977}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3437963.3441654}, doi = {10.1145/3437963.3441654}, abstract = {The recent availability of diverse health data resources on large cohorts of human individuals presents many challenges and opportunities. I will present our work aimed at developing machine learning algorithms for predicting future onset of disease and identifying causal drivers of disease based on nationwide electronic health record data as well as data from high-throughput omics profiling technologies such as genetics, microbiome, and metabolomics. Our models provide novel insights into potential drivers of obesity, diabetes, and heart disease, and identify hundreds of novel markers at the microbiome, metabolite, and immune system level. Overall, our predictive models can be translated into personalized disease prevention and treatment plans, and to the development of new therapeutic modalities based on metabolites and the microbiome.}, location = {Virtual Event, Israel}, series = {WSDM '21}, pages = {3}, numpages = {1}, keywords = {microbiome, metabolomics, machine learning, genetics, electronic health records}}
@inproceedings{10.1145/3284103.3284112,title = {Traffic State Estimation with Big Data}, author = {Xing Han , Zhang Ke , Yang Zifan , Sun Lianying , Liu Yi },year = {2018}, isbn = {9781450360449}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3284103.3284112}, doi = {10.1145/3284103.3284112}, abstract = {Traffic state estimation helps urban traffic control and management. In this paper, a traffic state estimation model based on the fusion of Hidden Markov model and SEA algorithm is proposed considering the randomness and volatility of traffic systems. Traffic data of average travel speed in selected city were collected, and the mean and fluctuation values of average travel speed in adjacent time windows were calculated. With Hidden Markov model, the system state network is defined according to mean values and fluctuation values. The operation efficiency of traffic system, as well as stability and trend values, were calculated with System Effectiveness Analysis (SEA) algorithm based on system state network. Calculation results show that the method perform well and can be applied to both traffic state assessment of certain road sections and large scale road networks.}, location = {Seattle, WA, USA}, series = {Safety and Resilience'18}, pages = {1\u20135}, numpages = {5}, keywords = {System State Network, HMM, Traffic State Estimation, SEA}}
@inproceedings{10.5555/2819009.2819014,title = {An empirical study on quality issues of production big data platform}, author = {Zhou Hucheng , Lou Jian-Guang , Zhang Hongyu , Lin Haibo , Lin Haoxiang , Qin Tingting },year = {2015}, publisher = {IEEE Press}, abstract = {Big Data computing platform has evolved to be a multi-tenant service. The service quality matters because system failure or performance slowdown could adversely affect business and user experience. To date, there is few study in literature on service quality issues of production Big Data computing platform. In this paper, we present an empirical study on the service quality issues of Microsoft ProductA, which is a company-wide multi-tenant Big Data computing platform, serving thousands of customers from hundreds of teams. ProductA has a well-defined escalation process (i.e., incident management process), which helps customers report service quality issues on 24/7 basis. This paper investigates the common symptom, causes and mitigation of service quality issues in Big Data platform. We conduct a comprehensive empirical study on 210 real service quality issues of ProductA. Our major findings include (1) 21.0% of escalations are caused by hardware faults; (2) 36.2% are caused by system side defects; (3) 37.2% are due to customer side faults. We also studied the general diagnosis process and the commonly adopted mitigation solutions. Our study results provide valuable guidance on improving existing development and maintenance practice of production Big Data platform, and motivate tool support.}, location = {Florence, Italy}, series = {ICSE '15}, pages = {17\u201326}, numpages = {10}, keywords = {big data computing, escalations, fault tolerance, quality issues, empirical study}}
@inproceedings{10.1145/2939502.2939518,title = {Big data exploration requires collaboration between visualization and data infrastructures}, author = {Fisher Danyel },year = {2016}, isbn = {9781450342070}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2939502.2939518}, doi = {10.1145/2939502.2939518}, abstract = {As datasets grow to tera- and petabyte sizes, exploratory data visualization becomes very difficult: a screen is limited to a few million pixels, and main memory to a few tens of millions of data points. Yet these very large scale analyses are of tremendous interest to industry and academia. This paper discusses some of the major challenges involved in data analytics at scale, including issues of computation, communication, and rendering. It identifies techniques for handling large scale data, grouped into \"look at less of it,\" and \"look at it faster.\" Using these techniques involves a number of difficult design tradeoffs for both the ways that data can be represented, and the ways that users can interact with the visualizations.}, location = {San Francisco, California}, series = {HILDA '16}, pages = {1\u20135}, numpages = {5}, keywords = {data analysis, data visualization, big data}}
@inproceedings{10.1145/3292500.3340400,title = {Exploiting High Dimensionality in Big Data}, author = {Heckerman David },year = {2019}, isbn = {9781450362016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3292500.3340400}, doi = {10.1145/3292500.3340400}, abstract = {There are two aspects of data that make them big: sample size and dimensionality. The advantages of large sample size have long been touted. In contrast, high dimensionality has typically been seen as an obstacle to successful analysis. In this talk, using the area of genomics as an example, I will illustrate some of the advantages of high dimensionality.}, location = {Anchorage, AK, USA}, series = {KDD '19}, pages = {3172}, numpages = {1}, keywords = {Invited Talk}}