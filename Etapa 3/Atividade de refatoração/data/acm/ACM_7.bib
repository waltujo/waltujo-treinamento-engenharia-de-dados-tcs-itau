@inproceedings{10.1145/3264437.3264474,title = {Big Data Analytics Architecture for Security Intelligence}, author = {Dauda Ahmed , Mclean Scott , Almehmadi Abdulaziz , El-Khatib Khalil },year = {2018}, isbn = {9781450366083}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3264437.3264474}, doi = {10.1145/3264437.3264474}, abstract = {The need for security1 continues to grow in distributed computing. Today's security solutions require greater scalability and convenience in cloud-computing architectures, in addition to the ability to store and process larger volumes of data to address very sophisticated attacks. This paper explores some of the existing architectures for big data intelligence analytics, and proposes an architecture that promises to provide greater security for data intensive environments. The architecture is designed to leverage the wealth in the multi-source information for security intelligence.}, location = {Cardiff, United Kingdom}, series = {SIN '18}, pages = {1\u20134}, numpages = {4}, keywords = {Big Data, analytics architecture, Security Intelligence}}
@inproceedings{10.1145/2745754.2745771,title = {Querying Big Data by Accessing Small Data}, author = {Fan Wenfei , Geerts Floris , Cao Yang , Deng Ting , Lu Ping },year = {2015}, isbn = {9781450327572}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2745754.2745771}, doi = {10.1145/2745754.2745771}, abstract = {This paper investigates the feasibility of querying big data by accessing a bounded amount of the data. We study boundedly evaluable queries under a form of access constraints, when their evaluation cost is determined by the queries and constraints only. While it is undecidable to determine whether FO queries are boundedly evaluable, we show that for several classes of FO queries, the bounded evaluability problem is decidable. We also provide characterization and effective syntax for their boundedly evaluable queries.When a query Q is not boundedly evaluable, we study two approaches to approximately answering Q under access constraints. (1) We search for upper and lower envelopes of Q that are boundedly evaluable and warrant a constant accuracy bound. (2) We instantiate a minimum set of variables (parameters) in Q such that the specialized query is boundedly evaluable. We study problems for deciding the existence of envelopes and bounded specialized queries, and establish their complexity for various classes of FO queries.}, location = {Melbourne, Victoria, Australia}, series = {PODS '15}, pages = {173\u2013184}, numpages = {12}, keywords = {query answering, complexity, big data}}
@inproceedings{10.1109/CCGrid.2016.107,title = {Management of distributed big data for social networks}, author = {Leung Carson K. , Zhang Hao },year = {2016}, isbn = {9781509024520}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2016.107}, doi = {10.1109/CCGrid.2016.107}, abstract = {In the current era of big data, high volumes of a wide variety of valuable data can be easily collected and generated from a broad range of data sources of different veracities at a high velocity. Due to the well-known 5V's of these big data, many traditional data management approaches may not be suitable for handling the big data. Over the past few years, several applications and systems have developed to use cluster, cloud or grid computing to manage big data so as to support data science, big data analytics, as well as knowledge discovery and data mining. In this paper, we focus on distributed big data management. Specifically, we present our method for big data representation and management of distributed big data from social networks. We represent such big graph data in distributed settings so as to support big data mining of frequently occurring patterns from social networks.}, location = {Cartagena, Columbia}, series = {CCGRID '16}, pages = {639\u2013648}, numpages = {10}, keywords = {distributed data, data analytics, data representation, graph data, big data, distributed computing, big data management}}
@inproceedings{10.1145/3409801,title = {Big Data Processing on Volunteer Computing}, author = {Lv Zhihan , Chen Dongliang , Singh Amit Kumar },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3409801}, doi = {10.1145/3409801}, abstract = {In order to calculate the node big data contained in complex networks and realize the efficient calculation of complex networks, based on voluntary computing, taking ICE middleware as the communication medium, the loose coupling distributed framework DCBV based on voluntary computing is proposed. Then, the Master, Worker, and MiddleWare layers in the framework, and the development structure of a DCBV framework are designed. The task allocation and recovery strategy, message passing and communication mode, and fault tolerance processing are discussed. Finally, to calculate and verify parameters such as the average shortest path of the framework and shorten calculation time, an improved accurate shortest path algorithm, the N-SPFA algorithm, is proposed. Under different datasets, the node calculation and performance of the N-SPFA algorithm are explored. The algorithm is compared with four approximate shortest-path algorithms: Combined Link and Attribute (CLA), Lexicographic Breadth First Search (LBFS), Approximate algorithm of shortest path length based on center distance of area division (CDZ), and Hub Vertex of area and Core Expressway (HEA-CE). The results show that when the number of CPU threads is 4, the computation time of the DCBV framework is the shortest (514.63 ms). As the number of CPU cores increases, the overall computation time of the framework decreases gradually. For every 2 additional CPU cores, the number of tasks increases by 1. When the number of Worker nodes is 8 and the number of nodes is 1, the computation time of the framework is the shortest (210,979 ms), and the IO statistics data increase with the increase of Worker nodes. When the datasets are Undirected01 and Undirected02, the computation time of the N-SPFA algorithm is the shortest, which is 4520 ms and 7324 ms, respectively. However, the calculation time in the ca-condmat_undirected dataset is 175,292 ms, and the performance is slightly worse. Overall, however, the performance of the N-SPFA and SPFA algorithms is good. Therefore, the two algorithms are combined. For networks with less complexity, the computational scale coefficient of the SPFA algorithm can be set to 0.06, and for general networks, 0.2. When compared with other algorithms in different datasets, the pretreatment time, average query time, and overall query time of N-SPFA algorithm are the shortest, being 49.67 ms, 5.12 ms, and 94,720 ms, respectively. The accuracy (1.0087) and error rate (0.024) are also the best. In conclusion, voluntary computing can be applied to the processing of big data, which has a good reference significance for the distributed analysis of large-scale complex networks.}, pages = {1\u201320}, numpages = {20}, keywords = {Volunteer computing, big data, accurate shortest paths, large-scale complex networks, loosely coupled distributed frameworks}}
@inproceedings{10.1145/2351316.2351329,title = {Online feature selection for mining big data}, author = {Hoi Steven C. H. , Wang Jialei , Zhao Peilin , Jin Rong },year = {2012}, isbn = {9781450315470}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2351316.2351329}, doi = {10.1145/2351316.2351329}, abstract = {Most studies of online learning require accessing all the attributes/features of training instances. Such a classical setting is not always appropriate for real-world applications when data instances are of high dimensionality or the access to it is expensive to acquire the full set of attributes/features. To address this limitation, we investigate the problem of Online Feature Selection (OFS) in which the online learner is only allowed to maintain a classifier involved a small and fixed number of features. The key challenge of Online Feature Selection is how to make accurate prediction using a small and fixed number of active features. This is in contrast to the classical setup of online learning where all the features are active and can be used for prediction. We address this challenge by studying sparsity regularization and truncation techniques. Specifically, we present an effective algorithm to solve the problem, give the theoretical analysis, and evaluate the empirical performance of the proposed algorithms for online feature selection on several public datasets. We also demonstrate the application of our online feature selection technique to tackle real-world problems of big data mining, which is significantly more scalable than some well-known batch feature selection algorithms. The encouraging results of our experiments validate the efficacy and efficiency of the proposed techniques for large-scale applications.}, location = {Beijing, China}, series = {BigMine '12}, pages = {93\u2013100}, numpages = {8}, keywords = {online learning, feature selection, classification}}
@inproceedings{10.1145/2465848.2480346,title = {To cloud my big data or not to? musings at the intersection of big data, intense computing and clouds}, author = {Sion Radu },year = {2013}, isbn = {9781450319799}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2465848.2480346}, doi = {10.1145/2465848.2480346}, location = {New York, New York, USA}, series = {Science Cloud '13}, pages = {3\u20134}, numpages = {2}, keywords = {cloud economics, big data, cloud computing}}
@inproceedings{10.1145/3207677.3278000,title = {Research and Application of Enterprise Big Data Governance}, author = {Ke Changwen , Wang Kuisheng },year = {2018}, isbn = {9781450365123}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3207677.3278000}, doi = {10.1145/3207677.3278000}, abstract = {With1 the further development of information technology, data has become one of the core resources of enterprises. In the current era of big data, data governance has gradually become an important means for enterprises to make intelligent decisions, helping enterprises to occupy a favorable position in a highly competitive market environment. This paper briefly describes the current situation of enterprise big data governance, and analyzes the problems of current data governance from the perspectives of enterprise management and data itself. To deal with these problems, this paper proposes several suggestions and strategies from the perspectives of organization and management system construction, construction of a data standard system, improve the level of data quality management, data technology surpport, etc. Fully combining the advanced theories and methods of domestic data governance, this paper designs a data governance framework from the perspective of data application and innovation. The framework includes supervision and control modules, data governance staff organization modules, application and service modules, and data processing and integration modules. And the framework is applied to the data governance of electric power enterprises, which has important reference significance and value for enterprises to carry out data governance.}, location = {Hohhot, China}, series = {CSAE '18}, pages = {1\u20135}, numpages = {5}, keywords = {Data governance, governance framework, data quality}}
@inproceedings{10.1145/2746090.2746117,title = {Originalism, hypothesis testing and big data}, author = {McGinnis John O. , Stein Branden },year = {2015}, isbn = {9781450335225}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2746090.2746117}, doi = {10.1145/2746090.2746117}, abstract = {In this paper, we describe how data mining and hypothesis testing can advance the analysis of originalism in American constitutional law.}, location = {San Diego, California}, series = {ICAIL '15}, pages = {201\u2013205}, numpages = {5}, keywords = {big data, hypothesis testing, originalism}}
@inproceedings{10.1145/2857218.2857256,title = {Challenges and opportunities with big data visualization}, author = {Agrawal Rajeev , Kadadi Anirudh , Dai Xiangfeng , Andres Frederic },year = {2015}, isbn = {9781450334808}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2857218.2857256}, doi = {10.1145/2857218.2857256}, abstract = {In this big data era, huge amount data are continuously acquired for a variety of purposes. Advanced computing, imaging, and sensing technologies enable scientists to study natural and physical phenomena at unprecedented precision, resulting in an explosive growth of data. It is a huge challenge to visualize this growing data in static or in dynamic form. Most traditional data visualization approaches and tools can't support at \"big\" scale. In this paper, we identified the challenges and opportunities in big data visualization and review some current approaches and visualization tools.}, location = {Caraguatatuba, Brazil}, series = {MEDES '15}, pages = {169\u2013173}, numpages = {5}, keywords = {scalability, visualization, data reduction, big data}}
@inproceedings{10.1145/2928294.2928295,title = {Utilizing semantic big data for realizing a national-scale infrastructure vulnerability analysis system}, author = {Lee Sangkeun , Chinthavali Supriya , Duan Sisi , Shankar Mallikarjun },year = {2016}, isbn = {9781450342995}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2928294.2928295}, doi = {10.1145/2928294.2928295}, abstract = {Critical Infrastructure systems (CIs) such as energy, water, transportation, and communication are highly interconnected and mutually dependent in complex ways. Robust modeling of CIs' interconnections is crucial to identify vulnerabilities in the CIs. We present a vision of national-scale Infrastructure Vulnerability Analysis System (IVAS) leveraging Semantic Big Data (SBD) tools, Big Data, and Geographical Information Systems (GIS) tools. We first survey existing approaches on vulnerability analysis of critical infrastructures and discuss relevant systems and tools aligned with our vision. Next, we present a generic system architecture and discuss challenges including: (1) Constructing and managing a CI network-of-networks graph, (2) Performing analytic operations at scale, and (3) Interactive visualization of analytic output to generate meaningful insights. We argue that this architecture acts as a baseline to realize a national-scale network based vulnerability analysis system.}, location = {San Francisco, California}, series = {SBD '16}, pages = {1\u20136}, numpages = {6}, keywords = {vulnerability analysis, large-scale, big data, critical infrastructure network, interdependency, graph analysis}}
@inproceedings{10.1145/2837060.2837062,title = {A GPS Trajectory Map-Matching Mechanism with DTG Big Data on the HBase System}, author = {Cho Wonhee , Choi Eunmi },year = {2015}, isbn = {9781450338462}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2837060.2837062}, doi = {10.1145/2837060.2837062}, abstract = {Since smartphones equipped with GPS have been produced, the need to conduct an analysis by matching the mass of GPS trajectory data on a digital map has increased. However, the study of the existing map-matching algorithm technique is mainly for navigation. In order to analyze large amounts of GPS trajectories on a server, issues of the speed and performance of the system exist. The purpose of this study is to utilize a map-matching system using HBase, which is a distributed NoSQL DB in a Hadoop ecosystem. We defined the table specification of HBase for mounting the digital map and proposed and implemented the method for analysis with a map-matching algorithm. In this paper, we present the map-matching methodology using the NoSQL DB of Hadoop ecosystem for analyzing GPS trajectory.}, location = {Jeju Island, Republic of Korea}, series = {BigDAS '15}, pages = {22\u201329}, numpages = {8}, keywords = {map matching, spatial analysis, HBase, Hadoop, Big data}}
@inproceedings{10.1145/3183713.3183757,title = {Big Data Linkage for Product Specification Pages}, author = {Qiu Disheng , Barbosa Luciano , Crescenzi Valter , Merialdo Paolo , Srivastava Divesh },year = {2018}, isbn = {9781450347037}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3183713.3183757}, doi = {10.1145/3183713.3183757}, abstract = {An increasing number of product pages are available from thousands of web sources, each page associated with a product, containing its attributes and one or more product identifiers. The sources provide overlapping information about the products, using diverse schemas, making web-scale integration extremely challenging. In this paper, we take advantage of the opportunity that sources publish product identifiers to perform big data linkage across sources at the beginning of the data integration pipeline, before schema alignment. To realize this opportunity, several challenges need to be addressed: identifiers need to be discovered on product pages, made difficult by the diversity of identifiers; the main product identifier on the page needs to be identified, made difficult by the many related products presented on the page; and identifiers across pages need to beresolved, made difficult by the ambiguity between identifiers across product categories. We present our RaF (Redundancy as Friend) solution to the problem of big data linkage for product specification pages, which takes advantage of the redundancy of identifiers at a global level, and the homogeneity of structure and semantics at the local source level, to effectively and efficiently link millions of pages of head and tail products across thousands of head and tail sources. We perform a thorough empirical evaluation of our RaF approach using the publicly available Dexter dataset consisting of 1.9M product pages from 7.1k sources of 3.5k websites, and demonstrate its effectiveness in practice.}, location = {Houston, TX, USA}, series = {SIGMOD '18}, pages = {67\u201381}, numpages = {15}, keywords = {big data, data integration, data extraction, data linkage}}
@inproceedings{10.1145/3183440.3183458,title = {Big data software analytics with Apache Spark}, author = {Gousios Georgios },year = {2018}, isbn = {9781450356633}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3183440.3183458}, doi = {10.1145/3183440.3183458}, abstract = {At the beginning of every research effort, researchers in empirical software engineering have to go through the processes of extracting data from raw data sources and transforming them to what their tools expect as inputs. This step is time consuming and error prone, while the produced artifacts (code, intermediate datasets) are usually not of scientific value. In the recent years, Apache Spark has emerged as a solid foundation for data science and has taken the big data analytics domain by storm. We believe that the primitives exposed by Apache Spark can help software engineering researchers create and share reproducible, high-performance data analysis pipelines.In our technical briefing, we discuss how researchers can profit from Apache Spark, through a hands-on case study.}, location = {Gothenburg, Sweden}, series = {ICSE '18}, pages = {542\u2013543}, numpages = {2}, keywords = {Apache Spark, data analytics, big data}}
@inproceedings{10.1145/3481646.3481650,title = {Secure Big Data Management based on Secret Sharing and Verifiable Computing}, author = {Shiraishi Momoko },year = {2021}, isbn = {9781450390408}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3481646.3481650}, doi = {10.1145/3481646.3481650}, abstract = {Of late, more and more sensitive data have been connected with cyberspace, which has led to an increase in the importance of secure cloud computing. This paper shows an overall scheme of securely managing big data in a cloud supported by two technological methods, secret sharing and verifiable computing. While each of them was implemented and analyzed individually in previous works, this paper shows a comprehensive framework combining them to realize both security and efficiency in cloud data management. Specifically, secret sharing enables hiding the outsourced encrypted data from any unauthorized entity, and verifiable computing enable resource-constraint users to access the database in which the correctness is undoubtedly confirmed. This paper also compares the database scheme based on blockchain and the proposed method in terms of performance. The analysis shows that the proposed scheme entirely based on the conventional Internet computes the operations lighter than the blockchain scheme.}, location = {Liverpool, United Kingdom}, series = {ICCBDC '21}, pages = {21\u201325}, numpages = {5}, keywords = {Verifiable Computing, Secret Sharing, Cloud Database}}
@inproceedings{10.1145/3102254.3102271,title = {Big-data-driven innovation for enterprises: innovative big value paradigms for next-generation digital ecosystems}, author = {Cuzzocrea Alfredo , Loia Vincenzo , Tommasetti Aurelio },year = {2017}, isbn = {9781450352253}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3102254.3102271}, doi = {10.1145/3102254.3102271}, abstract = {Among the various interpretations and meanings of the well-known Vs (Volume, Velocity, Variety) of Big Data, V as Value represents the most significant and critical innovation for enterprises, which are a well-known case of digital ecosystems. The key issue for big enterprise data consists in extracting knowledge for creating new value and innovation for the target enterprise. Therefore, the big data analytics phase plays a critical role to this end. Following these considerations, in this paper we provide the following three contributions: (i) an overview of most relevant proposals in the context of big data innovation for enterprises; (ii) a reference architecture for supporting advanced big data analytics over big enterprise data; (iii) a discussion on future challenges in the context of big data innovation for enterprises.}, location = {Amantea, Italy}, series = {WIMS '17}, pages = {1\u20135}, numpages = {5}, keywords = {big data, digital ecosystems, big data analytics, big value, big data prediction, big enterprise data}}
@inproceedings{10.1145/3066911,title = {Proceedings of The International Workshop on Semantic Big Data},year = {2017}, isbn = {9781450349871}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The currentWorld-WideWeb enables an easy, instant access to a vast amount of online information. However, the content in theWeb is typically for human consumption, and is not tailored for machine processing. The Semantic Web is hence intended to establish a machine-understandable Web, and is currently also used in many other domains and not only in the Web. The World Wide Web Consortium (W3C) has developed a number of standards around this vision. Among them is the Resource Description Framework (RDF), which is used as the data model of the Semantic Web. The W3C has also defined SPARQL as the RDF query language, RIF as the rule language, and the ontology languages RDFS and OWL to describe schemas of RDF. The usage of common ontologies increases interoperability between heterogeneous data sets, and the proprietary ontologies with the additional abstraction layer facilitate the integration of these data sets. Therefore, we can argue that the Semantic Web is ideally designed to work in heterogeneous Big Data environments.We define Semantic Big Data as the intersection of Semantic Web data and Big Data. There are masses of Semantic Web data freely available to the public - thanks to the efforts of the linked data initiative. According to http://stats.lod2.eu/ the current freely available Semantic Web data is approximately 150 billion triples in over 2,800 datasets, many of which are accessible via SPARQL query servers called SPARQL endpoints. Everyone can submit SPARQL queries to SPARQL endpoints via a standardized protocol, where the queries are processed on the datasets of the SPARQL endpoints and the query results are sent back in a standardized format. Hence, not only Semantic Big Data is freely available, but also distributed execution environments for Semantic Big Data are freely accessible. This makes the Semantic Web an ideal playground for Big Data research.The goal of this workshop is to bring together academic researchers and industry practitioners to address the challenges and report and exchange the research findings in Semantic Big Data, including new approaches, techniques and applications, make substantial theoretical and empirical contributions to, and significantly advance the state of the art of Semantic Big Data.}, location = {Chicago, Illinois}}
@inproceedings{10.1145/3460620.3460622,title = {Meteorological forecasting based on big data analysis}, author = {Aljawarneh Shadi , Lara Torralbo Juan Alfonso },year = {2021}, isbn = {9781450388382}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3460620.3460622}, doi = {10.1145/3460620.3460622}, abstract = {In this paper, we present the main ideas behind the development of a system that can be used to deal with meteorological big data. In particular, the system captures data online and downloads it locally onto a MongoDB database. After that, the user can create a particular database and corresponding minable views for analysis. The results provided by the systems are predictive models with the ability to predict some weather-related variables, such as temperature and rainfall. The system has been validated from a triple perspective (usability, experts\u2019 validation, and performance assessment), obtaining satisfactory results. This paper aims to be a brief guide for authors who intend to developed similar systems either in the meteorological field or other domains generating big amounts of data.}, location = {Ma&apos;an, Jordan}, series = {DATA'21}, pages = {9\u201311}, numpages = {3}, keywords = {Knowledge Discovery, Big Data, Weather Forecasting}}
@inproceedings{10.1145/2948992.2949024,title = {Data Warehousing in Big Data: From Multidimensional to Tabular Data Models}, author = {Santos Maribel Yasmina , Costa Carlos },year = {2016}, isbn = {9781450340755}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2948992.2949024}, doi = {10.1145/2948992.2949024}, abstract = {Data warehouses are central pieces in business intelligence and analytics as these repositories ensure proper data storage and querying, being supported by data models that allow the analysis of data by different perspectives. Those perspectives support users and organizations in the decision-making process. In Big Data environments, Hive is used as a distributed storage mechanism that provides data warehousing capabilities. Its data schemas are defined attending to the analytical requirements specified by the users. In this work, multidimensional data models are used as the source of those requirements, allowing the automatic transformation of a multidimensional schema into a tabular schema suited to be implemented in Hive. To achieve this objective, a set of rules is proposed and tested in a demonstration case, showing the applicability and usefulness of the proposed approach.}, location = {Porto, Portugal}, series = {C3S2E '16}, pages = {51\u201360}, numpages = {10}, keywords = {Analytical Data Model, Hive, Big Data, Data Warehousing}}
@inproceedings{10.1145/3106237.3117774,title = {Static analysis for optimizing big data queries}, author = {Garbervetsky Diego , Pavlinovic Zvonimir , Barnett Michael , Musuvathi Madanlal , Mytkowicz Todd , Zoppi Edgardo },year = {2017}, isbn = {9781450351058}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3106237.3117774}, doi = {10.1145/3106237.3117774}, abstract = {Query languages for big data analysis provide user extensibility through a mechanism of user-defined operators (UDOs). These operators allow programmers to write proprietary functionalities on top of a relational query skeleton. However, achieving effective query optimization for such languages is extremely challenging since the optimizer needs to understand data dependencies induced by UDOs. SCOPE, the query language from Microsoft, allows for hand coded declarations of UDO data dependencies. Unfortunately, most programmers avoid using this facility since writing and maintaining the declarations is tedious and error-prone. In this work, we designed and implemented two sound and robust static analyses for computing UDO data dependencies. The analyses can detect what columns of an input table are never used or pass-through a UDO unchanged. This information can be used to significantly improve execution of SCOPE scripts. We evaluate our analyses on thousands of real-world queries and show we can catch many unused and pass-through columns automatically without relying on any manually provided declarations.}, location = {Paderborn, Germany}, series = {ESEC/FSE 2017}, pages = {932\u2013937}, numpages = {6}, keywords = {Query optimization, Static analysis, UDOs, Big Data}}
@inproceedings{10.1145/3490322.3490348,title = {Reconstruction and Trial Verification of the Collatz Conjecture Based On Big Data}, author = {Ye Hongyuan , Zhang Lei },year = {2021}, isbn = {9781450385091}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3490322.3490348}, doi = {10.1145/3490322.3490348}, abstract = {This paper redefines the Collatz conjecture and proposes the strong Collatz conjecture, which is a sufficient condition for the Collatz conjecture. Based on the computer data structure\u2013tree, we construct a non-negative integer inheritance decimal tree. The nodes on the decimal tree correspond to non-negative integers. We further define the Collatz-leaf node (corresponding to the Collatz-leaf integer) on the decimal tree. The Collatz-leaf nodes satisfy the strong Collatz conjecture. Through mathematical derivation, we prove that the Collatz-leaf node (Collatz-leaf integer) has\u00a0the\u00a0characteristics of\u00a0inheritance. With computer large numbers and big data calculation, we conclude that all nodes at a depth of 800 are Collatz-leaf nodes. Thus, we prove that the strong Collatz conjecture is true, and therefore the Collatz conjecture must also be true. For any positive integer N greater than 1, the minimum number of Collatz transforms from N to 1 is log2 N, the maximum number of Collatz transforms is 800 * (N-1). The non-negative integer inheritance decimal tree proposed and constructed in this paper also can be used for proofs of other mathematical problems.}, location = {Zibo, China}, series = {ICBDT '21}, pages = {160\u2013170}, numpages = {11}, keywords = {Number of Collatz transforms, Collatz-leaf integer, Number of strong Collatz transforms, Collatz-leaf node inheritance, Non-negative integer inheritance decimal tree, Collatz conjecture, Strong Collatz conjecture, Collatz-leaf node, Collatz transform}}
@inproceedings{10.1145/3524383.3524431,title = {Opportunities and Challenges of Joint Training of Postgraduate Students by the University-Industry Collaboration Institutions in Big Data Era}, author = {Wu Min , Hao Xinxin , Wan Xuehong , Ma Chenwei , Wu Yu },year = {2022}, isbn = {9781450395793}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3524383.3524431}, doi = {10.1145/3524383.3524431}, abstract = {University-industry cooperative education is an important way to cultivate graduate students' innovative ability and practical ability. However, there are some problems in the traditional joint training model of graduate students, such as low efficiency, conflict of objectives of cooperative subjects, a mismatch between supply and demand of cooperative entities, and so on. The big data technology has brought new opportunities and challenges to the joint training of graduate students by university-industry cooperation institutions. Based on analyzing the connotation and characteristics of the big data era, the paper points out that the arrival of the big data era can improve the information integration efficiency of university-industry cooperation institutions, optimize the traditional joint training model of graduate students, and provide an effective evaluation mechanism of educational quality for university-industry cooperation institutions. At the same time, the paper discusses the difficulties of data collection and disclosure of data privacy faced by university-industry cooperative education in the big data era. The paper also discusses how to deal with the challenges from the perspective of the government, colleges and universities, scientific research institutions and enterprises.}, location = {Shanghai, China}, series = {ICBDE '22}, pages = {194\u2013198}, numpages = {5}, keywords = {cooperative education of university-industry institutions, postgraduate education, big data}}
@inproceedings{10.1145/3220199.3220211,title = {Demand-oriented Energy Big Data Services using Hadoop-based Large-scale Distributed System Platform for District Heating}, author = {Song Mingoo , Choi Jungin },year = {2018}, isbn = {9781450364263}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3220199.3220211}, doi = {10.1145/3220199.3220211}, abstract = {In the energy industry, a variety of Big Data services have been developed and provided for both producers and consumers. These services bring more convenience, reliability stability and a higher operational efficiency in energy control and management. For demand-oriented energy services, the primary objectives of are set to reduce the cost or improve the energy efficiency on the consumption side. The association of a Hadoop-based large-scale distributed system with SCADA has engendered various applications using energy Big Data. Especially, the integration of Spark with Hadoop has promoted the development of demand-oriented energy services for Big Data with growing necessities in the industry. In this paper, we investigate demand-oriented energy Big Data services using a Hadoop-based large-scale distributed system platform for district heating as a case. Moreover, we introduce a social energy Big Data service which can ameliorate energy control and management with the demonstration in district heating.}, location = {Shenzhen, China}, series = {ICBDC '18}, pages = {10\u201313}, numpages = {4}, keywords = {Big Data analytics, energy Big Data services, district heating, energy control and management, social energy service}}
@inproceedings{10.1145/3434581.3434721,title = {Design of Big Data Algorithm Based on MapReduce}, author = {Luo Liangfu },year = {2020}, isbn = {9781450375764}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3434581.3434721}, doi = {10.1145/3434581.3434721}, abstract = {With the widespread application of Internet technology, the utilization rate of similar blogs and social networks has been significantly improved. With the intervention of cloud computing and other technologies, a large amount of data generated during browsing the Internet can be effectively accumulated and processed, which indicates that big data has been integrated into people's daily life. MapReduce, as a parallel programming environment, can effectively deal with big data related problems, and has been applied in large Internet companies such as Google and Amazon Maxdiff, an efficient histogram algorithm based on MapReduce, includes accurate algorithm and approximate algorithm, and proposes two-table equijoin algorithm and multistandard equijoin algorithm for data skew And optimize the efficiency of the connection algorithm when one or more data in the data set appear too much. This paper starts with the efficiency optimization of big data connection algorithm based on MapReduce, and studies the efficiency optimization of equijoin algorithm, data skew connection algorithm and connection algorithm in detail, and then puts forward an algorithm that can effectively improve the program execution efficiency, hoping to provide reference for the follow-up research work.}, location = {Weihai City, China}, series = {ICASIT 2020}, pages = {722\u2013724}, numpages = {3}, keywords = {MapReduce, Algorithm, Big data}}
@inproceedings{10.1145/3141128,title = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},year = {2017}, isbn = {9781450353434}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {It is our pleasure to introduce you to the proceedings of 2017 International Conference on Cloud and Big Data Computing (ICCBDC 2017). Big data is a rapidly expanding research area spanning the fields of computer science and information management, and has become a ubiquitous term in understanding and solving complex problems in different disciplinary fields such as engineering, applied mathematics, medicine, computational biology, healthcare, social networks, finance, business, government, education, transportation and telecommunications.}, location = {London, United Kingdom}}
@inproceedings{10.1145/2378975.2378987,title = {Federated cloud-based big data platform in telecommunications}, author = {Deng Chao , Qian Ling , Xu Meng , Du Yujian , Luo Zhiguo , Sun Shaoling },year = {2012}, isbn = {9781450317542}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2378975.2378987}, doi = {10.1145/2378975.2378987}, abstract = {China Mobile is the biggest telecommunication operator in the world, with more than 600 million customers and an ever increasing information technology (IT). To provide better service to 600 million customers and reduce the cost of IT systems, China Mobile adopted a centralized IT strategy based on cloud computing. The big data issue becomes the most significant challenge to the cloud computing based China Mobile IT structure. This paper presents the China Mobile's big data platform based on the cloud. This platform integrates the big data storage, the development and deployment of big data ETL (Extract, Transfer, Load) and DM (Data Mining) into a unified framework. This big data analysis platform can effectively support the analytical tasks in telecommunications, but it can also help China Mobile provide public cloud computing service. In this paper, we introduce the detailed architecture of China Mobile's platform and discuss its performance.}, location = {San Jose, California, USA}, series = {FederatedClouds '12}, pages = {44\u201348}, numpages = {5}, keywords = {telecommunication, cloud computing, SaaS, hadoop, big data}}
@inproceedings{10.1145/2928294,title = {Proceedings of the International Workshop on Semantic Big Data},year = {2016}, isbn = {9781450342995}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {The current World-Wide Web enables an easy, instant access to a vast amount of online information. However, the content in the Web is typically for human consumption, and is not tailored for machine processing. The Semantic Web is hence intended to establish a machine-understandable Web, and is currently also used in many other domains and not only in the Web. The World Wide Web Consortium (W3C) has developed a number of standards around this vision. Among them is the Resource Description Framework (RDF), which is used as the data model of the Semantic Web. The W3C has also defined SPARQL as the RDF query language, RIF as the rule language, and the ontology languages RDFS and OWL to describe schemas of RDF. The usage of common ontologies increases interoperability between heterogeneous data sets, and the proprietary ontologies with the additional abstraction layer facilitate the integration of these data sets. Therefore, we can argue that the Semantic Web is ideally designed to work in heterogeneous Big Data environments.We define Semantic Big Data as the intersection of Semantic Web data and Big Data. There are masses of Semantic Web data freely available to the public - thanks to the efforts of the linked data initiative. According to http://stats.lod2.eu/ the current freely available Semantic Web data is approximately 90 billion triples in over 3,300 datasets, many of which are accessible via SPARQL query servers called SPARQL endpoints. Everyone can submit SPARQL queries to SPARQL endpoints via a standardized protocol, where the queries are processed on the datasets of the SPARQL endpoints and the query results are sent back in a standardized format. Hence, not only Semantic Big Data is freely available, but also distributed execution environments for Semantic Big Data are freely accessible. This makes the Semantic Web an ideal playground for Big Data research.The goal of this workshop is to bring together academic researchers and industry practitioners to address the challenges and report and exchange the research findings in Semantic Big Data, including new approaches, techniques and applications, make substantial theoretical and empirical contributions to, and significantly advance the state of the art of Semantic Big Data.}, location = {San Francisco, California}}
@inproceedings{10.1145/3438872.3439086,title = {Design and Implementation of Componentized Big Data Platform}, author = {Guo Naiwang , Shen Quanjiang , Yang Hongshan },year = {2020}, isbn = {9781450388306}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3438872.3439086}, doi = {10.1145/3438872.3439086}, abstract = {The rapid development of the Internet has produced massive amounts of data, and has promoted the development of various big data components in the big data field, such as Hadoop, Spark and other big data components. However, many big data components face many problems in actual industrial production, make the construction and maintenance cost of the big data platform high. Although there is an automated deployment platform for big data components on the market, it lacks comprehensive management and monitoring functions for the entire big data platform, resulting in users not being able to fully pay attention to all aspects of the big data platform, and the service and resource management is not flexible enough. Therefore, we have designed and implemented an open componentized big data platform, so that users can not only easily install and deploy big data components in the cluster, but also manage the resources and services of big data components conveniently and flexibly. Besides, complete monitoring information also provides the necessary guarantee for the stable running of the big data platform. The open componentized big data platform provides users with one-stop big data services.}, location = {Shanghai, China}, series = {RICAI 2020}, pages = {234\u2013238}, numpages = {5}, keywords = {Convenient management, Big data components, Big data platform, Automated deployment, Comprehensive monitoring}}
@inproceedings{10.1145/2775292.2775319,title = {Hybrid visualisation of digital production big data}, author = {Evans Alun , Agenjo Javi , Blat Josep },year = {2015}, isbn = {9781450336475}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2775292.2775319}, doi = {10.1145/2775292.2775319}, abstract = {In this paper, we present a web application for the hybrid visualisation of digital production Big Data. In a typical film or television production, several terabytes of data can be recorded per day, such as film footage from multiple cameras or background information regarding the set. Interactive visualisation of this multimodal data, integrating 2D (image and video) and 3D graphics modes, would result in enhanced use. A browser-based context is capable of this integration in a seamless and powerful manner, but faces significant challenges related to data transfer and compression which must be overcome. This paper presents an application designed to harness the power of a hybrid web context while attempting to overcome or compensate for the difficulties of data transfer limitations and rendering power. Results are presented from three, publicly available test datasets, which represent a realistic sample of data recorded on a typical high-budget production set.}, location = {Heraklion, Crete, Greece}, series = {Web3D '15}, pages = {69\u201372}, numpages = {4}, keywords = {web, hybrid, pointclouds, WebGL, visualisation, video, big data}}
@inproceedings{10.1145/2513190.2513198,title = {Can we analyze big data inside a DBMS?}, author = {Ordonez Carlos },year = {2013}, isbn = {9781450324120}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2513190.2513198}, doi = {10.1145/2513190.2513198}, abstract = {Relational DBMSs remain the main data management technology, despite the big data analytics and no-SQL waves. On the other hand, for data analytics in a broad sense, there are plenty of non-DBMS tools including statistical languages, matrix packages, generic data mining programs and large-scale parallel systems, being the main technology for big data analytics. Such large-scale systems are mostly based on the Hadoop distributed file system and MapReduce. Thus it would seem a DBMS is not a good technology to analyze big data, going beyond SQL queries, acting just as a reliable and fast data repository. In this survey, we argue that is not the case, explaining important research that has enabled analytics on large databases inside a DBMS. However, we also argue DBMSs cannot compete with parallel systems like MapReduce to analyze web-scale text data. Therefore, each technology will keep influencing each other. We conclude with a proposal of long-term research issues, considering the \"big data analytics\" trend.}, location = {San Francisco, California, USA}, series = {DOLAP '13}, pages = {85\u201392}, numpages = {8}, keywords = {big data, dbms, mapreduce, sql, parallel algorithms}}
@inproceedings{10.1145/2463676.2463724,title = {Knowledge harvesting in the big-data era}, author = {Suchanek Fabian , Weikum Gerhard },year = {2013}, isbn = {9781450320375}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2463676.2463724}, doi = {10.1145/2463676.2463724}, abstract = {The proliferation of knowledge-sharing communities such as Wikipedia and the progress in scalable information extraction from Web and text sources have enabled the automatic construction of very large knowledge bases. Endeavors of this kind include projects such as DBpedia, Freebase, KnowItAll, ReadTheWeb, and YAGO. These projects provide automatically constructed knowledge bases of facts about named entities, their semantic classes, and their mutual relationships. They contain millions of entities and hundreds of millions of facts about them. Such world knowledge in turn enables cognitive applications and knowledge-centric services like disambiguating natural-language text, semantic search for entities and relations in Web and enterprise data, and entity-oriented analytics over unstructured contents. Prominent examples of how knowledge bases can be harnessed include the Google Knowledge Graph and the IBM Watson question answering system. This tutorial presents state-of-the-art methods, recent advances, research opportunities, and open challenges along this avenue of knowledge harvesting and its applications. Particular emphasis will be on the twofold role of knowledge bases for big-data analytics: using scalable distributed algorithms for harvesting knowledge from Web and text sources, and leveraging entity-centric knowledge for deeper interpretation of and better intelligence with Big Data.}, location = {New York, New York, USA}, series = {SIGMOD '13}, pages = {933\u2013938}, numpages = {6}, keywords = {ontology, big data, web contents, entity recognition, information extraction, knowledge base}}
@inproceedings{10.1145/2612733.2612762,title = {Local big data: the role of libraries in building community data infrastructures}, author = {Bertot John Carlo , Butler Brian S. , Travis Diane M. },year = {2014}, isbn = {9781450329019}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2612733.2612762}, doi = {10.1145/2612733.2612762}, abstract = {Communities face opportunities and challenges in many areas, including education, health and wellness, workforce and economic development, housing, and the environment [21]. At the same time, governments have significant fiscal constraints on their ability to address these challenges and opportunities. Through a combination of open government, open data, and civic engagement, however, governments, citizens, civil society groups, and others are reinventing the relationship between governments and the governed by developing crowdsourced and other innovative solutions for community advancement. Underlying this reinvention and innovation is data -- particularly local data about housing, air quality, graduation rates, literacy rates, poverty, disease, and more. And yet, not all communities have the capacity to create, work with, or leverage data at the local level. Using a case study approach in a medium-sized U.S. city, this paper focuses on the issues that smaller communities face when seeking to create local data infrastructures and the extent to which libraries can develop their capabilities, capacity, and abilities to work with community information and data to facilitate community engagement and high-impact, locally relevant analytics.}, location = {Aguascalientes, Mexico}, series = {dg.o '14}, pages = {17\u201323}, numpages = {7}, keywords = {community engagement, data infrastructure, big data, libraries, data curation, data management, communities}}
@inproceedings{10.1145/3372454,title = {Proceedings of the 2019 3rd International Conference on Big Data Research},year = {2019}, isbn = {9781450372015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {As the datasets often available in many disciplines and application areas exponentially grow thanks to the increasing availability of novel sensors and computational resources, there is nowadays still a need to develop novel approaches, methods and computational frameworks to deal with the high volumes and variety of the large data sources generated in many fields. The range of research challenges still opened are extremely large, from the representation, integration, storage, manipulation, data mining and visualization techniques to the development of secure computing architectures, cloud and distributed computing platforms to highly scalable storage systems. Clearly, big data challenges should require novel approaches and success stories should be shared, this favoring exchanges, cross-fertilization and finally advances as well as future research directions to consider.}, location = {Cergy-Pontoise, France}}
@inproceedings{10.1145/3175684,title = {Proceedings of the International Conference on Big Data and Internet of Thing},year = {2017}, isbn = {9781450354301}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {BDIOT provides a scientific platform for both local and international scientists, engineers and technologists who work in all aspects of big data and internet of things. In addition to the contributed papers, internationally known experts from several countries are also invited to deliver keynote speeches at BDIOT 2017.}, location = {London, United Kingdom}}
@inproceedings{10.1145/3368691.3368717,title = {Big data challenges and achievements: applications on smart cities and energy sector}, author = {Mohammed Tareq Abed , Ghareeb Ahmed , Al-bayaty Hussein , Aljawarneh Shadi },year = {2019}, isbn = {9781450372848}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3368691.3368717}, doi = {10.1145/3368691.3368717}, abstract = {In this paper, the Big Data challenges and the processing is analyzed, recently great attention has been paid to the challenges for great data, largely due to the wide spread of applications and systems used in real life, such as presentation, modeling, processing and large (often unlimited) data storage. Mass Data Survey, OLAP Mass Data, Mass Data Dissemination and Mass Data Protection. Consequently, we focus on further research trends and, as a default, we will explore a future research challenge research project in this area of research.}, location = {Dubai, United Arab Emirates}, series = {DATA '19}, pages = {1\u20135}, numpages = {5}, keywords = {OLAP, data mining, big data, machin learning, data processing}}
@inproceedings{10.1145/2691195.2691196,title = {Towards big data analytics framework: ICT professionals salary profile compilation perspective}, author = {Ramasamy Ramachandran },year = {2014}, isbn = {9781605586113}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2691195.2691196}, doi = {10.1145/2691195.2691196}, abstract = {This paper elucidates the opportunity on expanding the on-going preparation of salary profile of information communications technology (ICT) professionals of Malaysia into a big data analytics (BDA) activity. The current activity is based on structured database provided by the online job service providers. In essence, BDA framework entailing 5 Vs namely, volume, variety, velocity, veracity and value used in gauging the gaps and potential areas to consider in next stages.}, location = {Guimaraes, Portugal}, series = {ICEGOV '14}, pages = {450\u2013451}, numpages = {2}, keywords = {big data analytics, ICT salary profile, business intelligence}}
@inproceedings{10.1145/3209281.3209360,title = {Big data on cloud for government agencies: benefits, challenges, and solutions}, author = {Rashed Alaa Hussain , Karakaya Ziya , Yazici Ali },year = {2018}, isbn = {9781450365260}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3209281.3209360}, doi = {10.1145/3209281.3209360}, abstract = {Big Data and Cloud computing are the most important technologies that give the opportunity for government agencies to gain a competitive advantage and improve their organizations. On one hand, Big Data implementation requires investing a significant amount of money in hardware, software, and workforce. On the other hand, Cloud Computing offers an unlimited, scalable and on-demand pool of resources which provide the ability to adopt Big Data technology without wasting on the financial resources of the organization and make the implementation of Big Data faster and easier. The aim of this study is to conduct a systematic literature review in order to collect data to identify the benefits and challenges of Big Data on Cloud for government agencies and to make a clear understanding of how combining Big Data and Cloud Computing help to overcome some of these challenges. The last objective of this study is to identify the solutions for related challenges of Big Data. Four research questions were designed to determine the information that is related to the objectives of this study. Data is collected using literature review method and the results are deduced from there.}, location = {Delft, The Netherlands}, series = {dg.o '18}, pages = {1\u20139}, numpages = {9}, keywords = {cloud computing, solutions, benefits, big data, challenges, big data on cloud, government agencies}}
@inproceedings{10.1145/2745754.2745782,title = {Computational Thinking, Inferential Thinking and \"Big Data\"}, author = {Jordan Michael I. },year = {2015}, isbn = {9781450327572}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2745754.2745782}, doi = {10.1145/2745754.2745782}, abstract = {The phenomenon of \"Big Data\" is creating a need for research perspectives that blend computational thinking (with its focus on, e.g., abstractions, algorithms and scalability) with inferential thinking (with its focus on, e.g., underlying populations, sampling patterns, error bars and predictions). Database researchers and statistical machine learning researchers are centrally involved in the creation of this blend, and research that incorporates perspectives from both databases and machine learning will be of particular value in the bigger picture. This is true both for methodology and for theory. I present highlights of several research initiatives that draw jointly on database and statistical foundations, including work on concurrency control and distributed inference, subsampling, time/data tradeoffs and inference/privacy tradeoffs.}, location = {Melbourne, Victoria, Australia}, series = {PODS '15}, pages = {1}, numpages = {1}, keywords = {computational thinking, big data, statistical machine learning, inferential thinking}}
@inproceedings{10.1145/3361525.3361545,title = {Self-adaptive Executors for Big Data Processing}, author = {Khorasani Sobhan Omranian , Rellermeyer Jan S. , Epema Dick },year = {2019}, isbn = {9781450370097}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3361525.3361545}, doi = {10.1145/3361525.3361545}, abstract = {The demand for additional performance due to the rapid increase in the size and importance of data-intensive applications has considerably elevated the complexity of computer architecture. In response, systems offer pre-determined behaviors based on heuristics and then expose a large number of configuration parameters for operators to adjust them to their particular infrastructure. Unfortunately, in practice this leads to a substantial manual tuning effort. In this work, we focus on one of the most impactful tuning decisions in big data systems: the number of executor threads. We first show the impact of I/O contention on the runtime of workloads and a simple static solution to reduce the number of threads for I/O-bound phases. We then present a more elaborate solution in the form of self-adaptive executors which are able to continuously monitor the underlying system resources and detect contentions. This enables the executors to tune their thread pool size dynamically at runtime in order to achieve the best performance. Our experimental results show that being adaptive can significantly reduce the execution time especially in I/O intensive applications such as Terasort and PageRank which see a 34% and 54% reduction in runtime.}, location = {Davis, CA, USA}, series = {Middleware '19}, pages = {176\u2013188}, numpages = {13}, keywords = {Self-Adaptive Executors, Big Data, Apache Spark}}
@inproceedings{10.1145/3041021.3053063,title = {Top-K Entity Units Retrieval Over Big Data}, author = {Zhang Da , Kabuka Mansur R. },year = {2017}, isbn = {9781450349147}, publisher = {International World Wide Web Conferences Steering Committee}, address = {Republic and Canton of Geneva, CHE}, url = {https://doi.org/10.1145/3041021.3053063}, doi = {10.1145/3041021.3053063}, abstract = {During the past several years, data size has increased explosively. This data explosion tendency has impacted various fields ranging from biomedical engineering, business consulting to social media and mobile application. Big Data is a two sided sword. While it provides incredibly treasured insights in commercial scope and innovative discovery in the scientific field, Big Data also has many challenges, such as complication in data storage, data processing, data analysis and data visualization. Among all these challenges, keyword searching over a large volume of data prevails as one of the four tasks defined by Bizer et al. at the year of 2012. Keyword searching refers to retrieving the objects relevant to the entities of concern using scientific computational methods. Consequently, efficiently solving the problem of keyword searching can contribute as a foundation to diverse Big Data applications.}, location = {Perth, Australia}, series = {WWW '17 Companion}, pages = {1269\u20131272}, numpages = {4}, keywords = {keyword searching, information retrieval, big data}}
@inproceedings{10.1145/2640087.2644159,title = {Creating a Chemistry of Sciences with Big Data: Building the Data Science Institute at Imperial College London}, author = {Guo Yike , Johnson David },year = {2014}, isbn = {9781450328913}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2640087.2644159}, doi = {10.1145/2640087.2644159}, abstract = {The Data Science Institute at Imperial College London launched in April 2014, and will provide a hub for data-driven research and education. Its mission is to provide a focal point for the College's capabilities in multidisciplinary data-driven research by coordinating advanced data science research for college scientists and partners, and educating the next generation of data scientists. We surveyed the data-driven research needs at Imperial College London to gain an understanding across all disciplines offered by the College, and analysed the responses to gain insights into scientific and engineering needs for data science research.A clear message is that multidisciplinarity is essential for Big Data and data science research to enable a \"chemistry of sciences\": connecting all disciplines by integrating data. This paper presents our efforts to best understand data-driven research needs in a highly multidisciplinary research-intensive institution and describes our vision for the future of the Data Science Institute at Imperial College London.}, location = {Beijing, China}, series = {BigDataScience '14}, pages = {1\u20137}, numpages = {7}}
@inproceedings{10.1145/2345316.2345347,title = {On clusterization of \"big data\" streams}, author = {Berkovich Simon , Liao Duoduo },year = {2012}, isbn = {9781450311137}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2345316.2345347}, doi = {10.1145/2345316.2345347}, abstract = {Current technology provides wonderful facilities for operating with extremely vast amounts of data. These facilities are expanding due to capabilities of \"Cloud Computing.\" The developing situation gives rise to the \"Big Data\" concept posing specific engineering and organizational challenges. Big data refers to the rising flood of digital data from many sources, including the sensors, digitizers, scanners, software-based modeling, mobile phones, internet, videos, e-mails, and social network communications. The data type could be texts, geometries, images, videos, sounds, or their combination. Many of such data are directly or indirectly related to geospatial information. In this paper, we suggest to enhance the available information processing resources with a novel software/hardware technique for on-the-fly clusterization of amorphous data from diverse sources. The presented approach is based on the previously developed construction of FuzzyFind Dictionary utilizing the error-correction Golay Code. Realization of this technique requires processing of intensive continuous data streams, which can be effectively implemented using multi-core pipelining with forced interrupts. The objective of this paper is to bring forward a new simple and efficacious tool for one of the most demanding operations of this \"Big Data\" methodology --clustering of diverse information items in a data stream mode. Improving our ability to extract knowledge and insights from large and complex collections of digital data promises to solve some the Nation's most pressing challenges. Furthermore, the paper reveals a parallel between the computational model integrating \"Big Data\" streams and the organization of information processing in the brain. The uncertainties in relation to the considered method of clusterization are moderated due to the idea of the bounded rationality, an approach that does not require a complete exact knowledge for sensible decision-making.}, location = {Washington, D.C., USA}, series = {COM.Geo '12}, pages = {1\u20136}, numpages = {6}, keywords = {on-the-fly processing, heterogeneous computing, multi-core pipelining, \"big data\" concept, stream processing, clustering algorithms, Golay code, meta knowledge\" ontology}}
@inproceedings{10.1145/3361758.3361759,title = {Analysis of AMI, Smart Metering Deployment and Big Data Management Challenges}, author = {Saleh Mohammed , Khdour Thair , Qasaymeh Mahmoud },year = {2019}, isbn = {9781450372466}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3361758.3361759}, doi = {10.1145/3361758.3361759}, abstract = {Sustainability, rise of consumer power, IOT, Cloud Computing and digitalization era are major factors driving the fourth industrial revolution. Electric energy stakeholders are automating technological and business processes to the new requirements of the energy sector. Deploying the right combination of AI, Advanced Metering Infrastructure and Big Data Analysis, will increase efficiency and self-optimize operations. The emerging markets including the Middle East will deploy nearly 250 million meters, representing an investment of almost $35bn. The GCC increasing appetite power generating capacity is 157 GW; which is almost half the total Middle East and North Africa power generating capacity Smart Grid market in Gulf Council Countries will top US$ 1.68 billion by 2026. In addition, GCC is projected to spend US$137 Billion to increase its power generating capacity, Transmission and Distribution by 69 MW within five years. This research focus on the analysis of AMI and smart metering deployment and challenges in GCC region.}, location = {Melbourn, VIC, Australia}, series = {BDIOT 2019}, pages = {3\u20137}, numpages = {5}, keywords = {Smart Grid, Digital Utility, AMI}}
@inproceedings{10.1145/3290420.3290466,title = {Big data prototype practice for unmanned surface vehicle}, author = {Jia Shuli , Ma Liyong , Zhang Shuting },year = {2018}, isbn = {9781450365345}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3290420.3290466}, doi = {10.1145/3290420.3290466}, abstract = {In recent years, unmanned surface vehicle (USV) has made great progress with the development of communication and electronic technologies. In the future, USVs will be widely used in marine tasks. USV has begun to offer a wide variety of large heterogeneous data sources. The architecture of USV big data is discussed. A prototype system of USV big data is established, and a preliminary anomaly detection application based on isolation forest method is developed. The effective anomaly detection results show that the USV big data prototype system and the anomaly detection application is feasible. The establishment of USV big data system is meaningful and the system can provide effective application services.}, location = {Qingdao, China}, series = {ICCIP '18}, pages = {43\u201347}, numpages = {5}, keywords = {isolation forest, unmanned surface vehicle (USV), big data}}
@inproceedings{10.1145/3505745,title = {2021 the 5th International Conference on Big Data Research (ICBDR)},year = {2021}, isbn = {9781450384339}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, location = {Tokyo, Japan}}
@inproceedings{10.1145/2487575.2487588,title = {Beyond myopic inference in big data pipelines}, author = {Raman Karthik , Swaminathan Adith , Gehrke Johannes , Joachims Thorsten },year = {2013}, isbn = {9781450321747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2487575.2487588}, doi = {10.1145/2487575.2487588}, abstract = {Big Data Pipelines decompose complex analyses of large data sets into a series of simpler tasks, with independently tuned components for each task. This modular setup allows re-use of components across several different pipelines. However, the interaction of independently tuned pipeline components yields poor end-to-end performance as errors introduced by one component cascade through the whole pipeline, affecting overall accuracy. We propose a novel model for reasoning across components of Big Data Pipelines in a probabilistically well-founded manner. Our key idea is to view the interaction of components as dependencies on an underlying graphical model. Different message passing schemes on this graphical model provide various inference algorithms to trade-off end-to-end performance and computational cost. We instantiate our framework with an efficient beam search algorithm, and demonstrate its efficiency on two Big Data Pipelines: parsing and relation extraction.}, location = {Chicago, Illinois, USA}, series = {KDD '13}, pages = {86\u201394}, numpages = {9}, keywords = {probabilistic inference, modular design, big data pipelines}}
@inproceedings{10.1145/3312614.3312623,title = {The 51 V's Of Big Data: Survey, Technologies, Characteristics, Opportunities, Issues and Challenges}, author = {Khan Nawsher , Naim Arshi , Hussain Mohammad Rashid , Naveed Quadri Noorulhasan , Ahmad Naim , Qamar Shamimul },year = {2019}, isbn = {9781450366403}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3312614.3312623}, doi = {10.1145/3312614.3312623}, abstract = {Currently Big Data is the biggest buzzword, and definitely, we believe that Big Data is changing the world. Some researchers say Big Data will be even bigger buzzword than the Internet. With fast-growing computing resources, information and knowledge a new digital globe has emerged. Information is being created and stored at a fast rate and is being accessed by a vast range of applications through scientific computing, commercial workloads, and social media. In 2018, over 28 billion devices globally, are connected to the internet. In 2020, more than 50 billion smart appliances will be connected worldwide and internet traffic flow will be 92 times greater than it was in 2005. The usage of such a massive number of connected devices not only increase the data volume but also the velocity of data addition with speed of light on fiber optic and various wireless networks. This fast generation of enormous data creates numerous threats and challenges. There exist various approaches that are addressing issues and challenges of Big Data with the theory of Vs such as 3 V's, 5 V's, 7 V's etc. The objective of this work is to explore and investigate the status of the current Big Data domain. Further, a comprehensive overview of Big Data, its characteristics, opportunities, issues, and challenges have been explored and described with the help of 51 V's. The outcome of this research will help in understanding the Big Data in a systematic way.}, location = {Crete, Greece}, series = {COINS '19}, pages = {19\u201324}, numpages = {6}, keywords = {data generation, data characteristics, Big Data, data storage}}
@inproceedings{10.1145/3444757.3485106,title = {Locality-Aware Workflow Orchestration for Big Data}, author = {Corodescu Andrei-Alin , Nikolov Nikolay , Khan Akif Quddus , Soylu Ahmet , Matskin Mihhail , Payberah Amir H. , Roman Dumitru },year = {2021}, isbn = {9781450383141}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3444757.3485106}, doi = {10.1145/3444757.3485106}, abstract = {The development of the Edge computing paradigm shifts data processing from centralised infrastructures to heterogeneous and geographically distributed infrastructure. Such a paradigm requires data processing solutions that consider data locality in order to reduce the performance penalties from data transfers between remote (in network terms) data centres. However, existing Big Data processing solutions have limited support for handling data locality and are inefficient in processing small and frequent events specific to Edge environments. This paper proposes a novel architecture and a proof-of-concept implementation for software container-centric Big Data workflow orchestration that puts data locality at the forefront. Our solution considers any available data locality information by default, leverages long-lived containers to execute workflow steps, and handles the interaction with different data sources through containers. We compare our system with Argo workflow and show significant performance improvements in terms of speed of execution for processing units of data using our data locality aware Big Data workflow approach.}, location = {Virtual Event, Tunisia}, series = {MEDES '21}, pages = {62\u201370}, numpages = {9}, keywords = {data locality, software containers, Big Data workflows}}
@inproceedings{10.1145/2992786,title = {Luzzu\u2014A Methodology and Framework for Linked Data Quality Assessment}, author = {Debattista Jeremy , Auer S\u00d6ren , Lange Christoph },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2992786}, doi = {10.1145/2992786}, abstract = {The increasing variety of Linked Data on the Web makes it challenging to determine the quality of this data and, subsequently, to make this information explicit to data consumers. Despite the availability of a number of tools and frameworks to assess Linked Data Quality, the output of such tools is not suitable for machine consumption, and thus consumers can hardly compare and rank datasets in the order of fitness for use. This article describes a conceptual methodology for assessing Linked Datasets, and Luzzu; a framework for Linked Data Quality Assessment. Luzzu is based on four major components: (1) an extensible interface for defining new quality metrics; (2) an interoperable, ontology-driven back-end for representing quality metadata and quality problems that can be re-used within different semantic frameworks; (3) scalable dataset processors for data dumps, SPARQL endpoints, and big data infrastructures; and (4) a customisable ranking algorithm taking into account user-defined weights. We show that Luzzu scales linearly against the number of triples in a dataset. We also demonstrate the applicability of the Luzzu framework by evaluating and analysing a number of statistical datasets against a variety of metrics. This article contributes towards the definition of a holistic data quality lifecycle, in terms of the co-evolution of linked datasets, with the final aim of improving their quality.}, pages = {1\u201332}, numpages = {32}, keywords = {Data quality, linked data, quality assessment}}
@inproceedings{10.5555/2835377.2835395,title = {Big data: motivating the development of an advanced database systems course}, author = {Villa Adam H. },year = {2016}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {The creation of massive data sets, commonly referred to as Big Data, has motivated the development of new database systems and techniques for managing, monitoring, querying, and analyzing data [5]. As data sizes grow, so do the technologies developed to the meet this new demand, which in turn generates new employment opportunities for graduating students. Preparing students for these new positions requires the integration of these new techniques and methodologies into the curriculum. The growth of Big Data is motivating the development of advanced database courses. This paper presents an approach to creating such a course using flexible modules.}, pages = {119\u2013128}, numpages = {10}}
@inproceedings{10.1145/2872518.2890512,title = {Scholarly Big Data Knowledge and Semantics}, author = {Giles C. Lee },year = {2016}, isbn = {9781450341448}, publisher = {International World Wide Web Conferences Steering Committee}, address = {Republic and Canton of Geneva, CHE}, url = {https://doi.org/10.1145/2872518.2890512}, doi = {10.1145/2872518.2890512}, location = {Montr\u00e9al, Qu\u00e9bec, Canada}, series = {WWW '16 Companion}, pages = {371}, numpages = {1}, keywords = {semantic search, big data, digital libraries}}