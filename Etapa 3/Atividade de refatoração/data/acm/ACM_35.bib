@inproceedings{10.1145/3381343.3381345,title = {Big data driven genetic improvement for maintenance of legacy software systems}, author = {Langdon W. B. },year = {2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3381343.3381345}, doi = {10.1145/3381343.3381345}, abstract = {Software is vital to modern life, yet much of it is old and suffers from bit-rot. There are not and never will be enough software experts to keep it all up to date by hand. Instead we suggest combining data driven learning with evolutionary search to maintain computer systems. @RE: <1>N. Alshahwan. Industrial experience of genetic improvement in Facebook. In J. Petke, S. H. Tan, W. B. Langdon, and W. Weimer, editors, GI-2019, ICSE workshops proceedings, page 1, Montreal, 28 May 2019. IEEE. Invited Keynote. <2>W. Banzhaf, P. Nordin, R. E. Keller, and F. D. Francone. Genetic Programming - An Introduction; On the Automatic Evolution of Computer Programs and its Applications. Morgan Kaufmann, San Francisco, CA, USA, Jan. 1998. <3>N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary Computation, 9(2):159--195, Summer 2001. <4>S. Haraldsson, A. Brownlee, and J. R. Woodward. Computers will soon be able to fix themselves - are IT departments for the chop? The Conversation, page 3.29pm BST, Oct. 12 2017. <5>S. O. Haraldsson, J. R. Woodward, A. E. I. Brownlee, and K. Siggeirsdottir. Fixing bugs in your sleep: How genetic improvement became an overnight success. In J. Petke, D. R. White, W. B. Langdon, and W. Weimer, editors, GI-2017, pages 1513--1520, Berlin, 15--19 July 2017. ACM. Best paper. <6>M. Harman and B. F. Jones. Search based software engineering. Information and Software Technology, 43(14):833--839, Dec. 2001. <7>F. Hutter, H. H. Hoos, K. Leyton-Brown, and T. Stuetzle. ParamILS: An automatic algorithm configuration framework. JAIR, 36:267--306, 2009. <8>G. Kendall. Evolutionary computation has been promising self-programming machines for 60 years - so where are they? The Conversation, page 8.54am BST, Mar. 27 2018. <9>J. R. Koza. Genetic Programming: On the Programming of Computers by Natural Selection. MIT press, 1992. <10>W. B. Langdon. Genetic improvement of programs. In R. Matousek, editor, 18th International Con- ference on Soft Computing, MENDEL 2012, Brno, Czech Republic, 27--29 June 2012. Brno University of Technology. Invited keynote. <11>W. B. Langdon and M. Harman. Optimising existing software with genetic programming. IEEE Transactions on Evolutionary Computation, 19(1):118--135, Feb. 2015. <12>W. B. Langdon, B. Y. H. Lam, J. Petke, and M. Harman. Improving CUDA DNA analysis soft- ware with genetic programming. In S. Silva, A. I. Esparcia-Alcazar, M. Lopez-Ibanez, S. Mostaghim, J. Timmis, C. Zarges, L. Correia, T. Soule, M. Giacobini, R. Urbanowicz, Y. Akimoto, T. Glasmach- ers, F. Fernandez de Vega, A. Hoover, P. Larranaga, M. Soto, C. Cotta, F. B. Pereira, J. Handl, J. Koutnik, A. Gaspar-Cunha, H. Trautmann, J.-B. Mouret, S. Risi, E. Costa, O. Schuetze, K. Kraw- iec, A. Moraglio, J. F. Miller, P. Widera, S. Cagnoni, J. Merelo, E. Hart, L. Trujillo, M. Kessentini, G. Ochoa, F. Chicano, and C. Doerr, editors, GECCO '15: Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, pages 1063--1070, Madrid, 11--15 July 2015. ACM. <13>W. B. Langdon and R. Lorenz. Improving SSE parallel code with grow and graft genetic programming. In J. Petke, D. R. White, W. B. Langdon, and W. Weimer, editors, GI-2017, pages 1537--1538, Berlin, 15--19 July 2017. ACM. <14>W. B. Langdon and J. Petke. Evolving better software parameters. In T. E. Colanzi and P. McMinn, editors, SSBSE 2018 Hot off the Press Track, volume 11036 of LNCS, pages 363--369, Montpellier, France, 8--9 Sept. 2018. Springer. <15>W. B. Langdon, J. Petke, and R. Lorenz. Evolving better RNAfold structure prediction. In M. Castelli, L. Sekanina, and M. Zhang, editors, EuroGP 2018: Proceedings of the 21st European Conference on Genetic Programming, volume 10781 of LNCS, pages 220--236, Parma, Italy, 4--6 Apr. 2018. Springer Verlag. <16>C. Le Goues, M. Pradel, and A. Roychoudhury. Automated program repair. Communications of the ACM. To appear. <17>M. Orlov. Evolving software building blocks with FINCH. In J. Petke, D. R. White, W. B. Langdon, and W. Weimer, editors, GI-2017, pages 1539--1540, Berlin, 15--19 July 2017. ACM. <18>L. Perez Caceres, M. Lopez-Ibanez, H. Hoos, and T. Stuetzle. An experimental study of adaptive capping in irace. In R. Battiti, D. E. Kvasov, and Y. D. Sergeyev, editors, Learning and Intelligent Optimization - 11th International Conference, LION 11, Nizhny Novgorod, Russia, June 19--21, 2017, Revised Selected Papers, volume 10556 of Lecture Notes in Computer Science, pages 235--250. Springer, 2017. <19>J. Petke. Constraints: The future of combinatorial interaction testing. In 2015 IEEE/ACM 8th International Workshop on Search-Based Software Testing, pages 17--18, Florence, May 2015. <20>J. Petke, S. O. Haraldsson, M. Harman, W. B. Langdon, D. R. White, and J. R. Woodward. Genetic improvement of software: a comprehensive survey. IEEE Transactions on Evolutionary Computation, 22(3):415--432, June 2018. <21>J. Petke, M. Harman, W. B. Langdon, and W. Weimer. Specialising software for different downstream applications using genetic improvement and code transplantation. IEEE Transactions on Software Engineering, 44(6):574--594, June 2018. <22>R. Poli, W. B. Langdon, and N. F. McPhee. A field guide to genetic programming. Published via http://lulu.com and freely available at http://www.gp-field-guide.org.uk, 2008. (With contri- butions by J. R. Koza). <23>E. Schulte, S. Forrest, and W. Weimer. Automated program repair through the evolution of assembly code. In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering, pages 313--316, Antwerp, 20--24 Sept. 2010. ACM. <24>E. Schulte, W. Weimer, and S. Forrest. Repairing COTS router firmware without access to source code or test suites: A case study in evolutionary software repair. In W. B. Langdon, J. Petke, and D. R. White, editors, Genetic Improvement 2015 Workshop, pages 847--854, Madrid, 11--15 July 2015. ACM. Best Paper. <25>J. R. Woodward, J. Petke, and W. Langdon. How computers are learning to make human software work more efficiently. The Conversation, page 10.08am BST, June 25 2015. <26>K. Yeboah-Antwi and B. Baudry. Online genetic improvement on the java virtual machine with ECSELR. Genetic Programming and Evolvable Machines, 18(1):83--109, Mar. 2017.}, pages = {6\u20139}, numpages = {4}}
@inproceedings{10.1145/2818869.2818871,title = {Cloud Based Dynamical Digital Game Learning Scenario Corresponding to Individual Learner Big Data}, author = {Jian Ming-Shen , Chen Jun-Lin , Fang Yi-Chi , Li Siang-Jyun },year = {2015}, isbn = {9781450337359}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2818869.2818871}, doi = {10.1145/2818869.2818871}, abstract = {In this paper, we propose a Cloud Based Dynamical Digital Game Learning Scenario Corresponding to Individual Learner Big Data according to the proposed dynamic assigning algorithm to separate individual learner into different levels. In dynamic assigning algorithm, student's answering times and answering correct rate for each education scenario in cloud based digital game learning system is considered. Suitable teaching scenario will be provided for individual user corresponding to personal learning ability. Each designed modular teaching materials can be embedded into the game platform on the virtual machine in cloud. The information about users in the individual game learning system can be stored in the cloud database. In addition, the whole learning system is established as a virtual machine. System maintainer can configure the learning system easily and quickly. Based on cloud, different remote devices can connect to server for learning.}, location = {Kaohsiung, Taiwan}, series = {ASE BD&amp;SI '15}, pages = {1\u20136}, numpages = {6}, keywords = {Virtual Machine, Dynamic assigning algorithm, Cloud Computing, RPG, Game-Learning, Digital game, Virtualization}}
@inproceedings{10.14778/3137628.3137634,title = {Comparative evaluation of big-data systems on scientific image analytics workloads}, author = {Mehta Parmita , Dorkenwald Sven , Zhao Dongfang , Kaftan Tomer , Cheung Alvin , Balazinska Magdalena , Rokem Ariel , Connolly Andrew , Vanderplas Jacob , AlSayyad Yusra },year = {2017}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3137628.3137634}, doi = {10.14778/3137628.3137634}, abstract = {Scientific discoveries are increasingly driven by analyzing large volumes of image data. Many new libraries and specialized database management systems (DBMSs) have emerged to support such tasks. It is unclear how well these systems support real-world image analysis use cases, and how performant the image analytics tasks implemented on top of such systems are. In this paper, we present the first comprehensive evaluation of large-scale image analysis systems using two real-world scientific image data processing use cases. We evaluate five representative systems (SciDB, Myria, Spark, Dask, and TensorFlow) and find that each of them has shortcomings that complicate implementation or hurt performance. Such shortcomings lead to new research opportunities in making large-scale image analysis both efficient and easy to use.}, pages = {1226\u20131237}, numpages = {12}}
@inproceedings{10.1145/2788402.2788404,title = {Special Issue on Performance and Resource Management in Big Data Applications}, author = {Ardagna Danilo , Squillante Mark S. },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2788402.2788404}, doi = {10.1145/2788402.2788404}, pages = {2}, numpages = {1}}
@inproceedings{10.1145/3488466.3488485,title = {A Study on the Influencing Factors and Cultivation Path of Engineering College Students' Leadership from the Perspective of Big Data}, author = {Tao Yuan , Ren Weixin },year = {2021}, isbn = {9781450384995}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3488466.3488485}, doi = {10.1145/3488466.3488485}, abstract = {Leadership is an indispensable force to promote social reform and progress. At present, research on the leadership of college students has become an issue that the education circle generally attaches importance to, and has received widespread attention from the society. In order to understand in detail the influencing factors of the leadership of college students, and then explore an effective way to cultivate leadership, this paper collects the big data related to students in the \u201cLeader Class of Dong Liang Project\u201d (student leadership training program) in X School of a \u201cDouble Top\u201d university in western China and compares them with those of non-\u201cLeader Class\u201d students, and finds that there are obvious differences between the two groups in academic performance, student leader experience and leadership training project experience. Subsequently, based on the questionnaire analysis of 375 college students, it further proves that the experience of serving as student leaders and leadership training program has a significant positive effect on the improvement of leadership of college students. On this basis, it is suggested that to cultivate the leadership of college students, we should pay attention to the construction and development of student societies, build a benign community ecology of students, integrate Internet technology to optimize the training mode of student leaders and build an innovative training platform for the leadership of college students.}, location = {Busan, Republic of Korea}, series = {ICDTE 2021}, pages = {151\u2013155}, numpages = {5}, keywords = {Leadership, Big data, Engineering college students}}
@inproceedings{10.1109/CCGrid.2014.129,title = {A cloud-based framework for supporting effective and efficient OLAP in big data environments}, author = {Cuzzocrea Alfredo , Moussa Rim },year = {2014}, isbn = {9781479927838}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2014.129}, doi = {10.1109/CCGrid.2014.129}, abstract = {Inspired by the emerging Big Data challenge, in this paper we provide the description of OLAP*, a Cloud-based framework for supporting effective and efficient OLAP in Big Data environments. OLAP* combines data warehouse partitioning techniques with Cloud Computing paradigms, and provides a suitable implementation on top of the well-known ROLAP server Mondrian where the main task consists in applying meaningful transformation of multidimensional database schemas. We complement our analytical contribution by mean of a case study showing the effectiveness of our framework in a practical setting.}, location = {Chicago, Illinois}, series = {CCGRID '14}, pages = {680\u2013684}, numpages = {5}}
@inproceedings{10.1145/2493190.2499469,title = {Informing future design via large-scale research methods and big data}, author = {Rost Mattias , Morrison Alistair , Cramer Henriette , Bentley Frank },year = {2013}, isbn = {9781450322737}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2493190.2499469}, doi = {10.1145/2493190.2499469}, abstract = {With the launch of 'app stores' on several mobile platforms and the great uptake of smartphones among the general population, researchers have begun utilising these distribution channels to deploy research software to large numbers of users. Previous Research In The Large workshops have sought to establish base-line practice in this area. We have seen the use of app stores as being successful as a methodology for gathering large amounts of data, leading to design implications, but we have yet to explore the full potential for this data's use and interpretation. How is it possible to leverage the practices of large-scale research, beyond the current approaches, to more directly inform future designs? We propose that the time is right to re-energise discussions on large-scale research, looking further than the basic methodological issues and assessing the potential for informing the design of new mobile software.}, location = {Munich, Germany}, series = {MobileHCI '13}, pages = {612\u2013615}, numpages = {4}, keywords = {design, large-scale mobile hci, research in the large, mass participation, app stores, user study}}
@inproceedings{10.1145/3494454,title = {Editorial: Special Issue on Data Transparency\u2014Data Quality, Annotation, and Provenance}, author = {Barhamgi Mahmoud , Bertino Elisa },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3494454}, doi = {10.1145/3494454}, pages = {1\u20133}, numpages = {3}, keywords = {Data transparency, data provenance, fairness, privacy, accountability, data quality and annotation}}
@inproceedings{10.1145/3482632.3487469,title = {Path Analysis of Pre-school Education Talents Training under the Background of Big Data}, author = {Han Ying },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3487469}, doi = {10.1145/3482632.3487469}, abstract = {With the improvement of Chinese people's living standards, the cause of preschool education has developed rapidly in China, and the demand for preschool teachers and administrators has increased sharply. According to the progress of the times and the changes of society, it is very important and necessary to constantly optimize and perfect the existing teaching mode and personnel training mode. Big data is used in all aspects of the computer industry, especially the recommendation information for a single user is a common application, which brings a lot of inspiration to the education industry. In teaching, we can use big data to analyze children, and know what children need, what courses they are interested in, and how they react to different teachers' teaching methods. Facing the new situation, new tasks and new opportunities for development, normal vocational colleges should speed up the training of high-quality preschool teachers to meet the needs of the whole society for the development of preschool education. Based on this, this paper discusses and analyzes the reform path of pre-school education talents training in higher vocational colleges under big data.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {2554\u20132557}, numpages = {4}}
@inproceedings{10.5555/3398761.3398924,title = {Using Cognitive Models to Train Big Data Models with Small Data}, author = {Trafton J. Gregory , Hiatt Laura M. , Brumback Benjamin , McCurry J. Malcolm },year = {2020}, isbn = {9781450375184}, publisher = {International Foundation for Autonomous Agents and Multiagent Systems}, address = {Richland, SC}, abstract = {Modeling and predicting human behavior pose a difficult challenge for AI and other related fields. Some current techniques (e.g., cognitive architectures) are able to model people's goals and actions from little data, but have poor predictive capabilities. Other methods (e.g., deep networks) have strong predictive capabilities but require large amounts of data to train the model; such abundant empirical data on human performance is not available for many human-based tasks. We show a novel and general method of generating copious synthetic data of human behavior using a cognitive architecture, and then use the data to train a deep network classifier to predict ensuing human actions. We test our approach by predicting human actions on a supervisory control task; the results show that our approach provides superior prediction when compared to training a classifier with only (limited) empirical data.}, location = {Auckland, New Zealand}, series = {AAMAS '20}, pages = {1413\u20131421}, numpages = {9}, keywords = {act-r, cognitive models, agent-based analysis of human interactions, agents for improving human cooperative activities}}
@inproceedings{10.1145/3404555.3404642,title = {Research on Search Intent Prediction for Big Data of National Grid System Standards}, author = {Xueyong Hu , Bei Wang , Lei Zhao , Yang Yang , Aiyu Hu , Ge Pan , Baoxian Zhou },year = {2020}, isbn = {9781450377089}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3404555.3404642}, doi = {10.1145/3404555.3404642}, abstract = {Smart grids are becoming more complex due to the development of big data., and technical documents and institutional standards are constantly updated. As a result, It is difficult for workers in different positions to obtain the required information and data. This thesis is oriented towards this problem, and combined with deep learning algorithms to build a user intent prediction model based on the existing knowledge map. By extracting user characteristics and using a dynamic matching algorithm, the purpose of intent prediction is achieved. In this way, the required standards and requirements can be found faster and more directly in the work process, which effectively improves the working efficiency of employees and reduces the difficulty of learning and training.}, location = {Tianjin, China}, series = {ICCAI '20}, pages = {89\u201393}, numpages = {5}, keywords = {convolutional neural network, search intent prediction, knowledge map, Deep learning, personalization, dynamic matching}}
@inproceedings{10.1145/3503047.3503090,title = {Facial recognition with mask during pandemic period by big data technical of GMM}, author = {Hsieh Su-Tzu , Chen Chin-Ta },year = {2021}, isbn = {9781450385862}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3503047.3503090}, doi = {10.1145/3503047.3503090}, abstract = {At this pandemic period, for the safety demand of emigration, footprint tracking of disease carrier, pandemic control\u2026etc., it is urgent as well as important to do an automatic recognition of a person with mask. This study uses Mel-frequency Cep-strum technic to simulate and extract human features; uses big data technician of supervising learning method and VQGMM to find out the impact factors of human features that affecting human recognition hit rate. This study using same algorithm to do four time of testing with mask and without mask. The study result show, after supervising training, the testing result of the people with mask is better than without mask which gave evidence of the algorithms of this study is robust.}, location = {Sanya, China}, series = {AISS 2021}, pages = {1\u20133}, numpages = {3}, keywords = {feature extracting, GMM, facial recognition of individual, Mel-frequency Cestrum extraction}}
@inproceedings{10.1145/3075564.3078883,title = {Towards Big Data Visualization for Monitoring and Diagnostics of High Volume Semiconductor Manufacturing}, author = {Gkorou Dimitra , Ypma Alexander , Tsirogiannis George , Giollo Manuel , Sonntag Dag , Vinken Geert , van Haren Richard , van Wijk Robert Jan , Nije Jelle , Hoogenboom Tom },year = {2017}, isbn = {9781450344876}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3075564.3078883}, doi = {10.1145/3075564.3078883}, abstract = {In semiconductor manufacturing, continuous on-line monitoring prevents production stop and yield loss. The challenges towards this accomplishment are: 1) the complexity of lithography machines which are composed of hundreds of mechanical and optical components, 2) the high rate and volume data acquisition from different lithography and metrology machines, and 3) the scarcity of performance measurements due to their cost. This paper addresses these challenges by 1) visualizing and ranking the most relevant factors to a performance metric, 2) organizing efficiently Big Data from different sources and 3) predicting the performance with machine learning when measurements are lacking. Even though this project targets semiconductor manufacturing, its methodology is applicable to any case of monitoring complex systems, with many potentially interesting features, and imbalanced datasets.}, location = {Siena, Italy}, series = {CF'17}, pages = {338\u2013342}, numpages = {5}, keywords = {anomaly detection, machine learning, visualization of high dimensional data, continuous monitoring of high volume manufacturing, data science, analytics}}
@inproceedings{10.5555/645926.671870,title = {Telcordia's Database Reconciliation and Data Quality Analysis Tool}, author = {Caruso Francesco , Cochinwala Munir , Ganapathy Uma , Lalk Gail , Missier Paolo },year = {2000}, isbn = {1558607153}, publisher = {Morgan Kaufmann Publishers Inc.}, address = {San Francisco, CA, USA}, series = {VLDB '00}, pages = {615\u2013618}, numpages = {4}}
@inproceedings{10.1145/3325112.3325212,title = {Processes, Potential Benefits, and Limitations of Big Data Analytics: A Case Analysis of 311 Data from City of Miami}, author = {Hagen Loni , Seon Yi Hye , Pietri Siana , E. Keller Thomas },year = {2019}, isbn = {9781450372046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3325112.3325212}, doi = {10.1145/3325112.3325212}, abstract = {As part of the open government movement, an increasing number of 311 call centers have made their datasets available to the public. Studies have found that 311 request patterns are associated with personal attributes and living conditions. Most of these studies used New York City 311 data. In this study, we use 311 data from the City of Miami, a smaller local government, as a case study. This study contributes to digital government research and practices by making suggestions on best practices regarding the use of big data analytics on 311 data. In addition, we discuss limitations of 311 data and analytics results. Finally, we expect our results to inform decision making within the City of Miami government and other local governments.}, location = {Dubai, United Arab Emirates}, series = {dg.o 2019}, pages = {1\u201310}, numpages = {10}, keywords = {big data analytics, information visualization, e-government, 311 data}}
@inproceedings{10.1145/2588555.2612174,title = {Histograms as a side effect of data movement for big data}, author = {Istvan Zsolt , Woods Louis , Alonso Gustavo },year = {2014}, isbn = {9781450323765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2588555.2612174}, doi = {10.1145/2588555.2612174}, abstract = {Histograms are a crucial part of database query planning but their computation is resource-intensive. As a consequence, generating histograms on database tables is typically performed as a batch job, separately from query processing. In this paper, we show how to calculate statistics as a side effect of data movement within a DBMS using a hardware accelerator in the data path. This accelerator analyzes tables as they are transmitted from storage to the processing unit, and provides histograms on the data retrieved for queries at virtually no extra performance cost. To evaluate our approach, we implemented this accelerator on an FPGA. This prototype calculates histograms faster and with similar or better accuracy than commercial databases. Moreover, the FPGA can provide various types of histograms such as Equi-depth, Compressed, or Max-diff on the same input data in parallel, without additional overhead.}, location = {Snowbird, Utah, USA}, series = {SIGMOD '14}, pages = {1567\u20131578}, numpages = {12}, keywords = {query optimization, histogram, FPGA, statistics}}
@inproceedings{10.1145/2790755.2790762,title = {Big Data Techniques For Supporting Accurate Predictions of Energy Production From Renewable Sources}, author = {Ceci Michelangelo , Corizzo Roberto , Fumarola Fabio , Ianni Michele , Malerba Donato , Maria Gaspare , Masciari Elio , Oliverio Marco , Rashkovska Aleksandra },year = {2015}, isbn = {9781450334143}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2790755.2790762}, doi = {10.1145/2790755.2790762}, abstract = {Predicting the output power of renewable energy production plants distributed on a wide territory is a really valuable goal, both for marketing and energy management purposes. Vi-POC (Virtual Power Operating Center) project aims at designing and implementing a prototype which is able to achieve this goal. Due to the heterogeneity and the high volume of data, it is necessary to exploit suitable Big Data analysis techniques in order to perform a quick and secure access to data that cannot be obtained with traditional approaches for data management. In this paper, we describe Vi-POC -- a distributed system for storing huge amounts of data, gathered from energy production plants and weather prediction services. We use HBase over Hadoop framework on a cluster of commodity servers in order to provide a system that can be used as a basis for running machine learning algorithms. Indeed, we perform one-day ahead forecast of PV energy production based on Artificial Neural Networks in two learning settings, that is, structured and non-structured output prediction. Preliminary experimental results confirm the validity of the approach, also when compared with a baseline approach.}, location = {Yokohama, Japan}, series = {IDEAS '15}, pages = {62\u201371}, numpages = {10}}
@inproceedings{10.1145/3178158.3178190,title = {A case study of ICT used by big data processing in education: discuss on visualization of RE research paper}, author = {Tsujioka Keiko },year = {2018}, isbn = {9781450353595}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3178158.3178190}, doi = {10.1145/3178158.3178190}, abstract = {The latest technologies and information have been introduced one after another at the present field of education. On the other hand, however, it is questionable, whether those technologies and information are understood conveniently. In the case of research papers, for example, is really it true that the paper has been written well about her/his idea, in which the author would like to express? From those questions, we have carried out a survey of research papers.In this paper, we have an awareness especially concerning with academic writing as our core research question. As one of the problem solving, we have extracted the main problems of writing and found out the solution by referring to visualized RE research paper in the field of mechanical technologies.We consider that the contents of the visualized RE (Rotary Engine) research paper are written clearly and logically about the latest technologies and information.}, location = {Osaka, Japan}, series = {ICIET '18}, pages = {160\u2013164}, numpages = {5}, keywords = {methodology of clear writing paper, big data processing, conceptual metaphor, visualization, cognitive processing, human information processing}}
@inproceedings{10.1145/3419635.3419734,title = {Empirical Study of Big Data Mining Technology in English Teaching Integration and Optimization Analysis}, author = {Qiu Chenxia },year = {2020}, isbn = {9781450387729}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3419635.3419734}, doi = {10.1145/3419635.3419734}, abstract = {In recent years, teaching staff has increased the frequency of application of multimedia teaching equipment in subject teaching. The scientific application of the teaching staff to the database can gain efficiency of teaching for English. According to situation, we describe the role of English teaching. This article uses big data, takes college English teaching as the research object, applies data mining technology, analyses from the teaching environment, faculty, teaching process and students' learning methods, learning motivation, learning time and other factors, statistics of college English learning behaviour data. Combined with questionnaire survey and interview, the validity of the model in learning activities, knowledge acquisition and other aspects are verified. Results show the English teaching model can improve teaching effect under the big data environment.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2020}, pages = {495\u2013499}, numpages = {5}, keywords = {Teaching, Data mining, Optimization analysis, Empirical research}}
@inproceedings{10.1145/2749469.2750381,title = {Towards sustainable in-situ server systems in the big data era}, author = {Li Chao , Hu Yang , Liu Longjun , Gu Juncheng , Song Mingcong , Liang Xiaoyao , Yuan Jingling , Li Tao },year = {2015}, isbn = {9781450334020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2749469.2750381}, doi = {10.1145/2749469.2750381}, abstract = {Recent years have seen an explosion of data volumes from a myriad of distributed sources such as ubiquitous cameras and various sensors. The challenges of analyzing these geographically dispersed datasets are increasing due to the significant data movement overhead, time-consuming data aggregation, and escalating energy needs. Rather than constantly move a tremendous amount of raw data to remote warehouse-scale computing systems for processing, it would be beneficial to leverage in-situ server systems (InS) to pre-process data, i.e., bringing computation to where the data is located.This paper takes the first step towards designing server clusters for data processing in the field. We investigate two representative in-situ computing applications, where data is normally generated from environmentally sensitive areas or remote places that lack established utility infrastructure. These very special operating environments of in-situ servers urge us to explore standalone (i.e., off-grid) systems that offer the opportunity to benefit from local, self-generated energy sources. In this work we implement a heavily instrumented proof-of-concept prototype called InSURE: in-situ server systems using renewable energy. We develop a novel energy buffering mechanism and a unique joint spatio-temporal power management strategy to coordinate standalone power supplies and in-situ servers. We present detailed deployment experiences to quantify how our design fits with in-situ processing in the real world. Overall, InSURE yields 20%~60% improvements over a state-of-the-art baseline. It maintains impressive control effectiveness in under-provisioned environment and can economically scale along with the data processing needs. The proposed design is well complementary to today's grid-connected cloud data centers and provides competitive cost-effectiveness.}, location = {Portland, Oregon}, series = {ISCA '15}, pages = {14\u201326}, numpages = {13}}
@inproceedings{10.1145/2872887.2750381,title = {Towards sustainable in-situ server systems in the big data era}, author = {Li Chao , Hu Yang , Liu Longjun , Gu Juncheng , Song Mingcong , Liang Xiaoyao , Yuan Jingling , Li Tao },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2872887.2750381}, doi = {10.1145/2872887.2750381}, abstract = {Recent years have seen an explosion of data volumes from a myriad of distributed sources such as ubiquitous cameras and various sensors. The challenges of analyzing these geographically dispersed datasets are increasing due to the significant data movement overhead, time-consuming data aggregation, and escalating energy needs. Rather than constantly move a tremendous amount of raw data to remote warehouse-scale computing systems for processing, it would be beneficial to leverage in-situ server systems (InS) to pre-process data, i.e., bringing computation to where the data is located.This paper takes the first step towards designing server clusters for data processing in the field. We investigate two representative in-situ computing applications, where data is normally generated from environmentally sensitive areas or remote places that lack established utility infrastructure. These very special operating environments of in-situ servers urge us to explore standalone (i.e., off-grid) systems that offer the opportunity to benefit from local, self-generated energy sources. In this work we implement a heavily instrumented proof-of-concept prototype called InSURE: in-situ server systems using renewable energy. We develop a novel energy buffering mechanism and a unique joint spatio-temporal power management strategy to coordinate standalone power supplies and in-situ servers. We present detailed deployment experiences to quantify how our design fits with in-situ processing in the real world. Overall, InSURE yields 20%~60% improvements over a state-of-the-art baseline. It maintains impressive control effectiveness in under-provisioned environment and can economically scale along with the data processing needs. The proposed design is well complementary to today's grid-connected cloud data centers and provides competitive cost-effectiveness.}, pages = {14\u201326}, numpages = {13}}
@inproceedings{10.1145/2733373.2807985,title = {A Semantic Geo-Tagged Multimedia-Based Routing in a Crowdsourced Big Data Environment}, author = {Rehman Faizan Ur , Lbath Ahmed , Murad Abdullah , Rahman Md. Abdur , Sadiq Bilal , Ahmad Akhlaq , Qamar Ahmad , Basalamah Saleh },year = {2015}, isbn = {9781450334594}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2733373.2807985}, doi = {10.1145/2733373.2807985}, abstract = {Traditional routing algorithms for calculating the fastest or shortest path become ineffective or difficult to use when both source and destination are dynamic or unknown. To solve the problem, we propose a novel semantic routing system that leverages geo-tagged rich crowdsourced multimedia information such as images, audio, video and text to add semantics to the conventional routing. Our proposed system includes a Semantic Multimedia Routing Algorithm (SMRA) that uses an indexed spatial big data environment to answer multimedia spatio-temporal queries in real-time. The results are customized to the users' smartphone bandwidth and resolution requirements. The system has been designed to be able to handle a very large number of multimedia spatio-temporal requests at any given moment. A proof of concept of the system will be demonstrated through two scenarios. These are 1) multimedia enhanced routing and 2) finding lost individuals in a large crowd using multimedia. We plan to test the system's performance and usability during Hajj 2015, where over four million pilgrims from all over the world gather to perform their rituals.}, location = {Brisbane, Australia}, series = {MM '15}, pages = {759\u2013760}, numpages = {2}, keywords = {crowdsourcing, geo-tagged multimedia, spatio-temporal multimedia queries, semantic multimedia routing}}
@inproceedings{10.1145/3424978.3425006,title = {Research on the Influence of Big Data Knowledge Transfer on Value of Co-creation and Competitive Advantage of Smart Tourism Destinations}, author = {Bu Naipeng Tom , Yin Zihan , Barry Ng, Jay , Kong Haiyan },year = {2020}, isbn = {9781450377720}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3424978.3425006}, doi = {10.1145/3424978.3425006}, abstract = {Big data analysis used for smart tourism destinations has become a new trend in the increasing demand for tourism. However, relevant research on this problem is limited. This paper reports a literature review, in-depth interviews, and content analysis about tourism destinations and tourists. This study aims to discuss how knowledge transfer based on big data promotes the value of tourists and the competitive advantages of smart tourism destinations through value co-creation between them. An influencing model is explored to reflect such an interactive process. This study also makes some contributions to tourism destinations.}, location = {Sanya, China}, series = {CSAE 2020}, pages = {1\u20138}, numpages = {8}, keywords = {Value co-creation, Tourists, Value, Smart tourism destinations, Knowledge transfer, Big data}}
@inproceedings{10.1145/3495018.3495481,title = {Implementation of Parallel Algorithms for Liquid Metal Solidification Molecular Dynamics Based on Big Data}, author = {Huhemandula , Bai Jie , Ji Wenhui },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495481}, doi = {10.1145/3495018.3495481}, abstract = {With the advancement of science and technology and the popularization of the Internet, large amounts of data and information will be generated from people's daily work and life to the state of molecular motion trajectories. How to efficiently mine the hidden information of these massive data information is the key to scientific and social development, which requires big data analysis technology. The solidification process of metals is a very complex process, including both macroscopic and microscopic processes. Molecular dynamics can simulate the process of liquid metal solidification from a microscopic scale. In molecular dynamics simulation, the more particles the simulation system contains, the more accurate the data obtained. However, the molecular dynamics serial program runs slowly, and the number of simulated atoms is limited, which makes the simulation results far from the real situation. The rise of parallel computing provides conditions for large-scale molecular dynamics parallelization and promotes the rapid development of molecular dynamics parallelization. The purpose of this paper is to study the dynamic parallel algorithm of liquid metal solidification molecules based on big data. In this paper, through in-depth research on big data, analysis of the current research results of parallel computing of liquid metal solidification molecules, and combined with the current research status of liquid metal solidification molecules in my country, to discuss the research of liquid metal solidification molecular dynamics parallel algorithm under big data. Research shows that the spatial domain decomposition method is more general, and its communication cost is relatively small. It reflects that it can obtain relatively high parallel performance and computing efficiency in local communication, especially in large-scale simulation systems with a large number of atoms.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {1769\u20131773}, numpages = {5}}
@inproceedings{10.1145/3427476,title = {Potential Deep Learning Solutions to Persistent and Emerging Big Data Challenges\u2014A Practitioners\u2019 Cookbook}, author = {Mirza Behroz , Syed Tahir Q. , Khan Behraj , Malik Yameen },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3427476}, doi = {10.1145/3427476}, abstract = {The phenomenon of Big Data continues to present moving targets for the scientific and technological state-of-the-art. This work demonstrates that the solution space of these challenges has expanded with deep learning now moving beyond traditional applications in computer vision and natural language processing to diverse and core machine learning tasks such as learning with streaming and non-iid-data, partial supervision, and large volumes of distributed data while preserving privacy. We present a framework coalescing multiple deep methods and corresponding models as responses to specific Big Data challenges. First, we perform a detailed per-challenge review of existing techniques, with benchmarks and usage advice, and subsequently synthesize them together into one organic construct that we discover principally uses extensions of one underlying model, the autoencoder. This work therefore provides a synthesis where challenges at scale across the Vs of Big Data could be addressed by new algorithms and architectures being proposed in the deep learning community. The value being proposed to the reader from either community in terms of nomenclature, concepts, and techniques of the other would advance the cause of multi-disciplinary, transversal research and accelerate the advance of technology in both domains.}, pages = {1\u201339}, numpages = {39}, keywords = {extreme transfer learning, extreme learning machine, zero shot learning, semi-supervised learning, reinforcement learning, self-supervision, adaptive deep belief networks, ladder networks, regenerative chaining, Distributed deep learning, representation learning, federated learning}}
@inproceedings{10.1145/3510249.3510289,title = {Analysis on Influencing Factors of Pure Electric Vehicle Sales in Big Data Environment}, author = {ZENG RONG , XU ZHONGQING , LI HAN },year = {2021}, isbn = {9781450387392}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510249.3510289}, doi = {10.1145/3510249.3510289}, abstract = {The electric vehicle industry in the field of new energy is developing rapidly, but the proportion of pure electric vehicles in the whole automobile industry is relatively small. In order to promote the rapid development of pure electric vehicle industry, Firstly, combined with the big data of consumers' car purchase, this paper analyzes the influencing factors of electric vehicle sales, and then conduct a questionnaire on customers and car owners who have purchased vehicles in different Direct stores of Wei lai automobile, The reliability and validity of the questionnaire were verified by SPSS data processing software, and the hypotheses were effectively verified by regression analysis. The analysis results show that the assumption that the product performance image, brand image, consumers' perception of Wei lai pure electric vehicles and government policies have a significant impact on the sales of Wei lai vehicles is correct. Accordingly, the corresponding specific measures are put forward.}, location = {Sanya, China}, series = {EBEE 2021}, pages = {223\u2013226}, numpages = {4}}
@inproceedings{10.1145/3510858.3511002,title = {Design of an Accurate Recommendation System for Mathematical Resources Based on Big Data Technology}, author = {Wang Kuang },year = {2021}, isbn = {9781450390422}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510858.3511002}, doi = {10.1145/3510858.3511002}, abstract = {Mathematics has penetrated into various fields from natural sciences to social sciences, and is an important channel for cultivating logical thinking. In the era of big data, teaching resources are exploding. The degree of informatization of education continues to increase. Various new forms of teaching resources continue to appear. Accurate recommendation is based on students' learning behaviors and actively recommends learning resources of interest to students. In order to give full play to the role of mathematical teaching resources under the background of big data, this paper designs an accurate recommendation process, compares commonly used recommendation algorithms, and builds an accurate recommendation process for mathematical resources based on content recommendation. It does not require data from other users. There is the independence between the users, new resources can be recommended immediately, providing support for the precise recommendation system design of mathematical resources.}, location = {Changsha, China}, series = {ICASIT 2021}, pages = {528\u2013532}, numpages = {5}}
@inproceedings{10.1145/3482632.3483022,title = {Enhancing and Ameliorating Ideological and Political Education in Universities under the Context of Big Data}, author = {Li Huan },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483022}, doi = {10.1145/3482632.3483022}, abstract = {Since big data technology has largely affected and altered people's learning and life style, and at the same time it has brought about good luck and requirements for ideological and political education in universities such as never previously existed. By using big data, educators can know students' lives as a whole and predict students' conditions in advance. However, it is difficult to guide students to rationally use big data and effectively integrate and refine big data. Under the condition of a large amount of data, educators must change their ideas and set up data awareness. They also need to integrate data and realize resource sharing. They should improve their professional quality and the skills to put big data into their education life, try to integrate traditional ideological and political education methods and big data technology, enhance and ameliorate the corresponding work of education in universities so as to blaze new trails in modern times.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {800\u2013802}, numpages = {3}}
@inproceedings{10.5555/2693848.2693962,title = {Handling big data on agent-based modeling of online social networks with mapreduce}, author = {de C. Gatti Ma\u00edra A. , Vieira Marcos R. , de Melo Jo\u00e3o Paulo F. , Cavalin Paulo Rodrigo , Pinhanez Claudio Santos },year = {2014}, publisher = {IEEE Press}, abstract = {There is an increasing interest on using Online Social Networks (OSNs) in a wide range of applications. Two interesting problems that have received a lot of attention in OSNs is how to provide effective ways to understand and predict how users behave, and how to build accurate models for specific domains (e.g., marketing campaigns). In this context, stochastic multi-agent based simulation can be employed to reproduce the behavior observed in OSNs. Nevertheless, the first step to build an accurate behavior model is to create an agent-based system. Hence, a modeler needs not only to be effective, but also to scale up given the huge volume of streaming graph data. To tackle the above challenges, this paper proposes a MapReduce-based method to build a modeler to handle big data. We demonstrate in our experiments how efficient and effective our proposal is using the Obama's Twitter network on the 2012 U.S. presidential election.}, location = {Savannah, Georgia}, series = {WSC '14}, pages = {851\u2013862}, numpages = {12}}
@inproceedings{10.5555/3400397.3400444,title = {Solving challenges at the interface of simulation and big data using machine learning}, author = {Giabbanelli Philippe J. },year = {2019}, isbn = {9781728132839}, publisher = {IEEE Press}, abstract = {Modeling & Simulation (M&S) and Machine Learning (ML) have been used separately for decades. They can also straightforwardly be employed in the same study by contrasting the results of a theory-driven M&S model with the most accurate data-driven ML model. In this paper, we propose a paradigm shift from seeing ML and M&S as two independent activities to identifying how their integration can solve challenges that emerge in a big data context. Since several works have already examined this interaction for conceptual modeling or model building (e.g., creating components with ML and embedding them in the M&S model), our analysis is devoted on three relatively under-studied stages: calibrating a simulation model using ML, dealing with the issues of large search space by employing ML for experimentation, and identifying the right visualizations of model output by applying ML to characteristics of the output or actions of the users.}, location = {National Harbor, Maryland}, series = {WSC '19}, pages = {572\u2013583}, numpages = {12}}
@inproceedings{10.1145/3456887.3459714,title = {Analysis of the Mechanism of Cross-Border E-Commerce Multilateral Trade Integration from the Perspective of Big Data Collaboration Theory}, author = {Li Min },year = {2021}, isbn = {9781450389969}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456887.3459714}, doi = {10.1145/3456887.3459714}, abstract = {This paper provides an in-depth analysis of the mechanism of cross-border e-commerce multilateral trade integration based on the big data synergy theory, and uses the big data synergy theory method to form an overall causal feedback map of the credit synergy system by analysing the causal feedback relationship between the static cross-border e-commerce credit synergy system indicators. Through the form of an expert questionnaire, the relatively important system indicators are selected and the indicator weights are calculated. Analyse the existing problems and put forward optimization suggestions; innovative research methods, this paper uses supply chain management performance indicators to measure the level of supply chain management optimization, combined with factor analysis to find out the factors that affect the supply chain optimization of cross-border e-commerce importers. Analysing the implementation effect of cross-border e-commerce policy system, exploring effective policy \"combinations\" to promote the development of cross-border e-commerce, and realizing the quantitative analysis of the whole process of cross-border e-commerce policy from policy choice, policy implementation to the response to international trade shocks is an effective way to ensure that the policy system is formulated scientifically and rationally and to solve the problem of police inefficiency and maladaptation.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2021}, pages = {1525\u20131530}, numpages = {6}, keywords = {Synergy theory, Mechanisms of multilateral trade convergence, Cross-border e-commerce, Big data}}
@inproceedings{10.1145/2831244.2831253,title = {Big data analytics on traditional HPC infrastructure using two-level storage}, author = {Xuan Pengfei , Denton Jeffrey , Srimani Pradip K. , Ge Rong , Luo Feng },year = {2015}, isbn = {9781450339933}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2831244.2831253}, doi = {10.1145/2831244.2831253}, abstract = {Data-intensive computing has become one of the major workloads on traditional high-performance computing (HPC) clusters. Currently, deploying data-intensive computing software framework on HPC clusters still faces performance and scalability issues. In this paper, we develop a new two-level storage system by integrating Tachyon, an in-memory file system with OrangeFS, a parallel file system. We model the I/O throughputs of four storage structures: HDFS, OrangeFS, Tachyon and two-level storage. We conduct computational experiments to characterize I/O throughput behavior of two-level storage and compare its performance to that of HDFS and OrangeFS, using TeraSort benchmark. Theoretical models and experimental tests both show that the two-level storage system can increase the aggregate I/O throughputs. This work lays a solid foundation for future work in designing and building HPC systems that can provide a better support on I/O intensive workloads with preserving existing computing resources.}, location = {Austin, Texas}, series = {DISCS '15}, pages = {1\u20138}, numpages = {8}, keywords = {OrangeFS, data-intensive computing, file system design, hadoop, two-level storage, high performance computing, tachyon, HPC}}
@inproceedings{10.5555/2831432.2831475,title = {Teaching big data through project-based learning in computational linguistics and information retrieval}, author = {Gruss Richard , Farag Mohamed , Kanan Tarek , English Mary C. , Zhang Xuan , Fox Edward A. },year = {2015}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {In Project Based Learning (PBL), students acquire knowledge just-in-time while completing a large project driven by a particular question. We demonstrate that this approach is particularly well suited to courses in two computer science (CS) domains pertaining to Big Data: Computational Linguistics and Information Retrieval. The courses presented here proved successful, as evidenced by both the high quality of the student projects and the positive responses on end-of-semester surveys.}, pages = {260\u2013270}, numpages = {11}}
@inproceedings{10.1145/3514221.3517892,title = {Juggler: Autonomous Cost Optimization and Performance Prediction of Big Data Applications}, author = {Al-Sayeh Hani , Memishi Bunjamin , Jibril Muhammad Attahir , Paradies Marcus , Sattler Kai-Uwe },year = {2022}, isbn = {9781450392495}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3514221.3517892}, doi = {10.1145/3514221.3517892}, abstract = {Distributed in-memory processing frameworks accelerate iterative workloads by caching suitable datasets in memory rather than recomputing them in each iteration. Selecting appropriate datasets to cache as well as allocating a suitable cluster configuration for caching these datasets play a crucial role in achieving optimal performance. In practice, both are tedious, time-consuming tasks and are often neglected by end users, who are typically not aware of workload semantics, sizes of intermediate data, and cluster specification.To address these problems, we present Juggler, an end-to-end framework, which autonomously selects appropriate datasets for caching and recommends a correspondingly suitable cluster configuration to end users, with the aim of achieving optimal execution time and cost. We evaluate Juggler on various iterative, real-world, machine learning applications. Compared with our baseline, Juggler reduces execution time to 25.1% and cost to 58.1%, on average, as a result of selecting suitable datasets for caching. It recommends optimal cluster configuration in 50% of cases and near-to-optimal configuration in the remaining cases. Moreover, Juggler achieves an average performance prediction accuracy of 90%.}, location = {Philadelphia, PA, USA}, series = {SIGMOD '22}, pages = {1840\u20131854}, numpages = {15}, keywords = {database caching, cluster configuration, performance prediction}}
@inproceedings{10.1145/3510249.3510302,title = {Research on Risk Evaluation of Emergency Network Public Opinion Based on PSR - AHP Model under the Background of Big Data}, author = {QI KAI , WO XINYUE , WEI XIAOYU },year = {2021}, isbn = {9781450387392}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510249.3510302}, doi = {10.1145/3510249.3510302}, abstract = {In the big data environment, information is mixed, and multiple information collisions are inevitable to generate risks. Risks occur from time to time in online public opinion of emergencies. How to correctly and effectively evaluate the risks of online public opinion of emergencies is an urgent problem to be solved. Analyzing the influencing factors of network public opinion risks and constructing a risk evaluation index system can effectively evaluate risks, not only provide scientific basis for government governance, but also provide a more valuable basis for the clarity and guidance of big data environment. Combining PSR and AHP, the risk evaluation index system of emergency network public opinion was constructed, the weight of each index was calculated by MATLAB, and the case of \"Epidemic situation in Wangkui County, Heilongjiang Province\" is analyzed by fuzzy comprehensive evaluation. The PSR model is introduced to analyze the life evolution cycle of online public opinion of emergencies. According to the weight results of online public opinion risk evaluation index of emergencies, it was concluded that the risk of \"Epidemic situation in Wangkui County, Heilongjiang Province\" was between the third-level risk and the fourth-level risk, it is indicate that the risk evaluation results are consistent with the actual situation. It has guiding significance for future research on risk evaluation of network public opinion related to emergencies.}, location = {Sanya, China}, series = {EBEE 2021}, pages = {303\u2013310}, numpages = {8}, keywords = {Risk assessment, Crisis management, PSR \u2013 AHP, Emergency, Network public opinion risk, Big data}}
@inproceedings{10.1145/3495018.3495131,title = {Supplier Evaluation of Smart Grid Material Suppliers Based on Big Data Environment and AHP}, author = {Teng Yun , Wang Jianjun , Wang Ran },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495131}, doi = {10.1145/3495018.3495131}, abstract = {In the advanced material manufacturing supply chain management of smart grid, the advanced manufacturing material suppliers are important external resources of the power grid enterprises. With the strengthening of environmental protection awareness, power grid is also willing to favor advanced manufacturing enterprises in selecting material suppliers. In the current situation of big data advanced manufacturing environment, it can easier to get related data of advanced manufacturing suppliers than before. This paper creates a frame of the big data analysis environment of the power grid and from an index system, then uses the analytic hierarchy process to study the current classification method of power material supply, makes a practical case study, and constructs a material supplier evaluation system in line with the smart grid, which provides constructive significance for the smart grid to select green material suppliers.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {628\u2013634}, numpages = {7}}
@inproceedings{10.1145/3395032.3395324,title = {CoreBigBench: Benchmarking big data core operations}, author = {Ivanov Todor , Ghazal Ahmad , Crolotte Alain , Kostamaa Pekka , Ghazal Yoseph },year = {2020}, isbn = {9781450380010}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3395032.3395324}, doi = {10.1145/3395032.3395324}, abstract = {Significant effort was put into big data benchmarking with focus on end-to-end applications. While covering basic functionalities implicitly, the details of the individual contributions to the overall performance are hidden. As a result, end-to-end benchmarks could be biased toward certain basic functions. Micro-benchmarks are more explicit at covering basic functionalities but they are usually targeted at some highly specialized functions. In this paper we present CoreBigBench, a benchmark that focuses on the most common big data engines/platforms functionalities like scans, two way joins, common UDF execution and more. These common functionalities are benchmarked over relational and key-value data models which covers majority of data models. The benchmark consists of 22 queries applied to sales data and key-value web logs covering the basic functionalities. We ran CoreBigBench on Hive as a proof of concept and verified that the benchmark is easy to deploy and collected performance data. Finally, we believe that CoreBigBench is a good fit for commercial big data engines performance testing focused on basic engine functionalities not covered in end-to-end benchmarks.}, location = {Portland, Oregon}, series = {DBTest '20}, pages = {1\u20136}, numpages = {6}, keywords = {BigBench, big data benchmarking, benchmark operations}}
@inproceedings{10.1109/CCGrid.2015.46,title = {The challenge of scaling genome big data analysis software on TH-2 supercomputer}, author = {Peng Shaoliang , Liao Xiangke , Yang Canqun , Lu Yutong , Liu Jie , Cui Yingbo , Wang Heng , Wu Chengkun , Wang Bingqiang },year = {2015}, isbn = {9781479980062}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2015.46}, doi = {10.1109/CCGrid.2015.46}, abstract = {Whole genome re-sequencing plays a crucial role in biomedical studies. The emergence of genomic big data calls for an enormous amount of computing power. However, current computational methods are inefficient in utilizing available computational resources. In this paper, we address this challenge by optimizing the utilization of the fastest supercomputer in the world - TH-2 supercomputer. TH-2 is featured by its neo-heterogeneous architecture, in which each compute node is equipped with 2 Intel Xeon CPUs and 3 Intel Xeon Phi coprocessors. The heterogeneity and the massive amount of data to be processed pose great challenges for the deployment of the genome analysis software pipeline on TH-2. Runtime profiling shows that SOAP3-dp and SOAPsnp are the most time-consuming components (up to 70% of total runtime) in a typical genome-analyzing pipeline. To optimize the whole pipeline, we first devise a number of parallel and optimization strategies for SOAP3-dp and SOAPsnp, respectively targeting each node to fully utilize all sorts of hardware resources provided both by CPU and MIC. We also employ a few scaling methods to reduce communication between different nodes. We then scaled up our method on TH-2. With 8192 nodes, the whole analyzing procedure took 8.37 hours to finish the analysis of a 300 TB dataset of whole genome sequences from 2,000 human beings, which can take as long as 8 months on a commodity server. The speedup is about 700x.}, location = {Shenzhen, China}, series = {CCGRID '15}, pages = {823\u2013828}, numpages = {6}, keywords = {whole genome re-sequencing, sequence alignment, TH-2 supercomputer, parallel optimization, SNP detection}}
@inproceedings{10.1145/3414274.3414511,title = {A Real-time Anomaly Detection Algorithm for Taxis Based on Trajectory Big Data}, author = {Zhu Jiahui , Chen Yuepeng , Fu Qingwen , Zhang Jiawen },year = {2020}, isbn = {9781450376044}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3414274.3414511}, doi = {10.1145/3414274.3414511}, abstract = {In order to prevent taxi drivers from deliberately detouring or other fraudulent behaviors, a real-time anomaly trajectory detection algorithm based on spatial-temporal big data of taxi is proposed to obtain the real-time detection results with fast feedback. Firstly, the road network map is divided into grid and numbered, and the feature points obtained with direction angle, then transform original trajectory into feature grid trajectory sequence by preprocessing such as matching and completion. Then, the common sequence of the trajectory is obtained from a large number of taxi historical data, and the standard K trajectory between the start point and the end point is obtained by clustering. When the real-time trajectory detection, it is matched with the standard K trajectory, only edit distance of two trajectory need to be calculated considering the spatial-temporal threshold. Finally, based on the real data set of taxi GPS trajectory in Beijing from March to May 2011, a large number of experiments are carried out, and verified the effectiveness and efficiency of the proposed algorithm.}, location = {Xiamen, China}, series = {DSIT 2020}, pages = {227\u2013231}, numpages = {5}, keywords = {spatial temporal mining, GPS trajectory, anomaly detection, cluster trajectory}}
@inproceedings{10.1145/3129676.3129719,title = {A Study on Prediction Comparison by Time Series Analysis Model of Load Big data}, author = {Kim Jaehyun , Kim Taehyoung , Ham Kyung Sun },year = {2017}, isbn = {9781450350273}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3129676.3129719}, doi = {10.1145/3129676.3129719}, abstract = {As energy supply changes become more important, interest in the field of efficient energy management is increasing. In this paper, demand forecasting is performed through time series analysis of power big data. And we measured the performance through predictive comparison of time series prediction model.}, location = {Krakow, Poland}, series = {RACS '17}, pages = {75\u201376}, numpages = {2}, keywords = {Model Comparison, Bigdata Analysis, Energy Forecast, Datamining, Time Series Data}}
@inproceedings{10.1145/3407703.3407706,title = {Analyzing Spatial-Temporal Patterns of House Price Based on Network Big Data in the Main City Zone of Kunming}, author = {Zhao Zhengxian , Xu Quanli , Peng Shuangyun , Hong Liang },year = {2020}, isbn = {9781450377270}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3407703.3407706}, doi = {10.1145/3407703.3407706}, abstract = {With the rapid development of information technologies such as the Internet, the Internet of Things and cloud computing, network big data is widely used in various fields of society, and making good use of network big data is great significance to the sustainable development of cities. Based on the network big data as the data source, this paper analyzes the spatial distribution and spatial-temporal pattern of house prices in the main city zone of Kunming through the Getis-Ord Gi* index and Kriging method, and quantifies the spatial-temporal changes of house prices through statistical analysis and spatial overlay analysis. The result shows that in hot and cold spot analysis, hot spot (high house price agglomeration area) is surrounded by cold spot (low house price agglomeration area), and Chenggong District is the cold spot dominant area. The house price in the main city zone of Kunming forms two high-value areas in the spatial distribution, and presents a pile-shaped ring structure with a central high price and low price in circumference. During the study period, the overall house price fluctuations were small, showing a pattern of north high and south low. The results of statistical analysis and spatial overlay analysis show that there is a significant difference in house prices among various administrative districts. The high-value clusters and low-value clusters in house prices are randomly distributed in various administrative regions. The price changes in Chenggong District are the most significant, and Panlong District has the smallest change.}, location = {Wuhan, China}, series = {AICSconf '20}, pages = {5\u201310}, numpages = {6}, keywords = {House price, Kriging, Getis-Ord Gi* Index, Spatial-temporal patterns, Network big data}}
@inproceedings{10.1145/2751957.2751960,title = {Using Stakeholder Knowledge for Data Quality Assessment in IS Security Risk Management Processes}, author = {Sillaber Christian , Breu Ruth },year = {2015}, isbn = {9781450335577}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2751957.2751960}, doi = {10.1145/2751957.2751960}, abstract = {The availability of high quality documentation of the IS as well as knowledgeable stakeholders are an important prerequisite for successful IS security risk management processes. However, little is known about the relationship between stakeholders, their knowledge about the IS, security documentation and how quality aspects influence the security and risk properties of the IS under investigation. We developed a structured data quality assessment process to identify quality issues in the security documentation of an information system. For this, organizational stakeholders were interviewed about the IS under investigation and models were created from their description in the context of an ongoing security risk management process process. Then, the research model was evaluated in a case study. We found that contradictions between the models created from stakeholder interviews and those created from documentation were a good indicator for potential security risks. The findings indicate that the proposed data quality assessment process provides valuable inputs for the ongoing security and risk management process. While current research considers users as the most important resource in security and risk management processes, little is known about the hidden value of various entities of documentation available at the organizational level. This study highlights the importance of utilizing existing IS security documentation in the security and risk management process and provides risk managers with a toolset for the prioritization of security documentation driven improvement activities.}, location = {Newport Beach, California, USA}, series = {SIGMIS-CPR '15}, pages = {153\u2013159}, numpages = {7}, keywords = {information systems security risk management, data quality of information system, information system security documentation quality}}
@inproceedings{10.1145/3018896.3018974,title = {Design of a big data GIS platform for a near-real-time environmental monitoring system based on GOES-R satellite imagery}, author = {Vega M. A. L\u00f3pez , Couturier S. },year = {2017}, isbn = {9781450347747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3018896.3018974}, doi = {10.1145/3018896.3018974}, abstract = {Geographic Information Systems will gradually tend to incorporate technologies such as Big Data and the Internet of things, so that data can be processed to information towards near real time spatial analysis with unprecedented power and applications. Perhaps one of the most visible obstacles to this is the large amount of data involved. Indeed, the magnitude of stored data which implies its constant acquisition and generation, is rapidly growing to Terabyte systems through to Petabyte systems. By designing a near real time environmental monitoring system based on GOES-R next generation satellite imagery, we show in this paper that current tools actually allow the proper management of such amount of data and enable the integration of a variety of data sources that makes possible the rapid analysis of one of the most voluminous spatial dataset available.}, location = {Cambridge, United Kingdom}, series = {ICC '17}, pages = {1\u20134}, numpages = {4}, keywords = {GIS, big data, GOES-R, spatial database}}
@inproceedings{10.1145/1322263.1322324,title = {A framework for data quality and feedback in participatory sensing}, author = {Reddy Sasank , Burke Jeff , Estrin Deborah , Hansen Mark , Srivastava Mani },year = {2007}, isbn = {9781595937636}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1322263.1322324}, doi = {10.1145/1322263.1322324}, abstract = {The rapid adoption of mobile phones by society over the last decade and the increasing ability to capture, classifying, and transmit a wide variety of data (image, audio, and location) have enabled a new sensing paradigm - where humans carrying mobile phones can act as sensor systems. Human-in-the-loop sensor systems raise many new challenges in areas of sensor data quality assessment, mobility and sampling coordination, and user interaction procedures.}, location = {Sydney, Australia}, series = {SenSys '07}, pages = {417\u2013418}, numpages = {2}, keywords = {mobile systems, human sensor networks}}
@inproceedings{10.1145/3465480.3468676,title = {Real-time big data stream analytics and complex event detection: modular visual framework, data science platform, and industry applications}, author = {Klinkenberg Ralf },year = {2021}, isbn = {9781450385558}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3465480.3468676}, doi = {10.1145/3465480.3468676}, abstract = {In many industry applications, larger and larger amounts of data become available, allowing to gain deeper insights, to generate more accurate forecats, to optimize and automate processes, and to thereby create significant value. Often the data is not static, but arrives continuously in large data streams, which ideally are processed and leveraged in real-time. This talk will present a modular and flexible platform for real-time big data stream processing, complex event detection, data science and machine learning with an easy-to-use visual process design user interface, seamlessly integrating the most relevant big data and stream processing frameworks (Hadoop, Spark, Spark Streaming, Kafka, Flink, etc.) within one unifying platform and user interface, based on the widely used data science platform RapidMiner.This talk will also provide an overview of applications of this framework across many industries like machine failure prediction and prevention and predictive maintenance in industrial production in the manufacturing industry, criticial event detection, prediction and prevention in the chemical indutry, product quality prediction and optimization as well as energy consumption and cost reduction in the steel and metal industry, data-driven process optimization in the food and beverage industry, various use cases in the automotive and aviation industry, maritime data analysis to detect complex events like piracy or illegal fishing and to avoid collisions, drug effectiveness prediction for cancer drug development and biomedical research, financial time series analysis and forecasting for the investment industry. The latter three use cases are addresed in the European reseach projects INFORE, which will also be shortly introduced.}, location = {Virtual Event, Italy}, series = {DEBS '21}, pages = {102}, numpages = {1}}
@inproceedings{10.1145/3456887.3456976,title = {University Students Crisis events Managements in Big Data Age \u2013 A Case Study of Nanjing}, author = {Zou Yuxiang },year = {2021}, isbn = {9781450389969}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3456887.3456976}, doi = {10.1145/3456887.3456976}, abstract = {This research surveys students in Xianlin university town in Nanjing through questionnaires and interviews. Current situation of campus critical incidents managements is analyzed. This research finds that objective information resources in college students crisis events managements are more comprehensive, more concentrated and crisis events are of higher occurrence, elusiveness and contingency. The current management model of college student crisis events has no remarkable effects and big data technology is not widely adopted in managements.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2021}, pages = {395\u2013398}, numpages = {4}}
@inproceedings{10.1145/2630729.2630741,title = {Limits and Opportunities of Big Data For Macro-Prudential Modeling of Financial Systemic Risk}, author = {Brammertz Willi , Mendelowitz Allan I. },year = {2014}, isbn = {9781450330121}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2630729.2630741}, doi = {10.1145/2630729.2630741}, abstract = {We explore the use of \"big data\", i.e. large unstructured data sets, within financial risk analysis. We conclude it has value, but structured data remain critical. We show that forward-looking financial analysis on the systemic level needs a data structure that represents financial contracts as algorithms that produce state contingent cash flows. Currently the industry lacks such a standard, which precludes meaningful systemic forward-looking analysis. We introduce ACTUS as an emerging standard that will enable consistent analysis on all levels. This standard will also create an infrastructure for macro financial analysis.}, location = {Snowbird, UT, USA}, series = {DSMM'14}, pages = {1\u20136}, numpages = {6}, keywords = {Data Standard for Financial Contracts, Financial Modeling, Systemic Level}}
@inproceedings{10.1145/3482632.3482988,title = {The New Curriculum Standard English Education Ecosystem for College Students Based on Big Data Technology}, author = {Huang Xinan },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3482988}, doi = {10.1145/3482632.3482988}, abstract = {Since the 21st century, educational ecology, using the theory and principles of ecology to study the educational system, has become a powerful theoretical tool. In the process of improving teaching effectiveness, researchers have established \"class ecology\" and used its theory to construct ecological classrooms. \"Ecological classroom\", starting from the point of view of education ecology, enables English teaching to embark on a path of sustainable development. The purpose of this article is to study the new curriculum standard English education ecosystem for college students based on big data. This article first summarizes the basic theories of big data, and then extends its core technology, and then analyzes the current situation of college English education in our country, to explore its shortcomings and shortcomings, and analyze and research the new curriculum standard English education ecosystem for college students through the combination of big data technology. This article systematically expounds the impact of big data on college English education. It is scientifically based on the traditional English education ecosystem, through teaching experiments, to study the actual effects of big data technology and the application of English education. This article uses big data in English education the application in the ecology is the research object, and the subject of this article is studied through the application of analytic hierarchy process and comparison method. Experimental research shows that compared with the traditional English education ecosystem, the application of the English education ecosystem based on big data is more stable, the feasibility is higher.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {647\u2013651}, numpages = {5}}
@inproceedings{10.1145/3482632.3482733,title = {Research on Risk and Supervision of Financial Big Data Application Based on Decision Tree Algorithm}, author = {Wang Xiaoyi },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3482733}, doi = {10.1145/3482632.3482733}, abstract = {As an important field of China's social and economic development, big data plays an important role in channel expansion, service popularization, and precision marketing. The application of big data technology in the financial industry can effectively broaden the marketing channels of financial enterprises and improve their popular service levels. Big data technology is an important support for the healthy development of the financial industry. Used in precision marketing, credit rating, asset pricing, etc. Although it has promoted the informatization development of the financial industry, there are also certain risks. Risks and vulnerabilities such as data security, supervision, transmission rate, rules, etc. frequently appear, threatening the healthy development of the financial industry. On the basis of explaining the application of big data in the development of the financial industry, it analyzes the existing risks in depth, and proposes targeted regulatory measures based on the decision tree algorithm to promote the healthy and sustainable development of the financial industry.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {478\u2013481}, numpages = {4}}
@inproceedings{10.1145/3482632.3483067,title = {Application of Big Data Technology in the Innovation of University Education Management Work}, author = {Yu Wenjun },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483067}, doi = {10.1145/3482632.3483067}, abstract = {With the development of higher education in China, the scale of colleges and universities is expanding rapidly and the information of teaching management is developing rapidly. Facing the growing management information, it is inevitable to carry out education management innovation to meet the needs of the times. The use of teachers' resources and the assessment of teachers' classroom teaching quality must be solved by modern means to achieve the rational use and optimal allocation of teaching resources and improve the effectiveness of teaching management. The purpose of this paper is to study the application of big data(BD) technology in the innovation of university education management. This paper uses case study and literature research methods, combines data warehousing and data mining technologies in BD technology for evaluating teaching data with data warehousing as an organizational tool, and analyzes the current situation and future development trends of database technology data application in information management of educational administration construction in universities. The experimental results show that the ID3 algorithm performs better when there are fewer number of things and is about 10 seconds faster than the Apriori algorithm. But when the number of things are much more, the Apriori algorithm is significantly better than the ID3 algorithm. Moreover, the Apriori algorithm is more suitable for mining the deep laws of ungrasped objective things, and reveals unknown dependencies between data for rule mining with fuzzy concepts. Therefore, through analysis of data warehouse, data mining techniques and mining algorithms, by data mining of a large amount of assessment data, this paper finds that combining decision trees and association rule algorithms in data mining algorithms with teaching systems, data mining techniques introduced in educational assessment systems for assessment, not only improves the scientific nature of educational management, but also improves the effectiveness of digital education construction.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {988\u2013992}, numpages = {5}}
@inproceedings{10.1145/3482632.3483145,title = {Analysis of English Machine Translation Standards of Professional Terms under Big Data Context Technology}, author = {Yang Fan },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483145}, doi = {10.1145/3482632.3483145}, abstract = {In recent years, with the development of science and technology, information technology has also been widely used in various industries. In today's era of big data, great changes have taken place in translation, from the traditional manual translation at the beginning to the current machine translation. The advantage of machine translation lies in its fast translation speed and low cost, but certain errors will also occur in the translated translation. The translation errors of professional terms account for a larger proportion, which greatly affects the quality of the translation. At the same time, professional terminology is the core knowledge in the article, which will directly affect people's understanding of technical information in the professional field. The purpose of this article is to study the English machine translation standards of professional terms in the context of big data. This article focuses on the English machine translation of professional terms, and analyzes the English machine translation standards of professional terms in the context of big data based on relevant research at home and abroad. This article will analyze its professional terminology translation standards from all aspects, summarize and summarize the translation standards studied by various scholars, analyze the factors that affect translation standards, and discover various characteristics of translation standards. At the same time, in order to solve the shortcomings of machine translation in the translation of professional terminology in the past, this article attempts to summarize a set of objective and comprehensive English machine translation system evaluation standards for professional terminology, so as to improve its use efficiency. The experimental results show that from the ranking results, the highest comprehensive evaluation score is candidate translation 1, and the weight is as high as 27%. In the analysis of the translation results, information characteristics are added to reorder the candidate translations, and the most suitable context and the best translation are selected.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1332\u20131336}, numpages = {5}}