@inproceedings{10.1145/3383972.3384034,title = {Towards Big Data Analytics and Mining for UK Traffic Accident Analysis, Visualization & Prediction}, author = {Feng Mingchen , Zheng Jiangbin , Ren Jinchang , Liu Yanqin },year = {2020}, isbn = {9781450376426}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3383972.3384034}, doi = {10.1145/3383972.3384034}, abstract = {Road traffic accident (RTA) is a big issue to our society due to it is among the main causes of traffic congestion, human death, health problems, environmental pollution, and economic losses. Facing these fatal and unexpected traffic accidents, understanding what happened and discover factors that relate to them and then make alarms in advance play critical roles for possibly effective traffic management and reduction of accidents. This paper presents our work to establish a novel big data analytics platform for UK traffic accident analysis using machine learning and deep learning techniques. Our system consists of three parts in which we first cluster accident incidents in an interactive Google map to highlight some hotspots and then narratively visualize accident attributes to uncover potentially related factors, finally we explored several state-of-the-art machine learning, deep learning and time series forecasting models to predict the number of road accidents in the future. The experimental results show that our big data processing platform can not only effectively handle large amount of data but also give new insights into what happened and reasonably prediction of what will happen in the future to assist decision making, which will undoubtedly show its great value as a generic platform for other big data analytics fields.}, location = {Shenzhen, China}, series = {ICMLC 2020}, pages = {225\u2013229}, numpages = {5}, keywords = {Big Data Analytics, Traffic Accident Analysis, Time series Forecasting, Deep Learning}}
@inproceedings{10.1145/3449301.3449322,title = {Research on Belt and Road Big Data Visualization Based on Text Clustering Algorithm}, author = {Wen Yana , Wei Tingyue , Cui Kewei , Ling Bai , Zhang Yahao , Huang Meng },year = {2020}, isbn = {9781450388597}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3449301.3449322}, doi = {10.1145/3449301.3449322}, abstract = {In the era of big data, people's visual needs for data expression are increasing. In order to achieve better big data display effects, this article introduced the use of text clustering algorithms to achieve data crawling and Echarts technology to realize big data visualization. This system used mvvm's architecture and vue framework development platform, ThinkPHP was used as the background framework, and ES6 related technologies and specifications were used for application development. This system used Echarts, IView, GIS technology and JavaScript development methods to demonstrate economic big data module functions on the web side; Applied CSS3, HTML5, GIS technology to implement project achievement module and university alliance module; Applied Echarts, HTML5, JS function library technology to achieve national information module. This system used stored procedure, database index optimization technology to achieve rapid screening of massive data, and dynamically update and displayed related data through two-way data binding. This system combined real-time location technology with GIS technology to measure the distance between the user and the destination, and automatically plan the tour route to provide related services. This system can provide feasibility suggestions for strategic researchers or experts in related areas of the \u201cBelt and Road\u201d, and provide theoretical basis and technical support.}, location = {Singapore, Singapore}, series = {ICRAI 2020}, pages = {121\u2013125}, numpages = {5}, keywords = {One Belt One Road, Keywords-component, big data visualization, Text clustering algorithm}}
@inproceedings{10.5555/1812530.1812588,title = {Improving data quality with dynamic forms}, author = {Chen Kuang , Chen Harr , Conway Neil , Dolan Heather , Hellerstein Joseph M. , Parikh Tapan S. },year = {2009}, isbn = {9781424446629}, publisher = {IEEE Press}, abstract = {Organizations in developing regions want to efficiently collect digital data, but standard data gathering practices from the developed world are often inappropriate. Traditional techniques for form design and data quality are expensive and labour-intensive. We propose a new data-driven approach to form design, execution (filling) and quality assurance. We demonstrate USHER, an end-to-end system that automatically generates data entry forms that enforce and maintain data quality constraints during execution. The system features a probabilistic engine that drives form-user interactions to encourage correct answers.}, location = {Doha, Qatar}, series = {ICTD'09}, pages = {487}, numpages = {1}}
@inproceedings{10.1145/3018896.3018913,title = {A new secure model for the use of cloud computing in big data analytics}, author = {Chaoui Habiba , Makdoun Ibtissam },year = {2017}, isbn = {9781450347747}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3018896.3018913}, doi = {10.1145/3018896.3018913}, abstract = {When Big data and cloud computing join forces together, several domains like: healthcare, disaster prediction and decision making become easier and much more beneficial to users in term of information gathering, although cloud computing will reduce time and cost of analyzing information for big data, it may harm the confidentiality and integrity of the sensitive data, for instance, in healthcare, when analyzing disease's spreading area, the name of the infected people must remain secure, hence the obligation to adopt a secure model that protect sensitive data from malicious users. Several case studies on the integration of big data in cloud computing, urge on how easier it would be to analyze and manage big data in this complex envronement. Companies must consider outsourcing their sensitive data to the cloud to take advantage of its beneficial resources such as huge storage, fast calculation, and availability, yet cloud computing might harm the security of data stored and computed in it (confidentiality, integrity). Therefore, strict paradigm must be adopted by organization to obviate their outsourced data from being stolen, damaged or lost. In this paper, we compare between the existing models to secure big data implementation in the cloud computing. Then, we propose our own model to secure Big Data on the cloud computing environement, considering the lifecycle of data from uploading, storage, calculation to its destruction.}, location = {Cambridge, United Kingdom}, series = {ICC '17}, pages = {1\u201311}, numpages = {11}, keywords = {cloud computing, safe data destruction, big data, functional encryption, search over encrypted data, authentication protocols}}
@inproceedings{10.1145/3401895.3402092,title = {A big data architecture to a multiple purpose in healthcare surveillance: the Brazilian syphilis case}, author = {Silva Rodrigo Dantas da , de Ara\u00fajo Jean Jar Pereira , de Paiva \u00c1lvaro Ferreira Pires , de Medeiros Valentim Ricardo Alexsandro , Coutinho Karilany Dantas , de Paiva Jailton Carlos , Roussanaly Azim , Boyer Anne },year = {2020}, isbn = {9781450377119}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3401895.3402092}, doi = {10.1145/3401895.3402092}, abstract = {For many decades society did need to monitor and assess the standard of living of the population. In the 1950s, the United Nations (UN) saw this need and proposed 12 areas that should be evaluated, the first of which is listed under \"Health and Demography\", which focuses on what is expressed as the level of a population's health. Decades have passed and great results have been gained from similar initiatives such as reducing mortality from infectious diseases and even eradicating some others. In the age of the digital society, needs have grown. Monitoring demands that once perished from data to become concrete now suffer from the opposite effect, the excess of data from everywhere. Healthcare systems around the world use many different information systems, collecting and generating hundreds of data at unimaginable speed. We are billions of people on the planet and most of us are connected to the virtual world, sharing information, experiences and events with some kind of cloud. In this information age, the ability to aggregate and process this data is a major factor in raising public health to a new level. The development of tools capable of analyzing a large volume of data in seconds and producing knowledge for targeted decision making can help in the fight against specific diseases, in the process of continuing education of professionals, in the formation of new professionals, in the elaboration of new policies. with the specific locoregional look, in the analysis of hidden trends in front of so much information faced in everyday life and other possibilities. The present work proposes an architecture capable of storing and manipulating seeking to standardize the variables in order to allow to correlate this large amount of data in a systematic way, providing to several services and researchers the possibility of consuming health, social, economic and educational data for the promotion of public health.}, location = {Aveiro, Portugal}, series = {EATIS '20}, pages = {1\u20136}, numpages = {6}, keywords = {big data, epidemiology, healthcare surveillance, syphilis}}
@inproceedings{10.5555/3320516.3321154,title = {Session details: Advanced tutorials: Inferential big data}, author = {Uhrmacher Adelinde , Sanchez Susan M. },year = {2018}, isbn = {978153866570}, publisher = {IEEE Press}, location = {Gothenburg, Sweden}, series = {WSC '18}, pages = {}}
@inproceedings{10.1145/3255781,title = {Session details: Web queries and big data}, author = {Abiteboul Serge },year = {2014}, isbn = {9781450323758}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3255781}, doi = {10.1145/3255781}, location = {Snowbird, Utah, USA}, series = {PODS '14}, pages = {}}
@inproceedings{10.1145/3510858.3510934,title = {Big Data Financial Algorithm Technology Based on Machine Learning Technology}, author = {Zhao Yiming },year = {2021}, isbn = {9781450390422}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510858.3510934}, doi = {10.1145/3510858.3510934}, abstract = {With the development and wide application of machine learning technology, the use of machine learning technology for economic algorithm technology research has become a new type of financial technology field. Today's financial big data has penetrated into all walks of life and has become an important factor of production. The extraction and application of massive amounts of data by humans heralds the arrival of a new wave of productivity growth and consumer surplus. Big data originally refers to a large number of data sets generated through batch processing or web search index analysis. This paper uses machine learning technology to explore and research big data financial algorithms, analyze risk control measures, report on the improvement and perfection of traditional finance, and analyze and study the future development of big data finance. The main research content of this paper is the analysis of big data financial algorithm technology by machine learning algorithms. Machine learning technology is one of the main methods to solve big data mining problems. Machine learning technology is a process of self-improvement using the system itself, so that computer programs can automatically improve performance through accumulated experience. This paper analyzes the relevant theories and characteristics of machine learning algorithms, and integrates them into the research of big data economic algorithm technology. The final result of the research shows that when the data volume is 1G, the training time of SVM is 8 minutes, while the training time of Bayesian is 12 minutes, and the data volume is relatively small. The SVM algorithm still has obvious advantages in training time.}, location = {Changsha, China}, series = {ICASIT 2021}, pages = {218\u2013222}, numpages = {5}}
@inproceedings{10.1145/3254069,title = {Session details: Big Data and Blind Users}, author = {McCoy Kathleen },year = {2016}, isbn = {9781450341240}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3254069}, doi = {10.1145/3254069}, location = {Reno, Nevada, USA}, series = {ASSETS '16}, pages = {}}
@inproceedings{10.5555/645919.672811,title = {The Need for Data Quality}, author = {Patterson Blake },year = {1993}, isbn = {155860152X}, publisher = {Morgan Kaufmann Publishers Inc.}, address = {San Francisco, CA, USA}, series = {VLDB '93}, pages = {709}}
@inproceedings{10.1109/CCGrid.2015.170,title = {Cloud-based machine learning tools for enhanced big data applications}, author = {Cuzzocrea Alfredo , Mumolo Enzo , Corona Pietro },year = {2015}, isbn = {9781479980062}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2015.170}, doi = {10.1109/CCGrid.2015.170}, abstract = {We propose Cloud-based machine learning tools for enhanced Big Data applications, where the main idea is that of predicting the \"next\" workload occurring against the target Cloud infrastructure via an innovative ensemble-based approach that combine the effectiveness of different well-known classifiers in order to enhance the whole accuracy of the final classification, which is very relevant at now in the specific context of Big Data. So-called workload categorization problem plays a critical role towards improving the efficiency and the reliability of Cloud-based big data applications. Implementation-wise, our method proposes deploying Cloud entities that participate to the distributed classification approach on top of virtual machines, which represent classical \"commodity\" settings for Cloud-based big data applications. Preliminary experimental assessment and analysis clearly confirm the benefits deriving from our classification framework.}, location = {Shenzhen, China}, series = {CCGRID '15}, pages = {908\u2013914}, numpages = {7}}
@inproceedings{10.1145/3500931.3500938,title = {Big data and artificial intelligence platform construction plan for epidemic prevention and control}, author = {Liu Dan , Xu Han , Liu Bo },year = {2021}, isbn = {9781450395588}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3500931.3500938}, doi = {10.1145/3500931.3500938}, abstract = {This paper analyzes the core technologies in the construction of the epidemic prevention and control platform, including big data technology, artificial intelligence technology, Echarts data visualization technology, Python language, YAPI technology. Key points of application in the construction plan include cross-industry big data platform, macro-oriented prevention and control model, real-time epidemic prevention and control map, clinical symptom screening system and multi-platform information linkage. This paper studies the optimization measures to improve the comprehensive sharing of information, standardize the prevention and control information data standards, and optimize the information collection interface. The purpose is to accumulate corresponding value data and constantly improve the content of the epidemic prevention and control platform construction plan.}, location = {Beijing, China}, series = {ISAIMS 2021}, pages = {28\u201334}, numpages = {7}, keywords = {Big data technology, Epidemic Prevention and Control Platform, Artificial intelligence technology}}
@inproceedings{10.5555/1564131.1564137,title = {Data quality from crowdsourcing: a study of annotation selection criteria}, author = {Hsueh Pei-Yun , Melville Prem , Sindhwani Vikas },year = {2009}, publisher = {Association for Computational Linguistics}, address = {USA}, abstract = {Annotation acquisition is an essential step in training supervised classifiers. However, manual annotation is often time-consuming and expensive. The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates. In this paper, we consider the difficult problem of classifying sentiment in political blog snippets. Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined. Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty. Analysis confirm the utility of these criteria on improving data quality. We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency.}, location = {Boulder, Colorado}, series = {HLT '09}, pages = {27\u201335}, numpages = {9}}
@inproceedings{10.1145/2925995.2926010,title = {Providing Support for Automated Product Data Quality Assurance: A Case Study}, author = {J\u00e4ger Alexandra , Breu Ruth },year = {2016}, isbn = {9781450340649}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2925995.2926010}, doi = {10.1145/2925995.2926010}, abstract = {Data quality is of utmost importance in large product databases. This is especially true for food products, since potentially health-critical data is contained. With growing database size, manual quality assurance becomes infeasible. GS1 Sync, governed by GS1 Austria, is rapidly becoming the largest national food product database. In order to support manual quality assurance, we have conceptualized a process to conduct product data quality assurance in an automatic way, based on defining rules and classifying product data. In order to evaluate our approach, we have implemented a prototype and performed a proof-of-concept. Although our research is still a work-in-progress, we were able to show that our approach is able to find a substantial number of issues that did not appear during manual control.}, location = {Hagen, Germany}, series = {KMO '16}, pages = {1\u20136}, numpages = {6}, keywords = {Product Data Classification, Product Data Quality, GS1 Sync}}
@inproceedings{10.1145/3159652.3160602,title = {Scalable Algorithms in the Age of Big Data and Network Sciences: Characterization, Primitives, and Techniques}, author = {Teng Shang-Hua },year = {2018}, isbn = {9781450355810}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3159652.3160602}, doi = {10.1145/3159652.3160602}, abstract = {In the age of network sciences and machine learning, efficient algorithms are now in higher demand more than ever before. Big Data fundamentally challenges the classical notion of efficient algorithms: Algorithms that used to be considered efficient, according to polynomial-time characterization, may no longer be adequate for solving today\u00bbs problems. It is not just desirable, but essential, that efficient algorithms should be scalable. In other words, their complexity should be nearly linear or sub-linear with respect to the problem size. Thus, scalability, not just polynomial-time computability, should be elevated as the central complexity notion for characterizing efficient computation. In this talk, I will highlight a family of fundamental algorithmic techniques for designing provably-good scalable algorithms: (1) scalable primitives and scalable reduction, (2) spectral approximation of graphs and matrices, (3) sparsification by multilevel structures, (4) advanced sampling, (5) local network exploration. For the first, I will focus on the emerging Laplacian Paradigm, that has led to breakthroughs in scalable algorithms for several fundamental problems in network analysis, machine learning, and scientific computing. I will then illustrate these algorithmic techniques with four recent applications: (1) sampling from graphic models, (2) network centrality approximation, (3) social-influence analysis (4) local clustering. Mathematical and algorithmic solution to these problems exemplify the fusion of combinatorial, numerical, and statistical thinking in data and network analysis.}, location = {Marina Del Rey, CA, USA}, series = {WSDM '18}, pages = {6\u20137}, numpages = {2}, keywords = {scalable algorithms, local algorithms, big data, network sciences, advanced sampling, machine learning, graph sparsification}}
@inproceedings{10.1109/SC.2014.66,title = {Parallel deep neural network training for big data on blue gene/Q}, author = {Chung I-Hsin , Sainath Tara N. , Ramabhadran Bhuvana , Picheny Michael , Gunnels John , Austel Vernon , Chauhari Upendra , Kingsbury Brian },year = {2014}, isbn = {9781479955008}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/SC.2014.66}, doi = {10.1109/SC.2014.66}, abstract = {Deep Neural Networks (DNNs) have recently been shown to significantly outperform existing machine learning techniques in several pattern recognition tasks. DNNs are the state-of-the-art models used in image recognition, object detection, classification and tracking, and speech and language processing applications. The biggest drawback to DNNs has been the enormous cost in computation and time taken to train the parameters of the networks - often a tenfold increase relative to conventional technologies. Such training time costs can be mitigated by the application of parallel computing algorithms and architectures. However, these algorithms often run into difficulties because of the cost of inter-processor communication bottlenecks. In this paper, we describe how to enable Parallel Deep Neural Network Training on the IBM Blue Gene/Q (BG/Q) computer system. Specifically, we explore DNN training using the data-parallel Hessian-free 2nd order optimization algorithm. Such an algorithm is particularly well-suited to parallelization across a large set of loosely coupled processors. BG/Q, with its excellent inter-processor communication characteristics, is an ideal match for this type of algorithm. The paper discusses how issues regarding programming model and data-dependent imbalances are addressed. Results on large-scale speech tasks show that the performance on BG/Q scales linearly up to 4096 processes with no loss in accuracy. This allows us to train neural networks using billions of training examples in a few hours.}, location = {New Orleans, Louisana}, series = {SC '14}, pages = {745\u2013753}, numpages = {9}, keywords = {speech recognition, big data, high performance computing}}
@inproceedings{10.1145/2107536.2107538,title = {Improving data quality by source analysis}, author = {M\u00fcller Heiko , Freytag Johann-Christoph , Leser Ulf },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2107536.2107538}, doi = {10.1145/2107536.2107538}, abstract = {In many domains, data cleaning is hampered by our limited ability to specify a comprehensive set of integrity constraints to assist in identification of erroneous data. An alternative approach to improve data quality is to exploit different data sources that contain information about the same set of objects. Such overlapping sources highlight hot-spots of poor data quality through conflicting data values and immediately provide alternative values for conflict resolution. In order to derive a dataset of high quality, we can merge the overlapping sources based on a quality assessment of the conflicting values. The quality of the resulting dataset, however, is highly dependent on our ability to asses the quality of conflicting values effectively.The main objective of this article is to introduce methods that aid the developer of an integrated system over overlapping, but contradicting sources in the task of improving the quality of data. Value conflicts between contradicting sources are often systematic, caused by some characteristic of the different sources. Our goal is to identify such systematic differences and outline data patterns that occur in conjunction with them. Evaluated by an expert user, the regularities discovered provide insights into possible conflict reasons and help to assess the quality of inconsistent values. The contributions of this article are two concepts of systematic conflicts: contradiction patterns and minimal update sequences. Contradiction patterns resemble a special form of association rules that summarize characteristic data properties for conflict occurrence. We adapt existing association rule mining algorithms for mining contradiction patterns. Contradiction patterns, however, view each class of conflicts in isolation, sometimes leading to largely overlapping patterns. Sequences of set-oriented update operations that transform one data source into the other are compact descriptions for all regular differences among the sources. We consider minimal update sequences as the most likely explanation for observed differences between overlapping data sources. Furthermore, the order of operations within the sequences point out potential dependencies between systematic differences. Finding minimal update sequences, however, is beyond reach in practice. We show that the problem already is NP-complete for a restricted set of operations. In the light of this intractability result, we present heuristics that lead to convincing results for all examples we considered.}, pages = {1\u201338}, numpages = {38}, keywords = {data cleaning, quality assessment, Conflict resolution, semantic distance measure}}
@inproceedings{10.1145/3209582.3209599,title = {Incentivizing Truthful Data Quality for Quality-Aware Mobile Data Crowdsourcing}, author = {Gong Xiaowen , Shroff Ness },year = {2018}, isbn = {9781450357708}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3209582.3209599}, doi = {10.1145/3209582.3209599}, abstract = {Mobile data crowdsourcing has found a broad range of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the \"wisdom\" of a potentially large crowd of \"workers\" (i.e., mobile users). A key metric of crowdsourcing is data accuracy, which relies on the quality of the participating workers' data (e.g., the probability that the data is equal to the ground truth). However, the data quality of a worker can be its own private information (which the worker learns, e.g., based on its location) that it may have incentive to misreport, which can in turn mislead the crowdsourcing requester about the accuracy of the data. This issue is further complicated by the fact that the worker can also manipulate its effort made in the crowdsourcing task and the data reported to the requester, which can also mislead the requester. In this paper, we devise truthful crowdsourcing mechanisms for Quality, Effort, and Data Elicitation (QEDE), which incentivize strategic workers to truthfully report their private worker quality and data to the requester, and make truthful effort as desired by the requester. The truthful design of the QEDE mechanisms overcomes the lack of ground truth and the coupling in the joint elicitation of worker quality, effort, and data. Under the QEDE mechanisms, we characterize the socially optimal and the requester's optimal task assignments, and analyze their performance. We show that the requester's optimal assignment is determined by the largest \"virtual valuation\" rather than the highest quality among workers, which depends on the worker's quality and the quality's distribution. We evaluate the QEDE mechanisms using simulations which demonstrate the truthfulness of the mechanisms and the performance of the optimal task assignments.}, location = {Los Angeles, CA, USA}, series = {Mobihoc '18}, pages = {161\u2013170}, numpages = {10}, keywords = {Mobile data crowdsourcing, incentive mechanism, data quality}}
@inproceedings{10.1145/2628194.2628231,title = {Innovative power operating center management exploiting big data techniques}, author = {Ceci Michelangelo , Cassavia Nunziato , Corizzo Roberto , Dicosta Pietro , Malerba Donato , Maria Gaspare , Masciari Elio , Pastura Camillo },year = {2014}, isbn = {9781450326278}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2628194.2628231}, doi = {10.1145/2628194.2628231}, abstract = {The problem of accurately predicting the energy production from renewable sources has recently received an increasing attention from both the industrial and the research communities. It presents several challenges, such as facing with the rate data are provided by sensors, the heterogeneity of the data collected, power plants efficiency, as well as uncontrollable factors, such as weather conditions and user consumption profiles. In this paper we describe Vi-POC (Virtual Power Operating Center), a project conceived to assist energy producers and decision makers in the energy market. In this paper we present the Vi-POC project and how we face with challenges posed by the specific application. The solutions we propose have roots both in big data management and in stream data mining.}, location = {Porto, Portugal}, series = {IDEAS '14}, pages = {326\u2013329}, numpages = {4}}
@inproceedings{10.1145/2684200.2684333,title = {Efficiently Managing the Security and Costs of Big Data Storage using Visual Analytics}, author = {Hassan Sabri , Pernul G\u00fcnther },year = {2014}, isbn = {9781450330015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2684200.2684333}, doi = {10.1145/2684200.2684333}, abstract = {We are currently living in the age of big data with ever growing volumes of heterogeneous and fast moving data. Whether they are mobile devices, internal or external systems or cloud-based systems data is generated, stored, processed and distributed in many different systems. This leads to various information security and privacy risks. To address these issues, especially from the viewpoint of data management and data governance we propose a conceptual analysis model. Thereby, our model takes into account the dimension of data storage location together with their respective risks and costs while considering the strategic value and sensitivity of data assets. For demonstrating our approach we developed a visual analytics web application which is based on parallel sets visualizations. By being able to interactively explore the analysis dimensions users are supported in developing enhanced situational awareness for making decisions in the context of secure and economical data storage.}, location = {Hanoi, Viet Nam}, series = {iiWAS '14}, pages = {180\u2013184}, numpages = {5}, keywords = {Data Management, Data Governance, Big Data, Cloud Storage, Visual Analytics, Information Security, Distributed Systems}}
@inproceedings{10.1145/3482632.3483150,title = {Analysis of Japanese Teaching Data Based on Big Data}, author = {Zhang Haiyan },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3483150}, doi = {10.1145/3482632.3483150}, abstract = {With the development of big data's Japanese teaching reform in colleges and universities, the evaluation of Japanese teaching quality in colleges and universities has become more and more important. The evaluation of Japanese teaching quality in colleges and universities involves all aspects of teaching activities and the vital interests of teachers and students. The scientific and reasonable evaluation of Japanese teaching in colleges and universities not only provides a good foundation for the management of Japanese teaching in colleges and universities. It is also of great significance to the development of teachers and students' ability. In order to improve the evaluation of teachers' Japanese teaching quality and provide reasonable reference for the examination of education management department, this paper uses the relevant theory and SPSS software of factor analysis. The data of Japanese teaching quality evaluation of college students are analyzed statistically, and the method of data statistical analysis has certain application prospect and space.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {1356\u20131360}, numpages = {5}}
@inproceedings{10.1145/3269206.3269208,title = {BBoxDB - A Scalable Data Store for Multi-Dimensional Big Data}, author = {Nidzwetzki Jan Kristof , G\u00fcting Ralf Hartmut },year = {2018}, isbn = {9781450360142}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3269206.3269208}, doi = {10.1145/3269206.3269208}, abstract = {BBoxDB is a distributed and highly available key-bounding-box-value store which enhances the classical key-value data model with an axis-parallel bounding box. The bounding box describes the location of the values in an n-dimensional space, and enables BBoxDB to efficiently distribute multi-dimensional data across a cluster of nodes. Well-known geometric algorithms (such as the K-D Tree) are used to create distribution regions (multi-dimensional shards). Distribution regions are created dynamically, based on the stored data. BBoxDB stores data of multiple tables co-partitioned, which enables efficient distributed spatial joins. Spatial joins on co-partitioned tables can be executed without data shuffling between nodes. A two-level index structure is employed to retrieve stored data quickly. We demonstrate the interaction with the system, the dynamic creation of distribution regions and the data redistribution feature of BBoxDB.}, location = {Torino, Italy}, series = {CIKM '18}, pages = {1867\u20131870}, numpages = {4}, keywords = {distributed system, multi-dimensional data store, co-partitioned data, multi-dimensional big data, spatial join}}
@inproceedings{10.1145/3424978.3425002,title = {Research on Equilibrium Control Method of Urban Road Network Based on Big Data}, author = {Liu Jianhua , Gao Taotao , Du Yunxia },year = {2020}, isbn = {9781450377720}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3424978.3425002}, doi = {10.1145/3424978.3425002}, abstract = {Because the traffic flow on the urban road network continues to gather to the congested area, which exceeds the regional traffic carrying capacity, causing the traffic condition to deteriorate gradually. Based on the spatial proximity characteristics of the urban road network and relying on big data analysis technology, a traffic balance control method based on the urban regional capacity is designed and implemented. Through the collected data of urban vehicle travel, the correlation between regional capacity and traffic state is analyzed, the maximum bearing capacity of the region is determined, and the key adjacent areas that affect the regional state are identified. In the process of actual traffic control, real-time traffic flow data of urban road network is collected. When the regional traffic volume is close to the capacity threshold, the traffic flow in key adjacent areas is controlled and dynamically allocated to effectively prevent the continuous accumulation and state deterioration of regional traffic flow. Finally, the algorithm simulation test based on big data analysis platform is conducted. The results show that the variance of traffic volume in each area is reduced by 15% and the average congestion duration is reduced by 12%.}, location = {Sanya, China}, series = {CSAE 2020}, pages = {1\u20135}, numpages = {5}, keywords = {Dynamic distribution, Traffic state, Big data analysis, Equilibrium control}}
@inproceedings{10.1145/2627534.2627560,title = {Security problems and challenges in a machine learning-based hybrid big data processing network systems}, author = {Whitworth Jeff , Suthaharan Shan },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2627534.2627560}, doi = {10.1145/2627534.2627560}, abstract = {The data source that produces data continuously in high volume and high velocity with large varieties of data types creates Big Data, and causes problems and challenges to Machine Learning (ML) techniques that help extract, analyze and visualize important information. To overcome these problems and challenges, we propose to make use of the hybrid networking model that consists of multiple components such as Hadoop distributed file system (HDFS), cloud storage system, security module and ML unit. Processing of Big Data in this networking environment with ML technique requires user interaction and additional storage hence some artificial delay between the arrivals of data domains through external storage can help HDFSto process the Big Data efficiently. To address this problem we suggest using public cloud for data storage which will induce meaningful time delay to the data while making use of its storage capability. However, the use of public cloud will lead to security vulnerability to the data transmission and storage. Therefore, we need some form of security algorithm that provides a flexible key-based encryption technique that can provide tradeoffs between time-delay, security strength and storage risks. In this paper we propose a model for using public cloud provider trust levels to select encryption types for data storage for use within a Big Data analytics network topology.}, pages = {82\u201385}, numpages = {4}, keywords = {machine learning, encryption, hybrid cloud, retrievability, big data}}
@inproceedings{10.1145/3164541.3164560,title = {I/O Performance Improvement of Secure Big Data Analyses with Application Support on SSD Cache}, author = {Nakashima Kenji , Kon Joichiro , Yamaguchi Saneyasu },year = {2018}, isbn = {9781450363853}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3164541.3164560}, doi = {10.1145/3164541.3164560}, abstract = {For utilizing private big data, such as DNA data, encryption and anonymization are essential for preserving privacy. However, encryption and anonymization sometimes increase the size of data largely. Thus, increasing I/O performance for large-scale data is important. Caching data with Solid State Drive (SSD) is a popular method for by using SSD as cache, which is a proposed method for improving access performance of data in Hard Disk Drive (HDD). In this paper, we focus on SSD cache and discuss a method for improving I/O performance of a big data application using SSD cache. First, we evaluate the basic I/O performance with and without SSD cache. Second, we reveal the behavior of flash cache. Third, we propose a method for improving I/O performance of a large scale DNA application with SSD cache. Fourth, we evaluate the proposed method and demonstrate that the method can improve I/O performance effectively.}, location = {Langkawi, Malaysia}, series = {IMCOM '18}, pages = {1\u20137}, numpages = {7}, keywords = {SSD cache, Big data, anonymized analysis}}
@inproceedings{10.1145/3418896,title = {An Overview of End-to-End Entity Resolution for Big Data}, author = {Christophides Vassilis , Efthymiou Vasilis , Palpanas Themis , Papadakis George , Stefanidis Kostas },year = {2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3418896}, doi = {10.1145/3418896}, abstract = {One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows\u00a0for\u00a0Big Data, critically review the pros and cons of existing methods, and conclude with the main open research\u00a0directions.}, pages = {1\u201342}, numpages = {42}, keywords = {deep learning, crowdsourcing, Entity blocking and matching, block processing, strongly and nearly similar entities, batch and incremental entity resolution workflows}}
@inproceedings{10.5555/2390524.2390640,title = {Big data versus the crowd: looking for relationships in all the right places}, author = {Zhang Ce , Niu Feng , R\u00e9 Christopher , Shavlik Jude },year = {2012}, publisher = {Association for Computational Linguistics}, address = {USA}, abstract = {Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.}, location = {Jeju Island, Korea}, series = {ACL '12}, pages = {825\u2013834}, numpages = {10}}
@inproceedings{10.1145/3380688.3380702,title = {Enhancement of Convolutional Neural Networks Classifier Performance in the Classification of IoT Big Data}, author = {Amaechi Eloanyi Samson , Van Pham Hai },year = {2020}, isbn = {9781450376310}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3380688.3380702}, doi = {10.1145/3380688.3380702}, abstract = {Current developments in technologies occupy a central role in weather forecasting and the Internet-of-Things for both organizations and the IT sector. Big-data analytics and the classification of data (derived from many sources including importantly the Internet-of-Things) provides significant information on which organizations can optimize their current and future business planning. This paper considers convolutional neural networks and data classification as it relates to big-data and presents a novel approach to weather forecasting. The proposed approach targets the enhancement of convolutional neural networks and data classification to enable improved classification performance for big-data classifiers. Our contribution combines the positive benefits of convolutional neural networks with expert knowledge represented by fuzzy rules for prepared data sets in time series, the aim being to achieve improvements in the predictive quality of weather forecasting. Experimental testing demonstrates that the proposed enhanced convolutional network approach achieves a high level of accuracy in weather forecasting when compared to alternative methods evaluated.}, location = {Haiphong City, Viet Nam}, series = {ICMLSC 2020}, pages = {25\u201329}, numpages = {5}, keywords = {Neural Network, Convolutional Neural Network, Internet of Things (loT), Big data, fuzzy rules}}
@inproceedings{10.1109/CCGrid.2013.100,title = {A holistic architecture for the internet of things, sensing services and big data}, author = {Tracey David , Sreenan Cormac },year = {2013}, isbn = {9780768549965}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2013.100}, doi = {10.1109/CCGrid.2013.100}, abstract = {Wireless Sensor Networks (WSNs) increasingly enable applications and services to interact with the physical world. Such services may be located across the Internet from the sensing network. Cloud services and big data approaches may be used to store and analyse this data to improve scalability and availability, which will be required for the billions of devices envisaged in the Internet of Things (IoT). The potential of WSNs is limited by the relatively low number deployed and the difficulties imposed by their heterogeneous nature and limited (or proprietary) development environments and interfaces. This paper proposes a set of requirements for achieving a pervasive, integrated information system of WSNs and associated services. It also presents an architecture which is termed holistic as it considers the flow of the data from sensors through to services. The architecture provides a set of abstractions for the different types of sensors and services. It has been designed for implementation on a resource constrained node and to be extensible to server environments. This paper presents a 'C' implementation of the core architecture, including services on Linux and Contiki (using the Constrained Application Protocol (CoAP)) and a Linux service to integrate with the Hadoop HBase datastore.}, location = {Delft, Netherlands}, series = {CCGRID '13}, pages = {546\u2013553}, numpages = {8}, keywords = {wireless sensor networks, protocols, big data, tuple space, information model, cloud computing}}
@inproceedings{10.1145/3325773.3325779,title = {An Online Password Guessing Method Based on Big Data}, author = {Li Zhiyong , Li Tao , Zhu Fangdong },year = {2019}, isbn = {9781450372114}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3325773.3325779}, doi = {10.1145/3325773.3325779}, abstract = {Password authentication is the most widely used authentication method in information systems. The traditional proactive password detection method is generally implemented by counting password length, character class number and computing password information entropy to improve password security. However, passwords that pass proactive password detection do not represent that they are secure. In this paper, based on the research of the characteristics of password distribution under big data, we propose an online password guessing method, which collects a dataset of guessing passwords composed of weak passwords, high frequency passwords and personal information related passwords. It is used to guess the 13k password dataset leaked in China's largest ticketing website, China Railways 12306 website. The experimental results show that even if our guess object has passed the strict proactive password detection, we can construct a guessing password dataset contain only 100 passwords, and effectively guess 4.84% of the passwords.}, location = {Male, Maldives}, series = {ISMSI 2019}, pages = {59\u201362}, numpages = {4}, keywords = {Password security, proactive password check, password guessing attack}}
@inproceedings{10.1145/3495018.3495293,title = {Distributed Sensing of Animation Art Style under Big Data Technology}, author = {Hu Bofei },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495293}, doi = {10.1145/3495018.3495293}, abstract = {The distributed optical fiber sensor has the unique advantages of small size, light weight, electrical insulation, anti-electromagnetic interference and long-distance transmission and sensing. It is widely used in the fields of transportation, industry and mining, national defense, military, biological medical, aerospace and so on. This paper mainly analyzes the status quo and significance of using big data technology to solve the problems existing in distributed optical fiber sensing power cable and pipeline safety monitoring. Considering the characteristics of distributed fiber optic sensing data and big data processing technology, an overall framework of distributed fiber optic sensing big data storage, management and processing is constructed, which adopts a three-layer architecture of data acquisition layer, data storage management layer and data processing layer, and combined with the actual application scenarios to build a big data cluster.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {868\u2013872}, numpages = {5}}
@inproceedings{10.14778/2536222.2536235,title = {Adaptive and big data scale parallel execution in oracle}, author = {Bellamkonda Srikanth , Li Hua-Gang , Jagtap Unmesh , Zhu Yali , Liang Vince , Cruanes Thierry },year = {2013}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2536222.2536235}, doi = {10.14778/2536222.2536235}, abstract = {This paper showcases some of the newly introduced parallel execution methods in Oracle RDBMS. These methods provide highly scalable and adaptive evaluation for the most commonly used SQL operations - joins, group-by, rollup/cube, grouping sets, and window functions. The novelty of these techniques is their use of multi-stage parallelization models, accommodation of optimizer mistakes, and the runtime parallelization and data distribution decisions. These parallel plans adapt based on the statistics gathered on the real data at query execution time. We realized enormous performance gains from these adaptive parallelization techniques. The paper also discusses our approach to parallelize queries with operations that are inherently serial. We believe all these techniques will make their way into big data analytics and other massively parallel database systems.}, pages = {1102\u20131113}, numpages = {12}}
@inproceedings{10.1145/3463531.3463539,title = {Determinants Affecting Intention of Use of Big Data Analytics on Remote Audits: TOE Framework Approach}, author = {Leo Handoko Bambang , Edward Riantono Ignatius , Wigna Sunarto Felicia },year = {2021}, isbn = {9781450389662}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3463531.3463539}, doi = {10.1145/3463531.3463539}, abstract = {During the COVID-19 pandemics, auditors are required to do remote audit from their home or office. With the limitation of movement of the auditor, big data is the solution for the auditor as the data availability in large amounts in order to make the right decision where the process can be done quickly in effective and efficient ways. In order to help auditors do their jobs, it requires a supporting factor on doing remote audit which is technology, organization and environment. The possibilities to do audit with population based increased the fraud detection and quality of audit generated by the audit process. This article intends to define the differences, roles, fraud detection and impact of big data analytics on audit quality generated from remote audit process. This study used questionnaire to the respondents who have criteria of having experience in audit process, in order to determine which factors influence remote audit process.}, location = {Macau, China}, series = {ICETT 2021}, pages = {53\u201359}, numpages = {7}, keywords = {TOE Framework, Big Data, Audit, Remote, Analysis}}
@inproceedings{10.5555/3172795.3172829,title = {Quantifying duplication to improve data quality}, author = {Huang Yu , Chiang Fei , Maier Albert , Petitclerc Martin , Saillet Yannick , Spisic Damir , Zuzarte Calisto },year = {2017}, publisher = {IBM Corp.}, address = {USA}, abstract = {Deduplication is a costly and tedious task that involves identifying duplicate records in a dataset. High duplication rates lead to poor data quality, where data ambiguity occurs as to whether two records refer to the same entity. Existing deduplication techniques compare a set of attribute values, and verify whether given similarity thresholds are satisfied. While potential duplicate records are identified, these techniques do not provide users with any information about the degree of duplication, i.e., the varying levels of closeness among the attribute values and between records that define the duplicates.In this paper, we present a duplication metric that quantifies the level of duplication for an attribute value, and within an attribute. This metric can be used by analysts to understand the distribution and similarity of values during the data cleaning process. We present a deduplication framework that differentiates terms during similarity matching step, and is agnostic to the ordering of values within a record. We compare our framework against two existing approaches, and show that we achieve improved accuracy and performance over real data collections.}, location = {Markham, Ontario, Canada}, series = {CASCON '17}, pages = {272\u2013278}, numpages = {7}}
@inproceedings{10.14778/2367502.2367512,title = {Solving big data challenges for enterprise application performance management}, author = {Rabl Tilmann , G\u00f3mez-Villamor Sergio , Sadoghi Mohammad , Munt\u00e9s-Mulero Victor , Jacobsen Hans-Arno , Mankovskii Serge },year = {2012}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2367502.2367512}, doi = {10.14778/2367502.2367512}, abstract = {As the complexity of enterprise systems increases, the need for monitoring and analyzing such systems also grows. A number of companies have built sophisticated monitoring tools that go far beyond simple resource utilization reports. For example, based on instrumentation and specialized APIs, it is now possible to monitor single method invocations and trace individual transactions across geographically distributed systems. This high-level of detail enables more precise forms of analysis and prediction but comes at the price of high data rates (i.e., big data). To maximize the benefit of data monitoring, the data has to be stored for an extended period of time for ulterior analysis. This new wave of big data analytics imposes new challenges especially for the application performance monitoring systems. The monitoring data has to be stored in a system that can sustain the high data rates and at the same time enable an up-to-date view of the underlying infrastructure. With the advent of modern key-value stores, a variety of data storage systems have emerged that are built with a focus on scalability and high data rates as predominant in this monitoring use case.In this work, we present our experience and a comprehensive performance evaluation of six modern (open-source) data stores in the context of application performance monitoring as part of CA Technologies initiative. We evaluated these systems with data and workloads that can be found in application performance monitoring, as well as, on-line advertisement, power monitoring, and many other use cases. We present our insights not only as performance results but also as lessons learned and our experience relating to the setup and configuration complexity of these data stores in an industry setting.}, pages = {1724\u20131735}, numpages = {12}}
@inproceedings{10.1145/2668260.2668264,title = {Big data fuzzy management methods in gene regulatory networks inference: a review}, author = {Al-Quzlan Tuqya , Hamdi-Cherif Aboubekeur , Kara-Mohamed Chafia },year = {2014}, isbn = {9781450327671}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2668260.2668264}, doi = {10.1145/2668260.2668264}, abstract = {To address one of the most challenging ecosystems issues at the cellular level, this paper surveys the fuzzy methods used in gene regulatory networks (GRNs) inference. GRNs represent causal relationships between genes that have a direct influence on the life and the development of living organisms, and provide a useful contribution to the understanding of the cellular functions as well as the mechanisms of diseases. The ecosystems impacted by GRN inference span various levels from cell to society -- globally.}, location = {Buraidah, Al Qassim, Saudi Arabia}, series = {MEDES '14}, pages = {201\u2013203}, numpages = {3}, keywords = {GRN inference, Big data, Gene regulatory networks (GRNs), Fuzzy systems, Soft computing, Link and graph mining}}
@inproceedings{10.1145/3026480.3026491,title = {Study on Governmental Cultural Resources Purchase Management Based on Public Information Behavior Big Data}, author = {Ying Yang , Xialing Tang , Wei Tang },year = {2017}, isbn = {9781450348218}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3026480.3026491}, doi = {10.1145/3026480.3026491}, abstract = {With the rapid development of computer and network technology, big data analytics has been more and more widely used in public management. In the process of people obtaining and making use of cultural resources, the data of information behavior which closely related to the cultural resources is produced. These data reflect the cultural needs of people. So if the government takes the analysis of these data into account while purchasing cultural resources, the time efficiency and efficiency of fund will be improved. The governmental culture resources purchase management based on the big data analysis of public information behavior is put forward after research the public information behavior model and the book resources purchase way based on people's reading behavior -PDA(Patron - driven - acquisition).}, location = {Kuala Lumpur, Malaysia}, series = {IC4E '17}, pages = {72\u201375}, numpages = {4}, keywords = {PDA, big data, culture resources purchase management, public cultural service, information behavior}}
@inproceedings{10.1145/3377672.3378049,title = {Knowledge Maps of Tourism Big Data Research in China Based on Visualization Analysis}, author = {Jie Liu },year = {2019}, isbn = {9781450362481}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377672.3378049}, doi = {10.1145/3377672.3378049}, abstract = {OBJECTIVE:We scientifically analyze knowledge structure, development stages, research hotspots and research frontiers of tourism big data in China to provide practical and useful references for researchers to understand the research status and development trends of this field.METHODS:Published journal literatures were retrieved. A scientific collaboration analysis was conducted to visualize the relations of authors and institutions. A co-occurrence analysis was used to visualize the network of key words that was classified by the clustering analysis. Burst detection was conducted to visualize emerging words across the entire research field.RESULTS:We retrieved 964 literatures, from which 668 literatures were identified after screening. Wang Dong has published the most papers. A cooperative group of scientific research institutions with Beijing Union University as the core has been formed. The key words were classified into 6 clusters, and the frequency of \"tourism industry\" is the largest, and top 14 key words with the highest emergence intensity were detected.CONCLUSIONS:The literature of tourism big data research in China has been increasing rapidly since 2016. Three cooperative groups with Wang Dong, Liu Ligang and Pan Xinqin as the core respectively were formed, and a cooperative group of scientific research institutions with Beijing Union University as the core has been formed. The research hotspots of tourism big data in China mainly focus on six aspects: tourism industry development, key technologies of tourism big data, global tourism, tourism public service, tourists behavior, problems and countermeasures. The evolution of in this field can basically be divided into three stages: exploration (before 2012), start-up (2013-2016) and rapid development (from 2017 to present).}, location = {Kuala Lumpur, Malaysia}, series = {AMME 2019}, pages = {144\u2013153}, numpages = {10}, keywords = {Burst Detection, Scientific Collaboration, Knowledge Map, Citespace, Tourism Big Data, Co-occurrence Analysis}}
@inproceedings{10.1145/3383845.3383900,title = {Conceptualization on the Cost Management Model of Enterprise Supply Chain Under the background of Big Data}, author = {Wang Cuihong , Wang Fengzhou , He Shu },year = {2020}, isbn = {9781450376778}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3383845.3383900}, doi = {10.1145/3383845.3383900}, abstract = {In the era of big data, a company can collect useful data to obtain relevant information timely, strengthen the cost management of its supply chain, and use intelligent, digital, and sophisticated analysis methods to enhance its core business competitiveness. Firstly, this article analyzes the problems in the supply chain links such as procurement, sales, production, and logistics, and then constructs a model of enterprise supply chain system that combines a big data platform and a supply chain, and designs an enterprise supply chain cost management model in the context of big data. It also focuses on the procedures and measures of cost management in the internal supply chain and external supply chain, providing a reference for cost control in the era of big data.}, location = {Tokyo, Japan}, series = {ICCMB 2020}, pages = {19\u201324}, numpages = {6}, keywords = {Big data, cost management, supply chain costs}}
@inproceedings{10.5555/3417639.3417658,title = {Jupyter notebooks versus a textbook in a big data course}, author = {DePratti Roland },year = {2020}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {In building curriculum in new areas of computer science, often the tools introduced in the course are an important component. This is especially true in the area of big data, where the complexity of the problems the area tackles is high. In the 4 years since its inception, my big data course has gone through two major redesigns and has settled on a tool set including: the Hadoop platform, Spark processing engine, the Python programming language, Eclipse IDE, and Jupyter Notebooks. Many of the changes were driven by input from professional peers on big data teams, who were struggling with the complexity resulting from the low-level programming model used by MapReduce. Jupyter Notebook, a type of computational notebook, was added to the course to introduce students to the Python programming language. Data scientists and researchers have found computational notebooks an effective tool to manage their work by providing a way to track their thinking process, their code, and conclusions in one web document. To assess the effectiveness of using Jupyter Notebook in a big data course, students' views on the use of computational notebooks and traditional textbooks were captured and statistically analyzed.}, pages = {208\u2013220}, numpages = {13}}
@inproceedings{10.1145/3469890,title = {Novel Machine Learning for Big Data Analytics in Intelligent Support Information Management Systems}, author = {Lv Zhihan , Lou Ranran , Feng Hailin , Chen Dongliang , Lv Haibin },year = {2021}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3469890}, doi = {10.1145/3469890}, abstract = {Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields.}, pages = {1\u201321}, numpages = {21}, keywords = {big data analysis, Machine learning, accuracy rate, intelligent support information system, lightGBM}}
@inproceedings{10.1145/2786983,title = {Data Quality Challenge: Toward a Tool for String Processing by Examples}, author = {Bartoli Alberto , Lorenzo Andrea De , Medvet Eric , Tarlao Fabiano },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2786983}, doi = {10.1145/2786983}, pages = {1\u20134}, numpages = {4}, keywords = {String processing, Programming by example}}
@inproceedings{10.1109/CCGRID.2017.56,title = {Medical Imaging Processing on a Big Data platform using Python: Experiences with Heterogeneous and Homogeneous Architectures}, author = {Serrano Estefania , Blas Javier Garcia , Carretero Jesus , Abella Monica , Desco Manuel },year = {2017}, isbn = {9781509066100}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGRID.2017.56}, doi = {10.1109/CCGRID.2017.56}, abstract = {The apparition of new paradigms, programming models, and languages that offer better programmability and better performance turns the implementation of current scientific applications into a less time-consuming task than years ago. One significant example of this trend is the MapReduce programming model and its implementation using Apache Spark. Nowadays, this programming model is mainly used for data analysis and machine learning applications, although it has been expanded to its usage in the HPC community. On the side of programming languages, Python has positioned itself as an alternative to other scientific programming languages, such as Matlab or Julia. In this work we explore the capabilities of Python and Apache Spark as partners in the implementation of the backprojection operator of a CT reconstruction application. We present two interesting approaches with two different types of architectures: a heterogeneous architecture including NVidia GPUs and a full performance CPU mode with the compatibility with C/C++ native source code. We experimentally demonstrate that current CPU-based implementations scale with the number of computational units.}, location = {Madrid, Spain}, series = {CCGrid '17}, pages = {830\u2013837}, numpages = {8}, keywords = {Backprojection, Big Data, CUDA, CT, Python, Apache Spark}}
@inproceedings{10.1145/3419635.3419736,title = {Empirical Analysis for English Teaching Integration and Optimization Based on Big Data Mining Technology}, author = {Guo Jianliang },year = {2020}, isbn = {9781450387729}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3419635.3419736}, doi = {10.1145/3419635.3419736}, abstract = {This article takes English teaching behavior analysis as a breakthrough point, combines big data mining technology, optimizes English teaching plan analysis methods, and uses data mining methods on the basis of inductive analysis to compare the K nearest neighbour algorithm and the support vector machine algorithm. The algorithm is combined with the support vector machine algorithm to obtain the advantages and disadvantages of the new algorithm. A classification prediction model is built. Through the built prediction model, the prediction of English teaching integration is optimized. The classifier's prediction results for accuracy, precision, and recall are 89.63%, 90.63%, and 71.01% respectively. This can provide a basis for educators to optimize decision-making and optimize teaching methods.}, location = {Ottawa, ON, Canada}, series = {CIPAE 2020}, pages = {504\u2013508}, numpages = {5}, keywords = {English teaching, Big data, Integration and optimization, Data mining}}
@inproceedings{10.1145/3379247.3379270,title = {Use Case and Performance Analyses for Missing Data Imputation Methods in Big Data Analytics}, author = {Yang Lan , Chiang Jason Amaro },year = {2020}, isbn = {9781450376730}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3379247.3379270}, doi = {10.1145/3379247.3379270}, abstract = {In big data analytics the phenomenon of missing data is universal due to reasons such as faulty equipment and nonresponses in surveys. Imputation is the process of replacing missing data with substituted values. Proper imputation could greatly improve the accuracy and effectiveness of big data analytics.In this paper, we analyze a rich set of deletion and imputation methods, focusing on strengths, weaknesses, best use cases, implementation strategies, and error-examination based performance analysis. Our goal is to find the best fitted imputation method(s) for each given use case.}, location = {Sanya, China}, series = {ICCDE 2020}, pages = {107\u2013111}, numpages = {5}, keywords = {error estimation, imputation, Big data analytics, missing data}}
@inproceedings{10.1145/2508859.2516701,title = {When private set intersection meets big data: an efficient and scalable protocol}, author = {Dong Changyu , Chen Liqun , Wen Zikai },year = {2013}, isbn = {9781450324779}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2508859.2516701}, doi = {10.1145/2508859.2516701}, abstract = {Large scale data processing brings new challenges to the design of privacy-preserving protocols: how to meet the increasing requirements of speed and throughput of modern applications, and how to scale up smoothly when data being protected is big. Efficiency and scalability become critical criteria for privacy preserving protocols in the age of Big Data. In this paper, we present a new Private Set Intersection (PSI) protocol that is extremely efficient and highly scalable compared with existing protocols. The protocol is based on a novel approach that we call oblivious Bloom intersection. It has linear complexity and relies mostly on efficient symmetric key operations. It has high scalability due to the fact that most operations can be parallelized easily. The protocol has two versions: a basic protocol and an enhanced protocol, the security of the two variants is analyzed and proved in the semi-honest model and the malicious model respectively. A prototype of the basic protocol has been built. We report the result of performance evaluation and compare it against the two previously fastest PSI protocols. Our protocol is orders of magnitude faster than these two protocols. To compute the intersection of two million-element sets, our protocol needs only 41 seconds (80-bit security) and 339 seconds (256-bit security) on moderate hardware in parallel mode.}, location = {Berlin, Germany}, series = {CCS '13}, pages = {789\u2013800}, numpages = {12}, keywords = {bloom filters, private set intersection}}
@inproceedings{10.1145/3366030.3366103,title = {An Application of Distributed Data Mining to Identify Data Quality Problems}, author = {Januzaj Eshref , Januzaj Visar , Mandl Peter },year = {2019}, isbn = {9781450371797}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3366030.3366103}, doi = {10.1145/3366030.3366103}, abstract = {When dealing with huge data sets, during the integration process of distributed data into a single data warehouse, one is not only confronted with time and security factors but with the well known problem of low data quality as well. In order to cope with such issues that the integration of distributed data often is faced with, we present in this paper an approach that applies distributed data mining, to facilitate a data quality analysis of the data in their distributed state. Data quality problems are identified by a classifier, which uses the knowledge gained from the clustering (subspace clustering) process performed on the distributed data. Experiments on real data show that the distributed analysis results are comparable to those conducted on the central data warehouse using classical data mining.}, location = {Munich, Germany}, series = {iiWAS2019}, pages = {418\u2013422}, numpages = {5}, keywords = {Data Quality, Distributed Clustering, Data Mining}}
@inproceedings{10.1145/1864708.1864767,title = {Time dependency of data quality for collaborative filtering algorithms}, author = {De Pessemier Toon , Dooms Simon , Deryckere Tom , Martens Luc },year = {2010}, isbn = {9781605589060}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1864708.1864767}, doi = {10.1145/1864708.1864767}, abstract = {The efficiency of personal suggestions generated by collaborative filtering techniques is highly dependent on the quality and quantity of the available consumption data. Extending data sets with additional consumption data (from the past) might enrich the user profiles and generally leads to more accurate recommendations. Although if a considerable amount of profile information is already available and detailed personal preferences can be derived, supplementary consumption data may not have any (or a very limited) added value for the recommendation algorithm. These additional consumption data increase the required storage capacity and the computational load to generate the personal recommendations. Moreover, since personal preferences and the relevance of content items may vary over time, older consumption data might be outdated and lead to inaccurate recommendations. Therefore, we investigate which consumption data are (the most) relevant to feed the conventional collaborative filtering algorithms. For provider-generated content systems, we demonstrate that the accuracy of collaborative filtering algorithms increases by extending user profiles with additional older consumption data. In contrast, we witness the opposite effect for user-generated content systems: involving older consumption data has a negative influence on the recommender accuracy. These results are important for website owners who intend to employ a recommendation system at a minimum storage and computation cost.}, location = {Barcelona, Spain}, series = {RecSys '10}, pages = {281\u2013284}, numpages = {4}, keywords = {data quality, recommender systems, collaborative filtering}}
@inproceedings{10.5555/2888619.2888701,title = {Searching for effects in big data: why p-values are not advised and what to use instead}, author = {Hofmann Marko A. },year = {2015}, isbn = {9781467397414}, publisher = {IEEE Press}, abstract = {p-values of null hypothesis significance testing have long been the standard and decisive measure of deductive statistics. However, for decades, top statistical methodologists have argued that focusing on p-values is not conducive to science, and that these tests are regularly misunderstood. The standard replacement or at least complement proposed for p-values by those critics are confidence intervals and statistical effects sizes. Regrettably, analyzing and comparing huge data sets (from data mining or simulation based data farming) with two measures is awkward. As a single-value measure of first interpretation for the scanning of Big Data this article proposes statistically secured effect sizes either based on exact, mathematically sophisticated confidence intervals for effect sizes or simplified approximations. It is further argued that simplified secured effect sizes are among the most instructive single measures of statistical interpretation completely perspicuous for the layman.}, location = {Huntington Beach, California}, series = {WSC '15}, pages = {725\u2013736}, numpages = {12}}
@inproceedings{10.1145/3053600.3053621,title = {Recent Trends in Performance Modeling of Big Data Systems}, author = {Apte Varsha },year = {2017}, isbn = {9781450348997}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3053600.3053621}, doi = {10.1145/3053600.3053621}, abstract = {With the advent of big data through social media and continuous creation of digital footprints through various mobile devices, special-purpose programming models were developed that would make it easy to write programs to process such data. MapReduce and its Hadoop implementation is one of the most popular platforms for writing such programs. The MapReduce framework involves a \"map\" phase where various tasks work in parallel for intermediate processing of data and a \"reduce\" phase where again various tasks work in parallel to extract information from this processed data. Performance modeling of such systems will need different approaches than are used for traditional multi-threaded multi-core systems supporting Web applications, primarily because the dependencies and synchronization required between various tasks is not easily expressible using standard queuing network models. In this talk we will review work done by researchers to address this modeling problem. The work done encompasses first-principles calculations of execution time completion, queuing network models, and finally, simulation. We will review these efforts as well as highlight opportunities for further work in this area.}, location = {L&apos;Aquila, Italy}, series = {ICPE '17 Companion}, pages = {105}, numpages = {1}}