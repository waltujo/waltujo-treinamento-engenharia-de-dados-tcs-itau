@inproceedings{10.1145/1014052.1016916,
author = {Davidson, Ian and Grover, Ashish and Satyanarayana, Ashwin and Tayi, Giri K.},
title = {A General Approach to Incorporate Data Quality Matrices into Data Mining Algorithms},
year = {2004},
isbn = {1581138881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1014052.1016916},
doi = {10.1145/1014052.1016916},
abstract = {Data quality is a central issue for many information-oriented organizations. Recent advances in the data quality field reflect the view that a database is the product of a manufacturing process. While routine errors, such as non-existent zip codes, can be detected and corrected using traditional data cleansing tools, many errors systemic to the manufacturing process cannot be addressed. Therefore, the product of the data manufacturing process is an imprecise recording of information about the entities of interest (i.e. customers, transactions or assets). In this way, the database is only one (flawed) version of the entities it is supposed to represent. Quality assurance systems such as Motorola's Six-Sigma and other continuous improvement methods document the data manufacturing process's shortcomings. A widespread method of documentation is quality matrices. In this paper, we explore the use of the readily available data quality matrices for the data mining classification task. We first illustrate that if we do not factor in these quality matrices, then our results for prediction are sub-optimal. We then suggest a general-purpose ensemble approach that perturbs the data according to these quality matrices to improve the predictive accuracy and show the improvement is due to a reduction in variance.},
booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {794–798},
numpages = {5},
keywords = {ensemble approaches, classification, six-sigma, data quality, decision trees},
location = {Seattle, WA, USA},
series = {KDD '04}
}

@inproceedings{10.1145/2740908.2778845,
author = {Cappiello, Cinzia},
title = {On the Role of Data Quality in Improving Web Information Value},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2778845},
doi = {10.1145/2740908.2778845},
abstract = {n today's information era, every day more and more information is generated and people, on the one hand, have advantages due the increasing support in decision processes and, on the other hand, are experiencing difficulties in the selection of the right data to use. That is, users may leverage on more data but at the same time they may not be able to fully value such data since they lack the necessary knowledge about their provenance and quality. The data quality research area provides quality assessment and improvement methods that can be a valuable support for users that have to deal with the complexity of Web content. In fact, such methods help users to identify the suitability of information for their purposes. Most of the methods and techniques proposed, however, address issues for structured data and/or for defined contexts. Clearly, they cannot be easily used on the Web, where data come from heterogeneous sources and the context of use is most of the times unknown.In this keynote, the need for new assessment techniques is highlighted together with the importance of tracking data provenance as well as the reputation and trustworthiness of the sources. In fact, it is well known that the increase of data volume often corresponds to an increase of value, but to maximize such value the data sources to be used have to carefully analyzed, selected and integrated depending on the specific context of use. The talk discusses the data quality dimensions necessary to analyze different Web data sources and provides a set of illustrative examples that show how to maximize the quality of gathered information.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1433},
numpages = {1},
keywords = {data quality, web quality, data quality assessment},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/2637002.2637056,
author = {Traub, Myriam C.},
title = {Measuring and Improving Data Quality of Media Collections for Professional Tasks},
year = {2014},
isbn = {9781450329767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2637002.2637056},
doi = {10.1145/2637002.2637056},
abstract = {Carrying out research tasks on data collections is hampered, or even made impossible, by data quality issues of different types, such as incompleteness or inconsistency, and severity. We identify research tasks carried out by professional users of data collections that are hampered by inherent quality issues. We investigate what types of issues exist and how they influence these research tasks. To measure the quality perceived by professional users, we develop a quality metric. This allows us to measure the suitability of the data quality for a chosen user task. For a chosen task, we study how the data quality can be improved using crowdsourcing. We validate our quality metric by investigating whether professionals perform better on the chosen research task.},
booktitle = {Proceedings of the 5th Information Interaction in Context Symposium},
pages = {333–335},
numpages = {3},
location = {Regensburg, Germany},
series = {IIiX '14}
}

@inproceedings{10.1145/3301551.3301610,
author = {Li, Yonghong and Zhang, Shuwen and Jia, Nan},
title = {Research on the Transformation and Upgrading Path and Selection of Traditional Industries from the Perspective of Big Data},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301610},
doi = {10.1145/3301551.3301610},
abstract = {With the emergence of a new generation of information technology, big data has become an important driving force for current social development. Digitalization has become the main direction of the transformation and upgrading of traditional industries. As the product of current informatization, big data includes data quantity, data quality and data analysis ability. It is used as two different ways to interpret the value creation of big data, making it clear that it can promote the transformation and upgrading of traditional industries through value creation. Then, it puts forward the traditional industrial transformation and upgrading path from the perspective of big data, namely the linear path of "traditional industry + digital" and the transitional non-linear "digital + traditional industry". Its path selection will be analyzed by combining external and internal factors.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {54–59},
numpages = {6},
keywords = {The path, Big data, Transformation and upgrading, Traditional industries},
location = {Hong Kong, Hong Kong},
series = {ICIT 2018}
}

@inproceedings{10.1145/2351316.2351327,
author = {Magnusson, Jonathan and Kvernvik, Tor},
title = {Subscriber Classification within Telecom Networks Utilizing Big Data Technologies and Machine Learning},
year = {2012},
isbn = {9781450315470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351316.2351327},
doi = {10.1145/2351316.2351327},
abstract = {This paper describes a scalable solution for identifying influential subscribers in for example telecom networks. The solution estimates one weighted value of influence out of several Social Network Analysis(SNA) metrics. The novel method for aggregation of several metrics utilizes machine learning to train models. A prototype solution has been implemented on a Hadoop platform to support scalability and to reduce hard ware cost by enabling the usage of commodity computers. The SNA algorithms have been adapted to efficiently execute on the MapReduce distributed platform. The prototype solution has been tested on a Hadoop cluster. The tests have verified that the solution can scale to support networks with millions of subscribers. Both real data from a telecom network operator with 2.4 million subscribers and synthetic data for networks up to 100 million subscribers have been used to verify the scalability and accuracy of the solution. The correlation between metrics have been analyzed to identify the information gain from each metric.},
booktitle = {Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
pages = {77–84},
numpages = {8},
keywords = {scalability, big data, social network analysis, telecommunication, machine learning},
location = {Beijing, China},
series = {BigMine '12}
}

@inproceedings{10.1145/2896387.2896423,
author = {Pise, Priya Dudhale and Uke, Nilesh J.},
title = {Efficient Security Framework for Sensitive Data Sharing and Privacy Preserving on Big-Data and Cloud Platforms},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2896423},
doi = {10.1145/2896387.2896423},
abstract = {Now day's use of big data platforms is increasing for storing large amount of end user's data remotely on big data servers. Cloud computing storage was widely used for storing user's data, but cloud computing only providing the tasks of data storage but not supporting the important functionalities like computation and database operations. These operations are supported by big data systems and hence currently use of big data platform for storage in increases worldwide by enterprises. Sharing sensitive information and data resulted into big reduction in costs of enterprises for users to provide value added data and personalized services. As enterprises are sharing their important and sensitive information on big data platforms from different and many domains, it becomes necessary to provide the security and privacy in big data platform. Data security and privacy is gaining significant attentions of researchers. There are many security methods already proposed for cloud computing platform, now same methods slowly adopted on big data platform. For Big Data platforms, secure sharing of sensitive data is challenging research problem. In this paper, first we are introducing the different security and privacy preserving methods of cloud computing and big data platforms with their limitations, and then presenting the novel hybrid framework for secure sensitive data sharing and privacy preserving public auditing for shared data over big data systems including functionalities such as privacy preserving, public auditing, data security, storage, data access, deletion or secure data destruction using cloud services.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {38},
numpages = {5},
keywords = {Public Auditing, Ring Search, Sensitive Data, Data Sharing, Privacy Preserving, Big Data, Data Security, Proxy Re-encryption},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.5555/3400397.3400444,
author = {Giabbanelli, Philippe J.},
title = {Solving Challenges at the Interface of Simulation and Big Data Using Machine Learning},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Modeling &amp; Simulation (M&amp;S) and Machine Learning (ML) have been used separately for decades. They can also straightforwardly be employed in the same study by contrasting the results of a theory-driven M&amp;S model with the most accurate data-driven ML model. In this paper, we propose a paradigm shift from seeing ML and M&amp;S as two independent activities to identifying how their integration can solve challenges that emerge in a big data context. Since several works have already examined this interaction for conceptual modeling or model building (e.g., creating components with ML and embedding them in the M&amp;S model), our analysis is devoted on three relatively under-studied stages: calibrating a simulation model using ML, dealing with the issues of large search space by employing ML for experimentation, and identifying the right visualizations of model output by applying ML to characteristics of the output or actions of the users.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {572–583},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3129676.3129719,
author = {Kim, Jaehyun and Kim, Taehyoung and Ham, Kyung Sun},
title = {A Study on Prediction Comparison by Time Series Analysis Model of Load Big Data},
year = {2017},
isbn = {9781450350273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129676.3129719},
doi = {10.1145/3129676.3129719},
abstract = {As energy supply changes become more important, interest in the field of efficient energy management is increasing. In this paper, demand forecasting is performed through time series analysis of power big data. And we measured the performance through predictive comparison of time series prediction model.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {75–76},
numpages = {2},
keywords = {Model Comparison, Time Series Data, Datamining, Energy Forecast, Bigdata Analysis},
location = {Krakow, Poland},
series = {RACS '17}
}

@inproceedings{10.1145/3365109.3368775,
author = {Yan, Tao and Han, Chongzhao and Jia, Yong},
title = {A Fused Intelligent Computing Approach Using Stock Big Data for Near Future Trend Prediction},
year = {2019},
isbn = {9781450370165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365109.3368775},
doi = {10.1145/3365109.3368775},
abstract = {This research makes an attempt of using historical stock big data to predict near future trend of the stock. To fulfill this task, a novel approach based on fused intelligent computing is introduced and investigated. It is composited of four main parts, including data discretization, attribute reduction, classification and decision fusion. Further, one or two algorithms are adopted to realize specific function in each part, respectively. The given stock indexes are selected by the reduction algorithm of discernibility matrix, and the outputs of multiple classifiers are fused by the decision fusion algorithm. These processes and other ones are dedicated to enhancing the accuracy of stock trend prediction. To demonstrate the effectiveness of our approach, a variety of experimental simulations utilizing historical data of three stocks in NASDAQ are carried out, and the prediction accuracy of the proposed approach are compared as well. The experimental results prove that our approach could accomplish the prediction task with high accuracy.},
booktitle = {Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {113–116},
numpages = {4},
keywords = {data discretization, attribute reduction, stock trend prediction, decision fusion, fused intelligent computing},
location = {Auckland, New Zealand},
series = {BDCAT '19}
}

@inproceedings{10.1145/3265639.3265680,
author = {Guo, Aizhang and Liu, Xiuyuan and Sun, Tao},
title = {Research on Key Problems of Data Quality in Large Industrial Data Environment},
year = {2018},
isbn = {9781450365307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265639.3265680},
doi = {10.1145/3265639.3265680},
abstract = {At present, the modern manufacturing and management concepts such as digitalization, networking and intellectualization have been popularized in the industry, and the degree of industrial automation and information has been improved unprecedentedly. Industrial products are everywhere in the world. They are involved in design, manufacture, operation, maintenance and recycling. The whole life cycle involves huge amounts of data. Improving data quality is very important for data mining and data analysis. To solve the problem of data inconsistency is a very important part of improving data quality.},
booktitle = {Proceedings of the 3rd International Conference on Robotics, Control and Automation},
pages = {245–248},
numpages = {4},
keywords = {Data Quality, Data Inconsistency, Data Cleaning},
location = {Chengdu, China},
series = {ICRCA '18}
}

@inproceedings{10.1145/3289600.3291372,
author = {Zhou, Xiangmin and Zhang, Ji and Zhang, Yanchun},
title = {The 1st International Workshop on Context-Aware Recommendation Systems with Big Data Analytics (CARS-BDA)},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3291372},
doi = {10.1145/3289600.3291372},
abstract = {With the explosive growth of online service platforms, increasing number of people and enterprises are doing everything online. In order for organizations, governments, and individuals to understand their users, and promote their products or services, it is necessary for them to analyse big data and recommend the media or online services in real time. Effective recommendation of items of interest to consumers has become critical for enterprises in domains such as retail, e-commerce, and online media. Driven by the business successes, academic research in this field has also been active for many years. Through many scientific breakthroughs have been achieved, there are still tremendous challenges in developing effective and scalable recommendation systems for real-world industrial applications. Existing solutions focus on recommending items based on pre-set contexts, such as time, location, weather etc. The big data sizes and complex contextual information add further challenges to the deployment of advanced recommender systems. This workshop aims to bring together researchers with wide-ranging backgrounds to identify important research questions, to exchange ideas from different research disciplines, and, more generally, to facilitate discussion and innovation in the area of context-aware recommender systems and big data analytics.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {842–843},
numpages = {2},
keywords = {context-aware recommendation, big data analysis},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/2910674.2910685,
author = {Sideris, Costas and Shaikh, Sakib and Kalantarian, Haik and Sarrafzadeh, Majid},
title = {A Big-Data Platform for Medical Knowledge Extraction from Electronic Health Records: Automatic Assignment of ICD-9 Codes},
year = {2016},
isbn = {9781450343374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910674.2910685},
doi = {10.1145/2910674.2910685},
abstract = {In this paper, we present a big data plarform for knowledge categorization in Electronic Health Records and examine its application to automatic assignment of ICD-9 codes. Our platform relies on reusable, adaptable components that can perform knowledge extraction at a large scale. For the ICD-9 automatic assignment, we build and validate our approach using data from the MIMIC II Clinical Database that contains over 20,000 discharge summaries. We show that our platform can achieve state of the art performance in this dataset and that the classification results improve with more data. Overall, in the first level of the ICD-9 hierarchy our algorithm achieves an average precision of 79.7% for an average recall of 70.2%.},
booktitle = {Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {77},
numpages = {2},
keywords = {big data, knowledge extraction, ICD},
location = {Corfu, Island, Greece},
series = {PETRA '16}
}

@article{10.1145/2627534.2627560,
author = {Whitworth, Jeff and Suthaharan, Shan},
title = {Security Problems and Challenges in a Machine Learning-Based Hybrid Big Data Processing Network Systems},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627560},
doi = {10.1145/2627534.2627560},
abstract = {The data source that produces data continuously in high volume and high velocity with large varieties of data types creates Big Data, and causes problems and challenges to Machine Learning (ML) techniques that help extract, analyze and visualize important information. To overcome these problems and challenges, we propose to make use of the hybrid networking model that consists of multiple components such as Hadoop distributed file system (HDFS), cloud storage system, security module and ML unit. Processing of Big Data in this networking environment with ML technique requires user interaction and additional storage hence some artificial delay between the arrivals of data domains through external storage can help HDFSto process the Big Data efficiently. To address this problem we suggest using public cloud for data storage which will induce meaningful time delay to the data while making use of its storage capability. However, the use of public cloud will lead to security vulnerability to the data transmission and storage. Therefore, we need some form of security algorithm that provides a flexible key-based encryption technique that can provide tradeoffs between time-delay, security strength and storage risks. In this paper we propose a model for using public cloud provider trust levels to select encryption types for data storage for use within a Big Data analytics network topology.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {82–85},
numpages = {4},
keywords = {encryption, machine learning, hybrid cloud, big data, retrievability}
}

@inproceedings{10.1145/3219819.3220015,
author = {Nguyen, Khanh and Le, Trung and Nguyen, Tu Dinh and Phung, Dinh and Webb, Geoffrey I.},
title = {Robust Bayesian Kernel Machine via Stein Variational Gradient Descent for Big Data},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220015},
doi = {10.1145/3219819.3220015},
abstract = {Kernel methods are powerful supervised machine learning models for their strong generalization ability, especially on limited data to effectively generalize on unseen data. However, most kernel methods, including the state-of-the-art LIBSVM, are vulnerable to the curse of kernelization, making them infeasible to apply to large-scale datasets. This issue is exacerbated when kernel methods are used in conjunction with a grid search to tune their kernel parameters and hyperparameters which brings in the question of model robustness when applied to real datasets. In this paper, we propose a robust Bayesian Kernel Machine (BKM) - a Bayesian kernel machine that exploits the strengths of both the Bayesian modelling and kernel methods. A key challenge for such a formulation is the need for an efficient learning algorithm. To this end, we successfully extended the recent Stein variational theory for Bayesian inference for our proposed model, resulting in fast and efficient learning and prediction algorithms. Importantly our proposed BKM is resilient to the curse of kernelization, hence making it applicable to large-scale datasets and robust to parameter tuning, avoiding the associated expense and potential pitfalls with current practice of parameter tuning. Our extensive experimental results on 12 benchmark datasets show that our BKM without tuning any parameter can achieve comparable predictive performance with the state-of-the-art LIBSVM and significantly outperforms other baselines, while obtaining significantly speedup in terms of the total training time compared with its rivals},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2003–2011},
numpages = {9},
keywords = {multiclass supervised learning, variational method, stein divergence, big data, kernel methods, random feature, bayesian inference},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/2591062.2591088,
author = {Camilli, Matteo},
title = {Formal Verification Problems in a Big Data World: Towards a Mighty Synergy},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591088},
doi = {10.1145/2591062.2591088},
abstract = {Formal verification requires high performance data processing software for extracting knowledge from the unprecedented amount of data coming from analyzed systems. Since cloud based computing resources have became easily accessible, there is an opportunity for verification techniques and tools to undergo a deep technological transition to exploit the new available architectures. This has created an increasing interest in parallelizing and distributing verification techniques. In this paper we introduce a distributed approach which exploits techniques typically used by the bigdata community to enable verification of very complex systems using bigdata approaches and cloud computing facilities.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {638–641},
numpages = {4},
keywords = {Big Data, Formal Verification, MapReduce, CTL},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.5555/3382225.3382363,
author = {Zhao, Ying and Zhou, Charles C.},
title = {A Game-Theoretic Lexical Link Analysis for Discovering High-Value Information from Big Data},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {We demonstrate a machine learning and artificial intelligence method, i.e., lexical link analysis (LLA) to discover high-value information from big data. In this paper, high-value information refers to the information that has the potential to grow its value over time. LLA is a unsupervised learning method that does not require manually labeled training data. New value metrics are defined based on a game-theoretic framework for LLA. In this paper, we show the value metrics generated from LLA in a use case of analyzing business news. We show the results from LLA are validated and correlated with the ground truth. We show that by using game theory, the high-value information selected by LLA reaches a Nash equilibrium by superpositioning popular and anomalous information, and at the same time generates high social welfare, therefore, contains higher intrinsic value.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {621–625},
numpages = {5},
keywords = {unsupervised learning, nash equilibrium, big data, lexical link analysis, pareto efficient, social welfare, game theory, high-value, pareto superior},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/2486767.2486771,
author = {Qin, Chengie and Rusu, Florin},
title = {Scalable I/O-Bound Parallel Incremental Gradient Descent for Big Data Analytics in GLADE},
year = {2013},
isbn = {9781450322027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486767.2486771},
doi = {10.1145/2486767.2486771},
abstract = {Incremental gradient descent is a general technique to solve a large class of convex optimization problems arising in many machine learning tasks. GLADE is a parallel infrastructure for big data analytics providing a generic task specification interface. In this paper, we present a scalable and efficient parallel solution for incremental gradient descent in GLADE. We provide empirical evidence that our solution is limited only by the physical hardware characteristics, uses effectively the available resources, and achieves maximum scalability. When deployed in the cloud, our solution has the potential to dramatically reduce the cost of complex analytics over massive datasets.},
booktitle = {Proceedings of the Second Workshop on Data Analytics in the Cloud},
pages = {16–20},
numpages = {5},
location = {New York, New York},
series = {DanaC '13}
}

@inproceedings{10.1145/3424978.3425137,
author = {Shi, Qiao and Wang, Honglv and Lu, Hailong},
title = {Research and Application of AHP-EWM-Based Comprehensive Evaluation of Data Quality},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425137},
doi = {10.1145/3424978.3425137},
abstract = {In order to improve the operation quality of the cross-enterprise batch management system in the tobacco industry, a comprehensive data quality index evaluation model is proposed in this paper. By analyzing the characteristics of batch management data, 7 indicators are selected to evaluate the system data, and the influence of human factors is eliminated by the combination weight determined by the analytic hierarchy process (AHP) and entropy weight method (EWM). The proposed evaluation model is applied to the quality assessment of the actual statistical data of Zhejiang Tobacco Industries Co. and six cooperative production enterprises in the cigarette industry. The results show that the model can scientifically evaluate the system data quality of each enterprise, intuitively rank the data quality of various enterprises, and truly reflect the change trend of data quality, indicating the feasibility and effectiveness of the model. This method can provide support for improving the data quality level of the cross-enterprise batch management system.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {152},
numpages = {5},
keywords = {Tobacco industry, Analytic hierarchy process, Data quality, Batch management, Synthetic evaluation, Entropy weight method},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.5555/3374138.3374194,
author = {Bawatna, Mohammed and Green, Bertram and Kovalev, Sergey and Deinert, Jan-Christoph and Knodel, Oliver and Spallek, Rainer G.},
title = {Research and Implementation of Efficient Parallel Processing of Big Data at TELBE User Facility},
year = {2019},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {In recent years, improvements in high-speed Analog-to-Digital Converters (ADC) and sensor technology has encouraged researchers to improve the performance of Data Acquisition (DAQ) systems for scientific experiments which require high speed and continuous data measurements --- in particular, measuring the electronic and magnetic properties of materials using pump-probe experiments at high repetition rates. Experiments at TELBE are capable of acquiring almost 100 Gigabytes of raw data every ten minutes. The DAQ system used at TELBE partitions the raw data into various subdirectories for further parallel processing utilizing the multicore structure of modern CPUs.Furthermore, several other types of processors that accelerate data processing like the GPU and FPGA have emerged to solve the challenges of processing the massive amount of raw data. However, the memory and network bottlenecks become a significant challenge in big data processing, and new scalable programming techniques are needed to solve these challenges. In this contribution, we will outline the design and implementation of our practical software approach for efficient parallel processing of our large data sets at the TELBE user facility.},
booktitle = {Proceedings of the 2019 Summer Simulation Conference},
articleno = {56},
numpages = {6},
keywords = {data processing pipeline, big data, data analytics, signal processing, data acquisition systems},
location = {Berlin, Germany},
series = {SummerSim '19}
}

@inproceedings{10.1145/2631775.2631812,
author = {Hidalgo, Cesar},
title = {Big Data Visualization Engines for Understanding the Development of Countries, Social Networks, Culture and Cities},
year = {2014},
isbn = {9781450329545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2631775.2631812},
doi = {10.1145/2631775.2631812},
abstract = {Big data can be used for more than improving the targeting of marketing campaigns. In this talk I will present five big data visualization engines we have created at the MIT Media Lab's Macro Connections group and will show how we can use big data and visualizations to improve our understanding of the development of economies, cultures and cities. The data visualization engines I will demo include (i) the Observatory of Economic Complexity (atlas.media.mit.edu), which is the most comprehensive tool for exploring international trade data created to date; (ii) DataViva (dataviva.info), which is a tool we created to open up data for the entire formal sector economy of Brazil, including data on all of the working force, municipalities, industries, and occupations of Brazil; (iii) Pantheon (pantheon.media.mit.edu), a dataset and visualization engine we created to explore global patterns of cultural production; (iv) Immersion (immersion.media.mit.edu), which is a tool that inverts the email interface, by focusing it on people rather than messages; and (v) Place Pulse and StreetScore (pulse.media.mit.edu &amp; streetscore.media.mit.edu), which are crowd-sourcing and machine learning tools we have developed to help understand the aesthetic aspects of cities and their evolution.},
booktitle = {Proceedings of the 25th ACM Conference on Hypertext and Social Media},
pages = {3},
numpages = {1},
keywords = {urban computing, big data, cultural production, data visualization, information visualization, economic complexity},
location = {Santiago, Chile},
series = {HT '14}
}

@inproceedings{10.1145/3383845.3383900,
author = {Wang, Cuihong and Wang, Fengzhou and He, Shu},
title = {Conceptualization on the Cost Management Model of Enterprise Supply Chain Under the Background of Big Data},
year = {2020},
isbn = {9781450376778},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383845.3383900},
doi = {10.1145/3383845.3383900},
abstract = {In the era of big data, a company can collect useful data to obtain relevant information timely, strengthen the cost management of its supply chain, and use intelligent, digital, and sophisticated analysis methods to enhance its core business competitiveness. Firstly, this article analyzes the problems in the supply chain links such as procurement, sales, production, and logistics, and then constructs a model of enterprise supply chain system that combines a big data platform and a supply chain, and designs an enterprise supply chain cost management model in the context of big data. It also focuses on the procedures and measures of cost management in the internal supply chain and external supply chain, providing a reference for cost control in the era of big data.},
booktitle = {Proceedings of the 2020 the 3rd International Conference on Computers in Management and Business},
pages = {19–24},
numpages = {6},
keywords = {Big data, cost management, supply chain costs},
location = {Tokyo, Japan},
series = {ICCMB 2020}
}

@inproceedings{10.1109/CCGrid.2015.46,
author = {Peng, Shaoliang and Liao, Xiangke and Yang, Canqun and Lu, Yutong and Liu, Jie and Cui, Yingbo and Wang, Heng and Wu, Chengkun and Wang, Bingqiang},
title = {The Challenge of Scaling Genome Big Data Analysis Software on TH-2 Supercomputer},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.46},
doi = {10.1109/CCGrid.2015.46},
abstract = {Whole genome re-sequencing plays a crucial role in biomedical studies. The emergence of genomic big data calls for an enormous amount of computing power. However, current computational methods are inefficient in utilizing available computational resources. In this paper, we address this challenge by optimizing the utilization of the fastest supercomputer in the world - TH-2 supercomputer. TH-2 is featured by its neo-heterogeneous architecture, in which each compute node is equipped with 2 Intel Xeon CPUs and 3 Intel Xeon Phi coprocessors. The heterogeneity and the massive amount of data to be processed pose great challenges for the deployment of the genome analysis software pipeline on TH-2. Runtime profiling shows that SOAP3-dp and SOAPsnp are the most time-consuming components (up to 70% of total runtime) in a typical genome-analyzing pipeline. To optimize the whole pipeline, we first devise a number of parallel and optimization strategies for SOAP3-dp and SOAPsnp, respectively targeting each node to fully utilize all sorts of hardware resources provided both by CPU and MIC. We also employ a few scaling methods to reduce communication between different nodes. We then scaled up our method on TH-2. With 8192 nodes, the whole analyzing procedure took 8.37 hours to finish the analysis of a 300 TB dataset of whole genome sequences from 2,000 human beings, which can take as long as 8 months on a commodity server. The speedup is about 700x.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {823–828},
numpages = {6},
keywords = {TH-2 supercomputer, parallel optimization, sequence alignment, SNP detection, whole genome re-sequencing},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1109/SC.2014.66,
author = {Chung, I-Hsin and Sainath, Tara N. and Ramabhadran, Bhuvana and Picheny, Michael and Gunnels, John and Austel, Vernon and Chauhari, Upendra and Kingsbury, Brian},
title = {Parallel Deep Neural Network Training for Big Data on Blue Gene/Q},
year = {2014},
isbn = {9781479955008},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2014.66},
doi = {10.1109/SC.2014.66},
abstract = {Deep Neural Networks (DNNs) have recently been shown to significantly outperform existing machine learning techniques in several pattern recognition tasks. DNNs are the state-of-the-art models used in image recognition, object detection, classification and tracking, and speech and language processing applications. The biggest drawback to DNNs has been the enormous cost in computation and time taken to train the parameters of the networks - often a tenfold increase relative to conventional technologies. Such training time costs can be mitigated by the application of parallel computing algorithms and architectures. However, these algorithms often run into difficulties because of the cost of inter-processor communication bottlenecks. In this paper, we describe how to enable Parallel Deep Neural Network Training on the IBM Blue Gene/Q (BG/Q) computer system. Specifically, we explore DNN training using the data-parallel Hessian-free 2nd order optimization algorithm. Such an algorithm is particularly well-suited to parallelization across a large set of loosely coupled processors. BG/Q, with its excellent inter-processor communication characteristics, is an ideal match for this type of algorithm. The paper discusses how issues regarding programming model and data-dependent imbalances are addressed. Results on large-scale speech tasks show that the performance on BG/Q scales linearly up to 4096 processes with no loss in accuracy. This allows us to train neural networks using billions of training examples in a few hours.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
pages = {745–753},
numpages = {9},
keywords = {high performance computing, speech recognition, big data},
location = {New Orleans, Louisana},
series = {SC '14}
}

@inproceedings{10.1145/3260596,
author = {Suciu, Dan},
title = {Session Details: Data Quality, Data Privacy, &amp; Invited Tutorial 2},
year = {2008},
isbn = {9781605581521},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3260596},
doi = {10.1145/3260596},
booktitle = {Proceedings of the Twenty-Seventh ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
location = {Vancouver, Canada},
series = {PODS '08}
}

@inproceedings{10.1145/3396452.3396453,
author = {Conglin, Cheng and Lin, Li and Yi, Li and Lei, Tan},
title = {A Study on College Students' Mental Health Education and Early Warning Mechanism Based on Big Data},
year = {2020},
isbn = {9781450374989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396452.3396453},
doi = {10.1145/3396452.3396453},
abstract = {Since the mental health education of college students has attracted more and more attention, it is of great importance and necessity to strengthen the construction of psychological warning mechanism for college students. With the advent of the new media era, it is time to apply big data technology to mental health education in colleges and universities and analyze the mental health data of college students. It can be said that the construction of an early warning mechanism based on big data and the realization of crisis intervention will help promote the innovative development of mental health education for college students.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Big Data and Education},
pages = {1–4},
numpages = {4},
keywords = {psychological early warning mechanism, mental health education, big data},
location = {London, United Kingdom},
series = {ICBDE '20}
}

@inproceedings{10.1145/3358528.3358581,
author = {Qian, Jianfa and Zhang, Lina},
title = {Improved Constructions for Optimal Multi-Erasure Locally Recoverable Codes for Big Data Storage},
year = {2019},
isbn = {9781450371926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358528.3358581},
doi = {10.1145/3358528.3358581},
abstract = {Multi-erasure locally recoverable codes play a very significant role in distributed data storage. The advantage of multi-erasure locally recoverable codes is that it has local and global erasure-correcting characteristics. Recently, based on classical algebraic geometry codes, Huang et al. constructed a family of explicit optimal multi-erasure locally recoverable codes over small finite fields F4. In this work, based on the work of Huang et al., we use cyclic codes to construct a family of new optimal multi-erasure locally recoverable codes over small finite fields F3. It turns out that our multi-erasure locally recoverable codes have smaller finite fields than the previously known results.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
pages = {44–47},
numpages = {4},
keywords = {data storage systems, optimal code, Multi-erasure locally recoverable code},
location = {Jinan, China},
series = {ICBDT2019}
}

@inproceedings{10.5555/2814058.2814137,
author = {Attorre, Brunno F. M. and Silva, Leandro A.},
title = {Open Source Tools Applied to Text Data Recovery in Big Data Environments},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {As the volume of data on the web continue to increase, it is getting more challenging for the search mechanism to find with a high precision rate what the users want to find. As a solution to improve these results, the development of a recommender engine, based on the content of the documents, would prove itself very useful. In this context, this research has the objective to show how the current search and indexing tools could be improved with recommendation, Machine Learning and textual analysis algorithms. The idea behind these project would be to, based on the content of the documents recovered in the search, find similar documents using most of the Open Source technology we have available right now.},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
pages = {487–492},
numpages = {6},
keywords = {Machine Learning Tools, Machine Learning, big data, Index tools},
location = {Goiania, Goias, Brazil},
series = {SBSI 2015}
}

@inproceedings{10.1145/3361758.3361761,
author = {Song, Meina and Xu, Xiangyu and Haihong, E.},
title = {Research and Top-Level Framework Design on Unified Resource Management of Big Data in Strategic Consulting},
year = {2019},
isbn = {9781450372466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361758.3361761},
doi = {10.1145/3361758.3361761},
abstract = {This paper puts forward a set of top-level framework design methodology for unified data resource management aiming at the characteristics of big data, multi-source and heterogeneous, and the difficulty of unified organization and management, and completes the top-level framework of big data for strategic consulting based on this methodology for the scenario of strategic consulting. This paper investigates the unified data resource integration model at home and abroad, and proposed a set of architecture design methodology based on it. The methodology includes three aspects: metadata-driven, hierarchical organization, separation and reorganization. Then the article takes the real data as an example and gives the preliminary results of the model to prove the validity of the model.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Internet of Things},
pages = {8–12},
numpages = {5},
keywords = {Unified Resource Management, Strategic consultation, Big data, Top level framework},
location = {Melbourn, VIC, Australia},
series = {BDIOT 2019}
}

@inproceedings{10.1109/CCGRID.2017.56,
author = {Serrano, Estefania and Blas, Javier Garcia and Carretero, Jesus and Abella, Monica and Desco, Manuel},
title = {Medical Imaging Processing on a Big Data Platform Using Python: Experiences with Heterogeneous and Homogeneous Architectures},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.56},
doi = {10.1109/CCGRID.2017.56},
abstract = {The apparition of new paradigms, programming models, and languages that offer better programmability and better performance turns the implementation of current scientific applications into a less time-consuming task than years ago. One significant example of this trend is the MapReduce programming model and its implementation using Apache Spark. Nowadays, this programming model is mainly used for data analysis and machine learning applications, although it has been expanded to its usage in the HPC community. On the side of programming languages, Python has positioned itself as an alternative to other scientific programming languages, such as Matlab or Julia. In this work we explore the capabilities of Python and Apache Spark as partners in the implementation of the backprojection operator of a CT reconstruction application. We present two interesting approaches with two different types of architectures: a heterogeneous architecture including NVidia GPUs and a full performance CPU mode with the compatibility with C/C++ native source code. We experimentally demonstrate that current CPU-based implementations scale with the number of computational units.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {830–837},
numpages = {8},
keywords = {Python, Apache Spark, Big Data, Backprojection, CUDA, CT},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/3358528.3358566,
author = {Lu, Zeshan and Liu, Kun and Liu, Zhen and Wang, Cong and Shen, Maoxin and Xu, Tao},
title = {An Efficient Annotation Method for Big Data Sets of High-Resolution Earth Observation Images},
year = {2019},
isbn = {9781450371926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358528.3358566},
doi = {10.1145/3358528.3358566},
abstract = {High-resolution earth observation images have increased dramatically because of the increasing of remote sensing satellites. Researchers must do large-scale target annotations to meet the training needs of deep neural network based model. However, most existing datasets contain an insufficient number of annotated samples, due to the inefficient manual annotation process which reason lies in the large number of remote sensing images, huge size, numerous targets, and high accuracy requirements. This paper proposed an efficient annotation method for big data sets of high-resolution earth observation images, in which the annotation process is divided into two parallel sub-processes, fast panchromatic image labeling and multi-spectral image fusion. Automatic scale transform is utilized for annotation of fused imagery. Experimental results show that the proposed method could improve the accuracy and efficiency of target labeling. Mask-RCNN and Faster-RCNN based target detection results demonstrate the validity of the big dataset annotated via our method.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
pages = {240–243},
numpages = {4},
keywords = {Mask-RCNN, Annotation method, high-resolution images, target detection},
location = {Jinan, China},
series = {ICBDT2019}
}

@inproceedings{10.1145/3257761,
author = {Kostkova, Patty},
title = {Session Details: Big Data and Social Media for Public Health Surveillance},
year = {2016},
isbn = {9781450342247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257761},
doi = {10.1145/3257761},
booktitle = {Proceedings of the 6th International Conference on Digital Health Conference},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {DH '16}
}

@inproceedings{10.1145/3194554.3194633,
author = {Chang, Mu-Tien and Choi, I. Stephen and Niu, Dimin and Zheng, Hongzhong},
title = {Performance Impact of Emerging Memory Technologies on Big Data Applications: A Latency-Programmable System Emulation Approach},
year = {2018},
isbn = {9781450357241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194554.3194633},
doi = {10.1145/3194554.3194633},
abstract = {This paper presents a performance analysis framework for studying emerging memories. The key component of the framework is a memory-latency programmable emulator, which is based on a FPGA-attached server system. The emulator allows users extend read and/or write latency. In addition, we use regression models to enable system performance studies for memory latencies beyond hardware limitations. Finally, we demonstrate Spark application case studies, analyzing the impact of two key characteristics of emerging memories: extended memory access times and enlarged memory capacities. Results show that the benefit of high capacity memory could outweigh the performance loss due to longer memory latency.},
booktitle = {Proceedings of the 2018 on Great Lakes Symposium on VLSI},
pages = {439–442},
numpages = {4},
keywords = {big data, performance analysis, emerging memories},
location = {Chicago, IL, USA},
series = {GLSVLSI '18}
}

@proceedings{10.1145/3421537,
title = {BDIOT '20: Proceedings of the 2020 4th International Conference on Big Data and Internet of Things},
year = {2020},
isbn = {9781450375504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the conference committees, it is my pleasure to address a warm welcome to all delegates to participate in2020 the 4th International Conference on Big Data and Internet of Things(BDIOT2020). This conference is technically sponsored by University of Macau, La trobe University and Universita Di Pisa, and also attracts some media partners.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/2818869.2818934,
author = {Lee, Chung-Hong and Yang, Hsin-Chang and Cheng, Shou-Chen and Tsai, Sheng-Wen},
title = {A Hybrid Big Data Analytics Method for Yield Improvement in Semiconductor Manufacturing},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818934},
doi = {10.1145/2818869.2818934},
abstract = {In the manufacturing of semiconductor encapsulation, the production yield is one of critical issues concerned by all foundries. It is because that yield rate can directly affects the quality of the final product and the profitability. In this work we take the defect-products as samples and use machine learning techniques to classify the samples and verify the accuracy and feasibility of the experiment. We use Support Vector Machines (SVM) model to perform classification and compare the resulting accuracy with the results of Back Propagation Neural Network (BPN) model. Furthermore, we employ a statistical method namely Pearson product-moment correlation coefficient to identify the influential factors for production quality. The experimental result demonstrates that our hybrid method has great potentials for yield improvement in semiconductor manufacturing.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {9},
numpages = {4},
keywords = {Machine Learning, Big Data Analysis, Support Vector Machines},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@inproceedings{10.1145/3203217.3205863,
author = {Bartolini, Andrea and Borghesi, Andrea and Libri, Antonio and Beneventi, Francesco and Gregori, Daniele and Tinti, Simone and Gianfreda, Cosimo and Alto\`{e}, Piero},
title = {The D.A.V.I.D.E. Big-Data-Powered Fine-Grain Power and Performance Monitoring Support},
year = {2018},
isbn = {9781450357616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3203217.3205863},
doi = {10.1145/3203217.3205863},
abstract = {On the race toward exascale supercomputing systems are facing important challenges which limit the efficiency of the system. Among all, power and energy consumption fueled by the end of Dennard's scaling start to show their impact on limiting supercomputers peak performance and cost effectiveness.In this paper we present and describe a new methodology based on a set of HW and SW extensions for fine-grain monitoring of power and aggregation of them for fast analysis and visualization. We propose a turn-key system which uses MQTT communication layer, NoSQL database, fine grain monitoring and in future AI technology to measure and control power and performance. This methodology is shown as an integrated feature of the D.A.V.I.D.E. supercomputing machine.},
booktitle = {Proceedings of the 15th ACM International Conference on Computing Frontiers},
pages = {303–308},
numpages = {6},
keywords = {beaglebone black, high performance computing, big data, fine-grain power and performance monitoring, AMESTER},
location = {Ischia, Italy},
series = {CF '18}
}

@inproceedings{10.1145/3264560.3264562,
author = {Ezzikouri, Hanane and Oukessou, Mohamed and Youness, Madani and Erritali, Mohamed},
title = {Fuzzy Cross Language Plagiarism Detection (Arabic-English) Using WordNet in a Big Data Environment},
year = {2018},
isbn = {9781450364744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264560.3264562},
doi = {10.1145/3264560.3264562},
abstract = {Cross-Language Plagiarism refers to the unacknowledged reuse of a text involving its translation from one natural language to another without proper referencing to the original source. One of the common problems in data processing is efficient large-scale text comparison, especially semantic based similarity due to the increase in the number of publications and the rate of suspicious documents sources of plagiarism. CLPD nature could be more complicated than simple copy+translate and paste, thus the detecting process exposes the need for a vague concept and fuzzy sets techniques in a big data environment to reveal dishonest practices in Arabic documents. In this paper, we propose a new Cross-Language Plagiarism Detection based on fuzzy-semantic similarity using WordNet and two semantic approaches Wu&amp;Palmer and Lin; the work is done in a parallel way using Apache Hadoop with its distributed file system HDFS and the MapReduce programming model. The experimental results show that the Fuzzy Wu &amp; Palmer have high performance than Fuzzy Lin.},
booktitle = {Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing},
pages = {22–27},
numpages = {6},
keywords = {Semantic Similarity, MapReduce, Fuzzy sets, Hadoop, HDFS, CLPD},
location = {Barcelona, Spain},
series = {ICCBDC'18}
}

@inproceedings{10.1145/3055219.3055234,
author = {Sethi, Tavpritesh and Nagori, Aditya and Bhatnagar, Ambika and Gupta, Priyanka and Fletcher, Richard and Lodha, Rakesh},
title = {Validating the Tele-Diagnostic Potential of Affordable Thermography in a Big-Data Data-Enabled ICU},
year = {2017},
isbn = {9781450349307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055219.3055234},
doi = {10.1145/3055219.3055234},
abstract = {The potential for whole body thermal patterns in diagnosis of hemodynamic perfusion disturbances in critical care as well as community settings is unexplored. In this study we have combined an in-house digitized Big-data resource from ICU settings with Infra-red thermography to derive novel inferences about the tele-diagnostic potential of IR thermography in diagnosis of shock and perfusion disturbances. While Data-science and Big-data are expected to revolutionize the next generation medicine and healthcare, the scientific efforts towards building Big-data resources for enhancing patient safety and healthcare governance are missing, especially in developing countries. We addressed this challenge and describe our experience on deployment of Big-data warehousing and data-analytics software using lean pipelines developed using open-source technologies and their utility in deriving knowledge and high utility patterns from Affordable Infrared Thermography. These knowledge frameworks and potentially translatable technology were developed in the Pediatric Intensive Care environment through extensive cross-talk between expert clinicians and data-scientists. In this work, we first demonstrate the successful creation of a unique Pediatric ICU resource of over 60,000 hours of continuous multivariate monitoring data followed by validation of the potential of whole body IR thermography in diagnosis of hemodynamic compromise. These patterns were validated through linear mixed models, a state-of-the-art statistical method for longitudinal data. The validated technology is affordable, and can be coupled to smartphones thus providing a huge potential in tele-medicine and electronic governance in healthcare and has the potential to be deployed in a tele-medicine setting with capturing of whole body temperature patterns by Accredited Social Health Activist (ASHA) workers. Therefore, this can enable early diagnosis of critical conditions such as sepsis and shock that are commonly associated with epidemics such as Dengue hemorrhagic fever in developing countries such as India. These images can be remotely shared with expert physicians and data-analysts via telemedicine thus aiding decisions in the Critical Care as well as Community settings.},
booktitle = {Proceedings of the Special Collection on EGovernment Innovations in India},
pages = {64–69},
numpages = {6},
keywords = {Intensive Care Units, Child health, Open Source Technologies, Community Care, Telemedicine, Smartphones, Big-data Pipelines, Thermal Imaging, Clinical Decision Support, Affordable},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3400934.3400996,
author = {Amalia, Sarah Sholihatul and Sommeng, Andy Noorsaman},
title = {Process Safety Management (PSM) and Reliability for Compressor Inspection Using Big Data Analytics: A Conceptual Study},
year = {2020},
isbn = {9781450376006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400934.3400996},
doi = {10.1145/3400934.3400996},
abstract = {Process safety is related to leak prevention, oil spills, monitoring of equipment damage, overpressure, excess temperature, corrosion, metal fatigue, and other similar conditions. Besides, operations are related to productivity and risk management, so it is essential to monitor the process in depth. This paper is focusing on risk management of the downstream segment on the priority element of Process Safety Management (PSM). Based on research, Mechanical Integrity is the most critical element in PSM that have to be focused. The aspect of essential reliability of equipment, in this case, the compressor becomes vital to prevent shutdown/trip and unplanned maintenance, which will have an impact on oil and gas production. Historical failure data and support that include structured and unstructured data from the reciprocating compressor approximately from 2014 until 2019 will be collected. It will use to identify the damage patterns and reliability rates of the equipment. The regression value will be calculated by R as Big Data Analytics Software to determine whether Weibull distribution is sufficient. By using Weibull analysis, we can conclude that it will be more useful to use preventive maintenance as the first barrier from getting fail.},
booktitle = {Proceedings of the 3rd Asia Pacific Conference on Research in Industrial and Systems Engineering 2020},
pages = {339–343},
numpages = {5},
keywords = {Maintenance, Risk Management, Compressor, Reliability, R Software, Big Data},
location = {Depok, Indonesia},
series = {APCORISE 2020}
}

@inproceedings{10.5555/2984464.2984475,
author = {Rahman, Mohamed Abdur},
title = {Gesture-Based Cyber-Physical in-Home Therapy System in a Big Data Environment: Demo Abstract},
year = {2016},
publisher = {IEEE Press},
abstract = {This demo provides an overview of a gesture-based cyber-physical therapy system, which integrates entities in the physical as well as cyber world for therapy sensing, therapeutic data computation, interaction between cyber and physical world, and holistic in-home therapy support through a cloud-based big data architecture. To provide appropriate therapeutic services and environment, the CPS uses a multi-modal multimedia sensory framework to support therapy recording and playback of a therapy session and visualization of effectiveness of an assigned therapy. The physical world interaction with the cyber world is stored as a rich gesture semantics with the help of multiple media streams, which is then uploaded to a tightly synchronized cyber physical cloud environment for deducing real-time and historical whole-body Range of Motion (ROM) kinematic data.},
booktitle = {Proceedings of the 7th International Conference on Cyber-Physical Systems},
articleno = {11},
numpages = {1},
keywords = {gesture recognition, in-home therapy, therapy CPS, multimedia sensors},
location = {Vienna, Austria},
series = {ICCPS '16}
}

@inproceedings{10.1145/3180457.3180463,
author = {Gupta, Maanak and Patwa, Farhan and Sandhu, Ravi},
title = {An Attribute-Based Access Control Model for Secure Big Data Processing in Hadoop Ecosystem},
year = {2018},
isbn = {9781450356336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180457.3180463},
doi = {10.1145/3180457.3180463},
abstract = {Apache Hadoop is a predominant software framework for distributed compute and storage with capability to handle huge amounts of data, usually referred to as Big Data. This data collected from different enterprises and government agencies often includes private and sensitive information, which needs to be secured from unauthorized access. This paper proposes extensions to the current authorization capabilities offered by Hadoop core and other ecosystem projects, specifically Apache Ranger and Apache Sentry. We present a fine-grained attribute-based access control model, referred as HeABAC, catering to the security and privacy needs of multi-tenant Hadoop ecosystem. The paper reviews the current multi-layered access control model used primarily in Hadoop core (2.x), Apache Ranger (version 0.6) and Sentry (version 1.7.0), as well as a previously proposed RBAC extension (OT-RBAC). It then presents a formal attribute-based access control model for Hadoop ecosystem, including the novel concept of cross Hadoop services trust. It further highlights different trust scenarios, presents an implementation approach for HeABAC using Apache Ranger and, discusses the administration requirements of HeABAC operational model. Some comprehensive, real-world use cases are also discussed to reflect the application and enforcement of the proposed HeABAC model in Hadoop ecosystem.},
booktitle = {Proceedings of the Third ACM Workshop on Attribute-Based Access Control},
pages = {13–24},
numpages = {12},
keywords = {access control, hadoop ecosystem, role based, authorization, data lake, attributes based, trust, big data},
location = {Tempe, AZ, USA},
series = {ABAC'18}
}

@inproceedings{10.1145/2808797.2808883,
author = {Lee, Chung-Hong and Yang, Hsin-Chang and Lin, Shih-Jan},
title = {Incorporating Big Data and Social Sensors in a Novel Early Warning System of Dengue Outbreaks},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2808883},
doi = {10.1145/2808797.2808883},
abstract = {In this work, an "analytical data model of mosquito vector" was developed to perform analytical computation to the character of the dengue vectors. Our goal is to investigate a way to understand how the temporal trend of collected dataset correlates with the incidence dengue as identified by national health authorities. Based upon the mosquito-vector big data collections, we investigate how changes in some specific variables such as rainfall, temperature, and humidity can dramatically affect the population of mosquito vectors, in order to provide early warnings of dengue outbreaks. Thus, our system will collectively collect online sensing data of the variables and store them in a database, in order to combine the historical big data as training datasets for analytical computation. Also, the developed model is able to merge the experimental datasets with current hot-topic information which is relevant to mosquito vectors obtained from data of social sensors (i.e. social messages). The experimental data show that our system is of great potentials in providing early warnings of dengue outbreaks.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {1428–1433},
numpages = {6},
keywords = {Big Data, Social Sensors, Machine Learning, Dengue Outbreaks, Data Mining},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/2676723.2691948,
author = {MacHardy, Zachary and Garcia, Daniel D.},
title = {Using Big Data and BKT to Evaluate Course Resources (Abstract Only)},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2691948},
doi = {10.1145/2676723.2691948},
abstract = {Now finding footing in objective research methodology, MOOCs have made significant strides toward developing into mature platforms for not only offering educational materials but also performing exploratory analysis of educational methods. Bayesian Knowledge Tracing (BKT) has been repeatedly shown to be successful at providing an accurate model of student knowledge in more traditional classroom settings, and recent research has explored the application of BKT to MOOCs with promising results. Using data from several MOOCs run by Stanford university, we propose to extend earlier research into the application of BKT to MOOCS by developing a framework within which the use of course resources and student performance can be leveraged both to increase the predictive accuracy of BKT modeling and to provide an evaluative metric for the utility of those resources. We hope that such a framework can contribute not only to MOOC courses, but traditional classrooms as well.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {683},
numpages = {1},
keywords = {course resources, moocs, bkt, prediction, student modeling, machine learning},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1145/3322134.3322152,
author = {Gu, Yang},
title = {A Probe into the Application of Big Data to Innovating the Education and Management of College Students},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322152},
doi = {10.1145/3322134.3322152},
abstract = {The Application of Big Data brings considerable benefits to the resource integration of the management of college students, a boost in technical advantages of the education and management of college students and an improvement in time efficiency of the education and management of college students. This paper, based on the analysis of the application of existing big data, discusses the merits of the application of big data to the education and management of college students from fostering a sense of big data, diversifying working methods to innovating education and management mechanism.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {113–117},
numpages = {5},
keywords = {college, education and management, Big data, innovation},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@inproceedings{10.1145/3289402.3289536,
author = {Amazal, Houda and Ramdani, Mohammed and Kissi, Mohamed},
title = {A Text Classification Approach Using Parallel Naive Bayes in Big Data Context},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289536},
doi = {10.1145/3289402.3289536},
abstract = {Text classification is a domain that has been inspiring researchers since many years. Indeed, several approaches have been developed in order to find methods that improve the performance of text classification. But in last decades, because of the technological evolution, textual data becomes more and more abundant on the web. So that classical classification methods are unable to process this huge amount of data and consequently cannot produce satisfied results. Thus, new ways have been explored; to overcome the big dimensions of data, it was necessary to reduce the size of the features of documents and use parallel processing. For this, in our work, we developed a Term Frequency- Inverse Document Frequency (TF-IDF) parallel model to save only the most relevant words in documents. Then, we feed the dataset to a parallel Naive Bayes classifier. Both, the TF-IDF parallel model and parallel Na\"{\i}ve Bayes classifier were implemented on Hadoop system using the MapReduce architecture. The experimental results demonstrate the efficiency of the proposed method to improve the classification accuracy.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {36},
numpages = {6},
keywords = {Machine Learning, MapReduce, Na\"{\i}ve Bayes, TF-IDF, Text classification, Big Data},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1145/3164541.3164560,
author = {Nakashima, Kenji and Kon, Joichiro and Yamaguchi, Saneyasu},
title = {I/O Performance Improvement of Secure Big Data Analyses with Application Support on SSD Cache},
year = {2018},
isbn = {9781450363853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3164541.3164560},
doi = {10.1145/3164541.3164560},
abstract = {For utilizing private big data, such as DNA data, encryption and anonymization are essential for preserving privacy. However, encryption and anonymization sometimes increase the size of data largely. Thus, increasing I/O performance for large-scale data is important. Caching data with Solid State Drive (SSD) is a popular method for by using SSD as cache, which is a proposed method for improving access performance of data in Hard Disk Drive (HDD). In this paper, we focus on SSD cache and discuss a method for improving I/O performance of a big data application using SSD cache. First, we evaluate the basic I/O performance with and without SSD cache. Second, we reveal the behavior of flash cache. Third, we propose a method for improving I/O performance of a large scale DNA application with SSD cache. Fourth, we evaluate the proposed method and demonstrate that the method can improve I/O performance effectively.},
booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
articleno = {90},
numpages = {7},
keywords = {anonymized analysis, SSD cache, Big data},
location = {Langkawi, Malaysia},
series = {IMCOM '18}
}

@article{10.1145/2669368,
author = {Berndt, Donald J. and McCart, James A. and Finch, Dezon K. and Luther, Stephen L.},
title = {A Case Study of Data Quality in Text Mining Clinical Progress Notes},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2669368},
doi = {10.1145/2669368},
abstract = {Text analytic methods are often aimed at extracting useful information from the vast array of unstructured, free format text documents that are created by almost all organizational processes. The success of any text mining application rests on the quality of the underlying data being analyzed, including both predictive features and outcome labels. In this case study, some focused experiments regarding data quality are used to assess the robustness of Statistical Text Mining (STM) algorithms when applied to clinical progress notes. In particular, the experiments consider the impacts of task complexity (by removing signals), training set size, and target outcome quality. While this research is conducted using a dataset drawn from the medical domain, the data quality issues explored are of more general interest.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {apr},
articleno = {1},
numpages = {21},
keywords = {data quality, Machine learning, predictive model quality, noisy text analysis, health informatics, text mining, electronic health records, feature selection, clinical progress notes}
}

@inproceedings{10.1145/3341620.3341636,
author = {Tan, Haowen and Chung, Ilyong},
title = {A Secure Cloud-Assisted Certificateless Group Authentication Scheme for VANETs in Big Data Environment},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341636},
doi = {10.1145/3341620.3341636},
abstract = {Nowadays, the construction of efficient intelligent transportation system (ITS) has become a new trend for metropolitan cities with increasingly large populations. As one of the most significant component of ITS, the vehicular ad hoc networks (VANETs) are capable of building temporary vehicular sensor networks for efficient and dynamic information exchange between vehicles and road side units (RSUs). As a matter of fact, the traditional VANETs have limited computing and storing capabilities, which restrict the rapid development VANETs services provided to the drivers. Hence, with the rapid development of big data facilities, the cloud-assisted VANETs structure is proposed in order to enhance the capabilities of VANETs. In addition, due to the inherent wireless communication characteristics, data transmissions of VANETs suffer from charted and uncharted security risks and attacks. Thus proper security strategies should be adopted to guarantee secure communication and driver privacy. Emphasizing on the above issues, we develop an efficient cloud-assisted certificateless grouping authentication scheme for VANETs. In our design, vehicle anonymity is provided during the entire communication process. Note that most of the current authenticating schemes assume the secure channel between the RSU and vehicles in order for initial key message transmission, which is not necessary in our scheme.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {107–113},
numpages = {7},
keywords = {certificateless, VANETs, conditional privacy, anonymous identity, group authentication},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3149572.3149598,
author = {Kim, Tae-Hak and Kim, Seong-Jin and Ok, Hyun},
title = {A Study on the Cargo Vehicle Traffic Patterns Analysis Using Big Data},
year = {2017},
isbn = {9781450353373},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149572.3149598},
doi = {10.1145/3149572.3149598},
abstract = {Overloading is a major factor in the damage to road facilities such as bridges and traffic accidents, and the Ministry of Land, Transport and Maritime Affairs is in the process of fixing and moving the main points to overhaul. Moving control is more effective than stationary control, but it is possible to avoid interrupting the driver because the existing control pattern is well known along with the location of intruder by intuition. Therefore, in this study, we analyzed data such as investigation of oversight activities, existing enforcement information, and traffic volume information as an improvement measure to preemptively block intermittent vehicles. The analysis methods were analyzed to derive future delivery methods, along with analysis techniques such as logistic regression analysis and analysis methods, and methods for selecting the best possible location for selecting the routes.},
booktitle = {Proceedings of the 9th International Conference on Information Management and Engineering},
pages = {55–59},
numpages = {5},
keywords = {Patterns, Overload, DTG, Cargo Vehicle Traffic, MOLIT, Fines imposition system, Traffic volume information system, Big-Data},
location = {Barcelona, Spain},
series = {ICIME 2017}
}

@inproceedings{10.1145/3209914.3234637,
author = {Hua, Zhao},
title = {A Study on the Management Model of Smart Tourism Industry under the Era of Big Data},
year = {2018},
isbn = {9781450364218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209914.3234637},
doi = {10.1145/3209914.3234637},
abstract = {With the rapid development of Internet and the communication technology, the construction of smart tourism is no longer a slogan that can not be realized. The construction of smart tourism in tourist destinations conforms to the strategic goal of tourism industry development in China. Based on the background of big data, this paper elaborated the connotation of big data and smart tourism, and built a large data platform to realize the forecast and feedback of smart tourism through the analysis of tourism development. The platform could be divided into government tourism platform, tourists platform, tourism enterprises platform and community residents platform relying on big data do their own duty. Eventually this paper put forward a construction model and path to realize the smart tourism platform.},
booktitle = {Proceedings of the 2018 International Conference on Information Science and System},
pages = {102–106},
numpages = {5},
keywords = {smart tourism, Big data, management model},
location = {Jeju, Republic of Korea},
series = {ICISS '18}
}

@inproceedings{10.1145/2379436.2379437,
author = {Regola, Nathan and Cieslak, David A. and Chawla, Nitesh V.},
title = {The Constraints of Magnetic versus Flash Disk Capabilities in Big Data Analysis},
year = {2012},
isbn = {9781450314442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2379436.2379437},
doi = {10.1145/2379436.2379437},
abstract = {Solid state disks (or flash disks) are decreasing in cost per gigabyte and are being incorporated into many appliances, such as the Oracle Database Appliance [8]. Databases--and more specifically data warehouses--are often utilized to support large scale data analysis and decision support systems. Decision makers prefer information in real time. Traditional storage systems that are based on magnetic disks achieve high performance by utilizing many disks for parallel operations in RAID arrays. However, this performance is only possible if requests represent a reasonable fraction of the RAID stripe size, or I/O transactions will suffer from high overhead. Solid state disks have the potential to increase the speed of data retrieval for mission critical workloads that require real time applications, such as analytic dashboards. However, solid state disks behave differently than magnetic hard disks due to the limitations of rewriting NAND flash based blocks. Therefore, this work presents benchmark results for a modern relational database that stores data on solid state disks, and contrasts this performance to a ten disk RAID 10 array, a traditional storage design for high performance database data blocks. The preliminary results show that a single solid state disk is able to outperform the array for queries summarizing a data set for a variety of OLAP cube dimensions. Future work will explore the low level database performance in more detail.},
booktitle = {Proceedings of the 2nd Workshop on Architectures and Systems for Big Data},
pages = {4–9},
numpages = {6},
keywords = {solid state disk, query performance},
location = {Portland, Oregon, USA},
series = {ASBD '12}
}

