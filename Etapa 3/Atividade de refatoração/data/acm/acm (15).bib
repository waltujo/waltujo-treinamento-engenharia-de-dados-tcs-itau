@inproceedings{10.1145/3297280.3297386,
author = {Ianni, Michele and Masciari, Elio and Mazzeo, Giuseppe M. and Zaniolo, Carlo},
title = {How to Implement a Big Data Clustering Algorithm: A Brief Report on Lesson Learned},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297386},
doi = {10.1145/3297280.3297386},
abstract = {The current era of Big Data [7] has forced both researchers and industries to rethink the computational solutions for analyzing massive data. In fact, a great deal of attention has been devoted to the design of new algorithms for analyzing information available from Twitter, Google, Facebook, and Wikipedia, just to cite a few of the main big data producers. Although this massive volume of data can be quite useful for people and companies, it makes analytical and retrieval operations really time consuming due to their high computational cost. A possible solution relies upon the possibility to cluster big data in a compact but still informative version of the entire data set. Obviously, such clustering techniques should produce clusters (or summaries) having high accuracy. Clustering algorithms could be beneficial in several application scenarios such as cybersecurity, user profiling and recommendation systems, to cite a few.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1073–1080},
numpages = {8},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3330482.3330510,
author = {Gu, Yuanhu and Malicdem, Alvin R. and Cruz, Josephine S. Dela and Palaoag, Thelma Domingo},
title = {Using Big Data Analysis to Retain Customers for Telecom Industry},
year = {2019},
isbn = {9781450361064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330482.3330510},
doi = {10.1145/3330482.3330510},
abstract = {Nowadays, telecommunication markets are becoming more and more competitive, and customer churn is becoming more and more serious. In the tough competitive mobile market, Customer Churn Management is becoming more and more critical. In developing countries, most customers switch service providers because of good promotional incentives and lower monthly costs offered by competitive service providers. How to predict customer churn quickly and accurately becomes very important. In this paper, the researchers successfully analyzed the customer churn using big data feature analysis and multi-feature analysis. User data were modeled by XGBoost algorithm. The model is optimized repeatedly with GridSearchCV as a parameter tool. The accuracy of the model on the test set is 85.1%. The researchers predicted about 11000 customer lists per month that may be about to churn. Using K-means clustering method, 11000 churn target customers per month were classified into three categories and telecom companies are suggested to take some solutions which are found by feature analysis to retain customers. This big data analysis can be used to retain customers for the telecom industry.},
booktitle = {Proceedings of the 2019 5th International Conference on Computing and Artificial Intelligence},
pages = {38–43},
numpages = {6},
keywords = {telecom industry, customer churn, feature analysis, Big data analysis, retain customers},
location = {Bali, Indonesia},
series = {ICCAI '19}
}

@inproceedings{10.1145/3322134.3323932,
author = {Zou, Xiaohui and Zou, Shunpeng and Wang, Xiaoqun},
title = {New Approach of Big Data and Education: Any Term Must Be in the Characters Chessboard as a Super Matrix},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3323932},
doi = {10.1145/3322134.3323932},
abstract = {The purpose of this paper is to introduce a new approach that must cover all the terms. Therefore, people's educational process is like making a variety of choices in a super-chessboard of language or a matrix composed of words formally. The method of redemption is: First, construct the chessboard, and then, through human-computer interaction and collaboration, generate massive amounts of big data, including various terms representing knowledge, and finally, through machine learning and man-machine interactive to analyze, compare, and query or reuse any of these terms. The result: an accurate query of terms, which can be automatically queried in multiple ways through bilingual or multi-lingual converters. The significance is that the method and its results can be used not only for machine-assisted instruction in the network environment, but also for machine-assisted intelligent text analysis and knowledge module finishing in the network environment, thus opening up view of big data and education. The new approach, because any term must be in the word matrix, each user and its agents query them very accurately and efficiently.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {129–134},
numpages = {6},
keywords = {Data Management, E-education, Big Data Applications},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@inproceedings{10.1145/3006299.3006334,
author = {Shreenath, Vinutha Magal and Meijer, Sebastiaan},
title = {Spatial Big Data for Designing Large Scale Infrastructure: A Case-Study of Electrical Road Systems},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006334},
doi = {10.1145/3006299.3006334},
abstract = {Decision making and planning of large scale infrastructures within cities is often a long process encompassing years, between multiple institutions represented by experts that require negotiations and consensus of demands and goals. The role big data plays in such design could be crucial, by providing access to otherwise elusive information on movements of people and goods in a city which can then transparently inform the design process, especially about possible demands and related complexities on the infrastructure being planned. To harness this data, it is necessary to formulate the problem technically such that data can inform experts, by articulating their expertise through the data. In this paper we present an application to analyze millions of instances of spatial data to identify potential locations for electrical road installation(s) in a city, to aid urban planners and other relevant stakeholders in planning and designing an Electrical Road System for a city. The dataset being used is gathered from a major vehicle manufacturer in Sweden, containing millions of instances of GPS data emitted by thousands of vehicles. A plan for electrified transport system is formulated by retrieving locations suitable for both static and dynamic charging installations. We investigate the technical formulation of methods and metrics for such a complex design problem, based on criteria set by experts, thus contributing to the science of big data for design of infrastructure and to methodology of data science in an institutional context.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {143–148},
numpages = {6},
keywords = {decision making, spatio-temporal data, infrastructure design, big data, urban planning},
location = {Shanghai, China},
series = {BDCAT '16}
}

@inproceedings{10.1145/3286606.3286841,
author = {Elyusufi, Z. and Elyusufi, Y. and Aitkbir, M.},
title = {Customer Profiling Using CEP Architecture in a Big Data Context},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286841},
doi = {10.1145/3286606.3286841},
abstract = {Today Big Data tools are not just a phenomenon of the massive information collection; they are also the best way to approach a customer target. These technologies allow the profiling of the customers of an organization thanks to the histories of purchases, the products that they consult; the data that they share through the social networks. They also make it possible to anticipate the purchase of actions via behavioral analysis. Therefore, the combination of the power of CRM and the performance of BIG DATA tools brings a great added value for customers profile analysis, especially if it is about events triggered in real time. It is in this context that the present work is positioned. Our goal is to intercept events (customer behaviors) and analyze them in real time. We will use the Complex Events Process (CEP) architecture that perfectly meets this need. In order to successfully implement our CEP architecture, we will use the ontology approach.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {64},
numpages = {6},
keywords = {CEP, Big Data, Ontology, CRM, Profiling},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inbook{10.5555/3042094.3042329,
author = {Volovoi, Vitali},
title = {Simulation of Maintenance Processes in the Big Data Era},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {Maintenance processes of repairable systems have been extensively studied in the past. The resulting simple solutions have proven to be remarkably effective. It requires complex and time-consuming simulations to improve on those simple solutions, and reliable input data is even harder to get. However, new technologies, epitomized by Big Data and the Internet of Things, change the data-availability part of the equation. As a result, there are new exciting possibilities for modeling more subtle effects, and developing processes for easily (and therefore frequently) updated inputs. Modeling decisions can be repeatedly tested on the data, and the models can be quickly adjusted to better reflect reality and even to compensate for missing pieces of the data. In this context, the transparency and simplicity of models becomes a larger virtue. Several examples of the insights based on real-world large-scale applications of predictive analytics using simulation are discussed.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {1872–1883},
numpages = {12}
}

@inproceedings{10.1145/2910896.2925435,
author = {Wang, Wei and Liu, Jiaying and Yu, Shuo and Zhang, Chenxin and Xu, Zhenzhen and Xia, Feng},
title = {Mining Advisor-Advisee Relationships in Scholarly Big Data: A Deep Learning Approach},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2925435},
doi = {10.1145/2910896.2925435},
abstract = {Mining advisor-advisee relationships can benefit many interesting applications such as advisor recommendation and protege performance analysis. Based on the hypothesis that, advisor-advisee relationships among researchers are hidden in scholarly big data, we propose in this work a deep learning based advisor-advisee relationship identification method which considers the personal properties and network characteristics with a stacked autoencoder model. To the best of our knowledge, this is the first time that a deep learning model is utilized to represent coauthor network features for relationships identification. Moreover, experiments demonstrate that the proposed method has better performance compared with other state-of-the-art methods.},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {209–210},
numpages = {2},
keywords = {stacked autoencoders, relationship mining, deep learning},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.5555/2888619.2888701,
author = {Hofmann, Marko A.},
title = {Searching for Effects in Big Data: Why p-Values Are Not Advised and What to Use Instead},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {p-values of null hypothesis significance testing have long been the standard and decisive measure of deductive statistics. However, for decades, top statistical methodologists have argued that focusing on p-values is not conducive to science, and that these tests are regularly misunderstood. The standard replacement or at least complement proposed for p-values by those critics are confidence intervals and statistical effects sizes. Regrettably, analyzing and comparing huge data sets (from data mining or simulation based data farming) with two measures is awkward. As a single-value measure of first interpretation for the scanning of Big Data this article proposes statistically secured effect sizes either based on exact, mathematically sophisticated confidence intervals for effect sizes or simplified approximations. It is further argued that simplified secured effect sizes are among the most instructive single measures of statistical interpretation completely perspicuous for the layman.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {725–736},
numpages = {12},
location = {Huntington Beach, California},
series = {WSC '15}
}

@inproceedings{10.1145/3147234.3151010,
author = {Gong, Yikai and Rimba, Paul and Sinnott, Richard},
title = {A Big Data Architecture for Near Real-Time Traffic Analytics},
year = {2017},
isbn = {9781450351959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147234.3151010},
doi = {10.1145/3147234.3151010},
abstract = {Big data is a popular research topic that has brought about a range of new IT challenges and opportunities. The transport domain is one area that has much to benefit from big data platforms. It requires capabilities for processing voluminous amounts of heterogeneous data that is often created in near real time and at high velocity from a multitude of distributed sensors. It can also require the application of performance-oriented spatial data processing of such data. In this paper, we present a platform (SMASH) that tackles many of the specific challenges raised by the transport domain. We present a range of case studies applying SMASH to transport and other data used to understand traffic phenomenon across the State of Victoria, Australia. The novelty of this work is that this Cloud-based platform is not designed for a specific type of data or for a specific form of data processing. Rather it supports a range of data flavours with a range of data processing possibilities. In particular we show how the platform can be used for analyzing social media data used for traffic jam identification through spatial and temporal clustering tweets on the road network and compare the results with official real-time traffic data based on the Sydney Coordinated Adaptive Traffic System (SCATS - www.scats.com.au) that has been rolled out across Victoria.},
booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {157–162},
numpages = {6},
keywords = {big data, cloud, traffic analysis},
location = {Austin, Texas, USA},
series = {UCC '17 Companion}
}

@article{10.1145/2788402.2788406,
author = {Ros\`{a}, Andrea and Chen, Lydia Y. and Birke, Robert and Binder, Walter},
title = {Demystifying Casualties of Evictions in Big Data Priority Scheduling},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2788402.2788406},
doi = {10.1145/2788402.2788406},
abstract = {The ever increasing size and complexity of large-scale datacenters enhance the difficulty of developing efficient scheduling policies for big data systems, where priority scheduling is often employed to guarantee the allocation of system resources to high priority tasks, at the cost of task preemption and resulting resource waste. A large number of related studies focuses on understanding workloads and their performance impact on such systems; nevertheless, existing works pay little attention on evicted tasks, their characteristics, and the resulting impairment on the system performance. In this paper, we base our analysis on Google cluster traces, where tasks can experience three diffierent types of unsuccessful events, namely eviction, kill and fail. We particularly focus on eviction events, i.e., preemption of task execution due to higher priority tasks, and rigorously quantify their performance drawbacks, in terms of wasted machine time and resources, with particular focus on priority. Motivated by the high dependency of eviction on underlying scheduling policies, we also study its statistical patterns and its dependency on other types of unsuccessful events. Moreover, by considering co-executed tasks and system load, we deepen the knowledge on priority scheduling, showing how priority and machine utilization affect the eviction process and related tasks.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {12–21},
numpages = {10}
}

@inproceedings{10.5555/2693848.2693981,
author = {He, Miao and Ji, Hao and Wang, Qinhua and Ren, Changrui and Lougee, Robin},
title = {Big Data Fueled Process Management of Supply Risks: Sensing, Prediction, Evaluation and Mitigation},
year = {2014},
publisher = {IEEE Press},
abstract = {Supplier risks jeopardize on-time or complete delivery of supply in a supply chain. Traditionally, a company can merely do an ex-post evaluation of a supplier's performance, and handles emergencies in a reactive rather than a proactive way. We propose an agile process management framework to monitor and manage supply risks. The innovation is two fold - Firstly, a business process is established to make sure that the right data, the right insights, and the right decision-makers are in place at the right time. Secondly, we install a big data analytics component, a simulation component and an optimization component into the business process. The big data analytics component senses and predicts supply disruptions with internally (operational) and external (environmental) data. The simulation component supports risk evaluation to convert predicted risk severity to key performance indices (KPIs) such as cost and stockout percentage. The optimization component assists the risk-hedging decision-making.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {1005–1013},
numpages = {9},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/2588555.2610498,
author = {Zoumpatianos, Kostas and Idreos, Stratos and Palpanas, Themis},
title = {Indexing for Interactive Exploration of Big Data Series},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2610498},
doi = {10.1145/2588555.2610498},
abstract = {Numerous applications continuously produce big amounts of data series, and in several time critical scenarios analysts need to be able to query these data as soon as they become available, which is not currently possible with the state-of-the-art indexing methods and for very large data series collections. In this paper, we present the first adaptive indexing mechanism, specifically tailored to solve the problem of indexing and querying very large data series collections. The main idea is that instead of building the complete index over the complete data set up-front and querying only later, we interactively and adaptively build parts of the index, only for the parts of the data on which the users pose queries. The net effect is that instead of waiting for extended periods of time for the index creation, users can immediately start exploring the data series. We present a detailed design and evaluation of adaptive data series indexing over both synthetic data and real-world workloads. The results show that our approach can gracefully handle large data series collections, while drastically reducing the data to query delay: by the time state-of-the-art indexing techniques finish indexing 1 billion data series (and before answering even a single query), adaptive data series indexing has already answered $3*10^5$ queries.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {1555–1566},
numpages = {12},
keywords = {adaptive indexing, data-series, similarity search, nearest neighbor, adaptive data-series index},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/3319535.3363267,
author = {Verma, Rakesh M. and Zeng, Victor and Faridi, Houtan},
title = {Data Quality for Security Challenges: Case Studies of Phishing, Malware and Intrusion Detection Datasets},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363267},
doi = {10.1145/3319535.3363267},
abstract = {Techniques from data science are increasingly being applied by researchers to security challenges. However, challenges unique to the security domain necessitate painstaking care for the models to be valid and robust. In this paper, we explain key dimensions of data quality relevant for security, illustrate them with several popular datasets for phishing, intrusion detection and malware, indicate operational methods for assuring data quality and seek to inspire the audience to generate high quality datasets for security challenges.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2605–2607},
numpages = {3},
keywords = {data poisoning, data difficulty, semiotics, data quality},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.1145/3194206.3194229,
author = {Zhichao, Xu and Jiandong, Zhao and Huan, Huang},
title = {Based on Hadoop's Tech Big Data Combination and Mining Technology Framework},
year = {2018},
isbn = {9781450363457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194206.3194229},
doi = {10.1145/3194206.3194229},
abstract = {With the advent of the Internet + era in the field of Tech big data, the big data of Tech big data has a large amount of data and various characteristics. It is an important means to carry out research on the big data of Tech big data to realize the combination and mining of efficient multi-source foreign technology data. However, at present, the big data of Tech big data are divided into disciplines and different formats, which are difficult to realize the intersection of effective scientific and technological information and realize data sharing. This paper puts forward a kind of big data combined with Tech big data and mining technology based on the Hadoop framework.It includes a unified collection and preprocessing method of big data of Tech big data and the design of storage and management platform for data sources. It is based on Map/Reduce Tech big data parallelization computing model and system.Its correlation with important scientific data mining services.The framework has good practicability and expansibility.},
booktitle = {Proceedings of the 2nd International Conference on Innovation in Artificial Intelligence},
pages = {59–63},
numpages = {5},
keywords = {Hadoop, combination, mining, tech big data},
location = {Shanghai, China},
series = {ICIAI '18}
}

@proceedings{10.1145/3358528,
title = {ICBDT2019: Proceedings of the 2nd International Conference on Big Data Technologies},
year = {2019},
isbn = {9781450371926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Due to the explosive evolution of Information Technology and Computer Science, we have entered in the Big Data Age, and this is really a scientific revolution. As always, the technological aspects evolve faster than the scientific community mentality. Transforming Big Data into Big Knowledge and developing a new kind of Knowledge-Based Systems require new visions and approaches. ICBDT 2019 is annual conferences for researchers, academicians and industry persons in Big Data Technologies. This platform provides opportunities for the delegates to exchange new ideas and application experiences face to face, to establish business or research relations and to find global cooperation.},
location = {Jinan, China}
}

@inproceedings{10.5555/2888619.2888976,
author = {Taylor, Siman J. E.},
title = {The Impact of Big Data on M&amp;S: Do We Need to Get "Big"?},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {Driven by innovations such as mass customisation, complex supply chains, smart cities and emerging cyber-physical and Internet of Things systems, Big Data is presenting a fascinating range of challenges to Analytics. New fields are emerging such as Big Data Analytics and Data Science. Modeling &amp; Simulation (M&amp;S) is core to Analytics. Arguably, contemporary M&amp;S practices cannot deal with the demands of Big Data. The implication of this is that M&amp;S may not feature in the Big Data Analytics techniques and tools of the future. Based on recent experiences from the i4MS FP7 European Cloud-based Simulation platform for Manufacturing and Engineering (CloudSME) and associated industrial projects, this talk will outline the key challenges that Big Data has to M&amp;S and strongly argue that M&amp;S has to get "Big" to meet these challenges. Exciting opportunities lie ahead for multi-disciplinary teams of practitioners and researchers from OR/MS, Computer Science and domain specific fields. Indeed "Big" Simulation presents its own possibilities and the talk will conclude with thoughts on the potential for "Big" Simulation Analytics to move beyond Big Data into future Dynamic Data Driven Application Systems.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {3085},
numpages = {1},
location = {Huntington Beach, California},
series = {WSC '15}
}

@inproceedings{10.5555/3375069.3375113,
author = {Jain, Shashwat and Khandelwal, Manish and Katkar, Ashutosh and Nygate, Joseph},
title = {Applying Big Data Technologies to Manage QoS in an SDN},
year = {2016},
isbn = {9783901882852},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {Managing QoS in a telecommunications network is a complex process. Effective network design and sizing in conjunction with load balancing, access control and traffic prioritization need to be orchestrated to optimize CAPEX investment, maximize network utilization and ensure that performance metrics and SLAs are met. This work shows how big data analytics were used to improve the management of QoS in an SDN by performing multi-dimensional analysis of Key Performance Indicators (KPIs) and applying machine learning algorithms to discover new correlations, perform root cause analysis and predict traffic congestion.},
booktitle = {Proceedings of the 12th Conference on International Conference on Network and Service Management},
pages = {302–306},
numpages = {5},
keywords = {Performance Management, Big Data, data movement, QoS, tSDN},
location = {Montreal, Quebec, Canada},
series = {CNSM 2016}
}

@inproceedings{10.1145/3206157.3206181,
author = {Pirouz, Matin and Parsa, Sai Phani and Zhan, Justin},
title = {Optimized Rank Estimator in Big Data Social Networks},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206181},
doi = {10.1145/3206157.3206181},
abstract = {In this study, FAST Personalized PageRank is utilized to find the target node set. Using the mentioned target set, the algorithm gives an estimation of the closeness of any pair of nodes in the graph. Personalized Page Vector is used to find the most popular nodes, also known as hubs, in the network. The time taken by the estimation of Personalized PageRank is directly proportional to the network size. In this work, we proposed a node reduction method to prune the graph. To decrease the entropy and reduce the number of alternate paths to the target nodes, redundant popular nodes are identified and flagged. The flagged nodes are, then, given a lower priority in the computation. After pruning the graph, estimation results achieve an improved time complexity. The proposed method achieves a twice shorter computation time as compared to FAST PPR and Local Update.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {80–84},
numpages = {5},
keywords = {Personalized PageRank, Graph Theory, Hub Nodes},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@article{10.1145/2647748,
author = {Efros, Alexei A.},
title = {Portraiture in the Age of Big Data: Technical Perspective},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/2647748},
doi = {10.1145/2647748},
journal = {Commun. ACM},
month = {sep},
pages = {92},
numpages = {1}
}

@inproceedings{10.1145/2897010.2897014,
author = {Fredericks, Erik M. and Hariri, Reihaneh H.},
title = {Extending Search-Based Software Testing Techniques to Big Data Applications},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897014},
doi = {10.1145/2897010.2897014},
abstract = {Massive datasets are quickly becoming a concern for many industries. For example, many web-based applications must be able to handle petabytes worth of transactions on a daily basis, and moreover, be able to quickly and efficiently act upon data that exists in each transaction. As a result, providing testing capabilities for such applications becomes a challenge of scale. We argue that existing approaches, such as automated test suite generation, may not necessarily scale without assistance. To this end, we discuss open issues and possible solutions specific to testing big data applications.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {41–42},
numpages = {2},
keywords = {test suite generation, search-based software testing, big data},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.1145/3383923.3383964,
author = {Xu, Qingzheng and Wang, Na and Tian, Balin and Xing, Lipeng and Bai, Wenhua},
title = {Challenges and Countermeasures of Education in the Era of Big Data},
year = {2020},
isbn = {9781450375085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383923.3383964},
doi = {10.1145/3383923.3383964},
abstract = {The advent of the era of big data provides some new ideas for the reform and development of higher education. In this paper, the importance of educational big data is analyzed from six aspects: improving the effectiveness of teaching and learning, promoting scientific decision-making in education, completing the quality monitoring system, facilitating comprehensive evaluation of education quality, promoting the popularization and personalization of education, and improving personalized teaching. Then, some challenges faced in the application process of education big data are analyzed, including thinking mode, data sharing, data technology, talent support, and data security and privacy. Based on them, several feasible countermeasures are proposed from the aspects of enhancing data awareness, realizing open sharing, accelerating professional talent training and strengthening privacy protection. At last, some research directions of educational big data are put forward.},
booktitle = {Proceedings of the 2020 9th International Conference on Educational and Information Technology},
pages = {215–218},
numpages = {4},
keywords = {higher education, challenges, countermeasures, Big data},
location = {Oxford, United Kingdom},
series = {ICEIT 2020}
}

@inproceedings{10.1145/3220199.3220220,
author = {Hong, Huang and Khan, Latifur and Xiaojuan, Liao},
title = {SAT-Based Important Data Reliability Enhancement Model for Big Data Storage},
year = {2018},
isbn = {9781450364263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220199.3220220},
doi = {10.1145/3220199.3220220},
abstract = {Disk reliability is a serious problem in the big data foundation environment. Although the reliability of disk drives has greatly improved over the past few years, they are still the most vulnerable core components in the server. If they fail, the result can be catastrophic: it can take some days to recover data, sometimes data lost forever. These are unacceptable for some important data. XOR parity is a typical method to generate reliability syndrome, thus improving the reliability of the data. In practice, we find that the data is still likely to be lost. In most storage systems reliability improvements are achieved through the allocation of additional disks in Redundant Arrays of Independent Disks (RAID), which will increase the hardware costs, thus it will be very difficult for cost-constrained environments. Therefore, how to improve the data integrity without raising the hardware cost has aroused much interest of big data researchers. This challenge is when creating non-traditional RAID geometries, care must be taken to respect data dependence relationships to ensure that the new RAID strategy improves reliability, which is a NP-hard problem. In this paper, we present an approach for characterizing these challenges using high-dimension variants of the n-queens problem that enables performable solutions via the SAT solver MiniSAT, and use the greedy algorithm to analyze the queen's attack domain, as a basis for reliability syndrome generation. A large number of experiments show that the approach proposed in this paper is feasible in software-defined data centers and the performance of the algorithm can meet the current requirements of the big data environment.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Computing},
pages = {20–26},
numpages = {7},
keywords = {big data, NP-hard, Boolean Satisfiability Problem, n-queens, data reliability},
location = {Shenzhen, China},
series = {ICBDC '18}
}

@inproceedings{10.1145/2693182.2693185,
author = {Yesudas, Michael and S, Girish Menon and Nair, Satheesh K},
title = {High-Volume Performance Test Framework Using Big Data},
year = {2015},
isbn = {9781450333375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2693182.2693185},
doi = {10.1145/2693182.2693185},
abstract = {The inherent issues with handling large files and complex scenarios cause the data-driven approach [1] to be rarely used for performance tests. Volume and scalability testing of enterprise solutions typically requires custom-made test frameworks because of the complexity and uniqueness of data flow. The generation, transformation and transmission of large sets of data pose a unique challenge for testing a highly transactional back-end system like the IBM Sterling Order Management (OMS). This paper describes a test framework built on document-oriented NoSQL database, a design that helps validate the functionality and scalability of the solution simultaneously. This paper also describes various phases of planning, development, and testing of the OMS solution that was executed for a large retailer in Europe to test an extremely high online sales scenario. An out-of-the-box configuration of the OMS with the feature support for database sharding was used to drive scalability. The exercise was a success, and it is the world's largest IBM Sterling Order Management benchmark in terms of sales order volume, to date.},
booktitle = {Proceedings of the 4th International Workshop on Large-Scale Testing},
pages = {13–16},
numpages = {4},
keywords = {document oriented storage, test harness, rapid prototyping, load testing, test automation tool, order management, big data},
location = {Austin, Texas, USA},
series = {LT '15}
}

@inproceedings{10.1145/3405962.3405970,
author = {Yamamoto, Yukio and Ishikawa, Hiroshi},
title = {Data Management in Japanese Planetary Explorations for Big Data Era},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405970},
doi = {10.1145/3405962.3405970},
abstract = {The data obtained by planetary explorations has various aspects such as decision making during an ongoing mission, anomaly detection for spacecraft safety, data archives for scientific analysis, and attractive snapshots for outreach. Each aspect requires each data formats and processing techniques. In this paper, we discuss changes in the environment surrounding planetary explorations and the handling of big data on computers. As a result, for the long-term preservation of scientific data, there must be standards and a community to endorse the standards. After standards, each community prepares the analysis tools. Furthermore, scientists need to make efforts not only in standardization but also in ensuring the quality of science. For highly informative data in recent years, the processing of data archives requires information science experts. Also, data providers or distributors should define data policies to clarify data usages to users. Finally, scientific analysis of cloud-based architecture due to big data and computer resources.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {88–90},
numpages = {3},
keywords = {SPICE, planetary exploration, Planetary Data System},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/2783258.2788573,
author = {Zheng, Yu and Yi, Xiuwen and Li, Ming and Li, Ruiyuan and Shan, Zhangqing and Chang, Eric and Li, Tianrui},
title = {Forecasting Fine-Grained Air Quality Based on Big Data},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2788573},
doi = {10.1145/2783258.2788573},
abstract = {In this paper, we forecast the reading of an air quality monitoring station over the next 48 hours, using a data-driven method that considers current meteorological data, weather forecasts, and air quality data of the station and that of other stations within a few hundred kilometers. Our predictive model is comprised of four major components: 1) a linear regression-based temporal predictor to model the local factors of air quality, 2) a neural network-based spatial predictor to model global factors, 3) a dynamic aggregator combining the predictions of the spatial and temporal predictors according to meteorological data, and 4) an inflection predictor to capture sudden changes in air quality. We evaluate our model with data from 43 cities in China, surpassing the results of multiple baseline methods. We have deployed a system with the Chinese Ministry of Environmental Protection, providing 48-hour fine-grained air quality forecasts for four major Chinese cities every hour. The forecast function is also enabled on Microsoft Bing Map and MS cloud platform Azure. Our technology is general and can be applied globally for other cities.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2267–2276},
numpages = {10},
keywords = {big data, urban air, urban computing, air quality forecast},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@inproceedings{10.1145/3134271.3134300,
author = {Zhang, Yong'an and Zhang, Yuxiaodan},
title = {Research on Evolution and Visualization Analysis of Application of Big Data},
year = {2017},
isbn = {9781450352765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134271.3134300},
doi = {10.1145/3134271.3134300},
abstract = {Based on literatures relevant to application of big data included in the Web of Science database as the data sources, this paper used CiteSpace as the research tool to visualize the distribution of keywords, evolution of hot spots and evolution rules. After that, it analyzed the hot spots, contexts, tool technologies and application fields of application of big data, so as to reveal the current situation of researches. The study indicates that existing researches involve a wide range of disciplines and technology is still the center of current researches. At present, the majority of the studies all focused on the field of management, network, information, medicine, health, environment, energy and etc. Overall, technology is still the important part in current research on application of big data. Technical tools, such as data mining, algorithms, cloud computing, mapreduce, hadoop, machine learning all provide strong supports for the practical application of big data.},
booktitle = {Proceedings of the International Conference on Business and Information Management},
pages = {126–130},
numpages = {5},
keywords = {CiteSpace, Application, Big data, Knowledge map},
location = {Bei Jing, China},
series = {ICBIM 2017}
}

@article{10.1145/3374749,
author = {Tian, Zhihong and Luo, Chaochao and Lu, Hui and Su, Shen and Sun, Yanbin and Zhang, Man},
title = {User and Entity Behavior Analysis under Urban Big Data},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {2691-1922},
url = {https://doi.org/10.1145/3374749},
doi = {10.1145/3374749},
abstract = {Recently, the urban network infrastructure has undergone a rapid expansion that is increasingly generating a large quantity of data and transforming our cities into smart cities. However, serious security problems arise with this development with more and more smart devices collecting private information under smart city scenario. In this article, we investigate the task of detecting insiders’ anomalous behaviors to prevent urban big data leakage. Specifically, we characterize a user's daily activities from four perspectives and use several deep learning algorithms (long short-term memory (LSTM) and convolutional LSTM (convLSTM)) to calculate deviations between realistic actions and normalcy of daily behaviors and use multilayer perceptron (MLP) to identify abnormal behaviors according to those deviations. To evaluate the proposed multimodel-based system (MBS), we conducted experiments on the CERT (United States Computer Emergency Readiness Team) dataset. The experimental results show that our proposed MBS has a remarkable ability to learn the normal pattern of users’ daily activities and detect anomalous behaviors.},
journal = {ACM/IMS Trans. Data Sci.},
month = {sep},
articleno = {16},
numpages = {19},
keywords = {deep learning, security, anomaly detection, UEBA}
}

@inproceedings{10.1145/3372938.3372977,
author = {Bensassi, Ismail and Elyusufi, Yasyn and El Mokhtar, En-Naimi},
title = {Smart Connection of User Profiles in a Big Data Context},
year = {2020},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3372977},
doi = {10.1145/3372938.3372977},
abstract = {The idea we propose in this article is a follow-up to our research series in the ontology-based profiling framework. The approach relies on tracking user profile changes for user connection within a Big Data context. We have worked in our series of research on the identification and qualification of profiles in web 2.0 context based on the ontological approach and multi agent system. Among the limitations of our research is the fact that changing interests over time does not affect the relationships between profiles. The goal of our approach is to follow the change of the interests of internet users and to propose afterwards new relations having changed activities in the same direction. In order to implement this approach, we will first use the ontology approach. The ontology approach will allows describing semantic models and determine the properties, restrictions and axioms of our application domain. The ontology we propose will generate a set of domains and sub domains of activities used to identify user profiles. On the other hand, we will use the Multi Agents approach to process users' activities before classifying them in their profiles.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {39},
numpages = {8},
keywords = {Big Data, MAS, Ontology, Profiling},
location = {Rabat, Morocco},
series = {BDIoT'19}
}

@inproceedings{10.1145/2818869.2818904,
author = {Yuan, Yu-Lan and Ho, Chaang-Iuan},
title = {Rethinking the Destination Marketing Organization Management in the Big Data Era},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818904},
doi = {10.1145/2818869.2818904},
abstract = {Big data is an important part of modern information management, providing new strategies for Destination Marketing Organizations (DMOs) for DMOs to gather large amount of data from tourists and stakeholders. DMOs that have the domain knowledge to analyze these data can take the opportunities presented by Big Data. This work describes the opportunities and challenges presented to DMOs in use of Big Data.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {60},
numpages = {5},
keywords = {Opportunities, Destination Management Organization, Challenges, Big Data},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@inproceedings{10.5555/3018100.3018105,
author = {Yang, Xi and Lehman, Tom},
title = {Model Driven Advanced Hybrid Cloud Services for Big Data: Paradigm and Practice},
year = {2016},
isbn = {9781509061587},
publisher = {IEEE Press},
abstract = {Advanced hybrid cloud services aim to serve big data applications by bridging multi-provider high performance cloud resources including direct connects, hypervisor bypassing VM interfaces, on premise clusters, parallel storage and high speed inter-cloud networks. We present a new "full-stack model driven orchestration" paradigm to integrate these diverse resources through semantic modeling and provide complex high-end services through dynamic orchestrated workflows. We also present architectural design of a real-world orchestration system, VersaStack, that implements the paradigm as well as a case study for providing full-scale advanced hybrid cloud services in practice.},
booktitle = {Proceedings of the 7th International Workshop on Data-Intensive Computing in the Cloud},
pages = {32–36},
numpages = {5},
keywords = {semantic modeling, service orchestration, advanced hybrid cloud, big data},
location = {Salt Lake City, Utah},
series = {DataCloud '16}
}

@inproceedings{10.1145/2538862.2544280,
author = {Hamid, Nadeem Abdul and Benzel, Steven},
title = {Towards Engaging Big Data for CS1/2 (Abstract Only)},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538862.2544280},
doi = {10.1145/2538862.2544280},
abstract = {A number of contextualized approaches to teaching introductory Computer Science (CS) courses have been developed in the past few years, catering to students with different interests and learning styles. For instance, entire courses have been developed around media computation or robots (real and virtual). There is however one context which, to our knowledge, has not been exploited in a systematic fashion - that of "big data," by which we mean massive, openly accessible online datasets from a wide variety of sources. We present progress on a code framework and methodology to facilitate the incorporation of large, online data sets into traditional CS1 and CS2 courses. The goal of our project is to develop a way to provide students a library that relieves them from low-level issues of reading and parsing raw data from web-based data sources and that interfaces with data structures and representations defined by students themselves. In addition, the library requires minimal syntactic overhead to use its functionality and allows students and instructors to focus on algorithmic exercises involving processing live and large data obtained from the Internet. At a minimum, the library should serve to create drop-in replacements for traditional programming exercises in introductory courses - raising the engagement level by having students deal with "real" data rather than artificial data provided through standard input.},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
pages = {710},
numpages = {1},
keywords = {big data, CS1},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1145/2483574.2483579,
author = {Rupprecht, Lukas},
title = {Exploiting In-Network Processing for Big Data Management},
year = {2013},
isbn = {9781450321556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483574.2483579},
doi = {10.1145/2483574.2483579},
abstract = {Data processing systems face the task of efficiently storing and processing data at petabyte scale, with the amount set to increase in the future. To meet such a requirement, highly scalable, shared-nothing systems, e.g. Google's BigTable [6] or Facebook's Cassandra [14], are built to partition data and process it in parallel on distributed nodes in a cluster. This allows the handling of data at scale but introduces new challenges due to the distribution of data. Running queries involves a high network overhead because data has to be exchanged between cluster nodes and hence, the network becomes a critical part of the system. To avoid the network bottleneck, it is essential for distributed data processing systems (DDPS) to be aware of the network rather than treating it as a black box.We propose in-network processing as a way of achieving network-awareness to decrease bandwidth usage by custom routing, redundancy elimination, and on-path data reduction. Thereby, we can increase the query throughput of a DDPS. The challenges of an in-network processing system range from design issues, such as performance and transparency, to the integration with query optimisation and deployment in data centres. We formulate these challenges as possible research directions and provide a prototype implementation. Our preliminary results suggest that we can significantly improve query throughput in a DDPS by performing partial data reduction within the network.},
booktitle = {Proceedings of the 2013 SIGMOD/PODS Ph.D. Symposium},
pages = {1–6},
numpages = {6},
keywords = {scale-out, date centres, network-awareness, nosql},
location = {New York, New York, USA},
series = {SIGMOD'13 PhD Symposium}
}

@inproceedings{10.1145/2664591.2664619,
author = {Ayankoya, Kayode and Calitz, Andre and Greyling, Jean},
title = {Intrinsic Relations between Data Science, Big Data, Business Analytics and Datafication},
year = {2014},
isbn = {9781450332460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2664591.2664619},
doi = {10.1145/2664591.2664619},
abstract = {Data recording and storage have evolved over the past decades from manual gathering of data by using simple writing materials to the automation of data collection. Data storage has evolved significantly in the past decades and today databases no longer suffice as the only medium for the storage and management of data. This is due to the emergence of the Big Data and Data Science concepts. Previous studies have indicated that the multiplication of processing power of computers and the availability of larger data storage at reduced cost are part of the catalysts for the volume and rate at which data is now made available and captured.In this paper, the concepts of Big Data, Data Science and Business Analytics are reviewed. This paper discusses datafication of different aspects of life as the fundamental concept behind the growth of Big Data and Data Science. A review of the characteristics and value of Big Data and Data Science suggests that these emerging concepts will bring a paradigm change to a number of areas. Big Data was described as the basis for Data Science and Business Analytics which are tools employed in Data Science. Because these fields are still developing, there are diverse opinions, especially on the definition of Data Science. This paper provides a revised definition of Data Science, based on the review of available literature and proposes a schematic representation of the concepts.},
booktitle = {Proceedings of the Southern African Institute for Computer Scientist and Information Technologists Annual Conference 2014 on SAICSIT 2014 Empowered by Technology},
pages = {192–198},
numpages = {7},
keywords = {Big Data, Business Analytics, Business Intelligence, Data Science, Datafication},
location = {Centurion, South Africa},
series = {SAICSIT '14}
}

@article{10.1109/TCBB.2019.2951555,
author = {Yan, Lu and Huang, Weihong and Wang, Liming and Feng, Song and Peng, Yonghong and Peng, Jie},
title = {Data-Enabled Digestive Medicine: A New Big Data Analytics Platform},
year = {2019},
issue_date = {May-June 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2951555},
doi = {10.1109/TCBB.2019.2951555},
abstract = {This paper presents a big data analystics platform for clinical research and practice in the Gastroenterology Department of Xiangya Hospital at Central South University in China. This platform features a comprehensive and systematic support of big data in digestive medicine including geneneral health management, clinical gastroenterology practice, and related genomics research, which is proven to be helpful in real world clinical practices. A typical use case of integrated analysis based on electronic medical records and colonoscopy data was presented and discussed, the analaystic report on risk factors of colorectal diseases shows a reasonable recommendation about the age when people should start to screen the colorectal cancer, which could be very useful to individual and group health management for the general population in China.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {nov},
pages = {922–931},
numpages = {10}
}

@inproceedings{10.1145/2851613.2852015,
author = {Ahmad, Awais},
title = {Social Element of Big Data Analytics: Integrating Social Network with the Internet of Things: Student Research Abstract},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2852015},
doi = {10.1145/2851613.2852015},
abstract = {As we delve deeper into the Internet of Things (IoT), we are observing the intensive interaction and heterogeneous communication among different social objects over the Internet. Such knowledge gives us the concept of Social Internet of Things (SIoT). SIoT comprises billions of interconnected objects that generate massive volume of heterogeneous, multisource, dynamic, and sparse data, which lead a system towards a major computational challenges, such as processing, analyzing, and storing data in an efficient manner. To address this problem, we propose a system architecture for processing a stream of Big Data with the enhanced features of parallel processing techniques. The proposed architecture consists of three functional domains, i.e., object domain, SIoT server domain, and application domain. The performance of the system architecture is tested on Hadoop using UBUNTU 14.04 LTS core™i5 machine with 3.2 GHz processor and 4 GB memory. The analysis and discussion show that the performance of the proposed system architecture fulfills the required desires if we increase the size of the datasets.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {216–217},
numpages = {2},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3349614.3356026,
author = {Jiang, Junchen and Zhou, Yuhao and Ananthanarayanan, Ganesh and Shu, Yuanchao and Chien, Andrew A.},
title = {Networked Cameras Are the New Big Data Clusters},
year = {2019},
isbn = {9781450369282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349614.3356026},
doi = {10.1145/3349614.3356026},
abstract = {The increasing complexity of deep learning and massive deployment of cameras at the edge have drastically increased the resource demand of edge data analytics. Compared to traditional Internet web applications, such resource demand (in computing, storage and networking) is not limited by millions of human users, but rather the continuous activities of billions of sensors. This paper presents the abstraction of camera cluster as an attempt to address this challenge in the context of video analytics. We envision a novel analytics stack that orchestrates the computing resource of massive networked cameras to enable efficient edge video analytics.},
booktitle = {Proceedings of the 2019 Workshop on Hot Topics in Video Analytics and Intelligent Edges},
pages = {1–7},
numpages = {7},
keywords = {video analytics, camera cluster, edge},
location = {Los Cabos, Mexico},
series = {HotEdgeVideo'19}
}

@inproceedings{10.1145/3127404.3130250,
author = {Zhong, Ning},
title = {Brain Big Data Based Wisdom Service: A Brain Informatics Based Systematic Approach},
year = {2017},
isbn = {9781450353526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127404.3130250},
doi = {10.1145/3127404.3130250},
abstract = {In this talk, I demonstrate a Brain Informatics based systematic approach to an integrated understanding of macroscopic and microscopic level working principles of the brain by means of experimental, computational, and cognitive neuroscience studies, as well as utilizing advanced Web intelligence centric information technologies. I discuss research issues and challenges with respect to brain computing from three aspects of Brain Informatics studies that deserve closer attention: systematic investigations for complex brain science problems, new information technologies for supporting systematic brain science studies, and Brain Informatics studies based on Web intelligence (AI on the Internet) research needs. These three aspects offer different ways to study traditional cognitive science, neuroscience, brain and mental health, and artificial intelligence.},
booktitle = {Proceedings of the 12th Chinese Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1},
numpages = {1},
keywords = {cognitive science, brain big data, wisdom service, Brain informatics},
location = {Chongqing, China},
series = {ChineseCSCW '17}
}

@inproceedings{10.1109/BDC.2014.15,
author = {Kune, Raghavendra and Konugurthi, Pramod Kumar and Agarwal, Arun and Chillarige, Raghavendra Rao and Buyya, Rajkumar},
title = {Genetic Algorithm Based Data-Aware Group Scheduling for Big Data Clouds},
year = {2014},
isbn = {9781479918973},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BDC.2014.15},
doi = {10.1109/BDC.2014.15},
abstract = {Cloud computing is a promising cost efficient service oriented computing platform in the fields of science, engineering, business and social networking for delivering the resources on demand. Big Data Clouds is a new generation data analytics platform using Cloud computing as a back end technologies, for information mining, knowledge discovery and decision making based on statistical and empirical tools. MapReduce scheduling models for Big Data computing operate in the cluster mode, where the data nodes are pre-configured with the computing facility for processing. These MapReduce models are based on compute push model-pushing the logic to the data node for analysis, which is primarily for minimizing or eliminating data migration overheads between computing resources and data nodes. Such models, however, substantially perform well in the cluster setups, but are infelicitous for the platforms having the decoupled data storage and computing resources. In this paper, we propose a Genetic Algorithm based scheduler for such Big Data Cloud where decoupled computational and data services are offered as services. The approach is based on evolutionary methods focussed on data dependencies, computational resources and effective utilization of bandwidth thus achieving higher throughputs.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing},
pages = {96–104},
numpages = {9},
keywords = {Big Data, Genetic algorithms, Big Data Clouds, Cloud computing, Data Intensive Scheduling},
series = {BDC '14}
}

@inproceedings{10.1145/3277139.3277155,
author = {Sarker, Md Nazirul Islam and Hossin, Md Altab and Frimpong, Adasa Nkrumah Kofi and Xiaohua, Yin},
title = {Promoting Information Resource Management for E-Government through Big Data Approach},
year = {2018},
isbn = {9781450364867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277139.3277155},
doi = {10.1145/3277139.3277155},
abstract = {Big data has a potential to transform traditional government system to data-driven e-government system by utilizing modern analytical techniques. The aim of this article is to explore the applicability of big data for ensuring e-government. An extensive literature review has been administered using various levels of scales and indicators. Literature survey shows that a number of models have been developed to explain e-governance but systematic research on the suitability of big data for e-government is still lacking. This article argues that big data can help the information resource management system of the government for improving transparency and reducing corruption, fastest public service delivery, reducing public hassle, providing easy access to public services, reducing error and reducing poverty through e-services, e-management, e-democracy, and e-commerce. This article further argues that big data has a significant role in cost-effective service delivery to citizens, policy coherence, access to public services, participation and engagement, representation, access to information, open government and corruption control. The finding suggests that big data technologies should be implemented in every public-sector organization by minimizing technological challenges and threats, ensuring the privacy of citizen's information, maximizing utilization of data and promoting information management capacity.},
booktitle = {Proceedings of the 2018 International Conference on Information Management &amp; Management Science},
pages = {99–104},
numpages = {6},
keywords = {governance, administration, big data, public agency, e-government, IRM},
location = {Chengdu, China},
series = {IMMS '18}
}

@techreport{10.5555/2849516,
author = {Markus, M. Lynne and Topi, Heikki},
title = {Big Data, Big Decisions for Science, Society, and Business: Report on a Research Agenda Setting Workshop},
year = {2015},
publisher = {National Science Foundation},
address = {USA},
abstract = {The report from the workshop, "Big Data, Big Decisions for Government, Business and Society," makes a number of astute contributions. There is no need to replicate them in this foreword - they are in the report. What might be missed comes between the lines, where provocative points are made. Big Data means big opinions and big stakes. Those who think Big Data important want to be proven right, those who think Big Data a passing fad want Big Data to fade, and those who think Big Data will bring profound change hope for change. Big Data, like everything important, is political.}
}

@inproceedings{10.1145/3206157.3206177,
author = {Yanhui, Wu},
title = {Language E-Learning Based on Learning Analytics in Big Data Era},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206177},
doi = {10.1145/3206157.3206177},
abstract = {Language E-learning, an online language learning mode, completely transforms the traditional learning and teaching mode. However, with the coming of Big Data era, it not only enjoys some benefits, but also is confronted with great challenges. The article first concludes the reforms Big Data brought to the world, and then introduces the definition, key elements, the applying model, main analyzing methods and tools of learning analytics. Finally, the article fully shows the implementation of learning analytics to the study of language E-learning. Through the study of learning analytics, the designer of Language E-learning can learn the learners' learning behaviors and provide the efficient learning material, tools and systems.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {106–111},
numpages = {6},
keywords = {Big Data, learning analytics, language E-learning},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/3264560.3266428,
author = {Ayvaz, Serkan and Shiha, Mohammed O.},
title = {A Scalable Streaming Big Data Architecture for Real-Time Sentiment Analysis},
year = {2018},
isbn = {9781450364744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264560.3266428},
doi = {10.1145/3264560.3266428},
abstract = {The systems with a short window of opportunity for actions and decisions require developing solutions providing real-time streaming analytics. Real-time big data streaming analytics is a challenging task. In this paper, we propose a streaming big data architecture for real-time social network analysis. As a case study, we investigated the relation between the public opinions on social media about cryptocurrencies and the changes in their prices using lexicon-based sentiment analysis approaches with the goal of assessing the feasibility of predicting the prices of cryptocurrencies. Two different approaches with two lexicons were used for sentiment analysis score calculations to assess the consistency of correlation measures on the collected dataset. Our model indicates that the prediction of cryptocurrency price changes using lexicon-based sentiment analysis methods is not reliable.},
booktitle = {Proceedings of the 2018 2nd International Conference on Cloud and Big Data Computing},
pages = {47–51},
numpages = {5},
keywords = {data streaming, opinion mining, big data, cryptocurrency, sentiment analysis},
location = {Barcelona, Spain},
series = {ICCBDC'18}
}

@inproceedings{10.1145/3330431.3330447,
author = {Rakhimova, Diana and Turganbayeva, Aliya},
title = {Lemmatization of Big Data in the Kazakh Language},
year = {2019},
isbn = {9781450372121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330431.3330447},
doi = {10.1145/3330431.3330447},
abstract = {In paper are considered existing algorithms for automatically isolating the bases for a number of natural languages and possible ways of synthesizing a normal form of a word for the Kazakh language. In paper are described the complete system of endings of the Kazakh language. The article presents the classification of affixes Kazakh language. The paper proposes a new approach to constructing a lemmatization algorithm for the Kazakh language on the basis of a complete set of endings of the Kazakh language. The lemmatization algorithm will be used for Kazakh language information retrieval to finding specific words in the documents by stemming base of word.},
booktitle = {Proceedings of the 5th International Conference on Engineering and MIS},
articleno = {16},
numpages = {4},
keywords = {information, lemmatization, big data, language, algorithm, retrieval, Kazakh},
location = {Astana, Kazakhstan},
series = {ICEMIS '19}
}

@inbook{10.1145/3310205.3310211,
title = {Data Quality Rule Definition and Discovery},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310211},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.14778/2536360.2536368,
author = {Fan, Wenfei and Geerts, Floris and Neven, Frank},
title = {Making Queries Tractable on Big Data with Preprocessing: Through the Eyes of Complexity Theory},
year = {2013},
issue_date = {July 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536360.2536368},
doi = {10.14778/2536360.2536368},
abstract = {A query class is traditionally considered tractable if there exists a polynomial-time (PTIME) algorithm to answer its queries. When it comes to big data, however, PTIME algorithms often become infeasible in practice. A traditional and effective approach to coping with this is to preprocess data off-line, so that queries in the class can be subsequently evaluated on the data efficiently. This paper aims to provide a formal foundation for this approach in terms of computational complexity. (1) We propose a set of Π-tractable queries, denoted by ΠTQ0, to characterize classes of queries that can be answered in parallel poly-logarithmic time (NC) after PTIME preprocessing. (2) We show that several natural query classes are Π-tractable and are feasible on big data. (3) We also study a set ΠTQ of query classes that can be effectively converted to Π-tractable queries by refactorizing its data and queries for preprocessing. We introduce a form of NC reductions to characterize such conversions. (4) We show that a natural query class is complete for ΠTQ. (5) We also show that ΠTQ0 ⊂ P unless P = NC, i.e., the set ΠTQ0 of all Π-tractable queries is properly contained in the set P of all PTIME queries. Nonetheless, ΠTQ = P, i.e., all PTIME query classes can be made Π-tractable via proper refactorizations. This work is a step towards understanding the tractability of queries in the context of big data.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {685–696},
numpages = {12}
}

@article{10.1145/3012286,
author = {Amato, Flora and Moscato, Vincenzo and Picariello, Antonio and Colace, Francesco and Santo, Massimo De and Schreiber, Fabio A. and Tanca, Letizia},
title = {Big Data Meets Digital Cultural Heritage: Design and Implementation of SCRABS, A Smart Context-AwaRe Browsing Assistant for Cultural EnvironmentS},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3012286},
doi = {10.1145/3012286},
abstract = {Information and Communication Technologies have radically changed the modern Cultural Heritage scenery: Simple traditional Information Systems supporting the management of cultural artifacts have left the place to complex systems that expose rich information extracted from heterogeneous data sources—like Sensor Networks, Social Networks, Digital Libraries, Multimedia Collections, Web Data Service, and so on—by means of sophisticated applications that enhance the users’ experience. In this article, we describe SCRABS, a Smart Context-awaRe Browsing assistant for cultural EnvironmentS. SCRABS has been developed during the Cultural Heritage Information Systems national project and promoted by DATABENC, the Cultural Heritage Technological District of the Campania Region, in Italy. SCRABS has been designed on top of a Big Data technological stack as the result of a multidisciplinary project carried out by a heterogeneous team of computer scientists, archeologists, architects, and experts in humanities. We describe the main ideas that support the system, showing its use in some real application scenarios located in the Paestum Archeologica Sites.},
journal = {J. Comput. Cult. Herit.},
month = {apr},
articleno = {6},
numpages = {23},
keywords = {Big data, cultural heritage, multimedia}
}

@inproceedings{10.1145/3302424.3303988,
author = {Bruno, Rodrigo and Patricio, Duarte and Sim\~{a}o, Jos\'{e} and Veiga, Luis and Ferreira, Paulo},
title = {Runtime Object Lifetime Profiler for Latency Sensitive Big Data Applications},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303988},
doi = {10.1145/3302424.3303988},
abstract = {Latency sensitive services such as credit-card fraud detection and website targeted advertisement rely on Big Data platforms which run on top of memory managed runtimes, such as the Java Virtual Machine (JVM). These platforms, however, suffer from unpredictable and unacceptably high pause times due to inadequate memory management decisions (e.g., allocating objects with very different lifetimes next to each other, resulting in severe memory fragmentation). This leads to frequent and long application pause times, breaking Service Level Agreements (SLAs). This problem has been previously identified, and results show that current memory management techniques are ill-suited for applications that hold in memory massive amounts of long-lived objects (which is the case for a wide spectrum of Big Data applications).Previous works reduce such application pauses by allocating objects in off-heap, in special allocation regions/generations, or by using ultra-low latency Garbage Collectors (GC). However, all these solutions either require a combination of programmer effort and knowledge, source code access, offline profiling (with clear negative impacts on programmer's productivity), or impose a significant impact on application throughput and/or memory to reduce application pauses.We propose ROLP, a Runtime Object Lifetime Profiler that profiles application code at runtime and helps pretenuring GC algorithms allocating objects with similar lifetimes close to each other so that the overall fragmentation, GC effort, and application pauses are reduced. ROLP is implemented for the OpenJDK 8 and was evaluated with a recently proposed open-source pretenuring collector (NG2C). Results show long tail latencies reductions of up to 51% for Lucene, 85% for GraphChi, and 69% for Cassandra. This is achieved with negligible throughput (&lt; 6%) and memory overhead, with no programmer effort, and no source code access.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {28},
numpages = {16},
keywords = {Profiling, Garbage Collection, Big Data, Tail Latency, Pretenuring},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@inproceedings{10.1145/1882291.1882293,
author = {Eagle, Nathan},
title = {Big Data, Global Development, and Complex Social Systems},
year = {2010},
isbn = {9781605587912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882291.1882293},
doi = {10.1145/1882291.1882293},
abstract = {Petabytes of data about human movements, transactions, and communication patterns are continuously being generated by everyday technologies such as mobile phones and credit cards. In collaboration with the mobile phone, internet, and credit card industries, Eagle and colleagues are aggregating and analyzing behavioral data from over 250 million people from North and South America, Europe, Asia and Africa. Eagle discusses projects arising from these collaborations that involve inferring behavioral dynamics on a broad spectrum of scales from risky behavior in a group of MIT freshman to population-level behavioral signatures, including cholera outbreaks in Rwanda and wealth in the UK. The research group is developing a range of large-scale network analysis and machine learning algorithms that will provide deeper insight into human behavior.},
booktitle = {Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {3–4},
numpages = {2},
keywords = {behavioral dynamics, network analysis, machine learning, data analysis},
location = {Santa Fe, New Mexico, USA},
series = {FSE '10}
}

@inproceedings{10.5555/3233397.3233441,
author = {Gheid, Zakaria and Challal, Yacine},
title = {An Efficient and Privacy-Preserving Similarity Evaluation for Big Data Analytics},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {Big data systems are gathering more and more information in order to discover new values through data analytics and depth insights. However, mining sensitive personal information breaches privacy and degrades services' reputation. Accordingly, many research works have been proposed to address the privacy issues of data analytics, but almost seem to be not suitable in big data context either in data types they support or in computation time efficiency. In this paper we propose a novel privacy-preserving cosine similarity computation protocol that will support both binary and numerical data types within an efficient computation time, and we prove its adequacy for big data high volume, high variety and high velocity.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {281–289},
numpages = {9},
keywords = {privacy, data analytics, big data, cosine similarity},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@article{10.14778/3352063.3352130,
author = {Kandula, Srikanth and Lee, Kukjin and Chaudhuri, Surajit and Friedman, Marc},
title = {Experiences with Approximating Queries in Microsoft's Production Big-Data Clusters},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352130},
doi = {10.14778/3352063.3352130},
abstract = {With the rapidly growing volume of data, it is more attractive than ever to leverage approximations to answer analytic queries. Sampling is a powerful technique which has been studied extensively from the point of view of facilitating approximation. Yet, there has been no large-scale study of effectiveness of sampling techniques in big data systems. In this paper, we describe an in-depth study of the sampling-based approximation techniques that we have deployed in Microsoft's big data clusters. We explain the choices we made to implement approximation, identify the usage cases, and study detailed data that sheds insight on the usefulness of doing sampling based approximation.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2131–2142},
numpages = {12}
}

