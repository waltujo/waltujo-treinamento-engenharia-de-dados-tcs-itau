@inproceedings{10.1145/3017680.3022386,title = {Developing Big Data Curriculum with Open Source Infrastructure (Abstract Only)}, author = {Nagar Anurag },year = {2017}, isbn = {9781450346986}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3017680.3022386}, doi = {10.1145/3017680.3022386}, abstract = {This lightning talk will focus on our experience of developing and managing large undergraduate and graduate Big Data courses. The demand for trained professionals in the field of Big Data technologies is huge, and there is urgent need to develop and update courses in this area. One of the biggest hurdles for many schools is establishment, maintenance, and constant update of high performance computing infrastructure. Further, the technology landscape for Big Data is constantly evolving, and newer technologies, such as Apache Spark, require significant expenditure to set up and upgrade at the cluster level. Traditional infrastructure at most higher educational institutions is insufficient for this, and is also not able to scale up to meet the expectations of large class sizes and multiple simultaneous sessions. In this lightening talk, we will share our experience of running large undergraduate and graduate Big Data courses using open source infrastructure. Some of this infrastructure is cloud based, while others require students to create virtualized environment on their personal computers. Both types of resources are freely available, easy to setup, and provide students with enough computational power to run most academic tasks and projects. We will provide specific examples of using such technologies for common tasks, such as setting up a distributed file system, running MapReduce algorithms on large datasets, performing large scale machine learning and graph mining using Apache Spark, and maintaining a high availability Cassandra instance.}, location = {Seattle, Washington, USA}, series = {SIGCSE '17}, pages = {700\u2013701}, numpages = {2}, keywords = {infrastructure for teaching big data, big data teaching, distributed computing teaching}}
@inproceedings{10.1145/3129292.3129296,title = {Towards Real-Time Road Traffic Analytics using Telco Big Data}, author = {Costa Constantinos , Chatzimilioudis Georgios , Zeinalipour-Yazti Demetrios , Mokbel Mohamed F. },year = {2017}, isbn = {9781450354257}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3129292.3129296}, doi = {10.1145/3129292.3129296}, abstract = {A telecommunication company (telco) is traditionally only perceived as the entity that provides telecommunication services, such as telephony and data communication access to users. However, the IP backbone infrastructure of such entities spanning densely urban spaces and widely rural areas, provides nowadays a unique opportunity to collect immense amounts of mobility data that can provide valuable insights for road traffic management and avoidance. In this paper we outline the components of the Traffic-TBD (Traffic Telco Big Data) architecture, which aims to become an innovative road traffic analytic and prediction system with the following desiderata: i) provide micro-level traffic modeling and prediction that goes beyond the current state provided by Internet-based navigation enterprises utilizing crowdsourcing; ii) retain the location privacy boundaries of users inside their mobile network operator, to avoid the risks of exposing location data to third-party mobile applications; and iii) be available with minimal costs and using existing infrastructure (i.e., cell towers and TBD data streams are readily available inside a telco). Road traffic understanding, management and analytics can minimize the number of road accidents, optimize fuel and energy consumption, avoid unexpected delays, contribute to a macroscopic spatio-temporal understanding of traffic in cities but also to \"smart\" societies through applications in city planning, public transportation, logistics and fleet management for enterprises, startups and governmental bodies.}, location = {Munich, Germany}, series = {BIRTE '17}, pages = {1\u20135}, numpages = {5}, keywords = {Big Data, Road Traic, Data Analytics, Telco}}
@inproceedings{10.1145/3246336,title = {Session details: Towards big data}, author = {Chen Lei },year = {2013}, isbn = {9781450321556}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3246336}, doi = {10.1145/3246336}, location = {New York, New York, USA}, series = {SIGMOD'13 PhD Symposium}, pages = {}}
@inproceedings{10.1109/CCGrid.2015.175,title = {Cross-layer SLA management for cloud-hosted big data analytics applications}, author = {Zeng Xuezhi , Ranjan Rajiv , Strazdins Peter , Garg Saurabh Kumar , Wang Lizhe },year = {2015}, isbn = {9781479980062}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2015.175}, doi = {10.1109/CCGrid.2015.175}, abstract = {As we come to terms with various big data challenges, one vital issue remains largely untouched. That is service level agreement (SLA) management to deliver strong Quality of Service (QoS) guarantees for big data analytics applications (BDAA) sharing the same underlying infrastructure, for example, a public cloud platform. Although SLA and QoS are not new concepts as they originated much before the cloud computing and big data era, its importance is amplified and complexity is aggravated by the emergence of time-sensitive BDAAs such as social network-based stock recommendation and environmental monitoring. These applications require strong QoS guarantees and dependability from the underlying cloud computing platform to accommodate real-time responses while handling ever-increasing complexities and uncertainties. Hence, the over-reaching goal of this PhD research is to develop novel simulation, modeling and benchmarking tools and techniques that can aid researchers and practitioners in studying the impact of uncertainties (contention, failures, anomalies, etc.) on the final SLA and QoS of a cloud-hosted BDAA.}, location = {Shenzhen, China}, series = {CCGRID '15}, pages = {765\u2013768}, numpages = {4}, keywords = {big data, cloud computing, service level agreement}}
@inproceedings{10.5555/2849516,title = {Big Data, Big Decisions for Science, Society, and Business: Report on a Research Agenda Setting Workshop}, author = {Markus M. Lynne , Topi Heikki },year = {2015}, publisher = {National Science Foundation}, address = {USA}, abstract = {The report from the workshop, \"Big Data, Big Decisions for Government, Business and Society,\" makes a number of astute contributions. There is no need to replicate them in this foreword - they are in the report. What might be missed comes between the lines, where provocative points are made. Big Data means big opinions and big stakes. Those who think Big Data important want to be proven right, those who think Big Data a passing fad want Big Data to fade, and those who think Big Data will bring profound change hope for change. Big Data, like everything important, is political.}}
@inproceedings{10.1145/3297663.3310302,title = {Characterization of a Big Data Storage Workload in the Cloud}, author = {Talluri Sacheendra , \u0141uszczak Alicja , Abad Cristina L. , Iosup Alexandru },year = {2019}, isbn = {9781450362399}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3297663.3310302}, doi = {10.1145/3297663.3310302}, abstract = {The proliferation of big data processing platforms has led to radically different system designs, such as MapReduce and the newer Spark. Understanding the workloads of such systems facilitates tuning and could foster new designs. However, whereas MapReduce workloads have been characterized extensively, relatively little public knowledge exists about the characteristics of Spark workloads in representative environments. To address this problem, in this work we collect and analyze a 6-month Spark workload from a major provider of big data processing services, Databricks. Our analysis focuses on a number of key features, such as the long-term trends of reads and modifications, the statistical properties of reads, and the popularity of clusters and of file formats. Overall, we present numerous findings that could form the basis of new systems studies and designs. Our quantitative evidence and its analysis suggest the existence of daily and weekly load imbalances, of heavy-tailed and bursty behaviour, of the relative rarity of modifications, and of proliferation of big data specific formats.}, location = {Mumbai, India}, series = {ICPE '19}, pages = {33\u201344}, numpages = {12}, keywords = {interarrival time, apache spark, characterization, file formats, popularity, big data, long-term trend, cloud storage}}
@inproceedings{10.1145/3530050,title = {Proceedings of The International Workshop on Big Data in Emergent Distributed Environments},year = {2022}, isbn = {9781450393461}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, abstract = {Today, new forms of distributed environments beyond Cloud Computing occur that offer new kinds of applications, but pose new challenges for data management. The recent efforts for serverless computing aim at simplifying the process of deploying code in the Cloud into production by hiding scaling, capacity planning and maintenance operations from the developer or operator. Other initiatives work on avoiding the communication to the Cloud by deploying and running environments for data processing near data sources in Internet-of-Things scenarios (e.g., fog and edge computing) for large-scale smart homes, companies and cities, and near the applications (e.g., Cloudlets for mobile applications and Offline First technologies for web applications).Research on distributed data management evolves addressing new challenges specific to these new environments. Properties of emergent distributed environments regarding capabilities of nodes, bandwidth for communication, battery lifetime of nodes, reliability of nodes and communication, and heterogeneity of configurations impact data management mechanisms and approaches, such as those for fault tolerance, replication, resource provisioning, buffer management, query processing and optimization, and transaction management. In addition, federated approaches and polystores spanning over several emergent distributed environments are also remaining research challenges based on the need for combining these different distributed environments into one distributed runtime environment for easy handling of Big Data in different models and globally optimizing data management tasks across these different environments.The goal of this workshop is to bring together academic researchers and industry practitioners to discuss the challenges and solutions, including new approaches, techniques and applications, that significantly would advance the state of the art of Big Data in emergent distributed environments.}, location = {Philadelphia, Pennsylvania}}
@inproceedings{10.1145/3175603.3175613,title = {Key Technologies of Big Data and Its Development in Intelligent Ship}, author = {Cao Mengmeng , Guo Chaoyou },year = {2017}, isbn = {9781450353588}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3175603.3175613}, doi = {10.1145/3175603.3175613}, abstract = {The application of big data techniques will contribute to the transforming and upgrading in shipbuilding industry and produce a profound influence in the development of intelligent ship. An overall framework of shipbuilding big data platform is established in this paper, which is based on the relationships among the intelligent ship, cloud computing and big data. Then key techniques of big data are discussed in four aspects, including data generation, data acquisition, data storage, and data analysis. Finally, an overview of the architecture of big data and relevant techniques are demonstrated.}, location = {Shanghai, China}, series = {ICRAI 2017}, pages = {61\u201365}, numpages = {5}, keywords = {data acquisition, data mining, data storage, data visualization, intelligent ship, big data}}
@inproceedings{10.14778/1453856.1453980,title = {Discovering data quality rules}, author = {Chiang Fei , Miller Ren\u00e9e J. },year = {2008}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/1453856.1453980}, doi = {10.14778/1453856.1453980}, abstract = {Dirty data is a serious problem for businesses leading to incorrect decision making, inefficient daily operations, and ultimately wasting both time and money. Dirty data often arises when domain constraints and business rules, meant to preserve data consistency and accuracy, are enforced incompletely or not at all in application code.In this work, we propose a new data-driven tool that can be used within an organization's data quality management process to suggest possible rules, and to identify conformant and non-conformant records. Data quality rules are known to be contextual, so we focus on the discovery of context-dependent rules. Specifically, we search for conditional functional dependencies (CFDs), that is, functional dependencies that hold only over a portion of the data. The output of our tool is a set of functional dependencies together with the context in which they hold (for example, a rule that states for CS graduate courses, the course number and term functionally determines the room and instructor). Since the input to our tool will likely be a dirty database, we also search for CFDs that almost hold. We return these rules together with the non-conformant records (as these are potentially dirty records).We present effective algorithms for discovering CFDs and dirty values in a data instance. Our discovery algorithm searches for minimal CFDs among the data values and prunes redundant candidates. No universal objective measures of data quality or data quality rules are known. Hence, to avoid returning an unnecessarily large number of CFDs and only those that are most interesting, we evaluate a set of interest metrics and present comparative results using real datasets. We also present an experimental study showing the scalability of our techniques.}, pages = {1166\u20131177}, numpages = {12}}
@inproceedings{10.1145/2609876.2609883,title = {Human, Model and Machine: A Complementary Approach to Big Data}, author = {Thomson Robert , Lebiere Christian , Bennati Stefano },year = {2014}, isbn = {9781450329385}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2609876.2609883}, doi = {10.1145/2609876.2609883}, abstract = {In this paper, we describe a framework for processing big data that maximizing the efficiency of human data scientists by having them primarily operate over information that is best structured to human processing demands. We accomplish this through the use of cognitive models as an intermediary between machine learning algorithms and human data scientists. The ACT-R cognitive architecture is a computational implementation of a unified theory of cognition. ACT-R cognitive models can take weakly structured data and learn to filter information and make accurate inferences orders of magnitude faster than machine learning, and then present these well-structured inferences to human data scientists. The role for human data scientists is both oversight and feedback; one complementary piece of a hierarchy of cognitive and machine learning techniques that are computationally appropriate for their level of information complexity.}, location = {Raleigh, NC, USA}, series = {HCBDR '14}, pages = {27\u201331}, numpages = {5}, keywords = {Cognitive architectures, deep learning, ACT-R, Big Data}}
@inproceedings{10.1109/CCGRID.2017.73,title = {Towards Big Data Analytics across Multiple Clusters}, author = {Wu Dongyao , Sakr Sherif , Zhu Liming , Wu Huijun },year = {2017}, isbn = {9781509066100}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGRID.2017.73}, doi = {10.1109/CCGRID.2017.73}, abstract = {Big data are increasingly collected and stored in a highly distributed infrastructures due to the development of sensor network, cloud computing, IoT and mobile computing among many other emerging technologies. In practice, the majority of existing big-data-processing frameworks (e.g., Hadoop and Spark) are designed based on the single-cluster setup with the assumptions of centralized management and homogeneous connectivity which makes them sub-optimal and sometimes infeasible to apply for scenarios that require implementing data analytics jobs on highly distributed data sets (across racks, data centers or multi-organizations). In order to tackle this challenge, we present HDM-MC, a multi-cluster big data processing framework which is designed to enable the capability of performing large scale data analytics across multi-clusters with minimum extra overhead due to additional scheduling requirements. In this paper, we present the architecture and realization of the system. In addition, we evaluate the performance of our framework in comparison to other state-of-art single cluster big data processing frameworks.}, location = {Madrid, Spain}, series = {CCGrid '17}, pages = {218\u2013227}, numpages = {10}}
@inproceedings{10.1145/2094114.2094129,title = {The meaningful use of big data: four perspectives -- four challenges}, author = {Bizer Christian , Boncz Peter , Brodie Michael L. , Erling Orri },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2094114.2094129}, doi = {10.1145/2094114.2094129}, abstract = {Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.}, pages = {56\u201360}, numpages = {5}}
@inproceedings{10.1145/3299815.3314439,title = {Intrusion Detection Using Big Data and Deep Learning Techniques}, author = {Faker Osama , Dogdu Erdogan },year = {2019}, isbn = {9781450362511}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3299815.3314439}, doi = {10.1145/3299815.3314439}, abstract = {In this paper, Big Data and Deep Learning Techniques are integrated to improve the performance of intrusion detection systems. Three classifiers are used to classify network traffic datasets, and these are Deep Feed-Forward Neural Network (DNN) and two ensemble techniques, Random Forest and Gradient Boosting Tree (GBT). To select the most relevant attributes from the datasets, we use a homogeneity metric to evaluate features. Two recently published datasets UNSW NB15 and CICIDS2017 are used to evaluate the proposed method. 5-fold cross validation is used in this work to evaluate the machine learning models. We implemented the method using the distributed computing environment Apache Spark, integrated with Keras Deep Learning Library to implement the deep learning technique while the ensemble techniques are implemented using Apache Spark Machine Learning Library. The results show a high accuracy with DNN for binary and multiclass classification on UNSW NB15 dataset with accuracies at 99.16% for binary classification and 97.01% for multiclass classification. While GBT classifier achieved the best accuracy for binary classification with the CICIDS2017 dataset at 99.99%, for multiclass classification DNN has the highest accuracy with 99.56%.}, location = {Kennesaw, GA, USA}, series = {ACM SE '19}, pages = {86\u201393}, numpages = {8}, keywords = {machine learning, artificial neural networks, feature selection, Intrusion detection system, big data, ensemble techniques, deep learning}}
@inproceedings{10.1145/2396636.2396678,title = {Scalable interaction design for collaborative visual exploration of big data}, author = {Leftheriotis Ioannis },year = {2012}, isbn = {9781450312097}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2396636.2396678}, doi = {10.1145/2396636.2396678}, abstract = {Novel input devices such as tangibles, smartphones, multi-touch surfaces etc. have given impetus to new interaction techniques. In this PhD research, the main motivation is to study novel interaction techniques and designs that augment collaboration in a collocated environment. Furthermore, the main research aim is to take advantage of scalable interaction design techniques and tools that can be applied in a variety of devices so as to help users to work together on a problem with an abstract big data set, using visualizations on a collocated context.}, location = {Cambridge, Massachusetts, USA}, series = {ITS '12}, pages = {271\u2013276}, numpages = {6}, keywords = {big data, scalability, collaboration, user interfaces, interaction design, visual exploration}}
@inproceedings{10.1145/1651415.1651421,title = {Data quality through model quality: a quality model for measuring and improving the understandability of conceptual models}, author = {Mehmood Kashif , Si-Said Cherfi Samira , Comyn-Wattiau Isabelle },year = {2009}, isbn = {9781605588162}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1651415.1651421}, doi = {10.1145/1651415.1651421}, abstract = {Data quality has emerged as an important and challenging topic in recent years. This article addresses the conceptual model quality as it has been widely accepted that better conceptual models produce better information systems and thus implicitly improve the data quality. Conceptual Models are designed as part of the analysis phase and serve as a communicating mediator between the users and the development team. Consequently, their understandability is a real challenge to avoid the propagation of inaccurate interpretation of the user requirements to the underlying system design and implementation. In this paper, we propose an adaptive quality model. We illustrate its usefulness by describing how it can be used to model and evaluate the understandability of conceptual models. Our quality evaluation is enriched with corrective actions provided to the designer, leading to a guidance modeling process. A first validation based on a survey is proposed.}, location = {Hong Kong, China}, series = {MoSE+DQS '09}, pages = {29\u201332}, numpages = {4}, keywords = {quality factor, conceptual model, conceptual model understandability, quality metrics, conceptual modeling quality}}
@inproceedings{10.1145/3262388,title = {Session details: Mining big data}, author = {Fan Wei , Bifet Albert },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3262388}, doi = {10.1145/3262388}, pages = {}}
@inproceedings{10.1145/3428363.3428369,title = {Distributed Principal Component Analysis for Real-time Big Data Processing}, author = {Meem Jannat Ara , Ahmad Farzana Yasmin , Adnan Muhammad Abdullah },year = {2020}, isbn = {9781450389051}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3428363.3428369}, doi = {10.1145/3428363.3428369}, abstract = {Real-time big data analytics, which is the combination of real-time analytics and big data, works on processing large scale data as it arrives and strives to obtain insights from it without exceeding a limited time period. Massive amount of data is being generated every moment through web sites, social networks, scientific experiments, etc. which is stored in the cloud. When decision making requires an insight of the raw data in real-time (often by applying machine learning algorithms such as dimensionality reduction), these algorithms fail to analyze these incessantly flowing high volume data i.e dynamic big data. Our work proposes a variant of scalable principal component analysis (PCA) which is suited for real-time big data applications. We maintain a sliding window (representing incoming data in real-time) over the most recent data and project every incoming data into lower dimensional subspace. Our goal is to minimize the reconstruction error of the output from the input and keep updating the principal components depending on it. We have implemented our scalable algorithm on popular Spark framework for distributed platform and performed extensive experiments on datasets from a variety of real-time applications e.g. activity recognition, customer expenditure, etc. Furthermore, we have demonstrated that our algorithm can capture the changing distributions of real-life datasets, thus enabling real-time PCA. We have also compared the performances of distributed and non-distributed versions of our algorithm over a variety of window sizes and showed that our distributed algorithm is scalable and performs better when window size and target dimension increase.}, location = {Dhaka, Bangladesh}, series = {7th NSysS 2020}, pages = {89\u201399}, numpages = {11}, keywords = {Real-time Processing, Distributed Algorithm., Big Data Analytics}}
@inproceedings{10.1109/CCGrid.2015.138,title = {Understanding unsuccessful executions in big-data systems}, author = {Ros\u00e0 Andrea , Chen Lydia Y. , Binder Walter },year = {2015}, isbn = {9781479980062}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2015.138}, doi = {10.1109/CCGrid.2015.138}, abstract = {Big-data applications are being increasingly used in today's large-scale datacenters for a large variety of purposes, such as solving scientific problems, running enterprise services, and computing data-intensive tasks. Due to the growing scale of these systems and the complexity of running applications, jobs running in big-data systems experience unsuccessful terminations of different nature. While a large body of existing studies sheds light on failures occurred in large-scale datacenters, the current literature overlooks the characteristics and the performance impairment of a broader class of unsuccessful executions which can arise due to application failures, dependency violations, machine constraints, job kills, and task preemption. Nonetheless, deepening our understanding in this field is of paramount importance, as unsuccessful executions can lower user satisfaction, impair reliability, and lead to a high resource waste. In this paper, we describe the problem of unsuccessful executions in big-data systems, and highlight the critical importance of improving our knowledge on this subject. We review the existing literature on this field, discuss its limitations, and present our own contributions to the problem, along with our research plan for the future.}, location = {Shenzhen, China}, series = {CCGRID '15}, pages = {741\u2013744}, numpages = {4}}
@inproceedings{10.5555/3204979.3204985,title = {Automatic text summarization within big data frameworks}, author = {Mackey Andrew , Cuevas Israel },year = {2018}, publisher = {Consortium for Computing Sciences in Colleges}, address = {Evansville, IN, USA}, abstract = {Data increasing at unabated rates will prove to be challenging for individuals trying to effectively leverage it. This paper describes an approach to automatically summarize text documents by utilizing statistical and information retrieval methodologies within big data frameworks that support the distributed parallel processing of data. A pedagogical approach to the incorporation of automatic summarization and big data topics within data mining, machine learning, or natural language processing courses is detailed.}, pages = {26\u201332}, numpages = {7}}
@inproceedings{10.1145/3321454.3321474,title = {Research on the Construction of Big Data Trading Platform in China}, author = {Yu Bangbo , Zhao Haijun },year = {2019}, isbn = {9781450366335}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3321454.3321474}, doi = {10.1145/3321454.3321474}, abstract = {As a new type of asset, the value of big data resources can only be realized in the transaction circulation. Establishing and improving the big data trading platform market system is a systematic project to transform data from resources into assets. Through the comparative analysis of several typical big data trading platform construction practices in China, this research finds some problems, such as unclear positioning of some platforms leading to overlapping functions, extensive data transaction, lack of unified data pricing methods, unclear data ownership. In addition, there is no difference between the data escrow transaction mode and the aggregate transaction mode, and the rights of the data supply parties cannot be guaranteed. And it also discusses how to promote the construction and improvement of China's big data trading market more systematically, normally and institutionally. Finally, it proposes to build local big data trading platforms according to local conditions, establish a data transaction system based on blockchain, establish a big data transaction pricing index system, and establish a big data standard system.}, location = {Da, Nang, Viet Nam}, series = {ICIIT '19}, pages = {107\u2013112}, numpages = {6}, keywords = {Data assets, big data trading platform, regulatory construction}}
@inproceedings{10.5555/2483628.2483630,title = {Mining big data streams: the fallacy of blind correlation and the importance of models}, author = {Abbass Hussein },year = {2011}, isbn = {9781921770029}, publisher = {Australian Computer Society, Inc.}, address = {AUS}, abstract = {Big data streams mark a new era in artificial intelligence and the data mining literature. Video and voice streams have grown rapidly in recent years. A single lab--based human--computer interaction experiment with one human subject collecting Cognitive, Physiological, and other data can easily generate a few terabytes of data in a single hour; growing rapidly to a Petabyte within a timeframe less than a month. In an article in the Wired Magazine, 2008, by Chris Anderson, he wrote \"the data deluge makes the scientific method obsolete\" He predicted that in the age of Petabyte and beyond, a meaningful correlation analysis is enough! Chris comment was provocative; but some started believing it. So was Chris right or wrong? Why? What can we do to face the outburst of big data? Do we have the data mining tools to manage these data? Where is the future of data mining heading? In this talk, I will discuss the above questions and demonstrate some answers using examples of my work and analysis.}, location = {Ballarat, Australia}, series = {AusDM '11}, pages = {5\u20136}, numpages = {2}}
@inproceedings{10.1145/2903150.2917755,title = {Big data analytics and the LHC}, author = {Girone Maria },year = {2016}, isbn = {9781450341288}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2903150.2917755}, doi = {10.1145/2903150.2917755}, abstract = {The Large Hadron Collider is one of the largest and most complicated pieces of scientific apparatus ever constructed. The detectors along the LHC ring see as many as 800 million proton-proton collisions per second. An event in 10 to the 11th power is new physics and there is a hierarchical series of steps to extract a tiny signal from an enormous background. High energy physics (HEP) has long been a driver in managing and processing enormous scientific datasets and the largest scale high throughput computing centers. HEP developed one of the first scientific computing grids that now regularly operates 500k processor cores and half of an exabyte of disk storage located on 5 continents including hundred of connected facilities. In this presentation I will discuss the techniques used to extract scientific discovery from a large and complicated dataset. While HEP has developed many tools and techniques for handling big datasets, there is an increasing desire within the field to make more effective use of additional industry developments. I will discuss some of the ongoing work to adopt industry techniques in big data analytics to improve the discovery potential of the LHC and the effectiveness of the scientists who work on it.}, location = {Como, Italy}, series = {CF '16}, pages = {ii}}
@inproceedings{10.1145/2839509.2850516,title = {Enhancing Teaching of Big Data by Using Real World Datasets}, author = {Nagar Anurag },year = {2016}, isbn = {9781450336857}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2839509.2850516}, doi = {10.1145/2839509.2850516}, abstract = {This lightning talk will focus on our experience of teaching a graduate level Big Data course. Traditionally, such courses have relied on \"WordCount\" style problems, which involve computing the simple count of words in a corpus of documents using the distributed MapReduce framework. While this is certainly a good way of introducing the students to the BigData framework, more real world examples are needed to motivate students. Further, since a majority of courses require students to work on a large project as part of this course, it is essential that they have access to a diverse and interesting set of data. In our course, we experimented with various data sources, such as text from real-time, streaming news articles, twitter feeds, and property price data from various zip codes in a county. The students were involved in gathering the data, designing and implementing MapReduce style algorithms for distributed processing, and presenting their findings. The feedback was extremely positive and we would like to develop this approach further. In this talk, we will present some ideas on how to collect and analyze real world datasets that are suitable for Big Data analysis. We would also encourage further inputs from the audience about this topic.}, location = {Memphis, Tennessee, USA}, series = {SIGCSE '16}, pages = {496}, numpages = {1}, keywords = {distributed computing teaching, datasets for teaching big data, big data teaching}}
@inproceedings{10.1145/2896387.2900334,title = {Secure and Efficient Big Data Gathering in Heterogeneous Wireless Sensor Networks}, author = {Boubiche Djallel Eddine },year = {2016}, isbn = {9781450340632}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2896387.2900334}, doi = {10.1145/2896387.2900334}, abstract = {The emergence of heterogeneous wireless sensor networks in recent years has helped to address the limitations of conventional WSNs resources and has opened new research areas. When more than one type of node is integrated in a WSN, it is called heterogeneous. Placing heterogeneous nodes in a WSN is an effective way to increase the life of the network. While most of the existing civil and military applications of the heterogeneous WSNs are not materially different from their homogeneous counterparts, there are compelling reasons of incorporating the heterogeneity in the network. These reasons include: improving scalability of wireless sensor networks, reducing energy requirements without sacrificing performance, the equilibration of the cost and the network functionality, improving the security mechanisms using more complex protocols and supporting new broadband applications and big data.Recently, and because of the growth of the amount of data transmitted in the heterogeneous sensor networks, the term big data has emerged as a widely recognized trend. The term Big Data does not only concern the volume of data but also the high-speed transmission and the variety of information which are difficult to collect, store, and process using available technologies. Although the data generated by the individual sensors may not appear to be significant, all the data generated through the many sensors are capable of producing large volumes of data. The management of big data imposes additional constraints on WSNs. Effectively manage and secure big data gathering is a challenge in heterogeneous WSNs. Therefore, it represents an interesting research area. In this talk, I will address the emerging big data concept in heterogeneous wireless sensor networks and point out its main research issues.}, location = {Cambridge, United Kingdom}, series = {ICC '16}, pages = {1}, numpages = {1}, keywords = {Heterogeneous Sensor Networks, Secure and efficient data gathering, Big data}}
@inproceedings{10.14778/2367502.2367562,title = {Efficient big data processing in Hadoop MapReduce}, author = {Dittrich Jens , Quian\u00e9-Ruiz Jorge-Arnulfo },year = {2012}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2367502.2367562}, doi = {10.14778/2367502.2367562}, abstract = {This tutorial is motivated by the clear need of many organizations, companies, and researchers to deal with big data volumes efficiently. Examples include web analytics applications, scientific applications, and social networks. A popular data processing engine for big data is Hadoop MapReduce. Early versions of Hadoop MapReduce suffered from severe performance problems. Today, this is becoming history. There are many techniques that can be used with Hadoop MapReduce jobs to boost performance by orders of magnitude. In this tutorial we teach such techniques. First, we will briefly familiarize the audience with Hadoop MapReduce and motivate its use for big data processing. Then, we will focus on different data management techniques, going from job optimization to physical data organization like data layouts and indexes. Throughout this tutorial, we will highlight the similarities and differences between Hadoop MapReduce and Parallel DBMS. Furthermore, we will point out unresolved research problems and open issues.}, pages = {2014\u20132015}, numpages = {2}}
@inproceedings{10.1145/2287076.2287078,title = {Putting a \"big-data\" platform to good use: training kinect}, author = {Budiu Mihai },year = {2012}, isbn = {9781450308052}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2287076.2287078}, doi = {10.1145/2287076.2287078}, location = {Delft, The Netherlands}, series = {HPDC '12}, pages = {1\u20132}, numpages = {2}, keywords = {big data, kinect, parallel computing, dryadlinq, linq}}
@inproceedings{10.1109/CCGrid.2015.122,title = {HPC-ABDS high performance computing enhanced apache big data stack}, author = {Fox Geoffrey C. , Qiu Judy , Kamburugamuve Supun , Jha Shantenu , Luckow Andre },year = {2015}, isbn = {9781479980062}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2015.122}, doi = {10.1109/CCGrid.2015.122}, abstract = {We review the High Performance Computing Enhanced Apache Big Data Stack HPC-ABDS and summarize the capabilities in 21 identified architecture layers. These cover Message and Data Protocols, Distributed Coordination, Security & Privacy, Monitoring, Infrastructure Management, DevOps, Interoperability, File Systems, Cluster & Resource management, Data Transport, File management, NoSQL, SQL (NewSQL), Extraction Tools, Object-relational mapping, In-memory caching and databases, Inter-process Communication, Batch Programming model and Runtime, Stream Processing, High-level Programming, Application Hosting and PaaS, Libraries and Applications, Workflow and Orchestration. We summarize status of these layers focusing on issues of importance for data analytics. We highlight areas where HPC and ABDS have good opportunities for integration.}, location = {Shenzhen, China}, series = {CCGRID '15}, pages = {1057\u20131066}, numpages = {10}, keywords = {apache big data stack, HPC}}
@inproceedings{10.1145/3148453.3306250,title = {Simulation of Dark Network Scene Based on the Big Data Environment}, author = {Wang Pengpeng , Liu Hongri , Wang Bailing , Dong Kaikun , Wang Lianhai , Xu Shujiang },year = {2018}, isbn = {9781450363525}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3148453.3306250}, doi = {10.1145/3148453.3306250}, abstract = {In the era of big data, the amount of information on dark network resources has exploded. Massive dark network data contain abundant information. To detect dark network resources and obtain dark network information, in-depth understanding of the dark network is a prerequisite. However, due to the high anonymity of dark network, it is usually difficult to be found by traditional search engines. Users need to register strictly and use specific tools to log in dynamically. In this paper, we explore the simulation of dark network scene in the big data environment. The Tor network is built on the openstack platform, which simulate the dark network scene. By using wireshark software to analyze network traffic, and using nmon tool to analyze network performance, the results show that the dark network scene can be simulated realistically.}, location = {Xiamen, Fujian, China}, series = {ICITEE '18}, pages = {1\u20136}, numpages = {6}, keywords = {Big data, Tor, Scene simulation, Dark network}}
@inproceedings{10.1145/3134472.3134516,title = {Big data visual analytics: fundamentals, techniques, and tools}, author = {Nguyen Quang Vinh , Engelke Ulrich },year = {2017}, isbn = {9781450354035}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3134472.3134516}, doi = {10.1145/3134472.3134516}, location = {Bangkok, Thailand}, series = {SA '17}, pages = {1\u2013203}, numpages = {203}}
@inproceedings{10.1145/3234698.3234723,title = {Investigating Business Intelligence in the era of Big Data: concepts, benefits and challenges}, author = {El Bousty Hicham , krit Salah-ddine , Elasikri Mohamed , Dani Hassan , Karimi Khaoula , Bendaoud Kaoutar , Kabrane Mustapha },year = {2018}, isbn = {9781450363921}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3234698.3234723}, doi = {10.1145/3234698.3234723}, abstract = {Business intelligence suppose retrieving value from data floating in the organization environment. It provides methods and tools for collecting, storing, formatting and analyzing data for the purpose of helping managers in decision-making. At the start, only data from enterprise internal activities were examined. Now and in this turbulent business environment, organizations should incorporate analysis of the huge amount of external data gathered from multifarious sources. It is argued that BI systems accuracy depends on quantity of data at their disposal, yet some storage and analysis methods are phased out and should be reviewed by academics and practitioners.This paper presents an overview of BI challenges in the context of Big Data (BD) and some available solutions provided, either by using Cloud Computing (CC) or improving Data Warehouse (DW) efficiency.}, location = {Istanbul, Turkey}, series = {ICEMIS '18}, pages = {1\u20139}, numpages = {9}, keywords = {Data Warehouse, Big Data, Cloud Computing, Business Intelligence}}
@inproceedings{10.1109/CHASE.2017.81,title = {Big data techniques for public health: a case study}, author = {Katsis Yannis , Balac Natasha , Chapman Derek , Kapoor Madhur , Block Jessica , Griswold William G. , Huang Jeannie , Koulouris Nikos , Menarini Massimiliano , Nandigam Viswanath , Ngo Mandy , Ong Kian Win , Papakonstantinou Yannis , Smith Besa , Zarifis Konstantinos , Woolf Steven , Patrick Kevin },year = {2017}, isbn = {9781509047215}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CHASE.2017.81}, doi = {10.1109/CHASE.2017.81}, abstract = {Public health researchers increasingly recognize that to advance their field they must grapple with the availability of increasingly large (i.e., thousands of variables) traditional population-level datasets (e.g., electronic medical records), while at the same time integrating additional large datasets (e.g., data on genomics, the microbiome, environmental exposures, socioeconomic factors, and health behaviors). Leveraging these multiple forms of data might well provide unique and unexpected discoveries about the determinants of health and wellbeing. However, we are in the very early stages of advancing the techniques required to understand and analyze big population-level data for public health research.To address this problem, this paper describes how we propose that big data can be efficiently used for public health discoveries. We show that data analytics techniques traditionally employed in public health studies are not up to the task of the data we now have in hand. Instead we present techniques adapted from big data visualization and analytics approaches used in other domains that can be used to answer important public health questions utilizing these existing and new datasets. Our findings are based on an exploratory big data case study carried out in San Diego County, California where we analyzed thousands of variables related to health to gain interesting insights on the determinants of several health outcomes, including life expectancy and anxiety disorders. These findings provide a promising early indication that public health research will benefit from the larger set of activities in contemporary big data research.}, location = {Philadelphia, Pennsylvania}, series = {CHASE '17}, pages = {222\u2013231}, numpages = {10}}
@inproceedings{10.1145/3407666,title = {Session details: Big Data Management}, author = {Lofstead Jay },year = {2020}, isbn = {9781450370523}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3407666}, doi = {10.1145/3407666}, location = {Stockholm, Sweden}, series = {HPDC '20}, pages = {}}
@inproceedings{10.1145/3253877,title = {Session details: Big Data Algorithms}, author = {Lempel Ronny },year = {2016}, isbn = {9781450337168}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3253877}, doi = {10.1145/3253877}, location = {San Francisco, California, USA}, series = {WSDM '16}, pages = {}}
@inproceedings{10.1145/3127479.3129253,title = {Exploring memory locality for big data analytics in virtualized clusters}, author = {Hwang Eunji , Kim Hyungoo , Nam Beomseok , Choi Young-ri },year = {2017}, isbn = {9781450350280}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3127479.3129253}, doi = {10.1145/3127479.3129253}, abstract = {In this work, we investigate techniques to improve the performance of big data analytics in virtualized clusters by effectively increasing the utilization of cached data and efficiently using scarce memory resources.}, location = {Santa Clara, California}, series = {SoCC '17}, pages = {657}, numpages = {1}, keywords = {hadoop, memory locality, cloud computing, big data analytics frameworks}}
@inproceedings{10.1145/2872518.2890583,title = {Web Communities in Big Data Era. Editorial}, author = {Maret Pierre , Akerkar Rajendra , Vercouter Laurent },year = {2016}, isbn = {9781450341448}, publisher = {International World Wide Web Conferences Steering Committee}, address = {Republic and Canton of Geneva, CHE}, url = {https://doi.org/10.1145/2872518.2890583}, doi = {10.1145/2872518.2890583}, abstract = {Web-based community is a self-defined web-based network of interactive communication organized around a shared interest or purpose. It provides the means of interactions among people in which they create, share, and exchange information and ideas in virtual space and networks. Working with big data often requires querying and reasoning that data to isolate information of interest and manipulate it in various ways. This editorial paper explores recent big data research topics -- stream querying and reasoning -- over data from web based communities. It combines aspects from some well-studied research domains, such as, social network analysis, graph databases, and data streams. We provide a brief synopsis of some research issues in supporting reasoning and querying tasks. This editorial also presents the WI&C-16 workshop's goal and programme.}, location = {Montr\u00e9al, Qu\u00e9bec, Canada}, series = {WWW '16 Companion}, pages = {945\u2013947}, numpages = {3}, keywords = {datastream, querying, web communities, and reasoning}}
@inproceedings{10.1145/2664591.2664619,title = {Intrinsic Relations between Data Science, Big Data, Business Analytics and Datafication}, author = {Ayankoya Kayode , Calitz Andre , Greyling Jean },year = {2014}, isbn = {9781450332460}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2664591.2664619}, doi = {10.1145/2664591.2664619}, abstract = {Data recording and storage have evolved over the past decades from manual gathering of data by using simple writing materials to the automation of data collection. Data storage has evolved significantly in the past decades and today databases no longer suffice as the only medium for the storage and management of data. This is due to the emergence of the Big Data and Data Science concepts. Previous studies have indicated that the multiplication of processing power of computers and the availability of larger data storage at reduced cost are part of the catalysts for the volume and rate at which data is now made available and captured.In this paper, the concepts of Big Data, Data Science and Business Analytics are reviewed. This paper discusses datafication of different aspects of life as the fundamental concept behind the growth of Big Data and Data Science. A review of the characteristics and value of Big Data and Data Science suggests that these emerging concepts will bring a paradigm change to a number of areas. Big Data was described as the basis for Data Science and Business Analytics which are tools employed in Data Science. Because these fields are still developing, there are diverse opinions, especially on the definition of Data Science. This paper provides a revised definition of Data Science, based on the review of available literature and proposes a schematic representation of the concepts.}, location = {Centurion, South Africa}, series = {SAICSIT '14}, pages = {192\u2013198}, numpages = {7}, keywords = {Datafication, Big Data, Business Analytics, Business Intelligence, Data Science}}
@inproceedings{10.1145/3252644,title = {Session details: Exploiting Big Data}, author = {Graus Mark },year = {2017}, isbn = {9781450349055}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3252644}, doi = {10.1145/3252644}, location = {Limassol, Cyprus}, series = {HUMANIZE '17}, pages = {}}
@inproceedings{10.1145/2737817.2737829,title = {Exploring Big Data with Helix: Finding Needles in a Big Haystack}, author = {Ellis Jason , Fokoue Achille , Hassanzadeh Oktie , Kementsietsidis Anastasios , Srinivas Kavitha , Ward Michael J. },year = {2015}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2737817.2737829}, doi = {10.1145/2737817.2737829}, abstract = {While much work has focused on efficient processing of Big Data, little work considers how to understand them. In this paper, we describe Helix, a system for guided exploration of Big Data. Helix provides a unified view of sources, ranging from spreadsheets and XML files with no schema, all the way to RDF graphs and relational data with well-defined schemas. Helix users explore these heterogeneous data sources through a combination of keyword searches and navigation of linked web pages that include information about the schemas, as well as data and semantic links within and across sources. At a technical level, the paper describes the research challenges involved in developing Helix, along with a set of real-world usage scenarios and the lessons learned.}, pages = {43\u201354}, numpages = {12}}
@inproceedings{10.1145/3289430.3289431,title = {Study on Urban Microclimate Based on API System on the Background of Big Data: A Case Study of Beijing}, author = {Yang Xin , Lu Xinsheng , Geng Chao },year = {2018}, isbn = {9781450365192}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3289430.3289431}, doi = {10.1145/3289430.3289431}, abstract = {With the rapid development of data and information, API system provided massive data downloading platform for various industries, which provided opportunities and challenges for urban microclimate research. This paper briefly introduced the research and application status of related data of API system and utilization of various data. Taking Beijing as an example, the paper extracted and analyzed meteorological station data in the city area, including temperature, relative humidity, solar radiation, wind speed and pressure. Interpolation analysis was carried out on the ArcGIS platform to get the climate distribution characteristics in a certain day in summer. Then the paper analyzed influence of transportation infrastructure distribution density on temperature, relative humidity, wind speed and pressure, and proposed the correlation between transportation infrastructure and temperature and relative humidity and no correlation with pressure. The climate environment in the city were complex and the big data platform provided new ways and new ideas for the related research.}, location = {Beijing, China}, series = {BDIOT 2018}, pages = {3\u20136}, numpages = {4}, keywords = {city microclimate environment, API data system, Beijing}}
@inproceedings{10.1145/2789168.2802150,title = {Big Data, IoT, .... Buzz Words for Academia or Reality for Industry?}, author = {Aguiar Rui Luis , Benhabiles Nora , Pfeiffer Tobias , Rodriguez Pablo , Viswanathan Harish , Wang Jia , Zang Hui },year = {2015}, isbn = {9781450336192}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2789168.2802150}, doi = {10.1145/2789168.2802150}, abstract = {The concepts of Big Data have became intertwined with those of the Internet of Things, creating mental pictures of a fully connected, all-encompassing, cyber-physical world, where each and every object will contribute with information to a \"fully aware\" society. Academic works are presenting this as the natural evolution for our current technologies. The panel looks at these promises from the hard perspective of reality: what is being done, how much it cost, what needs to be developed, and what can be expected in the near and mid-term.}, location = {Paris, France}, series = {MobiCom '15}, pages = {550\u2013551}, numpages = {2}, keywords = {IoT, big data, industry-applications}}
@inproceedings{10.1145/3440084.3441210,title = {Analysis of Data Ownership Rights in the Big Data Era}, author = {Xiao Wei , Tu Yaqing , Wan Ping , Li Ming , Ma Jingheng },year = {2020}, isbn = {9781450388894}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3440084.3441210}, doi = {10.1145/3440084.3441210}, abstract = {For working out the problem of ownership rights of data in the Big Data era, this paper proposes an establishment method of data ownership rights based on data classification. By summarizing characteristics of big data and analyzing current main views of the data ownership rights, this proposed method is following the principles of protecting data confidentiality and acknowledging the greatest contributors. First, according to the different involvement degree of participants in data generation processes, the data is divided into two categories: participatory data and non-participatory data. The participatory data is subdivided into equal participatory data and non-equal participatory data based on different contributions of participants. Since the non-participatory data generally involves the private and confidential information of the recorded parties, it is proposed that the ownership rights of this kind of data should belong to the recorded parties. Following the principle of acknowledging the greatest contributors, this paper proposes that the ownership rights of the equal participatory data belongs to all participants and that of non-equal participatory data belongs to the active participants.}, location = {Newcastle upon Tyne, United Kingdom}, series = {ISCSIC 2020}, pages = {1\u20135}, numpages = {5}, keywords = {Big Data Ownership Rights, Non-Participatory Data, Equal Participatory Data, The Big Data Era, Non-Equal Participatory Data, Participatory Data}}
@inproceedings{10.1145/3448748.3448787,title = {Promotion of Marketing Efficiency of SMEs Based on Big Data}, author = {Guo Xuebing , Yuan Kexin },year = {2021}, isbn = {9781450390002}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3448748.3448787}, doi = {10.1145/3448748.3448787}, abstract = {Small and medium-sized enterprises (SMEs) are an important part of the national economic system, which can promote employment, improve people's living standards, and promote economic development and technological innovation. In the new business environment based on big data and artificial intelligence, it is a key issue for SMEs to flexibly use big data services, design competitive marketing strategies, and achieve efficient market management. This paper will explore the impact of marketing environment changes on SMEs in the era of big data, and improve the marketing efficiency of SMEs by establishing relevant models based on big data analysis.}, location = {Harbin, China}, series = {BIC 2021}, pages = {244\u2013249}, numpages = {6}, keywords = {Precision marketing, Marketing effectiveness, Big data analysis, Small and medium-sized enterprises (SMEs)}}
@inproceedings{10.1145/3331453.3361308,title = {Construction and Implementation of Big Data Framework for Crop Germplasm Resources}, author = {Jing Furong , Cao Yongsheng , Fang Wei , Chen Yanqing },year = {2019}, isbn = {9781450362948}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3331453.3361308}, doi = {10.1145/3331453.3361308}, abstract = {Based on understanding the application of big data and the research status of crop germplasm resources, this paper proposes a system architecture that is suitable for crop germplasm resources big data. Among them, the overall architecture of germplasm resources is elaborated through six functional modules, including data source, data integration, data processing, data application, big data operation and maintenance platform, and data management and security. The logical functional architecture specification was formulated and the technical implementation and selection are defined. The technical implementation framework describes the technical implementation of germplasm resources big data, and jointly supports the construction and operation of germplasm resources big data. Finally, a verification system is established to verify the feasibility of the big data system framework for germplasm resources.}, location = {Sanya, China}, series = {CSAE 2019}, pages = {1\u20137}, numpages = {7}, keywords = {Data management, Crop germplasm resources, Data analysis, Big data architecture}}
@inproceedings{10.1145/3403951,title = {Density-based Algorithms for Big Data Clustering Using MapReduce Framework: A Comprehensive Study}, author = {Khader Mariam , Al-Naymat Ghazi },year = {2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3403951}, doi = {10.1145/3403951}, abstract = {Clustering is used to extract hidden patterns and similar groups from data. Therefore, clustering as a method of unsupervised learning is a crucial technique for big data analysis owing to the massive number of unlabeled objects involved. Density-based algorithms have attracted research interest, because they help to better understand complex patterns in spatial datasets that contain information about data related to co-located objects. Big data clustering is a challenging task, because the volume of data increases exponentially. However, clustering using MapReduce can help answer this challenge. In this context, density-based algorithms in MapReduce have been largely investigated in the past decade to eliminate the problem of big data clustering. Despite the diversity of the algorithms proposed, the field lacks a structured review of the available algorithms and techniques for desirable partitioning, local clustering, and merging. This study formalizes the problem of density-based clustering using MapReduce, proposes a taxonomy to categorize the proposed algorithms, and provides a systematic and comprehensive comparison of these algorithms according to the partitioning technique, type of local clustering, merging technique, and exactness of their implementations. Finally, the study highlights outstanding challenges and opportunities to contribute to the field of density-based clustering using MapReduce.}, pages = {1\u201338}, numpages = {38}, keywords = {clustering, Big data, mapreduce framework, density clustering}}
@inproceedings{10.1145/3299902.3311063,title = {FPGA-based Computing in the Era of AI and Big Data}, author = {Nurvitadhi Eriko },year = {2019}, isbn = {9781450362535}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3299902.3311063}, doi = {10.1145/3299902.3311063}, abstract = {The continued rapid growth of data, along with advances in Artificial Intelligence (AI) to extract knowledge from such data, is reshaping the computing ecosystem landscape. With AI becoming an essential part of almost every end-user application, our current computing platforms are facing several challenges. The data-intensive nature of current AI models requires minimizing data movement. Furthermore, interactive intelligent datacenter-scale services require scalable and real-time solutions to provide a compelling user experience. Finally, algorithmic innovations in AI demand a flexible and programmable computing platform that can keep up with this rapidly changing field. We believe that these trends and their accompanying challenges present tremendous opportunities for FPGAs. FPGAs are a natural substrate to provide a programmable, near-data, real-time, and scalable platform for AI analytics. FPGAs are already embedded in several places where data flows throughout the computing ecosystem (e.g., \"smart\" network/storage, near image/audio sensors). Intel FPGAs are System-in-Package (SiP), scalable with 2.5D chiplets. They are also scalable at datacenter-scale as reconfigurable cloud, enabling real-time AI services. Using overlays, FPGAs can be programmed through software without needing long-running RTL synthesis. With further innovations, and leveraging their existing strengths, FPGAs can leap forward to realize their true potentials in AI analytics. In this talk, we first discuss the current trends in AI and big data. We then present trends in FPGA and opportunities for FPGAs in the era of AI and big data. Finally, we highlight selected research efforts to seize some of these opportunities: (1) 2.5D SiP integration of FPGA and AI chiplets to improve the performance and efficiency of AI workloads, and (2) AI overlay for FPGA to facilitate software-level programmability and compilation-speed.}, location = {San Francisco, CA, USA}, series = {ISPD '19}, pages = {35}, numpages = {1}, keywords = {fpga, artificial intelligence, big data}}
@inproceedings{10.1145/3231830.3231840,title = {Information Management Technologies for Big Data: A Case of Oracle}, author = {Kaloyanova Kalinka , Hristov Tsvetomir , Naydenova Ina , Kovacheva Zlatinka },year = {2017}, isbn = {9781450353106}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3231830.3231840}, doi = {10.1145/3231830.3231840}, abstract = {During the last decade the volume, the rate of accumulation and the diversity of data in general have been steadily increasing, which leads to the rapid development of Big data and technological enhancements associated with it. Many leading companies in the area took the challenge and provided different solutions to manage Big data. New hardware and software technologies are introduced for information management. The paper analyzes the main Big data technologies provided by Oracle as well their implementation in several specific cases.}, location = {Paris, France}, series = {AWICT 2017}, pages = {1\u20134}, numpages = {4}, keywords = {Database, RDBMS, NoSQL, MapReduce}}
@inproceedings{10.1145/2783258.2788563,title = {Big Data System for Analyzing Risky Procurement Entities}, author = {Dhurandhar Amit , Graves Bruce , Ravi Rajesh , Maniachari Gopikrishanan , Ettl Markus },year = {2015}, isbn = {9781450336642}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2783258.2788563}, doi = {10.1145/2783258.2788563}, abstract = {An accredited biennial 2014 study by the Association of Certified Fraud Examiners claims that on average 5% of a company's revenue is lost because of unchecked fraud every year. The reason for such heavy losses are that it takes around 18 months for a fraud to be caught and audits catch only 3% of the actual fraud. This begs the need for better tools and processes to be able to quickly and cheaply identify potential malefactors. In this paper, we describe a robust tool to identify procurement related fraud/risk, though the general design and the analytical components could be adapted to detecting fraud in other domains. Besides analyzing standard transactional data, our solution analyzes multiple public and private data sources leading to wider coverage of fraud types than what generally exists in the marketplace. Moreover, our approach is more principled in the sense that the learning component, which is based on investigation feedback has formal guarantees. Though such a tool is ever evolving, a deployment of this tool over the past 12 months has found many interesting cases from compliance risk and fraud point of view across more than 150 countries and 65000+ vendors, increasing the number of true positives found by over 80\\% compared with other state-of-the-art tools that the domain experts were previously using.}, location = {Sydney, NSW, Australia}, series = {KDD '15}, pages = {1741\u20131750}, numpages = {10}, keywords = {fraud, risk, big data, online learning, procurement, social network, collusion}}
@inproceedings{10.1145/2567948.2577274,title = {Learning to efficiently rank on big data}, author = {Wang Lidan , Lin Jimmy , Metzler Donald , Han Jiawei },year = {2014}, isbn = {9781450327459}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2567948.2577274}, doi = {10.1145/2567948.2577274}, abstract = {Ranking in response to user queries is a central problem in information retrieval, data mining, and machine learning. In the era of \"Big data\", traditional effectiveness-centric ranking techniques tend to get more and more costly (requiring additional hardware and energy costs) to sustain reasonable ranking speed on large data. The mentality of combating big data by throwing in more hardware/machines will quickly become highly expensive since data is growing at an extremely fast rate oblivious to any cost concerns from us. \"Learning to efficiently rank\" offers a cost-effective solution to ranking on large data (e.g., billions of documents). That is, it addresses a critically important question -- whether it is possible to improve ranking effectiveness on large data without incurring (too much) additional cost?}, location = {Seoul, Korea}, series = {WWW '14 Companion}, pages = {209\u2013210}, numpages = {2}, keywords = {efficiency, effectiveness}}
@inproceedings{10.1145/2024587.2024599,title = {A process for assessing data quality}, author = {Sneed Harry M. , Majnar Rudolf },year = {2011}, isbn = {9781450308519}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2024587.2024599}, doi = {10.1145/2024587.2024599}, abstract = {Abstract: This industrial report stems from practical experience in assessing the quality of customer databases. The process it describes unites three automated audits, - an audit of the database schema, an audit of the database structure and an audit of the database content. The audit of the database schema checks for design smells and rule violations. The audit of the database structure measures the size, complexity and quality of the database model. The audit of the database content processes the data itself to uncover invalid data values, missing records and redundant records. The purpose of these audits is to assess the quality of the database and to determine whether a data reengineering or data clean-up project is required.}, location = {Szeged, Hungary}, series = {WoSQ '11}, pages = {50\u201357}, numpages = {8}, keywords = {data content validation, data auditing, data quality, data metrics}}
@inproceedings{10.1145/3134271.3134296,title = {Establishment of Business Intelligence and Big Data Analysis for Higher Education}, author = {Peng Michael Yao-Ping , Tuan Sheng-Hwa , Liu Feng-Chi },year = {2017}, isbn = {9781450352765}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3134271.3134296}, doi = {10.1145/3134271.3134296}, abstract = {The applications on business intelligence and big data analysis to extract useful information are getting more mature, but the development and operations in higher education institutions are still be lack. This study aims to explore how HEIs employ business intelligence to analysis and mining student learning and HEIs' operation data from database. The outcomes are benefit for universities to support the management of decision-making.}, location = {Bei Jing, China}, series = {ICBIM 2017}, pages = {121\u2013125}, numpages = {5}, keywords = {Big data, Database, Business Intelligence, Institutional Research}}