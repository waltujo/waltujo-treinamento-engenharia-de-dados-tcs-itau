@inproceedings{10.1145/3341620.3341629,
author = {El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi},
title = {Big Data Quality Metrics for Sentiment Analysis Approaches},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341629},
doi = {10.1145/3341620.3341629},
abstract = {In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {36–43},
numpages = {8},
keywords = {Sentiment analysis, Big data quality metrics, Big data},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3419604.3419803,
author = {Reda, Oumaima and Sassi, Imad and Zellou, Ahmed and Anter, Samir},
title = {Towards a Data Quality Assessment in Big Data},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419803},
doi = {10.1145/3419604.3419803},
abstract = {In recent years, as more and more data sources have become available and the volumes of data potentially accessible have increased, the assessment of data quality has taken a central role whether at the academic, professional or any other sector. Given that users are often concerned with the need to filter a large amount of data to better satisfy their requirements and needs, and that data analysis can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality, it makes everyone wonder what the results of these analyses will really be like. However, there is a very complex process involved in the identification of new, valid, potentially useful and meaningful data from a large data collection and various information systems, and is critically dependent on a number of measures to be developed to ensure data quality. To this end, the main objective of this paper is to introduce a general study on data quality related with big data, by providing what other researchers came up with on that subject. The paper will be finalized by a comparative study between the different existing data quality models.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {16},
numpages = {6},
keywords = {Data Quality, Quality Models, Big Data, Data Quality evaluation},
location = {Rabat, Morocco},
series = {SITA'20}
}

@inproceedings{10.1145/3281022.3281026,
author = {Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and Rivas Garcia, Bibiano and Piattini, Mario},
title = {From Big Data to Smart Data: A Data Quality Perspective},
year = {2018},
isbn = {9781450360548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281022.3281026},
doi = {10.1145/3281022.3281026},
abstract = {Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company’s core business data, using typically large datasets. However, data that doesn’t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI…). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to “smartizing” data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Ensemble-Based Software Engineering},
pages = {19–24},
numpages = {6},
keywords = {Smart Data, Data Quality, Big Data},
location = {Lake Buena Vista, FL, USA},
series = {EnSEmble 2018}
}

@inproceedings{10.1145/3010089.3010090,
author = {Emmanuel, Isitor and Stanier, Clare},
title = {Defining Big Data},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010090},
doi = {10.1145/3010089.3010090},
abstract = {As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {5},
numpages = {6},
keywords = {Big Data, Big Data characteristics, Data Quality, Data Quality Dimensions},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/2513591.2527071,
author = {Cuzzocrea, Alfredo and Sacc\`{a}, Domenico and Ullman, Jeffrey D.},
title = {Big Data: A Research Agenda},
year = {2013},
isbn = {9781450320252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513591.2527071},
doi = {10.1145/2513591.2527071},
abstract = {Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.},
booktitle = {Proceedings of the 17th International Database Engineering &amp; Applications Symposium},
pages = {198–203},
numpages = {6},
keywords = {big data posting, privacy of big data, big data, OLAP over big data},
location = {Barcelona, Spain},
series = {IDEAS '13}
}

@inproceedings{10.1145/2790755.2790785,
author = {Neves, Pedro and Bernardino, Jorge},
title = {Big Data Issues},
year = {2015},
isbn = {9781450334143},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790755.2790785},
doi = {10.1145/2790755.2790785},
abstract = {Big Data is a new trend regarded by both academics and business areas as an interesting concept. The paradigm includes the storage and processing of petabyte-size datasets, boosting knowledge discovery over data and providing organizations with competitive advantage over their contenders. This paper comes to provide an overview of the concept, answering questions that one has when faced with the term for the first time: What is Big Data and what are its advantages? How does it work? How is it accepted among enterprises? How to deploy a Big Data solution?},
booktitle = {Proceedings of the 19th International Database Engineering &amp; Applications Symposium},
pages = {200–201},
numpages = {2},
keywords = {Big Data, SaaS, IaaS, DBMS, PaaS},
location = {Yokohama, Japan},
series = {IDEAS '15}
}

@inproceedings{10.1145/2378356.2378368,
author = {Baru, Chaitan and Bhandarkar, Milind and Nambiar, Raghunath and Poess, Meikel and Rabl, Tilmann},
title = {Big Data Benchmarking},
year = {2012},
isbn = {9781450317528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2378356.2378368},
doi = {10.1145/2378356.2378368},
abstract = {We provide a summary of the outcomes from the Workshop on Big Data Benchmarking (WBDB2012) held on May 8-9, 2012 in San Jose, CA. The workshop discussed a number of issues related to big data benchmarking definitions and benchmark processes, and was attended by 60 invitees representing 45 different organizations from industry and academia. Attendees were selected based on their experience and expertise in one or more areas of big data, database systems, performance benchmarking, and big data applications. The participants concluded that there exists both a need and an opportunity for defining benchmarks to capture the end-to-end aspects of big data applications. The metrics for such benchmarks would need to include metrics for performance as well as price/performance, and consider several costs including total system cost, setup cost, and energy costs. The next Workshop on Big Data Benchmarking is scheduled to be held on December 17-18, 2012 in Pune, India.},
booktitle = {Proceedings of the 2012 Workshop on Management of Big Data Systems},
pages = {39–40},
numpages = {2},
keywords = {mbds},
location = {San Jose, California, USA},
series = {MBDS '12}
}

@inproceedings{10.1145/2383276.2383278,
author = {Novikov, Boris and Vassilieva, Natalia and Yarygina, Anna},
title = {Querying Big Data},
year = {2012},
isbn = {9781450311939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2383276.2383278},
doi = {10.1145/2383276.2383278},
abstract = {The term "Big Data" became a buzzword and is widely used in both research and industrial worlds. Typically the concept of big data assumes a variety of different sources of information and velocity of complex analytical processing, rather than just a huge and growing volume of data. All variety, velocity, and volume create new research challenges, as nearly all techniques and tools commonly used in data processing have to be re-considered. Variety and uncertainty of big data require a mixture of exact and similarity search and grouping of complex objects based on different attributes. High-level declarative query languages are important in this context due to expressiveness and potential for optimization.In this talk we are mostly interested in an algebraic layer for complex query processing which resides between user interface (most likely, graphical) and execution engine in layered system architecture. We analyze the applicability of existing models and query languages. We describe a systematic approach to similarity handling of complex objects, simultaneous application of different similarity measures and querying paradigms, complex searching and querying, combined semi-structured and unstructured search. We introduce the adaptive abstract operations based on the concept of fuzzy set, which are needed to support uniform handling of different kinds of similarity processing. To ensure an efficient implementation, approximate algorithms with controlled quality are required to enable quality versus performance trade-off for timeliness of similarity processing. Uniform and adaptive operations enable high-level declarative definition of complex queries and provide options for optimization.},
booktitle = {Proceedings of the 13th International Conference on Computer Systems and Technologies},
pages = {1–10},
numpages = {10},
keywords = {computer systems and technologies, query processing, query languages, big data},
location = {Ruse, Bulgaria},
series = {CompSysTech '12}
}

@article{10.14778/2536222.2536253,
author = {Dong, Xin Luna and Srivastava, Divesh},
title = {Big Data Integration},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536222.2536253},
doi = {10.14778/2536222.2536253},
abstract = {The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data.BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This tutorial explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1188–1189},
numpages = {2}
}

@inproceedings{10.1145/2611040.2611042,
author = {Benjamins, V. Richard},
title = {Big Data: From Hype to Reality?},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611042},
doi = {10.1145/2611040.2611042},
abstract = {The traditional world of relational databases and enterprise data warehouses is being challenged by growth in data volumes, the rise of unstructured and semi-structured data, and the desire to extract more valuable business insights. In order to remain competitive: we are entering the world of 'BIG DATA'. Scale-out, commodity hardware-based solutions based on the map-reduce programming model for parallel processing on large hardware are emerging to address these BIG DATA requirements that have challenged traditional technologies. The focus of this talk is on the potential business value to be created in this area by describing the opportunities and risks arising from the recent emergence of BIG DATA Analytics technology for companies. The role businesses can play in BIG DATA is also under discussion, and finally Telefonica's experience is explained in applying BIG DATA technology, both internally for enhancement of its own business processes and externally, where we are applying the technology to benefit our customers directly.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {2},
numpages = {2},
keywords = {Analytics, Big Data, Business},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@article{10.1145/3158352,
author = {Johnson, Jeffrey and Denning, Peter and Delic, Kemal A. and Sousa-Rodrigues, David},
title = {Big Data: Big Data or Big Brother? That is the Question Now.},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2018},
number = {August},
url = {https://doi.org/10.1145/3158352},
doi = {10.1145/3158352},
abstract = {This ACM Ubiquity Symposium presented some of the current thinking about big data developments across four topical dimensions: social, technological, application, and educational. While 10 articles can hardly touch the expanse of the field, we have sought to cover the most important issues and provide useful insights for the curious reader. More than two dozen authors from academia and industry provided shared their points of view, their current focus of interest and their outlines of future research. Big digital data has changed and will change the world in many ways. It will bring some big benefits in the future, but combined with big AI and big IoT devices creates several big challenges. These must be carefully addressed and properly resolved for the future benefit of humanity.},
journal = {Ubiquity},
month = {aug},
articleno = {2},
numpages = {10}
}

@inproceedings{10.1145/2485732.2485757,
author = {Gross, Thomas},
title = {Big Data: Little Software?},
year = {2013},
isbn = {9781450321167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485732.2485757},
doi = {10.1145/2485732.2485757},
abstract = {The steps of accessing, storing, and transmitting "Big Data" raise many interesting problems. But big data sets also amplify any system or software inefficiencies when large data sets require processing. So the efficiency of the generate code (and the runtime system) is crucial if we want to see widespread use of applications based on big data.Adaptive software exploits platform and data properties to custom-tailor program executions to the current environment. However, modern platforms have many features that make it difficult to support adaptive software. Multi-core systems with a non-uniform memory architecture expose various asymmetries and complicate the runtime system's task of data management, yet even modest multi-processors exhibit NUMA properties. Processor features like prefetchers are difficult to model by a compiler and may influence the execution in unexpected ways. Finally, performance monitoring units are supposed to allow a (just-in-time) compiler to obtain the information needed to adapt the generated code. But current performance monitoring units are incomplete and, worse, subject to change over time. An adaptive software system needs performance data that is readily available, reliable, and stable.In this talk I will discuss our experiences in modeling modern systems and argue for portable performance monitoring units that allow higher levels of the software tool chain to rely on live performance data.},
booktitle = {Proceedings of the 6th International Systems and Storage Conference},
articleno = {23},
numpages = {1},
location = {Haifa, Israel},
series = {SYSTOR '13}
}

@article{10.1145/2614512.2629588,
author = {Lukesh, Susan S.},
title = {Big Data},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2153-2184},
url = {https://doi.org/10.1145/2614512.2629588},
doi = {10.1145/2614512.2629588},
journal = {ACM Inroads},
month = {jun},
pages = {56},
numpages = {1}
}

@article{10.1145/2331042.2331045,
author = {Cron, Andrew and Nguyen, Huy L. and Parameswaran, Aditya},
title = {Big Data},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331045},
doi = {10.1145/2331042.2331045},
journal = {XRDS},
month = {sep},
pages = {7–8},
numpages = {2}
}

@article{10.1145/3079064,
author = {CACM Staff},
title = {Big Data},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3079064},
doi = {10.1145/3079064},
journal = {Commun. ACM},
month = {may},
pages = {24–25},
numpages = {2}
}

@inproceedings{10.5555/2694476.2694478,
author = {Srivastava, Divesh},
title = {Big Data Integration},
year = {2013},
publisher = {Computer Society of India},
address = {Mumbai, Maharashtra, IND},
abstract = {The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This talk explores the progress that has been made by the data integration community in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.},
booktitle = {Proceedings of the 19th International Conference on Management of Data},
pages = {3},
numpages = {1},
location = {Ahmedabad, India},
series = {COMAD '13}
}

@inproceedings{10.5555/2694476.2694483,
author = {Ramachandra, Karthik and Sudarshan, S.},
title = {Big Data: From Querying to Transaction Processing},
year = {2013},
publisher = {Computer Society of India},
address = {Mumbai, Maharashtra, IND},
abstract = {The term Big Data has been used and abused extensively in the past few years, and means different things to different people. A commonly used notion says Big Data is about "volume" (of data), "velocity" (rate at which data is inserted/updated) and "variety" (of data types). In this tutorial, we use the term Big Data to refer to any data processing need that requires a high degree of parallelism. In other words, we focus primarily on the "volume" and "velocity" aspects.As part of this tutorial, we will cover some aspects of Big Data management, in particular scalable storage, scalable query processing, and scalable transaction processing.This is an introductory tutorial for those who are not familiar with the areas that we will be covering. The focus will be conceptual; it is not meant as a tutorial on how to use any specific system.},
booktitle = {Proceedings of the 19th International Conference on Management of Data},
pages = {10},
numpages = {1},
location = {Ahmedabad, India},
series = {COMAD '13}
}

@inproceedings{10.1145/3195528.3195533,
author = {Masabo, Emmanuel and Kaawaase, Kyanda Swaib and Sansa-Otim, Julianne},
title = {Big Data: Deep Learning for Detecting Malware},
year = {2018},
isbn = {9781450357197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195528.3195533},
doi = {10.1145/3195528.3195533},
abstract = {Malicious software, commonly known as malware are constantly getting smarter with the capabilities of undergoing self-modifications. They are produced in big numbers and widely deployed very fast through the Internet-capable devices. This is therefore a big data problem and remains challenging in the research community. Existing detection methods should be enhanced in order to effectively deal with today's malware. In this paper, we propose a novel real-time monitoring, analysis and detection approach that is achieved by applying big data analytics and machine learning in the development of a general detection model. The learnings achieved through big data render machine learning more efficient. Using the deep learning approach, we designed and developed a scalable detection model that brings improvement to the existing solutions. Our experiments achieved an accuracy of 97% and ROC of 0.99.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering in Africa},
pages = {20–26},
numpages = {7},
keywords = {machine learning, deep learning, malware detection, big data analytics},
location = {Gothenburg, Sweden},
series = {SEiA '18}
}

@article{10.1145/2854006.2854008,
author = {Fan, Wenfei},
title = {Data Quality: From Theory to Practice},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2854006.2854008},
doi = {10.1145/2854006.2854008},
abstract = {Data quantity and data quality, like two sides of a coin, are equally important to data management. This paper provides an overview of recent advances in the study of data quality, from theory to practice. We also address challenges introduced by big data to data quality management.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {7–18},
numpages = {12}
}

@article{10.1145/3408314,
author = {Davoudian, Ali and Liu, Mengchi},
title = {Big Data Systems: A Software Engineering Perspective},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3408314},
doi = {10.1145/3408314},
abstract = {Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {110},
numpages = {39},
keywords = {software engineering, quality assurance, Big Data, software reference architecture, Big Data systems, requirements engineering}
}

@inproceedings{10.1145/3105831.3105857,
author = {Mani, Murali and Fei, Si},
title = {Effective Big Data Visualization},
year = {2017},
isbn = {9781450352208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105831.3105857},
doi = {10.1145/3105831.3105857},
abstract = {In the last several years, big data analytics has found an increasing role in our everyday lives. Data visualization has long been accepted as an integral part of data analytics. However, data visualization systems are not equipped to handle the complexities typically found in big data. Our work examines effective ways of visualizing big data, while also realizing that most visualization processes are interactive. During an interactive visualization session, an analyst issues several visualization requests, each of which builds on prior visualizations. In our approach, we integrate a distributed data processing system that can effectively process big data with a visualization system that can provide effective interactive visualization but for smaller amounts of data. The analyst's current request is used to infer contextual information about the analyst such as their expertise and tolerance for delay. This information is used to carefully determine additional data that can be sent to the visualization system for decreasing the response time for future requests, thus providing a better experience for the analyst and increasing their productivity.},
booktitle = {Proceedings of the 21st International Database Engineering &amp; Applications Symposium},
pages = {298–303},
numpages = {6},
keywords = {Big Data, Data Analytics, Data Visualization},
location = {Bristol, United Kingdom},
series = {IDEAS '17}
}

@article{10.1145/2524248.2524253,
author = {Pflugfelder, Ehren Helmut},
title = {Big Data, Big Questions},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/2524248.2524253},
doi = {10.1145/2524248.2524253},
abstract = {One significant concern I have for the future of technical communication, a concern I often share with my students, involves the impact of "big data." Though the term is frequently used with a sneer, or at least a slightly unsettled laugh, the methods for retrieving information from large data sets are improving as I write this. One significant question the field faces is: "what new relationships will develop and what new work will technical communicators be responsible for in emergent big data projects, in coming years?"},
journal = {Commun. Des. Q. Rev},
month = {aug},
pages = {18–21},
numpages = {4}
}

@article{10.1145/3011286.3011307,
author = {Castelluccia, Daniela and Caldarola, Enrico G. and Boffoli, Nicola},
title = {Environmental Big Data: A Systematic Mapping Study},
year = {2017},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/3011286.3011307},
doi = {10.1145/3011286.3011307},
abstract = {Big data sets and analytics are increasingly being used by government agencies, non-governmental organizations, and privatecompanies to forward environmental protection. Improving energy efficiency, promoting environmental justice, tracking climate change, and monitoring water quality are just a few of the objectives being furthered by the use of Big Data. The authors provide a more detailed analysis of the emerging evidence-based insights on Environmental Big Data (EBD), by applying the well-defined method of systematic mapping. The analysis of results throws light on the current open issues of Environmental Big Data. Moreover, different facets of the study can be combined nto answer more specific research questions. The report reveals the need for more empirical research able to provide new metrics measuring efficiency and effectiveness of the proposed analytics and new methods and tools supporting data processing workflow in EBD},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {1–4},
numpages = {4},
keywords = {Data Management, Data Integration, Systematic Mapping, Environment, Big Data}
}

@inproceedings{10.1145/2811222.2811235,
author = {Abell\'{o}, Alberto},
title = {Big Data Design},
year = {2015},
isbn = {9781450337854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811222.2811235},
doi = {10.1145/2811222.2811235},
abstract = {It is widely accepted today that Relational databases are not appropriate in highly distributed shared-nothing architectures of commodity hardware, that need to handle poorly structured heterogeneous data. This has brought the blooming of NoSQL systems with the purpose of mitigating such problem, specially in the presence of analytical workloads. Thus, the change in the data model and the new analytical needs beyond OLAP take us to rethink methods and models to design and manage these newborn repositories. In this paper, we will analyze state of the art and future research directions.},
booktitle = {Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP},
pages = {35–38},
numpages = {4},
keywords = {big data, database design, nosql},
location = {Melbourne, Australia},
series = {DOLAP '15}
}

@inproceedings{10.1145/2837060.2837068,
author = {Zhou, Ming and Cao, Menglin and Park, Taeho and Pyeon, Jae-Ho},
title = {Clarifying Big Data: The Concept and Its Applications},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837068},
doi = {10.1145/2837060.2837068},
abstract = {This paper clarifies the concept of Big Data with a discussion of its managerial implications and presents its defining characteristics differentiating Big Data with traditional analytics. This paper also introduces the concept of Big Data in the context of three industries, namely, finance, supply chain and marketing and discusses how this concept can be applied in the business world. With regard to this concept, fundamental yet critical discussions were made for any further understanding of Big Data. Finally, this paper contributes to our current knowledge of Big Data by relating and contrasting Big Data to traditional analysis while presenting context specific discussions for its applications. Although technical aspects of Big Data were not covered in this paper, this paper focused on serving as a business discussion for the concept of Big Data. For future work, business contents must be related to technical capabilities and solutions in order to provide a better understanding of Big Data.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {10–13},
numpages = {4},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/2593882.2593889,
author = {Mockus, Audris},
title = {Engineering Big Data Solutions},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593889},
doi = {10.1145/2593882.2593889},
abstract = {Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles.},
booktitle = {Future of Software Engineering Proceedings},
pages = {85–99},
numpages = {15},
keywords = {Operational Data, Analytics, Data Quality, Statistics, Game Theory, Data Engineering, Data Science},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@article{10.1145/3071088.3071090,
author = {Hepworth, Katherine},
title = {Big Data Visualization: Promises &amp; Pitfalls},
year = {2017},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3071088.3071090},
doi = {10.1145/3071088.3071090},
abstract = {A few weeks ago, I was having dinner with a friend when a controversial subject came up. My friend had an extremely strong opinion about the harm caused by vaccination, and his argument went something like this: "I've seen the data. There was an infographic laying it all out." He couldn't remember specific numbers from the visualization he'd seen or the author of the article. He couldn't even remember the name of the publication, but the data visualization's overall argument was firmly lodged in his mind. His situation is not unique, and it provides telling insights on how we, as humans, perceive and respond to big data visualization.},
journal = {Commun. Des. Q. Rev},
month = {mar},
pages = {7–19},
numpages = {13}
}

@inproceedings{10.5555/2555523.2555559,
author = {Statchuk, Craig and Iles, Michael and Thomas, Fenny},
title = {Big Data and Analytics},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Business Analytics is maturing and moving towards mass adoption. The emergence of big data increases the need for innovative tools and methodologies. Of particular interest is the established Business Intelligence market segment, built on structured data and reporting. How does big data affect methodologies like ETL, modeling and report authoring? Business Intelligence is at a crossroads between less formal data analysis at scale and business imperatives like regulatory reporting that runs an enterprise. This paper highlights new technologies and services that move the methodologies of old into the data-centric world of high volume and velocity that defines the modern information landscape.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {341–343},
numpages = {3},
location = {Ontario, Canada},
series = {CASCON '13}
}

@inproceedings{10.1145/2505515.2527109,
author = {Giles, C. Lee},
title = {Scholarly Big Data: Information Extraction and Data Mining},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2527109},
doi = {10.1145/2505515.2527109},
abstract = {Collections of scholarly documents are usually not thought of as big data. However, large collections of scholarly documents often have many millions of publications, authors, citations, equations, figures, etc., and large scale related data and structures such as social networks, slides, data sets, etc. We discuss scholarly big data challenges, insights, methodologies and applications. We illustrate scholarly big data issues with examples of specialized search engines and recommendation systems that use information extraction and data mining in various areas such as computer science, chemistry, archaeology, acknowledgements, reference recommendation, collaboration recommendation, and others.},
booktitle = {Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management},
pages = {1–2},
numpages = {2},
keywords = {entity resolution, big data, information extraction, data mining, information retrieval, digital libraries},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@inproceedings{10.1145/2684822.2697027,
author = {Broder, Andrei and Adamic, Lada and Franklin, Michael and Rijke, Maarten de and Xing, Eric and Yu, Kai},
title = {Big Data: New Paradigm or "Sound and Fury, Signifying Nothing"?},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684822.2697027},
doi = {10.1145/2684822.2697027},
abstract = {The Gartner's 2014 Hype Cycle released last August moves Big Data technology from the Peak of Inflated Expectations to the beginning of the Trough of Disillusionment when interest starts to wane as reality does not live up to previous promises. As the hype is starting to dissipate it is worth asking what Big Data (however defined) means from a scientific perspective: Did the emergence of gigantic corpora exposed the limits of classical information retrieval and data mining and led to new concepts and challenges, the way say, the study of electromagnetism showed the limits of Newtonian mechanics and led to Relativity Theory, or is it all just "sound and fury, signifying nothing", simply a matter of scaling up well understood technologies? To answer this question, we have assembled a distinguished panel of eminent scientists, from both Industry and Academia: Lada Adamic (Facebook), Michael Franklin (University of California at Berkeley), Maarten de Rijke (University of Amsterdam), Eric Xing (Carnegie Mellon University), and Kai Yu (Baidu) will share their point of view and take questions from the moderator and the audience.},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
pages = {5–6},
numpages = {2},
keywords = {big data},
location = {Shanghai, China},
series = {WSDM '15}
}

@inproceedings{10.1145/3010089.3010143,
author = {Kechadi, M-Tahar},
title = {Healthcare Big Data: Challenges and Opportunities},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010143},
doi = {10.1145/3010089.3010143},
abstract = {In healthcare sector huge quantities of data about patients and their medical conditions have been gathered through clinical databases and various other healthcare processes. Currently, it records nearly all aspects of care, including patient personal information, clinical trials, hospital records, diagnosis, medication, test results, imaging data, costs, administrative reports, etc. Like in other application domains, the big data revolution holds also great promise in the area of healthcare, as the available data about individual patients is very rich, and hides crucial knowledge that can be exploited to improve patients' care while reducing its cost. For instance, in 2012 worldwide collected healthcare data was estimated to be in the region of 500 petabytes and it is expected to grow 50 times more in 2020 (25 Exabytes). Turning this massive amount of data into knowledge that can be used to identify needs, predict and prevent critical patients' conditions, and help practitioners to make rapid and accurate decisions is not only a desire but is of urgent and crucial necessity. Therefore, healthcare organisations must have the ability to manage and analyse their data in a rapid and efficient manner to answer several critical questions related to diseases, treatments, patients' behaviours, and care management. However, building such system faces huge challenges: 1) data complexity, 2) Privacy, security, ethical, legal, and social issues, and 3) Interoperability, portability, and compatibility. We will discuss all these challenges and the requirements of healthcare ecosystem. This will lead us to describe some innovative methodologies of how to build such ecosystem to face the healthcare challenges of the next decade or so.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {3},
numpages = {1},
keywords = {Ecosystems, Big data, Healthcare Data, Sensor Data, Data Analytics, System Design, Data Mining},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@article{10.1145/3150226,
author = {Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Shyu, Mei-Ling and Iyengar, S. S.},
title = {Multimedia Big Data Analytics: A Survey},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3150226},
doi = {10.1145/3150226},
abstract = {With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {10},
numpages = {34},
keywords = {indexing, multimedia analysis, multimedia databases, machine learning, mobile multimedia, 5V challenges, Big data analytics, retrieval, data mining, survey}
}

@inproceedings{10.1145/3216122.3216154,
author = {Ianni, Michele and Masciari, Elio and Mazzeo, Giuseppe M. and Zaniolo, Carlo},
title = {Efficient Big Data Clustering},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216154},
doi = {10.1145/3216122.3216154},
abstract = {The need to support advanced analytics on Big Data is driving data scientist' interest toward massively parallel distributed systems and software platforms, such as Map-Reduce and Spark, that make possible their scalable utilization. However, when complex data mining algorithms are required, their fully scalable deployment on such platforms faces a number of technical challenges that grow with the complexity of the algorithms involved. Thus algorithms, that were originally designed for a sequential nature, must often be redesigned in order to effectively use the distributed computational resources. In this paper, we explore these problems, and then propose a solution which has proven to be very effective on the complex hierarchical clustering algorithm CLUBS+. By using four stages of successive refinements, CLUBS+ delivers high-quality clusters of data grouped around their centroids, working in a totally unsupervised fashion. Experimental results confirm the accuracy and scalability of CLUBS+.},
booktitle = {Proceedings of the 22nd International Database Engineering &amp; Applications Symposium},
pages = {103–109},
numpages = {7},
keywords = {Clustering, Big Data, Spark},
location = {Villa San Giovanni, Italy},
series = {IDEAS '18}
}

@inproceedings{10.1145/2378356.2378364,
author = {Menon, Aravind},
title = {Big Data @ Facebook},
year = {2012},
isbn = {9781450317528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2378356.2378364},
doi = {10.1145/2378356.2378364},
abstract = {The Facebook Data Infrastructure supports a wide range of applications, including both external facing products and services and internal applications. This paper focuses on the Data Warehousing and Analytics platform of Facebook that provides support for batch-oriented analytics applications. Facebook's data infrastructure is built largely on top of open-source technologies such as Apache Hadoop, HDFS, MapReduce and Hive, and provides a rich set of tools for different users to perform analytics queries on Facebook data. As the Facebook user base continues to grow, we continue to enhance our data platform in order to deal with the challenges of scaling with increasing amounts of data.},
booktitle = {Proceedings of the 2012 Workshop on Management of Big Data Systems},
pages = {31–32},
numpages = {2},
keywords = {mbds},
location = {San Jose, California, USA},
series = {MBDS '12}
}

@inproceedings{10.1145/2640087.2644175,
author = {Miao, Xin},
title = {Big Data and Smart Grid},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644175},
doi = {10.1145/2640087.2644175},
abstract = {Big Data brings the challenge for Smart Grid. By using the method of SWOT, the double-edged sword effect of Big Data for the Smart Grid has been analyzed. Big Data provides both opportunities and challenges. The benefits and opportunities are which, Big Data bringing data view, changing thinking methods and tools, expanding the application scene, providing better service to the society, enhancing the value of the opportunity. At the same time, Big Data will lead to the challenges in Smart Grid, for example, because of security challenges of Big Data itself, Big Data more concentrated, cause safety challenges in Smart Grid is more serious; the energy consumption challenges of Big Data; Big Data privacy threat Smart Grid. From the viewpoint of information theory of Shannon, the following conclusions can be reached: the electric power consumption is positively correlated closely with the volume of Big Data; the energy consumption of data grows exponentially.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {26},
numpages = {2},
keywords = {Big Data, architecture, privacy protection, Smart Grid, power consumption, safety, data exploration},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/2656346.2656358,
author = {Mohammad, Atif and Mcheick, Hamid and Grant, Emanuel},
title = {Big Data Architecture Evolution: 2014 and Beyond},
year = {2014},
isbn = {9781450330282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656346.2656358},
doi = {10.1145/2656346.2656358},
abstract = {This paper aims at developing the Big Data Architecture, and its relation with Analytics, Cloud Services as well as Business Intelligence. The chief aim from all mentioned is to enable the Enterprise Architecture and the Vision of an Organizational target to utilize all the data they are ingesting and regressing data for their short-term or long-terms analytical needs, while making sure that they are addressing during the design phase of such data architecture for both directly and indirectly related stakeholder. Since all stakeholders have their relative interests to utilize the transformed data-sets. This paper also identifies most of the Big Data Architecture, threat analysis within a Big Data System and Big Data Analytic Roadmaps, in terms of smaller components by conducting a gap-analysis that has significant importance as Baseline Big Data Architecture, targeting the end resultant Architectures, once the distillation process of main Big Data Architecture is completed by the Data Architects.},
booktitle = {Proceedings of the Fourth ACM International Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications},
pages = {139–144},
numpages = {6},
keywords = {cloud computing, big data},
location = {Montreal, QC, Canada},
series = {DIVANet '14}
}

@inproceedings{10.1145/2699026.2699136,
author = {Thuraisingham, Bhavani},
title = {Big Data Security and Privacy},
year = {2015},
isbn = {9781450331913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2699026.2699136},
doi = {10.1145/2699026.2699136},
abstract = {This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.},
booktitle = {Proceedings of the 5th ACM Conference on Data and Application Security and Privacy},
pages = {279–280},
numpages = {2},
keywords = {big data, privacy, security},
location = {San Antonio, Texas, USA},
series = {CODASPY '15}
}

@inproceedings{10.1145/3018896.3025151,
author = {Sultan, Kashif and Ali, Hazrat},
title = {Where Big Data Meets 5G?},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3025151},
doi = {10.1145/3018896.3025151},
abstract = {Due to massive increase in data collection from wireless devices, wireless sensor networks, and network operators, data processing has become a challenge. The massive data can broadly be categorized into raw data and right data. Future generation network (5G) can be optimized if right data is extracted efficiently from such a massive raw data. Such a solution is provided through big data analytics. In this article, we discuss big data analytics solution for 5G network. We also outline existing big data architectures proposed for the network optimization. We propose a generalized flow structure for big data based analytics in 5G. Finally, we summarize our article by highlighting some challenges for big data analytics in 5G.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {103},
numpages = {4},
keywords = {data analtyics, 5G networks, big data, mobile communication},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/2609876.2609890,
author = {Endert, Alex and Szymczak, Samantha and Gunning, Dave and Gersh, John},
title = {Modeling in Big Data Environments},
year = {2014},
isbn = {9781450329385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609876.2609890},
doi = {10.1145/2609876.2609890},
abstract = {Human-Centered Big Data Research (HCBDR) is an area of work that focuses on the methodologies and research areas focused on understanding how humans interact with "big data". In the context of this paper, we refer to "big data" in a holistic sense, including most (if not all) the dimensions defining the term, such as complexity, variety, velocity, veracity, etc. Simply put, big data requires us as researchers of to question and reconsider existing approaches, with the opportunity to illuminate new kinds of insights that were traditionally out of reach to humans.},
booktitle = {Proceedings of the 2014 Workshop on Human Centered Big Data Research},
pages = {56–58},
numpages = {3},
location = {Raleigh, NC, USA},
series = {HCBDR '14}
}

@inproceedings{10.1145/2659651.2659655,
author = {Miloslavskaya, Natalia and Senatorov, Mikhail and Tolstoy, Alexander and Zapechnikov, Sergey},
title = {Big Data Information Security Maintenance},
year = {2014},
isbn = {9781450330336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659651.2659655},
doi = {10.1145/2659651.2659655},
abstract = {The need to protect big data, particularly those relating to information security maintenance (ISM) of an enterprise's IT infrastructure (ITI), and their processing is shown. Related worldwide experience of addressing big data ISM issues is summarized. An attempt to formulate a big data ISM problem statement is undertaken. An infrastructure for big data ISM is proposed. The importance of big data visualization is discussed.},
booktitle = {Proceedings of the 7th International Conference on Security of Information and Networks},
pages = {89–94},
numpages = {6},
keywords = {Information Security, Big Data, Secure Infrastructure},
location = {Glasgow, Scotland, UK},
series = {SIN '14}
}

@inproceedings{10.1109/CCGRID.2018.00100,
author = {Cuzzocrea, Alfredo and Damiani, Ernesto},
title = {Pedigree-Ing Your Big Data: Data-Driven Big Data Privacy in Distributed Environments},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00100},
doi = {10.1109/CCGRID.2018.00100},
abstract = {This paper introduces a general framework for supporting data-driven privacy-preserving big data management in distributed environments, such as emerging Cloud settings. The proposed framework can be viewed as an alternative to classical approaches where the privacy of big data is ensured via security-inspired protocols that check several (protocol) layers in order to achieve the desired privacy. Unfortunately, this injects considerable computational overheads in the overall process, thus introducing relevant challenges to be considered. Our approach instead tries to recognize the "pedigree" of suitable summary data representatives computed on top of the target big data repositories, hence avoiding computational overheads due to protocol checking. We also provide a relevant realization of the framework above, the so-called Data-dRIven aggregate-PROvenance privacy-preserving big Multidimensional data (DRIPROM) framework, which specifically considers multidimensional data as the case of interest.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {675–681},
numpages = {7},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/2609876.2609889,
author = {Argenta, Chris and Benson, Jordan and Bos, Nathan and Paletz, Susannah B. F. and Pike, William and Wilson, Aaron},
title = {Sensemaking in Big Data Environments},
year = {2014},
isbn = {9781450329385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2609876.2609889},
doi = {10.1145/2609876.2609889},
abstract = {We report on the sensemaking breakout group at the Human Centered Big Data Research (HCBDR-2014) workshop. The authors are a multi-disciplinary team of invited researchers and stakeholders who participated in this breakout session. This report includes an overview of our discussions on the many research challenges associated with sensemaking within a big data environment. Specifically, we focused on key topics that fit squarely in the intersection of the sensemaking and big data research, as other communities already exist for decision making and big data technologies independently. As part of this effort, our group developed and proposed a framework around which this community can target and structure future research. This framework is intended to allow the community to systematically identify areas where innovative research might make large contributions to sensemaking in a big data environment.},
booktitle = {Proceedings of the 2014 Workshop on Human Centered Big Data Research},
pages = {53–55},
numpages = {3},
location = {Raleigh, NC, USA},
series = {HCBDR '14}
}

@inproceedings{10.1145/2351316.2351318,
author = {Gopalkrishnan, Vivekanand and Steier, David and Lewis, Harvey and Guszcza, James},
title = {Big Data, Big Business: Bridging the Gap},
year = {2012},
isbn = {9781450315470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351316.2351318},
doi = {10.1145/2351316.2351318},
abstract = {Business analytics, occupying the intersection of the worlds of management science, computer science and statistical science, is a potent force for innovation in both the private and public sectors. The successes of business analytics in strategy, process optimization and competitive advantage has led to data being increasingly recognized as a valuable asset in many organizations. In recent years, thanks to a dramatic increase in the volume, variety and velocity of data, the loosely defined concept of "Big Data" has emerged as a topic of discussion in its own right -- with different viewpoints in both the business and technical worlds. From our perspective, it is important for discussions of "Big Data" to start from a well-defined business goal, and remain moored to fundamental principles of both cost/benefit analysis as well as core statistical science. This note discusses some business case considerations for analytics projects involving "Big Data", and proposes key questions that businesses should ask. With practical lessons from Big Data deployments in business, we also pose a number of research challenges that may be addressed to enable the business analytics community bring best data analytic practices when confronted with massive data sets.},
booktitle = {Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
pages = {7–11},
numpages = {5},
location = {Beijing, China},
series = {BigMine '12}
}

@inproceedings{10.1145/3322431.3326330,
author = {Kantarcioglu, Murat},
title = {Securing Big Data: New Access Control Challenges and Approaches},
year = {2019},
isbn = {9781450367530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322431.3326330},
doi = {10.1145/3322431.3326330},
abstract = {Recent cyber attacks have shown that the leakage/stealing of big data may result in enormous monetary loss and damage to organizational reputation, and increased identity theft risks for individuals. Furthermore, in the age of big data, protecting the security and privacy of stored data is paramount for maintaining public trust, and getting the full value from the collected data. In this talk, we first discuss the unique security and privacy challenges arise due to big data and the NoSQL systems designed to analyze big data. Also we discuss our proposed SecureDL system that is built on top of existing NoSQL databases such as Hadoop and Spark and designed as a data access broker where each request submitted by a user app is automatically captured. These captured requests are logged, analyzed and then modified (if needed) to conform with security and privacy policies (e.g.,[5]), and submitted to underlying NoSQL database. Furthermore, SecureDL can allow organizations to audit their big data usage to prevent data misuse and comply with various privacy regulations[2]. SecureDL is totally transparent from the user point of view and does not require any change to the user's code and/or the underlying NoSQL database systems. Therefore, it can be deployed on existing NoSQL databases.Later on, we discuss how to add additional security layer for protecting big data using encryption techniques (e.g., [1, 3, 4]). Especially, we discuss our work on leveraging the modern hardware based trusted execution environments (TEEs) such as Intel SGX for secure encrypted data processing. We also discuss how to provide a simple, secure and high level language based framework that is suitable for enabling generic data analytics for non-security experts who do not have security concepts such as "oblivious execution''. Our proposed framework allows data scientists to perform the data analytic tasks with TEEs using a Python/Matlab like high level language; and automatically compiles programs written in our language to optimal execution code by managing issues such as optimal data block sizes for I/O, vectorized computations to simplify much of the data processing, and optimal ordering of operations for certain tasks. Using these design choices, we show how to provide guarantees for efficient and secure big data analytics over encrypted data.},
booktitle = {Proceedings of the 24th ACM Symposium on Access Control Models and Technologies},
pages = {1–2},
numpages = {2},
keywords = {nosql databases, access control, privacy, security, intrusion detection, encrypted data processing},
location = {Toronto ON, Canada},
series = {SACMAT '19}
}

@inproceedings{10.5555/2819289.2819291,
author = {Villanustre, Flavio},
title = {Industrial Big Data Analytics: Lessons from the Trenches},
year = {2015},
publisher = {IEEE Press},
abstract = {Big Data Analytics in particular and Data Science in general have become key disciplines in the last decade. The convergence of Information Technology, Statistics and Mathematics, to explore and extract information from Big Data have challenged the way many industries used to operate, shifting the decision making process in many organizations. A new breed of Big Data platforms has appeared, to fulfill the needs to process data that is large, complex, variable and rapidly generated. The author describes the experience in this field from a company that provides Big Data analytics as its core business.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {1–3},
numpages = {3},
keywords = {big data, dataflow programming, declarative programming, abstraction models, distributed algorithms},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@inproceedings{10.1145/3377049.3377051,
author = {Tahsin, Anika and Hasan, Md. Manzurul},
title = {Big Data &amp; Data Science: A Descriptive Research on Big Data Evolution and a Proposed Combined Platform by Integrating R and Python on Hadoop for Big Data Analytics and Visualization},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377051},
doi = {10.1145/3377049.3377051},
abstract = {In this technological era, Big Data is a new glorified term in where Data Science is the secret sauce of it. Undoubtedly, the digitalization of data is not the whole story; it is just a beginning of Data Science area of study. There was a time when the main focus was on building framework and processing of this data. After Hadoop HDFS and MapReduce resolved this issue already typically the concentration will follow to the next level. In terms of this, Big Data on Data Science becoming the most hyped solving area. At the moment of zettabytes data, R, Python, Hadoop all are in progressing phase in where integration among individual framework and tools will be highlighted and newest data handling tools are integrating with latest technology in terms of analytics competence. There will be a positivity when this integration will expose a new horizon for researchers and develop the preeminent solution based on the challenges.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {83},
numpages = {2},
keywords = {Hadoop, Python, Big Data, Data Science, R},
location = {Dhaka, Bangladesh},
series = {ICCA 2020}
}

@article{10.5555/2835377.2835395,
author = {Villa, Adam H.},
title = {Big Data: Motivating the Development of an Advanced Database Systems Course},
year = {2016},
issue_date = {January 2016},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {31},
number = {3},
issn = {1937-4771},
abstract = {The creation of massive data sets, commonly referred to as Big Data, has motivated the development of new database systems and techniques for managing, monitoring, querying, and analyzing data [5]. As data sizes grow, so do the technologies developed to the meet this new demand, which in turn generates new employment opportunities for graduating students. Preparing students for these new positions requires the integration of these new techniques and methodologies into the curriculum. The growth of Big Data is motivating the development of advanced database courses. This paper presents an approach to creating such a course using flexible modules.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {119–128},
numpages = {10}
}

@inproceedings{10.1145/2939672.2945385,
author = {De Francisci Morales, Gianmarco and Bifet, Albert and Khan, Latifur and Gama, Joao and Fan, Wei},
title = {IoT Big Data Stream Mining},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2945385},
doi = {10.1145/2939672.2945385},
abstract = {The challenge of deriving insights from the Internet of Things (IoT) has been recognized as one of the most exciting and key opportunities for both academia and industry. Advanced analysis of big data streams from sensors and devices is bound to become a key area of data mining research as the number of applications requiring such processing increases. Dealing with the evolution over time of such data streams, i.e., with concepts that drift or change completely, is one of the core issues in IoT stream mining. This tutorial is a gentle introduction to mining IoT big data streams. The first part introduces data stream learners for classification, regression, clustering, and frequent pattern mining. The second part deals with scalability issues inherent in IoT applications, and discusses how to mine data streams on distributed engines such as Spark, Flink, Storm, and Samza.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2119–2120},
numpages = {2},
keywords = {big data, data streams, IoT, data science},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/2783258.2789989,
author = {Yang, Tianbao and Lin, Qihang and Jin, Rong},
title = {Big Data Analytics: Optimization and Randomization},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2789989},
doi = {10.1145/2783258.2789989},
abstract = {As the scale and dimensionality of data continue to grow in many applications of data analytics (e.g., bioinformatics, finance, computer vision, medical informatics), it becomes critical to develop efficient and effective algorithms to solve numerous machine learning and data mining problems. This tutorial will focus on simple yet practically effective techniques and algorithms for big data analytics. In the first part, we plan to present the state-of-the-art large-scale optimization algorithms, including various stochastic gradient descent methods, stochastic coordinate descent methods and distributed optimization algorithms, for solving various machine learning problems. In the second part, we will focus on randomized approximation algorithms for learning from large-scale data. We will discuss i) randomized algorithms for low-rank matrix approximation; ii) approximation techniques for solving kernel learning problems; iii) randomized reduction methods for addressing the high-dimensional challenge. Along with the description of algorithms, we will also present some empirical results to facilitate understanding of different algorithms and comparison between them.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2327},
numpages = {1},
keywords = {randomized approximation, machine learning, optimization, randomized reduction},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@article{10.1145/3186549.3186559,
author = {Sadiq, Shazia and Dasu, Tamraparni and Dong, Xin Luna and Freire, Juliana and Ilyas, Ihab F. and Link, Sebastian and Miller, Miller J. and Naumann, Felix and Zhou, Xiaofang and Srivastava, Divesh},
title = {Data Quality: The Role of Empiricism},
year = {2018},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/3186549.3186559},
doi = {10.1145/3186549.3186559},
abstract = {We outline a call to action for promoting empiricism in data quality research. The action points result from an analysis of the landscape of data quality research. The landscape exhibits two dimensions of empiricism in data quality research relating to type of metrics and scope of method. Our study indicates the presence of a data continuum ranging from real to synthetic data, which has implications for how data quality methods are evaluated. The dimensions of empiricism and their inter-relationships provide a means of positioning data quality research, and help expose limitations, gaps and opportunities.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {35–43},
numpages = {9}
}

