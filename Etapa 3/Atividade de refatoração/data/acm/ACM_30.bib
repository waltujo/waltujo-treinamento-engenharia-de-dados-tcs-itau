@inproceedings{10.1145/2614512.2614518,title = {Learning to think about broader implications of big data}, author = {Topi Heikki },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2614512.2614518}, doi = {10.1145/2614512.2614518}, pages = {24\u201325}, numpages = {2}}
@inproceedings{10.14778/3368289.3368292,title = {Pushing data-induced predicates through joins in big-data clusters}, author = {Kandula Srikanth , Orr Laurel , Chaudhuri Surajit },year = {2019}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3368289.3368292}, doi = {10.14778/3368289.3368292}, abstract = {Using data statistics, we convert predicates on a table into data induced predicates (diPs) that apply on the joining tables. Doing so substantially speeds up multi-relation queries because the benefits of predicate pushdown can now apply beyond just the tables that have predicates. We use diPs to skip data exclusively during query optimization; i.e., diPs lead to better plans and have no overhead during query execution. We study how to apply diPs for complex query expressions and how the usefulness of diPs varies with the data statistics used to construct diPs and the data distributions. Our results show that building diPs using zone-maps which are already maintained in today's clusters leads to sizable data skipping gains. Using a new (slightly larger) statistic, 50% of the queries in the TPC-H, TPC-DS and JoinOrder benchmarks can skip at least 33% of the query input. Consequently, the median query in a production big-data cluster finishes roughly 2x faster.}, pages = {252\u2013265}, numpages = {14}}
@inproceedings{10.1145/3361242.3361260,title = {From Data Quality to Model Quality: An Exploratory Study on Deep Learning}, author = {He Tianxing , Yu Shengcheng , Wang Ziyuan , Li Jieqiong , Chen Zhenyu },year = {2019}, isbn = {9781450377010}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3361242.3361260}, doi = {10.1145/3361242.3361260}, abstract = {In the field of deep learning, people strive to construct high-quality deep neural networks (DNNs) to improve the accuracy of predicting. As well known, the quality of training data have great impacts on the quality of DNN models, since all the DNN models are obtained by training using these training data. However, there is not any reported systematic study on how the quality of training data affects the quality of DNN model. To study the relationships between data quality and model quality, we mainly consider four aspects of data quality including Skewed Classes, Sample Complexity, Label Quality, and Noisy Data in this paper. We design experiments on MNIST and Cifar-10, and attempt to find out the influences of four aspects on the quality of DNN models. Pearson correlation coefficient and Spearman correlation coefficient are utilized to evaluate such influences. Experimental results show that all the four aspects of data quality have significant impacts on the quality of DNN models. It means that the decrease of data quality in these four aspects will reduce the accuracy of the DNN models.}, location = {Fukuoka, Japan}, series = {Internetware '19}, pages = {1\u20136}, numpages = {6}}
@inproceedings{10.1145/2768830,title = {Biomedical Ontology Quality Assurance Using a Big Data Approach}, author = {Cui Licong , Tao Shiqiang , Zhang Guo-Qiang },year = {2016}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2768830}, doi = {10.1145/2768830}, abstract = {This article presents recent progresses made in using scalable cloud computing environment, Hadoop and MapReduce, to perform ontology quality assurance (OQA), and points to areas of future opportunity. The standard sequential approach used for implementing OQA methods can take weeks if not months for exhaustive analyses for large biomedical ontological systems. With OQA methods newly implemented using massively parallel algorithms in the MapReduce framework, several orders of magnitude in speed-up can be achieved (e.g., from three months to three hours). Such dramatically reduced time makes it feasible not only to perform exhaustive structural analysis of large ontological hierarchies, but also to systematically track structural changes between versions for evolutional analysis. As an exemplar, progress is reported in using MapReduce to perform evolutional analysis and visualization on the Systemized Nomenclature of Medicine\u2014Clinical Terms (SNOMED CT), a prominent clinical terminology system. Future opportunities in three areas are described: one is to extend the scope of MapReduce-based approach to existing OQA methods, especially for automated exhaustive structural analysis. The second is to apply our proposed MapReduce Pipeline for Lattice-based Evaluation (MaPLE) approach, demonstrated as an exemplar method for SNOMED CT, to other biomedical ontologies. The third area is to develop interfaces for reviewing results obtained by OQA methods and for visualizing ontological alignment and evolution, which can also take advantage of cloud computing technology to systematically pre-compute computationally intensive jobs in order to increase performance during user interactions with the visualization interface. Advances in these directions are expected to better support the ontological engineering lifecycle.}, pages = {1\u201328}, numpages = {28}, keywords = {Hadoop, SNOMED CT, lattice, terminology quality assurance}}
@inproceedings{10.1145/288195.288292,title = {Investigating data quality problems in the PSP}, author = {Disney Anne M. , Johnson Philip M. },year = {1998}, isbn = {1581131089}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/288195.288292}, doi = {10.1145/288195.288292}, abstract = {The Personal Software Process (PSP) is used by software engineers to gather and analyze data about their work. Published studies typically use data collected using the PSP to draw quantitative conclusions about its impact upon programmer behavior and product quality. However, our experience using PSP in both industrial and academic settings revealed problems both in collection of data and its later analysis. We hypothesized that these two kinds of data quality problems could make a significant impact upon the value of PSP measures. To test this hypothesis, we built a tool to automate the PSP and then examined 89 projects completed by ten subjects using the PSP manually in an educational setting. We discovered 1539 primary errors and categorized them by type, subtype, severity, and age. To examine the collection problem we looked at the 90 errors that represented impossible combinations of data and at other less concrete anomalies in Time Recording Logs and Defect Recording Logs. To examine the analysis problem we developed a rule set, corrected the errors as far as possible, and compared the original and corrected data. This resulted in significant differences for measures such as yield and the cost-performance ratio, confirming our hypothesis. Our results raise questions about the accuracy of manually collected and analyzed PSP data, indicate that integrated tool support may be required for high quality PSP data analysis, and suggest that external measures should be used when attempting to evaluate the impact of the PSP upon programmer behavior and product quality.}, location = {Lake Buena Vista, Florida, USA}, series = {SIGSOFT '98/FSE-6}, pages = {143\u2013152}, numpages = {10}, keywords = {measurement dysfunction, automated process support, empirical software engineering, personal software process, defects}}
@inproceedings{10.1145/291252.288292,title = {Investigating data quality problems in the PSP}, author = {Disney Anne M. , Johnson Philip M. },year = {1998}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/291252.288292}, doi = {10.1145/291252.288292}, abstract = {The Personal Software Process (PSP) is used by software engineers to gather and analyze data about their work. Published studies typically use data collected using the PSP to draw quantitative conclusions about its impact upon programmer behavior and product quality. However, our experience using PSP in both industrial and academic settings revealed problems both in collection of data and its later analysis. We hypothesized that these two kinds of data quality problems could make a significant impact upon the value of PSP measures. To test this hypothesis, we built a tool to automate the PSP and then examined 89 projects completed by ten subjects using the PSP manually in an educational setting. We discovered 1539 primary errors and categorized them by type, subtype, severity, and age. To examine the collection problem we looked at the 90 errors that represented impossible combinations of data and at other less concrete anomalies in Time Recording Logs and Defect Recording Logs. To examine the analysis problem we developed a rule set, corrected the errors as far as possible, and compared the original and corrected data. This resulted in significant differences for measures such as yield and the cost-performance ratio, confirming our hypothesis. Our results raise questions about the accuracy of manually collected and analyzed PSP data, indicate that integrated tool support may be required for high quality PSP data analysis, and suggest that external measures should be used when attempting to evaluate the impact of the PSP upon programmer behavior and product quality.}, pages = {143\u2013152}, numpages = {10}, keywords = {automated process support, measurement dysfunction, personal software process, defects, empirical software engineering}}
@inproceedings{10.5555/2627817.2627920,title = {Turning big data into tiny data: constant-size coresets for k-means, PCA and projective clustering}, author = {Feldman Dan , Schmidt Melanie , Sohler Christian },year = {2013}, isbn = {9781611972511}, publisher = {Society for Industrial and Applied Mathematics}, address = {USA}, abstract = {We prove that the sum of the squared Euclidean distances from the n rows of an n x d matrix A to any compact set that is spanned by k vectors in @@@@d can be approximated up to (1 + \u03b5)-factor, for an arbitrary small \u03b5 > 0, using the O(k/\u03b52)-rank approximation of A and a constant. This implies, for example, that the optimal k-means clustering of the rows of A is (1 + \u03b5)-approximated by an optimal k-means clustering of their projection on the O(k/\u03b52) first right singular vectors (principle components) of A.A (j, k)-coreset for projective clustering is a small set of points that yields a (1 + \u03b5)-approximation to the sum of squared distances from the n rows of A to any set of k affine subspaces, each of dimension at most j. Our embedding yields (0, k)-coresets of size O(k) for handling k-means queries, (j, 1)-coresets of size O(j) for PCA queries, and (j, k)-coresets of size (log n)O(jk) for any j, k \u2265 1 and constant \u03b5 \u03b5 (0, 1/2). Previous coresets usually have a size which is linearly or even exponentially dependent of d, which makes them useless when d ~ n.Using our coresets with the merge-and-reduce approach, we obtain embarrassingly parallel streaming algorithms for problems such as k-means, PCA and projective clustering. These algorithms use update time per point and memory that is polynomial in log n and only linear in d.For cost functions other than squared Euclidean distances we suggest a simple recursive coreset construction that produces coresets of size @@@@ for k-means and a special class of bregman divergences that is less dependent on the properties of the squared Euclidean distance.}, location = {New Orleans, Louisiana}, series = {SODA '13}, pages = {1434\u20131453}, numpages = {20}}
@inproceedings{10.1145/3232116.3232143,title = {Research on Personalized Exercises and Teaching Feedback Based on Big Data}, author = {Xie Xiaolan , Li Xinrong },year = {2018}, isbn = {9781450364966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3232116.3232143}, doi = {10.1145/3232116.3232143}, abstract = {With the rapid development of information technology, big data plays an increasingly important role in the research and practice of education and teaching. Online education has also become a research hotspot. To solve the problem of lack of personalized exercises and accurate teaching feedback in online education, a content-based recommendation model in big data and a clustering model based on EM algorithm is proposed in this paper. First of all, the students' answer of questions is recorded. Then the characteristic information is extracted, so recommends of the exercises are provided by the model according to the personal characteristic information. Then, all the students' recommendation information is stored in the feature library, in which the information of students are clustered, and the teaching effect is fed back according to the characteristic parameters of each category. On the one hand, the status of students' learning is fed back; On the other hand, the level of teachers' teaching level is also fed back. Finally, the model works well through experiments, with the good performance that it can improve the efficiency of online learning.}, location = {Guilin, China}, series = {ICIIP '18}, pages = {166\u2013171}, numpages = {6}, keywords = {teaching feedback, online learning, personalized exercises, EM algorithm, recommendation algorithm}}
@inproceedings{10.1145/3017611.3017627,title = {On storing and retrieving geospatial big-data in cloud}, author = {Liu Kuien , Wang Haozhou , Yao Yandong },year = {2016}, isbn = {9781450345804}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3017611.3017627}, doi = {10.1145/3017611.3017627}, abstract = {Cloud storage is a kind of external storage which can provide by unlimited storage space with high availability and low cost on maintenance. On the other side, the size of geospatial data is too large and is increasing dramatically which makes such data is hard to be stored in the local data warehouse. Hence following the benefits of Cloud storage, such geospatial data is suitable to be stored in Cloud storage and managed by local data warehouse. However, there is a gap between Cloud storages and data warehouses built on traditional infrastructures, such as the mostly adopted massive parallel processing (MPP) based data warehouse. Therefore, in this paper, we propose a middleware-like architecture to connect MPP data warehouse and Cloud storage. It supports traditional geospatial data retrieving while integrating the Cloud storage lineage by a set of technical designs. Based on the prototype system and practical data, we demonstrate the appreciable performance and the flexibility for other third parties' development. Another major contribution of this paper is that we implement the prototype on open-source data warehouse and we make it open-sourced to public.}, location = {Burlingame, California}, series = {EM-GIS '16}, pages = {1\u20134}, numpages = {4}, keywords = {geospatial, data warehouse, cloud storage}}
@inproceedings{10.5555/3042094.3042253,title = {Modeling traffic flow using simulation and big data analytics}, author = {Bowman Casey N. , Miller John A. },year = {2016}, isbn = {9781509044849}, publisher = {IEEE Press}, abstract = {Improving the efficiency, safety, and cost of road systems is an essential social problem that must be solved as the number of drivers, and the size of mass transit systems increase. Methodologies used for the construction of traffic simulations need to be examined in the context of real world big traffic data. This data can be used to create models for vehicle arrivals, turning behavior, and traffic flow. Our work focuses mainly on generating models for these concepts and using them to drive microscopic traffic simulations built upon real world data. Strengths and weaknesses of various simulation optimization techniques are also considered as a methodology issue, since the nature of traffic systems weakens the effectiveness of some optimization techniques.}, pages = {1206\u20131217}, numpages = {12}}
@inproceedings{10.1145/2643132,title = {Privacy, anonymity, and big data in the social sciences}, author = {Daries Jon P. , Reich Justin , Waldo Jim , Young Elise M. , Whittinghill Jonathan , Ho Andrew Dean , Seaton Daniel Thomas , Chuang Isaac },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2643132}, doi = {10.1145/2643132}, abstract = {Quality social science research and the privacy of human subjects require trust.}, pages = {56\u201363}, numpages = {8}}
@inproceedings{10.1145/2799979.2799995,title = {Ontology-based big data approach to automated penetration testing of large-scale heterogeneous systems}, author = {Stepanova Taiana , Pechenkin Alexander , Lavrova Daria },year = {2015}, isbn = {9781450334532}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2799979.2799995}, doi = {10.1145/2799979.2799995}, abstract = {Global corporations and government organizations are nowadays represented in cyberspace in the form of numerous large-scale heterogeneous information systems, which implement corresponding business, technological and other types of processes. This extends the set of security analysis tasks, stated for these infrastructures, and tangles already existing tasks. This paper addresses the challenge of increasing penetration testing automation level through the adoption of semi-automatic knowledge extraction from the huge amounts of heterogeneous regularly updated data. The proposed solution is based on the novel penetration testing ontology, which gives a holistic view on the results of security analysis. Designed ontology is evaluated within the penetration testing framework prototype and binds together the conceptual (process) abstraction level, addressed by security experts, and technical abstraction level, employed in modern security analysis tools and methods.}, location = {Sochi, Russia}, series = {SIN '15}, pages = {142\u2013149}, numpages = {8}, keywords = {penetration testing, ontology, big data, large-scale systems}}
@inproceedings{10.1145/3102254.3102259,title = {Evolutionary mining of relaxed dependencies from big data collections}, author = {Caruccio Loredana , Deufemia Vincenzo , Polese Giuseppe },year = {2017}, isbn = {9781450352253}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3102254.3102259}, doi = {10.1145/3102254.3102259}, abstract = {Many modern application contexts, especially those related to the semantic Web, advocate for automatic techniques capable of extracting relationships between semi-structured data, for several purposes, such as the identification of inconsistencies or patterns of semantically related data, query rewriting, and so forth. One way to represent such relationships is to use relaxed functional dependencies (rfds), since they can embed approximate matching paradigms to compare unstructured data, and admit the possibility of exceptions for them. To this end, thresholds might need to be specified in order to limit the similarity degree in approximate comparisons or the occurrence of exceptions. Thanks to the availability of huge amount of data, including unstructured data available on the Web, nowadays it is possible to automatically discover rfds from data. However, due to the many different combinations of similarity and exception thresholds, the discovery process has an exponential complexity. Thus, it is vital devising proper optimization strategies, in order to make the discovery process feasible. To this end, in this paper, we propose a genetic algorithm to discover rfds from data, also providing an empirical evaluation demonstrating its effectiveness.}, location = {Amantea, Italy}, series = {WIMS '17}, pages = {1\u201310}, numpages = {10}, keywords = {discovery from data, functional dependency, genetic algorithm}}
@inproceedings{10.1145/3107514.3107518,title = {Usage Patterns and Data Quality: A Case Study of a National Type-1 Diabetes Study}, author = {Wu Jinrong , Sinnott Richard O. , Effendy Jemie , Gl\u00f6ckner Stephan , Hu William , Li Jiajie },year = {2017}, isbn = {9781450352246}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3107514.3107518}, doi = {10.1145/3107514.3107518}, abstract = {The Environmental Determinants of Islet Auto- immunity (ENDIA) project is Australia's largest study into the causes of Type-1 Diabetes (T1D). The ENDIA study is supported by a Cloud-based software platform including a clinical registry comprising extensive longitudinal information on families at risk of having a child that might go on to develop T1D. This registry includes both demographic and clinical information on families and children as well as the environmental factors that might influence the onset of T1D. A multitude of samples are obtained through the study and used to support a diverse portfolio of bioinformatics data analytics. The quality of the data in the registry is essential to the overall success of the project. This paper presents a Cloud-based log-analytics platform that supports the detailed analysis of patterns of usage of the registry by the clinical centres and collaborators involved. We explore the impact that the usage patterns have on the overall data quality. We also consider ways of improving data quality by mothers entering their own data through targeted mobile applications that have been developed for dietary data collection.}, location = {Taichung City, Taiwan}, series = {ICMHI '17}, pages = {18\u201327}, numpages = {10}, keywords = {auditing, Type-1 diabetes, log analysis, Cloud}}
@inproceedings{10.1145/2859889.2883587,title = {Tutorial on Challenges for Big Data Application Performance Tuning and Prediction}, author = {Singhal Rekha },year = {2016}, isbn = {9781450341479}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2859889.2883587}, doi = {10.1145/2859889.2883587}, abstract = {Digitization of user services and cheap access to the internet has led to two critical problems- quick response to end-user queries and faster analysis of large accumulated data to serve users better. This has also led to the advent of various big data processing technologies, each of them has architecture specific parameters to tune for optimal execution of the application. There are also challenges in optimal scheduling of analytic queries for faster analysis, which lead to the problem of estimating analytic queries execution time for large data sizes on the production system. A production system may be an enterprise database system or a cluster of machines with Hadoop etc, where each machine may be of different hardware configuration (known as heterogeneous environment). In the first part of this tutorial, we shall present need and challenges for tuning big data applications on various platforms. This is followed by discussion on various existing solutions for application tuning. The second part of the tutorial presents the challenges and state of the art for estimating application execution time.}, location = {Delft, The Netherlands}, series = {ICPE '16 Companion}, pages = {29}, numpages = {1}}
@inproceedings{10.1145/3442705.3442714,title = {Research on Big Data Storage Method based on IPFS and Blockchain}, author = {Tang Jing , Jia Tao , Chen Haibo , Wei Chuncheng },year = {2020}, isbn = {9781450388931}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3442705.3442714}, doi = {10.1145/3442705.3442714}, abstract = {With the popularity of digital cryptocurrency such as bitcoin, blockchain, as a new distributed framework with decentralization, non rewriting and traceability, has sprung up rapidly and has been applied in many industries such as finance, medical treatment, information security, etc. In order to ensure the security of transaction data, all key information in the business needs to enter the blockchain network. In the field of artificial intelligence, model data (i.e. effective feature point data set) will be the key information and will be used frequently. However, these feature point datasets may be megabytes, auspicious, or even terahertz. Then, the performance of big data transaction in blockchain network will be a problem worthy of study. Therefore, this paper proposes a blockchain big data storage method based on IPFs. This method mainly solves the transaction performance problem of large text data in the blockchain network. The data larger than 100 megabytes are stored in IPFs to obtain the hash certificate of text. The hash code is the only transaction voucher in the blockchain network. It greatly improves the transaction efficiency of blockchain network. In this paper, a comparative experiment is set up to further prove the efficiency of our method.}, location = {Jakarta, Indonesia}, series = {VSIP '20}, pages = {55\u201360}, numpages = {6}, keywords = {BigData, blockchain, IPFS, Decentralization}}
@inproceedings{10.1145/2432596.2432601,title = {Where is big data in your information systems curriculum?}, author = {Topi Heikki },year = {2013}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2432596.2432601}, doi = {10.1145/2432596.2432601}, pages = {12\u201313}, numpages = {2}}
@inproceedings{10.1145/3357223.3362720,title = {Libra and the Art of Task Sizing in Big-Data Analytic Systems}, author = {Li Rui , Guo Peizhen , Hu Bo , Hu Wenjun },year = {2019}, isbn = {9781450369732}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3357223.3362720}, doi = {10.1145/3357223.3362720}, abstract = {Despite extensive investigation of job scheduling in data-intensive computation frameworks, less consideration has been given to optimizing job partitioning for resource utilization and efficient processing. Instead, partitioning and job sizing are a form of dark art, typically left to developer intuition and trial-and-error style experimentation.In this work, we propose that just as job scheduling and resource allocation are out-sourced to a trusted mechanism external to the workload, so too should be the responsibility for partitioning data as a determinant for task size. Job partitioning essentially involves determining the partition sizes to match the resource allocation at the finest granularity. This is a complex, multi-dimensional problem that is highly application specific: resource allocation, computational runtime, shuffle and reduce communication requirements, and task startup overheads all have strong influence on the most effective task size for efficient processing. Depending on the partition size, the job completion time can differ by as much as 10 times!Fortunately, we observe a general trend underlying the tradeoff between full resource utilization and system overhead across different settings. The optimal job partition size balances these two conflicting forces. Given this trend, we design Libra to automate job partitioning as a framework extension. We integrate Libra with Spark and evaluate its performance on EC2. Compared to state-of-the-art techniques, Libra can reduce the individual job execution time by 25% to 70%.}, location = {Santa Cruz, CA, USA}, series = {SoCC '19}, pages = {364\u2013376}, numpages = {13}, keywords = {Big Data Systems, Data-analytic Systems, Automatic Task Sizing}}
@inproceedings{10.1145/3482632.3482734,title = {Application and Innovation of Traditional Financial Big Data Based on AI Algorithm}, author = {Ren Xiaojie },year = {2021}, isbn = {9781450390255}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3482632.3482734}, doi = {10.1145/3482632.3482734}, abstract = {Under the background of big data era, online banking has achieved rapid development and has undergone qualitative changes. Online banking is a new financial form. The application of big data has promoted the development of online banking to a great extent, provided strong technical support for it, and improved the business model of online banking. The application of AI (artificial intelligence) in insurance, credit reporting, asset allocation, big data risk control and other financial fields has brought new changes to the financial industry. As a brand-new technology model, the combination of big data and AI with economy has realized the deep excavation of various resources, which can ensure the quality and efficiency of economic development and reduce the comprehensive cost, thus effectively enhancing the overall competitiveness of China's economy and gaining more initiative in the fierce international competition. This paper investigates the current situation of big data application in online banking, analyzes its potential value and main challenges, and discusses the traditional financial big data application and innovation path based on AI algorithm.}, location = {Dalian, China}, series = {ICISCAE 2021}, pages = {482\u2013485}, numpages = {4}}
@inproceedings{10.1145/3544109.3544143,title = {Smart Tourism Management Model Based on Big Data Technology}, author = {Yang Na },year = {2022}, isbn = {9781450395786}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3544109.3544143}, doi = {10.1145/3544109.3544143}, location = {Dalian, China}, series = {IPEC '22}, pages = {187\u2013190}, numpages = {4}}
@inproceedings{10.14778/3090163.3090168,title = {Bridging the gap between HPC and big data frameworks}, author = {Anderson Michael , Smith Shaden , Sundaram Narayanan , Capot\u0103 Mihai , Zhao Zheguang , Dulloor Subramanya , Satish Nadathur , Willke Theodore L. },year = {2017}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3090163.3090168}, doi = {10.14778/3090163.3090168}, abstract = {Apache Spark is a popular framework for data analytics with attractive features such as fault tolerance and interoperability with the Hadoop ecosystem. Unfortunately, many analytics operations in Spark are an order of magnitude or more slower compared to native implementations written with high performance computing tools such as MPI. There is a need to bridge the performance gap while retaining the benefits of the Spark ecosystem such as availability, productivity, and fault tolerance. In this paper, we propose a system for integrating MPI with Spark and analyze the costs and benefits of doing so for four distributed graph and machine learning applications. We show that offloading computation to an MPI environment from within Spark provides 3.1\u221217.7\u00d7 speedups on the four sparse applications, including all of the overheads. This opens up an avenue to reuse existing MPI libraries in Spark with little effort.}, pages = {901\u2013912}, numpages = {12}}
@inproceedings{10.1145/3538228,title = {Incentivizing Data Quality in Blockchain-Based Systems\u2014The Case of the Digital Cardossier}, author = {Spychiger Florian , Tessone Claudio J. , Zavolokina Liudmila , Schwabe Gerhard },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3538228}, doi = {10.1145/3538228}, abstract = {Inspired by an industry initiative to address the celebrated market for lemons (poor-quality used cars), we investigate how incentives for a permissioned blockchain-based system in the automobile ecosystem can be designed to ensure high-quality data storage and use by different stakeholders. The peer-to-peer distributed ledger platform connects organizations and car owners with disparate interests and hidden intentions. While previous literature has chiefly examined incentives for permissionless platforms, we leverage studies about crowdsensing applications to stimulate research on incentives in permissioned blockchains. This article uses the action design research approach to create an incentive system featuring a rating mechanism influenced by data correction measures. Furthermore, we propose relying on certain institutions capable of assessing data generated within the system. This combined approach of a decentralized data correction and an institutionalized data assessment is distinct from similar incentive systems suggested by literature. By using an agent-based model with strategy evolution, we evaluate the proposed incentive system. Our findings indicate that a rating-based revenue distribution leads to markedly higher data quality in the system. Additionally, the incentive system reveals hidden information of the agents and alleviates agency problems, contributing to an understanding of incentive design in inter-organizational blockchain-based data platforms. Furthermore, we explore incentive design in permissioned blockchains and discuss its latest implications.}, pages = {1\u201327}, numpages = {27}, keywords = {Blockchain, data quality, action design research, incentives, peer-to-peer market}}
@inproceedings{10.1145/3097983.3105813,title = {Mining Big Data in NeuroGenetics to Understand Muscular Dystrophy}, author = {Berglund Andy },year = {2017}, isbn = {9781450348874}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3097983.3105813}, doi = {10.1145/3097983.3105813}, abstract = {The recent advances in genome sequencing and analyses of the billions of base pairs in genomic data have been a boon for moving forward our understanding of human disease. In this talk I will describe how genome sequencing has dramatically improved our understanding of the most common adult form of muscular dystrophy, which is myotonic dystrophy. Two different genetic mutations cause thousands of changes in the cells and tissues of myotonic dystrophy patients. Genome sequencing has allowed us to precisely determine the degree of changes across patients, correlate these changes to disease symptoms and allow us to determine quickly in cell and animal models the effectiveness of therapeutic strategies for myotonic dystrophy.}, location = {Halifax, NS, Canada}, series = {KDD '17}, pages = {11}, numpages = {1}, keywords = {genomic data, myotonic dystrophy, genome sequencing, muscular dystrophy, data mining.}}
@inproceedings{10.1145/3078468.3078500,title = {Big data analysis of cloud storage logs using spark}, author = {Garion Shelly , Kolodner Hillel , Adir Allon , Aharoni Ehud , Greenberg Lev },year = {2017}, isbn = {9781450350358}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3078468.3078500}, doi = {10.1145/3078468.3078500}, abstract = {We use Apache Spark analytics to investigate the logs of an operational cloud object store service to understand how it is being used. This investigation involves going over very large amounts of historical data (PBs of records in some cases) collected over long periods of time retroactively. Existing tools, such as Elasticsearch-Logstash-Kibana (ELK), are mainly used for presenting short-term metrics and can-not perform advanced analytics such as machine learning. A possible solution is to save for long periods only certain aggregations or calculations produced from the raw log data, such as averages or histograms, however these must be decided in advance, and cannot be changed retroactively since the raw data has already been discarded. Spark allows us to gain insights going over historical data collected over long periods of time and to apply the historical models on online data in a simple and efficient way.}, location = {Haifa, Israel}, series = {SYSTOR '17}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/2378016.2378019,title = {Improving the Data Quality of Drug Databases using Conditional Dependencies and Ontologies}, author = {Cur\u00e9 Olivier },year = {2012}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2378016.2378019}, doi = {10.1145/2378016.2378019}, abstract = {Many health care systems and services exploit drug related information stored in databases. The poor data quality of these databases, e.g. inaccuracy of drug contraindications, can lead to catastrophic consequences for the health condition of patients. Hence it is important to ensure their quality in terms of data completeness and soundness.In the database domain, standard Functional Dependencies (FDs) and INclusion Dependencies (INDs), have been proposed to prevent the insertion of incorrect data. But they are generally not expressive enough to represent a domain-specific set of constraints. To this end, conditional dependencies, i.e. standard dependencies extended with tableau patterns containing constant values, have been introduced and several methods have been proposed for their discovery and representation. The quality of drug databases can be considerably improved by their usage.Moreover, pharmacology information is inherently hierarchical and many standards propose graph structures to represent them, e.g. the Anatomical Therapeutic Chemical classification (ATC) or OpenGalen\u2019s terminology. In this article, we emphasize that the technologies of the Semantic Web are adapted to represent these hierarchical structures, i.e. in RDFS and OWL. We also present a solution for representing conditional dependencies using a query language defined for these graph oriented structures, namely SPARQL. The benefits of this approach are interoperability with applications and ontologies of the Semantic Web as well as a reasoning-based query execution solution to clean underlying databases.}, pages = {1\u201321}, numpages = {21}, keywords = {description logics, conditional dependencies, Data quality}}
@inproceedings{10.1145/2345316.2345324,title = {Big data computing for traffic information by GPS sensing}, author = {Tossavainen Olli-Pekka },year = {2012}, isbn = {9781450311137}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2345316.2345324}, doi = {10.1145/2345316.2345324}, abstract = {Real time traffic monitoring systems perform spatial and time dependent analysis of measurement data of different types such as traditional inductive loop detector data, microwave radar data, and GPS data. The goal of these systems is to provide information such as average speeds, volumes and densities on a given segment of a roadway. One of the fastest growing data source for traffic monitoring systems is GPS data collected from mobile devices. To some extent, in the industry GPS data is already replacing the traditional traffic sensing technologies.There is a large demand in industry and transportation agencies to have access to high resolution state of traffic on highways and arterial roads globally. This means that traffic information providers have to provide traffic information on a resolution that goes beyond the widely used TMC code based representation of the roadway.In order to obtain the high resolution state of traffic, noisy observations need to be fused into a mathematical model that represents the evolution of the system either based on physics or statistics. Common frameworks for fusing the data into physical models include for example Kalman filtering and particle filtering.Prior to the data fusion stage in the real time system, offline geospatial modelling has already been done. For example, construction and calibration of an accurate physics based traffic model includes tasks such as building a directed graph of the road network, detection of road geometry at lane level and speed limit detection. In all these tasks, GPS data is vital.Real time systems that use GPS data include geospatial pre-processing components such as map matching and path inference. The rapidly growing volume of GPS data cannot be handled using traditional methods but instead parallel computing systems are needed to handle the future volumes. Also, the new high resolution algorithms are developed to leverage the parallel processing frameworks.In this talk I will discuss directions taken to respond to the demand of providing high resolution information about the state of the traffic both in the context of modeling and implementation of a large scale system.}, location = {Washington, D.C., USA}, series = {COM.Geo '12}, pages = {1}, numpages = {1}}
@inproceedings{10.1145/2598902,title = {Design and ethics in the era of big data}, author = {Goodman Elizabeth },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2598902}, doi = {10.1145/2598902}, pages = {22\u201324}, numpages = {3}}
@inproceedings{10.1145/3498765.3498846,title = {Research on the Application of Big Data in the Evaluation System of Music Teaching in Chinese Colleges and Universities}, author = {Wang Chenyu , Jiang Bai'an , Zhong Shichang },year = {2021}, isbn = {9781450385114}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3498765.3498846}, doi = {10.1145/3498765.3498846}, abstract = {The teaching effect of music courses in Colleges and universities is closely related to the teaching quality of teachers. Starting with the application of big data in the teaching quality evaluation system of colleges and universities, this paper objectively summarizes the application of big data in the teaching quality evaluation system of music courses in Colleges and universities. Finally, Put forward the countermeasures to strengthen the application of big data in the teaching quality evaluation system of music courses in Colleges and Universities: gradually optimize the teaching quality evaluation system and enrich the evaluation content; Optimize the display form of teaching quality evaluation and highlight the role of evaluation; At the same time, improve students' enthusiasm for participation, mobilize students' enthusiasm for participating in the evaluation, and improve the quality of the course.}, location = {Wuhan, China}, series = {ICETC 2021}, pages = {252\u2013257}, numpages = {6}, keywords = {Teaching, Big data, College, Evaluation}}
@inproceedings{10.1145/3381027,title = {A Survey on Automatic Parameter Tuning for Big Data Processing Systems}, author = {Herodotou Herodotos , Chen Yuxing , Lu Jiaheng },year = {2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3381027}, doi = {10.1145/3381027}, abstract = {Big data processing systems (e.g., Hadoop, Spark, Storm) contain a vast number of configuration parameters controlling parallelism, I/O behavior, memory settings, and compression. Improper parameter settings can cause significant performance degradation and stability issues. However, regular users and even expert administrators grapple with understanding and tuning them to achieve good performance. We investigate existing approaches on parameter tuning for both batch and stream data processing systems and classify them into six categories: rule-based, cost modeling, simulation-based, experiment-driven, machine learning, and adaptive tuning. We summarize the pros and cons of each approach and raise some open research problems for automatic parameter tuning.}, pages = {1\u201337}, numpages = {37}, keywords = {MapReduce, stream, self-tuning, Storm, Spark, Parameter tuning}}
@inproceedings{10.1145/3424978.3425137,title = {Research and Application of AHP-EWM-based Comprehensive Evaluation of Data Quality}, author = {Shi Qiao , Wang Honglv , Lu Hailong },year = {2020}, isbn = {9781450377720}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3424978.3425137}, doi = {10.1145/3424978.3425137}, abstract = {In order to improve the operation quality of the cross-enterprise batch management system in the tobacco industry, a comprehensive data quality index evaluation model is proposed in this paper. By analyzing the characteristics of batch management data, 7 indicators are selected to evaluate the system data, and the influence of human factors is eliminated by the combination weight determined by the analytic hierarchy process (AHP) and entropy weight method (EWM). The proposed evaluation model is applied to the quality assessment of the actual statistical data of Zhejiang Tobacco Industries Co. and six cooperative production enterprises in the cigarette industry. The results show that the model can scientifically evaluate the system data quality of each enterprise, intuitively rank the data quality of various enterprises, and truly reflect the change trend of data quality, indicating the feasibility and effectiveness of the model. This method can provide support for improving the data quality level of the cross-enterprise batch management system.}, location = {Sanya, China}, series = {CSAE 2020}, pages = {1\u20135}, numpages = {5}, keywords = {Batch management, Analytic hierarchy process, Entropy weight method, Data quality, Tobacco industry, Synthetic evaluation}}
@inproceedings{10.1145/3207677.3277994,title = {Research on Customer Credit Demand Forecasting Based on Big Data Analysis}, author = {Zhang Han , Wang Kuisheng },year = {2018}, isbn = {9781450365123}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3207677.3277994}, doi = {10.1145/3207677.3277994}, abstract = {With1 the rapid development of the Internet, Internet credit business has emerged and the process is now booming. As a result, there is a problem of predicting credit demand of users. Therefore, we propose a method of using big data analysis to forecast the credit demand of users in this paper, which is used to reduce the risk of credit business and improve the utilization of funds.}, location = {Hohhot, China}, series = {CSAE '18}, pages = {1\u20135}, numpages = {5}, keywords = {K-means clustering algorithm, Data analysis, credit demand, GBDT (Gradient Boosting Decision Tree) model, ReliefF feature extraction algorithm}}
@inproceedings{10.14778/3402707.3402709,title = {Is it still \"Big Data\" if it fits in my pocket?}, author = {Campbell David },year = {2011}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/3402707.3402709}, doi = {10.14778/3402707.3402709}, abstract = {\"Big Data\" is a hot topic but, in many ways, we are still trying to define what the phrase \"Big Data\" means. For many, there are more questions than answers at this point. Is it about size alone? Complexity? Variability? Data shape? Price/performance? New workloads? New types of users? Are existing data models, data management systems, data languages, and BI/ETL tools relevant in this space? Is MapReduce really a \"major step backwards\"? I have spent time over the last several years trying to answer many of these questions to my own satisfaction. As part of the journey I have witnessed a number of natural patterns that emerge in big data processing. In this talk I will present a catalog of these patterns and illustrate them across a scale spectrum from megabytes to 100s of petabytes. Finally, I will offer some thoughts around a systems and research agenda for this new world.}, pages = {694}, numpages = {1}}
@inproceedings{10.1145/3318299.3318388,title = {Research on the Application of Big Data Management in Enterprise Management Decision-making and Execution Literature Review}, author = {Zhuo Zhiyi , Zhang Shanhu },year = {2019}, isbn = {9781450366007}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3318299.3318388}, doi = {10.1145/3318299.3318388}, abstract = {This article reviews relevant theories and literature on big data management, management decision-making, execution, and other aspects, discusses the two significant factors of decision-making force and executive power that are the realization of corporate strategic goals, and puts forward the corporate data in the context of big data. The operating model (mainly for the enterprise's decision-making and implementation) faces new opportunities and challenges, that is, through in-depth analysis and exploration of big data management can effectively improve the company's decision-making ability and execution efficiency, and promote the realization of corporate strategic goals.}, location = {Zhuhai, China}, series = {ICMLC '19}, pages = {268\u2013273}, numpages = {6}, keywords = {execution, decision-making, literature review, Big data management}}
@inproceedings{10.1109/TNET.2019.2943884,title = {Advising Big Data Transfer Over Dedicated Connections Based on Profiling Optimization}, author = {Yun Daqing , Wu Chase Q. , Rao Nageswara S. V. , Kettimuthu Rajkumar },year = {2019}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/TNET.2019.2943884}, doi = {10.1109/TNET.2019.2943884}, abstract = {Big data transfer in next-generation scientific applications is now commonly carried out over dedicated channels in high-performance networks (HPNs), where transport protocols play a critical role in maximizing application-level throughput. Optimizing the performance of these protocols is challenging: i) transport protocols perform differently in various network environments, and the protocol choice is not straightforward; ii) even for a given protocol in a given environment, different parameter settings of the protocol may lead to significantly different performance and oftentimes the default setting does not yield the best performance. However, it is prohibitively time-consuming to conduct exhaustive transport profiling due to the large parameter space. In this paper, we propose a PRofiling Optimization Based DAta Transfer Advisor (ProbData) to help end users determine the most effective transport method with the most appropriate parameter settings to achieve satisfactory performance for big data transfer over dedicated connections in HPNs. ProbData employs a fast profiling scheme based on the Simultaneous Perturbation Stochastic Approximation algorithm, namely, FastProf, to accelerate the exploration of the optimal operational zones of various transport methods to improve profiling efficiency. We first present a theoretical background of the optimized profiling approach in ProbData and then detail its design and implementation. The advising procedure and performance benefits of FastProf and ProbData are illustrated and evaluated by both extensive emulations based on real-life performance measurements and experiments over various physical connections in existing production HPNs.}, pages = {2280\u20132293}, numpages = {14}}
@inproceedings{10.1145/3263376,title = {Session details: Special issue on big data analytics workshop}, author = {Lall Ashwin , Czajkowski Grzegorz , Wang Haixun },year = {2014}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3263376}, doi = {10.1145/3263376}, pages = {}, numpages = {1}}
@inproceedings{10.1145/304182.304568,title = {Improving OLTP data quality using data warehouse mechanisms}, author = {Jarke Matthias , Quix Christoph , Blees Guido , Lehmann Dirk , Michalk Gunter , Stierl Stefan },year = {1999}, isbn = {1581130848}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/304182.304568}, doi = {10.1145/304182.304568}, abstract = {Research and products for the integration of heterogeneous legacy source databases in data warehousing have addressed numerous data quality problems in or between the sources. Such a solution is marketed by Team4 for the decision support of mobile sales representatives, using advanced view maintenance and replication management techniques in an environment based on relational data warehouse technology and Lotus Notes-based client systems. However, considering total information supply chain management, the capture of poor operational data, to be cleaned later in the data warehouse, appears sub-optimal. Based on the observation that decision support clients are often closely linked to operational data entry, we have addressed the problem of mapping the data warehouse data quality techniques back to data quality measures for improving OLTP data. The solution requires a warehouse-to-OLTP workflow which employs a combination of view maintenance and view update techniques.}, location = {Philadelphia, Pennsylvania, USA}, series = {SIGMOD '99}, pages = {536\u2013537}, numpages = {2}}
@inproceedings{10.1145/304181.304568,title = {Improving OLTP data quality using data warehouse mechanisms}, author = {Jarke Matthias , Quix Christoph , Blees Guido , Lehmann Dirk , Michalk Gunter , Stierl Stefan },year = {1999}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/304181.304568}, doi = {10.1145/304181.304568}, abstract = {Research and products for the integration of heterogeneous legacy source databases in data warehousing have addressed numerous data quality problems in or between the sources. Such a solution is marketed by Team4 for the decision support of mobile sales representatives, using advanced view maintenance and replication management techniques in an environment based on relational data warehouse technology and Lotus Notes-based client systems. However, considering total information supply chain management, the capture of poor operational data, to be cleaned later in the data warehouse, appears sub-optimal. Based on the observation that decision support clients are often closely linked to operational data entry, we have addressed the problem of mapping the data warehouse data quality techniques back to data quality measures for improving OLTP data. The solution requires a warehouse-to-OLTP workflow which employs a combination of view maintenance and view update techniques.}, pages = {536\u2013537}, numpages = {2}}
@inproceedings{10.5555/3233397.3233483,title = {Using big data analytics for authorship authentication of arabic tweets}, author = {Albadarneh Jafar , Talafha Bashar , Al-Ayyoub Mahmoud , Zaqaibeh Belal , Al-Smadi Mohammad , Jararweh Yaser , Benkhelifa Elhadj },year = {2015}, isbn = {9780769556970}, publisher = {IEEE Press}, abstract = {Authorship authentication of a certain text is concerned with correctly attributing it to its author based on its contents. It is a very important problem with deep root in history as many classical texts have doubtful attributions. The information age and ubiquitous use of the Internet is further complicating this problem and adding more dimensions to it. We are interested in the modern version of this problem where the text whose authorship needs authentication is an online text found in online social networks. Specifically, we are interested in the authorship authentication of tweets. This is not the only challenging aspect we consider here. Another challenging aspect is the language of the tweets. Most current works and existing tools support English. We chose to focus on the very important, yet largely understudied, Arabic language. Finally, we add another challenging aspect to the problem at hand by addressing it at a very large scale. We present our effort to employ big data analytics to address the authorship authentication problem of Arabic tweets. We start by crawling a dataset of more than 53K tweets distributed across 20 authors. We then use preprocessing steps to clean the data and prepare it for analysis. The next step is to compute the feature vectors of each tweet. We use the Bag-Of-Words (BOW) approach and compute the weights using the Term Frequency-Inverse Document Frequency (TF-IDF). Then, we feed the dataset to a Naive Bayes classifier implemented on a parallel and distributed computing framework known as Hadoop. To the best of our knowledge, none of the previous works on authorship authentication of Arabic text addressed the unique challenges associated with (1) tweets and (2) large-scale datasets. This makes our work unique on many levels. The results show that the testing accuracy is not very high (61.6%), which is expected in the very challenging setting that we consider.}, location = {Limassol, Cyprus}, series = {UCC '15}, pages = {448\u2013452}, numpages = {5}}
@inproceedings{10.1145/3416028.3416029,title = {University Dropout Prevention through the Application of Big Data}, author = {Shiau Yeajou },year = {2020}, isbn = {9781450375467}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3416028.3416029}, doi = {10.1145/3416028.3416029}, abstract = {This study explores the reasons for the suspension and dropout of full-time university students at a university in Taiwan and suggests better timing and strategies for student counseling. In this study, the narrative statistical analysis is used to analyze and discuss the sample objects, and then use data mining technology to find characteristic phenomena and classification conditions of the students who are suspended or dropout. Other studies related to dropouts rarely use leading indicators to predict the student dropout probability in real-time, most likely because of the timeliness and availability of student data. Therefore, this study proposes to use daily changes in absence indicators as predictive variables. Through the use of discriminant analysis to construct discriminant functions, the coefficient value of each student's withdrawal from university and early warning threshold for determining withdrawal from university can be presented in real-time in order to effectively provide the student with immediate counseling.}, location = {London, United Kingdom}, series = {IMMS 2020}, pages = {1\u20137}, numpages = {7}, keywords = {Decision tree model, Bayesian probability classification table, Counseling decision, Data mining}}
@inproceedings{10.1145/3510361,title = {SUMMER: Bias-aware Prediction of Graduate Employment Based on Educational Big Data}, author = {Xia Feng , Guo Teng , Bai Xiaomei , Shatte Adrian , Liu Zitao , Tang Jiliang },year = {2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3510361}, doi = {10.1145/3510361}, abstract = {The failure of obtaining employment could lead to serious psychosocial outcomes such as depression and substance abuse, especially for college students who may be less cognitively and emotionally mature. In addition to academic performance, employers\u2019 unconscious biases are a potential obstacle to graduating students in becoming employed. Thus, it is necessary to understand the nature of such unconscious biases to assist students at an early stage with personalized intervention. In this paper, we analyze the existing bias in college graduate employment through a large-scale education dataset and develop a framework called SUMMER (biaS-aware gradUate eMployMEnt pRediction) to predict students\u2019 employment status and employment preference while considering biases. The framework consists of four major components. Firstly, we resolve the heterogeneity of student courses by embedding academic performance into a unified space. Next, we apply a Wasserstein generative adversarial network with gradient penalty (WGAN-GP) to overcome the label imbalance problem of employment data. Thirdly, we adopt a temporal convolutional network to comprehensively capture sequential information of academic performance across semesters. Finally, we design a bias-based regularization to smooth the job market biases. We conduct extensive experiments on a large-scale educational dataset and the results demonstrate the effectiveness of our prediction framework.}, pages = {1\u201324}, numpages = {24}, keywords = {bias, prediction, Graduate employment, educational big data, data analysis}}
@inproceedings{10.1145/3055167.3055180,title = {On Concise Explanations of Non-Answers over Big Data}, author = {Qi Danrui },year = {2017}, isbn = {9781450341998}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3055167.3055180}, doi = {10.1145/3055167.3055180}, location = {Chicago, Illinois, USA}, series = {SIGMOD '17}, pages = {10\u201312}, numpages = {3}, keywords = {explanation, data cleaning}}
@inproceedings{10.1109/CCGrid.2014.123,title = {Towards a collective layer in the big data stack}, author = {Gunarathne Thilina , Qiu Judy , Gannon Dennis },year = {2014}, isbn = {9781479927838}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CCGrid.2014.123}, doi = {10.1109/CCGrid.2014.123}, abstract = {We generalize MapReduce, Iterative MapReduce and data intensive MPI runtime as a layered Map-Collective architecture with Map-AllGather, Map-AllReduce, MapReduceMergeBroadcast and Map-ReduceScatter patterns as the initial focus. Map-collectives improve the performance and efficiency of the computations while at the same time facilitating ease of use for the users. These collective primitives can be applied to multiple runtimes and we propose building high performance robust implementations that cross cluster and cloud systems. Here we present results for two collectives shared between Hadoop (where we term our extension H-Collectives) on clusters and the Twister4Azure Iterative MapReduce for the Azure Cloud. Our prototype implementations of Map-AllGather and Map-AllReduce primitives achieved up to 33% performance improvement for K-means Clustering and up to 50% improvement for Multi-Dimensional Scaling, while also improving the user friendliness. In some cases, use of Map-collectives virtually eliminated almost all the overheads of the computations.}, location = {Chicago, Illinois}, series = {CCGRID '14}, pages = {236\u2013245}, numpages = {10}, keywords = {performance, twister, K-means, cloud, collectives, HPC, MDS, MapReduce}}
@inproceedings{10.1145/3358505.3358509,title = {Accounting Information Systems and ERP in the UAE: An Assessment of the Current and Future Challenges to Handle Big Data}, author = {Faccia Alessio , Mosteanu Narcisa Roxana , Fahed Mariam , Capitanio Fabian },year = {2019}, isbn = {9781450371650}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3358505.3358509}, doi = {10.1145/3358505.3358509}, abstract = {Accounting Information Systems (AISs) are fundamental for the recording of the accounting transactions of any company and for the preparation of financial statements, as required by the legislation on financial accounting. However, information systems are not only limited to accounting for the management of a company, but it is often necessary to implement additional information systems to manage other operational aspects such as inventory management, personnel, security, big data management for marketing and the provision of a broad corporate strategy. All these aspects, in general, in order to lead the company towards success, must be integrated with each other. Many software can manage all these aspects of management, but they do not allow integrated management and top management cannot control and reconcile all information. For this reason Enterprise resource plannings (ERPs), formed by different modules, are very common. ERPs allow the flow of information to be managed in an integrated manner using a single system, avoiding duplication, errors and loss and information. All the operations are reported systematically also and above all on the accounting module, which is the one that guides all the others. This research paper is focused on the comparison of currently most used AISs and ERPs in the UAE, also analyzing the market and the size of businesses in the area. The research highlighted the opportunities and limitations of the information systems currently available, as well as the characteristics of the companies that determined the dissemination of the programs in use.}, location = {Oxford, United Kingdom}, series = {ICCBDC 2019}, pages = {90\u201394}, numpages = {5}, keywords = {ERP, Accounting Information Systems, UAE, market analysis, big data}}
@inproceedings{10.1145/3288599.3295594,title = {Community cloud architecture to improve use accessibility with security compliance in health big data applications}, author = {Valluripally Samaikya , Raju Murugesan , Calyam Prasad , Chisholm Matthew , Sivarathri Sai Swathi , Mosa Abu , Joshi Trupti },year = {2019}, isbn = {9781450360944}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3288599.3295594}, doi = {10.1145/3288599.3295594}, abstract = {The adoption of big data analytics in healthcare applications is overwhelming not only because of the huge volume of data being analyzed, but also because of the heterogeneity and sensitivity of the data. Effective and efficient analysis and visualization of secure patient health records are needed to e.g., find new trends in disease management, determining risk factors for diseases, and personalized medicine. In this paper, we propose a novel community cloud architecture to help clinicians and researchers to have easy/increased accessibility to data sets from multiple sources, while also ensuring security compliance of data providers is not compromised. Our cloud-based system design configuration with cloudlet principles ensures application performance has high-speed processing, and data analytics is sufficiently scalable while adhering to security standards (e.g., HIPAA, NIST). Through a case study, we show how our community cloud architecture can be implemented along with best practices in an ophthalmology case study which includes health big data (i.e., Health Facts database, I2B2, Millennium) hosted in a campus cloud infrastructure featuring virtual desktop thin-clients and relevant Data Classification Levels in storage.}, location = {Bangalore, India}, series = {ICDCN '19}, pages = {377\u2013380}, numpages = {4}, keywords = {smart healthcare, electronic health records, security standard compliance, cloud architecture, big data application}}
@inproceedings{10.1145/3149572.3149598,title = {A Study on the Cargo Vehicle Traffic Patterns Analysis Using Big Data}, author = {Kim Tae-Hak , Kim Seong-Jin , Ok Hyun },year = {2017}, isbn = {9781450353373}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3149572.3149598}, doi = {10.1145/3149572.3149598}, abstract = {Overloading is a major factor in the damage to road facilities such as bridges and traffic accidents, and the Ministry of Land, Transport and Maritime Affairs is in the process of fixing and moving the main points to overhaul. Moving control is more effective than stationary control, but it is possible to avoid interrupting the driver because the existing control pattern is well known along with the location of intruder by intuition. Therefore, in this study, we analyzed data such as investigation of oversight activities, existing enforcement information, and traffic volume information as an improvement measure to preemptively block intermittent vehicles. The analysis methods were analyzed to derive future delivery methods, along with analysis techniques such as logistic regression analysis and analysis methods, and methods for selecting the best possible location for selecting the routes.}, location = {Barcelona, Spain}, series = {ICIME 2017}, pages = {55\u201359}, numpages = {5}, keywords = {Cargo Vehicle Traffic, Big-Data, Patterns, Overload, MOLIT, Fines imposition system, DTG, Traffic volume information system}}
@inproceedings{10.14778/2367502.2367519,title = {Interactive analytical processing in big data systems: a cross-industry study of MapReduce workloads}, author = {Chen Yanpei , Alspaugh Sara , Katz Randy },year = {2012}, publisher = {VLDB Endowment}, url = {https://doi.org/10.14778/2367502.2367519}, doi = {10.14778/2367502.2367519}, abstract = {Within the past few years, organizations in diverse industries have adopted MapReduce-based systems for large-scale data processing. Along with these new users, important new workloads have emerged which feature many small, short, and increasingly interactive jobs in addition to the large, long-running batch jobs for which MapReduce was originally designed. As interactive, large-scale query processing is a strength of the RDBMS community, it is important that lessons from that field be carried over and applied where possible in this new domain. However, these new workloads have not yet been described in the literature. We fill this gap with an empirical analysis of MapReduce traces from six separate business-critical deployments inside Facebook and at Cloudera customers in e-commerce, telecommunications, media, and retail. Our key contribution is a characterization of new MapReduce workloads which are driven in part by interactive analysis, and which make heavy use of query-like programming frameworks on top of MapReduce. These workloads display diverse behaviors which invalidate prior assumptions about MapReduce such as uniform data access, regular diurnal patterns, and prevalence of large jobs. A secondary contribution is a first step towards creating a TPC-like data processing benchmark for MapReduce.}, pages = {1802\u20131813}, numpages = {12}}
@inproceedings{10.1145/3230905.3230944,title = {A study of the application of statistical methods for Big data}, author = {Tajmouati Samya , Abarda Abdallah , El Moudden Mustapha , Dakkon Mohamed , Esghir Mustapha },year = {2018}, isbn = {9781450353045}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3230905.3230944}, doi = {10.1145/3230905.3230944}, abstract = {The use of analysis and classification methods for big data is difficult. Several proposals consist in dividing randomly the population into b sub-samples and aggregating the parameters using an estimator based on the average parameters of these selected sub-samples. This paper aims to find a solution that minimizes calculations by selecting a small number b* sub-samples and keeping the same precision. We can apply this approach to the several method to measure its relevance.}, location = {Rabat, Morocco}, series = {LOPAL '18}, pages = {1\u20136}, numpages = {6}, keywords = {Latent class analysis, massive data, classification method}}
@inproceedings{10.1145/3495018.3495446,title = {Multimedia Network Big Data Balanced Scheduling Based On Fibonacci Tree Optimization Algorithm}, author = {Chen Fei },year = {2021}, isbn = {9781450385046}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3495018.3495446}, doi = {10.1145/3495018.3495446}, abstract = {With the popularization of Internet technology, network big data has penetrated into every household. Various multimedia networks have enriched everyone's lives. At the same time, the scale of multimedia network data has become larger and larger, making the balanced scheduling of big data networks The problem is becoming more and more obvious. The problem of the unbalanced load of the multimedia network big data network has always been an object of attention. Therefore, for the problem of resource balance scheduling in multimedia network data, finding an effective network optimization algorithm plays an important role in improving the load balance of large-scale multimedia network resources. The main research content of this paper is based on the Fibonacci tree optimization algorithm. For the research on the balanced scheduling of media network big data, the Fibonacci tree optimization algorithm is a new intelligent optimization algorithm based on the Fibonacci method and the golden section method. The algorithm solves the optimal solution of the problem through alternate iterations of global exploration and local optimization. The final results of the study show that the throughput and delay of different algorithms are quite different when the number of tasks is the same. The Fibonacci tree optimization algorithm method in this paper has a throughput rate of 13.26mb/s and a delay of 16.28ms, which is similar. It has the highest throughput rate and lower latency than other algorithms, which shows that the Fibonacci tree optimization algorithm can adjust resources adaptively to the big data network environment, and the advantages of big data balanced scheduling are fully demonstrated.}, location = {Manchester, United Kingdom}, series = {AIAM2021}, pages = {1586\u20131590}, numpages = {5}}
@inproceedings{10.1145/3481056.3481080,title = {Exploration in the Ways of Mental Health Education of College Students in the Context of Big Data}, author = {Yuxi Zhai },year = {2021}, isbn = {9781450390224}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3481056.3481080}, doi = {10.1145/3481056.3481080}, abstract = {Under the background of big data, the mental health education of college students is facing new challenges and opportunities. Psychological health education for college students is an educational course that integrates theoretical knowledge teaching, psychological experience and training, and is the main front for disseminating mental health knowledge. This research comprehensively expands the various data collection methods of the curriculum, and on the basis of in-depth analysis of the massive data of college students, comprehensively grasps the problems of college students' mental health. This can guide the development of targeted education and activities, and enhance the effectiveness of mental health education for college students. Taking the mental health education of college students as the research object, explore the ways of college students' mental health education under the background of big data, and build an \"online and offline\" mixed teaching model with the help of positive psychology. The purpose of this model is to stimulate the inner positive forces of students and improve the mental health education system for college students.CCS CONCEPTS \u2022 Applied computing \u2022 Education \u2022 Computer-assisted instruction}, location = {Kyoto, Japan}, series = {ICEMT 2021}, pages = {51\u201355}, numpages = {5}, keywords = {Mental health education, Positive psychology, Big data, College students}}
@inproceedings{10.1145/3020078.3021787,title = {CPU-FPGA Co-Optimization for Big Data Applications: A Case Study of In-Memory Samtool Sorting (Abstract Only)}, author = {Cong Jason , Fang Zhenman , Huang Muhuan , Wang Libo , Wu Di },year = {2017}, isbn = {9781450343541}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3020078.3021787}, doi = {10.1145/3020078.3021787}, abstract = {To efficiently process a tremendous amount of data, today's big data applications tend to distribute the datasets into multiple partitions, such that each partition can be fit into memory and be processed by a separate core/server in parallel. Meanwhile, due to the limited scaling of general-purpose CPUs, FPGAs have emerged as an attractive alternative to accelerate big data applications due to their low power, high performance and energy efficiency. In this paper we aim to answer one key question: How should the multicore CPU and FPGA coordinate together to optimize the performance of big data applications? To address the above question, we conduct a step-by-step case study to perform CPU and FPGA co-optimization for in-memory Samtool sorting in genomic data processing, which is one of the most important big data applications for personalized healthcare. First, to accelerate the time-consuming compression algorithm and its associated cyclic redundancy check (CRC) in Samtool sorting, we implement a portable and maintainable FPGA accelerator using high-level synthesis (HLS). Although FPGAs are traditionally well-known to be suitable for compression and CRC, we find that a straightforward integration of this FPGA accelerator into the multi-threaded Samtool sorting only achieves marginal system throughput improvement over the software baseline running on a 12-core CPU. To improve system performance, we propose a dataflow execution model to effectively orchestrate the computation between the multi-threaded CPU and FPGA. Experimental results show that our co-optimized CPU-FPGA system achieves a 2.6x speedup for in-memory Samtool sorting.}, location = {Monterey, California, USA}, series = {FPGA '17}, pages = {291}, numpages = {1}, keywords = {genome data sorting, compression and CRC, dataflow execution}}
@inproceedings{10.1145/3352740.3352748,title = {An Empirical Analysis of the Performance Management System of Private College Teachers under the Background of Big Data: Taking A College as an example}, author = {Min Chen , Jinfen Ye , Haoyu Zhu },year = {2019}, isbn = {9781450372053}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3352740.3352748}, doi = {10.1145/3352740.3352748}, abstract = {In modern enterprise management, more and more companies value employee performance. Performance is equivalent to the external behavior of the company for employees. Effective performance management can stimulate all aspects of employees' work and enhance corporate image. Therefore, performance management runs through the whole work of employees. It is the key to improve employees' behavior and quality.With the acceleration of China's reform, opening up and modernization drive, more and more students need to receive higher education, which is followed by the emergence of private colleges. The number of private colleges is growing and the scale is expanding, which has gradually become an important part of higher education in China. Therefore, the management of private college teachers is crucial and inevitable. In fact, the performance management of private college teachers has become a widely studied issue. How to conduct an objective, fair and effective performance appraisal of teachers in private colleges has been paid more and more attention. Based on the reflection on the performance management of private college teachers, this paper takes a typical private college A College as an example, through the form of investigation report, to analyze the status quo and problems of teachers' performance. On this basis, the current performance management of teachers in private colleges is analyzed and suggested about improvement are put forward. Finally, through the analysis of the performance management of teachers in A College, this paper summarizes the problems that should be focused on in sustainable development of China's private colleges.}, location = {Guilin, China}, series = {EBDIT 2019}, pages = {40\u201347}, numpages = {8}, keywords = {performance, teachers, current situation, private colleges, analysis}}
@inproceedings{10.1145/3269206.3274270,title = {From Big Data to Big Information and Big Knowledge: the Case of Earth Observation Data}, author = {Bereta Konstantina , Koubarakis Manolis , Manegold Stefan , Stamoulis George , Demir Beg\u00fcm },year = {2018}, isbn = {9781450360142}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3269206.3274270}, doi = {10.1145/3269206.3274270}, abstract = {Some particularly important rich sources of open and free big geospatial data are the Earth observation (EO) programs of various countries such as the Landsat program of the US and the Copernicus programme of the European Union. EO data is a paradigmatic case of big data and the same is true for the big information and big knowledge extracted from it. EO data (satellite images and in-situ data), and the information and knowledge extracted from it, can be utilized in many applications with financial and environmental impact in areas such as emergency management, climate change, agriculture and security.}, location = {Torino, Italy}, series = {CIKM '18}, pages = {2293\u20132294}, numpages = {2}, keywords = {linked geospatial data, earth observation data, semantic web, copernicus program}}